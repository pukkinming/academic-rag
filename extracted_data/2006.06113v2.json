{
  "paper_id": "2006.06113v2",
  "title": "Continual Learning For Affective Computing",
  "published": "2020-06-10T23:36:06Z",
  "authors": [
    "Nikhil Churamani"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Real-world application requires affect perception models to be sensitive to individual differences in expression. As each user is different and expresses differently, these models need to personalise towards each individual to adequately capture their expressions and thus, model their affective state. Despite high performance on benchmarks, current approaches fall short in such adaptation. In this work, we propose the use of Continual Learning (CL) for affective computing as a paradigm for developing personalised affect perception.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Current approaches in affect perception predominantly focus on instantaneous (frame-based) analysis of human behaviour. They rely on glimpses of heightened audio-visual stimuli to infer the affective state of users  [1] ,  [2] . Even though this works well in providing a short-term evaluation of human expression, where only a snapshot of user behaviour is required, analysing long-term interactions, under varying affective contexts, is still an open problem  [3] . As a result, despite current (deep) learning approaches achieving high performance scores on expression recognition benchmarks (see  [1] ,  [4] ,  [5]  for an overview), they are not able to sufficiently model the dynamics of human affective behaviour during long-term interactions.\n\nThe development cycle for most (deep) learning approaches follows a fixed transition from first being trained in isolation on a 'large enough' dataset with high variability, and then being applied to real-world applications  [6] . With a lot of the existing datasets capturing posed expressions recorded in fixed laboratory conditions, generalisation to real-world scenarios becomes problematic  [5]  as the realworld may be very different to the conditions under which these datasets are recorded. As a result, the research has turned towards training and testing models on data that captures affect in-the-wild  [7] , containing samples collected from real-world scenarios. Yet, these models still follow the same development cycle, with little to no adaptability in their application, facing difficulties in capturing individual nuances in human affective expression.\n\nThus, there is a need for models to adapt to individual differences in expression, enabling them to personalise towards individuals, in real time. Personalisation, in this context, can be understood as the ability to account for individual differences in expression, as well as individual behaviour patterns while sensing and analysing their affective state during an interaction. Despite some efforts focussing on such personalisation to realise generic-to-specific perceptual adaptations  [8] ,  [9] , more work is needed on personalised affect perception.\n\nContinual Learning (CL) research  [10] ,  [11]  aims to address this very problem of long-term adaptability in agents, enabling them to learn incrementally as they interact with their environment. CL models are commonly applied to learning different objects and tasks in an incremental manner  [11] . The basic principles of CL, however, can also help in developing models for affect perception  [12] ,  [13]  that learn to personalise towards different users. This can be particularly beneficial in real-world interactions where social agents, embedded with such affect perception mechanisms, learn and adapt with each user they interact with. Starting from a limited general understanding, they can learn to personalise towards each user, while at the same time, learning global and generic features.\n\nIn this work, we propose Continual Learning as a learning paradigm for Affective Computing. In this paper, in particular, we discuss learning mechanisms that model generic-tospecific adaptations in Facial Expression Recognition (FER) models to enhance their personalisation capabilities. Our focus on CL approaches for lifelong learning of affect presents a two-fold problem. On the one side, the model should learn to personalise towards a particular user, learning how they express their affective state, yet, at the same time, it also needs to adapt with different users. Thus, learning happens at two-levels, individual, that is, learning different expressions of a particular user, and between-individuals, that is extending the learning to be sensitive towards different users. In our current work  [14] , we examine the former, learning different facial expression categories for the same individual. Future work will focus on extending this to between-individual adaptation, where the same model can be applied to learning with different users.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Work Summary A. Proposed Framework",
      "text": "Our recent work  [14]  presented a novel framework that investigates a Complementary Learning System (CLS)based  [15] , neuro-inspired approach for learning facial expressions. The proposed framework for Continual Learning with Imagination for Facial Expression Recognition (CLIFER) (see Fig.  1 ) consists of two components: (i) a generative auto-encoder model for imagination, that is, simulating additional facial images for individual subjects for seen and unseen classes to augment learning; and (ii) a CLS-based dual-memory learning model for FER that adapts to novel x r is encoded and passed on to the different models for further processing. Dual-memory model is also trained on the encoded x gen .  [14]  data as well as balances long-term retention of knowledge. The imagination model learns to generate facial images for 6 expression classes, namely, anger, happy, fear, sad, surprise and neutral. These generated images augment learning in the dual-memory model that learns to classify these images. The two components of the framework are briefly described here:\n\n1) Auto-Encoder-based Imagination Model: The recent success of generative models  [16] , has enabled the simulation of photo-realistic images containing human faces with different emotion expressions  [17] . Such models can be used to generate additional images for a user by translating images from seen expression classes to generate samples for yet unseen classes. Thus, having seen only a few images of a subject, the model can generate additional data (akin to imagination in humans) for the individual to preempt future interactions. For an agent, such a model can be applied to realise imagined contact  [18]  with a participant simulating imagination as a substitute for sensory experience.\n\nTo embed such capabilities in the CLIFER framework, we employ a Conditional Adversarial Auto-Encoder (CAAE)based  [17]  imagination model (see Fig.  1 ) that takes an original (input) image (x r ) and generates translated images (x gen ) for each of the 6 expressions.\n\n2) CLS-based Dual-Memory Model: The Growing Dual Memory (GDM) architecture  [15]  is used as the basis for incrementally acquiring and integrating knowledge in the CLIFER framework. It consists of two hierarchically arranged recurrent Growing When Required (GWR) neural networks  [19]  representing the episodic (GDM-E) and semantic (GDM-S) memories, respectively. which rapidly learns (using a high learning-rate) nonoverlapping representations. This is achieved using a distance-based similarity measure, implementing unsupervised Hebbian-based learning. As it receives data, one class at a time, it creates feature prototypes for each input sample, rapidly adapting to novel data.\n\n• Semantic Memory: The GDM-S learns compact overlapping representations that generalise across samples from a particular class. After each episode (mini-batch) of sequential input, GDM-S receives the winner neurons from GDM-E, along with label annotations. A frequency-based associative labelling scheme  [15]  is used to associate feature prototypes with their respective labels with new neurons added to the GDM-S only if the existing neurons cannot correctly classify the input. Periodically, neural activation trajectories from GDM-E are replayed to both GDM-E and GDM-S for pseudorehearsal of past knowledge, mitigating forgetting. • Imagination: After receiving data samples from a particular class, winner neurons (feature prototypes) from the GDM-E are passed to the imagination model which simulates facial images of a subject for each expression class, preserving the identity of the subject. These imagined images are encoded and replayed to both GDM-E and GDM-S, augmenting learning in CLIFER.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "B. Experimentation And Results",
      "text": "We conduct two experiments to evaluate the CLIFER framework on its ability to (1) remember previously seen expression classes for an individual and (2) to extend its learning to yet unseen facial expressions. CLIFER is trained and tested separately for each subject from the RAVDESS  [20] , MMI  [21]  and BAUM-1  [22]  datasets. While RAVDESS and MMI datasets provide an evaluation on posed samples, BAUM-1 evaluates the model on spontaneous FER.\n\nIn our experiments we compare four different models, namely; (i) the GDM model without replay, (ii) GDM model with the pseudo-replay mechanism (see  [15]  for details), (iii) the proposed CLIFER framework, and (iv) a Multilayer Perceptron (MLP)-based classifier that acts as a baseline for traditional batch learning.\n\n1) Results: The GDM architecture aims to learn distinguishable feature representations for each class, making learning class-order agnostic. In practice, however, for FER we found the model's performance to be sensitive to the order of learning different classes for each subject. To quantify this, we selected 6 different class orders, starting with each of the 6 classes used in this work. The rest of the order was randomised.\n\nKruskal-Wallis H-test results show a significant difference (p < 0.05) in model performance for Experiment 2 between the 6 class orders with starting with neutral resulting in the best performance, on average. A similar effect is seen for Experiment 1. As the model learns how one individual expresses different emotions, the learnt feature representations overlap significantly resulting in the order impacting model performance. Other approaches in curriculum-based learning  [23] , that focus on learning facial expressions one Fig.  2 : F1-Scores with 95% confidence intervals on RAVDESS, MMI and BAUM-1 datasets. Adapted from  [14] . class at a time, have also witnessed a specific order of learning (starting with high-intensity samples) enhancing model performance although they do not evaluate the models for continual learning. Starting with neutral could be beneficial for CL-bsed FER models for two reasons. Firstly, neutral represents a baseline for an individual's expressions and learning this norm enables the model to form distinct prototypes for subsequent samples that differ from this baseline. Secondly, as imagination impacts model performance, imagined images can carry forward some of the features from the original image. Starting with neutral, however, results in the least influence of the original image.\n\nAs a result, for our experimentation, we set the order of learning classes to start with neutral, followed by (randomly selected) happy, surprise, anger, fear and sadness. The results for the RAVDESS, MMI and BAUM-1 datasets, averaged across all subjects, for the two experiments can be seen in Fig.  2a  and Fig.  2b , respectively. The GDM model outperforms the MLP baseline for all the 3 datasets for both the experiments. The GDM + Replay and the proposed CLIFER framework (that is, GDM with imagination) perform better than the standard GDM model, with CLIFER, on average, performing the best across all settings resulting in high F1-scores: RAVDESS (episodic: F1= 0.98 ± 0.01, semantic: F1= 0.75±0.01), MMI (episodic: F1= 0.75±0.07, semantic: F1= 0.46 ± 0.04) and BAUM-1 (episodic: F1= 0.87 ± 0.05, semantic: F1= 0.51 ± 0.04).\n\nThese results highlight the framework's ability to adapt to an individual subject, extending its knowledge to novel classes while retaining performance on previously learnt classes. The model performance is comparable (if not better) to the state-of-the-art for RAVDESS (79%  [24] ), MMI (78%  [25] ) and BAUM-1 (47%  [22] ). Yet, it will not be correct to compare these scores directly as they do not use incremental learning for training the models and have all the data available to them apriori. Furthermore, one thing to note here is that, we select a sub-set of subjects from these datasets that provide data samples for each of the 6 expression classes  [14] . More experimentation is needed to further establish the performance of the CLIFER framework for all the subjects.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Iii. Future Plans",
      "text": "A. Expanding CLIFER Experimentation with CLIFER  [14] , as discussed above, substantiated the applicability of CL approaches for affect perception. Yet, one thing to be noted here is that such an application of CL is not as straightforward as other learning tasks, for example, as learning to classify objects. Important aspects such as the order of learning and context have a huge impact on learning to classify facial expressions. Furthermore, human expressions should not be viewed as isolated instances of occurrence, particularly in real-world interactions, but need to be understood as context-driven responses that evolve over a period of time in response to affective stimuli. Thus, accounting for such a temporal evolution of expression becomes crucial in recognising the affective state expressed by a user.\n\nFurthermore, adopting a lifelong and adaptive view on affect modelling, it is important not only recognise expressions but also model the person's long-term behaviour. Analysing how their affective behaviour evolves over time, the model can learn not just their expressions but also estimate the mood of an individual during an interaction as well as their long-term personality. We are currently exploring recurrent and self-organising neural models for spatio-temporal feature learning that can enable modelling the affective state of an individual at varying temporal resolutions. Based on neuroinspired mechanisms for affective learning, that is, the interplay between the short-term and long-term memory models in our brain that contribute towards affective association  [26] ,  [27] , these models will fit well with the CLIFER framework, extending it towards a multi-memory set-up that evaluate affective expressions at different temporal resolutions.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "B. Clifer For Human-Robot Interaction",
      "text": "Real-world human-robot interactions provide the best application conditions for CLIFER as they require robots to adapt to the dynamics of each interaction, offering personalised interactions to the users. In particular, longitudinal interactions, where a user and the robot interact with each other repeatedly, over several interactions, require the agent to incrementally improve its understanding of user behaviour. In such interactions, CLIFER, after each interaction, should be able to imagine the user under different interaction conditions and update its learning to improve its performance for each subsequent interaction round.\n\nTo evaluate such personalised affect perception models, we will conduct a user-study with the Pepper Robot 1 . The userstudy will involve participants repeatedly interacting with Pepper over multiple interaction sessions. Each session will be designed in a manner that it elicits a specific affective response (for example, anger or happiness) from the user. The task for the robot will be to learn to recognise the user's expression, personalising towards their expressions. Two conditions will be compared. In the first condition, Pepper will use a state-of-the-art FER model while the second condition will implement the CLIFER framework for affect perception. It is expected that the CLIFER framework should perform better in recognising the facial expressions of the users, even after only a few interactions, and also incrementally improve its performance.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Challenges",
      "text": "One of the key challenges faced for person-specific adaptation is the lack of long-term interaction data for training the models. Most of the existing datasets, even if recording spontaneous expressions, consist of data recorded for different individuals only over a handful of interaction sessions. Also, as these interaction sessions are usually recorded all together, the data does not enable modelling affective behaviour dynamics of the individual, over time.\n\nIn the CLIFER framework, we tackle the issue with lack of data by using the imagination model. It enables us to generate additional data for an individual for the different expression classes. Yet, this may restrict the model's capability to handling only a few expression classes. Thus, our future work plans to extend the framework to recognising different Action Units (AUs), which will enable adaptation to a wide variety of facial expressions. Additionally, we aim to consider dimensional (valence-arousal) information, moving away from categorical labels to enhance the applicability of the framework to the real-world.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ) consists of two components: (i) a",
      "page": 1
    },
    {
      "caption": "Figure 1: The CLIFER Framework for FER with Imagination:",
      "page": 2
    },
    {
      "caption": "Figure 1: ) that takes an",
      "page": 2
    },
    {
      "caption": "Figure 2: F1-Scores with 95% conﬁdence intervals on RAVDESS, MMI and BAUM-1 datasets. Adapted from [14].",
      "page": 3
    },
    {
      "caption": "Figure 2: a and Fig. 2b, respectively. The GDM",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "Anger",
          "Column_2": "Happy",
          "Column_3": "Fear",
          "Column_4": "Sad",
          "Column_5": "Surprise Neutral"
        }
      ],
      "page": 2
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Learning bases of activity for facial expression recognition",
      "authors": [
        "E Sariyanidi",
        "H Gunes",
        "A Cavallaro"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "2",
      "title": "Towards an intelligent framework for multimodal affective data analysis",
      "authors": [
        "S Poria",
        "E Cambria",
        "A Hussain",
        "G.-B Huang"
      ],
      "year": "2015",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "3",
      "title": "Automatic analysis of facial affect: A survey of registration, representation, and recognition",
      "authors": [
        "E Sariyanidi",
        "H Gunes",
        "A Cavallaro"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "4",
      "title": "Automatic analysis of facial actions: a survey",
      "authors": [
        "B Martinez",
        "M Valstar",
        "B Jiang",
        "M Pantic"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "5",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "6",
      "title": "Deep learning in neural networks: An overview",
      "authors": [
        "J Schmidhuber"
      ],
      "year": "2015",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "7",
      "title": "Emotion recognition in the wild challenge 2014: Baseline, data and protocol",
      "authors": [
        "A Dhall",
        "R Goecke",
        "J Joshi",
        "K Sikka",
        "T Gedeon"
      ],
      "year": "2014",
      "venue": "Proceedings of the 16th International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "8",
      "title": "Personalized machine learning for robot perception of affect and engagement in autism therapy",
      "authors": [
        "O Rudovic",
        "J Lee",
        "M Dai",
        "B Schuller",
        "R Picard"
      ],
      "year": "2018",
      "venue": "Science Robotics"
    },
    {
      "citation_id": "9",
      "title": "Selective transfer machine for personalized facial expression analysis",
      "authors": [
        "W Chu",
        "F Torre",
        "J Cohn"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "10",
      "title": "Neuroscience-inspired artificial intelligence",
      "authors": [
        "D Hassabis",
        "D Kumaran",
        "C Summerfield",
        "M Botvinick"
      ],
      "year": "2017",
      "venue": "Neuron"
    },
    {
      "citation_id": "11",
      "title": "Continual lifelong learning with neural networks: A review",
      "authors": [
        "G Parisi",
        "R Kemker",
        "J Part",
        "C Kanan",
        "S Wermter"
      ],
      "year": "2019",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "12",
      "title": "Zero-shot facial expression recognition with multi-label label propagation",
      "authors": [
        "Z Lu",
        "J Zeng",
        "S Shan",
        "X Chen"
      ],
      "year": "2019",
      "venue": "Computer Vision -ACCV 2018"
    },
    {
      "citation_id": "13",
      "title": "A personalized affective memory model for improving emotion recognition",
      "authors": [
        "P Barros",
        "G Parisi",
        "S Wermter"
      ],
      "year": "2019",
      "venue": "ICML"
    },
    {
      "citation_id": "14",
      "title": "CLIFER: Continual Learning with Imagination for Facial Expression Recognition",
      "authors": [
        "N Churamani",
        "H Gunes"
      ],
      "year": "2020",
      "venue": "Proceedings of the 15th International Conference on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "15",
      "title": "Lifelong learning of spatiotemporal representations with dual-memory recurrent selforganization",
      "authors": [
        "G Parisi",
        "J Tani",
        "C Weber",
        "S Wermter"
      ],
      "year": "2018",
      "venue": "Frontiers in Neurorobotics"
    },
    {
      "citation_id": "16",
      "title": "Adversarial training in affective computing and sentiment analysis: Recent advances and perspectives [review article]",
      "authors": [
        "J Han",
        "Z Zhang",
        "N Cummins",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "IEEE Computational Intelligence Magazine"
    },
    {
      "citation_id": "17",
      "title": "ExprGAN: Facial Expression Editing with Controllable Expression Intensity",
      "authors": [
        "H Ding",
        "K Sricharan",
        "R Chellappa"
      ],
      "year": "2018",
      "venue": "AAAI"
    },
    {
      "citation_id": "18",
      "title": "Imagine how to behave: the influence of imagined contact on human-robot interaction",
      "authors": [
        "R Wullenkord",
        "F Eyssel"
      ],
      "year": "2019",
      "venue": "Philosophical Transactions of the Royal Society B: Biological Sciences"
    },
    {
      "citation_id": "19",
      "title": "A self-organising network that grows when required",
      "authors": [
        "S Marsland",
        "J Shapiro",
        "U Nehmzow"
      ],
      "year": "2002",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "20",
      "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PLOS ONE"
    },
    {
      "citation_id": "21",
      "title": "Induced Disgust, Happiness and Surprise: an Addition to the MMI Facial Expression Database",
      "authors": [
        "M Valstar",
        "M Pantic"
      ],
      "year": "2010",
      "venue": "Proc. LREC Workshop on EMOTION"
    },
    {
      "citation_id": "22",
      "title": "BAUM-1: A Spontaneous Audio-Visual Face Database of Affective and Mental States",
      "authors": [
        "S Zhalehpour",
        "O Onder",
        "Z Akhtar",
        "C Erdem"
      ],
      "year": "2017",
      "venue": "IEEE Trans. Affective Computing"
    },
    {
      "citation_id": "23",
      "title": "Curriculum learning for facial expression recognition",
      "authors": [
        "L Gui",
        "T Baltrušaitis",
        "L Morency"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Automatic Face Gesture Recognition (FG 2017)"
    },
    {
      "citation_id": "24",
      "title": "Human emotion recognition in video using subtraction pre-processing",
      "authors": [
        "Z He",
        "T Jin",
        "A Basu",
        "J Soraghan",
        "G Caterina",
        "L Petropoulakis"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 11th International Conference on Machine Learning and Computing, ser. ICMLC '19"
    },
    {
      "citation_id": "25",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "26",
      "title": "Arousal-mediated memory consolidation: Role of the medial temporal lobe in humans",
      "authors": [
        "K Labar",
        "E Phelps"
      ],
      "year": "1998",
      "venue": "Psychological Science"
    },
    {
      "citation_id": "27",
      "title": "Remembering Emotional Experiences: The Contribution of Valence and Arousal, ser",
      "authors": [
        "E Kensinger"
      ],
      "year": "2004",
      "venue": "Reviews in the Neurosciences"
    }
  ]
}