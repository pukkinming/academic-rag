{
  "paper_id": "2312.04293v3",
  "title": "Gpt-4V With Emotion: A Zero-Shot Benchmark For Generalized Emotion Recognition",
  "published": "2023-12-07T13:27:37Z",
  "authors": [
    "Zheng Lian",
    "Licai Sun",
    "Haiyang Sun",
    "Kang Chen",
    "Zhuofan Wen",
    "Hao Gu",
    "Bin Liu",
    "Jianhua Tao"
  ],
  "keywords": [
    "Generalized Emotion Recognition (GER)",
    "GPT-4 with Vision (GPT-4V)",
    "Zero-shot Benchmark",
    "Multimodal Fusion",
    "Temporal Modeling ACC amusement, sadness, anger, fear, disgust, awe, content, excitement Tweet Sentiment Analysis (Image, Text) MVSA-Single [6] 451 ACC positive, neutral, negative MVSA-Multiple [6] 1,702 ACC positive, neutral, negative Micro-expression Recognition (Image) CASME [19] 195 ACC happiness, sadness, fear, surprise, disgust, repression, contempt, tense CASME II [20] 247 ACC happiness, sadness, fear, surprise, disgust, repression, others SAMM [21] 159 ACC happiness, sadness, fear, surprise, disgust, contempt, anger, others Facial Emotion Recognition (Image) CK+ [22] 981 ACC happiness, sadness, anger, fear, disgust, surprise, contempt SFEW 2.0 [23] 436 ACC happiness, sadness, anger, fear, disgust, surprise, neutral RAF-DB [24] 3,068 ACC happiness, sadness, anger, fear, disgust, surprise, neutral FERPlus [25] 3,589 ACC happiness, sadness, anger, fear, disgust, surprise, neutral, contempt AffectNet [26] 4,000 ACC happiness, sadness, anger, fear, disgust, surprise, neutral, contempt Dynamic Facial Emotion Recognition (Video) DFEW (fd1) [27] 2,341 WAR happiness, sadness, anger, fear, disgust, surprise, neutral FERV39k [9] 7,847 WAR happiness, sadness, anger, fear, disgust, surprise, neutral RAVDESS [28] 1,440 WAR happiness, sadness, anger, fear, disgust, surprise, neutral, calm eNTERFACE05 [29] 1,287 WAR happiness, sadness, anger, fear, disgust, surprise Multimodal Emotion Recognition (Video, Audio, Text) CMU-MOSI [30] 686 WAF sentiment intensity CH-SIMS [31] 457 WAF sentiment intensity MER-MULTI [32] 411 WAF happiness, sadness, anger, surprise, neutral, worry Dynamic Facial Emotion Recognition Multimodal Emotion Recognition CK+ SFEW 2.0 FERPlus RAF-DB AffectNet Facial Emotion Recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recently, GPT-4 with Vision (GPT-4V) has demonstrated remarkable visual capabilities across various tasks, but its performance in emotion recognition has not been fully evaluated. To bridge this gap, we present the quantitative evaluation results of GPT-4V on 21 benchmark datasets covering 6 tasks: visual sentiment analysis, tweet sentiment analysis, micro-expression recognition, facial emotion recognition, dynamic facial emotion recognition, and multimodal emotion recognition. This paper collectively refers to these tasks as \"Generalized Emotion Recognition (GER)\". Through experimental analysis, we observe that GPT-4V exhibits strong visual understanding capabilities in GER tasks. Meanwhile, GPT-4V shows the ability to integrate multimodal clues and exploit temporal information, which is also critical for emotion recognition. However, it's worth noting that GPT-4V is primarily designed for general domains and cannot recognize micro-expressions that require specialized knowledge. To the best of our knowledge, this paper provides the first quantitative assessment of GPT-4V for GER tasks. We have open-sourced the code and encourage subsequent researchers to broaden the evaluation scope by including more tasks and datasets. Our code and evaluation results are available at: https://github.com/zeroQiaoba/gpt4v-emotion.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion has attracted increasing attention from researchers due to its importance in human-computer interaction. Current research on emotion recognition primarily centers on two aspects: one aims to identify the emotions evoked by stimuli, essentially predicting how viewers might feel after seeing these stimuli  [1] ; the other aims to analyze the emotions conveyed by humans in various ways  [2] . We collectively refer to these tasks as \"Generalized Emotion Recognition (GER)\".\n\nEmotions are associated with lexical, visual, and acoustic information. Among them, visual information (such as colorfulness, brightness, facial expression, human action, etc) contains rich emotion-related content  [3] . To enhance visual understanding capabilities, researchers have proposed various algorithms and achieved noteworthy progress. With the development of deep learning, current research has shifted from handcrafted features to deep neural networks. Recently, GPT-4V has showcased impressive visual understanding capabilities across various tasks and domains. This leads to a question: can GPT-4V solve the GER problem to some extent? If so, what is the future direction following the emergence of GPT-4V?\n\nIn September 2023, GPT-4V was integrated into ChatGPT, triggering a series of user reports aimed at investigating its visual capabilities  [4] . At that time, OpenAI had not yet released the API service, and users could only manually upload test samples to the web service. Due to the high manual effort required, these reports generally involved a limited number of samples per task and merely provided qualitative insights into GPT-4V. In November 2023, OpenAI released the API, but it was initially limited to 100 requests per day. It remained difficult to evaluate GPT-4V against state-of-the-art systems on benchmark datasets. Recently, OpenAI has increased the daily limit, allowing us to conduct more comprehensive evaluations.\n\nIn this paper, we provide quantitative evaluation results of GPT-4V on GER tasks, covering visual sentiment analysis  [5] , tweet sentiment analysis  [6] , micro-expression recognition  [7] , facial emotion recognition  [8] , dynamic facial emotion recognition  [9] , and multimodal emotion recognition  [10] . Figure  1  shows the overall results of GPT-4V. We also report results of random guessing (a baseline that randomly selects labels from candidate classes) and supervised systems. To fairly compare different methods, we conduct experiments on benchmark datasets and use the same evaluation metric. Overall, GPT-4V outperforms random guessing but still lags behind supervised systems. To figure out the reason, we further conduct a comprehensive analysis of GPT-4V's multi-faceted capabilities, including multimodal fusion, temporal modeling, robustness, stability, etc. We hope this paper can inspire subsequent researchers and shed light on tasks that GPT-4V can effectively address and those that require further exploration. The main contribution of this paper can be summarized as follows:\n\nFigure  1 : Performance of different methods on GER tasks. Here, \"random\" is a heuristic baseline that randomly selects labels from candidate categories.\n\n• To the best of our knowledge, this is the first work to quantitatively assess GPT-4V's performance in GER tasks.\n\n• Our evaluation results indicate that GPT-4V is primarily designed for general-purpose domains. It performs poorly in micro-expression recognition that requires specialized knowledge. Meanwhile, it can integrate multimodal clues and capture temporal information during inference.\n\n• This paper can provide guidance on potential future directions following the emergence of GPT-4V. It can also serve as a zero-shot benchmark for subsequent research.\n\nThe remainder of this paper is organized as follows: In Section 2, we briefly review recent works. In Sections 3 and 4, we provide an overview of GER tasks and datasets, along with a detailed description of our GPT-4V calling strategy. In Section 5, we report results and conduct an in-depth analysis of GPT-4V's multi-faceted capabilities. In Sections 6 and 7, we summarize the main challenges and limitations of GPT-4, discuss potential directions for future research, and conclude the entire paper.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Works",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Generalized Emotion Recognition",
      "text": "GER covers a variety of tasks and this paper focuses on 6 representative tasks. In this section, we detail the similarities and distinctions among them. Visual sentiment analysis  [5]  explores the impact of image stimuli on people's emotions, while tweet sentiment analysis  [6]  aims to determine how individuals feel when posting visual and textual content on social networks. In these tasks, the image is not necessarily human-centric. In contrast, facial emotion recognition  [8]  aims to identify emotions in human-centric images, particularly focusing on observable macro-expressions. Besides macro-expressions, there is another type of expression known as micro-expressions, which are characterized by short duration, low intensity, and sparse facial action units. The task of identifying such expressions is termed micro-expression recognition  [7] . All of the aforementioned tasks identify emotions from static images. Dynamic facial emotion recognition  [9]  extends the analysis from static images to image sequences, requiring the use of temporal information. Furthermore, emotions can be conveyed through multiple modalities. Therefore, multimodal emotion recognition  [10]  is introduced to integrate multimodal clues for a more comprehensive understanding of emotions.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Unimodal Emotion Recognition",
      "text": "Emotions can be conveyed through multiple modalities, such as images, text, and audio. Since the early 1970s, Ekman et al.  [11, 12]  have conducted prior studies on facial expressions and developed the Facial Action Coding System (FACS) to code facial expressions. In this system, the face can be described as a set of action units, each associated with a muscular basis. Besides facial expressions, emotions can also be conveyed through text and audio, which carry important communicative messages. Text and audio are highly correlated. On the one hand, we can recognize the text from audio. On the other hand, if we ignore the manner in which the text is spoken, we might misunderstand the meaning of the text  [13] . With the development of deep learning, researchers have proposed various effective frameworks for emotion recognition  [14] . These models are usually task-specific, i.e., we need to train on emotion corpora. In contrast, this paper uses GPT-4V, which can recognize emotions in a zero-shot manner without further training.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Multimodal Emotion Recognition",
      "text": "Humans express emotions in a complementary manner across various modalities. To better understand emotions, we need to integrate multimodal clues. According to the fusion location, current methods can be roughly divided into three categories: feature-level fusion, model-level fusion, and decisionlevel fusion  [15] . Specifically, feature-level fusion concatenates unimodal features at the input level, while decision-level fusion combines unimodal decision results. Despite their promising performance, these methods ignore interactions and correlations between different modalities. Therefore, researchers propose model-level fusion to capture multimodal dependencies. Recently, Lian et al.  [10]  built MERBench and conducted a systematic analysis of different model-level fusion strategies. Unlike the above methods, GPT-4V achieves multimodal fusion through prompts. More details can be found in Table  2 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multimodal Large Language Model",
      "text": "Large language models (LLMs) with massive parameters have attracted increasing attention due to their unprecedented performance. In GER tasks, visual information plays an important role in emotion recognition, but LLMs only support lexical inputs. Therefore, we focus on multimodal LLMs (MLLMs) that support visual inputs. Among all MLLMs, MiniGPT-4  [33]  and LLaVA  [34]  are two representative models. They enhance the visual capability by aligning the frozen visual encoder with  LLMs using a simple projection layer, which preserves most of the information encoded in pre-trained models and achieves good performance in visual understanding. This approach is further extended by other works and applies to support more modalities, such as video  [35] , audio  [36] , and depth images  [37] . Currently, GPT-4V is the state-of-the-art MLLM for various tasks  [38, 39] . However, its performance in emotion recognition has not been fully studied. Hence, this paper evaluates the performance of GPT-4V on GER tasks, providing a valuable zero-shot benchmark for other MLLMs.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Evaluation On Gpt-4V",
      "text": "With the emergence of GPT-4V, a large number of user reports have surfaced to evaluate its visual ability  [40, 41] . Among them, Yang et al.  [4]  conducted the most comprehensive assessment, covering coding, captioning, and reasoning abilities. However, they primarily performed qualitative analysis on limited data and did not disclose the performance gap between GPT-4V and supervised systems on benchmark datasets. To bridge this gap, there have been efforts to quantitatively analyze GPT-4V's mathematical reasoning  [39]  and visual abilities  [42] , but none have specifically explored its emotional understanding capabilities. In this paper, we provide the first quantitative evaluation of GPT-4V on GER tasks. It is worth noting that the emotion is related to humans. However, human identity is treated as sensitive information in GPT-4V, which raises new requirements for the calling strategy. To this end, we also design a calling strategy specifically for GER tasks.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Task Description",
      "text": "In this section, we describe each task and dataset in detail. Table  1  summarizes statistics of different datasets. To ensure a fair comparison with supervised systems, we evaluate performance on the official test set and select the most common evaluation metric. Figure  2  shows examples of different datasets, demonstrating the diversity of dataset types. Some datasets are in-the-wild (such as AffectNet), while others are laboratorycontrolled (such as CASME and CK+). Meanwhile, there is a disparity in color spaces across distinct datasets. Some datasets use grayscale images (such as CK+), while others use RGB images (such as CASME and AffectNet).\n\nVisual Sentiment Analysis aims to identify the emotions that individuals might evoke from images. We focus on four datasets: Twitter I  [16] , Twitter II  [17] , ArtPhoto  [18] , and Abstract  [18] . Specifically, Twitter I  [16]  and Twitter II  [17]  are collected from social websites. Of the two, Twitter I provides raw annotations from 5 Amazon Mechanical Turk workers. Consistent with previous works, we employ three label processing methods, resulting in Twitter I(3), Twitter I(4), and Twitter I(5). Taking Twitter I(4) as an example, it contains all samples where at least 4 workers assign the same label. Different from Twitter I and Twitter II, ArtPhoto  [18]  contains artistic photographs from a photo-sharing site, while Abstract  [18]  consists of peer-rated abstract paintings. Both datasets are annotated into eight categories. In line with previous works, we reassign these labels into positive and negative classes and report results for the negative/positive classification task.\n\nTweet Sentiment Analysis aims to determine how people feel when posting visual and textual content on social networks. We conduct experiments on two benchmark datasets: MVSA-Single  [6]  and MVSA-Multiple  [6] . These datasets contain image-text pairs collected from Twitter, and annotators assign one of three sentiments (positive, negative, and neutral) to the image and text respectively. MVSA-Single contains 5,129 image-text pairs labeled by a single annotator. MVSA-Multiple is an extended version of MVSA-Single with more samples and more annotators. For a fair comparison, we adopt the same dataset processing strategy in previous works, and then randomly select 10% of the samples as the test set  [43, 44] .\n\nFacial Emotion Recognition focuses on five benchmark datasets: CK+  [22] , FERPlus  [25] , SFEW 2.0  [23] , RAF-DB  [24] , and AffectNet  [26] . The first two datasets contain grayscale images, while the last three datasets contain RGB images. Specifically, CK+  [22]  includes 593 video sequences from 123 subjects. Following previous work  [45] , we extract the last three frames of each sequence to construct the dataset. RAF-DB  [24]  contains thousands of samples with basic and compound expressions, and this paper focuses on basic emotions. AffectNet  [26]  has 8 labels and each label contains 500 samples for evaluation. For FERPlus  [25]  and SFEW 2.0  [23] , we use the official test set for performance evaluation.\n\nMicro-expression Recognition aims to identify subtle changes in human faces. Early works have proved the feasibility of recognizing micro-expressions from an apex frame  [46] . Therefore, we use the apex frame in our evaluation. Following previous works, we adopt the same label processing method and concentrate on major emotions. Specifically, CASME  [19]  comprises 195 samples across 8 categories, and we focus on the performance of four main labels: tense, disgust, repression, and surprise. CASME II  [20]  includes 247 samples collected from 26 subjects, and we focus on five major labels: happiness, surprise, disgust, repression, and others. SAMM  [21]  consists of 159 samples, and our evaluation is limited to labels with more than 10 samples: anger, contempt, happiness, surprise, others.\n\nDynamic Facial Emotion Recognition focuses on more challenging image sequences. In this task, we conduct experiments on four benchmark datasets: FERV39k  [9] , RAVDESS  [28] , eNTERFACE05  [29] , and DFEW  [27] . For the first three Algorithm 1: GPT-4V Calling Strategy\n\ni=1 with N samples, the maximum number of calls T max . Output: Prediction results for all samples.\n\n1 Def Main(D): datasets, we evaluate the performance on the official test set. DFEW  [27]  contains 5 folds with a total of 11,697 samples. To reduce the evaluation cost, we only report the results of fold 1 (fd1), consistent with previous works  [47, 48] .\n\nMultimodal Emotion Recognition focuses on three benchmark datasets: CH-SIMS  [30] , CMU-MOSI  [31] , and MER-MULTI  [32] . The first two datasets offer sentiment intensity scores. In this paper, we concentrate on the negative/positive classification task, where positive and negative classes are assigned for < 0 and > 0 scores, respectively. MER-MULTI is a subset of the MER2023 dataset  [32] , providing both discrete and dimensional labels for each sample. In this paper, we focus on the recognition performance of discrete emotions.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Gpt-4V Calling Strategy",
      "text": "This paper evaluates the performance of the GPT-4V API, \"gpt-4-vision-preview\". GER tasks involve diverse modalities, such as images, text, video, and audio. However, GPT-4V only supports image and text inputs. To process video data, we sample the video and convert it into multiple images. To process audio data, we try converting the audio to a mel-spectrogram, which is a typical visual representation of the frequency content of an audio signal over time, with the frequency axis scaled Table  2 : Prompt template for different tasks. Please replace \"Candidate labels\" in the following prompts into candidate labels of each dataset. This paper uses batch-wise calling strategy to handle request limits. By default, we set the batch size to 20 for image-level inputs and 6 for video-level inputs.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Task Prompt",
      "text": "Visual Sentiment Analysis Please play the role of an emotion recognition expert. We provide 20 images. Please recognize sentiments evoked by these images (i.e., guess how viewer might emotionally feel after seeing these images.) If there is a person in the image, ignore that person's identity. For each image, please sort the provided categories from high to low according to the similarity with the image. Here are the optional categories: {Candidate labels}. The output format should be {'name':, 'result':} for each image.",
      "page_start": 6,
      "page_end": 14
    },
    {
      "section_name": "Tweet Sentiment Analysis",
      "text": "Please play the role of an emotion recognition expert. We provide 20 image-text pairs. Please analyze how he will feel if he post this image-text pair on social media. If there is a person in the image, ignore that person's identity. For each image-text pair, please sort the provided categories from high to low according to the similarity with the input image-text pair. Here are the optional categories: {Candidate labels}. The output format should be {'name':, 'result':} for each image-text pair.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Micro-Expression Rec",
      "text": "Please play the role of a micro-expression recognition expert. We provide 20 images. For each image, please sort the provided categories from high to low according to the similarity with the input image. The expression may not be obvious, please pay attention to the details of the face. Here are the optional categories: {Candidate labels}. Please ignore the speaker's identity and focus on the facial expression. The output format should be {'name':, 'result':} for each image.\n\nFacial Emotion Rec.\n\nPlease play the role of a facial expression classification expert. We provide 20 images. For each image, please sort the provided categories from high to low according to the top 5 similarity with the input image. Here are the optional categories: {Candidate labels}. Please ignore the speaker's identity and focus on the facial expression. The output format should be {'name':, 'result':} for each image.\n\nDynamic Facial Emotion Rec.\n\nPlease play the role of a video expression classification expert. We provide 6 videos, each with 3 temporally uniformly sampled frames. For each video, please sort the provided categories from high to low according to the top 5 similarity with the input video. Here are the optional categories: {Candidate labels}. Please ignore the speaker's identity and focus on the facial expression. The output format should be {'name':, 'result':} for each video.\n\nMultimodal Emotion Rec.\n\nPlease play the role of a video expression classification expert. We provide 6 videos, each with the speaker's content and 3 temporally uniformly sampled frames. Please ignore the speaker's identity and focus on their emotions. For each video, please sort the provided categories from high to low according to the top 5 similarity with the input video. Here are the optional categories: {Candidate labels}. Please ignore the speaker's identity and focus on their emotions. The output format should be {'name':, 'result':} for each video.\n\naccording to the mel scale  [49] . However, GPT-4V fails to generate proper responses for the mel-spectrogram. Therefore, the primary focus of this paper revolves around images, text, and video. In this section, we design a calling strategy specifically for GER tasks, containing batch-wise, repeated, and recursive calling. Pseudo-code is summarized in Algorithm 1. Batch-wise Calling. GPT-4V API has three limitations on requests: tokens per minute (TPM), requests per minute (RPM), and requests per day (RPD), which introduces additional requirements to our prompt design. To meet the constraints of RPM and RPD, we follow previous work  [42]  and adopt batchwise inputs. That is, we feed multiple samples into GPT-4V and generate all results in one request. However, a large batch size may result in the total number of tokens exceeding the TPM limitation. Additionally, it increases task difficulty and may lead to incorrect responses. For example, if we input 100 samples in a request, GPT-4V might only generate predictions for 96 samples. Therefore, we set the batch size to 20 for imagelevel inputs and 6 for video-level inputs, aiming to satisfy TPM, RPM, and RPD limitations simultaneously. Prompts for different tasks can be found in Table  2 .\n\nRepeated Calling. GER tasks often trigger security checks, causing GPT-4V to refuse to provide a response. This is attributed to the inclusion of visual sentiment analysis and human emotion recognition. The former task contains violent and bloody images. In the latter task, human identities are also considered as sensitive information. To reduce rejected cases, we require GPT-4V to ignore speaker identity (see Table  2 ), but it still triggers security errors. Interestingly, these errors sometimes occur randomly. For example, although all images are human-centric, some pass security checks while others fail. Alternatively, a sample may initially fail the check but pass upon retry. Therefore, we make multiple calls to the rejected batches.\n\nRecursive Calling. During the evaluation, we notice that a batch-wise input may fail security checks, but splitting it into smaller parts can sometimes pass checks. Therefore, for the consistently failing batch, we split it into two smaller minibatches and feed them into GPT-4V separately. This operation is repeated until further splitting is no longer feasible.\n\nCombination. Our strategy combines batch-wise, repeated, and recursive calling. More details can be found in Algorithm 1. A qualified response O should satisfy two conditions. First, it should not trigger the security check. Second, it should contain the correct number of prediction results. Figures .8∼.  13  provide examples of qualified responses for different GER tasks.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results And Discussion",
      "text": "This section presents and discusses results from three levels: dataset-wise, class-wise, and sample-wise. Specifically, we first present the results of heuristics baselines, supervised systems, and GPT-4V. Then, we reveal the temporal modeling and multimodal fusion capabilities of GPT-4V. Following that, we analyze the system's stability and conduct an in-depth class-wise performance analysis. Next, we assess the system's robustness against changes in prompt templates and color spaces. Finally, we visualize the rejected and erroneously predicted samples.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Main Results",
      "text": "Besides GPT-4V, we introduce two heuristic baselines: random guessing and majority guessing. For the former, we randomly select a label from candidate labels. For the latter, we choose the majority label as the prediction. For both baselines, we run experiments 10 times and report average results.\n\nTable  3  shows the results of visual sentiment analysis. We observe that GPT-4V outperforms supervised systems on most datasets. This superior performance can be attributed to its strong visual understanding ability, coupled with its reasoning capability, allowing GPT-4V to accurately infer the emotional states evoked by images. But for micro-expression recognition (see Table  4 ), GPT-4V exhibits poor performance, even worse than heuristic baselines. These results suggest that GPT-4V is designed for general-purpose domains (i.e., emotions that can be perceived by normal people). It is not well-suited for tasks that require professional knowledge like micro-expressions.\n\nTables 5∼8 present the results of tweet sentiment analysis, multimodal emotion recognition, facial emotion recognition, and dynamic facial emotion recognition, respectively. To process video data, we uniformly sample frames and feed these frames into GPT-4V sequentially. To reduce call costs, we sample up to three frames per video. Despite a performance gap still existing between GPT-4V and supervised systems, it is worth noting that GPT-4V can significantly outperform heuristic baselines, demonstrating its potential in emotion recognition.\n\nIn Table  6 , we observe a larger performance gap between supervised systems and GPT-4V on MER-MULTI compared to CMU-MOSI and CH-SIMS. This can be attributed to the fact that supervised systems incorporate acoustic information, while GPT-4V does not support audio input. Given that audio is more crucial in MER-MULTI  [10] , GPT-4V loses more information in this dataset, leading to limited performance.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Temporal Modeling Ability",
      "text": "To reduce GPT-4V call costs, this paper samples up to three frames per video. In this section, we further reveal the impact of sampling numbers. In Table  8 , the performance improves when we increase the sampling number from two to three, indicating GPT-4V's temporal understanding ability. Additionally, it is worth noting that despite setting the sampling number to three, some key frames may still be ignored. Therefore, there is a possibility for further improvement by sampling more frames, and we leave this discussion for subsequent researchers.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Multimodal Fusion Ability",
      "text": "This section evaluates GPT-4V's multimodal fusion ability. Among all tasks, tweet emotion recognition and multimodal emotion recognition provide multimodal information. Hence, we conduct experiments on these tasks. Table  9  shows unimodal and multimodal results. In general, multimodal results outperform unimodal results, showcasing GPT-4V's ability to integrate and exploit multi-modalities. But for CMU-MOSI, we notice a slight decrease in multimodal results compared to unimodal results. This can be attributed to CMU-MOSI mainly relying on lexical content to convey emotions  [10] , and the incorporation of visual clues may introduce interference.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "System Stability",
      "text": "This section evaluates GPT-4V's prediction stability. During the experiments, we run GPT-4V 10 times for each sample in SFEW 2.0. Figure  3a  shows the frequency of identical predictions. Specifically, we assume that GPT-4V predicts negative 8 times and negative 2 times. Therefore, it predicts the same label at most c = 8 times. Then, we compute c for all samples and calculate its frequency. In Figure  3b , we report the test accuracy for each run. We observe that although more than 50% of the samples exhibit the same result across 10 runs, there are also some samples with different prediction results across distinct runs, leading to fluctuations in test accuracy. Meanwhile, we observe a 4.60% performance gap between the best and worst runs. Therefore, GPT-4V exhibits certain instability. We recommend that subsequent researchers evaluate GPT-4V multiple times and use majority voting to get final results.",
      "page_start": 8,
      "page_end": 10
    },
    {
      "section_name": "Class-Wise Performance Analysis",
      "text": "In Figure  4 , we visualize confusion matrices and perform a class-wise performance analysis. For visual sentiment analysis, GPT-4V generally performs well in positive and negative classes, showcasing its visual understanding ability. However, it exhibits lower results on Abstract compared to other datasets. This may be attributed to the fact that GPT-4V is mainly trained on natural images. The domain gap between abstract and natural images leads to increased difficulty and limited performance. For tweet sentiment analysis, GPT-4V performs relatively poorly in identifying neutral sentiment and often misclassifies it as positive or negative. This may be due to GPT-4V's ability to detect subtle emotional cues in image-text pairs. For micro-expression recognition, GPT-4V prefers tense and repression. Compared to other labels (e.g., surprise and disgust), tense and repression are less noticeable and closer to neutral. These results indicate that GPT-4V tends to view microexpressions as neutral, causing it to perform poorly on this task. For facial expression recognition, GPT-4V tends to exhibit lower accuracy on contempt, disgust, and fear, while achieving higher accuracy on happiness. This phenomenon is related to the clarity of emotion definition. Since contempt, disgust, and fear have less consensus between different annotations, they are inherently more difficult to recognize  [26, 99] .\n\nFor dynamic facial emotion recognition, most samples tend to be predicted as neutral, which is more obvious in FERV39. We attribute this phenomenon to our sampling operation. For each video, we sample up to three frames. GPT-4V tends to predict the emotion as neutral if there is no emotion-related information within these frames. For multimodal emotion recognition, GPT-4V performs well on worry and happiness. For  We provide 20 images. For each image, please sort the provided categories from high to low according to the top 5 similarity with the input image. Here are the optional categories: {Candidate labels}. Please ignore the speaker's identity and focus on the facial expression.\n\nThe output format should be {'name':, 'result':} for each image.",
      "page_start": 11,
      "page_end": 14
    },
    {
      "section_name": "#3",
      "text": "Please play the role of a facial expression classification expert. We provide 20 images. For each image, please select the most likely category according to the correlation with the input image. Here are the optional categories: {Candidate labels}. Please ignore the speaker's identity and focus on the facial expression. The output format should be {'name':, 'result':} for each image.\n\nworry, it has a strong reliance on lexical information. Differently, happiness is mainly expressed through facial expressions  [32] . To achieve good performance, GPT-4V should use different predominant modalities for distinct emotions, showcasing its multimodal fusion capabilities.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Robustness To Template Change",
      "text": "Previous works have pointed out that pre-trained language models (such as BERT and RoBERTa) are sensitive to prompt templates, with different templates yield varying results  [100] . This raises the question: is GPT-4V also sensitive to template changes? In this section, we conduct experiments on SFEW 2.0 and compare the performance of different templates. In Table  10 , Template#1 is our default template. Compared with Template#1, Template#2 deletes \"Please play the role of a facial expression classification expert\" and Template#3 changes the selection of the top 5 classes into the selection of the most likely class. Experimental results demonstrate that Template#2 (57.93%) and Template#3 (54.25%) can achieve similar perfor-mance compared to Template#1 (57.24%). Therefore, we can conclude that GPT-4V is robust to template changes.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Robustness To Color Space",
      "text": "This section evaluates the robustness of GPT-4V to color space changes. We conduct experiments on RAF-DB and convert RGB images into grayscale images. We observe that GPT-4V achieves nearly identical results on grayscale (74.28%) and RGB images (75.81%). These results demonstrate its robustness to color space changes. In Figure  5 , we further analyze the class-wise prediction consistency. We observe that the consistency rate is around 80% for most classes except for fear and disgust. We further compare Figure  5  with the confusion matrix in Figure  4m . We observe similarities between these figures, where fear and disgust also perform poorly in emotion recognition. Therefore, we can conclude that poorly-performing categories in GPT-4V exhibit greater inconsistency in different color spaces. This may be attributed to poorly-performing categories with greater randomness across different runs.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Security Check",
      "text": "GER includes visual sentiment analysis (containing images with violent and bloody content) and human emotion recognition (where human identity is considered sensitive information in GPT-4V). Therefore, it often triggers the security check.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Case Study",
      "text": "In Figure  7 , we visualize examples where GPT-4V makes incorrect predictions. More examples can be found in Figures .8∼.13 in the Appendix. For visual sentiment analysis (see Twitter I), we notice that GPT-4V can generate reasonable predictions. The main reason for these errors is the inherent multiple meanings of images. Take the image on the left as an example. One interpretation could emphasize that the sun rises, breaking through dark clouds and bathing the trees in sunlight, evoking positive emotions. Conversely, another explanation could emphasize that the sun is obstructed by dark clouds, triggering negative emotions. The same phenomenon can also be found in the picture on the right. On the one hand, there is a cute cat, but on the other hand, the cat is locked in a cage.\n\nFor tweet sentiment analysis (see MVSA-Multiple), the image shows a hat on wood, accompanied by descriptive text about the hat. Although the label is neutral, GPT-4V predicts a positive sentiment. We speculate that GPT-4V may interpret  this image as depicting a comfortable environment and a nice hat, thereby eliciting a positive response.\n\nFor micro-expression recognition (see CASME), we observe that her emotion is not obvious. Without specialized knowledge, it is difficult to identify her emotion as disgust and more likely to be predicted as neutral. For CASME, there are four candidate labels: tense, disgust, repression, and surprise. Among them, tense is closer to neutral. Consequently, GPT-4V incorrectly predicts her emotion as tense. In the future, we recommend that researchers provide clearer definitions of microexpressions in the prompt. For example, offer typical clues to distinguish these micro-expressions. We believe this strategy can enhance the performance of GPT-4V on this task.\n\nFor facial emotion recognition (see RAF-DB), the image depicts a baby with an open mouth, indicating that she is crying. Meanwhile, she is wearing an oxygen mask, indicating that she is sick. Hence, her emotion is likely to be sad. But GPT-4V incorrectly predicts her emotion as neutral. This may be due to its inability to recognize the oxygen mask and infer her health status, indicating its limitations in understanding the details.\n\nFor dynamic facial emotion recognition (see DFEW), GPT-4V might predict the emotion as happiness based on the raised corners of the mouth. But when considering the subtitle, \"cannot go to the academy, read books, and listen to music\", we can infer that he is more likely to be sad due to these restrictions. Hence, the error made by GPT-4V could be attributed to the limitations of this task, as it relies only on the visual modality.\n\nFor multimodal emotion recognition (see CH-SIMS), GPT-4V predicts the emotion as positive based on his facial expression. However, when taking into account the subtitle, \"I feel like I'm falling apart\", the content of his complaint shifts his emotion towards negative. However, GPT-4V is unable to capture such emotional transitions, indicating its limitations in multimodal fusion under complex scenarios.\n\nIn summary, GPT-4V can provide relatively reasonable responses. These errors are mainly caused by the inherent multiple meanings of images or the limited capability to understand the details. Meanwhile, micro-expressions are different from regular emotions and GPT-4V faces challenges in recognizing them. Furthermore, its multimodal fusion ability may face challenges when different modalities express distinct emotions.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Limitations And Future Directions",
      "text": "In this section, we summarize the main challenges and limitations of GPT-4V as revealed by the evaluation results. Additionally, we discuss potential directions for future research.\n\nPerformance. In GER tasks, GPT-4V outperforms heuristic baselines but still lags behind supervised systems. This may be attributed to the vague definition of emotion  [101] . In the future, we plan to use few-shot prompts to enhance task clarity and GPT-4V's emotion understanding ability.\n\nSupport Modality. GPT-4V does not support audio. However, audio can convey emotions through prosodic information such as pitch, duration, and intensity. To better recognize emotions, GPT-4V should support audio in the future. Alternatively, we can use outside models to convert audio into lexical descriptions and use them as additional input for GPT-4V. Domain Gap. GPT-4V is designed for general-purpose domains and performs poorly on micro-expressions that require specialized knowledge. For future work, we recommend that subsequent researchers can attempt to provide clearer definitions of micro-expressions in the prompt design.\n\nSystem Stability. GPT-4V exhibits certain instability in prediction results, i.e., it may predict different labels for a sample. Therefore, GPT-4V should improve its system stability in the future. Alternatively, subsequent works should evaluate GPT-4V multiple times and use majority voting to get final results.\n\nSecurity Check Stability. GPT-4V demonstrates certain instability in security checks. Specifically, a sample may initially fail the check but pass after a retry. To solve this problem, we design a more practical calling strategy in Section 4. In the future, GPT-4V should enhance its security check stability.\n\nDespite the above limitations, GPT-4V has demonstrated promising results in emotion recognition and holds application potential in practical scenarios such as social media analysis, online education, and customer service. Different from previous solutions that required task-specific models tailored to each scenario, GPT-4V can serve as a universal model capable of minimizing application costs.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Conclusion",
      "text": "This paper conducts a comprehensive evaluation of GPT-4V in GER tasks. Our evaluation results demonstrate that GPT-4V exhibits strong visual understanding capabilities and achieves promising results in general-purpose emotion recognition. However, it performs poorly in micro-expression recognition which requires specialized knowledge. This paper also showcases GPT-4V's temporal modeling and multimodal fusion capabilities, along with its robustness to color space and prompt template changes. Furthermore, we evaluate its prediction consistency and security check stability and visualize error cases to reveal its limitations in emotion understanding. This paper can serve as a zero-shot benchmark and provide guidance for subsequent research on emotion recognition and MLLMs.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows the overall results of GPT-4V. We also report results",
      "page": 1
    },
    {
      "caption": "Figure 1: Performance of different methods on GER tasks. Here, “random” is",
      "page": 2
    },
    {
      "caption": "Figure 2: Examples of different datasets. We pixelate faces due to the sensitive nature of human identity.",
      "page": 3
    },
    {
      "caption": "Figure 2: shows examples of different datasets,",
      "page": 3
    },
    {
      "caption": "Figure 3: a shows the frequency of identical predic-",
      "page": 6
    },
    {
      "caption": "Figure 3: b, we report the test accuracy",
      "page": 6
    },
    {
      "caption": "Figure 4: , we visualize confusion matrices and perform a",
      "page": 7
    },
    {
      "caption": "Figure 3: System stability test. We run GPT-4V 10 times, calculate the fre-",
      "page": 7
    },
    {
      "caption": "Figure 4: Confusion matrices for all evaluation datasets. Different colors represent distinct tasks.",
      "page": 8
    },
    {
      "caption": "Figure 5: , we further analyze the",
      "page": 8
    },
    {
      "caption": "Figure 5: with the confusion matrix",
      "page": 8
    },
    {
      "caption": "Figure 4: m. We observe similarities between these figures,",
      "page": 8
    },
    {
      "caption": "Figure 5: Prediction consistency on RGB and grayscale images.",
      "page": 9
    },
    {
      "caption": "Figure 6: Passed and rejected examples by GPT-4V.",
      "page": 9
    },
    {
      "caption": "Figure 7: , we visualize examples where GPT-4V makes",
      "page": 9
    },
    {
      "caption": "Figure 7: Error cases predicted by GPT-4V.",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table 1: summarizes statistics of different datasets. To ensure",
      "data": [
        {
          "Task (Modality)": "Visual Sentiment Analysis\n(Image)",
          "Dataset\n# Test samples Metric\nLabels": "Twitter I [16]\n1,269\nACC\npositive, negative\nTwitter II [17]\n603\nACC\npositive, negative\nAbstract [18]\n228\nACC\namusement, sadness, anger, fear, disgust, awe, content, excitement\nArtPhoto [18]\n806\nACC\namusement, sadness, anger, fear, disgust, awe, content, excitement"
        },
        {
          "Task (Modality)": "Tweet Sentiment Analysis\n(Image, Text)",
          "Dataset\n# Test samples Metric\nLabels": "MVSA-Single [6]\n451\nACC\npositive, neutral, negative\nMVSA-Multiple [6]\n1,702\nACC\npositive, neutral, negative"
        },
        {
          "Task (Modality)": "Micro-expression Recognition\n(Image)",
          "Dataset\n# Test samples Metric\nLabels": "CASME [19]\n195\nACC\nhappiness, sadness, fear, surprise, disgust, repression, contempt, tense\nCASME II [20]\n247\nACC\nhappiness, sadness, fear, surprise, disgust, repression, others\nSAMM [21]\n159\nACC\nhappiness, sadness, fear, surprise, disgust, contempt, anger, others"
        },
        {
          "Task (Modality)": "Facial Emotion Recognition\n(Image)",
          "Dataset\n# Test samples Metric\nLabels": "CK+ [22]\n981\nACC\nhappiness, sadness, anger, fear, disgust, surprise, contempt\nSFEW 2.0 [23]\n436\nACC\nhappiness, sadness, anger, fear, disgust, surprise, neutral\nRAF-DB [24]\n3,068\nACC\nhappiness, sadness, anger, fear, disgust, surprise, neutral\nFERPlus [25]\n3,589\nACC\nhappiness, sadness, anger, fear, disgust, surprise, neutral, contempt\nAffectNet [26]\n4,000\nACC\nhappiness, sadness, anger, fear, disgust, surprise, neutral, contempt"
        },
        {
          "Task (Modality)": "Dynamic Facial Emotion Recognition\n(Video)",
          "Dataset\n# Test samples Metric\nLabels": "DFEW (fd1) [27]\n2,341\nWAR\nhappiness, sadness, anger, fear, disgust, surprise, neutral\nFERV39k [9]\n7,847\nWAR\nhappiness, sadness, anger, fear, disgust, surprise, neutral\nRAVDESS [28]\n1,440\nWAR\nhappiness, sadness, anger, fear, disgust, surprise, neutral, calm\neNTERFACE05 [29]\n1,287\nWAR\nhappiness, sadness, anger, fear, disgust, surprise"
        },
        {
          "Task (Modality)": "Multimodal Emotion Recognition\n(Video, Audio, Text)",
          "Dataset\n# Test samples Metric\nLabels": "CMU-MOSI [30]\n686\nWAF\nsentiment intensity\nCH-SIMS [31]\n457\nWAF\nsentiment intensity\nMER-MULTI [32]\n411\nWAF\nhappiness, sadness, anger, surprise, neutral, worry"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Task": "Visual Sentiment Analysis",
          "Prompt": "Please play the role of an emotion recognition expert. We provide 20 images. Please recognize sentiments evoked by these\nimages (i.e., guess how viewer might emotionally feel after seeing these images.) If there is a person in the image, ignore that\nperson’s identity. For each image, please sort\nthe provided categories from high to low according to the similarity with the\nimage. Here are the optional categories: {Candidate labels}. The output format should be {’name’:, ’result’:} for each image."
        },
        {
          "Task": "Tweet Sentiment Analysis",
          "Prompt": "Please play the role of an emotion recognition expert. We provide 20 image-text pairs. Please analyze how he will feel if he\npost this image-text pair on social media.\nIf there is a person in the image, ignore that person’s identity. For each image-text\npair, please sort the provided categories from high to low according to the similarity with the input image-text pair. Here are\nthe optional categories: {Candidate labels}. The output format should be {’name’:, ’result’:} for each image-text pair."
        },
        {
          "Task": "Micro-expression Rec",
          "Prompt": "Please play the role of a micro-expression recognition expert. We provide 20 images. For each image, please sort the provided\ncategories from high to low according to the similarity with the input image. The expression may not be obvious, please pay\nattention to the details of the face. Here are the optional categories:\n{Candidate labels}. Please ignore the speaker’s identity\nand focus on the facial expression. The output format should be {’name’:, ’result’:} for each image."
        },
        {
          "Task": "Facial Emotion Rec.",
          "Prompt": "Please play the role of a facial expression classification expert. We provide 20 images. For each image, please sort the provided\ncategories from high to low according to the top 5 similarity with the input image. Here are the optional categories: {Candidate\nlabels}. Please ignore the speaker’s identity and focus on the facial expression. The output format should be {’name’:, ’result’:}\nfor each image."
        },
        {
          "Task": "Dynamic Facial Emotion Rec.",
          "Prompt": "Please play the role of a video expression classification expert. We provide 6 videos, each with 3 temporally uniformly\nsampled frames. For each video, please sort\nthe provided categories from high to low according to the top 5 similarity with\nthe input video. Here are the optional categories:\n{Candidate labels}. Please ignore the speaker’s identity and focus on the\nfacial expression. The output format should be {’name’:, ’result’:} for each video."
        },
        {
          "Task": "Multimodal Emotion Rec.",
          "Prompt": "Please play the role of a video expression classification expert. We provide 6 videos, each with the speaker’s content and\n3 temporally uniformly sampled frames. Please ignore the speaker’s identity and focus on their emotions. For each video,\nplease sort\nthe provided categories from high to low according to the top 5 similarity with the input video. Here are the\noptional categories:\n{Candidate labels}. Please ignore the speaker’s identity and focus on their emotions. The output format\nshould be {’name’:, ’result’:} for each video."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model\nCK+": "Cross-VAE [67]\n94.96\nIA-gen [71]\n96.57\nADFL [74]\n98.17\nIDFERM [78]\n98.35\nTER-GAN [82]\n98.47\n98.65\nIPD-FER [45]",
          "Model\nSFEW 2.0": "IACNN [68]\n50.98\nDLP-CNN [24]\n51.05\nTDTLN [75]\n53.10\nDAN [79]\n53.18\nRAN [70]\n54.19\n58.50\nFaceCaps [84]",
          "Model\nRAF-DB": "SCN [69]\n87.03\nEfficientFace [72]\n88.36\nMA-Net [73]\n88.42\nTransFER [80]\n90.91\nPOSTER [83]\n92.05\n92.21\nPOSTER++ [85]",
          "Model\nFERPlus": "RAN[70]\n89.16\nSCN [69]\n89.39\nEAC [76]\n89.64\nKTN[81]\n90.49\nTransFER[80]\n90.83\n91.62\nPOSTER [83]",
          "Model\nAffectNet": "SCN [69]\n60.23\nMA-Net [73]\n60.29\nARM [77]\n61.33\nDAN [79]\n62.09\nPOSTER [83]\n63.34\n63.77\nPOSTER++ [85]"
        },
        {
          "Model\nCK+": "Random\n14.61\nMajority\n25.38\n69.72\nGPT-4V",
          "Model\nSFEW 2.0": "Random\n13.98\nMajority\n19.77\n57.24\nGPT-4V",
          "Model\nRAF-DB": "Random\n14.57\nMajority\n38.64\n75.81\nGPT-4V",
          "Model\nFERPlus": "Random\n12.61\nMajority\n35.75\n64.25\nGPT-4V",
          "Model\nAffectNet": "Random\n12.73\nMajority\n12.50\n42.77\nGPT-4V"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model\nDFEW": "C3D [86]\n56.37\nResNet50-LSTM [47]\n63.32\nDPCNet [47]\n65.78\nSVFAP [95]\n74.81\nMAE-DFER [96]\n74.88\n76.16\nS2D [48]",
          "Model\nFERV39k": "M3DFEL [87]\n47.67\nSTT [90]\n48.11\nIAL [93]\n48.54\nMAE-DFER [96]\n52.07\nSVFAP [95]\n52.29\n52.56\nS2D [48]",
          "Model\nRAVDESS": "AV-LSTM [88]\n65.80\nMCBP [91]\n71.32\nMSAF [91]\n74.86\nSVFAP [95]\n75.01\nMAE-DFER [96]\n75.56\n75.76\nCFN-SR [98]",
          "Model\neNTERFACE05": "STA-FER [89]\n42.98\nEC-LSTM [92]\n49.26\nFAN [94]\n51.44\nGraph-Tran [97]\n54.62\nSVFAP [95]\n60.54\n61.64\nMAE-DFER [96]"
        },
        {
          "Model\nDFEW": "Random\n14.43\nMajority\n22.81",
          "Model\nFERV39k": "Random\n14.41\nMajority\n24.79",
          "Model\nRAVDESS": "Random\n11.86\nMajority\n13.33",
          "Model\neNTERFACE05": "Random\n16.87\nMajority\n16.71"
        },
        {
          "Model\nDFEW": "GPT-4V (2-frm)\n43.80\n54.80\nGPT-4V (3-frm)",
          "Model\nFERV39k": "GPT-4V (2-frm)\n34.29\n38.72\nGPT-4V (3-frm)",
          "Model\nRAVDESS": "GPT-4V (2-frm)\n22.29\n34.31\nGPT-4V (3-frm)",
          "Model\neNTERFACE05": "GPT-4V (2-frm)\n23.78\n33.28\nGPT-4V (3-frm)"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 10: , Template#1 is our default template. Compared with in Figure 4m. We observe similarities between these figures,",
      "data": [
        {
          "ID": "#1",
          "Prompt Template": "Please play the role of a facial expression classification expert. We provide 20 images.\nFor each image, please sort\nthe provided\ncategories from high to low according to the top 5 similarity with the input image. Here are the optional categories: {Candidate labels}.\nPlease ignore the speaker’s identity and focus on the facial expression. The output format should be {’name’:, ’result’:} for each image."
        },
        {
          "ID": "#2",
          "Prompt Template": "We provide 20 images. For each image, please sort the provided categories from high to low according to the top 5 similarity with the\ninput image. Here are the optional categories: {Candidate labels}. Please ignore the speaker’s identity and focus on the facial expression.\nThe output format should be {’name’:, ’result’:} for each image."
        },
        {
          "ID": "#3",
          "Prompt Template": "Please play the role of a facial expression classification expert. We provide 20 images. For each image, please select\nthe most\nlikely\ncategory according to the correlation with the input\nimage. Here are the optional categories:\n{Candidate labels}.\nPlease ignore the\nspeaker’s identity and focus on the facial expression. The output format should be {’name’:, ’result’:} for each image."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Prompt: Please play the role of an emotion recognition expert. We provide 4 images. Please recognize sentiments evoked\nby these images (i.e., guess how viewer might emotionally feel after seeing these images.) If there is a person in the image,\nignore that person's\nidentity. For each image, please sort\nsimilarity with the input\nimage. Here are the optional categories:\n{'name':,\n'result':} for each image.\nResponse from GPT-4V: As an emotion recognition expert,\nevoked by viewing each of the provided images. Please note that\nto person. 1. {'name':\n'Image 1',\n{'name':\n'Image 3',\n'result': ['positive’,\nAnswer Extraction: 1: negative, 2: negative, 3: positive, 4: negative\nGround Truth: 1: positive, 2: positive, 3: positive, 4: negative": ""
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Visual sentiment analysis by attending on local image regions",
      "authors": [
        "Q You",
        "H Jin",
        "J Luo"
      ],
      "year": "2017",
      "venue": "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "2",
      "title": "Smin: Semi-supervised multi-modal interaction network for conversational emotion recognition",
      "authors": [
        "Z Lian",
        "B Liu",
        "J Tao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "3",
      "title": "Emoset: A large-scale visual emotion dataset with rich attributes",
      "authors": [
        "J Yang",
        "Q Huang",
        "T Ding",
        "D Lischinski",
        "D Cohen-Or",
        "H Huang"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "4",
      "title": "The dawn of lmms: Preliminary explorations with gpt-4v (ision)",
      "authors": [
        "Z Yang",
        "L Li",
        "K Lin",
        "J Wang",
        "C.-C Lin",
        "Z Liu",
        "L Wang"
      ],
      "year": "2023",
      "venue": "The dawn of lmms: Preliminary explorations with gpt-4v (ision)",
      "arxiv": "arXiv:2309.174219"
    },
    {
      "citation_id": "5",
      "title": "Survey on visual sentiment analysis",
      "authors": [
        "A Ortis",
        "G Farinella",
        "S Battiato"
      ],
      "year": "2020",
      "venue": "IET Image Processing"
    },
    {
      "citation_id": "6",
      "title": "Sentiment analysis on multi-view social data",
      "authors": [
        "T Niu",
        "S Zhu",
        "L Pang",
        "A Saddik"
      ],
      "year": "2016",
      "venue": "MultiMedia Modeling: 22nd International Conference, MMM 2016"
    },
    {
      "citation_id": "7",
      "title": "Deep learning for microexpression recognition: A survey",
      "authors": [
        "Y Li",
        "J Wei",
        "Y Liu",
        "J Kauttonen",
        "G Zhao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "8",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2020",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "9",
      "title": "Ferv39k: A large-scale multi-scene dataset for facial expression recognition in videos",
      "authors": [
        "Y Wang",
        "Y Sun",
        "Y Huang",
        "Z Liu",
        "S Gao",
        "W Zhang",
        "W Ge",
        "W Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "10",
      "title": "Merbench: A unified evaluation benchmark for multimodal emotion recognition",
      "authors": [
        "Z Lian",
        "L Sun",
        "Y Ren",
        "H Gu",
        "H Sun",
        "L Chen",
        "B Liu",
        "J Tao"
      ],
      "year": "2024",
      "venue": "Merbench: A unified evaluation benchmark for multimodal emotion recognition",
      "arxiv": "arXiv:2401.03429"
    },
    {
      "citation_id": "11",
      "title": "Facial action coding system",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1978",
      "venue": "Environmental Psychology & Nonverbal Behavior"
    },
    {
      "citation_id": "12",
      "title": "Strong evidence for universals in facial expressions: a reply to russell's mistaken critique",
      "authors": [
        "P Ekman"
      ],
      "year": "1994",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "13",
      "title": "Multimodal approaches for emotion recognition: a survey",
      "authors": [
        "N Sebe",
        "I Cohen",
        "T Gevers",
        "T Huang"
      ],
      "year": "2005",
      "venue": "Internet Imaging VI"
    },
    {
      "citation_id": "14",
      "title": "Ctnet: Conversational transformer network for emotion recognition",
      "authors": [
        "Z Lian",
        "B Liu",
        "J Tao"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "15",
      "title": "Survey on audiovisual emotion recognition: databases, features, and data fusion strategies",
      "authors": [
        "C.-H Wu",
        "J.-C Lin",
        "W.-L Wei"
      ],
      "year": "2014",
      "venue": "APSIPA Transactions on Signal and Information Processing"
    },
    {
      "citation_id": "16",
      "title": "Robust image sentiment analysis using progressively trained and domain transferred deep networks",
      "authors": [
        "Q You",
        "J Luo",
        "H Jin",
        "J Yang"
      ],
      "year": "2015",
      "venue": "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "17",
      "title": "Large-scale visual sentiment ontology and detectors using adjective noun pairs",
      "authors": [
        "D Borth",
        "R Ji",
        "T Chen",
        "T Breuel",
        "S.-F Chang"
      ],
      "year": "2013",
      "venue": "Proceedings of the 21st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "18",
      "title": "Building a large scale dataset for image emotion recognition: the fine print and the benchmark",
      "authors": [
        "Q You",
        "J Luo",
        "H Jin",
        "J Yang"
      ],
      "year": "2016",
      "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "19",
      "title": "Casme database: A dataset of spontaneous micro-expressions collected from neutralized faces",
      "authors": [
        "W.-J Yan",
        "Q Wu",
        "Y.-J Liu",
        "S.-J Wang",
        "X Fu"
      ],
      "year": "2013",
      "venue": "Proceedings of the 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "20",
      "title": "Casme ii: An improved spontaneous micro-expression database and the baseline evaluation",
      "authors": [
        "W.-J Yan",
        "X Li",
        "S.-J Wang",
        "G Zhao",
        "Y.-J Liu",
        "Y.-H Chen",
        "X Fu"
      ],
      "year": "2014",
      "venue": "PLoS ONE"
    },
    {
      "citation_id": "21",
      "title": "Samm: A spontaneous micro-facial movement dataset",
      "authors": [
        "A Davison",
        "C Lansley",
        "N Costen",
        "K Tan",
        "M Yap"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "22",
      "title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "Z Ambadar",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "23",
      "title": "Video and image based emotion recognition challenges in the wild: Emotiw",
      "authors": [
        "A Dhall",
        "O Ramana Murthy",
        "R Goecke",
        "J Joshi",
        "T Gedeon"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 ACM on International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "24",
      "title": "Reliable crowdsourcing and deep localitypreserving learning for expression recognition in the wild",
      "authors": [
        "S Li",
        "W Deng",
        "J Du"
      ],
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni-2017"
    },
    {
      "citation_id": "25",
      "title": "Training deep networks for facial expression recognition with crowd-sourced label distribution",
      "authors": [
        "E Barsoum",
        "C Zhang",
        "C Ferrer",
        "Z Zhang"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "26",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "27",
      "title": "Dfew: A large-scale database for recognizing dynamic facial expressions in the wild",
      "authors": [
        "X Jiang",
        "Y Zong",
        "W Zheng",
        "C Tang",
        "W Xia",
        "C Lu",
        "J Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM international conference on multimedia"
    },
    {
      "citation_id": "28",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS One"
    },
    {
      "citation_id": "29",
      "title": "The enterface'05 audio-visual emotion database",
      "authors": [
        "O Martin",
        "I Kotsia",
        "B Macq",
        "I Pitas"
      ],
      "year": "2006",
      "venue": "Proceedings of the 22nd International Conference on Data Engineering Workshops"
    },
    {
      "citation_id": "30",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "A Zadeh",
        "M Chen",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "31",
      "title": "Chsims: A chinese multimodal sentiment analysis dataset with fine-grained annotation of modality",
      "authors": [
        "W Yu",
        "H Xu",
        "F Meng",
        "Y Zhu",
        "Y Ma",
        "J Wu",
        "J Zou",
        "K Yang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "32",
      "title": "Multi-label learning, modality robustness, and semi-supervised learning",
      "authors": [
        "Z Lian",
        "H Sun",
        "L Sun",
        "K Chen",
        "M Xu",
        "K Wang",
        "K Xu",
        "Y He",
        "Y Li",
        "J Zhao"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "33",
      "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "authors": [
        "D Zhu",
        "J Chen",
        "X Shen",
        "X Li",
        "M Elhoseiny"
      ],
      "year": "2023",
      "venue": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "arxiv": "arXiv:2304.10592"
    },
    {
      "citation_id": "34",
      "title": "Visual instruction tuning",
      "authors": [
        "H Liu",
        "C Li",
        "Q Wu",
        "Y Lee"
      ],
      "year": "2023",
      "venue": "Visual instruction tuning",
      "arxiv": "arXiv:2304.08485"
    },
    {
      "citation_id": "35",
      "title": "Videochat: Chat-centric video understanding",
      "authors": [
        "K Li",
        "Y He",
        "Y Wang",
        "Y Li",
        "W Wang",
        "P Luo",
        "Y Wang",
        "L Wang",
        "Y Qiao"
      ],
      "year": "2023",
      "venue": "Videochat: Chat-centric video understanding",
      "arxiv": "arXiv:2305.06355"
    },
    {
      "citation_id": "36",
      "title": "Speechgpt: Empowering large language models with intrinsic crossmodal conversational abilities",
      "authors": [
        "D Zhang",
        "S Li",
        "X Zhang",
        "J Zhan",
        "P Wang",
        "Y Zhou",
        "X Qiu"
      ],
      "year": "2023",
      "venue": "Speechgpt: Empowering large language models with intrinsic crossmodal conversational abilities",
      "arxiv": "arXiv:2305.11000"
    },
    {
      "citation_id": "37",
      "title": "One model to instruction-follow them all",
      "authors": [
        "Y Su",
        "T Lan",
        "H Li",
        "J Xu",
        "Y Wang",
        "D Cai"
      ],
      "year": "2023",
      "venue": "One model to instruction-follow them all",
      "arxiv": "arXiv:2305.16355"
    },
    {
      "citation_id": "38",
      "title": "Alpacaeval: An automatic evaluator of instructionfollowing models",
      "authors": [
        "X Li",
        "T Zhang",
        "Y Dubois",
        "R Taori",
        "I Gulrajani",
        "C Guestrin",
        "P Liang",
        "T Hashimoto"
      ],
      "year": "2023",
      "venue": "Alpacaeval: An automatic evaluator of instructionfollowing models"
    },
    {
      "citation_id": "39",
      "title": "Evaluating mathematical reasoning of foundation models in visual contexts",
      "authors": [
        "P Lu",
        "H Bansal",
        "T Xia",
        "J Liu",
        "C Li",
        "H Hajishirzi",
        "H Cheng",
        "K.-W Chang",
        "M Galley",
        "J Gao",
        "Mathvista"
      ],
      "year": "2024",
      "venue": "Proceedings of the International Conference on Learning Representations"
    },
    {
      "citation_id": "40",
      "title": "Mm-vid: Advancing video understanding with gpt-4v (ision)",
      "authors": [
        "K Lin",
        "F Ahmed",
        "L Li",
        "C.-C Lin",
        "E Azarnasab",
        "Z Yang",
        "J Wang",
        "L Liang",
        "Z Liu",
        "Y Lu"
      ],
      "year": "2023",
      "venue": "Mm-vid: Advancing video understanding with gpt-4v (ision)",
      "arxiv": "arXiv:2310.19773"
    },
    {
      "citation_id": "41",
      "title": "Can gpt-4v (ision) serve medical applications? case studies on gpt-4v for multimodal medical diagnosis",
      "authors": [
        "C Wu",
        "J Lei",
        "Q Zheng",
        "W Zhao",
        "W Lin",
        "X Zhang",
        "X Zhou",
        "Z Zhao",
        "Y Zhang",
        "Y Wang"
      ],
      "year": "2023",
      "venue": "Can gpt-4v (ision) serve medical applications? case studies on gpt-4v for multimodal medical diagnosis",
      "arxiv": "arXiv:2310.09909"
    },
    {
      "citation_id": "42",
      "title": "Gpt4vis: What can gpt-4 do for zero-shot visual recognition?",
      "authors": [
        "W Wu",
        "H Yao",
        "M Zhang",
        "Y Song",
        "W Ouyang",
        "J Wang"
      ],
      "year": "2023",
      "venue": "Gpt4vis: What can gpt-4 do for zero-shot visual recognition?",
      "arxiv": "arXiv:2311.15732"
    },
    {
      "citation_id": "43",
      "title": "Multisentinet: A deep semantic network for multimodal sentiment analysis",
      "authors": [
        "N Xu",
        "W Mao"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 ACM on Conference on Information and Knowledge Management"
    },
    {
      "citation_id": "44",
      "title": "Multimodal sentiment analysis with image-text interaction network",
      "authors": [
        "T Zhu",
        "L Li",
        "J Yang",
        "S Zhao",
        "H Liu",
        "J Qian"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "45",
      "title": "Disentangling identity and pose for facial expression recognition",
      "authors": [
        "J Jiang",
        "W Deng"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "46",
      "title": "Joint local and global information learning with single apex frame detection for micro-expression recognition",
      "authors": [
        "Y Li",
        "X Huang",
        "G Zhao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "47",
      "title": "Dpcnet: Dual path multi-excitation collaborative network for facial expression representation learning in videos",
      "authors": [
        "Y Wang",
        "Y Sun",
        "W Song",
        "S Gao",
        "Y Huang",
        "Z Chen",
        "W Ge",
        "W Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "48",
      "title": "From static to dynamic: Adapting landmark-aware image models for facial expression recognition in videos",
      "authors": [
        "Y Chen",
        "J Li",
        "S Shan",
        "M Wang",
        "R Hong"
      ],
      "year": "2023",
      "venue": "From static to dynamic: Adapting landmark-aware image models for facial expression recognition in videos",
      "arxiv": "arXiv:2312.05447"
    },
    {
      "citation_id": "49",
      "title": "Perceptual linear predictive (plp) analysis of speech",
      "authors": [
        "H Hermansky"
      ],
      "year": "1990",
      "venue": "Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "50",
      "title": "Exploring principles-of-art features for image emotion recognition",
      "authors": [
        "S Zhao",
        "Y Gao",
        "X Jiang",
        "H Yao",
        "T.-S Chua",
        "X Sun"
      ],
      "year": "2014",
      "venue": "Proceedings of the 22nd ACM international conference on Multimedia"
    },
    {
      "citation_id": "51",
      "title": "Deepsentibank: Visual sentiment concept classification with deep convolutional neural networks",
      "authors": [
        "T Chen",
        "D Borth",
        "T Darrell",
        "S.-F Chang"
      ],
      "year": "2014",
      "venue": "Deepsentibank: Visual sentiment concept classification with deep convolutional neural networks",
      "arxiv": "arXiv:1410.8586"
    },
    {
      "citation_id": "52",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2015",
      "venue": "Proceedings of the International Conference on Learning Representations"
    },
    {
      "citation_id": "53",
      "title": "Visual sentiment prediction based on automatic discovery of affective regions",
      "authors": [
        "J Yang",
        "D She",
        "M Sun",
        "M.-M Cheng",
        "P Rosin",
        "L Wang"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "54",
      "title": "Lbp with six intersection points: Reducing redundant information in lbp-top for micro-expression recognition",
      "authors": [
        "Y Wang",
        "J See",
        "R -W. Phan",
        "Y.-H Oh"
      ],
      "year": "2014",
      "venue": "Computer Vision-ACCV 2014: 12th Asian Conference on Computer Vision"
    },
    {
      "citation_id": "55",
      "title": "Spatiotemporal recurrent convolutional networks for recognizing spontaneous micro-expressions",
      "authors": [
        "Z Xia",
        "X Hong",
        "X Gao",
        "X Feng",
        "G Zhao"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "56",
      "title": "Can micro-expression be recognized based on single apex frame?",
      "authors": [
        "Y Li",
        "X Huang",
        "G Zhao"
      ],
      "year": "2018",
      "venue": "25th IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "57",
      "title": "Recognizing spontaneous micro-expression using a three-stream convolutional neural network",
      "authors": [
        "B Song",
        "K Li",
        "Y Zong",
        "J Zhu",
        "W Zheng",
        "J Shi",
        "L Zhao"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "58",
      "title": "Convolutional neural networks for multimedia sentiment analysis",
      "authors": [
        "G Cai",
        "B Xia"
      ],
      "year": "2015",
      "venue": "Natural Language Processing and Chinese Computing: 4th CCF Conference"
    },
    {
      "citation_id": "59",
      "title": "Visual and textual sentiment analysis of a microblog using deep convolutional neural networks",
      "authors": [
        "Y Yu",
        "H Lin",
        "J Meng",
        "Z Zhao"
      ],
      "year": "2016",
      "venue": "Algorithms"
    },
    {
      "citation_id": "60",
      "title": "A co-memory network for multimodal sen-timent analysis",
      "authors": [
        "N Xu",
        "W Mao",
        "G Chen"
      ],
      "year": "2018",
      "venue": "The 41st international ACM SIGIR conference on research & development in information retrieval"
    },
    {
      "citation_id": "61",
      "title": "Image-text multimodal emotion classification via multi-view attentional network",
      "authors": [
        "X Yang",
        "S Feng",
        "D Wang",
        "Y Zhang"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "62",
      "title": "Proceedings of the 7th International Conference on Learning Representations",
      "authors": [
        "Y.-H Tsai",
        "P Liang",
        "A Zadeh",
        "L.-P Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the 7th International Conference on Learning Representations"
    },
    {
      "citation_id": "63",
      "title": "Modality-invariant andspecific representations for multimodal sentiment analysis",
      "authors": [
        "D Hazarika",
        "R Zimmermann",
        "S Poria",
        "Misa"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "64",
      "title": "Memory fusion network for multi-view sequential learning",
      "authors": [
        "A Zadeh",
        "P Liang",
        "N Mazumder",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "65",
      "title": "Improving multimodal fusion with hierarchical mutual information maximization for multimodal sentiment analysis",
      "authors": [
        "W Han",
        "H Chen",
        "S Poria"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "66",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Y.-H Tsai",
        "S Bai",
        "P Liang",
        "J Kolter",
        "L.-P Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics"
    },
    {
      "citation_id": "67",
      "title": "Cross-vae: Towards disentangling expression from identity for human faces",
      "authors": [
        "H Wu",
        "J Jia",
        "L Xie",
        "G Qi",
        "Y Shi",
        "Q Tian"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "68",
      "title": "Identity-aware convolutional neural network for facial expression recognition",
      "authors": [
        "Z Meng",
        "P Liu",
        "J Cai",
        "S Han",
        "Y Tong"
      ],
      "year": "2017",
      "venue": "2017 12th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "69",
      "title": "Suppressing uncertainties for large-scale facial expression recognition",
      "authors": [
        "K Wang",
        "X Peng",
        "J Yang",
        "S Lu",
        "Y Qiao"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "70",
      "title": "Region attention networks for pose and occlusion robust facial expression recognition",
      "authors": [
        "K Wang",
        "X Peng",
        "J Yang",
        "D Meng",
        "Y Qiao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "71",
      "title": "Identity-adaptive facial expression recognition through expression regeneration using conditional generative adversarial networks",
      "authors": [
        "H Yang",
        "Z Zhang",
        "L Yin"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)"
    },
    {
      "citation_id": "72",
      "title": "Robust lightweight facial expression recognition network with label distribution training",
      "authors": [
        "Z Zhao",
        "Q Liu",
        "F Zhou"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "73",
      "title": "Learning deep global multi-scale and local attention features for facial expression recognition in the wild",
      "authors": [
        "Z Zhao",
        "Q Liu",
        "S Wang"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "74",
      "title": "Disentangled feature based adversarial learning for facial expression recognition",
      "authors": [
        "M Bai",
        "W Xie",
        "L Shen"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "75",
      "title": "Crossdomain facial expression recognition based on transductive deep transfer learning",
      "authors": [
        "K Yan",
        "W Zheng",
        "T Zhang",
        "Y Zong",
        "C Tang",
        "C Lu",
        "Z Cui"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "76",
      "title": "Learn from all: Erasing attention consistency for noisy label facial expression recognition",
      "authors": [
        "Y Zhang",
        "C Wang",
        "X Ling",
        "W Deng"
      ],
      "year": "2022",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "77",
      "title": "Learning to amend facial expression representation via de-albino and affinity",
      "authors": [
        "J Shi",
        "S Zhu",
        "Z Liang"
      ],
      "year": "2021",
      "venue": "Learning to amend facial expression representation via de-albino and affinity",
      "arxiv": "arXiv:2103.10189"
    },
    {
      "citation_id": "78",
      "title": "Hard negative generation for identity-disentangled facial expression recognition",
      "authors": [
        "X Liu",
        "B Kumar",
        "P Jia",
        "J You"
      ],
      "year": "2019",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "79",
      "title": "Distract your attention: Multi-head cross attention network for facial expression recognition",
      "authors": [
        "Z Wen",
        "W Lin",
        "T Wang",
        "G Xu"
      ],
      "year": "2023",
      "venue": "Biomimetics"
    },
    {
      "citation_id": "80",
      "title": "Learning relation-aware facial expression representations with transformers",
      "authors": [
        "F Xue",
        "Q Wang",
        "G Guo",
        "Transfer"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "81",
      "title": "Adaptively learning facial expression representation via cf labels and distillation",
      "authors": [
        "H Li",
        "N Wang",
        "X Ding",
        "X Yang",
        "X Gao"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "82",
      "title": "Facial expression recognition by using a disentangled identity-invariant expression representation",
      "authors": [
        "K Ali",
        "C Hughes"
      ],
      "year": "2021",
      "venue": "2020 25th International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "83",
      "title": "Poster: A pyramid cross-fusion transformer network for facial expression recognition",
      "authors": [
        "C Zheng",
        "M Mendieta",
        "C Chen"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "84",
      "title": "Facecaps for facial expression recognition",
      "authors": [
        "F Wu",
        "C Pang",
        "B Zhang"
      ],
      "year": "2021",
      "venue": "Computer Animation and Virtual Worlds"
    },
    {
      "citation_id": "85",
      "title": "Poster v2: A simpler and stronger facial expression recognition network",
      "authors": [
        "J Mao",
        "R Xu",
        "X Yin",
        "Y Chang",
        "B Nie",
        "A Huang"
      ],
      "year": "2023",
      "venue": "Poster v2: A simpler and stronger facial expression recognition network",
      "arxiv": "arXiv:2301.12149"
    },
    {
      "citation_id": "86",
      "title": "Learning spatiotemporal features with 3d convolutional networks",
      "authors": [
        "D Tran",
        "L Bourdev",
        "R Fergus",
        "L Torresani",
        "M Paluri"
      ],
      "year": "2015",
      "venue": "Learning spatiotemporal features with 3d convolutional networks"
    },
    {
      "citation_id": "87",
      "title": "Rethinking the learning paradigm for dynamic facial expression recognition",
      "authors": [
        "H Wang",
        "B Li",
        "S Wu",
        "S Shen",
        "F Liu",
        "S Ding",
        "A Zhou"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "88",
      "title": "Multimodal and temporal perception of audio-visual cues for emotion recognition",
      "authors": [
        "E Ghaleb",
        "M Popa",
        "S Asteriadis"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "89",
      "title": "A deep spatial and temporal aggregation framework for video-based facial expression recognition",
      "authors": [
        "X Pan",
        "G Ying",
        "G Chen",
        "H Li",
        "W Li"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "90",
      "title": "Spatio-temporal transformer for dynamic facial expression recognition in the wild",
      "authors": [
        "F Ma",
        "B Sun",
        "S Li"
      ],
      "year": "2022",
      "venue": "Spatio-temporal transformer for dynamic facial expression recognition in the wild",
      "arxiv": "arXiv:2205.04749"
    },
    {
      "citation_id": "91",
      "title": "Multimodal split attention fusion",
      "authors": [
        "L Su",
        "C Hu",
        "G Li",
        "D Cao",
        "Msaf"
      ],
      "year": "2020",
      "venue": "Multimodal split attention fusion",
      "arxiv": "arXiv:2012.07175"
    },
    {
      "citation_id": "92",
      "title": "Enhanced convolutional lstm with spatial and temporal skip connections and temporal gates for facial expression recognition from video",
      "authors": [
        "R Miyoshi",
        "N Nagata",
        "M Hashimoto"
      ],
      "year": "2021",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "93",
      "title": "Intensity-aware loss for dynamic facial expression recognition in the wild",
      "authors": [
        "H Li",
        "H Niu",
        "Z Zhu",
        "F Zhao"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "94",
      "title": "Frame attention networks for facial expression recognition in videos",
      "authors": [
        "D Meng",
        "X Peng",
        "K Wang",
        "Y Qiao"
      ],
      "year": "2019",
      "venue": "IEEE international conference on image processing (ICIP)"
    },
    {
      "citation_id": "95",
      "title": "Svfap: Self-supervised video facial affect perceiver",
      "authors": [
        "L Sun",
        "Z Lian",
        "K Wang",
        "Y He",
        "M Xu",
        "H Sun",
        "B Liu",
        "J Tao"
      ],
      "year": "2023",
      "venue": "Svfap: Self-supervised video facial affect perceiver",
      "arxiv": "arXiv:2401.00416"
    },
    {
      "citation_id": "96",
      "title": "Mae-dfer: Efficient masked autoencoder for self-supervised dynamic facial expression recognition",
      "authors": [
        "L Sun",
        "Z Lian",
        "B Liu",
        "J Tao"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "97",
      "title": "Spatial-temporal graphs plus transformers for geometry-guided facial expression recognition",
      "authors": [
        "R Zhao",
        "T Liu",
        "Z Huang",
        "D Lun",
        "K.-M Lam"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "98",
      "title": "A cross-modal fusion network based on self-attention and residual structure for multimodal emotion recognition",
      "authors": [
        "Z Fu",
        "F Liu",
        "H Wang",
        "J Qi",
        "X Fu",
        "A Zhou",
        "Z Li"
      ],
      "year": "2021",
      "venue": "A cross-modal fusion network based on self-attention and residual structure for multimodal emotion recognition",
      "arxiv": "arXiv:2111.02172"
    },
    {
      "citation_id": "99",
      "title": "Toward machine emotional intelligence: Analysis of affective physiological state",
      "authors": [
        "R Picard",
        "E Vyzas",
        "J Healey"
      ],
      "year": "2001",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "100",
      "title": "The biases of pre-trained language models: An empirical study on prompt-based sentiment analysis and emotion detection",
      "authors": [
        "R Mao",
        "Q Liu",
        "K He",
        "W Li",
        "E Cambria"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "101",
      "title": "Explainable multimodal emotion reasoning",
      "authors": [
        "Z Lian",
        "L Sun",
        "M Xu",
        "H Sun",
        "K Xu",
        "Z Wen",
        "S Chen",
        "B Liu",
        "J Tao"
      ],
      "year": "2023",
      "venue": "Explainable multimodal emotion reasoning",
      "arxiv": "arXiv:2306.15401"
    }
  ]
}