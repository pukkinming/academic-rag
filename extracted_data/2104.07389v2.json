{
  "paper_id": "2104.07389v2",
  "title": "Do Deep Neural Networks Forget Facial Action Units? -Exploring The Effects Of Transfer Learning In Health Related Facial Expression Recognition",
  "published": "2021-04-15T11:37:19Z",
  "authors": [
    "Pooja Prajod",
    "Dominik Schiller",
    "Tobias Huber",
    "Elisabeth André"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper, we present a process to investigate the effects of transfer learning for automatic facial expression recognition from emotions to pain. To this end, we first train a VGG16 convolutional neural network to automatically discern between eight categorical emotions. We then fine-tune successively larger parts of this network to learn suitable representations for the task of automatic pain recognition. Subsequently, we apply those fine-tuned representations again to the original task of emotion recognition to further investigate the differences in performance between the models. In the second step, we use Layer-wise Relevance Propagation to analyze predictions of the model that have been predicted correctly previously but are now wrongly classified. Based on this analysis, we rely on the visual inspection of a human observer to generate hypotheses about what has been forgotten by the model. Finally, we test those hypotheses quantitatively utilizing concept embedding analysis methods. Our results show that the network, which was fully fine-tuned for pain recognition, indeed payed less attention to two action units that are relevant for expression recognition but not for pain recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Facial expressions play a crucial part in the nonverbal communication of every social interaction. They can convey a persons inner state or intentions without the need of verbalization, which is especially useful for people that are not able to express themselves using speech (e. g. because of illness or accidents). This makes the assessment of a patient's facial expressions an important aspect to consider in many healthcare applications and also a valuable skill to learn for staff in hospitals and nursing homes. In everyday nursing home life for example, pain recognition is an important routine task for nursing staff in order to enable optimal pain management through individually adapted therapy. Additionally to the direct assessment of pain, facial expressions can provide valuable insights about associated emotional states like panic or confusion.\n\nHowever, it is practically impossible for medical staff to continuously monitor patients manually. As a consequence, there has been an growing interest in the research community to develop methods that automatically recognize if a person is in pain or in an associated emotional state. The SenseEmotion research project  Velana et al (2016)  for example investigated the optimization of pain treatment through automatic detection of physical pain and the reduction of mental pain through affect management. In a similar way the KRISTINA project  Wanner et al (2017)  aimed at automatically recognizing the emotions of elderly people in nursing homes.\n\nTo this end, modern state of the art approaches often rely on deep learning techniques that are able to learn a suitable representation for a given task from the raw data input  Luqin (2019) ;  Li and Deng (2020) . While those methods often achieve superior classification performance over traditional handcrafted features, they require large amounts of annotated training data to do so. This provides a challenging environment for the classification of sensitive tasks, like automatic pain recognition, where the availability of data is limited due to sparsity of patient contact, privacy concerns  Cowie et al (2017) , and due to the adherence of strict ethical guidelines  Charlton (1995) . A frequently used approach to facilitate the learning process in situations with scarce data comes in the form of transfer learning. Here the principal idea is to transfer the knowledge that a model has learned for a specific task A over to an adjacent task B. While such a transfer can contribute to an increase in performance for task B it is a common phenomenon that the model forgets crucial information, which it has previously learned for task  A Yosinski et al (2014) . In practical applications however, it is often desirable to learn representations of the input data that can be used for multiple tasks at the same time.\n\nIn this paper, we propose a novel, multi-step approach to investigate what neural networks forget during the process of transfer learning from emotions to pain. In the first step we evaluate the difference in emotion recognition performance before and after the transfer learning process. In the second step, we use explainable AI methods to analyze predictions of the model, that have been predicted correctly previously but are now wrongly classified. Based on this analysis we rely on the visual inspection of a human observer to generate hypotheses about what has been forgotten by the model. Finally, we test those hypotheses quantitatively utilizing concept embedding analysis methods.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Transfer Learning In Pain Recognition",
      "text": "Transfer learning is a widely used technique to circumvent the problem of small sized datasets. In this section, we discuss some of the works that propose facial expression based pain recognition using transfer learning.\n\nOne of the earlier works on pain recognition using transfer learning was by  Florea et al (2014) . They used hand-crafted Histograms of Topographical (HoT) features to learn a suitable data representation for emotion recognition. They transferred the learned emotion representation to fit a support vector regressor for estimating pain intensity, thereby achieving an overall improvement in robustness and generalization capabilities of the system.\n\nHowever,  Zamzmi et al (2018)  and  Egede et al (2017)  demonstrated that deep neural networks outperform traditional hand-crafted features in automatic pain recognition. They extracted deep learned features from pre-trained convolutional neural networks (CNN) and fused them with hand-crafted features. The fused feature representation was used to train classifiers for recognizing pain. Both works obtained a significant improvement over hand-crafted features.\n\nWhile Zamzmi et al and Egede et al transferred learned representations from CNNs by using the network as feature extractors, Wang et al (2017) adopted a different approach. In their experiments they used a fine-tuning technique to leverage the learned representations from a pre-trained CNN model for pain intensity estimation. Wang et al demonstrated that fine-tuning a pre-trained CNN from a data-rich domain (e.g. face verification) can mitigate the problem of fully training a deep learning network on small datasets (e.g. pain recognition).\n\nMost of the existing pain datasets are composed of consecutive video frames and contain temporal information that is often not utilized. This led to the idea of using recurrent neural networks.  Rodriguez et al (2017)  fine-tuned a pre-trained CNN on a pain dataset and used it as a feature extractor. They then used those features as input, to train a Long Short Term Memory network (LSTM) for a binary pain recognition task. Adding the the temporal information, they could achieve superior performance over just using the frame wise CNN representations. A study by  Haque et al (2018)  however showed, that the exploitation of the temporal information does not always improve results. In their experiments they fine-tuned three CNNs on pain intensity, using RGB, depth information, and thermal video frames as input. They concluded that, given the limited number of training sequences, the frame-wise CNN features were not sufficiently discriminative to obtain good LSTM generalizations.\n\nFine-tuning based transfer learning often changes the internal representations which were learned for the original task. These changes may result in forgetting of crucial information that reduces the performance of the model on the original task. The aforementioned studies focused on fine-tuning models to obtain the best result on the target task. However, they did not delve into the consequences of the fine-tuning process on the performance of the original task.  Kemker et al (2018)  investigated forgetting within a single task. However, as far as we know there is no work on quantifying forgetting between distinct sequentially learned tasks.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Explainable Artificial Intelligence",
      "text": "With the increasing popularity of fine-tuning techniques in high-risk domains like health-care, it is important to investigate what is learned and what is forgotten. A common way of analyzing the predictions of neural networks is the creation of so called saliency maps that highlight how important each input was for the prediction. For pain recognition,  Weitz et al (2019)  applied and compared Layerwise Relevance Propagation (LRP)  (Montavon et al (2019) ) and Local Interpretable Model-agnostic Explanations (LIME)  (Ribeiro et al (2016) ). Weitz et al conclude that the salience maps generated by those approaches already provide some insights into the reasoning of the network but are not expressive enough on their own since they are often ambiguous and hard to interpret for end-users.\n\nTo combat this problem, recent work on concept embedding analysis investigates which human-comprehensible concepts were learned by a given network.  Bau et al (2017)  showed that there are often semantic concepts embedded in single neurons of the latent space of a neural network. For instance,  Khorrami et al (2015)  showed that certain neurons inside the last convolutional layer of a network, which was trained for facial expression analysis, learned to recognize specific Action Units (AUs) -visible indicators of the operation of individual facial muscles. To extend this method to account for concepts that might span over several neurons in the latent space,  Kim et al (2018)  proposed to train a binary linear classifier that takes the output of a intermediate layer of the network as input and learns to recognize a given concept. If the linear classifier is able to identify the concept then it is likely that the network learned this concept. They tested their approach on several image classification models and a model for predicting diabetic retinopathy. Similarly, Schwalbe and Schels (2020) used convolutional networks instead of a linear classifier to search for segmented concepts in the intermediate layers. A common challenge for the aforementioned approaches is that the potential concepts have to be externally identified by human experts.\n\nIn this work, we utilize LRP saliency maps to facilitate the identification of potential concepts. Then we follow the approach by  Kim et al (2018)  to verify whether the network learned those concepts. As far as we know, concept embedding analysis has not yet been explored for facial expression recognition tasks. However, healthcare related applications like pain recognition would benefit from the additional understanding of the models inner working.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Pain Training",
      "text": "Usually, pain datasets are small and have very few pain samples  Wang et al (2017) . In such cases, a transfer learning approach is often adopted. It involves re-using some parameters from a pre-trained model and training the remaining parameters on a small target dataset. Ideally, the pre-trained model is trained on a large dataset from a domain similar to the target domain. Emotion recognition from facial expression is a well studied task with large existing datasets like AffectNet  Mollahosseini et al (2019) . Since facial expressions, in particular AUs, are used in both pain and emotion recognition  Simon et al (2008) , we choose facial emotion recognition as our source task.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Datasets",
      "text": "We use AffectNet  Mollahosseini et al (2019)  for the facial emotion recognition task. The AffectNet dataset contains 420299 manually annotated face images belonging to 11 classes -Neutral, Happy, Sad, Surprise, Fear, Disgust, Anger, Contempt, None, Uncertain, Non-Face. The images were collected through search queries containing emotional keywords. This dataset is imbalanced with very high number of samples (> 75000) in categories like 'Happy' and 'Neutral', and lower samples (around 4000) in 'Disgust' and 'Contempt'. We excluded images belonging to 'None', 'Uncertain' and 'Non-face' as they do not have an emotion label. From the remaining, we use 273269 images for training and 14382 for validation. As suggested by the authors, we use the validation images from the original dataset as test set. Hence, we have 4000 images (500 per class × 8 classes) for testing.\n\nFor pain recognition, we use the face images from the UNBC-McMaster shoulder pain expression database  Lucey et al (2011) . The images were generated from video recordings of 25 participants who suffered from shoulder pain. Each image has a Prkachin and Solomon Pain Intensity (PSPI) score, where 0 indicates no-pain and scores > 0 indicate different levels of pain. The dataset has 48398 (40029 no-pain + 8369 pain) images. We use all images from 4 participants (allowed to publish) as test set. We want to simulate a transfer learning use-case with very few samples available in the target domain. Hence, 1000 images (500 pain + 500 no-pain) are selected from the remaining 21 participants for the training (900 images) and validation (100 images) phase of transfer learning. To avoid very similar video frames, the training and validation images are chosen randomly but satisfy the conditions that (i) there is at least 1 image of pain and no-pain from each of the 21 participants, (ii) the set has no consecutive images.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Transfer Learning",
      "text": "As noted by  Yosinski et al (2014) , there are two methods for transfer learning in deep neural networks -freezing and fine-tuning. In freezing method, weights of first few layers are copied from a pre-trained model and these layers are marked as frozen or not trainable. Only the remaining unfrozen layers are trained on target dataset. Similar to freezing, fine-tuning also involves copying weights from pre-trained model. The difference is that no layer is marked frozen, i.e. all layers are further trained on the target dataset.\n\nWe adopt a hybrid method which involves: (i) training a base model on the source dataset, (ii) copying all the weights from the base model to the target model, (iii) freezing some layers and fine-tuning the remaining layers on the target dataset.\n\nWe use the VGG16 architecture with 5 convolution blocks for both emotion and pain recognition tasks. All images from both datasets are scaled to default VGG16 input dimensions (224 × 224). In both models, we use SGD optimizer (learning rate = 0.01), focal loss and data augmentation. The focal loss function Lin et al (  2017 ) is given by: _\n\nis the predicted probability of a sample belonging to its true class ( ) and is a hyperparameter.\n\nFor the first step, we train an emotion recognition model using images from the AffectNet dataset (see section 3.1). We use a VGG16 model pre-trained on ImageNet  Deng et al (2009)  as the starting point for the emotion recognition task. All the layers are marked trainable. The model is eventually connected to a dense layer with softmax activation to predict the probability of an image belonging to each of the 8 emotion classes -Neutral, Happy, Sad, Surprise, Fear, Disgust, Anger and Contempt. During training, we use horizontal flip to augment the data. We set the hyperparameter in focal loss function (equation 1) as = 5. Since our training and validation sets are heavily imbalanced, we use a weighted focal loss function. We use the weighting scheme proposed by  Cui et al (2019) , given by:\n\nWe set the hyperparameter = 0.99998. We use the emotion recognition model trained on AffectNet as the source model for transfer learning pain (figure  1 ). We copy the weights of all convolution blocks and freeze the first few blocks, making them unavailable for pain training. We vary the number of frozen blocks from 0 (all blocks are available for pain training) to 5 (none of the convolution blocks are available for pain training, only the output layers are trained). This yields six pain recognition models. These models are trained on a variant of the UNBC shoulder pain dataset (see section 3.1) to determine if a face image shows pain (PSPI pain score > 0) or not (PSPI pain score = 0). For this task, we use Keras data augmentation options: rotation ([-25 , 25 ]), height shift ([-10%, 10%]), width shift ([-10%, 10%]), shear ([-10%, 10%]), zoom",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Measuring Forgetting",
      "text": "Catastrophic forgetting occurs when transfer learning causes large changes in weights of the existing model and thus altering previous feature representations  (Kemker et al (2018) ). In our case, when more unfrozen layers are available for training pain, the model becomes increasingly tailored for pain recognition. We say our model forgot the recognition of an emotion if the recall of the emotion reduces after transfer learning.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Re-Train For Measuring Forgetting",
      "text": "Since we vary the number of frozen blocks in pain training from 0 to 5, we have 6 different pain recognition models. The models are labelled 'FreezeB ', where = 0 to 5. 'FreezeB ' is generated by freezing the first convolution blocks with initial emotion recognition weights and training the remaining blocks for pain recognition. 'FreezeB5' corresponds to the model where all convolution blocks are frozen and only the output layers are trained for pain recognition. On the other hand, 'FreezeB0' corresponds to the pain model generated by not freezing any blocks, i.e., all the layers were trained on pain dataset. Evaluation measures, like recalls, help in determining the best pain recognition model. We also want to evaluate these models in terms of forgetting and study the trade-off between pain recognition and emotion recognition. In other words, we want to determine how these models perform on emotion recognition after feature representations are adapted for pain recognition.\n\nTo measure forgetting we employ a re-training methodology (figure  2 ), i.e., we evaluate the capability of the pain models to recognize emotions instead of pain. First, we freeze all convolution blocks of a pain model so that the learned feature representations are intact. Next, we add output layers to get prediction probabilities for 8 emotion classes. Finally, we train the model on the AffectNet dataset with the same optimizer, data augmentation, loss function and hyperparameters as before (see section 3.2). This process is repeated for all 6 pain models and the resulting models are compared based on the their recalls for each emotion. We run McNemar's test to determine if there are any significant difference in recalls  (Dietterich (1998) ).",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Visual Analysis",
      "text": "While the quantitative evaluation of the emotion recognition models allows a performance comparison between them, we do not know which specific embedded concepts might have been learned or forgotten to cause any difference in performance. To analyze the difference between FreezeB5, which uses the same features as the original emotion model, and one of the fine-tuned models (say FreezeB , with ∈ {0, 1, 2, 3, 4}), we look at the emotion images that got predicted correctly by FreezeB5 but wrongly by FreezeB . For both of the models, we create a saliency map that highlights the areas of the input image which have the most influence on the model to correctly classify a given sample. The saliency maps are created using LRP with the -rule for fully connected layers and the + -rule for convolution layers. This is recommended by  Montavon et al (2019)  and was shown to be less independent of the models learned parameters than other LRP methods by  Sixt et al (2020) . Additionally, the saliency maps are normalized between 0 and 1, such that a value of 0.5, for example, indicated that this pixel accounted for half of the confidence in the prediction. Since both models were fine-tuned based on the same emotion recognition weights, the saliency maps for the same input image and the same class often appear indistinguishable to the human eye (see figure  3 ).\n\nUnlike humans however, neural networks can be sensitive to very small differences. To make those differences visible, we generate new saliency maps by subtracting the raw saliency maps from each other and normalizing these differences Input Image FreezeB5 FreezeB0 between 0 and 1 for visibility. We use them to identify concepts which FreezeB5 payed more attention to than FreezeB . The underlying intuition is that those concepts helped FreezeB5 to correctly predict the emotion but were not used enough by FreezeB (i.e. were forgotten by FreezeB ). To translate relevant image areas into semantically meaningful concepts, the interpretation needs to be done visually by a human. Hereby, we especially focus action units as candidates for concepts, since they are known to be good indicators for emotion as well as pain and have already been shown to be utilized by neural networks  (Khorrami et al (2015) ).",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Concept Embedding Detection",
      "text": "In this section, we describe how we verify the hypothesis that the concept candidates identified in section 4.2 were actually forgotten after fine-tuning our model for pain recognition. To this end, we investigate whether these concept candidates are embedded in the latent space of the models by applying concept embedding detection. One way to detect a specific concept, is training a binary linear classifier on the output of an intermediate layer of the network to recognize this concept  (Kim et al (2018) ). If the linear classifier is able to detect the concept then it is likely that the network learned this concept.\n\nWe focus on AUs as potential concepts but the AffectNet dataset does not contain labeled AUs. According to  Kim et al (2018) , concept detection need not be done on the dataset on which the model was trained. Hence, we use the CK+ dataset  (Lucey et al (2010) ), which has been specifically developed to serve as a comprehensive test-bed for comparative studies of facial expression analysis. This dataset consists of video recordings of acted emotions, where the frame displaying the peak of the emotion has been manually annotated with respect to action units, which makes it well suited for our case. By computing the output of the last convolution block of FreezeB5 and FreezeB ( ∈ {0, 1, 2, 3, 4}), we obtain two feature-sets that represent the latent space of the respective models. We then train a linear Support Vector Machine (SVM) to detect the concept candidate on each of those two feature-sets using 2-fold cross validation. For each feature-set, we compute the average f1-score of the two folds. We repeat this training process for 500 iterations using different random seeds for weight initialisation and fold image selection. Finally, we run a paired t-test between the 500 averaged F1-scores of each feature-set. The result of this test shows whether there is a significant difference between the performance of SVMs trained on the FreezeB5 features and the SVMs trained on the FreezeB features.  Dietterich (1998)  suggests this comparison metric for 5 iterations as 5 × 2 cross validation paired t-test and we extend it to 500 iteration as suggested by  Kim et al (2018) .",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Results",
      "text": "Our pain training procedure yields 6 different models. First, we compare the performance of these models in recognizing pain. Next, we evaluate these models in terms of forgetting emotion recognition. These metrics help us to assess the suitability of the learned representations for emotion as well as for pain recognition.. Additionally, we utilize XAI to analyze which embedded concepts (e.g. action units) are forgotten during transfer learning from emotions to pain.\n\nTable  1  lists the performance of the 6 pain recognition models in terms of precision, recall, f1-score and accuracy. Accounting for the imbalance in our test set for pain recognition (see section 3.1), we use macro average of the performance metrics (compute the metric for each class and average them) as it treats every class equally. As expected, models with higher number of convolution blocks available for pain training (FreezeB , = 0, 1, 2) have higher pain recall. The pain recalls saturate beyond a point, i.e., unfreezing blocks for pain training beyond a point (in our case FreezeB2) does not yield better pain recall. After generating different pain models, we freeze all convolution blocks of the model and re-train only the output layers for emotion recognition. This preserves the learned feature representations after pain training and helps measure it's impact on emotion recognition. Figure  4  shows the recalls of the 8 emotions when using the learned representation of the 6 pain recognition models. The recalls of surprise and contempt reduce over the blocks and is notably lower for FreezeB1 and FreezeB0. For demonstration of our approach, we choose FreezeB0 for further analysis. Using McNemar's test between FreezeB5 and FreezeB0, we found that the difference in recall is significant for both surprise (p-value: 1.56e-4) and contempt  (p-value: 8.19e-18) .  To investigate what concepts might have been forgotten, we generated saliency maps for images that were correctly classified as contempt or surprise by FreezeB5 but wrongly classified by FreezeB0, as described in section 4.2. An example image for each of the two emotions and the difference in their saliency maps can be seen in figure  5 . A visual comparison between saliency maps for all images in the test set from the contempt class (see section 4.2) indicates that FreezeB5 is paying more attention to dimples in the face than FreezeB0 (see figure  5  first row). Since dimples corresponds to AU14, this leads us to the hypothesis that, while both models use AU14 to an extent, FreezeB5 has a better representation for AU14 than FreezeB0. To test this hypothesis we trained pairs of SVMs to detect AU14 on multiple folds of the CK+ dataset using FreezeB5 and FreezeB0 as feature extractors (see 4.3). We compared the resulting F1 scores with a paired t-test. The SVMs trained on FreezeB5 features significantly outperformed the SVMs trained on FreezeB0 features (FreezeB0: mean F1 72.41%, FreezeB5: mean F1 74.13%, t-statistic: -13.13, p-value: 4.88 -34), showing that FreezeB0 indeed forgot AU14 to a certain degree.\n\nRegarding surprise, we observed that FreezeB5 has a stronger focus on AU5 (upper lid raise) than FreezeB0 (see figure  5  second row for example). After inspecting the CK+ images which contain AU5, it seems like most of them also contain a wide open mouth which corresponds to AU26 (jaw drop). To make sure that the SVMs are trained to recognize AU5 and not AU26, we crop the bottom of the images such that they do not contain the mouth area. As mentioned by  Kim et al (2018) , only taking cropped pictures of a concept does not hinder concept detection. Our evaluation shows that the SVMs trained on FreezeB5 features significantly outperformed the ones trained on FreezeB0 features (FreezeB0: mean F1 83.87%, FreezeB5: mean F1 84.36%, t-statistic: -9.33, p-value: 3.49e-19), confirming our hypothesis that FreezeB0 forgot AU5 to a degree.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Discussion",
      "text": "The goal of our analysis is to find which embedded concepts are forgotten during transfer learning from emotions to pain. As a demonstration of our methodology, we show that FreezeB0 (fine-tuned all convolution layers on pain dataset) pay less attention to AU5 (upper lid raise) and AU14 (dimple) in comparison to FreezeB5 (no fine-tuning of convolution layers). Since the forgotten embedded concepts are AUs, we investigate the relation between emotions and pain in terms of AUs. Figure  6  lists the typical AUs activated while expressing various emotions (derived from  Langner et al (2010) ). The figure also highlights the AUs activated for expressing pain (according to the typical and observed activation of AUs in  Simon et al (2008) ). When looking at the identified AUs (AU5, AU14) that the network has forgotten during the fine-tuning process, we can see that those are used for the detection of the emotions surprise and contempt but not for pain. This shows that the forgetting of the semantic embedded concepts, that we identified as reason for the observed drop of recall in those two emotions (figure  4 ), is in line with current theoretical findings found in literature.\n\nUsing our approach to investigate what was forgotten during transfer learning, we can conclude that pain training mainly affects the recognition of surprise and contempt. So, the trade-off between pain recognition and emotion recognition can be narrowed down to a trade-off between pain, surprise and contempt. Figure  7  shows the recall of pain, surprise and contempt in each of the 6 pain models (FreezeB0 to FreezeB5). Pain recall is considerably lower in FreezeB5 and FreezeB4. This is expected as these cases have lesser convolution blocks available for pain training. FreezeB1 and FreezeB0 yield the worst results for surprise and contempt. Hence, as highlighted by green ellipses in figure  7 , the best transfer learned pain recognition models that also minimises forgetting of previously learned emotion recognition are FreezeB3 and FreezeB2.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Conclusion",
      "text": "Within this paper we presented an XAI assisted approach to examine the process of knowledge transfer between two models with a focus on forgetting of already learned knowledge. A key benefit of our approach is the framing of forgotten knowledge as semantically meaningful concepts that can be understood by humans. In return this enables a human observer to better understand the trade-off between an automatically learned multipurpose latent representation of data and multiple specialized representations. We demonstrated this process on the example of automatic pain recognition from facial expressions, using a source model that has been trained on the task of emotion recognition. Our results show that during the transfer learn-ing process our model tends to pay less attention to specific action units that are known to be important markers for the emotions contempt and surprise but are not relevant for the problem of pain recognition. In the light of real world applications we argue that this knowledge about the model can be helpful to end users as well as machine learning engineers. For the described use case of automatic pain and emotion recognition we can see an application in hospitals and nursing homes as an assistant technology for the caregivers. Here it is important for the staff to know about the strength and weaknesses of a model to establish trust in the technology and drive the user acceptance of the technology. For machine learning engineers and research scientists our process can help to understand the trade offs of an automatically learned representation. It also provides clues to a developer on how to improve the model further. Given the use case of automatic pain recognition for example, it seems feasible to remove images from the contempt and surprise category from pre-training to potentially improve upon the results of the final model -an idea we would like to further investigate in the future.",
      "page_start": 13,
      "page_end": 14
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Illustration of the",
      "page": 7
    },
    {
      "caption": "Figure 2: Illustration of the",
      "page": 8
    },
    {
      "caption": "Figure 3: Example for an im-",
      "page": 9
    },
    {
      "caption": "Figure 4: shows the recalls of the 8 emotions when using the learned representation of the 6",
      "page": 10
    },
    {
      "caption": "Figure 4: Class-wise recalls of the six pain models obtained by re-training their output layers for",
      "page": 11
    },
    {
      "caption": "Figure 5: Visualization of",
      "page": 11
    },
    {
      "caption": "Figure 6: List of AUs that",
      "page": 12
    },
    {
      "caption": "Figure 6: lists the typical AUs activated while expressing various emotions (derived from",
      "page": 12
    },
    {
      "caption": "Figure 7: Plot of pain, surprise,",
      "page": 13
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Pain models": "",
          "Precision": "",
          "Recall": "",
          "F1-score": "",
          "Accuracy": ""
        },
        {
          "Pain models": "FreezeB5\nFreezeB4\nFreezeB3\nFreezeB2\nFreezeB1\nFreezeB0",
          "Precision": "0.92\n0.93\n0.95\n0.95\n0.95\n0.95",
          "Recall": "0.99\n0.99\n0.98\n0.96\n0.95\n0.95",
          "F1-score": "0.95\n0.96\n0.96\n0.95\n0.95\n0.95",
          "Accuracy": "0.92\n0.92\n0.93\n0.92\n0.91\n0.91"
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Network dissection: Quantifying interpretability of deep visual representations",
      "authors": [
        "D Bau",
        "B Zhou",
        "A Khosla",
        "A Oliva",
        "A Torralba"
      ],
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "2",
      "title": "Ethical guidelines for pain research in humans. committee on ethical issues of the international association for the study of pain",
      "authors": [
        "E Charlton"
      ],
      "year": "1995",
      "venue": "Pain"
    },
    {
      "citation_id": "3",
      "title": "Electronic health records to facilitate clinical research",
      "authors": [
        "M Cowie",
        "J Blomster",
        "L Curtis",
        "S Duclaux",
        "I Ford",
        "F Fritz",
        "S Goldman",
        "S Janmohamed",
        "J Kreuzer",
        "M Leenay"
      ],
      "year": "2017",
      "venue": "Clinical Research in Cardiology"
    },
    {
      "citation_id": "4",
      "title": "Class-balanced loss based on effective number of samples",
      "authors": [
        "Y Cui",
        "M Jia",
        "T Lin",
        "Y Song",
        "S Belongie"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "5",
      "title": "Imagenet: A large-scale hierarchical image database",
      "authors": [
        "J Deng",
        "W Dong",
        "R Socher",
        "L Li",
        "K Li",
        "L Fei-Fei"
      ],
      "year": "2009",
      "venue": "Imagenet: A large-scale hierarchical image database"
    },
    {
      "citation_id": "6",
      "title": "Approximate statistical tests for comparing supervised classification learning algorithms",
      "authors": [
        "T Dietterich"
      ],
      "year": "1998",
      "venue": "Neural computation"
    },
    {
      "citation_id": "7",
      "title": "Fusing deep learned and hand-crafted features of appearance, shape, and dynamics for automatic pain estimation",
      "authors": [
        "J Egede",
        "M Valstar",
        "B Martinez"
      ],
      "year": "2017",
      "venue": "12th IEEE international conference on automatic face & gesture recognition"
    },
    {
      "citation_id": "8",
      "title": "Learning pain from emotion: transferred hot data representation for pain intensity estimation",
      "authors": [
        "C Florea",
        "L Florea",
        "C Vertan"
      ],
      "year": "2014",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "9",
      "title": "Deep multimodal pain recognition: a database and comparison of spatio-temporal visual modalities",
      "authors": [
        "M Haque",
        "R Bautista",
        "F Noroozi",
        "K Kulkarni",
        "C Laursen",
        "R Irani",
        "M Bellantonio",
        "S Escalera",
        "G Anbarjafari",
        "K Nasrollahi"
      ],
      "year": "2018",
      "venue": "13th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "10",
      "title": "Measuring catastrophic forgetting in neural networks",
      "authors": [
        "R Kemker",
        "M Mcclure",
        "A Abitino",
        "T Hayes",
        "C Kanan"
      ],
      "year": "2018",
      "venue": "Thirty-second AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "11",
      "title": "Do deep neural networks learn facial action units when doing expression recognition?",
      "authors": [
        "P Khorrami",
        "T Paine",
        "T Huang"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE international conference on computer vision workshops"
    },
    {
      "citation_id": "12",
      "title": "Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)",
      "authors": [
        "B Kim",
        "M Wattenberg",
        "J Gilmer",
        "C Cai",
        "J Wexler",
        "F Viegas"
      ],
      "year": "2018",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "13",
      "title": "Presentation and validation of the radboud faces database",
      "authors": [
        "O Langner",
        "R Dotsch",
        "G Bĳlstra",
        "D Wigboldus",
        "S Hawk",
        "Van Knippenberg"
      ],
      "year": "2010",
      "venue": "Cognition and emotion"
    },
    {
      "citation_id": "14",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "15",
      "title": "Focal loss for dense object detection",
      "authors": [
        "T Lin",
        "P Goyal",
        "R Girshick",
        "K He",
        "P Dollár"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "16",
      "title": "The extended cohn-kanade dataset (CK+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "Z Ambadar",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR, Workshops"
    },
    {
      "citation_id": "17",
      "title": "Painful data: The unbc-mcmaster shoulder pain expression archive database",
      "authors": [
        "P Lucey",
        "J Cohn",
        "K Prkachin",
        "P Solomon",
        "I Matthews"
      ],
      "year": "2011",
      "venue": "Proceedings of the International Conference on Automatic Face & Gesture Recognition and Workshops"
    },
    {
      "citation_id": "18",
      "title": "A survey of facial expression recognition based on convolutional neural network",
      "authors": [
        "S Luqin"
      ],
      "year": "2019",
      "venue": "18th IEEE/ACIS International Conference on Computer and Information Science, ICIS 2019"
    },
    {
      "citation_id": "19",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hassani",
        "M Mahoor"
      ],
      "year": "2019",
      "venue": "IEEE Trans Affective Computing"
    },
    {
      "citation_id": "20",
      "title": "Layer-wise relevance propagation: An overview",
      "authors": [
        "G Montavon",
        "A Binder",
        "S Lapuschkin",
        "W Samek",
        "K Müller"
      ],
      "year": "2019",
      "venue": "Explainable AI: Interpreting, Explaining and Visualizing Deep Learning"
    },
    {
      "citation_id": "21",
      "title": "why should I trust you?\": Explaining the predictions of any classifier",
      "authors": [
        "M Ribeiro",
        "S Singh",
        "C Guestrin"
      ],
      "year": "2016",
      "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining"
    },
    {
      "citation_id": "22",
      "title": "Deep pain: Exploiting long short-term memory networks for facial expression classification",
      "authors": [
        "P Rodriguez",
        "G Cucurull",
        "J Gonzàlez",
        "J Gonfaus",
        "K Nasrollahi",
        "T Moeslund",
        "F Roca"
      ],
      "year": "2017",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "23",
      "title": "Concept enforcement and modularization as methods for the iso 26262 safety argumentation of neural networks",
      "authors": [
        "G Schwalbe",
        "M Schels"
      ],
      "year": "2020",
      "venue": "10th European Congress on Embedded Real Time Software and Systems"
    },
    {
      "citation_id": "24",
      "title": "Recognition and discrimination of prototypical dynamic expressions of pain and emotions",
      "authors": [
        "D Simon",
        "K Craig",
        "F Gosselin",
        "P Belin",
        "P Rainville"
      ],
      "year": "2008",
      "venue": "PAIN®"
    },
    {
      "citation_id": "25",
      "title": "When explanations lie: Why many modified BP attributions fail",
      "authors": [
        "L Sixt",
        "M Granz",
        "T Landgraf"
      ],
      "year": "2020",
      "venue": "Proceedings of the 37th International Conference on Machine Learning, ICML 2020"
    },
    {
      "citation_id": "26",
      "title": "The senseemotion database: A multimodal database for the development and systematic validation of an automatic pain-and emotion-recognition system",
      "authors": [
        "M Velana",
        "S Gruss",
        "G Layher",
        "P Thiam",
        "Y Zhang",
        "D Schork",
        "V Kessler",
        "S Meudt",
        "H Neumann",
        "J Kim",
        "F Schwenker",
        "E André",
        "H Traue",
        "S Walter"
      ],
      "year": "2016",
      "venue": "Multimodal Pattern Recognition of Social Signals in Human-Computer-Interaction -4th IAPR TC 9 Workshop, MPRSS 2016, Cancun"
    },
    {
      "citation_id": "27",
      "title": "Regularizing face verification nets for pain intensity regression",
      "authors": [
        "F Wang",
        "X Xiang",
        "C Liu",
        "T Tran",
        "A Reiter",
        "G Hager",
        "H Quon",
        "J Cheng",
        "A Yuille"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "28",
      "title": "KRISTINA: A knowledge-based virtual conversation agent",
      "authors": [
        "L Wanner",
        "E André",
        "J Blat",
        "S Dasiopoulou",
        "M Farrús",
        "T Fraga-Silva",
        "E Kamateri",
        "F Lingenfelser",
        "G Llorach",
        "O Martínez",
        "G Meditskos",
        "S Mille",
        "W Minker",
        "L Pragst",
        "D Schiller",
        "A Stam",
        "L Stellingwerff",
        "F Sukno",
        "B Vieru",
        "S Vrochidis"
      ],
      "year": "2017",
      "venue": "Advances in Practical Applications of Cyber-Physical Multi-Agent Systems: The PAAMS Collection -15th International Conference Proceedings"
    },
    {
      "citation_id": "29",
      "title": "Deep-learned faces of pain and emotions: Elucidating the differences of facial expressions with the help of explainable ai methods",
      "authors": [
        "K Weitz",
        "T Hassan",
        "U Schmid",
        "J Garbas"
      ],
      "year": "2019",
      "venue": "tm-Technisches Messen"
    },
    {
      "citation_id": "30",
      "title": "How transferable are features in deep neural networks?",
      "authors": [
        "J Yosinski",
        "J Clune",
        "Y Bengio",
        "H Lipson"
      ],
      "year": "2014",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "31",
      "title": "Neonatal pain expression recognition using transfer learning",
      "authors": [
        "G Zamzmi",
        "D Goldgof",
        "R Kasturi",
        "Y Sun"
      ],
      "year": "2018",
      "venue": "Neonatal pain expression recognition using transfer learning"
    }
  ]
}