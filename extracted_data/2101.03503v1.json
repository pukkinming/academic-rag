{
  "paper_id": "2101.03503v1",
  "title": "Capsfield: Light Field-Based Face And Expression Recognition In The Wild Using Capsule Routing",
  "published": "2021-01-10T09:06:02Z",
  "authors": [
    "Alireza Sepas-Moghaddam",
    "Ali Etemad",
    "Fernando Pereira",
    "Paulo Lobato Correia"
  ],
  "keywords": [
    "Face Recognition",
    "Expression Recognition",
    "Light Field",
    "Face Dataset",
    "Deep Learning",
    "Capsule Routing"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Light field (LF) cameras provide rich spatio-angular visual representations by sensing the visual scene from multiple perspectives and have recently emerged as a promising technology to boost the performance of human-machine systems such as biometrics and affective computing. Despite the significant success of LF representation for constrained facial image analysis, this technology has never been used for face and expression recognition in the wild. In this context, this paper proposes a new deep face and expression recognition solution, called CapsField, based on a convolutional neural network and an additional capsule network that utilizes dynamic routing to learn hierarchical relations between capsules. CapsField extracts the spatial features from facial images and learns the angular partwhole relations for a selected set of 2D sub-aperture images rendered from each LF image. To analyze the performance of the proposed solution in the wild, the first in the wild LF face dataset, along with a new complementary constrained face dataset captured from the same subjects recorded earlier have been captured and are made available. A subset of the in the wild dataset contains facial images with different expressions, annotated for usage in the context of face expression recognition tests. An extensive performance assessment study using the new datasets has been conducted for the proposed and relevant prior solutions, showing that the CapsField proposed solution achieves superior performance for both face and expression recognition tasks when compared to the state-of-the-art.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Face is the most common characteristic used by humans to perform personal identity  [1] ,  [2]  and expression recognition  [3] . Following the emergence of the first automatic facial image analysis system in the late 1960's  [4] , this field has attracted considerable research efforts leading to incredible progresses. Despite the recent advances in face and expression recognition, mostly thanks to the advancements in deep neural networks  [5] ,  [6] ,  [7] , highly accurate recognition results are still not achievable for some specific conditions. This is mostly the case when poor quality data is available, for instance due to the capture of face images in uncontrolled settings, commonly referred to as face recognition in the wild, where facial images are affected by multiple variations, notably in resolution, background, expression, pose, illumination, and occlusions, among others  [8] .\n\nThe emergence of new imaging sensors has opened new frontiers and also brought new challenges for face image analysis systems  [9] . Lenslet light field cameras, hereafter referred only as Light Field (LF) cameras, have recently come into prominence as they are able to simultaneously capture the intensity of light rays coming from multiple directions in space  [10] ,  [11] . An LF camera \"sees\" the visual scene from multiple angles or perspectives, allowing to render a set of 2D Sub-Aperture (SA) images, each corresponding to a 2D image for a different viewpoint of the observed scene. The set of rendered 2D SA images forms a multi-view SA array which offers intraview/spatial (within each view) and inter-view/angular (across views) correlations that can be effectively exploited for various visual analysis tasks, notably facial image analysis  [12] . In this context, the main challenge of LF-based face and emotion recognition is how to exploit both the intra-view and inter-view relationships to improve the performance of the recognition solutions. This is the recognition paradigm considered in this paper. The LF-based face and emotion recognition can also be formulated as an image set based recognition problem  [13] , where matching is performed between image sets instead of single images.\n\nDespite the significant success of the LF representation for constrained face recognition  [14] ,  [15] ,  [16] ,  [17] ,  [18] ,  [19] ,  [20] ,  [21]  and expression recognition  [22] ,  [23] , this type of visual representation has never been used for face and expression recognition in the wild, where the added value of the LF information may be more critical since the conditions are more extreme and unconstrained. In this context, this paper proposes to exploit the richer spatio-angular representation acquired with a LF camera, to improve the performance of face and expression recognition systems in the wild. In this paper, 'in the wild' refers to facial images captured under several unconstrained acquisition conditions such as random lighting conditions, shading, backgrounds, environments, poses, distances, etc., with minimal or no user cooperation. The key contributions and technical novelty of this paper in the context of LF face and expression recognition in the wild can be described as follows:\n\n• CapsField Solution: A new deep learning solution, called CapsField, is proposed for both face and expression recognition in the wild. CapsField combines convolutional and capsule networks  [24]  for exploiting not only spatial information but also the angular information available in LF images. A few solutions based on capsule networks have recently been proposed in the context of facial image analysis, including age and gender  [25] , and expression  [26]  recognition, showing that capsule networks are well suited to deal with face pose variations. CapsField goes one step further, for the first time exploiting dynamic routing between capsules for learning the angular variations across the different viewpoints captured in LF images. Additionally, CapsField assigns higher weights to the more relevant features while ignoring the spurious dimensions, to increase its robustness against face appearance and environmental changes. • In the wild face data collection: The captured dataset, named Light Field Faces in the Wild (LFFW), addresses a major weakness in the face and expression recognition research domains, which is the lack of publicly available LF facial images captured in the wild. To the best of the authors' knowledge, no similar dataset is available, even for private usage. The proposed LFFW dataset consists of LF images captured under several unconstrained acquisition variations, in both indoor and outdoor environments, at different locations, and from different distances. During the acquisition, there were no pre-defined protocol or any stimulus used to induce the desired expression. Additionally, a subset of the LFFW dataset includes facial images with different expressions, which has also been annotated to be used in the context of LF-based facial expression recognition. • Constrained face data collection: This paper also makes available a second LF dataset, called Light Field Face Constrained (LFFC), acquired in constrained conditions, with the same subjects present in LFFW. The complementary LFFC dataset was acquired between 1 day and 3 years prior to the LFFW images, allowing to study the constrained versus in the wild cross-dataset generalization ability of face recognition solutions. The two complementary face datasets (LFFW and LFFC) offer a unique set of facial data, facilitating a wide range of research possibilities related to face analysis. Both datasets, along with the annotation metadata, will be publicly available to the research community. • Benchmarking: Finally, this paper provides an extensive performance assessment of the proposed face and expression recognition solutions regarding the most competitive state-of-the-art solutions. As new datasets along with novel evaluation protocols are proposed in this paper, the authors had to re-implemented all the benchmarking solutions. The obtained results show that the proposed CapsField solution outperforms the state-of-the-art solutions by a significant margin for both face and expression recognition tasks, for the new face datasets. The rest of this paper is organized as follows: Section II briefly reviews the basic concepts and the added value of LF imaging for face recognition and provides a comprehensive review of recent advances in LF-based face and expression recognition solutions and datasets to better understand the technological landscape in this field. The proposed CapsField solution is presented in Section III and the new face datasets are described in Section IV. Section V presents an extensive performance evaluation for the proposed and state-of-theart solutions using a novel evaluation framework addressing varied and challenging recognition tasks. Finally, Section VI concludes the paper.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Background And Related Work",
      "text": "This section starts with an overview of light field imaging basics, followed by a LF-based face and expression recog-",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Object",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Object Point",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Main Lens",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Sensor",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Micro-Lens Array",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Different Angular Information",
      "text": "Fig.  1 : Lenslet light field imaging system. nition state-of-the-art review, and finally overviews the main face datasets available, including the LF ones.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Light Field Imaging Basics",
      "text": "LF imaging has been researched for more than a century  [27] . In 1908, Lippmann used small and closely spaced circular lenses to record different perspectives of a scene, so that an image could be observed from a selected perspective through an array of lenses  [28] . In 1936, Gershun coined the term light field, referring to the amount of light traveling in all directions through every point in space  [29] . In 1991, Adelson proposed the Plenoptic function P(x,y,z,t,λ,θ,φ), describing the information carried by light rays at any point in 3D space (x,y,z), for all directions (θ,φ) and wavelengths (λ), along time (t). The static 4D LF  [11] , L(x,y,u,v), also known as Lumigraph  [30] , proposed a representation based on the intersection points of the light rays with two parallel planes  [31] .\n\nFor capturing LF images, the type of visual data adopted in this work, two main practical setups are currently available, notably using a high density array of cameras or a lenslet LF camera. In a lenslet LF camera, as illustrated in Figure  1 , a micro-lens array, consisting of a set of micro-lenses, is placed at the focal plane of the main lens at a given distance from the photo sensor  [10] . This allows for the incoming light to be split based on its direction, and projected onto the photo sensor area. Each micro-lens can be thought of as a small camera, capturing light rays from a specific point in the scene, thus acquiring a so-called micro-image  [32] . A sample of a full set of LF color demosaiced micro-images is shown in Figure  2   In this context, there are two different types of correlation available within a multi-view SA array: i) the spatial intraview correlation within views; and ii) the angular inter-view correlation between different views. Therefore, LF images open the possibility for spatio-angular information to be exploited, allowing to boost the face and expression recognition performance by: i) a posteriori refocusing to improve the quality of the face region(s) that may be out-of-focus during acquisition; ii) exploiting the disparity for representing variations associated with different light directions captured in LF images; and iii) exploiting depth for providing geometric information about the position and shape of facial components.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Light Field-Based Face Recognition Solutions",
      "text": "Recently, several face recognition solutions using LF images have been proposed. This section reviews this category of face recognition solutions, which are able to exploit the availability of LF sensors and data. The main characteristics of the most notable recognition solutions in this area are summarized in Table  I , including the adopted feature extraction approach, classifier, exploited LF capability (as introduced in Section II), the type of LF data, and the datasets used by each solution for reporting performance results. For comparison, Table I also includes the characteristics of the CapsField solution proposed in this paper. The solutions summarized in the table are reviewed here, notably grouped based on the feature extraction approaches considered.\n\n1) Visual Descriptor-based Solutions: The first group of visual descriptor-based solutions relies on the a posteriori refocusing capability to improve the quality of out-of-focus facial regions. In  [14]  a wavelet energy method is proposed for selecting the best focus plane. Then, the Local Binary Pattern (LBP) descriptor is applied to extract features and a Nearest Neighbor (NN) classifier is used for performing recognition. Another solution uses a resolution enhancement scheme  [15]  based on a discrete wavelet transform, to capture the components with highest frequencies from different refocused images. This solution creates after an all-in-focus face image to be input to an LBP descriptor. In  [34] , the recognition of multiple faces placed at different distances with an all-in-focus image rendered from the LF is investigated. An LBP descriptor is then applied for feature extraction and a Sparse Reconstruction Classifier (SRC) is used for classification. In  [16] , the LFbased face recognition solution renders a set of refocused images using two different approaches. The first selects the best refocused image for recognition while the second combines the refocused images for creating a super-resolved image. Different local descriptors including Histogram of Oriented Gradient (HOG), LBP, Center-Symmetric LBP (CSLBP), and Binarized Statistical Image Features (BSIF) are used to extract features for classification.\n\nThe second group of visual descriptor-based solutions exploits the depth information that can be estimated from an LF image. These solutions explore the geometry of the facial components, notably their positions and shapes. In  [35] , a depth map is exploited to extract discriminative HOG features used as input to a linear Support Vector Machine (SVM) for classification.\n\nThe last group of visual descriptor-based solutions relies on exploiting the LF disparity information. In  [17] , a face recognition solution is proposed based on a visual descriptor named Light Field LBP (LFLBP), exploiting the richer spatioangular LF information. LFLBP has two main components, the spatial LBP and the angular LBP, capturing not only the usual spatial information but also the angular information available in a set of SA images. Another solution is proposed in  [18]  based on a the Light Field Histogram of Gradients (LFHG) descriptor, which fuses the conventional HOG with a Light Field Histogram of Disparity Gradients (LFHDG) descriptor, to consider both the orientation and magnitude information.\n\n2) Deep Learning-based Solutions: Recognizing the importance of deep learning in biometric recognition, an LF face recognition solution has been designed based on a VGG 2D+Disparity+Depth (VGG-D3) fused deep descriptor  [19] . The VGG-D3 description is formed by concatenating descriptions extracted from 2D images as well as disparity and depth maps using the VGG-16 descriptor trained on the VGG-Face 1 dataset  [36] . This was the first solution to adopt a fused deep CNN network to exploit the complementary LF information, including 2D texture and the corresponding disparity and depth maps. Another face recognition solution is based on a doubledeep descriptor, so-called VGG + Conventional Long Short Term Memory (Conv-LSTM)  [20] . This solution exploits the multi-perspective LF information While the VGG-D3  [19]  processes only the information available in the 2D central SA image. The double-deep descriptor extracts spatio-angular LF dependencies using VGG and LSTM networks in sequence, to provide a more powerful description for face recognition. The solution in  [21]  proposes three novel LSTM cell architectures to jointly learn from LF horizontal and vertical parallaxes. The three cell architectures perform: i) Gate-Level Fusion LSTM (GLF-LSTM); ii) State-Level Fusion LSTM (SLF-LSTM); and iii) Sequential Learning LSTM (SeqL-LSTM). These architectures create richer spatio-angular LF descriptions for face recognition and have been integrated into an end-to-end framework, where a VGG network feeds the LSTM networks composed of the novel LSTM cell architectures.\n\nIn summary, given the nature of the available face recognition solutions and the way they were assessed, notably using different datasets and evaluation protocols, it is difficult to compare the various solutions and even more difficult to estimate how these solutions would perform in unconstrained operational conditions. Since more recent research has shifted towards in the wild face recognition, this paper addresses this   [34]  2013 Visual Descriptor LBP SCR A Posteriori Refocusing LF 2D Rendered LiFFID Face-Iris MF LF  [16]  2016 Visual Descriptor HOG; LBP; BSIF SRC A Posteriori Refocusing 2D Rendered LiFFID DM LF  [35]  2016 Visual Descriptor LFHOG SVM Depth Computation M-V SA Array Private LFLBP  [17]  2017 Visual Descriptor LFLBP SVM Disparity Exploitation M-V SA Array LFFD LFHG  [18]  2018 Visual Descriptor HOG; LFHDG SVM Disparity Exploitation M-V SA Array LFFD VGG-D3  [19]  2018",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Light Field-Based Expression Recognition Solutions",
      "text": "There are currently two solutions available for exploiting additional LF information for face expression recognition. Inspired by  [20] , a deep learning spatio-angular fusion framework was adopted in  [22] , to model both the spatial and angular information with VGG-16 and LSTM recurrent networks. In  [23] , the deep framework proposed in  [22]  was extended by exploring both forward and backward angular relations using bidirectional LSTM. Additionally, an attention mechanism selectively focuses on the most important spatio-angular features, thus learning the final features more effectively.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Sampling Schemes And Fusion Strategies",
      "text": "In order to exploit the angular dependencies between LF views, the views should be arranged in the form of a pseudovideo sequence to feed an LSTM network. In  [20] , 11 different sampling schemes were considered to select and scan the set of SA images, notably varying their number, position, and scanning order. The results showed that capturing angular information along the horizontal and vertical directions leads to better average performance when compared to other sampling schemes, notably due to the consideration of larger disparities. The results also showed that the score-level fusion of horizontal and vertical angular classification models leads to additional improvements.\n\nAn alternative approach to the score-level fusion strategy is to jointly learn the horizontal and vertical view sequences, as done by the GLF-LSTM and SLF-LSTM cell architectures proposed in  [21]  (introduced in Section II.B). Additionally, there are other alternative solutions, not originally designed for the same task, but which are flexible enough to be adopted for simultaneously learning from the horizontal and vertical view sequences. Examples are the Spatio-Temporal LSTM (ST-LSTM)  [37]  and the Dual-Sequence LSTM (DS-LSTM) cell architecture  [38] , which were originally designed to fuse different sequences for activity and speech recognition, respectively. These alternative LSTM cell architectures will be considered for benchmarking purposes, when comparing to the proposed solution in this paper.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "E. Face Datasets: Status Quo",
      "text": "There are currently over 150 publicly available face datasets  [39] . The characteristics of the more notable face datasets are presented in Table  II , highlighting variations in terms of acquisition conditions, expressions, poses, and occlusions. For easier reading, the datasets are sorted according to their release date. For comparison, Table II also includes the characteristics of the LFFW and LFFC datasets proposed in this paper.\n\nSince LF imaging is a relatively new technology, to the best of the authors' knowledge, only two face datasets have been made publicly available. The Light Field Face and Iris Database (LiFFID)  [16]  was the first face dataset to include images acquired with a LF camera for facial recognition purposes. It includes a set of 2D greyscale images, focused at different depths, rendered from the LF content acquired using a first generation, lower resolution, Lytro lenslet camera. However, LiFFID does not include the raw LF images, which is a major limitation for many research applications. The IST-EURECOM Light Field Face Database (LFFD)  [50]  was the second LF face dataset made available. This dataset includes raw and rendered data from 100 subjects, with 20 LF samples per person, where each LF sample includes 225 2D SA images, captured by a second generation Lytro ILLUM lenslet camera, in two separate acquisition sessions. The images are captured in a controlled acquisition setup with different facial variations, referring to application scenarios where the subjects present themselves to a fixed camera with a uniform background.\n\nAs can be observed from Table  II , the more recent face datasets have shifted towards the 'in the wild' scenarios to consider more challenging variations. Nevertheless, since the two previously available LF face datasets were both collected in constrained conditions, it is not possible to assess the performance of LF-based face recognition solutions for unconstrained setups. The proposed LFFW dataset addresses this shortcoming by including 1908 LF images, corresponding to 429,300 2D images, from 53 subjects, thus providing a largescale publicly available LF face dataset captured in the wild. Since this dataset will be publicly available, it may be used as the basis for the future design, validation, and assessment of LF-based face recognition systems. The proposed LFFC dataset complements the LFFW dataset by including 1060 LF and 238,500 facial 2D images from the same 53 subjects, TABLE II: Overview of main face datasets with relevant characteristics.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Dataset Name",
      "text": "Year # of 2D Images In the Wild? Image Type Sensor Face Variation Time Env. Dist. Light Poses Exp. Occl. AR  [40]  1998 ≈ 4k No Color 2D FERET  [41]  2003 ≈ 14k No Gray/Color 2D LFW  [42]  2007 ≈ 13k Yes Color 2D Multi-PIE  [43]  2009 ≈ 750k No Color 2D MOBIO  [44]  2010 ≈ 30k Yes Color 2D YouTube Faces  [45]  2011 ≈ 606k Yes Color 2D Video SCface  [46]  2011 ≈ 4k No Color/Infra. 2D; Infra. BU-3DFE  [47]  2013 ≈ 370k No Color 2D+Depth Kinect Face DB  [48]  2014 ≈ 2k\n\nNo Color 2D+Depth PIPA  [49]  2015 ≈ 63k Yes Color 2D VGG-Face 1  [36]  2015 ≈ 2.6 M Yes Color 2D LiFFID  [16]  2016 ≈ 107k No Gray LF LFFD  [50]  2016 ≈ 900k No Color LF MegaFace  [51]  2017 ≈ 4.7M Yes Color 2D DFW  [52]  2018 ≈ 11k Yes Color 2D VGG-Face 2  [53]  2018\n\ncaptured between 1 day and 3 years before the corresponding LFFW acquisition. It must be highlighted that while the LFFC acquisition conditions are similar to the previously available IST-EURECOM LFFD dataset  [50] , the subjects are partly different. The newly proposed LFFW and LFFC datasets offer from now on a unique publicly available data platform as this dataset combination provides precious information for the face recognition community, for instance to study aging effects and cross-dataset generalization performance of different analysis solutions.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Iii. Capsfield Solution",
      "text": "This section presents the proposed face and expression recognition solution, exploiting the spatio-angular information available in LF images by using a combination of two deep learning networks.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Problem Formulation",
      "text": "Given a dataset D of LF face images, where each LF image LF i,j has been captured from a known face/expression with label x i ∈ {x 1 , x 2 , . . . , x N } and a set of viewing angles θ j ∈ {θ 1 , θ 2 , . . . , θ V }, the tasks of face or expression recognition for a probe face sequence LF probe can be defined as:\n\nwhere P r(x i |LF probe , D) is the probability of LF image LF probe belonging to label x i .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Solution Intuition And Overview",
      "text": "An LF image offers intra-view/spatial (within each view) and inter-view/angular (across views) information that can be used for improving face and expression recognition performance. Spatial features can be learned from each view using powerful CNN architectures. The inter-view information can be explored by combining the spatial features of individual views, for instance using feature level fusion, and feeding the result to a fully connected layer and then a classifier. However, the fully connected layer merely applies a weight vector to the concatenated feature vector, applying a non-linear function to the scalar output of a linear filter. This approach considers the concatenated features as a whole, not allowing to learn the relative positions of relevant face components in each view or across the different viewpoints, which is the main problem addressed in this paper.\n\nA capsule network includes layers of capsules, each consisting of a group of neurons representing different properties of the input. The mechanism to learn interactions between capsules is performed with routing algorithms such as dynamic routing  [24] . Capsules can share the knowledge across locations using a pose matrix that represents the relation between parts of an object and the whole object, thus making capsules capable of dealing with viewpoint changes. Accordingly, the activation of a capsule is also based on a comparison between multiple incoming pose predictions. Capsule networks have recently been adopted in the context of face analysis tasks, most notably in age and gender classification  [25] , and action detection  [26] . This paper proposes the combination of a capsule network with a CNN to, respectively, learn the available inter-view and intra-view relations available in an LF image for face and expression recognition. In this context, the novel CapsField solution not only learns viewpoint-invariant relations between different parts of a face and the whole face in each view, but also it learns the relations between the different views along the whole multi-view sequence. A capsule network is also capable of learning feature importance by assigning higher weights to the more relevant features, while ignoring the spurious dimensions  [54] . This functionality can act as an attention mechanism in the proposed CapsField solution, selectively focusing on the most important angular features making the solution more robust to changes in appearance and environment.\n\nFollowing the motivation and intuition described above, the proposed CapsField solution integrates three sub-networks that are applied to the horizontal and vertical views to predict a classification lable, L, as follows: where f H CN N and f V CN N are CNN feature extraction subnetworks, respectively, applied to horizontal and vertical spatial SA sequences; these sub-networks are composed of multiple convolutional, pooling, and fully connected layers. f H CAP S and f V CAP S are capsule routing sub-networks that learn deeper part-whole relations between the spatial features and then selectively assign higher weights to the more discriminative features while ignoring misleading ones. The last pair of sub-networks, f H CLS and f V CLS , respectively, perform classification for horizontal and vertical learned features using a softmax activation function. The results of the these three sub-networks, independently applied to horizontal and vertical views, are then fused using a sum-rule strategy, thus obtaining the predicted label, l.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Network Architecture And Walkthrough",
      "text": "Figure  3  represents the architecture of the proposed Caps-Field solution composed by a pre-processing step and three sub-networks: CNN, capsule, and classification, whose descriptions are provided in the following.\n\n1) Pre-processing: This paper uses the Light Field Toolbox v0.4 software  [55]  to render the lenslet Light Field Raw (LFR) images, thus creating a multi-view SA array with 15×15 SA images, as illustrated in Figure  3 -left. At this stage, the face region is cropped within each 2D SA image using the provided face bounding box locations available in the datasets. Next, the middle row and the middle column SA images from the multi-view SA array are scanned into two separate SA image viewpoint sequences (as shown by red and green rectangles in Figure  3 -left), thus capturing the viewpoint changes along the horizontal and vertical directions, respectively. These two separate SA image viewpoint sequences are used as inputs to the proposed deep learning network.\n\n2) CNN Feature Extraction: The proposed CapsField solution first utilizes CNNs to independently extract spatial features from the two input sequences, i.e., horizontal and vertical SA sequences. To this end, the VGG-16  [36]  and Resnet50  [56]  CNN architectures have been, respectively, used for the face and expression recognition tasks. As both CNNs have been trained on the large-scale VGG-Face 2 dataset  [53] , the proposed solution will not suffer from overfitting and there is no additional training performed at this stage. More details about the used pre-trained models are provided in Section V-D.\n\n3) Capsule Routing: A capsule network includes layers of capsules where each capsule consists of a group of neurons representing the information being learned. Capsule networks  [24]  are composed of primary and secondary layers.The first layer encapsulates the input using convolutional, reshaping, and squashing operations to represent the instantiating parameters, such as positional attributes, in the form of a vector. The secondary layer then learns deeper part-whole relations between the sub-parts of an object and the whole object. In the CapsField solution proposed in this paper, the convolution operation has been removed from the primary capsule layer, so that only reshaping and squashing functions are applied to the spatial features coming from the previous sub-network. This modification is due to the fact that the previous CNN feature extraction sub-network already encodes the spatial information through several convolutional layers.\n\nThe output generated by the CNN feature extraction subnetwork, i.e., the sequence of multi-view features, is first reshaped to a N c ×C s tensor where N c and C s are the number of primary capsules and their size, respectively. This operation is followed by a non-linear squashing activating function, creating an output vector in which each vector element takes a value in the [0,1] range:\n\nwhere v j is the j th vector output of the primary capsule layer and s j is the j th capsule input formed by reshaping the output of the CNN feature extraction sub-network. The orientation of the output vector, v j , represents the positional properties of the input. This means that when the viewpoint of a section of a face changes, the orientation of the vector also changes while its length remains unchanged.\n\nThe secondary capsule layer is constructed by N c capsules, where each capsule is updated by a dynamic routing process as originally proposed in  [24] . The iterative dynamic routing process first learns the coupling coefficients, C i,j , between two capsules, i and j, available in two subsequent layers. After the degree of agreement between them is computed using the routing softmax function:\n\nwhere b i,j is the log probability indicating whether capsules i and j should be coupled.\n\nNext, the input to capsule j, s j , is computed as follows:\n\nwhere W ij is a trainable pose matrix that encodes the relation between the view and the views' sequence, thus establishing an intrinsic capacity for learning angular information. Eq. (  5 ) can also be considered as an attention layer, where the coupling coefficients play the role of attention weights. Finally, a squashing function (Eq. (  3 )) is used again to create the output vector of capsule j. The iterative dynamic routing process is repeated between the primary and secondary layers for N r iterations, and the final outputs are used to feed the classifier.\n\n4) Classification: The proposed network uses two independent dense layers with softmax activation as the classifier for the horizontal and vertical capsule features, thus computing the corresponding classification scores. Score-level fusion is finally applied, averaging the horizontal and vertical classification scores to compute the final class probability vector, determining the face or expression recognition result.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "D. Implementation And Training Details",
      "text": "The parameter values for training the proposed network are empirically tuned to achieve the best performance for both the face and expression recognition tasks, as presented in Table  III . This table presents the optimal parameter values for each subnetwork as well as for the whole deep network, CapsField. As can be observed, the network settings for both face and expression recognition tasks are rather similar except for: i) the choice of CNNs, as the two tasks use different CNN models for spatial feature extraction; and ii) the training batch size, as the number of input samples is different for the face and expression recognition tasks. Concerning the number of routing iterations, different values ranging from 2 to 6 have been considered, where increasing the value to more than 3 did not improve the performance while increasing the computational time. The entire architecture has been implemented using TensorFlow  [57]  with Keras backend and is trained using a GeForce GTX 1080 GPU. different perspectives, the LFFW dataset includes 429,300 (53×36×225) 2D facial images in total. The participants were between 20 and 62 years old, from 10 different countries across the globe. The unconstrained variations considered in the proposed LFFW dataset can be categorized into two types: i) acquisition variations, notably different dates, environments, distances, and lighting conditions; and ii) facial variations, notably facial poses, random expressions, occlusions, and actions with no pre-defined protocols or any stimuli used to induce the desired variations. All LF images were taken under 'in the wild' conditions, meaning that no restrictions were imposed on the subjects when capturing their photos. The LF images have been captured using a Lytro ILLUM LF camera [33], with a 40 Megaray sensor and a 30-250 mm lens with 8.3x optical zoom and f/2.0 aperture.\n\nIn addition to the LFFW dataset, this paper also proposes a complementary, constrained dataset with the same 53 LFFW subjects, named LFFC, which includes their facial images captured in a controlled acquisition setup. The LFFC LF images have been acquired between 1 day and 3 years prior to acquiring the corresponding in the wild images. The LFFC dataset includes 20 LF shots per person, with different facial variations including expressions, actions, poses, illuminations and occlusions, thus providing 238,500 (53×20×225) 2D facial images in total. This dataset provides additional information essential for studying aging effects and the constrained versus in the wild cross-dataset generalization ability.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Lffw Dataset Structure",
      "text": "The hierarchical structure designed for the LFFW dataset is illustrated in Figure  4 . The LF image acquisition has been performed in both indoor and outdoor environments, at different locations, with different backgrounds. The acquisition has also been performed in different lighting conditions, e.g., from gloomy to extremely sunny weather, allowing images to be captured in the wild with different illuminations and in the presence of uncontrolled shadows, and backgrounds. As shown in Figure  5 (a), for each environment, LF images were",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Lffw Dataset Elements",
      "text": "The LFFW dataset is composed by the following elements:  acquisition of the two datasets also allows to study the impact of aging. In order to illustrate how challenging cross-dataset studies can be, t-Distributed Stochastic Neighbor Embedding (t-SNE) visualization  [60]  has been used to summarize the distribution of images available in the two LFFW and LFFC datasets in a two-dimensional space (see Figure  8 ). t-SNE maps the multi-dimensional input data to a lower dimensional feature space. The figure shows that considerable difference exists between the two datasets, further indicating that generalization across the two datasets (training on one and testing on the other) may be difficult.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "D. Ethics, Access, And Usage Conditions",
      "text": "The participants in the capture of both datasets all signed a consent form before the acquisition, allowing the collection of LF images of their faces, to become part of the proposed datasets, naturally, to be used for research purposes. All access to the dataset images is controlled and supervised, and images will be used anonymously. The generic information asked from the subjects (such as age or gender), to be included as metadata, will also be made available anonymously. Only images from some specific subjects (shown in Figure  9 ) may appear in publications as per received consent.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "E. Facial Expression Annotation",
      "text": "The LFFW dataset includes facial images that are captured with random expressions. To support their usage for expression recognition algorithms, the random expressions are carefully analyzed by the authors of this paper in order to validate and label the acted expressions. In this context, 832 face LF images have been manually annotated with different expression labels including: neutral, happiness, anger, surprise, sadness and disgust, as illustrated in Figure  9 . As can be seen, some face expressions are captured in very challenging acquisition conditions, e.g., at very high or very low illumination levels or with occlusions by hand or glasses, making the expression recognition task more difficult.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "V. Performance Assessment",
      "text": "This section presents the proposed experimental evaluation protocols and the corresponding performance results and analysis for the proposed and benchmarking recognition solutions. This section also performs ablation experiments, investigating the effect of the individual sub-networks on the overall recognition performance.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "A. Face Recognition Evaluation Protocols",
      "text": "To assess the overall face recognition performance for in the wild conditions, five evaluation protocols using the LFFW and LFFC datasets are proposed. These protocols divide the whole data into training and testing sets as described below, where 10% of the training set images have been used for validation in all the test protocols.\n\n1) Cross-environment Evaluation Protocol: This protocol uses the 18 indoor LFFW LF images of each subject for training, whereas the testing phase uses the LFFW outdoor LF images. Then, the process is repeated using the outdoor images as training data and the indoor images as test data. For this protocol, performance results are reported with respect to the various facial variations. Moreover, the sensitivity of the face recognition solutions to the unconstrained environmental conditions in the wild, such as lighting, shadows, occlusions, and backgrounds, is analyzed.\n\n2) Cross-distance Evaluation Protocol: In this protocol, the training stage uses the 12 LFFW LF images of each subject captured from one of the three distances (close, moderate and far), whereas the testing phase independently uses the 24 captured LF images from the two other distances. This process is repeated, changing the training and testing (distance) sets, for a total of six different iterations. This protocol evaluates the face recognition solutions' sensitivity to various details in the face region resulting from capturing images at different distances and in the wild.\n\n3) Cross-pose and expression Evaluation Protocol: This protocol considers practical scenarios where only frontalneutral LF images are available in the training dataset, and LF facial images captured in the wild are used for testing. Therefore, the training set contains only the 6 frontal-neutral LFFW LF images of each subject, while the testing set considers all the other facial variations.\n\n4) Cross-dataset Evaluation Protocol: This protocol performs the training with the LFFC dataset and the testing with the LFFW dataset, and vice-versa. The performance results are reported with respect to the different facial variations. This protocol evaluates the generalization ability of the face recognition solutions, and to some extent the robustness to aging effects. This type of assessment is rarely found in the literature as it requires the availability of appropriate complementary datasets as available in this work. This type of assessment is performed here, for the first time, with LF images.\n\n5) Subject-Independent Evaluation Protocol: This protocol performs the training with all the images from the first 28 subjects (ID No. 1 to 28) in the LFFW dataset. The trained model is then used as a feature extractor for the testing set including all the images from the remaining 25 subjects. Then, in the testing set, the frontal images have been used as the gallery samples and the rest of the images have been used as probe samples to perform the recognition process. In this protocol, the testing identities are disjoint from the training ones, thus making the face recognition process more challenging and closer to real-world application scenarios.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "B. Expression Recognition Evaluation Protocols",
      "text": "To assess the performance of the face expression recognition solutions using the annotated face images, two evaluation protocols are proposed. It should be noted that the number of evaluation protocols designed for expression recognition is less than for face recognition since the expression samples are comparatively limited, both in terms of numbers and variations.\n\n1) Subject-Independent 4-fold Cross Validation: The first evaluation protocol performs subject-independent 4-fold Cross Validation (CV) on the LFFW dataset. Accordingly, the proposed expression recognition solutions are trained with 1/4 of the annotated images available in each expression class, while the remaining 3/4 of the LF images are used for testing. This process is repeated 3 more times with the next folds and the average results are reported.\n\n2) Cross-dataset Evaluation Protocol: This protocol considers cross-dataset evaluation by using expression samples from the LFFC dataset for training and the LFFW dataset for testing, and vice-versa. The in the wild and constrained samples were, respectively, categorized into 6 and 4 expression classes. The two additional expression classes for the in the wild dataset not available in the constrained dataset, i.e., sadness and disgust, are naturally removed from the experiments for this evaluation protocol.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "C. Benchmarking Recognition Solutions",
      "text": "Since this paper proposes new evaluation protocols for the two developed datasets, the authors have re-implemented all the benchmarking solutions for both face and expression recognition tasks since otherwise no comparative results would be available. To do so, the best configuration of parameters reported in the original papers have been used to report the rank-1 performance results in this paper.\n\n1) Face Recognition: The LF-based benchmarking solutions used for the face recognition task include LFLBP  [17] , LFHG  [18] , VGG-D3  [19] , ResNet50+ST-LSTM  [37] , VGG+Conv-LSTM  [20] , VGG+GLF-LSTM  [21] , VGG+SLF-LSTM  [21] , and ResNet50+DS-LSTM  [38]  that have been introduced in Section III. Three non-LF solutions including the VGG-16  [36] , Resnet50  [53] , SE-ResNet-50  [53] , and ArcFace with ResNet-101 backbone  [61]  networks, have also been used in the comparisons. Naturally, these non-LF solutions only process the 2D central SA image. It must be noted that some of LF-based face recognition solutions, e.g.,  [14] ,  [15] ,  [16] , consider the a posteriori refocusing capability of LF cameras to improve the image quality of the already captured face images for 2D face recognition. Naturally, these methods are not comparable with the proposed solution which deals directly with the LF data, in its native 4D multi-view data format, to exploit the available spatio-angular information.\n\n2) Expression Recognition: Four available LF-based solutions including ResNet50+ST-LSTM  [37] , VGG+LSTM  [22] , VGG+BiLSTM+Attention  [23] , and ResNet50+DS-LSTM  [38] , as introduced in Section III-C, are used. The benchmarking solutions also include the VGG-16  [53]  network, pretrained with VGG-Face 2.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "D. Comparative Face Recognition Performance",
      "text": "This section reports the rank-1 face recognition rates, for the four evaluation protocols proposed earlier, when using CapsField and 12 state-of-the-art face recognition solutions, with the best results highlighted in bold.\n\n1) Cross-environment Performance: The results in Table IV refer to the cross-environment protocol and are independently reported for different facial variations, showing that the environment variations, including different lighting conditions, shadows, occlusions, and non-uniform backgrounds can negatively impact the face recognition performance in the wild. The proposed CapsField solution delivers very impressive performance results when trained with indoor and tested with outdoor samples. The proposed solution achieves less impressive results when tested with indoor samples, showing that it is sensitive, notably to low lighting conditions. Overall, the results show that, on average, the proposed CapsField solution is superior to the other benchmarking solutions for this evaluation protocol.\n\n2) Cross-distance Performance: The cross-distance protocol results are presented in Table  V . Since the Lytro ILLUM LF camera used in this research has a small baseline, it is expected that more disparity is captured at closer distances. Additionally, the cropped face images have different resolutions, due to the different capturing distances. As the CNN feature extractor first resizes all the images to 224×224 pixels, the images with larger or smaller resolutions were, respectively, down-sampled or up-sampled to be then processed by the network; this means the images captured at far distances may include up-sampling artefacts. Hence, the overall performance results for all benchmarking solutions are better when testing with the images captured from close and moderate distances, as these LF images feed the recognition solutions with more details, thus further contributing to an improved performance. The proposed CapsField solution obtains very good results, notably achieving a performance gain of 2.95% when compared to the best performing benchmarking solution, i.e., VGG+SeqL-LSTM  [21] .\n\n3) Cross-pose and expression Performance: The cross-pose and expression protocol results are presented in Table  VI  for the several facial variations considered, notably including different face orientations, random expressions, occlusions, and actions. The overall performance results for the benchmarking solutions show poor performance when trained with only frontal faces and tested with different poses, notably fullprofile images. The performance is also poor for random action variations as the type of actions during training can be very different from those used for testing. Overall, the performance of the proposed CapsField solution is considerably better than all other benchmarking solutions, notably showing a gain of 15.85% over the best benchmark.\n\n4) Cross-dataset Performance: The results in Tables VII and VIII refer to the cross-dataset evaluation protocol and correspond to two situations: i) LFFC is used for training and LFFW is used for testing; and ii) LFFW is used for training and LFFC is used for testing. As expected, better performance results are obtained when testing on the less challenging LFFC dataset. The results show that the proposed solution outperforms all the benchmarking solutions for both cases by a large margin, achieving performance gains of 11.05% (when testing with LFFC) and 14.62% (when testing with LFFW), compared to the best performing benchmark. These results, showing a better generalization ability of the proposed CapsField solution, clearly illustrate the advantage of exploring the richer information available in LF images with the consideration of a more robust model created by capsule networks.\n\n5) Subject-independent Performance: The subjectindependent protocol results are presented in Table  IX  for the facial variations considered, notably different poses, random expressions, occlusions, and actions. Overall, the average performance of the proposed CapsField solution is better than all the benchmarking solutions, even for this protocol, where the face recognition process is more challenging and closer to real-world application scenarios.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "E. Comparative Face Expression Recognition Performance",
      "text": "This section reports the expression recognition accuracy obtained for the two proposed evaluation protocols. The performance results are reported in Table X for the proposed and 6 benchmarking solutions, with the best results highlighted in bold.\n\nThe performance results for both evaluation protocols clearly show that the proposed CapsField solution achieves considerably better performance than the non-LF-based deep recognition solutions, presented in the first four rows of Table  X . This improvement is due to the fact that the proposed solution exploits the available spatio-angular information in the LFs for the face expression recognition task. The results also show the superiority of the proposed CapsField solution over the LF-based ones, notably VGG+LSTM  [22]  and VGG+BiLSTM+Attention  [23]  solutions. As the comparative LF-based and the proposed solutions use the same spatial CNN model, i.e., VGG-16 pre-trained with VGG-Face 2, the performance results reveal that the capsule network better models the richer angular features than the LSTM and BiL-STM recurrent networks used in the benchmarking solutions for the LF expression recognition task.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "F. Benefits Of Lf Images For Face Recognition",
      "text": "An additional experiment has been performed to evaluate the added value of LF images in comparison with single-view images for face recognition. This considers the comparison between four LF-based face recognition methods including LFLBP  [17] , HOG+HDG  [18] , VGG-16 + LSTM  [20] , and the proposed ResNet-50 + Capsule, and their corresponding single-view versions, including LBP  [62] , HOG  [63] , VGG-16  [36] , and ResNet-50  [53] , applied to the central SA image. The results presented in Table XI clearly demonstrate the superiority of the LF-based methods over their corresponding single-view baselines for the four subject-dependent test protocols, thus highlighting the added value of LF images for face recognition.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "G. Angular Feature Space Exploration",
      "text": "To study the impact of different strategies for dealing with the available LF angular information, t-SNE  [60]  is used again to plot the feature spaces produced by the different strategies, thus showing the different discrimination ability of the various LF angular features in a two dimensional space. Figure  10  offers a visualization of the features that are used as inputs to the classifier, when four different strategies are used to deal with the angular information, notably: i) only the 2D central SA image is used, thus ignoring the available angular information (Figure  10-a ) ; ii) the concatenated features extracted from all the views are used (without capsule network) to feed the classifier (Figure10-b); iii) LSTM RNN  [20]  is used to exploit the angular information (Figure10-c); and iv) capsule network, as proposed in this paper, is used to exploit the angular information (Figure  10-d ). To make the visualisation legible, the t-SNE analysis is only performed for the first 10 LFFW subjects using the cross-pose and expression face recognition protocol. The t-SNE plots show that the proposed CapsField solution forms denser and more effective clusters that can facilitate distinguishing between the various classes; this validates the superiority of the capsule network in learning angular information.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "H. Ablation Study",
      "text": "Ablation experiments have been performed based on the proposed evaluation protocols for the adopted datasets by systematically removing individual components of CapsField one by one and evaluating the performance of the 'reduced' models. It must be noted that the CNN sub-network must always be kept in order to extract the spatial features. Table XII presents the ablation experiment results for both the face and expression recognition tasks. For comparison, the performance  the classification scores of both LF sequences, average performance gains of 0.80% and 1.04% are obtained, respectively, when compared to a CapsField version considering only the horizontal or vertical SA image sequences.\n\n2) Impact of Capsule Sub-network: Finally, the capsule component is removed, and the CNN spatial features obtained from different LF SA images are directly concatenated for classification. It is worth noting that the SE-ResNet-50  [53]  and VGG-16  [53]  results presented in Tables IV through VIII to a single 2D central SA image, while the features extracted from all the views are concatenated here. Removing the capsule sub-network decreases the recognition performance of the proposed CapsField solution as it limits the ability to learn the inter-view/angular relations between the spatial features. Moreover, in the absence of the capsule component, it is not possible to assign higher weights to the more relevant features. These observations reveal the added value of the capsule sub-network in learning a model that fully exploits the angular features available in LF images.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "I. Complexity Analysis",
      "text": "The computational time and embedding size for the proposed and benchmarking recognition solutions are studied in this section. This analysis has been done by measuring the execution times on a 64-bit Intel PC with a 3.20 GHz Core i7 processor, 48 GB RAM, and a GeForce GTX 1080 Ti GPU, running TensorFlow with Keras backend. Table  XIII  shows the training and testing times (in seconds) per each 2D/LF image as well as the final embedding size in terms of the number of embedding elements. It can be observed from Table XIII that the proposed CapsField solution offers the most compact embedding when compared to the benchmarking solutions, thus simplifying the retrieval and transmission of the embeddings as well as reducing the computational time for the testing phase. The required training time for the CapsField solution is higher than other benchmarking solutions, which is the tradeoff for extracting more discriminative features. It is also worth noting that the very high training and testing times for the VGG-D3  [19]  solution derives from the necessary disparity and depth maps extraction processes that are computationally very expensive.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "This paper proposes a new solution for both face and expression recognition tasks, called CapsField, based on the combination of convolutional neural and capsule networks. In order to analyze the performance of CapsField in the wild, this paper proposes the first unconstrained LF face dataset,",
      "page_start": 14,
      "page_end": 14
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Lenslet light ﬁeld imaging system.",
      "page": 2
    },
    {
      "caption": "Figure 2: (c), respectively. Since each 2D SA image ‘sees’",
      "page": 2
    },
    {
      "caption": "Figure 2: Light ﬁeld representation: a) sample micro-images,",
      "page": 3
    },
    {
      "caption": "Figure 3: Architecture of the CapsField solution.",
      "page": 6
    },
    {
      "caption": "Figure 3: represents the architecture of the proposed Caps-",
      "page": 6
    },
    {
      "caption": "Figure 3: -left. At this stage, the face",
      "page": 6
    },
    {
      "caption": "Figure 3: -left), thus capturing the viewpoint changes along",
      "page": 6
    },
    {
      "caption": "Figure 4: The LF image acquisition has",
      "page": 7
    },
    {
      "caption": "Figure 5: (a), for each environment, LF images were",
      "page": 7
    },
    {
      "caption": "Figure 4: Hierarchical structure of the proposed LFFW dataset.",
      "page": 8
    },
    {
      "caption": "Figure 5: Illustration of acquisition setup for (a) LFFW and (b)",
      "page": 8
    },
    {
      "caption": "Figure 6: Illustration of the 2D central SA images for the",
      "page": 8
    },
    {
      "caption": "Figure 6: B. LFFW Dataset Elements",
      "page": 8
    },
    {
      "caption": "Figure 5: (b); the acquisition",
      "page": 8
    },
    {
      "caption": "Figure 7: The availability of the LFFC dataset enables constrained",
      "page": 8
    },
    {
      "caption": "Figure 7: Illustration of the 2D central SA images for the",
      "page": 9
    },
    {
      "caption": "Figure 8: Visualization of LFFW and LFFC distributions using",
      "page": 9
    },
    {
      "caption": "Figure 9: Illustration of cropped 2D SA images with different",
      "page": 9
    },
    {
      "caption": "Figure 9: As can be seen, some",
      "page": 9
    },
    {
      "caption": "Figure 10: offers a visualization of the features that are used as inputs to",
      "page": 11
    },
    {
      "caption": "Figure 10: -a) ; ii) the concatenated features extracted",
      "page": 11
    },
    {
      "caption": "Figure 10: -d). To make the visualisation",
      "page": 11
    },
    {
      "caption": "Figure 10: t-SNE visualization of the feature spaces using four",
      "page": 13
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "LF Face [14]\nMulti-Face LF [15]\nSuper Res. LF [34]\nFace-Iris MF LF [16]\nDM LF [35]\nLFLBP [17]\nLFHG [18]\nVGG-D3 [19]\nVGG+ Conv-LSTM [20]\nVGG+ GLF-LSTM [21]\nVGG+ SLF-LSTM [21]\nVGG+ SeqL-LSTM [21]": "Prop. ResNet+Capsule",
          "2013\n2013\n2013\n2016\n2016\n2017\n2018\n2018\n2019\n2019\n2019\n2019": "2020",
          "Visual Descriptor\nVisual Descriptor\nVisual Descriptor\nVisual Descriptor\nVisual Descriptor\nVisual Descriptor\nVisual Descriptor\nDeep Nets\nDeep Nets\nDeep Nets\nDeep Nets\nDeep Nets": "Deep Nets",
          "LBP\nLBP; LG ﬁlter\nLBP\nHOG; LBP; BSIF\nLFHOG\nLFLBP\nHOG; LFHDG\nVGG\nVGG; Conv-LSTM\nVGG; GLF-LSTM\nVGG; SLF-LSTM\nVGG; SeqL-LSTM": "ResNet50 +\nCapsule Network",
          "NN\nSRC\nSCR\nSRC\nSVM\nSVM\nSVM\nSVM\nSoftmax\nSoftmax\nSoftmax\nSoftmax": "Softmax",
          "Depth Computation\nA Posteriori Refocusing\nA Posteriori Refocusing\nA Posteriori Refocusing\nDepth Computation\nDisparity Exploitation\nDisparity Exploitation\nDisparity & Depth Exploit.\nDisparity Exploitation\nDisparity Exploitation\nDisparity Exploitation\nDisparity Exploitation": "Disparity Exploitation",
          "LF 2D Rendered\nLF 2D Rendered\nLF 2D Rendered\n2D Rendered\nM-V SA Array\nM-V SA Array\nM-V SA Array\nM-V SA Array\nM-V SA Array\nM-V SA Array\nM-V SA Array\nM-V SA Array": "M-V SA Array",
          "Private\nLiFFID\nLiFFID\nLiFFID\nPrivate\nLFFD\nLFFD\nLFFD\nLFFD\nLFFD\nLFFD\nLFFD": "LFFW;\nLFFC"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "AR [40]\nFERET [41]\nLFW [42]\nMulti-PIE [43]\nMOBIO [44]\nYouTube Faces [45]\nSCface [46]\nBU-3DFE [47]\nKinect Face DB [48]\nPIPA [49]\nVGG-Face 1 [36]\nLiFFID [16]\nLFFD [50]\nMegaFace [51]\nDFW [52]\nVGG-Face 2 [53]": "Proposed LFFW\nProposed LFFC",
          "1998\n2003\n2007\n2009\n2010\n2011\n2011\n2013\n2014\n2015\n2015\n2016\n2016\n2017\n2018\n2018": "2019\n2019",
          "≈ 4k\n≈ 14k\n≈ 13k\n≈ 750k\n≈ 30k\n≈ 606k\n≈ 4k\n≈ 370k\n≈ 2k\n≈ 63k\n≈ 2.6 M\n≈ 107k\n≈ 900k\n≈ 4.7M\n≈ 11k\n≈ 3.6M": "≈ 429k\n≈ 238k",
          "No\nNo\nYes\nNo\nYes\nYes\nNo\nNo\nNo\nYes\nYes\nNo\nNo\nYes\nYes\nYes": "Yes\nNo",
          "Color\nGray/Color\nColor\nColor\nColor\nColor\nColor/Infra.\nColor\nColor\nColor\nColor\nGray\nColor\nColor\nColor\nColor": "Color\nColor",
          "2D\n2D\n2D\n2D\n2D\n2D Video\n2D;\nInfra.\n2D+Depth\n2D+Depth\n2D\n2D\nLF\nLF\n2D\n2D\n2D": "LF\nLF",
          "(cid:51)\n(cid:51)\n(cid:51)\n(cid:55)\n(cid:51)\n(cid:51)\n(cid:55)\n(cid:55)\n(cid:51)\n(cid:51)\n(cid:51)\n(cid:55)\n(cid:51)\n(cid:51)\n(cid:51)\n(cid:51)": "(cid:51)\n(cid:51)",
          "(cid:55)\n(cid:55)\n(cid:51)\n(cid:55)\n(cid:51)\n(cid:51)\n(cid:55)\n(cid:55)\n(cid:55)\n(cid:51)\n(cid:51)\n(cid:55)\n(cid:55)\n(cid:51)\n(cid:51)\n(cid:51)": "(cid:51)\n(cid:55)",
          "(cid:51)\n(cid:51)\n(cid:51)\n(cid:51)\n(cid:51)\n(cid:51)\n(cid:51)\n(cid:55)\n(cid:51)\n(cid:51)\n(cid:51)\n(cid:51)\n(cid:51)\n(cid:51)\n(cid:51)\n(cid:51)": "(cid:51)\n(cid:51)",
          "(cid:55)\n(cid:51)\n(cid:51)\n(cid:51)\n(cid:51)\n(cid:51)\n(cid:51)\n(cid:51)\n(cid:51)\n(cid:51)\n(cid:51)\n(cid:51)\n(cid:51)\n(cid:51)\n(cid:51)\n(cid:51)": "(cid:51)\n(cid:51)",
          "(cid:51)\n(cid:51)\n(cid:51)\n(cid:51)\n(cid:51)\n(cid:51)\n(cid:51)\n(cid:51)\n(cid:51)\n(cid:51)\n(cid:51)\n(cid:51)\n(cid:51)\n(cid:51)\n(cid:51)\n(cid:51)": "(cid:51)\n(cid:51)"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "CNN\nfeature\nextraction": "Capsule\nrouting",
          "CNN type\nPre-trained model\nEmbeding layer\nEmbeding size": "# of capsules\nCapsule size\n# of\nrouting itetrations",
          "ResNet50\nVGG-Face2\nAverage pooling\n2048": "5\n64\n3",
          "VGG-16\nVGG-Face2\nFC6\n4096": "5\n64\n3"
        },
        {
          "CNN\nfeature\nextraction": "CapsField Mini-batch siz\nnetwork",
          "CNN type\nPre-trained model\nEmbeding layer\nEmbeding size": "Loss function\nOptimizer\nMetric",
          "ResNet50\nVGG-Face2\nAverage pooling\n2048": "53\nCross-entropy\nrmsprop\nAccuracy",
          "VGG-16\nVGG-Face2\nFC6\n4096": "52\nCross-entropy\nrmsprop\nAccuracy"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "VGG-Face [36]\nLFLBP [17]\nSE-ResNet-50 [53]\nResNet-50 [53]\nLFHG [18]\nVGG-D3 [19]\nResNet50+ST-LSTM [37]\nVGG + Conv-LSTM [20]\nArcFace [61]\nVGG + GLF-LSTM [21]\nVGG + SLF-LSTM [21]\nResNet50+DS-LSTM [38]": "Prop. ResNet50 + Capsule —",
          "2015\n2017\n2018\n2018\n2018\n2018\n2018\n2019\n2019\n2020\n2020\n2020": "",
          "68.23%\n68.39%\n77.67%\n75.31%\n77.04%\n74.84%\n13.83%\n11.01%\n16.03%\n15.56%\n12.89%\n16.66%\n81.76%\n75.94%\n90.25%\n77.25%\n83.30%\n82.54%\n83.33%\n76.57%\n95.44%\n79.72%\n92.45%\n84.59%\n25.47%\n23.42%\n26.57%\n24.52%\n24.52%\n25.00%\n67.29%\n67.29%\n78.45%\n74.68%\n76.25%\n74.05%\n83.02%\n73.27%\n96.86%\n80.82%\n95.75%\n89.15%\n83.78%\n82.16%\n85.63%\n85.76%\n88.28%\n87.10%\n80.02%\n72.74%\n79.08%\n77.04%\n78.45%\n76.57%\n84.10%\n81.64%\n86.18%\n85.71%\n88.08%\n87.13%\n84.79%\n82.67%\n85.70%\n86.33%\n88.93%\n87.55%\n82.08%\n75.31%\n95.91%\n81.13%\n94.81%\n89.47%": "86.79%\n79.24%\n97.80%\n83.96%\n96.85%\n89.93%",
          "73.58%\n16.26%\n81.84%\n85.35%\n24.92%\n73.00%\n86.47%\n85.45%\n77.31%\n85.47%\n86.00%\n86.45%": "89.10%"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "VGG-Face [36]\nLFLBP [17]\nSE-ResNet-50 [53]\nResNet-50 [53]\nLFHG [18]\nVGG-D3 [19]\nResNet50+ST-LSTM [37]\nVGG + Conv-LSTM [20]\nArcFace [61]\nVGG + GLF-LSTM [21]\nVGG + SLF-LSTM [21]\nResNet50+DS-LSTM [38]": "Prop. ResNet50 + Capsule",
          "2015\n2017\n2018\n2018\n2018\n2018\n2018\n2019\n2019\n2020\n2020\n2020": "—",
          "90.57%\n38.68%\n24.09%\n58.18%\n40.25%\n43.40%\n18.87%\n6.92%\n21.38%\n13.52%\n89.30%\n79.87%\n44.65%\n72.64%\n64.77%\n92.13%\n87.42%\n53.45%\n81.44%\n74.84 %\n38.99%\n6.92%\n3.14%\n23.58%\n18.24%\n90.25%\n38.68%\n24.09%\n58.49%\n41.28%\n92.14%\n86.79%\n61.32%\n87.11%\n77.36%\n98.11%\n78.62%\n22.01%\n81.13%\n71.38%\n94.34%\n84.91%\n51.57%\n80.50%\n70.44%\n99.37%\n80.82%\n15.41%\n83.96%\n69.18%\n98.74%\n77.36%\n22.64%\n80.82%\n67.92%\n90.25%\n84.28%\n60.69%\n85.85%\n76.10%": "96.22%\n92.13%\n67.29%\n89.93%\n85.22%",
          "50.35%\n20.81%\n70.25%\n77.86%\n18.17%\n51.66%\n80.95%\n70.25%\n76.35%\n69.75%\n69.50%\n79.43%": "86.16%"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "VGG-Face [36]\nLFLBP [17]\nSE-ResNet-50 [53]\nResNet-50 [53]\nLFHG [18]\nVGG-D3 [19]\nResNet50+ST-LSTM [37]\nVGG + Conv-LSTM [20]\nArcFace [61]\nVGG + GLF-LSTM [21]\nVGG + SLF-LSTM [21]\nResNet50+DS-LSTM [38]": "Prop. ResNet50 + Capsule",
          "2015\n2017\n2018\n2018\n2018\n2018\n2018\n2019\n2019\n2020\n2020\n2020": "—",
          "91.51%\n85.85%\n51.89%\n11.01%\n57.23%\n46.86%\n11.64%\n13.21%\n10.37%\n5.66%\n8.49%\n6.60%\n78.30%\n77.35%\n69.18%\n51.88%\n66.03%\n58.80%\n87.73%\n86.16%\n77.67%\n52.51%\n73.27%\n67.29%\n30.19%\n27.99%\n11.01%\n7.55%\n12.58%\n8.18%\n90.25%\n85.22%\n51.26%\n10.69%\n59.75%\n44.97%\n93.39%\n90.57%\n81.76%\n61.64%\n82.07%\n73.90%\n90.57%\n86.16%\n71.07%\n34.48%\n72.01%\n60.69%\n91.51%\n88.05%\n72.01%\n47.80%\n74.84%\n64.15%\n89.94%\n83.33%\n72.96%\n41.82%\n72.33%\n57.55%\n91.51%\n83.65%\n71.07%\n40.88%\n71.07%\n58.81%\n92.76%\n89.62%\n79.87%\n60.37%\n80.81%\n73.58%": "92.76%\n90.88%\n82.70%\n61.32%\n82.07%\n74.84%",
          "57.38%\n9.33%\n66.92%\n74.10%\n16.24%\n57.02%\n80.56%\n69.50%\n73.06%\n69.65%\n69.50%\n79.51%": "80.76%"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "VGG-Face [36]\nLFLBP [17]\nSE-ResNet-50 [53]\nResNet-50 [53]\nLFHG [18]\nVGG-D3 [19]\nResNet50+ST-LSTM [37]\nVGG + Conv-LSTM [20]\nArcFace [61]\nVGG + GLF-LSTM [21]\nVGG + SLF-LSTM [21]\nResNet50+DS-LSTM [38]": "Prop. ResNet50 + Capsule",
          "2015\n2017\n2018\n2018\n2018\n2018\n2018\n2019\n2019\n2020\n2020\n2020": "—",
          "84.91%\n85.53%\n84.91%\n68.87%\n82.08%\n67.92%\n17.92%\n13.21%\n16.98%\n15.09%\n13.21%\n17.92%\n83.01%\n79.87%\n77.35%\n81.76%\n85.84%\n69.81%\n96.22%\n94.96%\n92.45%\n86.62%\n96.22%\n82.38%\n27.35%\n34.90%\n25.47%\n18.86%\n33.01%\n16.03%\n84.91%\n86.16%\n83.96%\n69.18%\n83.02%\n68.24%\n98.11%\n96.23%\n92.45%\n94.65%\n97.17%\n82.28%\n92.45%\n89.94%\n90.57%\n76.10%\n88.68%\n73.27%\n98.11%\n98.11%\n97.16%\n78.93%\n97.16%\n90.88%\n90.57%\n90.57%\n88.68%\n78.30%\n89.62%\n71.07%\n88.68%\n89.94%\n89.62%\n79.25%\n87.74%\n71.70%\n96.27%\n96.86%\n93.40%\n93.71%\n97.17%\n82.70%": "100%\n98.11%\n96.22%\n97.48%\n100%\n90.56%",
          "74.81%\n15.72%\n77.92%\n89.52%\n25.94%\n75.09%\n91.98%\n80.85%\n90.00%\n80.75%\n80.94%\n91.32%": "95.75%"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "VGG-Face [36]\nLFLBP [17]\nSE-ResNet-50 [53]\nResNet-50 [53]\nLFHG [18]\nVGG-D3 [19]\nResNet50+ST-LSTM [37]\nVGG + Conv-LSTM [20]\nArcFace [61]\nVGG + GLF-LSTM [21]\nVGG + SLF-LSTM [21]\nResNet50+DS-LSTM [38]": "Prop. ResNet50 + Capsule",
          "2015\n2017\n2018\n2018\n2018\n2018\n2018\n2019\n2019\n2020\n2020\n2020": "—",
          "88.00%\n32.00%\n24.67%\n57.33%\n31.33%\n40.67%\n20.67%\n06.00%\n22.67%\n12.00%\n91.33%\n81.33%\n49.33%\n81.67%\n72.67%\n90.67%\n82.67%\n50.67%\n81.33%\n74.00%\n40.67%\n08.00%\n2.67%\n22.00%\n17.33%\n89.33%\n40.00%\n24.67%\n63.33%\n40.00%\n92.00%\n84.67%\n55.67%\n84.33%\n74.00%\n89.33%\n71.33%\n43.33%\n72.67%\n60.67%\n95.33%\n77.33%\n47.33%\n88.00%\n72.00%\n89.33%\n71.33%\n46.66%\n78.00%\n62.67%\n88.67%\n73.33%\n42.00%\n76.00%\n60.00%\n91.67%\n84.67%\n53.33%\n85.33%\n74.67%": "93.33%\n84.00%\n54.67%\n86.67%\n76.00%",
          "50.35%\n20.81%\n75.26%\n75.87%\n18.18%\n51.47%\n78.13%\n67.46%\n76.00%\n69.59%\n68.00%\n77.93%": "78.93%"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "LBP [62]\nLFLBP [17]": "HOG [63]\nHOG+HDG [18]",
          "06.24%\n07.81%": "20.65%\n23.85%",
          "10.95%\n16.26%": "21.96%\n24.92%",
          "19.11%\n20.81%": "17.73%\n18.17%",
          "10.27%\n12.52%": "19.06%\n21.09%"
        },
        {
          "LBP [62]\nLFLBP [17]": "VGG-16 [36]\nVGG-16+LSTM [20]",
          "06.24%\n07.81%": "72.27%\n85.80%",
          "10.95%\n16.26%": "73.58%\n85.45%",
          "19.11%\n20.81%": "46.35%\n70.25%",
          "10.27%\n12.52%": "66.09%\n75.17%"
        },
        {
          "LBP [62]\nLFLBP [17]": "ResNet-50 [53]\nProp. ResNet50+Caps",
          "06.24%\n07.81%": "84.17%\n87.83%",
          "10.95%\n16.26%": "81.84%\n89.10%",
          "19.11%\n20.81%": "77.86%\n86.16%",
          "10.27%\n12.52%": "81.81%\n88.25%"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "VGG-Face [36]\nSE-ResNet-50 [53]\nResNet-50 [53]\nArcFace [61]\nLFLBP [17]\nLFHG [18]\nVGG-D3 [19]\nVGG + Conv-LSTM [20]\nVGG + ST-LSTM [37]\nVGG + SLF-LSTM [21]\nVGG + GLF-LSTM [21]\nVGG + DS-LSTM [38]": "Prop. ResNet50 + Capsule",
          "2D\n2D\n2D\n2D\nLF\nLF\nLF\nLF\nLF\nLF\nLF\nLF": "LF",
          "0.045\n0.035\n4,096\n0.032\n0.027\n2,048\n0.031\n0.027\n2,048\n0.072\n0.023\n512\n0.466\n0.276\n65,536\n0.667\n0.577\n16,200\n397.657\n397.643\n16,384\n0.808\n0.513\n7,680\n1.183\n0.497\n3,840\n1.003\n0.497\n3,840\n1.002\n0.497\n3,840\n0.979\n0.497\n3,840": "1.971\n0.488\n320"
        }
      ],
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "Lflbp"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "",
      "authors": [
        "Lfhg"
      ],
      "venue": ""
    },
    {
      "citation_id": "3",
      "title": "Cross-dataset protocol assessment: Face recognition rank-1 results using LFFW for training and LFFC for testing. Solution Year Neutral Exp",
      "authors": [
        "Viii"
      ],
      "venue": "Action Pose Illum. Occlus. Average VGG-Face"
    },
    {
      "citation_id": "4",
      "title": "",
      "authors": [
        "Lflbp"
      ],
      "venue": ""
    },
    {
      "citation_id": "5",
      "title": "",
      "authors": [
        "Lfhg"
      ],
      "venue": ""
    },
    {
      "citation_id": "6",
      "title": "",
      "authors": [
        "Arcface"
      ],
      "venue": ""
    },
    {
      "citation_id": "7",
      "title": "An introduction to biometric recognition",
      "authors": [
        "A Jain",
        "A Ross"
      ],
      "year": "2004",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "8",
      "title": "Face recognition: A novel multi-level taxonomy based survey",
      "authors": [
        "A Sepas-Moghaddam",
        "F Pereira",
        "P Correia"
      ],
      "year": "2020",
      "venue": "IET Biometrics"
    },
    {
      "citation_id": "9",
      "title": "Automatic analysis of facial affect: A survey of registration, representation, and recognition",
      "authors": [
        "E Sariyanidi",
        "H Gunes",
        "A Cavallaro"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "10",
      "title": "Identification of human faces",
      "authors": [
        "A Goldstein",
        "L Harmon",
        "A Lesk"
      ],
      "year": "1971",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "11",
      "title": "Deep face recognition: A survey",
      "authors": [
        "M Wang",
        "W Deng"
      ],
      "year": "2019",
      "venue": "Deep face recognition: A survey"
    },
    {
      "citation_id": "12",
      "title": "When face recognition meets with deep learning: An evaluation of convolutional neural networks for face recognition",
      "authors": [
        "G Hu",
        "Y Yang",
        "D Yi",
        "J Kittler",
        "W Christmas",
        "S Li",
        "T Hospedales"
      ],
      "year": "2015",
      "venue": "International Conference on Computer Vision Workshop"
    },
    {
      "citation_id": "13",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2018",
      "venue": "Deep facial expression recognition: A survey",
      "arxiv": "arXiv:1804.08348"
    },
    {
      "citation_id": "14",
      "title": "Unconstrained face recognition: Identifying a person of interest from a media collection",
      "authors": [
        "L Best-Rowden",
        "H Han",
        "C Otto",
        "B Klare",
        "A Jain"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Information Forensics and Security"
    },
    {
      "citation_id": "15",
      "title": "Survey on RGB, 3D, thermal, and multimodal approaches for facial expression recognition: History, trends, and affect-related applications",
      "authors": [
        "C Corneanu",
        "M Simon",
        "J Cohn",
        "S Guerrero"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "16",
      "title": "Light field photography with a hand-held plenoptic camera",
      "authors": [
        "R Ng",
        "M Levoy",
        "M Bradif",
        "G Duval",
        "M Horowitz",
        "P Hanrahan"
      ],
      "year": "2005",
      "venue": "Light field photography with a hand-held plenoptic camera"
    },
    {
      "citation_id": "17",
      "title": "Light field rendering",
      "authors": [
        "M Levoy",
        "P Hanrahan"
      ],
      "year": "1996",
      "venue": "Annual Conference on Computer Graphics and Interactive Techniques"
    },
    {
      "citation_id": "18",
      "title": "Light fields for face analysis",
      "authors": [
        "C Galdi",
        "V Chiesa",
        "C Busch",
        "P Correia",
        "J.-L Dugelay",
        "C Guillemot"
      ],
      "year": "2019",
      "venue": "Sensors"
    },
    {
      "citation_id": "19",
      "title": "Face recognition with image sets using locally grassmannian discriminant analysis",
      "authors": [
        "H Hu"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "20",
      "title": "A new perspective: Face recognition with light-field camera",
      "authors": [
        "R Raghavendra",
        "B Yang",
        "K Raja",
        "C Busch"
      ],
      "year": "2013",
      "venue": "International Conference on Biometrics"
    },
    {
      "citation_id": "21",
      "title": "Multi-face recognition at a distance using light-field camera",
      "authors": [
        "R Raghavendra",
        "K Raja",
        "B Yang",
        "C Busch"
      ],
      "year": "2013",
      "venue": "International Conference on Intelligent Information Hiding and Multimedia Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "Exploring the usefulness of light field cameras for biometrics: An empirical study on face and iris recognition",
      "authors": [
        "R Raghavendra",
        "K Raja",
        "C Busch"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Information Forensics and Security"
    },
    {
      "citation_id": "23",
      "title": "Light field local binary patterns description for face recognition",
      "authors": [
        "A Sepas-Moghaddam",
        "P Correia",
        "F Pereira"
      ],
      "year": "2017",
      "venue": "International Conference on Image Processing"
    },
    {
      "citation_id": "24",
      "title": "Ear recognition in a light field imaging framework: A new perspective",
      "authors": [
        "A Sepas-Moghaddam",
        "F Pereira",
        "P Correia"
      ],
      "year": "2018",
      "venue": "Biometrics"
    },
    {
      "citation_id": "25",
      "title": "Light field based face recognition via a fused deep representation",
      "authors": [
        "A Sepas-Moghaddam",
        "P Correia",
        "K Nasrollahi",
        "T Moeslund",
        "F Pereira"
      ],
      "year": "2018",
      "venue": "International Workshop on Machine Learning for Signal Processing"
    },
    {
      "citation_id": "26",
      "title": "A double-deep spatio-angular learning framework for light field based face recognition",
      "authors": [
        "A Sepas-Moghaddam",
        "P Correia",
        "K Nasrolahi",
        "T Moeslund",
        "F Pereira"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "27",
      "title": "Long short-term memory with gate and state level fusion for light fieldbased face recognition",
      "authors": [
        "A Sepas-Moghaddam",
        "A Etemad",
        "F Pereira",
        "P Correia"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Information Forensics and Security"
    },
    {
      "citation_id": "28",
      "title": "A deep framework for facial emotion recognition using light field images",
      "authors": [
        "A Sepas-Moghaddam",
        "A Etemad",
        "P Correia",
        "F Pereira"
      ],
      "year": "2019",
      "venue": "International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "29",
      "title": "Facial emotion recognition using light field images with deep attention-based bidirectional LSTM",
      "authors": [
        "A Sepas-Moghaddam",
        "A Etemad",
        "P Correia",
        "F Pereira"
      ],
      "year": "2020",
      "venue": "International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "30",
      "title": "Dynamic routing between capsules",
      "authors": [
        "S Sabour",
        "N Frosst",
        "G Hinton"
      ],
      "year": "2017",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "31",
      "title": "GF-CapsNet: Using Gabor jet and capsule networks for facial age, gender, and expression recognition",
      "authors": [
        "S Hosseini",
        "N Cho"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Automatic Face Gesture Recognition"
    },
    {
      "citation_id": "32",
      "title": "FACSCaps: Poseindependent facial action coding with capsules",
      "authors": [
        "I Ertugrul",
        "L Jeni",
        "J Cohn"
      ],
      "year": "2018",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "33",
      "title": "Epreuves reversibles. photographies integrales",
      "authors": [
        "G Lippmann"
      ],
      "year": "1908",
      "venue": "Comptes Rendus de l'Academie des Sciences"
    },
    {
      "citation_id": "34",
      "title": "The light field",
      "authors": [
        "A Gershun"
      ],
      "year": "1939",
      "venue": "Journal of Mathematics and Physics"
    },
    {
      "citation_id": "35",
      "title": "The plenoptic function and the elements of early vision",
      "authors": [
        "E Adelson",
        "J Bergen"
      ],
      "year": "1991",
      "venue": "Computation Models of Visual Processing"
    },
    {
      "citation_id": "36",
      "title": "The lumigraph",
      "authors": [
        "S Gortler",
        "R Grzeszczuk",
        "R Szeliski",
        "M Cohen"
      ],
      "year": "1996",
      "venue": "Annual Conference on Computer Graphics and Interactive Techniques"
    },
    {
      "citation_id": "37",
      "title": "Plenoptic signal processing for robust vision in field robotics",
      "authors": [
        "D Dansereau"
      ],
      "year": "2014",
      "venue": "Mechatronic Engineering, Queensland University of Technology"
    },
    {
      "citation_id": "38",
      "title": "Efficient plenoptic imaging representation: Why do we need it",
      "authors": [
        "F Pereira",
        "E Silva"
      ],
      "year": "2016",
      "venue": "International Conference on Multimedia and Expo"
    },
    {
      "citation_id": "39",
      "title": "Comparative evaluation of super-resolution techniques for multi-face recognition using light-field camera",
      "authors": [
        "R Raghavendra",
        "K Raja",
        "B Yang",
        "C Busch"
      ],
      "year": "2013",
      "venue": "International Conference on Digital Signal Processing"
    },
    {
      "citation_id": "40",
      "title": "Facial expression recognition using depth map estimation of light field camera",
      "authors": [
        "T Shen",
        "H Fu",
        "J Chen"
      ],
      "year": "2016",
      "venue": "IEEE International Conference on Signal Processing, Communications and Computing"
    },
    {
      "citation_id": "41",
      "title": "Deep face recognition",
      "authors": [
        "O Parkhi",
        "A Vedaldi",
        "A Zisserman"
      ],
      "year": "2015",
      "venue": "British Machine Vision Conference"
    },
    {
      "citation_id": "42",
      "title": "Skeletonbased action recognition using spatio-temporal LSTM network with trust gates",
      "authors": [
        "J Liu",
        "A Shahroudy",
        "D Xu",
        "A Kot",
        "G Wang"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "43",
      "title": "Speech emotion recognition with dual-sequence LSTM architecture",
      "authors": [
        "J Wang",
        "M Xue",
        "R Culhane",
        "E Diao",
        "J Ding",
        "V Tarokh"
      ],
      "year": "2020",
      "venue": "International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "44",
      "title": "Face recognition homepage",
      "authors": [
        "M Grgic",
        "C Delac"
      ],
      "year": "2019",
      "venue": "Face recognition homepage"
    },
    {
      "citation_id": "45",
      "title": "The AR face database",
      "authors": [
        "A Martinez",
        "R Benavente"
      ],
      "year": "1998",
      "venue": "The AR face database"
    },
    {
      "citation_id": "46",
      "title": "The FERET evaluation methodology for face-recognition algorithms",
      "authors": [
        "J Phillips",
        "H Moon",
        "S Rizvi",
        "P Rauss"
      ],
      "year": "2000",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "47",
      "title": "Labeled faces in the wild: A database for studying face recognition in unconstrained environments",
      "authors": [
        "G Huang",
        "M Ramesh",
        "T Berg",
        "E Learned-Miller"
      ],
      "year": "2007",
      "venue": "Labeled faces in the wild: A database for studying face recognition in unconstrained environments"
    },
    {
      "citation_id": "48",
      "title": "Multi-PIE",
      "authors": [
        "R Gross",
        "I Matthews",
        "J Cohn",
        "T Kanade",
        "S Baker"
      ],
      "year": "2010",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "49",
      "title": "Bi-modal person recognition on a mobile phone: Using mobile phone data",
      "authors": [
        "C Mccool"
      ],
      "year": "2012",
      "venue": "IEEE International Conference on Multimedia and Expo Workshops"
    },
    {
      "citation_id": "50",
      "title": "Face recognition in unconstrained videos with matched background similarity",
      "authors": [
        "L Wolf",
        "T Hassner",
        "I Maoz"
      ],
      "year": "2011",
      "venue": "International Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "51",
      "title": "SCface -surveillance cameras face database",
      "authors": [
        "M Grgic",
        "K Delac",
        "S Grgic"
      ],
      "year": "2011",
      "venue": "Multimed Tools and Application"
    },
    {
      "citation_id": "52",
      "title": "BP4D-Spontaneous: a high-resolution spontaneous 3D dynamic facial",
      "authors": [
        "X Zhang",
        "L Yin",
        "F Cohn"
      ],
      "year": "2014",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "53",
      "title": "KinectFaceDB: A Kinect database",
      "authors": [
        "R Min",
        "N Kose",
        "J Dugelay"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics: Systems"
    },
    {
      "citation_id": "54",
      "title": "Beyond frontal faces: Improving person recognition using multiple cues",
      "authors": [
        "N Zhang",
        "M Paluri",
        "Y Taigman",
        "R Fergus",
        "L Bourdev"
      ],
      "year": "2015",
      "venue": "Beyond frontal faces: Improving person recognition using multiple cues"
    },
    {
      "citation_id": "55",
      "title": "International Workshop on Biometrics and Forensics",
      "authors": [
        "A Sepas-Moghaddam",
        "V Chiesa",
        "P Correia",
        "F Pereira",
        "J Dugelay"
      ],
      "year": "2017",
      "venue": "International Workshop on Biometrics and Forensics"
    },
    {
      "citation_id": "56",
      "title": "Level playing field for million scale face recognition",
      "authors": [
        "A Nech",
        "I Kemelmacher-Shlizerman"
      ],
      "year": "2017",
      "venue": "International Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "57",
      "title": "Disguised faces in the wild",
      "authors": [
        "V Kushwaha",
        "M Singh",
        "R Singh",
        "M Vatsa",
        "N Ratha",
        "R Chellappa"
      ],
      "year": "2018",
      "venue": "International Conference on Computer Vision and Pattern Recognition Workshop"
    },
    {
      "citation_id": "58",
      "title": "VGGFace2: A dataset for recognising faces across pose and age",
      "authors": [
        "Q Cao",
        "L Shen",
        "W Xie",
        "M Parkhi",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "59",
      "title": "Capsule attention for multimodal EEG and EOG spatiotemporal representation learning with application to driver vigilance estimation",
      "authors": [
        "G Zhang",
        "A Etemad"
      ],
      "year": "2020",
      "venue": "Capsule attention for multimodal EEG and EOG spatiotemporal representation learning with application to driver vigilance estimation",
      "arxiv": "arXiv:1912.07812"
    },
    {
      "citation_id": "60",
      "title": "Light field toolbox v",
      "authors": [
        "D Dansereau"
      ],
      "year": "2019",
      "venue": "Light field toolbox v"
    },
    {
      "citation_id": "61",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "International conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "62",
      "title": "Tensorflow: A system for large-scale machine learning",
      "authors": [
        "M Abadi",
        "P Barham",
        "J Chen"
      ],
      "year": "2016",
      "venue": "International Symposium on Operating Systems Design and Implementation"
    },
    {
      "citation_id": "63",
      "title": "You only look once: Unified, real-time object detection",
      "authors": [
        "J Redmon",
        "S Divvala",
        "R Girshick",
        "A Farhadi"
      ],
      "year": "2016",
      "venue": "International Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "64",
      "title": "Visualizing data using t-SNE",
      "authors": [
        "L Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "65",
      "title": "Arcface: Additive angular margin loss for deep face recognition",
      "authors": [
        "J Deng",
        "J Guo",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "66",
      "title": "Face description with local binary patterns: Application to face recognition",
      "authors": [
        "T Ahonen",
        "A Hadid",
        "M Pietikainen"
      ],
      "year": "2006",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "67",
      "title": "Face recognition using histograms of oriented gradients",
      "authors": [
        "O Déniz",
        "G Bueno",
        "J Salido",
        "F De"
      ],
      "year": "2011",
      "venue": "Pattern Recognition Letters"
    }
  ]
}