{
  "paper_id": "2202.00446v2",
  "title": "Multi-Order Networks For Action Unit Detection",
  "published": "2022-02-01T14:58:21Z",
  "authors": [
    "Gauthier Tallec",
    "Arnaud Dapogny",
    "Kevin Bailly"
  ],
  "keywords": [
    "Deep learning",
    "multi-task learning",
    "affective computing",
    "face analysis"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Action Units (AU) are muscular activations used to describe facial expressions. Therefore accurate AU recognition unlocks unbiaised face representation which can improve face-based affective computing applications. From a learning standpoint AU detection is a multi-task problem with strong inter-task dependencies. To solve such problem, most approaches either rely on weight sharing, or add explicit dependency modelling by decomposing the joint task distribution using Bayes chain rule. If the latter strategy yields comprehensive inter-task relationships modelling, it requires imposing an arbitrary order into an unordered task set. Crucially, this ordering choice has been identified as a source of performance variations. In this paper, we present Multi-Order Network (MONET), a multi-task method with joint task order optimization. MONET uses a differentiable order selection to jointly learn task-wise modules with their optimal chaining order. Furthermore, we introduce warmup and order dropout to enhance order selection by encouraging order exploration. Experimentally, we first demonstrate MONET capacity to retrieve the optimal order in a toy environment. Second, we validate MONET architecture by showing that MONET outperforms existing multi-task baselines on multiple attribute detection problems chosen for their wide range of dependency settings. More importantly, we demonstrate that MONET significantly extends state-of-the-art performance in AU detection.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Facial expressions are the main channel human uses to convey nonverbal emotional information. Consequently, efficient facial expression detection is key to better face-based computational emotion representation and human-machine interaction. However, such detection performance is limited by the person-specific aspect of facial expressions. To remove person-specific biases, the Facial Action Coding System (FACS) anatomically describe face expressions using a set of unitary muscular activations called Action Units (AU)  [1] .\n\nUsing FACS, facial expression detection is equivalent to the joint detection of each considered AUs. From a machine learning point of view, it can be framed as a multi-task problem in which each task corresponds to the prediction of a single AU.\n\nThe most widely adopted strategy  [2] ,  [3]  for solving multi-task problems is to use a common encoder and predicts the different tasks using separate regressors in parallel (Figure  1-(a) ). Yet, this strategy fails to model inter-task a priori dependencies. This is all the more a problem as AU are known to display such strong intertask dependencies. For example, One cannot simultaneously raise and frown his/her eyebrows, so that AU1 and AU4 cannot co-occur.\n\nTo better model inter-task dependencies, several works  [4] ,  [5]  leveraged recurrent architectures that predict tasks in a sequential fashion (Figure  1-(b) ). However, contrary to standard sequence processing, the set of tasks to predict has no natural order a priori. Therefore, making task prediction sequential requires enforcing an arbitrary order into the set of tasks. Most importantly, recurrent networks performance have been proven sensitive to the order in which elements are predicted  [6] . Consequently, in the frame of multitask learning using recurrent networks where predicted elements are tasks, the task prediction order matters.\n\nIn this paper, we follow this sequential order optimization paradigm and introduce Multi-Order Network (MONET). MONET leverages permutation matrices to represent task orders. More precisely, it navigates between orders by learning a convex combination of permutation matrices (a soft order). In that extent MONET explores the convex hull of permutation matrices (Birkhoff's polytope) to smoothly select the best order. Figure  1-(c ) illustrates MONET architecture for a 3-tasks problem. To summarize, the contributions of this work are the following: (b) MRNN imposes an arbitrary task order. (c) MONET predicts the set of tasks in different orders using separate recurrent cells, one for each task. Those predictions are normalized to a common order (rounded arrows) so that they can be weighted by a convex combination to output the final prediction. By jointly learning the coefficients associated with each order along with the task-specific cells, MONET smoothly selects the best order of prediction and achieves superior performance.\n\nretrieve the correct order on a toy dataset b) Showing that MONET outperforms several multi-task approaches on a wide range of attribute detection problems with diverse levels of inter-task dependency . Finally, we conclude by demonstrating that MONET extends state-of-the-art performance in action unit detection.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "2.1 Action Unit Detection.\n\nFrom a learning perspective, the action unit detection comes with three main specifities. First, action units are local events meaning that the face zones in which they occur are relatively small and constrained.\n\nFor instance AU1 (inner brow raiser) and AU2 (outer brow raiser) can only occur on the forehead. To tackle that challenge, prior work  [7]  relaxed convolutional layers weight sharing constraint by introducing region-wise filters. The incentive is that using different filters on each region will result in a refined skin texture representation. However, the performance of such method is conditioned upon face alignment i.e upon weither or not specific face parts are always located in the same rectangular region. To circumvent this limitation several works  [8] ,  [9] ,  [10]  used facial key-points along with predefined attention maps to guide the extraction of AU-related features. More precisely, in  [8] ,\n\npredefined key-points were used to crop AU related zones while the works in  [8]  and  [11]  used attention-like mechanisms based on fixed and jointly learned facial landmark predictors respectively. Second, because of both social and physiological constraints, action units display strong dependencies between each other (eg: AU1 and AU2 often co-occurs while AU1 and AU4 are incompatible). From an intuitive point of view the exploitation of those dependencies is key to unlock better performance. Indeed, it could enable the use of easy-to-predict AU as a proxy to estimate the harder ones. For example, predicting first AU4 (brow lowerer) could help predict more subtle movements of the eyes such as AU1 or AU6 (cheek raiser). Several works focus on explicitly modeling those dependencies. In particular, backpropagation through a probabilistic graphical model (PGM) was adopted in  [12] , an hybrid message passing strategy was used in  [13] , and a Graph Neural Network (GNN) was employed in  [14] . Others leveraged the local nature of AU to assume that label dependencies imply dependencies between local face zones. Attempts at capturing such spatial dependencies include attention map learning  [11] , LSTM-based spatial pooling  [15]  and more recently transformerlike architectures  [16] .\n\nFinally, the Action Units annotation work is time consuming and requires specifically trained experts. Hence the lack of a large scale AU annotated database. A way to bypass this issue is to help the AU detection learning with categorical facial expression recognition (FER) labels (happiness, sadness, surprise, fear, disgust) which are more easily obtained. Following this idea, multiple works  [17] ,  [18]  leveraged EMFACS  [19]  a priori dependency to learn to predict AU labels from a pretrained network facial expression predictions. Concurrently, the AU/FER joint learning have been tackled from a multi-task learning perspective: In  [20] , a cross-stitch-like architecture  [21]  is used to learn to share features between two networks trained on AU and FER datasets respectively. Similarly, in  [22] , a single network is trained on both AU and FER datasets using an adaptative loss weighting. However the benefits of such approaches are limitated by the absence of a dataset with joint AU and FER annotations which prevent AU/FER dependency learning.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multi-Task Learning",
      "text": "Deep multi-task learning methods is a subset of deep learning methods that aim at exploiting similarities between several tasks in order to improve individual task performance. To reach that goal, the most widely adopted method relies on implicit modelling of task dependencies using weight sharing. In a nutshell, it consists in splitting the model into parts that are shared across tasks and parts that are task specific.\n\nSeminal work  [2] ,  [3] ,  [23] ,  [24] , adopted this weight sharing strategy by making use of a common encoder along with task specific regressors. The intuition behind this is that forcing the same learned features to predict several related task should encourage the encoder to produce more general representations and consequently improve generalization performances. However, one weakness of this method comes with deciding how far features should be shared. Indeed, it intuitively depends on task relatedness (the more related the tasks the further the sharing) which can be hard to determine. To tackle this issue, numerous approaches  [21] ,  [25] ,  [26] ,  [27]  used adaptative architectures to jointly learn which layers should be shared between tasks, as well as the task prediction itself. This philosophy has also been used in modular approaches which consist in learning a set of trainable modules along with how they should be combined for each tasks. For instance, soft layer ordering  [28]  consists in learn the best module combination for each task in a fully differentiable way. In the same vein, a select or skip policy  [29]  was used to determine which module should be used for each task.\n\nWeight sharing may help finding features that are useful for all tasks and therefore implicitly models input-related task conditional dependencies, though, it doesn't capture inter-task relationships that do not depend on input (e.g the prior that detection of a beard implies high probability that the subject also has a mustache). In order to model those dependencies, several approaches  [4] ,  [25] ,  [30]  leveraged recurrent neural networks to decompose the task joint distribution into a product of conditional distributions using Bayes chain rule. Most importantly, the work in  [6]  showed that order matters, meaning that the order in which the chain rule is unrolled impacts the final joint estimate modelization performance. In the light of this observation it proposes a two-steps method: The first step consists in an exploration phase in which the performance of several orders are tested. At the end of this phase, a single order is fixed once and for all based on the exploration phase performance and predictions are computed using this order.\n\nOur work lies in the continuity of the order optimization paradigm proposed in  [6]  which aim at improving current Bayes chain rule based joint distribution estimation. However, we stand out from it by drawing inspiration in  [28]  to: (a) Propose a soft order selection mechanism that navigates through Birkhoff's polytope, and (b) Propose a new task-wise modular recurrent architectural design. More precisely, MONET smooth selection contrasts with the once and for all choice of order in  [6]  by keeping on learning several orders during all the training phase. We take full advantage of this by adding warm up and order dropout mechanisms that encourage modules to display good predictive performances for several orders of prediction.\n\nBy optimizing the task order, MONET takes advantage of situations where order matters. Furthermore, we believe that learning more than one task order all along the training improves MONET generalization capacity and thus predictive performance in more general multi-task settings where order do not necessarily matter.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "In the rest of the paper, scalars are denoted using regular characters, vectors are in bold. For vector v the t-th coordinate is denoted by v (t) , and the vector of its t -1 first coordinates is denoted by v (<t) .\n\ni=1 is a training dataset composed of input vectors x and labels y of size T such that ∀t ∈ [1, T ], y (t) ∈ {0, 1}.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Multi-Task Baselines",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Vanilla Multi-Task Networks (Vmn)",
      "text": "The most widely adopted deep multi-task approach is to model task dependencies using weight sharing only  [31] ,  [32] , i.e to assume labels conditional independance given the input image. Template networks for this approach are composed of a shared encoder f W parametrized by matrix W along with a specific prediction head g W (t) , parametrized by W (t) for each task t ∈ [1, T ]. We refer to instances of such template as Vanilla Multi-task Networks (VMN). Given input x, the prediction for task t is the output of the t-th prediction head:\n\nwhere S denotes the sigmoid function. Task t distribution is then estimated as follows:\n\nwhere θ = {W, (W (t) )t}, and BCE stands for binary cross entropy.\n\nTraining is done by minimizing the following maximum likelihoodbased loss:\n\nIf VMN are the most classic multitask learning approach, their modelization assumption do not take into account inter-task relationships that are independent of the input image. Those relationships include numerous human knowledge-based priors such as the statistical dependency between the presence of a beard and the presence of a mustache, for instance. Naturally, predictive performance of a deep network may benefit from exploiting these priors.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multi-Task Recurrent Neural Networks (Mrnn)",
      "text": "Different from VMN, Multi-task Recurrent Neural Networks (MRNN) model inter-task dependencies through both weight sharing and joint conditional distribution modelization. For that purpose, the joint conditional distribution of labels is decomposed using Bayes chain rule:\n\nThe MRNN approach consists in encoding the input vectors with network f W and to feed the output representation as the initial state h (0) of a recurrent computation process driven by cell g V with parameters V. At step t, this process takes one hot encoded ground truth for timestep t-1 task along with hidden state h (t-1) and outputs prediction p (t) and next timestep hidden state h (t) . In a nutshell:\n\n(\n\nwhere ỹ(t-1) = 2(y (t-1) -1) and (e1, . . . , eT ) denotes the canonical basis vectors of R T . Prediction p (t) is then:\n\nTask t conditional distribution w.r.t previous tasks is estimated as:\n\nwhere θ = {V, W}, and training is done by minimizing the following loss:\n\nFor task t and input x, inference consists in estimating p θ * (y (t) | x), where θ * = {W * , V * } denotes MRNN parameters at the end of the training phase. To compute this estimation we leverage Monte-Carlo sampling in the following way:\n\nwhere (ŷ l ) for sample l ∈ [1, L] are computed sequentially, as summarized in algorithm 1.\n\nAlgorithm 1 MRNN sampling for inference Require: Input vector x 1: for l = 1 to L do 2:\n\n3:\n\n5:\n\n6:\n\nend for 8: end for 9: return L output trajectories (ŷ l )\n\nThe performances of MRNN are directly linked to the conditional joint distribution estimate modelization performance that itself depends on the modelization performance of each element in the chain rule product. In  [6] , it is established that order matters, meaning that unrolling chain rule in a different task chaining order leads to different modelization performance. This comes from the fact that tasks may be easier to learn in a given order. By relying on a single arbitrary chain rule decomposition order, MRNN misses the opportunity to better exploit the inter-task relationships. By contrast, our method extends MRNN by parallely estimating the joint conditional distribution using different orders and smoothly selecting the best estimate.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Multi-Order Network (Monet)",
      "text": "In this section, we present MONET (Multi-Order Network) for joint task order and prediction modelling in multi-task learning. MONET is composed of an order selector that navigates through Birkhoff's polytope to learn a suitable task order in a differentiable way. As illustrated on Figure  2 , in inference mode, sampling from this order selector allows MONET to combine its T recurrent cells, one for each task, in an order that has been learned at train time.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Jointly Learning Task Order And Prediction",
      "text": "Let's define a soft order of T tasks as any real doubly stochastic matrix Ω of size T × T , i.e. a matrix such that :\n\nIntuitively, in such case, the coefficient Ωi,j associated to each row i and column j in Ω corresponds to the probability to address task j at step i. Therefore, in the extreme situation where all columns are one-hot vectors, a soft order matrix becomes a \"hard\" order (i.e., a permutation matrix) that models a deterministic task order. More precisely, if σ denotes a permutation, its associated order matrix is :\n\nThe Birkhoff-Von Neumann's theorem states that the class of doubly stochastic matrices (also called Birkhoff's polytope) is the convex hull of all the order matrices, i.e the set of all convex combinations of order matrices. In other words, any soft order matrix Ω can be decomposed as a convex combination of M order matrices. Formally, there exists M a finite number, π1, . . . , πM ∈ R, and Mσ 1 , . . . , Mσ M , M order matrices such that:\n\nTherefore, each soft order of T tasks can be parametrized by the coefficients π1, . . . πM associated to each possible order matrices, with M = T !. The reciprocal is also true: given M order matrices, with M ≤ T !, each convex combination π1, . . . , πM also defines a soft order.\n\nWe use this result to provide a differentiable parametrization of soft orders, that allows us to jointly learn both the task order and prediction by smoothly navigating Birkhoff's polytope. To do so, we first generate M ≤ T ! random permutations, denoted as (σm) m∈  [1,M ]  . For each permutation, σ, we estimate a joint distribution convex p σ by unrolling the Chain Rule in order σ:\n\nFinally, we compute the final joint distribution as a convex combination of each permutation-based joint distribution:\n\nwhere π1, . . . , πM are the order selector coefficients that position the learned soft order Ω = M m=1 πmMσ m inside Birkhoff's polytope.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Monet Architecture",
      "text": "MONET is composed of T recurrent cells (g W (t) ) 1≤t≤T , each being trained to predict the same task across all orders: For an order σ, task σ(t) is predicted using recurrent cell g W σ(t) and is conditioned on the results of all preceding tasks in the order σ (i.e tasks σ(1), . . . , σ(t-1)). The rationale behind this comes from traditionnal RNN usage, where each cell predicts a single task in different contexts. (e.g., RNN-based sentence translation is a repetition of word translations conditioned by the context of neighbouring words). Here, each task-associated predictor learns to predict the corresponding task in different contexts corresponding to the different orders.\n\nConcretely, for order σm, our computational graph unfolds as follows:\n\nwhere\n\nm at timestep t is computed as :\n\nand is used as follows:\n\nParameters θ = (W, (W (t) ) 1≤t≤T ) as well as the order selector π, defined as a softmax layer over logits u, are jointly learned through minimizing the following loss:",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Order Selection Strategy",
      "text": "In this section, we provide theoretical intuitions about MONET order selection. In particular, we highlight that the raw MONET order selection mechanism tends to allocate weights on the permutation with the lowest loss. For that purpose, we denote by L σ , the loss associated with order σ:\n\nWe also introduce element-wise losses for both order based and global losses as :\n\nMONET order selection is based on the variation of π, which itself is underpinned by gradient updates on order logits u. Consequently, Fig.  3 . Warm up learning rate schedule. αp represents the learning rate in the predicting part of the network while αos is the order selector learning rate. The order selector stay frozen for the n first epochs so that the predictor estimates the performance of each orders. At the end of this exploration phase, the order selector coefficients are released and uses those estimations to allocate weight to the orders with the best performance.\n\nwe compute the loss gradient element-wise. More precisely\n\nand :\n\nThe latter equivalence implies that, in the case of element by element loss minimization, coefficient πm increases if the loss associated to its order is inferior to the global loss. In a more realistic scenario, optimization is performed by batch of size B. The gradient of loss L on a batch is then:\n\nIt directly follows that:\n\nConsequently, orders whose losses are the lowest on a batch get positive updates on their order selector coefficients. As a consequence, using raw MONET order selection results in selecting the order whose joint estimation best fits the train set, i.e the order that overfits the most. This observation comes with two different issues:",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Warmup : Stepping Away From Bad Starting Points",
      "text": "When training begins the order losses mostly depend on network initialisation, rather than on their associated order respective performance. Hence, order selection in the first epochs is likely to lead to quasi-random solutions. This is all the more problematic as weight allocation is prone to snowballing i.e to keep allocating more and more weight to a previously selected order. If, for a given epoch, the loss L σ i for order σi is lower than the loss term corresponding to other orders, the proposed method will assign more weight to this order by positively updating the corresponding πi. As a consequence, the task modules will specialize in predicting the tasks using order σi. This will reinforce the advantage of order loss σi. This is a problem since, in such a case, the order selection mechanism does never have a chance to really try and compare different orders. To circumvent this, we draw inspiration from the exploration phase in  [6]  and freeze order logits u for the first n epochs (see figure  3 ). This provides the network with an opportunity to explore all orders and to get a better estimate of each order performance. In turn, it improves the quality of the selected orders.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Order Dropout: Avoiding Order Selection Snowball",
      "text": "Warm up may help the order selector to choose among the best performing orders. However it doesn't prevent order selection from snowballing on the first order it selects. This snowball effect yields increased risks of getting stuck in a suboptimal order and neglects the benefits of training several orders parallely. To avoid this pitfall, we propose an order dropout strategy which consists in training each example on a random subset of k (Instead of M ) orders by zeroing-out (M -k) order selector coefficients:\n\nwhere t i m is a randomly sampled binary mask with k ones and (M -k) zeros. For inference, we multiply each exp(um) by its probability p(k, M ) of presence, as in  [33] :\n\nWith this strategy the order with the lowest loss is not always included in the k trained order and do not get systematically reinforced by the successive gradient updates. Therefore it short-circuits the order selector snowballing behaviour and forces MONET to spread weight allocation, hence encouraging good predictive performance for several orders. We believe that forcing each task module to learn its associated task in different order have a regularizing effect and reduces overfitting leading to better predictions at inference time.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Inference With Monet",
      "text": "Let θ * , π * be MONET parameters at the end of the training phase. At test time, for an input x, R different orders (σr) 1≤r≤R are sampled from a random variable S whose discrete distribution is based on the order selector π * . The input x is then routed into the R networks corresponding to each order (figure  2 ). Each of those networks outputs a prediction for task t, and those predictions are averaged to form the global network prediction. Formally :  (27)  where (ŷ r,l ) for r ∈ [1, R], l ∈ [1, L] are sampled using algorithm 2. for l = 1 to L do 4:\n\nr,l = f W * (x)",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "5:",
      "text": "for t = 1 to T do 6:\n\nr,l = g W σr (t) (h\n\nr,l e σs r (t-1) )\n\n7:\n\nend for 10:\n\nend for 11: end for 12: return LR output trajectories (ŷ r,l )\n\nIn practice, we found out that, when training ends, π * is often close to a one-hot vector. Consequently, the sampling of R orders is likely to result in R times the same order. In that scenario, the final prediction becomes the average of LR trajectories generated using the same prediction order. Also, for the sake of simplicity, we regroup all trajectory numbers in variable L ← LR.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Datasets",
      "text": "Toy Dataset For method empirical validation, we designed a 2dimensional multi-task binary classification toy dataset represented in Figure  4 . For T tasks, it uses the following laws for input and labels:\n\n2i+1 ] (X (1) ),  (28)  where b\n\nvertically split in a recurrent way. This dataset has a natural order in which the tasks are easier to solve: for t ∈ [1, T ], conditioning task t + 1 by the t first tasks results transforms it into an easy-to-solve classification problem with a single linear boundary. Conversely, conditioning task t + 1 by the result of upcoming tasks in coordinate order do not simplify its complexity as a classification problem, it remains a 2 t -1 linear boundary problem . We generate 500, 250 and 250 examples for the train, val and test partitions, respectively. Those sizes are deliberately small to challenge networks modelization performance. CelebA is a widely used database in multi-task learning, composed of ≈ 200k celebrity images annotated with 40 different facial attributes. For performance evaluation, we measure accuracy score using the classic train (≈ 160k images), valid (≈ 20k images) and test (≈ 20k images) partitions for 5 different subsets of 5 attributes each:\n\n• gender: with moustache, beard, lipstick, heavy makeup and sex detection. Those attributes display statistical dependencies. For example a beard often implies a moustache.\n\n• accessory: with earrings, eyeglasses, necklaces and neckties detection.\n\n• beauty: with arched eyebrows, attractiveness , high cheekbones, rosy cheeks, and oval faces detection.\n\n• haircut: with baldness, black, blond, brown and gray hair detection. Those attributes are mutually exclusive. Both accessory and beauty were choosen for their lack of clear a priori on the type of dependencies that bind the tasks together. Those tasks subsets were constructed to assess MONET behaviour in different dependency settings, and show its interest as an overall better multitask approach compared with existing baselines. BP4D is a dataset for facial action unit detection. It is composed of approximately 140k images featuring 41 people (23 female, 18 male) with different ethnicities. Each image is annotated with the presence of 12 AU. For performance evaluation, we follow related work strategy that is to report F1-Score on all 12 AUs using a subject exclusive 3fold cross-validation with publicly available fold repartition from  [9]  and  [10] . DISFA is another dataset for facial action unit detection. It contains 27 videos for ≈ 100k face images. Those images were collected from 27 participants and annotated with 12 AUs. Originally, each AU label is an intensity score ranging from 0 to 5.. In detection, labels with an intensity score higher than 2 are considered positive  [7] . Similarly to BP4D, the performance evaluation protocol consists in measuring the F1-Score for 8 AUs using a subject exclusive 3-fold cross validation.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Implementation Details",
      "text": "In our experiments, we compare MONET with several multi-task baselines, each using a shared encoder and a number of prediction heads. For VMN-Common (VMNC), the prediction head consists in two dense → BN → ReLU applications followed by another dense → sigmoid layer with T outputs. VMN-Separate (VMNS) uses T prediction heads, each with the same structure, except the last layer is of size 1. Finally, MRNN uses a single Gated Recurrent Unit (GRU) cell which sequentially predicts the T tasks. Task order is randomly sampled for each MRNN experiment. Toy Experiments: The shared encoder consists of four dense layers with 64 units and ReLU activation. Prediction heads for both VMNC and VMNS consist in dense layers with 64 units. Both MRNN and MONET employ GRU cells with 64 units and L = 20 orders. All networks are trained by applying 500 epochs with Adam  [34] , batch size 64 with an exponentially decaying base learning rate 5e -4 and β = 0.99. MONET order selector is trained with Adam with learning rate 0.005. Other MONET related parameters (dropout k, warmup n) are determined by hyperparameter tuning on a dedicated validation dataset. Face Attributes: For CelebA we make use of an Inception resnet v1 encoder pretrained on VGGFace2  [35]  along with dense layers with 64 units as prediction heads for VMNC and VMNS. Both MRNN and MONET use GRU cells with 64 units and L = 20 orders. Networks are trained with 30 epochs using AdamW  [36]  with learning rate/weight decay set to 0.0005 and exponential decay (β = 0.96).\n\nFor MONET hyperparameters, we use M = 5! = 120 permutations and order dropout k = 32. Facial Action Unit Detection : Action Units are relatively short events. Therefore video-based datasets such as DISFA and BP4D display a data imbalance problem  [10]  that may act as a barrier to efficient learning. To circumvent this problem, we combine biais Fig.  5 . Performance of MONET trained with a single order σ (M = 1, k = 1) as a fonction of the frobenius distance between σ and the identity order. Accuracy scores are averaged over 10 runs.  initialization  [37]  and per AU loss weighting  [10] . To further adapt to the evaluation protocol that measure F1 score we include a Dice score contribution  [10] ,  [16]  to the final loss. For BP4D, we use an Inceptionv3 backbone pretrained on imagenet on top of which we put a MONET instance with M = 512 and k = 128. The order selection part of MONET is trained using Adam with warmup n = 5 and constant learning rate 5e -3 while the rest of the network uses AdamW optimizer with learning rate/weight decay set to 1e -4 and exponential decay β = 0.99. Batchsize is set to 16, number of epochs is 2 and dice loss coefficient is λ = 0.5.\n\nFor DISFA, we make use of the same encoder as for CelebA and retrain with AdamW learning rate 5e -4 with exponential decay β = 0.96, and batchsize 64. All other parameters stay the same as for BP4D.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Toy Experiments",
      "text": "On order importance in toy setting: Figure  5  shows the performance of MONET trained with a single, imposed order σ on the toy dataset with T = 3, 4, 5. For all considered T , MONET reaches its best performance when σ verifies Mσ -IT 2 F = 0, which only occurs when the imposed order σ is the identity order. Thus, the identity order is the optimal order in the toy experiment. Furthermore, we observe a significant decrease in performance as the imposed order σ moves away (i.e. as expressed by the frobenius norm of the difference) from the identity order. More precisely, the further the chosen σ is from the identity, the lower the accuracy of the network accross the T tasks. Therefore, order matters in the toy settings and the best performing orders are the identity order and the orders that are close to it. Ablation study on MONET individual components: Table  1  displays the performance of MONET with different order selection mechanism versions. It reports mean accuracy averaged on 10 runs along with the number of times MONET selected the identity order (correct order in this setting) over these runs. As mentioned in the previous paragraph, those two quantities depend from one another because selecting the correct order results in higher performance. The worst performing method, that we called Hard Selection is an implementation of  [6]  order selection mechanism. It uses an exploration phase similar to our warm up and then samples a single order using a distribution in which each order has a probability that is proportional to its performance (measured during exploration). Table  1  shows that this order selection mechanism perform poorly. In fact, we believe that such sampling strategy is sensitive to performance measurements noise and is therefore likely to result in a sub-optimal order. By smoothly learning its order selection coefficients all along the training phase, MONET gets a better estimation of each order performance and selects better orders.\n\nRefining the order selection mechanism with warm up helps MONET to find better orders by providing an exploration phase to estimate each order performance before making a choice. However it doesn't prevent the order selection from snowballing, i.e to keep allocating weights to the first selected order. It explains that adding warm up to MONET only provides a tiny boost in performance.\n\nDropout on the other hand, short-circuit the snowball effect by forcing the network to train on randomly selected orders: it allows MONET order selection mechanism to smoothly deviate from any previous choice of order. Indeed, figure  6 , shows that contrary to its standard version (k = 120, M = 120), the dropout version of MONET (k = 100, M = 120) is able to recover the correct order even if it started with a bad guess. Combined warmup and dropout provide MONET with an initial order guess that is likely to be good, and the ability to move away from this guess if needed. Those two ingredients result in better order selection which, in turn, implies better performance. Hyperparameter tuning: Figure  7 , compares the performance of MONET for T = 5 tasks with different settings for k and M . First, it shows that the performance of MONET without order dropout (red plot) are increasing with the number of randomly sampled order M . This is fairly logical. Indeed, the higher M , the more likely it is to get a good order in the set of randomly selected orders, the better the performance are. Moreover, it confirms that MONET with M = 120 and order dropout (green plot) achieve higher accuracy and get closer to the oracle with enforced correct order IT . Comparison with baseline methods and discussion: Lastly, Table  2  shows relative performances of MONET w.r.t multi-task baselines. MONET displays significantly better performances than all other considered methods, significantly narrowing the gap with the oracle performance.\n\nEventually, we demonstrated in a controlled benchmark where an optimal task chaining order is known that (a) MONET was able to consistently retrieve said order, and (b) that thanks to its joint order selection mechanism and task-specific recurrent cell sharing architecture, backed by the proposed order dropout strategy, MONET was able to consistently outperform other multi-task baselines, getting closer to an oracle predictor using the optimal order. We now consider real-world applications with potentially more complex inter-task dependencies and less clear ordering patterns.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Applications To Face Attribute Detection",
      "text": "Table  3  draws a comparison between different multi-task methods for attribute detection on CelebA. On the one hand, there is no clear winner between the two VMN versions: for instance, VMNS performs better on the gender and accessories subsets while VMNC performs better on haircut and misc.. Those performance discrepancies may result in practical difficulties to find an all-around, well performing architecture, as echoed in  [21] . Furthermore, MRNNs gets consistently outperformed by at least one of the VMN methods. In fact, we believe that MRNN recurrent cell sharing across tasks leads to early conflicts between task-associated gradients and prevents it from taking full advantage of its theoretically better inter-task relationship modelling. Additionally, random task order sampling may prevent MRNN from properly learning and hurt its predictive performance. MONET, on the other hand, shows consistently better performances than both VMN as well as MRNN on every subset, due to both its task-wise modular weight sharing strategy and its order selection mechanism that, in turn, allows to correctly model inter-task dependencies. Figure  8  depicts two soft-order matrices extracted at the end of MONET training on the gender subset. First, those two matrices are very similar, showing that MONET order selection mechanism is relatively stable across several networks and order selector initializations. Second, it seems that MONET learns to process easy tasks in priority and uses the result of those easy tasks to condition the prediction on harder ones. For example, it typically learns to predict beard (which is more visible and therefore easier to predict) before predicting mustache and lipstick (which has a very characteristic color) before heavy makeup (which exhibit more variability and is fairly subjective). From an intuitive point of view, such learned order is fairly reasonable. Indeed, predicting easy tasks earlier reduces the chances of propagating prediction mistakes along the processing chain and in turn improves performance.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Applications To Action Unit Detection",
      "text": "AU detection on BP4D:   of 12 action units on BP4D database. Thanks to its order selection mechanism, MONET outperforms all the methods that explicitly models AU label dependencies such as DSIN or HMP-PS. More interestingly, it outperforms methods that use external information such as landmarks (EAC-NET, JAANET, PT-MT-ATsup-CC-E) or textual description of Action Units (SEV-NET). Finally, MONET performs better than PT-MT-ATsup-CC-E which leverages transformers in its architecture. Thus, despite the fact that there is considerable room for improvement, MONET outperforms existing approaches due to its ability to jointly model task order and prediction. Figure  9  display AU-wise attribution maps for the three multi-task baselines along with MONET. Those maps are computed using Grad-Cam-like  [43]  techniques which consists in taking the gradients of each action unit prediction with respect to the input image or intermediate feature maps. VMNC and MRNN predicts action units using an architecture that is fully shared across tasks. Therefore, there is no space for task specialization which result in similar heatmaps across tasks. For example, MRNN heatmaps from AU14 to AU24 are all very close. On the other hand, VMNS which uses a specific regressor by task manage to specialize its attribution for each tasks (heatmaps by tasks are different). However it misses important localizations such as eyebrows zone for AU1, AU2 while MRNN catches them. In fact, we believe that the joint task distribution modelling in MRNN helps guide the network attention. MONET gets the best of the two worlds: On one side, its task-wise module enable task-wise attribution specialization, on the other side its joint task distribution modelling helps guide the attribution toward specific zones such as eyebrows for AU1-2.\n\nFinally Table  4  shows a comparison between MONET and the other multi-task baselines. First, we observe a large gap in performance between VMNC and VMNS. It demonstrates that weight sharing pattern choices critically affect performance. More importantly it shows that choosing the best sharing pattern for a task is a difficult problem. Second, MRNN performance lies lower than VMNC. We believe that this is due to the larger number of tasks, that makes it less likely to find a suitable order to efficiently learn the Action Unit dependencies. Lastly, thanks to its task-wise modular sharing pattern and order selection mechanism, MONET efficiently models the joint AU distribution and achieves better performance than the other three methods.\n\nAU detection on DISFA: Table  5  compares the performance of MONET with other state of the art approaches on DISFA. Similar to BP4D, MONET displays better performance than classical multitask methods and all other existing approaches. Therefore, MONET consistently outperform state of the art performance for facial action unit recognition.\n\nFigure  10  shows MONET soft order evolution when training on DISFA. During the warm up phase (first 5 epochs), the soft order is the average of 512 randomly sampled permutation matrices and is therefore close to uniform (all probabilities are close to 1 T ). At the end of the warm up, MONET starts learning the order selection coefficients. In particular, we observe that MONET learns to start by predicting AU1, 2, 4, 6, 9 which correspond to the upper part of the face (eyes and brows related action units) and then processes 12, 25 and 26 which are located in the lower part of the face (mostly the mouth). In fact we believe that tasks that use the same part of the image and more generally the same piece of information benefit from being processed close to each other. This coarse intuition seems to be fairly verified at a more detailed scale as we also observe smaller blocks of consecutive tasks. For instance, AU25-26 basically focuses on mouth and jaw, and AU4-6 both have their regions of interest located around the eyes. As far as brow movements are concerned, it is worth noticing that AU4 is processed before AU1. An explanation could be that AU4 is easier to predict (mainly because it is less local as it often comes with nose and corner of the eye wrinkles). Therefore using AU4 prediction and learned dependencies to help predict AU1 is more beneficial than doing the opposite.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusion And Discussion",
      "text": "In this paper, we tackled the problem of AU detection, which is intrinsically a multi-task problem with strong inter-task dependencies.\n\nTo efficiently model these relationships, we introduced MONET, a multi-order network for joint task order and prediction modelling in deep multi-task learning. MONET leverages a differentiable order selection mechanism based on soft order modelling inside Birkhoff's polytope, as well as task-wise recurrent cell sharing for concurrent multi-order prediction learning. Furthermore, we propose warmup and order dropout strategies that enhance order selection by preventing order overfitting.\n\nExperimentally, we first demonstrated that MONET was able to converge toward the correct order in a controlled scenario. Second, we showed that demonstrate that MONET display competitive performance on a wide variety of task relationship settings: applied to facial attribute detection (CelebA), MONET performs at least as well as the best multi-task baseline on each 5 attributes subset. Based on this empirical evidence, we argue that MONET is an all-around better multi-task method and could therefore constitute a valid architectural choice for many multi-task learning problems.\n\nThen, we demonstrated that, thanks to its order selection mechanism and task-wise modular architecture, MONET efficiently models the strong inter-task dependencies between AUs and consequently outperform multi-task baselines aswell as state-of-the art performance on both DISFA and BP4D datasets.\n\nFinally, the proposed work still suffers from certain limitations. The most important is the scalability to a large number of tasks. Indeed, the number of permutations for soft order modelling increases as the factorial of the number of tasks, and the number of recurrent cells in the architecture grows linearly with the number of tasks. Furthermore, strong correlations between AUs could be used to partition the set of AUs into several blocks with strong intra-block correlation and low inter-block correlation. Then we could use a recurrent cell by blocks of task and learn the best block order with MONET, which would also address the problem of the numbers of tasks. Furthermore, an interesting direction will consist in extending MONET to other families of tasks (eg: categorical classification, regression) to design efficient multi-task affective computing methods, including face recognition, AU intensity and arousal/valence estimation, as well as facial landmarks and head pose estimation. For action unit j, the coefficient associated with timestep i can be interpreted as the probability that action unit j is processed at timestep i.",
      "page_start": 10,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: -(a)). Yet, this",
      "page": 1
    },
    {
      "caption": "Figure 1: -(b)). However, contrary to standard sequence pro-",
      "page": 1
    },
    {
      "caption": "Figure 1: -(c) illustrates MONET",
      "page": 1
    },
    {
      "caption": "Figure 1: Overview of MONET along with existing multi-task methods.",
      "page": 1
    },
    {
      "caption": "Figure 2: , in inference mode, sampling from this order",
      "page": 3
    },
    {
      "caption": "Figure 2: MONET inference overview with T = 3 tasks and M = 2 orders σ1 = [1, 2, 3] and σ2 = [3, 2, 1]. All visual elements with the same color",
      "page": 4
    },
    {
      "caption": "Figure 3: Warm up learning rate schedule. αp represents the learning rate",
      "page": 5
    },
    {
      "caption": "Figure 4: Distribution of 2000 examples sampled from our toy dataset with",
      "page": 6
    },
    {
      "caption": "Figure 4: For T tasks, it uses the following laws for input and",
      "page": 6
    },
    {
      "caption": "Figure 5: Performance of MONET trained with a single order σ (M =",
      "page": 7
    },
    {
      "caption": "Figure 5: shows the performance",
      "page": 7
    },
    {
      "caption": "Figure 7: , compares the performance of",
      "page": 7
    },
    {
      "caption": "Figure 6: Evolution of the Frobenius distance between MONET soft order and the toy correct order for 20 MONET training with different values of k.",
      "page": 8
    },
    {
      "caption": "Figure 7: MONET performances comparaison for toy dataset with T = 5",
      "page": 8
    },
    {
      "caption": "Figure 8: depicts two soft-order matrices extracted at the end of",
      "page": 8
    },
    {
      "caption": "Figure 8: Two soft-order matrices extracted at the end of two different",
      "page": 9
    },
    {
      "caption": "Figure 9: display AU-wise attribution maps for the three multi-task",
      "page": 9
    },
    {
      "caption": "Figure 10: shows MONET soft order evolution when training on",
      "page": 9
    },
    {
      "caption": "Figure 9: AU-wise attribution maps for multi-task baselines as well as MONET. While other architectures may stuggle to correctly locate each AU,",
      "page": 10
    },
    {
      "caption": "Figure 10: Evolution of MONET soft order during 30 epochs of training",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table 1: showsthatthisorderselectionmechanismperformpoorly.Infact,",
      "data": [
        {
          "Accuracy": "VMNC\nVMNS\nMRNN\nMONET",
          "Task 1": "98.7\n98.7\n93.7\n99.4",
          "Task 2": "97.6\n97.5\n93.6\n98.3",
          "Task 3": "92.4\n72.8\n80.1\n95.8",
          "Task 4": "65.2\n51.5\n60.5\n90.0",
          "Task 5": "54.0\n52.9\n51.3\n68.2",
          "Mean": "81.5\n74.9\n75.9\n90.1"
        },
        {
          "Accuracy": "Oracle†",
          "Task 1": "99.6",
          "Task 2": "98.6",
          "Task 3": "96.1",
          "Task 4": "92.1",
          "Task 5": "80.6",
          "Mean": "93.4"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 3: draws a comparison between different multi-task methods",
      "data": [
        {
          "no dropout": "dropout"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 4: shows a comparison between MONET and the",
      "data": [
        {
          "F1 Score-AU": "DRML [7]\nEAC-NET [38]\nDSIN [39]\nJAANet [9]\nLP-Net [40]\nCMS [41]\nARL [11]\nSRERL [14]\nJ ˆAANET [10]\nHMP-PS [13]\nSEV-Net [42]\nPT-MT-ATsup-CC-E [16]",
          "1": "36.4\n39.0\n51.7\n47.2\n43.4\n49.1\n45.8\n46.9\n53.8\n53.1\n58.2\n51.7",
          "2": "41.8\n35.2\n40.4\n44.0\n38.0\n44.1\n39.8\n45.3\n47.8\n46.1\n50.4\n49.3",
          "4": "43.0\n48.6\n56.6\n54.9\n54.2\n50.3\n55.1\n55.6\n58.2\n56.0\n58.3\n61.0",
          "6": "55.0\n76.1\n76.1\n77.5\n77.1\n79.2\n75.7\n77.1\n78.5\n76.5\n81.9\n77.8",
          "7": "67.0\n72.9\n73.5\n74.6\n76.7\n74.7\n77.2\n78.4\n75.8\n76.9\n73.9\n79.5",
          "10": "66.3\n81.9\n79.9\n84.0\n83.8\n80.9\n82.3\n83.5\n82.7\n82.1\n87.8\n82.9",
          "12": "65.8\n86.2\n85.4\n86.9\n87.2\n88.3\n86.6\n87.6\n88.2\n86.4\n87.5\n86.3",
          "14": "54.1\n58.8\n62.7\n61.9\n63.3\n63.9\n58.8\n63.9\n63.7\n64.8\n61.6\n67.6",
          "15": "33.2\n37.5\n37.3\n43.6\n45.3\n44.4\n47.6\n52.2\n43.3\n51.5\n52.6\n51.9",
          "17": "48.0\n59.1\n62.9\n60.3\n60.5\n60.3\n62.1\n63.9\n61.8\n63.0\n62.2\n63.0",
          "23": "31.7\n35.9\n38.8\n42.7\n48.1\n41.4\n47.7\n47.1\n45.6\n49.9\n44.6\n43.7",
          "24": "30.0\n35.8\n41.6\n41.9\n54.2\n51.2\n55.4\n53.3\n49.9\n54.5\n47.6\n56.3",
          "Avg.": "48.3\n55.9\n58.9\n60.0\n61.0\n60.6\n61.1\n62.1\n62.4\n63.4\n63.9\n64.2"
        },
        {
          "F1 Score-AU": "VMNS\nVMNC\nMRNN\nMONET (ours)",
          "1": "51.7\n48.7\n47.1\n54.5",
          "2": "46.6\n45.2\n44.7\n45.0",
          "4": "57.8\n56.8\n59.1\n61.5",
          "6": "77.7\n77.9\n77.5\n75.9",
          "7": "74.2\n77.8\n78.3\n78.0",
          "10": "81.1\n83.2\n84.1\n84.5",
          "12": "88.3\n87.9\n85.3\n87.6",
          "14": "59.3\n62.9\n63.9\n65.1",
          "15": "45.7\n51.1\n41.6\n54.8",
          "17": "60.8\n59.1\n62.8\n60.5",
          "23": "45.0\n47.4\n43.3\n53.0",
          "24": "49.5\n52.7\n51.5\n53.2",
          "Avg.": "61.5\n62.6\n61.6\n64.5"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "F1 Score-AU": "DRML [7]\nEAC-NET [38]\nDSIN [39]\nSRERL [14]\nJAANet [9]\nLP-Net [40]\nCMS [41]\nARL [11]\nSEV-Net [42]\nHMP-PS [13]\nPT-MT-ATsup-CC-E [16]\nJ ˆAANET [10]",
          "1": "17.3\n41.5\n42.4\n45.7\n43.7\n29.9\n40.2\n43.9\n55.3\n38.0\n46.1\n62.4",
          "2": "17.7\n26.4\n39.0\n47.8\n46.2\n24.7\n44.3\n42.1\n53.1\n45.9\n48.6\n60.7",
          "4": "37.4\n66.4\n68.4\n59.6\n56.0\n72.7\n53.2\n63.6\n61.5\n65.2\n72.8\n67.1",
          "6": "29.0\n50.7\n28.6\n47.1\n41.4\n46.8\n57.1\n41.8\n53.6\n50.9\n56.7\n41.1",
          "9": "10.7\n8.5\n46.8\n45.6\n44.7\n49.6\n50.3\n40.0\n38.2\n50.8\n50.0\n45.1",
          "12": "37.7\n89.3\n70.8\n73.5\n69.6\n72.9\n73.5\n76.2\n71.6\n76.0\n72.1\n73.5",
          "25": "38.5\n88.9\n90.4\n84.3\n88.3\n93.8\n81.1\n95.2\n95.7\n93.3\n90.8\n90.9",
          "26": "20.1\n15.6\n42.2\n43.6\n58.4\n65.0\n59.7\n66.8\n41.5\n67.6\n55.4\n67.4",
          "Avg.": "26.7\n48.5\n53.6\n55.9\n56.0\n56.9\n57.4\n58.7\n58.8\n61.0\n61.5\n63.5"
        },
        {
          "F1 Score-AU": "VMNS\nVMNC\nMRNN\nMONET (ours)",
          "1": "53.4\n56.8\n47.4\n55.8",
          "2": "51.3\n59.0\n49.7\n60.4",
          "4": "64.8\n64.4\n61.8\n68.1",
          "6": "45.5\n51.4\n46.7\n49.8",
          "9": "36.0\n43.7\n38.8\n48.0",
          "12": "70.1\n75.1\n71.0\n73.7",
          "25": "89.8\n92.5\n91.9\n92.3",
          "26": "62.4\n62.8\n60.9\n63.1",
          "Avg.": "59.2\n63.2\n58.5\n63.9"
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "What the face reveals: Basic and applied studies of spontaneous expression using the Facial Action Coding System (FACS)",
      "authors": [
        "R Ekman"
      ],
      "year": "1997",
      "venue": "What the face reveals: Basic and applied studies of spontaneous expression using the Facial Action Coding System (FACS)"
    },
    {
      "citation_id": "2",
      "title": "Hyperface: A deep multi-task learning framework for face detection, landmark localization, pose estimation, and gender recognition",
      "authors": [
        "R Ranjan",
        "V Patel",
        "R Chellappa"
      ],
      "year": "2017",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "3",
      "title": "Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory",
      "authors": [
        "I Kokkinos"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "4",
      "title": "Maximizing subset accuracy with recurrent neural networks in multi-label classification",
      "authors": [
        "J Nam",
        "E Mencía",
        "H Kim",
        "J Fürnkranz"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "5",
      "title": "Recurrent neural network for text classification with multi-task learning",
      "authors": [
        "P Liu",
        "X Qiu",
        "X Huang"
      ],
      "year": "2016",
      "venue": "Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "6",
      "title": "Order matters: Sequence to sequence for sets",
      "authors": [
        "O Vinyals",
        "S Bengio",
        "M Kudlur"
      ],
      "year": "2016",
      "venue": "4th International Conference on Learning Representations"
    },
    {
      "citation_id": "7",
      "title": "Deep region and multi-label learning for facial action unit detection",
      "authors": [
        "K Zhao",
        "W.-S Chu",
        "H Zhang"
      ],
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "8",
      "title": "Eac-net: A region-based deep enhancing and cropping approach for facial action unit detection",
      "authors": [
        "W Li",
        "F Abtahi",
        "Z Zhu",
        "L Yin"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "9",
      "title": "Deep adaptive attention for joint facial action unit detection and face alignment",
      "authors": [
        "Z Shao",
        "Z Liu",
        "J Cai",
        "L Ma"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "10",
      "title": "Jaa-net: Joint facial action unit detection and face alignment via adaptive attention",
      "year": "2021",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "11",
      "title": "Facial action unit detection using attention and relation learning",
      "authors": [
        "Z Shao",
        "Z Liu",
        "J Cai",
        "Y Wu",
        "L Ma"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "12",
      "title": "Deep structure inference network for facial action unit recognition",
      "authors": [
        "C Corneanu",
        "M Madadi",
        "S Escalera"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "13",
      "title": "Hybrid message passing with performance-driven structures for facial action unit detection",
      "authors": [
        "T Song",
        "Z Cui",
        "W Zheng",
        "Q Ji"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "14",
      "title": "Semantic relationships guided representation learning for facial action unit recognition",
      "authors": [
        "G Li",
        "X Zhu",
        "Y Zeng",
        "Q Wang",
        "L Lin"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "15",
      "title": "Local relationship learning with person-specific shape regularization for facial action unit detection",
      "authors": [
        "X Niu",
        "H Han",
        "S Yang",
        "Y Huang",
        "S Shan"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "16",
      "title": "Facial action unit detection with transformers",
      "authors": [
        "G Jacob",
        "B Stenger"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "17",
      "title": "Knowledge augmented deep neural networks for joint facial expression and action unit recognition",
      "authors": [
        "Z Cui",
        "T Song",
        "Y Wang",
        "Q Ji"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "18",
      "title": "Expression-assisted facial action unit recognition under incomplete au annotation",
      "authors": [
        "S Wang",
        "Q Gan",
        "Q Ji"
      ],
      "year": "2017",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "19",
      "title": "Emfacs-7: Emotional facial action coding system",
      "authors": [
        "W Friesen",
        "P Ekman"
      ],
      "year": "1983",
      "venue": "Emfacs-7: Emotional facial action coding system"
    },
    {
      "citation_id": "20",
      "title": "Multi-task learning of emotion recognition and facial action unit detection with adaptively weights sharing network",
      "authors": [
        "C Wang",
        "J Zeng",
        "S Shan",
        "X Chen"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "21",
      "title": "Cross-stitch networks for multi-task learning",
      "authors": [
        "I Misra",
        "A Shrivastava",
        "A Gupta",
        "M Hebert"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "22",
      "title": "Meta auxiliary learning for facial action unit detection",
      "authors": [
        "Y Li",
        "S Shan"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "23",
      "title": "Facial landmark detection by deep multi-task learning",
      "authors": [
        "Z Zhang",
        "P Luo",
        "C Loy",
        "X Tang"
      ],
      "year": "2014",
      "venue": "Facial landmark detection by deep multi-task learning"
    },
    {
      "citation_id": "24",
      "title": "Deep neural networks employing multi-task learning and stacked bottleneck features for speech synthesis",
      "authors": [
        "Z Wu",
        "C Valentini-Botinhao",
        "O Watts",
        "S King"
      ],
      "year": "2015",
      "venue": "2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "25",
      "title": "An overview of multi-task learning in deep neural networks",
      "authors": [
        "S Ruder"
      ],
      "year": "2017",
      "venue": "An overview of multi-task learning in deep neural networks",
      "arxiv": "arXiv:1706.05098"
    },
    {
      "citation_id": "26",
      "title": "Nddr-cnn: Layerwise feature fusing in multi-task cnns by neural discriminative dimensionality reduction",
      "authors": [
        "Y Gao",
        "J Ma",
        "M Zhao",
        "W Liu",
        "A Yuille"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "27",
      "title": "DARTS: differentiable architecture search",
      "authors": [
        "H Liu",
        "K Simonyan",
        "Y Yang"
      ],
      "year": "2019",
      "venue": "7th International Conference on Learning Representations"
    },
    {
      "citation_id": "28",
      "title": "Beyond shared hierarchies: Deep multitask learning through soft layer ordering",
      "authors": [
        "E Meyerson",
        "R Miikkulainen"
      ],
      "year": "2018",
      "venue": "6th International Conference on Learning Representations, ICLR 2018"
    },
    {
      "citation_id": "29",
      "title": "Adashare: Learning what to share for efficient deep multi-task learning",
      "authors": [
        "X Sun",
        "R Panda",
        "R Feris",
        "K Saenko"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "30",
      "title": "Integrated perception with recurrent multi-task neural networks",
      "authors": [
        "H Bilen",
        "A Vedaldi"
      ],
      "year": "2016",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "31",
      "title": "Multi-task learning for multiple language translation",
      "authors": [
        "D Dong",
        "H Wu",
        "W He",
        "D Yu",
        "H Wang"
      ],
      "year": "2015",
      "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "32",
      "title": "Deep multi-task learning with low level tasks supervised at lower layers",
      "authors": [
        "A Søgaard",
        "Y Goldberg"
      ],
      "year": "2016",
      "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "33",
      "title": "Dropout: a simple way to prevent neural networks from overfitting",
      "authors": [
        "N Srivastava",
        "G Hinton",
        "A Krizhevsky",
        "I Sutskever",
        "R Salakhutdinov"
      ],
      "year": "2014",
      "venue": "The journal of machine learning research"
    },
    {
      "citation_id": "34",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2015",
      "venue": "3rd International Conference on Learning Representations, ICLR 2015"
    },
    {
      "citation_id": "35",
      "title": "Vggface2: A dataset for recognising faces across pose and age",
      "authors": [
        "Q Cao",
        "L Shen",
        "W Xie",
        "O Parkhi",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE international conference on automatic face & gesture recognition"
    },
    {
      "citation_id": "36",
      "title": "Decoupled weight decay regularization",
      "authors": [
        "I Loshchilov",
        "F Hutter"
      ],
      "year": "2019",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "37",
      "title": "Focal loss for dense object detection",
      "authors": [
        "T.-Y Lin",
        "P Goyal",
        "R Girshick",
        "K He",
        "P Dollár"
      ],
      "year": "2017",
      "venue": "Proceedings"
    },
    {
      "citation_id": "38",
      "title": "Eac-net: Deep nets with enhancing and cropping for facial action unit detection",
      "authors": [
        "W Li",
        "F Abtahi",
        "Z Zhu",
        "L Yin"
      ],
      "year": "2018",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "39",
      "title": "Deep structure inference network for facial action unit recognition",
      "authors": [
        "C Corneanu",
        "M Madadi",
        "S Escalera"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "40",
      "title": "Local relationship learning with person-specific shape regularization for facial action unit detection",
      "authors": [
        "X Niu",
        "H Han",
        "S Yang",
        "Y Huang",
        "S Shan"
      ],
      "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "41",
      "title": "Representation learning through cross-modality supervision",
      "authors": [
        "N Sankaran",
        "D Mohan",
        "S Setlur",
        "V Govindaraju",
        "D Fedorishin"
      ],
      "year": "2019",
      "venue": "2019 14th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "42",
      "title": "Exploiting semantic embedding and visual feature for facial action unit detection",
      "authors": [
        "H Yang",
        "L Yin",
        "Y Zhou",
        "J Gu"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "43",
      "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "authors": [
        "R Selvaraju",
        "M Cogswell",
        "A Das",
        "R Vedantam",
        "D Parikh",
        "D Batra"
      ],
      "year": "2017",
      "venue": "Proceedings"
    }
  ]
}