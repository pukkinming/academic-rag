{
  "paper_id": "2412.17907v1",
  "title": "A Multimodal Emotion Recognition System: Integrating Facial Expressions, Body Movement, Speech, And Spoken Language",
  "published": "2024-12-23T19:00:34Z",
  "authors": [
    "Kris Kraack"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Traditional psychological evaluations rely heavily on human observation and interpretation, which are prone to subjectivity, bias, fatigue, and inconsistency. To address these limitations, this work presents a multimodal emotion recognition system that provides a standardised, objective, and data-driven tool to support evaluators, such as psychologists, psychiatrists, and clinicians. The system integrates recognition of facial expressions, speech, spoken language, and body movement analysis to capture subtle emotional cues that are often overlooked in human evaluations. By combining these modalities, the system provides more robust and comprehensive emotional state assessment, reducing the risk of mis-and overdiagnosis. Preliminary testing in a simulated real-world condition demonstrates the system's potential to provide reliable emotional insights to improve the diagnostic accuracy. This work highlights the promise of automated multimodal analysis as a valuable complement to traditional psychological evaluation practices, with applications in clinical and therapeutic settings.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "raditional psychological evaluations are often prone to subjective interpretations and biases of the human evaluators, potentially leading to inconsistencies and inaccuracies in diagnosis  [1] . To help mitigate the potential for human-induced errors, we propose an objective, data-driven tool to support clinical decision-making, that can function as a valuable asset for psychologists, psychiatrists, and other trained evaluators. This paper presents a multimodal emotion recognition and analysis system that aims to improve the objectivity and accuracy of emotional assessments by addressing the limitations inherent in traditional methods.\n\nCurrent diagnostic practices rely on clinician observation and subjective interpretation of verbal and nonverbal cues during patient interviews. While these assessments are made by trained evaluators, they can be affected by human limitations such as attention and compassion fatigue, perceptual biases, and inconsistent recall  [2, 3] . To address these challenges, the proposed system integrates multiple modalities-facial expression, body movement, speech, and spoken language-to provide a comprehensive, quantitative approach to emotion recognition. This system serves as a second-opinion tool that supports evaluators' findings and enhances their clinical judgments with objective, data-driven insights.\n\nThe system employs a range of techniques, including machine and deep learning, computer vision (CV), intelligent signal processing (ISP)-such as audio feature extraction, and natural language processing (NLP), to provide emotional analysis in real-time or retrospectively. Facial expression recognition (FER) is conducted using a convolutional neural network (CNN), which classifies the emotional states. Body movements are evaluated through pose estimation, which assesses movement intensity. Speech recognition involves the extraction of audio features and the employment of a weighted convolutional model, which recognises emotional state from audio cues. Spoken language recognition is conducted by converting speech into text, preprocessing the textual data, and utilising a Bidirectional Long Short-Term Memory (Bi-LSTM) model to interpret spoken language and provide insights into emotional sentiment. By capturing and identifying subtle emotional cues that may be overlooked during human evaluations, this system provides an additional analytical layer that either corroborates or challenges initial human-derived conclusions. This can be particularly valuable in clinical settings where corroborating evidence is essential, such as in patient interviews and the review of prior evaluations.\n\nThe remainder of this paper is structured as follows: Section II reviews related works, highlighting existing unimodal and multimodal emotion recognition systems. Section III outlines the methodology, providing a walkthrough of the proposed multimodal system and its components. Section IV assesses the performance of the trained models. Section V presents the results of real-world testing conducted in a simulated ideal environment. Section VI offers a discussion of the findings, addressing performance and realworld applicability. Section VII concludes the work and summarises its key contributions. Finally, Section VIII outlines potential directions for future research.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "In recent years, multimodal emotion recognition systems have garnered significant attention due to their promising potential to predict emotions. These systems integrate data from a range of modalities, including facial expressions, speech, spoken language, and more, with the objective of further enhancing emotion recognition by addressing the limitations of unimodal approaches.\n\nFacial expression recognition has been subject of extensive research  [4, 5, 6, 7] , with numerous studies employing T convolutional neural networks (CNNs) and other deep learning models to analyse the subtle nuances of facial mimicry. The work of Pise et al.  [8]  emphasises the critical role of facial expressions in a multimodal emotion recognition system. The fields of speech emotion recognition (SER)  [9, 10, 11, 12]  and spoken language recognition  [13, 14, 15]  are widely researched, and can contribute essential auditory emotional cues in a multimodal system integration. A number of multimodal emotion recognition (MER) systems have been proposed with the objective of integrating these or similar modalities  [16, 17, 18, 19, 20, 21]  demonstrating the potential for a holistic emotion recognition approach that surpasses the limitations of unimodal systems.\n\nThe proposed system is distinguished from existing work in a number of significant ways. The system and recording environment have been specifically developed with a specific focus on its potential applications in mental health evaluations. The preliminary real-world testing in simulated mental health evaluation scenario has demonstrated promising results for the practical applicability of the system. The unimodal components' model performance is significantly better than related works using same datasets as inputs. In addition, the open-source nature of this work promotes accessibility and collaboration, with resources available to the public  [22, 23, 24, 25, 26] . This approach promotes societal benefit by encouraging innovation and reducing barriers to adoption in both research and clinical contexts.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Limitations Of Traditional Evaluation And Technological Solution",
      "text": "Traditional psychological evaluations rely heavily on human expertise, making them susceptible to subjectivity and various limitations. This section discusses the main limitations of human evaluations and explores how technological solutions, such as a multimodal recognition system, can help mitigate these issues.\n\nHuman subjectivity is a significant factor contributing to errors in psychological evaluations. Such biases, that may arise from cultural, gender, or socioeconomic factors, that affect the objectivity of clinical judgment  [2, 3] . Evaluators may encounter challenges in accurately identifying when patients are, consciously or unconsciously, being untruthful or withholding their true emotional state. The complexity of interpreting very subtle non-verbal cues, such as subtle shifts in body language, changes in speech patterns, or eye contact, such as micro expressions-brief, involuntary facial expressions lasting only fractions of a second-presents significant challenges for humans to detect or interpret due to their fleeting and subtle nature, especially under time constraints or stressful conditions  [27] . Misinterpreting cues or failing to identify deception can lead to misjudgements or erroneous conclusions. A system that captures every emotional state in real-time and relies on multiple modalities can potentially provide more nuanced information about the true emotional state of patience in a given time window of an interview based on a baseline prediction.\n\nPsychological tests, such as personality, intelligence, projective, etc., are often administered during a scheduled and very limited window of time, which can potentially provide a momentary view that is may be obscured by external factors surrounding the patient, such as tragic event, stress-inducing triggers, etc.  [28] . The proposed system could assist in identifying behaviours or emotional states that are out of the ordinary for a particular patient, prior to conducting such tests. Compassion and attention fatigue, often induced by stress, workload, or external factors, is another challenge that affects evaluators, particularly during extended assessment sessions. Research  [29, 30, 31]  highlights that fatigue can reduce the performance in terms of the diagnostic accuracy decisions and lead to increased errors. The variability introduced by human recall, which is often influenced by the clinician's memory and attention span, further compounds these challenges. Increased pressure on an already pressured healthcare system due to an increase in diagnosis  [32]  may further exacerbate the issue.\n\nThe occurrence of misdiagnosis and overdiagnosis in mental health poses significant risks to patient care. Misdiagnosis can lead to inappropriate treatment plans that may exacerbate symptoms, delay access to the right type of care, or worse  [33] . Overdiagnosis, in which individuals are diagnosed with a condition that they do not truly have, is another critical issue, contributing to unnecessary treatment and stigma  [34] .\n\nIntegrating technology into psychological evaluations presents a promising way to help mitigate these limitations  [35, 36] . A system that successfully leverages relevant modalities can provide a quantitative and objective complement to traditional evaluation methods to help mitigate bias, detect subtle emotional cues that human evaluators may miss, and reduce diagnostic inconsistencies.\n\nTo ensure consistent observational data, strict requirements are proposed for the interview environment, including the quality of the recording equipment, participant positioning, background interference, camera placement, angle, lighting, and microphone placement, to address these inconsistencies with a technological solution.\n\nBy providing a second layer of analysis, this system assists clinicians by highlighting potentially missed details or reinforcing initial observations. This can lead to a more comprehensive understanding of a patient's condition and assist in making more informed decisions.\n\nWhile technology cannot replace the nuanced understanding and empathy of human evaluators, it serves as an aid in enhancing diagnostic accuracy and supporting clinicians in their decision-making process.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Methodology",
      "text": "The methodology section outlines the processes involved in developing a robust multimodal emotion detection system. This approach integrates four main components: facial expression recognition, speech recognition, spoken language recognition, and body movement analysis, each contributing unique emotional cues. These components were combined to create a unified system that synthesises data for a holistic emotional profile of the patient.\n\nFuture discussions on results and evaluation, supported by performance metrics will demonstrate the system's effectiveness.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Facial Expression Recognition",
      "text": "The facial expression recognition (FER) component relies on two key datasets for training the model: Facial Expression Recognition 2013 (FER2013)  [37]  and Real-world Affective Faces database (RAF-DB)  [38] . A model was also trained using The Extended Cohn-Kanade (CK+) Dataset  [39] , achieving near-perfect performance. However, the limited number of samples and diversity of the data was determined to overly bias the model, resulting in predictions that may not generalise effectively in real-world application, for example, unseen test data was only comprised of 184 samples. The chosen data (FER-2013 and RAF-DB) was preprocessed in relation to the ideal real-world recording environment, e.g. data that does not satisfy the requirement are filtered out using cropping, blur filtering, and relevant face detection techniques. TABLE I illustrates raw and preprocessed data samples.\n\nThe FER-2013 dataset was filtered using various cropping strategies-tight, moderate, and original-were applied to enhance sample diversity. Facial detection was performed using Haar Cascade classifier from the OpenCV library  [40] , followed by blur filtering using a Laplacian operator to mitigate the presence of low-quality data.\n\nThe RAF-DB dataset was filtered using facial detection, which relied on the normalised face region landmarks utilising the face detector and the \"68 face landmarks\" predictor of the Dlib toolkit  [41]  in conjunction with the Laplacian operator filter.\n\nData augmentation techniques, such as brightness, rotation and flipping, were used to enrich each dataset further using ImageDataGenerator transformations of the TensorFlow library  [42] . For the combinational model trained using both datasets, a custom function was created that combines the two image data augmentations in relation to the emotional state labels.\n\nThe model was trained using a Convolutional Neural Network (CNN), which was designed with five convolutional layers and within four blocks, starting with a pair of layers using 32 and 64 filters. Each block incorporated batch normalization, max-pooling, and varying dropout rates to improve generalisation and reduce overfitting. Fully connected layers include a Flatten operation, a dense layer of 128 units with ReLU activation, and additional dropout. The final output layer classified the seven emotional categories using Softmax activation. The model was compiled with the Adam optimiser and categorical cross-entropy loss. Initially, the model was trained separately on the FER-2013 and RAF-DB image datasets and subsequently on a combined dataset generated.\n\nThe implementation of real-time detection on an OpenCV webcam capture instance was achieved through the integration of Dlib's  [41]  frontal face detector with a stabilisation mechanism. This mechanism employed a frame-based queue system to ensure the reliability of predictions, utilising the trained models for prediction. The output of the aforementioned setup provided a summary probability distribution of the recognised emotions from the real-time webcam capture.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. Body Movement Analysis",
      "text": "The body movement analysis component utilises the MediaPipe's pose estimation model  [43]  to extract joint positions, enabling the tracking of individual body parts and the calculation of movement metrics. These metrics are then used to classify body movements as low, medium, or high physical activity levels. The component's functionality can be further enhanced by implementing eye tracking, achieved through simple adjustments.\n\nThe component track movement in real-time utilising the video capture. Facilitating the tracking of facial movements (eye tracking) and body parts. The output of this process is a comprehensive summary of the body part movements, the body part that has moved the most, and an overall classification of the movement.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Speech Recognition",
      "text": "For the speech recognition component, a dataset comprising of 39,458 audio samples from Crowd-sourced Emotional Multimodal Actors (CREMA-D)  [44] , Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)  [45] , Surrey Audio-Visual Expressed Emotion (SAVEE)  [46] , Toronto Emotional Speech Set (TESS)  [47] , Emotional Speech Dataset (ESD)  [48, 49] , Multimodal EmotionLines Dataset (MELD)  [50, 51]  were compiled. The MELD dataset was further processed by extracting the audio signals from the video files and standardising them to WAV format. Emotional labels across the datasets were harmonised to maintain consistency. Augmentation techniques, including adding noise, dynamic compression, and pitch shifting, were employed to enhance robustness and improve generalisation in terms of the ideal real-world environment.\n\nThe process of feature extraction involved calculating zerocrossing rate (ZCR), root mean square (RMS), and Melfrequency cepstral coefficients (MFCC) on original data and with five augmentations applied: noise, dynamic compression, noised combined with dynamic compression, pitch shifted, and noised combined with pitch shifted.\n\nThe extracted audio features were then utilised the input for a CNN model, which comprised seven convolutional layers. These layers begin with a 512-filter block employing a 5kernel size, followed by a series of normalisation, max- pooling, and dropout layers. The architecture of the model included a sequence of progressively deeper convolutional blocks, with each of these blocks being followed by batch normalisation and pooling layers. Dropout was applied at varying stages in order to combat overfitting. Fully connected layers at the end, with a dense layer of 512 units and batch normalisation, were used before the output layer, which classified the audio samples into seven emotion categories using a Softmax activation function. The model was compiled with the Adam optimizer and categorical crossentropy loss for multi-class classification.\n\nThe recognition process initialises an audio recording using the microphone, and on closure, the recording is processed using the model to predict and return a summary with a probability distribution for each predefined emotion category..",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "D. Spoken Language Recognition",
      "text": "The recognition of spoken textual data employed Natural Language Processing (NLP) techniques to process the textbased emotion datasets; GoEmotions  [52] , Emotion Classification  [53] , Emotions  [54] , and Sentiment140  [55] .\n\nThe preprocessing stage entailed the implementation of Regular Expression operations (Regex) to harmonise the data, the creation of custom stop-words in observations, and the retrieval of the most frequent keywords for each emotional state (anger, disgust, joy, sadness, surprise, fear) to create a custom keyword dictionary, with the custom stop-words filtered out. The preprocessed text data were then classified in terms of emotional state observations using on the keywords dictionary. TABLE  II  showcases the number of samples in both the raw and preprocessed data.\n\nThe processed and classified data were then tokenized, converted into sequences, and padded to prepare for training the model; A Bidirectional Long Short-Term Memory (BiLSTM) architecture was employed to classify emotions based on the tokenized textual input. The model began with an embedding layer that converted text sequences into dense vector representations, followed by a BiLSTM layer with 256 units to capture bidirectional context. Batch normalization and dropout layers were incorporated for regularization, along with a dense layer of 128 units. The output layer utilised a Softmax activation function to classify text into the six emotional categories. Training was conducted using the Adam optimizer and sparse categorical cross-entropy loss to ensure effective multi-class classification.\n\nThe recognition process utilises the same audio recording initiated during speech recognition. The recording is transcribed into text using the Whisper speech-to-text model  [56]  and predicted using the trained BiLSTM model to generate an emotional profile with a probability distribution based on the spoken content.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "E. Multimodal Integration",
      "text": "The integration of the four components-facial expression recognition, body movement analysis, speech recognition, and of spoken language recognition-was pivotal in creating a comprehensive emotion detection system. By leveraging data from multiple modalities, the system provided a holistic evaluation of emotional states that transcended the limitations of any single component. The integration involved fusing outputs from the components to create a more unified emotion profile.\n\nFacial expression and body movement data were captured in real-time during webcam sessions, while audio recordings underwent post-session predictions to assess semantic content based on the vocal tone and spoken words in textual format. The outputs from each modality were aggregated by weighing and synthesising their respective probabilities, culminating in a unified and holistic emotional profile. This multimodal synthesis ensured a broader and more nuanced detection of emotional cues, enabling the system to perform reliably even in challenging scenarios. For instance, individuals with limited facial mimicry, such as those with Asperger's syndrome, who might be misinterpreted if facial expression analysis were relied upon in isolation. Similarly, an exclusive reliance on a single modality could fail to account for subtle or conflicting cues present in other channels. By integrating all four components, the system avoided such pitfalls and produced a balanced evaluation, making it a more effective tool for emotion detection. This approach aligns with the overarching objective of mitigating the limitations associated with traditional methods to provide a supportive tool to clinicians during psychological evaluations.\n\nThe multimodal system has been designed to facilitate seamless integration of each component, as illustrated in the system architecture diagram in Figure  1 . The architecture ensures that input data flows efficiently through each module, with outputs integrated to produce a unified emotional profile of the patient. The system, which incorporates a simple user interface, was made available online for the duration of testing phase. This enabled participants to access and interact with it remotely via a web-based platform. Each component, in conjunction with the multimodal integration, was subjected to testing in a simulated real-world condition.  IV. MODEL EVALUATION",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Dataset",
      "text": "In this section, an evaluation of the system is conducted of each classification model performance. This excludes body movement analysis and the multimodal integration. The evaluation considers performance metrics on unseen test data, such as accuracy, loss, precision, recall, F1 score, and AUC score.\n\nIn the context of facial expression recognition, the efficacy of models trained on the FER-2013 and RAF-DB datasets, as well as their combination, was evaluated. The model was exclusively trained on the FER-2013 dataset, demonstrated an accuracy of 68.35%, accompanied with a loss of 1.1072. The performance metrics included a precision of 67.92%, a recall of 68.35%, and an F1 score of 66.80%. Although the model displayed reasonable performance overall in classification tasks, its real-world deployment possibilities seemed limited due to misclassifications for both 'sad', fear', and 'angry' states, which often overlapped with 'neutral'. Conversely, the model trained exclusively on the RAF-DB dataset demonstrated significantly improved performance, achieving an accuracy of 85.21%, a loss of 0.6857, a precision of 85.04%, a recall of 85.21%, and an F1 score of 84.96%, suggesting promising potential. However, the model demonstrated marginal challenges in classifying 'fear'. Figure  2  illustrates the RAF-DB model's learning dynamics. The model utilising the combined datasets demonstrated an accuracy of 75.33%, a loss of 0.8569, a precision of 75.58%, a recall of 75.33%, and an F1 score of 74.64%. The combined model improved the performance of the FER-2013-only model with regard to misclassifications and demonstrated better generalisation, indicating better potential. However, it ultimately proved to be less effective than the model trained on RAF-DB dataset exclusively. The performance of the models is outlined in Table  III , which can be found in the appendix.  The recognition of spoken language conducted using NLP techniques and training of a Bidirectional Long Short-Term Memory (BiLSTM) model for this task, achieved a high validation accuracy of 97.96%, with a loss of 0.0621, a precision of 0.9801, a recall of 0.9796, and an F1 score of 0.9794. Figure  3  illustrates the convergence of loss and the enhancement in accuracy on both the training and validation datasets for the model. The multimodal integration process entailed the fusion of outputs from all four components-facial expression recognition, body movement analysis, speech recognition, and spoken language recognition-to create a unified emotional profile. alone. Although the aggregated metrics of the multimodal integration, presented in Figure  4 , do not indicate an overall improvement of performance, it is expected that combining these diverse data sources should significantly enhance the system's robustness and reliability as opposed to the single modality components on their own in a real-world setting. The combined confusion matrix, visualised in Figure  5 , provides an overview of the collective classification performance across all models, illustrating the interaction between true and predicted emotions across the unified emotional states.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "V. Preliminary Real-World Testing",
      "text": "The multimodal emotion recognition system was subjected to a simulated real-world experiment in which participants interacted with the system remotely via a simple web-based interface. The goal of the test was to evaluate the multimodal system's ability to recognise emotions in real-world applications. Participants were required to engage in tasks that elicited specific emotions, providing feedback on the system's classification accuracy. The testing process was divided into two phases; the first focused on individual modalities and the second on the multimodal integration.\n\nThe testing process was conducted online, with a total of 52 participants accessing the system via their local device (Mac or Windows) that was equipped with a webcam and microphone that satisfied the minimum specification requirement. It should be noted that while 159 individuals began the test, only 52 of these completed it. It is also important to emphasise that no personal identifiable information was collected; only demographic data was obtained through initial surveys, with non-disclosure answer option, to ensure a diverse test group. Figure  6  provides a visual representation of the demographic composition of the test group. The participants were instructed to create ideal testing environment, with textual instructions and guided image content. This was done in order to simulate the desired, highly standardised psychological interview circumstance. This is proposed to increase evaluation consistency in real-life applications, making it crucial for a robust and reliable technological solution to the problem.\n\nInformed consent was obtained from all participants, who were made aware of the utilisation of their cameras and microphones for the purpose of analysis. Webcam images were not stored, but audio recordings were temporarily saved for the purpose of audio processing of the speech and spoken language recognition components. These were securely deleted automatically afterwards.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "A. Phase 1: Single Modal Emotion Recognition",
      "text": "First phase of the real-world testing, centered on single modal emotion recognition of individual components. Participants were tasked with providing true/false feedback on the recognition accuracy of each component. For the facial expression recognition component, participants were instructed to replicate the seven target emotions, with image examples of each emotion presented for reference. For the body movement analysis component, participants were instructed to ensure that the entirety of their body was visible on the video capture, and to perform movement in accordance with the defined low, medium, and high intensity physical movements. The tests of the speech recognition component involved participants expressing self-composed sentences reflecting each target emotion, while mimicking the corresponding tone. The recognition of spoken language required participants to speak pre-defined sentences expressing each emotional state.\n\nThe test results presented in TABLE IV highlight each component's performance in a real-world setting for each of the components and the multimodal integration.\n\nFor the single modal component results: The facial expression recognition component did perform really well with an overall of 82.97% accuracy, successfully predicting 'happiness' in all cases. However, emotion 'fear' is often misclassified with a true prediction of only 61.54%, the same is, to some extent, true for 'disgust with 73.08%. The accuracy of body movement analysis components showcased accuracies of 94.23%, 92.31%, and 98.08% for low, moderate, and high physical movement respectively, and an overall accuracy of 94.72% indication a reliable component. Speech recognition demonstrated a good overall accuracy of 85.71%, both 'happiness' and 'anger' were predicted with a 94.23% accuracy, the lowest accuracy percentage was 'surprise' with a respectable 71.15%. Recognition of spoken language was the component achieving the best overall accuracy percentage of 87.64%, being able to predict 6 out of 7 emotions with an accuracy above 86%, but similar to what observed in the facial expression recognition feedback, the component seriously struggles with correctly classifying the 'neutral' emotion with a very poor reported accuracy of only 46.15%.\n\nWhile each of the single modal components in a real-world setting achieves a high overall accuracy, the performance is misleading, as it conceals significant issue with predicting some emotions, such as 'fear' for facial expression recognition, 'neutral' for spoken language recognition, and to some extent 'surprise for speech recognition. This discrepancy suggests that, despite a seemingly high overall averages, the single modal component's reliability, on their own, is compromised by its failure with inaccurate predictions for singular emotions. If an evaluator relied on a single modal component for predicting the emotional state of a patient; 'fear' would be misclassified in 38.46% of the cases based on facial expression alone, 'surprise' misclassified 28.82% based on vocal tone, and 'neutral' misclassified in an alarming 53.85% of the cases, if solely relying on recognition of spoken language. In the context of the objective of mitigating human limitations related to mis and overdiagnosis, single modality recognition is not a reliable solution to the problem",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "B. Phase 2: Multimodal Emotion Recognition",
      "text": "The second phase, which focused on multimodal integration, adopted a similar test structure to the previous phase. The test involved participants preparing and performing personal sentences designed to evoke specific emotions (anger, disgust, fear, happiness, surprise, or neutral), and providing true/false feedback based on the system's ability to correctly recognise the evoked feeling.\n\nThe multimodal emotion recognition test results, also presented in Table  IV , demonstrated a remarkably high level of accuracy, with an overall percentage accuracy of 96.43%. Notably, the feedback from the participants did not reveal any underperforming emotion predictions, with the lowest prediction being 92.31% for the 'neutral' emotional state. Overall, misclassifications are only present in 13 cases out of 351 predictions, indicating only a 3.57% misclassification occurrence. These findings demonstrated that combining diverse data sources significantly enhanced the system's robustness and reliability. Performance evaluations indicated that the multimodal approach outperforms single modalities, thereby underscoring the advantages of a holistic analysis for emotion detection. Moreover, the multimodal framework proved particularly effective in compensating for inaccuracies in individual components, thus further validating its effectiveness.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Vi. Discussion",
      "text": "This paper presents a multimodal emotion recognition system that has been designed to address the limitations of traditional psychological evaluations, including human limitations, mis-and overdiagnosis, inconsistencies in evaluation practices, and the increasing burden on healthcare systems. The proposed system provides quantitative predictive data and can therefore be used as a supplementary tool in the psychological evaluation process for psychologists, psychiatrists, or clinicians.\n\nThe findings underscore the limitations of single-modality emotion recognition systems, which are susceptible to misclassification for particular emotional states. The multimodal system effectively mitigates these risks, improving robustness and reliability. It enables evaluators to ability to address human limitations in traditional evaluations, such as attention and compassion fatigue, biases, inconsistent recall, and the failure to perceive subtle cues.\n\nWhile the results are promising, it is important to acknowledge that they were derived from a simulated controlled environment, designed to emulate the ideal realworld interview scenario proposed. These standardised conditions are required for all evaluations conducted using the system, to ensure consistency throughout. The preprocessing of the data used for training was specifically optimised for alignment with the system's intended application and its proposed solution to the inconsistencies uncovered in traditional evaluation methods.\n\nIt is imperative to empathise that the real-world testing did not include the system's intended target audience, such as clinicians, psychologists, psychiatrists, and other mental health professionals, nor did they involve actual mental health patients. Instead, the tests relied on the participation of randomly selected individuals and simulated scenarios, which do not fully replicate the real-world usage or the specific needs of the target group. It is therefore imperative that further real-world testing of the system to be carried out by trained professionals, including psychologists, psychiatrists, or clinicians, who are qualified to evaluate its impact on mental health patients. Real-world testing of the system in practice, conducted by such professionals, would yield more accurate and comprehensive insights into the system's functionality and effectiveness in its intended use cases, ensuring it is both safe and effective for practical deployment. It is recommended that further testing efforts should focus on engaging the target audience within controlled, professional settings to gather meaningful data and refine the system's design and implementation.\n\nThe primary objective of this paper was to develop an opensource system leveraging publicly available data in order to promote accessibility, transparency, and encourage collaboration to foster continuous innovation within the research community and to deliver tangible societal benefits by establishing a solid foundation for future advancements.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Vii. Conclusion",
      "text": "The proposed multimodal emotion recognition system offers a potential solution to the limitations of traditional psychological evaluation methods by providing a standardised, objective, and data-driven supportive tool. The system aims to reduce the pressures on healthcare systems and mitigate human limitations, such as subjectivity, attention fatigue, and difficulties in interpreting subtle emotional cues, thereby reducing the risks associated with traditional approaches. While the results of the study highlight the system's ability to predict emotional state in a simulated real-world scenario, further testing with the intended target audienceclinicians and mental health professionalsis necessary to validate its real-world applicability.\n\nThe multimodal integration demonstrated to be particularly valuable in cases of with atypical emotional expressions, including, but not limited to, instances of neurodevelopmental conditions or minimal facial mimicry arising from social or attentional impairments. The fusion of diverse single-modality recognition components has been shown to result in a significant improvements of emotion recognition accuracy and reliability over single modality, thus potentially rendering it a robust tool for addressing the unique challenges faced by these populations.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Viii. Future Work",
      "text": "Despite the fact that the proposed emotion recognition system provides a solid foundation, there are several areas that require improvement. Firstly, the analysis of body movements requires further refinement, as it currently lacks a meaningful correlation with emotional states. Ideally, the eye-tracking capabilities should be expanded to provide a better understanding of neurodiverse conditions related to social or attentional impairments. Furthermore, the underperformance of the system in recognising emotions in certain singlemodality components, for example 'fear' with facial expression recognition, highlights the necessity for either refined preprocessing or expanded training datasets in order to improve the model's ability to predict these emotions, the unused CK+ dataset, could potentially help address this issue.\n\nFuture iterations of the system could greatly benefit from incorporating components that utilize signals indicative of emotional states, such as heart rate variability (HRV), galvanic skin response (GSR), electroencephalography (EEG), and electrocardiography (ECG) to provide more profound insights into emotional states. However, due to financial limitations, these modalities are not incorporated into the present version of the system. The proposed future direction seeks to enhance the reliability and robustness of the system further.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Model",
      "text": "",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The architecture",
      "page": 4
    },
    {
      "caption": "Figure 1: System architecture diagram of the multimodal",
      "page": 5
    },
    {
      "caption": "Figure 2: illustrates the RAF-DB model's learning dynamics. The",
      "page": 5
    },
    {
      "caption": "Figure 2: Training and validation accuracy and loss curves over epochs",
      "page": 5
    },
    {
      "caption": "Figure 2: provides a visual representation of",
      "page": 5
    },
    {
      "caption": "Figure 2: Training and validation accuracy and loss curves over epochs",
      "page": 5
    },
    {
      "caption": "Figure 3: illustrates the convergence of loss and the",
      "page": 5
    },
    {
      "caption": "Figure 3: Training and validation accuracy and loss curves over epochs",
      "page": 5
    },
    {
      "caption": "Figure 4: , do not indicate",
      "page": 6
    },
    {
      "caption": "Figure 5: , provides an overview of the collective classification",
      "page": 6
    },
    {
      "caption": "Figure 4: Summarised model performance for classification",
      "page": 6
    },
    {
      "caption": "Figure 5: Fused confusion matrix of the of the multimodal",
      "page": 6
    },
    {
      "caption": "Figure 6: provides a",
      "page": 6
    },
    {
      "caption": "Figure 6: Pie-chart diagrams of the demographic composition of",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "GoEmotions": "ECD",
          "58,009 (9,716)": "5,934 (2,688)",
          "28 (6)": "3 (6)"
        },
        {
          "GoEmotions": "ED",
          "58,009 (9,716)": "393,822 (382,256)",
          "28 (6)": "6 (6)"
        },
        {
          "GoEmotions": "Sentiment140",
          "58,009 (9,716)": "1,600,000 (416,808)",
          "28 (6)": "3 (6)"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 71: 15%. Recognition of spoken language was the Surprise 43 9 82.70%",
      "data": [
        {
          "F\nacial \nExpression  \nRecognition": "",
          "Anger": "Disgust",
          "48": "36",
          "4": "16",
          "92.31%": "73.08%"
        },
        {
          "F\nacial \nExpression  \nRecognition": "",
          "Anger": "Fear",
          "48": "32",
          "4": "20",
          "92.31%": "61.54%"
        },
        {
          "F\nacial \nExpression  \nRecognition": "",
          "Anger": "Happiness",
          "48": "52",
          "4": "0",
          "92.31%": "100%"
        },
        {
          "F\nacial \nExpression  \nRecognition": "",
          "Anger": "Neutral",
          "48": "44",
          "4": "8",
          "92.31%": "84.62%"
        },
        {
          "F\nacial \nExpression  \nRecognition": "",
          "Anger": "Sadness",
          "48": "47",
          "4": "5",
          "92.31%": "90.38%"
        },
        {
          "F\nacial \nExpression  \nRecognition": "",
          "Anger": "Surprise",
          "48": "43",
          "4": "9",
          "92.31%": "82.70%"
        },
        {
          "F\nacial \nExpression  \nRecognition": "",
          "Anger": "Overall",
          "48": "302",
          "4": "62",
          "92.31%": "82.97%"
        },
        {
          "F\nacial \nExpression  \nRecognition": "B\nody \nMovement        \nAnalysis",
          "Anger": "Low",
          "48": "49",
          "4": "3",
          "92.31%": "94.23%"
        },
        {
          "F\nacial \nExpression  \nRecognition": "",
          "Anger": "Moderate",
          "48": "48",
          "4": "4",
          "92.31%": "92.31%"
        },
        {
          "F\nacial \nExpression  \nRecognition": "",
          "Anger": "High",
          "48": "51",
          "4": "1",
          "92.31%": "98.08%"
        },
        {
          "F\nacial \nExpression  \nRecognition": "",
          "Anger": "Overall",
          "48": "148",
          "4": "8",
          "92.31%": "94.72%"
        },
        {
          "F\nacial \nExpression  \nRecognition": "S\npeech \nRecognition",
          "Anger": "Anger",
          "48": "49",
          "4": "3",
          "92.31%": "94.23%"
        },
        {
          "F\nacial \nExpression  \nRecognition": "",
          "Anger": "Disgust",
          "48": "42",
          "4": "10",
          "92.31%": "80.77%"
        },
        {
          "F\nacial \nExpression  \nRecognition": "",
          "Anger": "Fear",
          "48": "48",
          "4": "4",
          "92.31%": "92.31%"
        },
        {
          "F\nacial \nExpression  \nRecognition": "",
          "Anger": "Happiness",
          "48": "49",
          "4": "3",
          "92.31%": "94.23%"
        },
        {
          "F\nacial \nExpression  \nRecognition": "",
          "Anger": "Neutral",
          "48": "43",
          "4": "9",
          "92.31%": "82.70%"
        },
        {
          "F\nacial \nExpression  \nRecognition": "",
          "Anger": "Sadness",
          "48": "44",
          "4": "8",
          "92.31%": "84.62%"
        },
        {
          "F\nacial \nExpression  \nRecognition": "",
          "Anger": "Surprise",
          "48": "37",
          "4": "15",
          "92.31%": "71.15%"
        },
        {
          "F\nacial \nExpression  \nRecognition": "",
          "Anger": "Overall",
          "48": "312",
          "4": "52",
          "92.31%": "85.71%"
        },
        {
          "F\nacial \nExpression  \nRecognition": "S\npoken \nLanguage \nRecognition",
          "Anger": "Anger",
          "48": "49",
          "4": "3",
          "92.31%": "94.23%"
        },
        {
          "F\nacial \nExpression  \nRecognition": "",
          "Anger": "Disgust",
          "48": "50",
          "4": "2",
          "92.31%": "96.15%"
        },
        {
          "F\nacial \nExpression  \nRecognition": "",
          "Anger": "Fear",
          "48": "50",
          "4": "2",
          "92.31%": "96.15%"
        },
        {
          "F\nacial \nExpression  \nRecognition": "",
          "Anger": "Happiness",
          "48": "51",
          "4": "1",
          "92.31%": "98.08%"
        },
        {
          "F\nacial \nExpression  \nRecognition": "",
          "Anger": "Neutral",
          "48": "25",
          "4": "27",
          "92.31%": "46.15%"
        },
        {
          "F\nacial \nExpression  \nRecognition": "",
          "Anger": "Sadness",
          "48": "49",
          "4": "3",
          "92.31%": "94.23%"
        },
        {
          "F\nacial \nExpression  \nRecognition": "",
          "Anger": "Surprise",
          "48": "45",
          "4": "7",
          "92.31%": "86.54%"
        },
        {
          "F\nacial \nExpression  \nRecognition": "",
          "Anger": "Overall",
          "48": "319",
          "4": "45",
          "92.31%": "87.64%"
        },
        {
          "F\nacial \nExpression  \nRecognition": "M\nultimodal \nSystem \nIntegration",
          "Anger": "Anger",
          "48": "51",
          "4": "1",
          "92.31%": "98.08%"
        },
        {
          "F\nacial \nExpression  \nRecognition": "",
          "Anger": "Disgust",
          "48": "50",
          "4": "2",
          "92.31%": "96.15%"
        },
        {
          "F\nacial \nExpression  \nRecognition": "",
          "Anger": "Fear",
          "48": "50",
          "4": "2",
          "92.31%": "96.15%"
        },
        {
          "F\nacial \nExpression  \nRecognition": "",
          "Anger": "Happiness",
          "48": "52",
          "4": "0",
          "92.31%": "100%"
        },
        {
          "F\nacial \nExpression  \nRecognition": "",
          "Anger": "Neutral",
          "48": "48",
          "4": "4",
          "92.31%": "92.31%"
        },
        {
          "F\nacial \nExpression  \nRecognition": "",
          "Anger": "Sadness",
          "48": "51",
          "4": "1",
          "92.31%": "98.08%"
        },
        {
          "F\nacial \nExpression  \nRecognition": "",
          "Anger": "Surprise",
          "48": "49",
          "4": "3",
          "92.31%": "94.23%"
        },
        {
          "F\nacial \nExpression  \nRecognition": "",
          "Anger": "Overall",
          "48": "351",
          "4": "13",
          "92.31%": "96.43%"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "F\nER-2013": "",
          "Anger": "Disgust",
          "54%": "52%"
        },
        {
          "F\nER-2013": "",
          "Anger": "Fear",
          "54%": "50%"
        },
        {
          "F\nER-2013": "",
          "Anger": "Happy",
          "54%": "88%"
        },
        {
          "F\nER-2013": "",
          "Anger": "Neutral",
          "54%": "67%"
        },
        {
          "F\nER-2013": "",
          "Anger": "Sadness",
          "54%": "33%"
        },
        {
          "F\nER-2013": "",
          "Anger": "Surprise",
          "54%": "76%"
        },
        {
          "F\nER-2013": "",
          "Anger": "Overall",
          "54%": "60%"
        },
        {
          "F\nER-2013": "R\nAF-DB",
          "Anger": "Anger",
          "54%": "83%"
        },
        {
          "F\nER-2013": "",
          "Anger": "Disgust",
          "54%": "70%"
        },
        {
          "F\nER-2013": "",
          "Anger": "Fear",
          "54%": "56%"
        },
        {
          "F\nER-2013": "",
          "Anger": "Happy",
          "54%": "94%"
        },
        {
          "F\nER-2013": "",
          "Anger": "Neutral",
          "54%": "79%"
        },
        {
          "F\nER-2013": "",
          "Anger": "Sadness",
          "54%": "79%"
        },
        {
          "F\nER-2013": "",
          "Anger": "Surprise",
          "54%": "83%"
        },
        {
          "F\nER-2013": "",
          "Anger": "Overall",
          "54%": "77.71%"
        },
        {
          "F\nER-2013": "C\nK+",
          "Anger": "Anger",
          "54%": "100%"
        },
        {
          "F\nER-2013": "",
          "Anger": "Disgust",
          "54%": "100%"
        },
        {
          "F\nER-2013": "",
          "Anger": "Fear",
          "54%": "99%"
        },
        {
          "F\nER-2013": "",
          "Anger": "Happy",
          "54%": "98%"
        },
        {
          "F\nER-2013": "",
          "Anger": "Neutral",
          "54%": "100%"
        },
        {
          "F\nER-2013": "",
          "Anger": "Sadness",
          "54%": "100%"
        },
        {
          "F\nER-2013": "",
          "Anger": "Surprise",
          "54%": "100%"
        },
        {
          "F\nER-2013": "",
          "Anger": "Overall",
          "54%": "99.57%"
        },
        {
          "F\nER-2013": "F\nER-2013 \n& RAF-\nDB \nCombined",
          "Anger": "Anger",
          "54%": "60%"
        },
        {
          "F\nER-2013": "",
          "Anger": "Disgust",
          "54%": "57%"
        },
        {
          "F\nER-2013": "",
          "Anger": "Fear",
          "54%": "54%"
        },
        {
          "F\nER-2013": "",
          "Anger": "Happy",
          "54%": "90%"
        },
        {
          "F\nER-2013": "",
          "Anger": "Neutral",
          "54%": "70%"
        },
        {
          "F\nER-2013": "",
          "Anger": "Sadness",
          "54%": "64%"
        },
        {
          "F\nER-2013": "",
          "Anger": "Surprise",
          "54%": "78%"
        },
        {
          "F\nER-2013": "",
          "Anger": "Overall",
          "54%": "67.57%"
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Clinical judgment and decision making",
      "authors": [
        "H Garb"
      ],
      "year": "2005",
      "venue": "Annu. Rev. Clin. Psychol",
      "doi": "10.1146/annurev.clinpsy.1.102803.143810"
    },
    {
      "citation_id": "2",
      "title": "Recognizing and reducing cognitive bias in clinical and forensic neurology",
      "authors": [
        "S Satya-Murti",
        "J Lockhart"
      ],
      "year": "2015",
      "venue": "Neurol. Clin. Pract",
      "doi": "10.1212/CPJ.0000000000000181"
    },
    {
      "citation_id": "3",
      "title": "Substance Abuse and Mental Health Services Administration (US)",
      "year": "2001",
      "venue": "Substance Abuse and Mental Health Services Administration (US)"
    },
    {
      "citation_id": "4",
      "title": "Deep Facial Expression Recognition: A Survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2020.2981446"
    },
    {
      "citation_id": "5",
      "title": "Facial Expression Recognition Using Facial Movement Features",
      "authors": [
        "L Zhang",
        "D Tjondronegoro"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/T-AFFC.2011.13"
    },
    {
      "citation_id": "6",
      "title": "Face Expression Recognition and Analysis: The State of the Art",
      "authors": [
        "V Bettadapura"
      ],
      "year": "2012",
      "venue": "arXiv",
      "doi": "10.48550/arXiv.1203.6722"
    },
    {
      "citation_id": "7",
      "title": "The first facial expression recognition and analysis challenge",
      "authors": [
        "M Valstar",
        "B Jiang",
        "M Mehu",
        "M Pantic",
        "K Scherer"
      ],
      "year": "2011",
      "venue": "2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG)",
      "doi": "10.1109/FG.2011.5771374"
    },
    {
      "citation_id": "8",
      "title": "Methods for Facial Expression Recognition with Applications in Challenging Situations",
      "authors": [
        "A Pise",
        "M Alqahtani",
        "P Verma",
        "D Karras",
        "A Halifa"
      ],
      "year": "2022",
      "venue": "Comput. Intell. Neurosci",
      "doi": "10.1155/2022/9261438"
    },
    {
      "citation_id": "9",
      "title": "A Comprehensive Review of Speech Emotion Recognition Systems",
      "authors": [
        "T Wani",
        "T Gunawan",
        "S Qadri",
        "M Kartiwi",
        "E Ambikairajah"
      ],
      "year": "2021",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2021.3068045"
    },
    {
      "citation_id": "10",
      "title": "Emotion Recognition from Speech Signals using Excitation Source and Spectral Features",
      "authors": [
        "A Choudhury",
        "A Ghosh",
        "R Pandey",
        "S Barman"
      ],
      "year": "2018",
      "venue": "2018 IEEE Applied Signal Processing Conference (ASPCON)",
      "doi": "10.1109/ASPCON.2018.8748626"
    },
    {
      "citation_id": "11",
      "title": "Learning Salient Features for Speech Emotion Recognition Using Convolutional Neural Networks",
      "authors": [
        "Q Mao",
        "M Dong",
        "Z Huang",
        "Y Zhan"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Multimedia",
      "doi": "10.1109/TMM.2014.2360798"
    },
    {
      "citation_id": "12",
      "title": "LSTM Based Feature Learning and CNN Based Classification for Speech Emotion Recognition",
      "authors": [
        "H Kalra"
      ],
      "year": "2023",
      "venue": "2023 International Conference on Data Science and Network Security (ICDSNS)",
      "doi": "10.1109/ICDSNS58469.2023.10244802"
    },
    {
      "citation_id": "13",
      "title": "Emotion Detection using Natural Language Processing",
      "authors": [
        "D Sakethnath",
        "H Kaur",
        "A Singh"
      ],
      "year": "2022",
      "venue": "2022 5th International Conference on Contemporary Computing and Informatics (IC3I)",
      "doi": "10.1109/IC3I56241.2022.10072841"
    },
    {
      "citation_id": "14",
      "title": "Multi-Class Twitter Emotion Classification: A New Approach",
      "authors": [
        "R Balabantaray",
        "M Mohammad",
        "N Sharma"
      ],
      "year": "2012",
      "venue": "Int. J. Appl. Inf. Syst",
      "doi": "10.5120/ijais12-450651"
    },
    {
      "citation_id": "15",
      "title": "Multi-class sentiment analysis on twitter: Classification performance and challenges",
      "authors": [
        "M Bouazizi",
        "T Ohtsuki"
      ],
      "year": "2019",
      "venue": "Big Data Mining and Analytics",
      "doi": "10.26599/BDMA.2019.9020002"
    },
    {
      "citation_id": "16",
      "title": "Multimodal Emotion Recognition Using Deep Learning Techniques",
      "authors": [
        "Jerald James",
        "L Jacob"
      ],
      "year": "2022",
      "venue": "2022 4th International Conference on Advances in Computing, Communication Control and Networking (ICAC3N)",
      "doi": "10.1109/ICAC3N56670.2022.10074512"
    },
    {
      "citation_id": "17",
      "title": "Multi-Modal Emotion recognition on IEMOCAP Dataset using Deep Learning",
      "authors": [
        "S Tripathi",
        "S Tripathi",
        "H Beigi"
      ],
      "year": "2018",
      "venue": "arXiv",
      "doi": "10.48550/arXiv.1804.05788"
    },
    {
      "citation_id": "18",
      "title": "FAF: A novel multimodal emotion recognition approach integrating face, body and text",
      "authors": [
        "Z Fang",
        "A He",
        "Q Yu",
        "B Gao",
        "W Ding",
        "T Zhang",
        "L Ma"
      ],
      "year": "2022",
      "venue": "arXiv",
      "doi": "10.48550/arXiv.2211.15425"
    },
    {
      "citation_id": "19",
      "title": "Multimodal mixed emotion detection",
      "authors": [
        "A Patwardhan"
      ],
      "year": "2017",
      "venue": "2017 2nd International Conference on Communication and Electronics Systems (ICCES)",
      "doi": "10.1109/CESYS.2017.8321250"
    },
    {
      "citation_id": "20",
      "title": "Multimodal Emotion Recognition using Deep Learning",
      "authors": [
        "S Abdullah",
        "S Ameen",
        "M Sadeeq",
        "S Zeebaree"
      ],
      "year": "2021",
      "venue": "J. Adv. Sci. Technol. Trends (JASTT)",
      "doi": "10.38094/jastt20291"
    },
    {
      "citation_id": "21",
      "title": "Exploring Fusion Methods for Multimodal Emotion Recognition with Missing Data",
      "authors": [
        "J Wagner",
        "E Andre",
        "F Lingenfelser",
        "J Kim"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/T-AFFC.2011.12"
    },
    {
      "citation_id": "22",
      "title": "Face Expression Recognition, GitHub repository",
      "authors": [
        "K Kraack"
      ],
      "year": "2024",
      "venue": "Face Expression Recognition, GitHub repository"
    },
    {
      "citation_id": "23",
      "title": "Body Movement Analysis, GitHub repository",
      "authors": [
        "K Kraack"
      ],
      "year": "2024",
      "venue": "Body Movement Analysis, GitHub repository"
    },
    {
      "citation_id": "24",
      "title": "Voice Emotion Detection, GitHub repository",
      "authors": [
        "K Kraack"
      ],
      "year": "2024",
      "venue": "Voice Emotion Detection, GitHub repository"
    },
    {
      "citation_id": "25",
      "title": "NLP Emotion Recognition, GitHub repository",
      "authors": [
        "K Kraack"
      ],
      "year": "2024",
      "venue": "NLP Emotion Recognition, GitHub repository"
    },
    {
      "citation_id": "26",
      "title": "Multimodal Emotion Detection, GitHub repository",
      "authors": [
        "K Kraack"
      ],
      "year": "2024",
      "venue": "Multimodal Emotion Detection, GitHub repository"
    },
    {
      "citation_id": "27",
      "title": "An ingroup disadvantage in recognizing micro-expressions",
      "authors": [
        "Q Wu",
        "K Peng",
        "Y Xie",
        "Y Lai",
        "X Liu",
        "Z Zhao"
      ],
      "year": "2022",
      "venue": "Front. Psychol",
      "doi": "10.3389/fpsyg.2022.1050068"
    },
    {
      "citation_id": "28",
      "title": "Life Events and Mental Status: A Longitudinal Study",
      "authors": [
        "J Myers",
        "J Lindenthal",
        "M Pepper",
        "D Ostrander"
      ],
      "year": "1972",
      "venue": "Journal of Health and Social Behavior",
      "doi": "10.2307/2136832"
    },
    {
      "citation_id": "29",
      "title": "Compassion fatigue and global compassion fatigue in practitioner psychologists: A qualitative study",
      "authors": [
        "K Stevens",
        "M Al-Abbadey"
      ],
      "year": "2024",
      "venue": "Curr. Psychol",
      "doi": "10.1007/s12144-023-04908-3"
    },
    {
      "citation_id": "30",
      "title": "Fatigue among clinicians and the safety of patients",
      "authors": [
        "D Gaba",
        "S Howard"
      ],
      "year": "2002",
      "venue": "N. Engl. J. Med",
      "doi": "10.1056/NEJMsa020846"
    },
    {
      "citation_id": "31",
      "title": "Breaking the taboo: Eight Swedish clinical psychologists' experiences of compassion fatigue",
      "authors": [
        "M Harling",
        "E Hgman",
        "E Schad"
      ],
      "year": "2020",
      "venue": "Int. J. Qual. Stud. Health Well-Being",
      "doi": "10.1080/17482631.2020.1785610"
    },
    {
      "citation_id": "32",
      "title": "Anatomy of an epidemic: Psychiatric drugs and the astonishing rise of mental illness in America",
      "authors": [
        "R Whitaker"
      ],
      "year": "2005",
      "venue": "Ethical Hum. Psychol. Psychiatry"
    },
    {
      "citation_id": "33",
      "title": "Diagnostic error in mental health: A review",
      "authors": [
        "A Bradford",
        "A Meyer",
        "S Khan",
        "T Giardina",
        "H Singh"
      ],
      "year": "2024",
      "venue": "BMJ Qual. Saf",
      "doi": "10.1136/bmjqs-2023-016996"
    },
    {
      "citation_id": "34",
      "title": "Overdiagnosis of mental disorders in children and adolescents (in developed countries)",
      "authors": [
        "E Merten",
        "J Cwik",
        "J Margraf"
      ],
      "year": "2017",
      "venue": "Child Adolesc. Psychiatry Ment. Health",
      "doi": "10.1186/s13034-016-0140-5"
    },
    {
      "citation_id": "35",
      "title": "Deep convolution network based emotion analysis towards mental health care",
      "authors": [
        "Z Fei",
        "E Yang",
        ".-U Li",
        "S Butler",
        "W Ijomah",
        "X Li",
        "H Zhou"
      ],
      "year": "2020",
      "venue": "Neurocomputing",
      "doi": "10.1016/j.neucom.2020.01.034"
    },
    {
      "citation_id": "36",
      "title": "Spoken Language Derived Measures for Detecting Mild Cognitive Impairment",
      "authors": [
        "B Roark",
        "M Mitchell",
        "J. -P Hosom",
        "K Hollingshead",
        "J Kaye"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing",
      "doi": "10.1109/TASL.2011.2112351"
    },
    {
      "citation_id": "37",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "I Goodfellow",
        "D Erhan",
        "P Carrier",
        "A Courville",
        "M Mirza",
        "B Hamner",
        "W Cukierski",
        "Y Tang",
        "D Thaler",
        "D.-H Lee",
        "Y Zhou",
        "C Ramaiah",
        "F Feng",
        "R Li",
        "X Wang",
        "D Athanasakis",
        "J Shawe-Taylor",
        "M Milakov",
        "J Park",
        "R Ionescu",
        "M Popescu",
        "C Grozea",
        "J Bergstra",
        "J Xie",
        "L Romaszko",
        "B Xu",
        "Z Chuang",
        "Y Bengio"
      ],
      "year": "2015",
      "venue": "Neural Networks",
      "doi": "10.1016/j.neunet.2014.09.005"
    },
    {
      "citation_id": "38",
      "title": "Reliable Crowdsourcing and Deep Locality-Preserving Learning for Unconstrained Facial Expression Recognition",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Image Processing",
      "doi": "10.1109/TIP.2018.2868382"
    },
    {
      "citation_id": "39",
      "title": "The Extended Cohn-Kanade Dataset (CK+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "Z Ambadar",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition -Workshops",
      "doi": "10.1109/CVPRW.2010.5543262"
    },
    {
      "citation_id": "40",
      "title": "OpenCV: Open Source Computer Vision Library",
      "authors": [
        "Opencv Development"
      ],
      "venue": "OpenCV: Open Source Computer Vision Library"
    },
    {
      "citation_id": "41",
      "title": "TensorFlow: An open-source machine learning framework",
      "authors": [
        "Tensorflow"
      ],
      "venue": "TensorFlow: An open-source machine learning framework"
    },
    {
      "citation_id": "42",
      "title": "dlib: A toolkit for machine learning",
      "authors": [
        "D King"
      ],
      "venue": "dlib: A toolkit for machine learning"
    },
    {
      "citation_id": "43",
      "title": "MediaPipe's pose estimation model",
      "authors": [
        "Mediapipe"
      ],
      "venue": "MediaPipe's pose estimation model"
    },
    {
      "citation_id": "44",
      "title": "CREMA-D: Crowd-Sourced Emotional Multimodal Actors Dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2014.2336244"
    },
    {
      "citation_id": "45",
      "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PLOS ONE",
      "doi": "10.1371/journal.pone.0196391"
    },
    {
      "citation_id": "46",
      "title": "Surrey Audio-Visual Expressed Emotion (SAVEE) Database",
      "authors": [
        "P Jackson",
        "S Haq"
      ],
      "year": "2015",
      "venue": "Surrey Audio-Visual Expressed Emotion (SAVEE) Database"
    },
    {
      "citation_id": "47",
      "title": "Toronto Emotional Speech Set (TESS)",
      "authors": [
        "K Dupuis",
        "M Pichora-Fuller"
      ],
      "year": "2010",
      "venue": "Toronto Emotional Speech Set (TESS)"
    },
    {
      "citation_id": "48",
      "title": "Emotional voice conversion: theory, databases, and ESD",
      "authors": [
        "K Zhou",
        "B Sisman",
        "R Liu",
        "H Li"
      ],
      "venue": "Emotional voice conversion: theory, databases, and ESD"
    },
    {
      "citation_id": "49",
      "title": "Seen and Unseen Emotional Style Transfer for Voice Conversion with A New Emotional Speech Dataset",
      "authors": [
        "K Zhou",
        "B Sisman",
        "R Liu",
        "H Li"
      ],
      "year": "2021",
      "venue": "ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "doi": "10.1109/ICASSP39728.2021.9413391"
    },
    {
      "citation_id": "50",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversation",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "R Mihalcea",
        "E Cambria"
      ],
      "year": "2018",
      "venue": "MELD: A multimodal multi-party dataset for emotion recognition in conversation"
    },
    {
      "citation_id": "51",
      "title": "EmotionLines: An emotion corpus of multi-party conversations",
      "authors": [
        "S Chen",
        "C Hsu",
        "C Kuo",
        "L Ku"
      ],
      "year": "2018",
      "venue": "EmotionLines: An emotion corpus of multi-party conversations",
      "arxiv": "arXiv:1802.08379"
    },
    {
      "citation_id": "52",
      "title": "GoEmotions: A dataset of fine-grained emotions",
      "authors": [
        "D Demszky",
        "D Movshovitz-Attias",
        "J Ko",
        "A Cowen",
        "G Nemade",
        "S Ravi"
      ],
      "year": "2020",
      "venue": "Proc. 58th Annu. Meet. Assoc. Comput. Linguistics (ACL)",
      "doi": "10.18653/v1/2020.acl-main.59"
    },
    {
      "citation_id": "53",
      "title": "Emotion dataset for NLP tutorial",
      "authors": [
        "A Ibrahim"
      ],
      "venue": "Emotion dataset for NLP tutorial"
    },
    {
      "citation_id": "54",
      "title": "Elgiriye withana, 2024",
      "venue": "Elgiriye withana, 2024",
      "doi": "10.34740/KAGGLE/DSV/7563141"
    },
    {
      "citation_id": "55",
      "title": "Twitter sentiment classification using distant supervision",
      "authors": [
        "A Go",
        "R Bhayani",
        "L Huang"
      ],
      "year": "2009",
      "venue": "CS224N Project Report"
    },
    {
      "citation_id": "56",
      "title": "Whisper: A general-purpose speech recognition model",
      "authors": [
        "Openai"
      ],
      "venue": "Whisper: A general-purpose speech recognition model"
    }
  ]
}