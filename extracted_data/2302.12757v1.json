{
  "paper_id": "2302.12757v1",
  "title": "Ensemble Knowledge Distillation Of Self-Supervised Speech Models",
  "published": "2023-02-24T17:15:39Z",
  "authors": [
    "Kuan-Po Huang",
    "Tzu-hsun Feng",
    "Yu-Kuan Fu",
    "Tsu-Yuan Hsu",
    "Po-Chieh Yen",
    "Wei-Cheng Tseng",
    "Kai-Wei Chang",
    "Hung-yi Lee"
  ],
  "keywords": [
    "Self-supervised Learning",
    "Ensemble Knowledge Distillation",
    "SUPERB",
    "Distortions"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Distilled self-supervised models have shown competitive performance and efficiency in recent years. However, there is a lack of experience in jointly distilling multiple self-supervised speech models. In our work, we performed Ensemble Knowledge Distillation (EKD) on various self-supervised speech models such as HuBERT, RobustHuBERT, and WavLM. We tried two different aggregation techniques, layerwise-average and layerwise-concatenation, to the representations of different teacher models and found that the former was more effective. On top of that, we proposed a multiple prediction head method for student models to predict different layer outputs of multiple teacher models simultaneously. The experimental results show that our method improves the performance of the distilled models on four downstream speech processing tasks, Phoneme Recognition, Speaker Identification, Emotion Recognition, and Automatic Speech Recognition in the hidden-set track of the SUPERB benchmark.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Recently, adopting self-supervised learned (SSL) models  [1]  has become a trend in speech processing. By leveraging a large amount of unlabeled data,  [2] [3] [4]  were able to achieve great performance on a variety of downstream speech processing tasks. As well as that, there is also some work focusing on the domain-shift problem of SSL models. A common out-of-domain scenario occurs when the testing data contains noises unseen during training. To overcome this problem,  [4] [5] [6] [7]  proposed various kinds of methods to enable models to produce robust representations of distorted speech.\n\nDespite the fact that SSL models are able to provide useful representations for various downstream speech processing tasks, these large models are wide and deep, making them inefficient for ondevice speech applications. A straightforward method to reduce model size is to distill one of these models to obtain a compressed model with comparable performance. DistilHuBERT  [8] , a small student model designed by reducing the depth of HuBERT  [3] , is trained by knowledge distillation with limited performance degradation compared to the HuBERT teacher model. Based on DistilHu-BERT, some previous works  [9, 10]  studied the variant of the student models regarding the layer width and model depth, while some others  [11]  developed methods for enhancing the noise-robustness of knowledge-distilled models.\n\nEqual contribution.\n\nHowever, these models only leverage the advantages of a single SSL model. An intuitive way to combine knowledge of different models is to utilize the representations of different distilled models during downstream training. This method is inapplicable since it requires additional parameters and also degrades performance in some cases. To leverage the strengths of each model while constraining the model size, a reasonable method is to adopt multiple SSL speech models as teachers during the knowledge distillation process. We hypothesize that having student models learning from multiple teachers helps acquire knowledge in a more general aspect. For example, distilling a model with high performance for clean speech and a model being robust to noise may result in a student model that performs well in both clean and noisy environments.\n\nThough incorporating the concept of model ensembling into the teacher-student learning framework sounds intuitive, the aggregation method of the representations of each layer from different teacher models should be carefully considered. Recently, previous work showed that training speech recognition models with the concatenation of HuBERT  [3]  and WavLM  [4]  representations benefit speech recognition  [12] . Unfortunately, this is not the case in a knowledge distillation scheme. We found that concatenating is less effective than averaging the representations of different models. On top of that, there are existing works that ensembled multiple supervised trained neural networks  [13] [14] [15] [16]  to improve downstream tasks. However, some of their methods rely on downstream results to decide the weights in the weighted-sum process of different model outputs in order to achieve the best performance. For SSL speech models, determining the weights for different model outputs according to various kinds of speech processing tasks cannot be easily achieved and is not reasonable since the prior knowledge of the types of downstream tasks should remain unknown during the pre-training stage. Different from previous work, we proposed to utilize multiple sets of prediction heads to predict different hidden layer outputs of multiple teacher models during knowledge distillation.\n\nOverall, we proposed to perform Ensemble Knowledge Distillation (EKD) to multiple SSL speech models to improve the performance of distilled models on different downstream speech processing tasks. Instead of averaging or concatenating the representations of different teacher models, we found that predicting each teacher model output with individual sets of prediction heads yields the best performance on four downstream tasks. Surprisingly, models trained with our proposed methods even improved downstream performance in noisy environments unseen during training. With our proposed method, it is no longer required to calculate a weighted sum of the representations of different teacher models and gives an insight into how ensemble learning can be conducted on SSL models. Most importantly, our proposed EKD method is downstream-independent, arXiv:2302.12757v1 [eess.AS] 24 Feb 2023 and does not have to be re-trained whenever there are new downstream tasks involved.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ensemble Knowledge Distillation (Ekd)",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Knowledge Distillation",
      "text": "Performing knowledge distillation refers to the process of distilling a teacher model T into a smaller version. This is done by having a smaller student network S to learn from the output of the teacher model. In DistilHuBERT  [8] , the last layer output z ∈ R t×D S of the student model is transformed by prediction heads\n\nthe teacher model. The role of the prediction heads is to transform the hidden layer output of the student to the same dimension as the hidden layer output of the teacher. The notation t denotes the number of timesteps, while DS and DT denote the feature dimension of z and h T i , respectively. The objective of the teacher-student framework of DistilHuBERT consists of an L1 loss and a cosine similarity loss as shown in Eq. (  1 ), where cossim 1 is the cosine similarity operation, and σ is the sigmoid activation.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Ensemble Of Teacher Models",
      "text": "Given a list of different teacher models, {T1, • • • , TM }, where M is the number of teacher models, to enable a student model to learn the outputs of different teacher models, some techniques will be required to define the output relationship between the teacher and student models. To adopt the same setting of DistilHuBERT having only one set of prediction heads to predict some of the hidden layer outputs of the teacher model, we aggregate the representations extracted from different teachers in a layerwise-averaged or a layerwise-concatenated manner. The distillation framework with aggregation of teacher representations is demonstrated in the left illustration of Fig.  1 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Layerwise-Averaged Representations",
      "text": "Layerwise-averaged representations are generated by calculating the mean of the i th hidden layer output h Tm i of each teacher model to form layerwise-averaged teacher representations h\n\nFor knowledge distillation, the objective for the student model to learn layerwise-averaged representations is shown in Eq. (3).\n\n1 The cosine similarity operation is calculated by averaging the cosine similarity of the two feature vectors for each timestep.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Layerwise-Concatenated Representations",
      "text": "Layerwise-concatenated representations are generated by concatenating the i th hidden layer output h T i for each of the m th teacher model to form layerwise-concatenated teacher representations ĥTm i ∈ R t×D T as shown in Eq. (  4 ), where D T = DT * M .\n\nFor knowledge distillation, the objective for the student model to learn layerwise-concatenated representations is shown in Eq. (  5 ). Note that the dimension of the prediction heads P ri ∈ R D S ×D T in this case are different from the previous cases since the dimension of the layerwise-concatenated representations is different.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Multiple Sets Of Prediction Heads",
      "text": "To keep the original form of the representations from each teacher model, we proposed to adopt multiple sets of prediction heads following the last transformer encoder layer of the student model. Under the single-teacher knowledge distillation scheme, only a set of prediction heads are required to predict different layers of a single teacher model. However, to simultaneously learn from multiple teacher models during knowledge distillation, multiple sets of prediction heads are needed.\n\nMultiple sets of prediction heads are adopted to aim at learning different teacher model outputs without having to aggregate the teacher representations. The illustration on the right in Fig.  1  demonstrates the architecture of the distillation framework containing multiple sets of prediction heads. The quantity of the sets is equal to the number of teacher models involved during EKD. The predictions for each teacher model are generated by transforming the last hidden layer output of the student model with separate sets of prediction heads. The objective of EKD when adopting different sets of prediction heads for each teacher model is shown in Eq.  (6) . h Sm i is the prediction of the m th set of prediction heads trying to predict the hidden layer output h Tm i of the m th teacher model.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Experimental Setup",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Superb Hidden-Set Track",
      "text": "From downstream evaluation, we report the results of tasks Phoneme Recognition (PR), Speaker Identification (SID), Emotion Recognition (ER), and Automatic Speech Recognition (ASR) in the hiddenset track of SUPERB challenge  [17] . This challenge aims to benchmark the generalizability of SSL speech models by adapting them to diverse speech processing tasks with lightweight downstream models appended. The datasets in the hidden-set track are all newly created by the challenge organizers and stand unseen for each task. For downstream training, this challenge allows a weighted sum of representations extracted from the hidden layers to serve as the input of the downstream models. Following the same fashion as the challenge, we calculate an overall score with respect to predefined reference values 2 . Downstream performance rankings are ranked according to this score.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Noisy Data",
      "text": "Teacher models in our work are pre-trained with various kinds of noisy speech data. During the downstream evaluation, we also tested the performance with speech utterances containing background noises. We elaborate on different noise corpora in the following.\n\nMusan  [18]  is an audio corpus containing audio samples of music, speech, and noises. The noise samples originate from the Freesound and Sound Bible database.\n\nWHAM!  [19]  is a dataset containing real ambient noise samples recorded in the San Francisco Bay Area. It is originally used to simulate noisy environments of overlapped speech.\n\nDNS  [20]  is a challenge that provides noise samples originating from Audio Set  [21] , Freesound, and DEMAND  [22]  database. Synthetic room impulse responses are also released for augmenting speech samples to simulate a reverberated environment. Noise samples provided in DNS are involved in the augmentation process in the pre-training stage of WavLM  [4] .\n\nCHiME3  [23]  refers to the third CHiME Challenge targeting speech recognition under real-world scenarios. The purpose of adopting CHiME3 is to construct a domain mismatch scenario during testing. Noises in this dataset are not involved during the pretraining stage of any of the self-supervised teacher models included in this work. For some speech processing tasks, it is difficult to find a corresponding noisy corpora. A simple way to obtain noisy testing data for downstream tasks is to add background noises to speech. Since we do not have access to the testing data in the hidden set, we consulted the SUPERB hidden-set committee and got permission to access the results of the testing sets of some tasks containing real-world background noises provided by the CHiME3 dataset. For Automatic Speech Recognition (ASR), since CHiME3 already provides a testing set for speech recognition recorded in real-world environments, we also report the results of this testing set (denoted as chime-real) in Table  1 . 2 Details for calculating the overall score are shown in https://superbbenchmark.org/challenge-slt2022/metrics.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Teacher Models",
      "text": "In our work, there are three different SSL teacher models involved.\n\nHuBERT  [3]  (HB), an abbreviation of Hidden-Unit BERT, is a self-supervised speech model trained to predict clustered features with masked inputs. In our experiments, we adopt the base variant pre-trained with 960 hours of LibriSpeech  [24] .\n\nRobustHuBERT  [11]  (RHB) is the distortion-robust version of HuBERT obtained by performing domain-adaptive pre-training  [11]  (DAPT) to HuBERT. The DAPT data originates from LibriSpeech but is distorted with background noises sampled from Musan, WHAM!, or Gaussian noise and augmented by applying reverberation, band rejection, or pitch-shifting.\n\nWavLM  [4]  (WL), a similar model compared to HuBERT, achieved state-of-the-art performance by including gated relative position bias in the transformer-based architecture and augmenting input data into noisy or overlapped speech. In our experiment, we adopt the base+ variant pre-trained with 94k hours of speech.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Training Details",
      "text": "For knowledge distillation 3 , 960 hours of LibriSpeech are used for pre-training. Though some teacher models are robust to distortions, no additional distortions are added to the pre-training data for EKD. The training hyper-parameters are similar to the original DistilHu-BERT. The student network architecture is the same as DistilHu-BERT except for the prediction heads under the multiple prediction head setting. To evaluate the models trained with EKD, we trained distilled versions of HuBERT, RobustHuBERT, and WavLM for comparison. For each model trained with EKD, their baselines are the single-teacher distilled versions of each teacher model involved in EKD. For downstream speech processing, the data configuration and hyper-parameters for training follow the SUPERB hidden-set track. Prediction heads are discarded during this stage to reduce parameter usage. The training and evaluation process were conducted by the SUPERB hidden-set committee.\n\nTable  1 . Evaluation results on clean testing sets and testing sets with CHiME3 background noise. DT. is the abbreviation of Distil. The column \"method\" specifies whether the hidden layer outputs of the teacher are layerwise-averaged (avg.), layerwise-concatenated (concat.), or predicted with multiple sets of prediction heads (multi. pred.). The baselines of our proposed models are the individually distilled versions of the teacher models involved in the ensembling process. The values that outperform their corresponding baselines are marked gray. The best performance values of each testing set are marked bold. The rank of each model represents the overall performance on the clean set of the four downstream speech processing tasks. PR (PER ↓ ) SID (Acc ↑ ) ER (Acc ↑ ) ASR (WER ↓ ) method # para. clean chime clean chime clean chime clean chime chime-real rank (a) DT. HB (baseline)  [",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Different Aggregation Methods",
      "text": "In Table  1 , (c)(d)(e) are models trained with EKD by having Hu-BERT and WavLM as their teachers. By comparing models (c) and (d), we discover that aggregating representations of the teacher in a layerwise-average manner tends to yield better performance than concatenating the representations. This may be due to the excessive length of representations being difficult to learn. By comparing model (c) to baseline models (a) and (b), we see that averaging the representations of HuBERT and WavLM improves the performance of SID and ER on the clean set, but degrades the performance of PR and ASR. This may indicate that the averaged representations from different models are not useful for contentbased tasks.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Multiple Prediction Heads",
      "text": "In Table  1 , we observe that model (e) outperforms models (c) and (d) on every task except for SID. This implies that having each set of prediction heads predict different teacher models is a better method compared to aggregating representations of the teacher in most cases.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Ensemble Of Distilled Models",
      "text": "The downstream results of model (j) are trained with the representations extracted from models (a)(b) and (f). From the rank, it is clear that utilizing representations of different distilled models improves downstream performance. However, the more distilled models used, the more parameters needed.\n\nBy comparing models (j) and (i), we discover that student models trained with EKD yield better performance than merely adopting representations of multiple individually-distilled student models. This is an inspiring result since the model trained with our proposed 3 Code modified from https://github.com/s3prl/s3prl. EKD method not only outperforms an ensemble of three distilled models but also requires only one-third of parameters.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Different Combinations Of Teacher Models",
      "text": "Models (e)(g)(h)(i) in Table  1  are all trained by performing EKD with multiple sets of prediction heads and have the same training configurations but with different combinations of teacher models.\n\nModel (g), trained to distill knowledge from HuBERT and Ro-bustHuBERT, outperforms baseline models (a) and (f) on every testing set except for the noisy testing set of PR. This suggests that models distilled from a teacher model and its domain-adaptive pretrained version not only gain robustness to noises but also gain improvement in clean environments for most of the tasks. Performing EKD shows great potential in this case for not degrading performance under clean and noisy settings.\n\nAmong models (e)(g)(h)(i), model (i) performs the best in overall performance on the clean testing sets. Since ensembling more models does not increase model parameters as long as the prediction heads are not used during downstream training, it is worth trying more combinations of high-performance teacher models for EKD.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In our work, we conclude that performing Ensemble Knowledge Distillation to SSL speech models has the potential for improving model performance with restricted size in both clean and noisy environments. Having each teacher model predicted with separate sets of prediction heads is the best method for student models during knowledge distillation. Our method is also able to enhance the robustness of compressed models by jointly distilling teacher models while some teachers are not robust. In the future, we will try to ensemble more models to obtain better generalizability for small and efficient SSL speech models, and also integrate other noise-robust techniques into the EKD process.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: 2.2.1. Layerwise-averaged representations",
      "page": 2
    },
    {
      "caption": "Figure 1: Illustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "However,\nthese models only leverage the advantages of a sin-"
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "gle SSL model. An intuitive way to combine knowledge of different"
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": ""
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "models is to utilize the representations of different distilled models"
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": ""
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "during downstream training. This method is inapplicable since it re-"
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": ""
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "quires additional parameters and also degrades performance in some"
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": ""
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "cases. To leverage the strengths of each model while constraining"
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": ""
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "the model size, a reasonable method is to adopt multiple SSL speech"
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": ""
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "models as teachers during the knowledge distillation process. We hy-"
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": ""
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "pothesize that having student models learning from multiple teachers"
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": ""
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "helps acquire knowledge in a more general aspect. For example, dis-"
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": ""
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "tilling a model with high performance for clean speech and a model"
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": ""
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "being robust\nto noise may result\nin a student model\nthat performs"
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": ""
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "well in both clean and noisy environments."
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": ""
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "Though incorporating the concept of model ensembling into the"
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": ""
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "teacher-student learning framework sounds intuitive, the aggregation"
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": ""
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "method of\nthe representations of each layer\nfrom different\nteacher"
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": ""
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "models\nshould be carefully considered.\nRecently, previous work"
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": ""
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "showed that training speech recognition models with the concatena-"
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "tion of HuBERT [3] and WavLM [4] representations beneﬁt speech"
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "recognition [12]. Unfortunately,\nthis is not\nthe case in a knowledge"
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "distillation scheme. We found that concatenating is less effective"
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "than averaging the representations of different models. On top of"
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": ""
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "that,\nthere are existing works\nthat ensembled multiple supervised"
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "trained neural networks [13–16] to improve downstream tasks. How-"
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": ""
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "ever, some of their methods rely on downstream results to decide the"
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": ""
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "weights in the weighted-sum process of different model outputs in"
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": ""
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "order to achieve the best performance. For SSL speech models, de-"
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": ""
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "termining the weights for different model outputs according to vari-"
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": ""
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "ous kinds of speech processing tasks cannot be easily achieved and"
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": ""
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "is not\nreasonable since the prior knowledge of\nthe types of down-"
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": ""
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "stream tasks should remain unknown during the pre-training stage."
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": ""
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "Different from previous work, we proposed to utilize multiple sets of"
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": ""
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "prediction heads to predict different hidden layer outputs of multiple"
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": ""
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "teacher models during knowledge distillation."
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": ""
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "Overall, we proposed to perform Ensemble Knowledge Distil-"
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "lation (EKD) to multiple SSL speech models to improve the perfor-"
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "mance of distilled models on different downstream speech process-"
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "ing tasks.\nInstead of averaging or concatenating the representations"
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "of different\nteacher models, we found that predicting each teacher"
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "model output with individual sets of prediction heads yields the best"
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "performance on four downstream tasks. Surprisingly, models trained"
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "with our proposed methods even improved downstream performance"
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "in noisy environments unseen during training. With our proposed"
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "method,\nit\nis no longer required to calculate a weighted sum of the"
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "representations of different teacher models and gives an insight into"
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "how ensemble learning can be conducted on SSL models. Most im-"
        },
        {
          "{f09922005, r10942095, r11942083, b08201047, b08901198, r09942094, f09921048, hungyilee}@ntu.edu.tw": "portantly, our proposed EKD method is downstream-independent,"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "and does not have to be re-trained whenever\nthere are new down-": "stream tasks involved.",
          "2.2.2.\nLayerwise-concatenated representations": ""
        },
        {
          "and does not have to be re-trained whenever\nthere are new down-": "",
          "2.2.2.\nLayerwise-concatenated representations": "Layerwise-concatenated\nrepresentations\nare\ngenerated\nby\ncon-"
        },
        {
          "and does not have to be re-trained whenever\nthere are new down-": "",
          "2.2.2.\nLayerwise-concatenated representations": "ith\ncatenating the\nhidden layer output hT\nfor\neach of\nthe mth"
        },
        {
          "and does not have to be re-trained whenever\nthere are new down-": "2. ENSEMBLE KNOWLEDGE DISTILLATION (EKD)",
          "2.2.2.\nLayerwise-concatenated representations": "teacher model\nto form layerwise-concatenated teacher\nrepresenta-"
        },
        {
          "and does not have to be re-trained whenever\nthere are new down-": "",
          "2.2.2.\nLayerwise-concatenated representations": "∈ Rt×D(cid:48)\ntions ˆhTm\nT as shown in Eq. (4), where D(cid:48)\nT = DT ∗ M .\ni"
        },
        {
          "and does not have to be re-trained whenever\nthere are new down-": "2.1. Knowledge distillation",
          "2.2.2.\nLayerwise-concatenated representations": ""
        },
        {
          "and does not have to be re-trained whenever\nthere are new down-": "",
          "2.2.2.\nLayerwise-concatenated representations": "hT\n(cid:107)hT2\n(cid:107) · · · (cid:107)hTM\n}\n(4)\ni = {hT1\ni\ni"
        },
        {
          "and does not have to be re-trained whenever\nthere are new down-": "Performing knowledge distillation refers to the process of distilling",
          "2.2.2.\nLayerwise-concatenated representations": ""
        },
        {
          "and does not have to be re-trained whenever\nthere are new down-": "a teacher model T into a smaller version. This is done by having",
          "2.2.2.\nLayerwise-concatenated representations": ""
        },
        {
          "and does not have to be re-trained whenever\nthere are new down-": "",
          "2.2.2.\nLayerwise-concatenated representations": "For knowledge distillation,\nthe objective for\nthe student model\nto"
        },
        {
          "and does not have to be re-trained whenever\nthere are new down-": "a smaller student network S to learn from the output of the teacher",
          "2.2.2.\nLayerwise-concatenated representations": ""
        },
        {
          "and does not have to be re-trained whenever\nthere are new down-": "",
          "2.2.2.\nLayerwise-concatenated representations": "learn layerwise-concatenated representations is shown in Eq.\n(5)."
        },
        {
          "and does not have to be re-trained whenever\nthere are new down-": "model. In DistilHuBERT [8], the last layer output z ∈ Rt×DS of the",
          "2.2.2.\nLayerwise-concatenated representations": ""
        },
        {
          "and does not have to be re-trained whenever\nthere are new down-": "",
          "2.2.2.\nLayerwise-concatenated representations": "T in\nNote that the dimension of the prediction heads P ri ∈ RDS ×D(cid:48)"
        },
        {
          "and does not have to be re-trained whenever\nthere are new down-": "student model\nis transformed by prediction heads P ri ∈ RDS ×DT",
          "2.2.2.\nLayerwise-concatenated representations": ""
        },
        {
          "and does not have to be re-trained whenever\nthere are new down-": "",
          "2.2.2.\nLayerwise-concatenated representations": "this case are different from the previous cases since the dimension"
        },
        {
          "and does not have to be re-trained whenever\nthere are new down-": "∈ Rt×DT\n∈\ninto hS\nto predict\nthe ith hidden layer output hT",
          "2.2.2.\nLayerwise-concatenated representations": ""
        },
        {
          "and does not have to be re-trained whenever\nthere are new down-": "",
          "2.2.2.\nLayerwise-concatenated representations": "of the layerwise-concatenated representations is different."
        },
        {
          "and does not have to be re-trained whenever\nthere are new down-": "Rt×DT\nof\nthe teacher model.\nThe role of\nthe prediction heads is",
          "2.2.2.\nLayerwise-concatenated representations": ""
        },
        {
          "and does not have to be re-trained whenever\nthere are new down-": "to transform the hidden layer output of the student\nto the same di-",
          "2.2.2.\nLayerwise-concatenated representations": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "teacher models. The illustration on the right illustrates how EKD is performed with multiple sets of prediction heads."
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "3.3. Teacher models\nFollowing the same fashion as the challenge, we calculate an"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "overall score with respect\nto predeﬁned reference values2. Down-"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "stream performance rankings are ranked according to this score."
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "In our work, there are three different SSL teacher models involved."
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "HuBERT [3]\n(HB), an abbreviation of Hidden-Unit BERT,\nis"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "3.2. Noisy data"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "a self-supervised speech model\ntrained to predict clustered features"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "with masked inputs.\nIn our experiments, we adopt\nthe base variant\nTeacher models in our work are pre-trained with various kinds of"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "pre-trained with 960 hours of LibriSpeech [24].\nnoisy speech data. During the downstream evaluation, we also tested"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "the\nperformance with\nspeech\nutterances\ncontaining\nbackground"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "RobustHuBERT [11] (RHB) is the distortion-robust version of"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "noises. We elaborate on different noise corpora in the following."
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "HuBERT obtained by performing domain-adaptive pre-training [11]"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "Musan [18]\nis an audio corpus containing audio samples of"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "(DAPT)\nto HuBERT. The DAPT data originates from LibriSpeech"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "music,\nspeech, and noises.\nThe noise samples originate from the"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "but\nis\ndistorted with\nbackground\nnoises\nsampled\nfrom Musan,"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "Freesound and Sound Bible database."
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "WHAM!,\nor Gaussian noise\nand augmented\nby\napplying rever-"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "WHAM! [19] is a dataset containing real ambient noise samples"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "beration, band rejection, or pitch-shifting."
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "recorded in the San Francisco Bay Area.\nIt\nis originally used to"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "WavLM [4]\n(WL),\na\nsimilar model\ncompared to HuBERT,"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "simulate noisy environments of overlapped speech."
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "achieved state-of-the-art performance by including gated relative"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "DNS [20]\nis a challenge that provides noise samples originat-"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "position bias in the transformer-based architecture and augmenting"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "ing from Audio Set [21], Freesound, and DEMAND [22] database."
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "input data into noisy or overlapped speech.\nIn our experiment, we"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "Synthetic room impulse responses are also released for augmenting"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "adopt the base+ variant pre-trained with 94k hours of speech."
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "speech samples to simulate a reverberated environment. Noise sam-"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "ples provided in DNS are involved in the augmentation process in"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "the pre-training stage of WavLM [4]."
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "CHiME3 [23]\nrefers\nto the\nthird CHiME Challenge\ntarget-"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "ing speech recognition under\nreal-world scenarios.\nThe purpose\n3.4. Training details"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "of adopting CHiME3 is to construct a domain mismatch scenario"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "during testing. Noises in this dataset are not involved during the pre-"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "For knowledge distillation3, 960 hours of LibriSpeech are used for"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "training stage of any of the self-supervised teacher models included"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "pre-training. Though some teacher models are robust to distortions,"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "in this work. For some speech processing tasks, it is difﬁcult to ﬁnd"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "no additional distortions are added to the pre-training data for EKD."
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "a corresponding noisy corpora. A simple way to obtain noisy testing"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "The training hyper-parameters are similar\nto the original DistilHu-"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "data for downstream tasks is to add background noises to speech."
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "BERT. The student network architecture is\nthe same as DistilHu-"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "Since we do not have access to the testing data in the hidden set, we"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "BERT except\nfor\nthe prediction heads under\nthe multiple predic-"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "consulted the SUPERB hidden-set committee and got permission"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "tion head setting.\nTo evaluate the models\ntrained with EKD, we"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "to access\nthe results of\nthe testing sets of\nsome tasks containing"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "trained distilled versions of HuBERT, RobustHuBERT, and WavLM"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "real-world background noises provided by the CHiME3 dataset."
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "for comparison. For each model\ntrained with EKD,\ntheir baselines"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "For Automatic Speech Recognition (ASR), since CHiME3 already"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "are the single-teacher distilled versions of each teacher model\nin-"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "provides a testing set for speech recognition recorded in real-world"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "volved in EKD. For downstream speech processing,\nthe data con-"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "environments, we also report\nthe results of this testing set (denoted"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "ﬁguration and hyper-parameters\nfor\ntraining follow the SUPERB"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "as chime-real) in Table 1."
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "hidden-set track. Prediction heads are discarded during this stage to"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "reduce parameter usage. The training and evaluation process were\n2Details\nfor\ncalculating\nthe\noverall\nscore\nare\nshown\nin"
        },
        {
          "Fig. 1.\nIllustrations of EKD. The illustration on the left shows how EKD is performed with the aggregation of representations from multiple": "https://superbbenchmark.org/challenge-slt2022/metrics.\nconducted by the SUPERB hidden-set committee."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: , (c)(d)(e) are models trained with EKD by having Hu-",
      "data": [
        {
          "Table 1. Evaluation results on clean testing sets and testing sets with CHiME3 background noise. DT.": "column “method” speciﬁes whether the hidden layer outputs of the teacher are layerwise-averaged (avg.),",
          "is the abbreviation of Distil. The": "layerwise-concatenated (concat.),"
        },
        {
          "Table 1. Evaluation results on clean testing sets and testing sets with CHiME3 background noise. DT.": "or predicted with multiple sets of prediction heads (multi. pred.). The baselines of our proposed models are the individually distilled versions",
          "is the abbreviation of Distil. The": ""
        },
        {
          "Table 1. Evaluation results on clean testing sets and testing sets with CHiME3 background noise. DT.": "of the teacher models involved in the ensembling process. The values that outperform their corresponding baselines are marked gray. The",
          "is the abbreviation of Distil. The": ""
        },
        {
          "Table 1. Evaluation results on clean testing sets and testing sets with CHiME3 background noise. DT.": "best performance values of each testing set are marked bold. The rank of each model represents the overall performance on the clean set of",
          "is the abbreviation of Distil. The": ""
        },
        {
          "Table 1. Evaluation results on clean testing sets and testing sets with CHiME3 background noise. DT.": "the four downstream speech processing tasks.",
          "is the abbreviation of Distil. The": ""
        },
        {
          "Table 1. Evaluation results on clean testing sets and testing sets with CHiME3 background noise. DT.": "PR (PER ↓ )\nSID (Acc ↑ )\nER (Acc ↑ )",
          "is the abbreviation of Distil. The": "ASR (WER ↓ )"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: , (c)(d)(e) are models trained with EKD by having Hu-",
      "data": [
        {
          "the four downstream speech processing tasks.": ""
        },
        {
          "the four downstream speech processing tasks.": ""
        },
        {
          "the four downstream speech processing tasks.": "(a) DT. HB (baseline) [8]"
        },
        {
          "the four downstream speech processing tasks.": "(b) DT. WL (baseline)"
        },
        {
          "the four downstream speech processing tasks.": "(c) DT. HB & WL"
        },
        {
          "the four downstream speech processing tasks.": "(d) DT. HB & WL"
        },
        {
          "the four downstream speech processing tasks.": "(e) DT. HB & WL"
        },
        {
          "the four downstream speech processing tasks.": "(f) DT. RHB (baseline) [11]"
        },
        {
          "the four downstream speech processing tasks.": "(g) DT. HB & RHB"
        },
        {
          "the four downstream speech processing tasks.": "(h) DT. RHB & WL"
        },
        {
          "the four downstream speech processing tasks.": "(i) DT. HB & RHB & WL"
        },
        {
          "the four downstream speech processing tasks.": "(j) DT. HB & DT. RHB & DT. WL"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: , (c)(d)(e) are models trained with EKD by having Hu-",
      "data": [
        {
          "31.14\n(i) DT. HB & RHB & WL\nmulti. pred.\n23M": "(j) DT. HB & DT. RHB & DT. WL\n-\n70M\n32.96",
          "38.06\n56.32\n66.50\n68.86\n59.07\n59.37\n75.75\n71.50\n1": "38.96\n74.25\n70.92\n56.59\n52.75\n62.40\n69.03\n68.62\n6"
        },
        {
          "31.14\n(i) DT. HB & RHB & WL\nmulti. pred.\n23M": "4. RESULTS",
          "38.06\n56.32\n66.50\n68.86\n59.07\n59.37\n75.75\n71.50\n1": "EKD method not only outperforms an ensemble of\nthree distilled"
        },
        {
          "31.14\n(i) DT. HB & RHB & WL\nmulti. pred.\n23M": "",
          "38.06\n56.32\n66.50\n68.86\n59.07\n59.37\n75.75\n71.50\n1": "models but also requires only one-third of parameters."
        },
        {
          "31.14\n(i) DT. HB & RHB & WL\nmulti. pred.\n23M": "4.1. Different aggregation methods",
          "38.06\n56.32\n66.50\n68.86\n59.07\n59.37\n75.75\n71.50\n1": ""
        },
        {
          "31.14\n(i) DT. HB & RHB & WL\nmulti. pred.\n23M": "In Table 1,\n(c)(d)(e) are models trained with EKD by having Hu-",
          "38.06\n56.32\n66.50\n68.86\n59.07\n59.37\n75.75\n71.50\n1": ""
        },
        {
          "31.14\n(i) DT. HB & RHB & WL\nmulti. pred.\n23M": "",
          "38.06\n56.32\n66.50\n68.86\n59.07\n59.37\n75.75\n71.50\n1": "4.4. Different combinations of teacher models"
        },
        {
          "31.14\n(i) DT. HB & RHB & WL\nmulti. pred.\n23M": "BERT and WavLM as their teachers. By comparing models (c) and",
          "38.06\n56.32\n66.50\n68.86\n59.07\n59.37\n75.75\n71.50\n1": ""
        },
        {
          "31.14\n(i) DT. HB & RHB & WL\nmulti. pred.\n23M": "(d), we discover\nthat aggregating representations of\nthe teacher\nin",
          "38.06\n56.32\n66.50\n68.86\n59.07\n59.37\n75.75\n71.50\n1": ""
        },
        {
          "31.14\n(i) DT. HB & RHB & WL\nmulti. pred.\n23M": "",
          "38.06\n56.32\n66.50\n68.86\n59.07\n59.37\n75.75\n71.50\n1": "Models (e)(g)(h)(i)\nin Table 1 are all\ntrained by performing EKD"
        },
        {
          "31.14\n(i) DT. HB & RHB & WL\nmulti. pred.\n23M": "a layerwise-average manner tends to yield better performance than",
          "38.06\n56.32\n66.50\n68.86\n59.07\n59.37\n75.75\n71.50\n1": ""
        },
        {
          "31.14\n(i) DT. HB & RHB & WL\nmulti. pred.\n23M": "",
          "38.06\n56.32\n66.50\n68.86\n59.07\n59.37\n75.75\n71.50\n1": "with multiple sets of prediction heads and have the same training"
        },
        {
          "31.14\n(i) DT. HB & RHB & WL\nmulti. pred.\n23M": "concatenating the representations. This may be due to the excessive",
          "38.06\n56.32\n66.50\n68.86\n59.07\n59.37\n75.75\n71.50\n1": ""
        },
        {
          "31.14\n(i) DT. HB & RHB & WL\nmulti. pred.\n23M": "",
          "38.06\n56.32\n66.50\n68.86\n59.07\n59.37\n75.75\n71.50\n1": "conﬁgurations but with different combinations of teacher models."
        },
        {
          "31.14\n(i) DT. HB & RHB & WL\nmulti. pred.\n23M": "length of representations being difﬁcult to learn.",
          "38.06\n56.32\n66.50\n68.86\n59.07\n59.37\n75.75\n71.50\n1": ""
        },
        {
          "31.14\n(i) DT. HB & RHB & WL\nmulti. pred.\n23M": "",
          "38.06\n56.32\n66.50\n68.86\n59.07\n59.37\n75.75\n71.50\n1": "Model (g),\ntrained to distill knowledge from HuBERT and Ro-"
        },
        {
          "31.14\n(i) DT. HB & RHB & WL\nmulti. pred.\n23M": "By comparing model (c) to baseline models (a) and (b), we see",
          "38.06\n56.32\n66.50\n68.86\n59.07\n59.37\n75.75\n71.50\n1": ""
        },
        {
          "31.14\n(i) DT. HB & RHB & WL\nmulti. pred.\n23M": "",
          "38.06\n56.32\n66.50\n68.86\n59.07\n59.37\n75.75\n71.50\n1": "bustHuBERT, outperforms baseline models (a) and (f) on every test-"
        },
        {
          "31.14\n(i) DT. HB & RHB & WL\nmulti. pred.\n23M": "that averaging the representations of HuBERT and WavLM improves",
          "38.06\n56.32\n66.50\n68.86\n59.07\n59.37\n75.75\n71.50\n1": ""
        },
        {
          "31.14\n(i) DT. HB & RHB & WL\nmulti. pred.\n23M": "",
          "38.06\n56.32\n66.50\n68.86\n59.07\n59.37\n75.75\n71.50\n1": "ing set except\nfor\nthe noisy testing set of PR. This\nsuggests\nthat"
        },
        {
          "31.14\n(i) DT. HB & RHB & WL\nmulti. pred.\n23M": "the performance of SID and ER on the clean set, but degrades the",
          "38.06\n56.32\n66.50\n68.86\n59.07\n59.37\n75.75\n71.50\n1": ""
        },
        {
          "31.14\n(i) DT. HB & RHB & WL\nmulti. pred.\n23M": "",
          "38.06\n56.32\n66.50\n68.86\n59.07\n59.37\n75.75\n71.50\n1": "models distilled from a teacher model and its domain-adaptive pre-"
        },
        {
          "31.14\n(i) DT. HB & RHB & WL\nmulti. pred.\n23M": "performance of PR and ASR. This may indicate that\nthe averaged",
          "38.06\n56.32\n66.50\n68.86\n59.07\n59.37\n75.75\n71.50\n1": ""
        },
        {
          "31.14\n(i) DT. HB & RHB & WL\nmulti. pred.\n23M": "",
          "38.06\n56.32\n66.50\n68.86\n59.07\n59.37\n75.75\n71.50\n1": "trained version not only gain robustness to noises but also gain im-"
        },
        {
          "31.14\n(i) DT. HB & RHB & WL\nmulti. pred.\n23M": "representations\nfrom different models are not useful\nfor content-",
          "38.06\n56.32\n66.50\n68.86\n59.07\n59.37\n75.75\n71.50\n1": ""
        },
        {
          "31.14\n(i) DT. HB & RHB & WL\nmulti. pred.\n23M": "",
          "38.06\n56.32\n66.50\n68.86\n59.07\n59.37\n75.75\n71.50\n1": "provement\nin clean environments for most of\nthe tasks.\nPerform-"
        },
        {
          "31.14\n(i) DT. HB & RHB & WL\nmulti. pred.\n23M": "based tasks.",
          "38.06\n56.32\n66.50\n68.86\n59.07\n59.37\n75.75\n71.50\n1": ""
        },
        {
          "31.14\n(i) DT. HB & RHB & WL\nmulti. pred.\n23M": "",
          "38.06\n56.32\n66.50\n68.86\n59.07\n59.37\n75.75\n71.50\n1": "ing EKD shows great potential in this case for not degrading perfor-"
        },
        {
          "31.14\n(i) DT. HB & RHB & WL\nmulti. pred.\n23M": "",
          "38.06\n56.32\n66.50\n68.86\n59.07\n59.37\n75.75\n71.50\n1": "mance under clean and noisy settings."
        },
        {
          "31.14\n(i) DT. HB & RHB & WL\nmulti. pred.\n23M": "4.2. Multiple prediction heads",
          "38.06\n56.32\n66.50\n68.86\n59.07\n59.37\n75.75\n71.50\n1": ""
        },
        {
          "31.14\n(i) DT. HB & RHB & WL\nmulti. pred.\n23M": "",
          "38.06\n56.32\n66.50\n68.86\n59.07\n59.37\n75.75\n71.50\n1": "Among models (e)(g)(h)(i), model (i) performs the best in over-"
        },
        {
          "31.14\n(i) DT. HB & RHB & WL\nmulti. pred.\n23M": "",
          "38.06\n56.32\n66.50\n68.86\n59.07\n59.37\n75.75\n71.50\n1": "all performance on the clean testing sets.\nSince ensembling more"
        },
        {
          "31.14\n(i) DT. HB & RHB & WL\nmulti. pred.\n23M": "In Table 1, we observe that model\n(e) outperforms models (c) and",
          "38.06\n56.32\n66.50\n68.86\n59.07\n59.37\n75.75\n71.50\n1": ""
        },
        {
          "31.14\n(i) DT. HB & RHB & WL\nmulti. pred.\n23M": "",
          "38.06\n56.32\n66.50\n68.86\n59.07\n59.37\n75.75\n71.50\n1": "models does not increase model parameters as long as the prediction"
        },
        {
          "31.14\n(i) DT. HB & RHB & WL\nmulti. pred.\n23M": "(d) on every task except\nfor SID. This\nimplies\nthat having each",
          "38.06\n56.32\n66.50\n68.86\n59.07\n59.37\n75.75\n71.50\n1": ""
        },
        {
          "31.14\n(i) DT. HB & RHB & WL\nmulti. pred.\n23M": "",
          "38.06\n56.32\n66.50\n68.86\n59.07\n59.37\n75.75\n71.50\n1": "heads are not used during downstream training,\nit\nis worth trying"
        },
        {
          "31.14\n(i) DT. HB & RHB & WL\nmulti. pred.\n23M": "set of prediction heads predict different\nteacher models is a better",
          "38.06\n56.32\n66.50\n68.86\n59.07\n59.37\n75.75\n71.50\n1": ""
        },
        {
          "31.14\n(i) DT. HB & RHB & WL\nmulti. pred.\n23M": "",
          "38.06\n56.32\n66.50\n68.86\n59.07\n59.37\n75.75\n71.50\n1": "more combinations of high-performance teacher models for EKD."
        },
        {
          "31.14\n(i) DT. HB & RHB & WL\nmulti. pred.\n23M": "method compared to aggregating representations of\nthe teacher\nin",
          "38.06\n56.32\n66.50\n68.86\n59.07\n59.37\n75.75\n71.50\n1": ""
        },
        {
          "31.14\n(i) DT. HB & RHB & WL\nmulti. pred.\n23M": "most cases.",
          "38.06\n56.32\n66.50\n68.86\n59.07\n59.37\n75.75\n71.50\n1": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. REFERENCES": "",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "from Ensembles of Neural Networks for Speech Recognition,”"
        },
        {
          "6. REFERENCES": "[1] Abdelrahman Mohamed,\nHung-yi\nLee,\nLasse\nBorgholt,",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "in Proc. Interspeech 2016, 2016, pp. 3439–3443."
        },
        {
          "6. REFERENCES": "Jakob D Havtorn, Joakim Edin, Christian Igel, Katrin Kirch-",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": ""
        },
        {
          "6. REFERENCES": "",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "[14] Yan Gao, Titouan Parcollet, and Nicholas D Lane,\n“Distill-"
        },
        {
          "6. REFERENCES": "hoff, Shang-Wen Li, Karen Livescu, Lars Maaløe, et al., “Self-",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": ""
        },
        {
          "6. REFERENCES": "",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "ing knowledge from ensembles of acoustic models\nfor\njoint"
        },
        {
          "6. REFERENCES": "arXiv\nsupervised speech representation learning: A review,”",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": ""
        },
        {
          "6. REFERENCES": "",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "ctc-attention end-to-end speech recognition,”\nin 2021 IEEE"
        },
        {
          "6. REFERENCES": "preprint arXiv:2205.10643, 2022.",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": ""
        },
        {
          "6. REFERENCES": "",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "Automatic Speech Recognition and Understanding Workshop"
        },
        {
          "6. REFERENCES": "[2] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,\nand",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "(ASRU). IEEE, 2021, pp. 138–145."
        },
        {
          "6. REFERENCES": "Michael Auli, “wav2vec 2.0: A framework for self-supervised",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": ""
        },
        {
          "6. REFERENCES": "",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "[15] Chuhan Wu,\nFangzhao Wu,\nand Yongfeng Huang,\n“One"
        },
        {
          "6. REFERENCES": "learning of speech representations,” Advances in Neural Infor-",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": ""
        },
        {
          "6. REFERENCES": "",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "teacher\nis\nenough?\npre-trained language model distillation"
        },
        {
          "6. REFERENCES": "mation Processing Systems, vol. 33, pp. 12449–12460, 2020.",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": ""
        },
        {
          "6. REFERENCES": "",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "from multiple teachers,”\narXiv preprint arXiv:2106.01023,"
        },
        {
          "6. REFERENCES": "",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "2021."
        },
        {
          "6. REFERENCES": "[3] Wei-Ning Hsu,\nBenjamin Bolte,\nYao-Hung Hubert\nTsai,",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": ""
        },
        {
          "6. REFERENCES": "Kushal Lakhotia, Ruslan Salakhutdinov,\nand Abdelrahman",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "[16] Chuhan Wu, Fangzhao Wu, Tao Qi,\nand Yongfeng Huang,"
        },
        {
          "6. REFERENCES": "Mohamed,\n“HuBERT: Self-supervised speech representation",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "“Uniﬁed and effective ensemble knowledge distillation,” arXiv"
        },
        {
          "6. REFERENCES": "IEEE/ACM\nlearning by masked prediction of hidden units,”",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": ""
        },
        {
          "6. REFERENCES": "",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "preprint arXiv:2204.00548, 2022."
        },
        {
          "6. REFERENCES": "Transactions on Audio, Speech, and Language Processing, vol.",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": ""
        },
        {
          "6. REFERENCES": "",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "[17] Tzu-hsun Feng, Annie Dong, Ching-Feng Yeh, Shu-wen Yang,"
        },
        {
          "6. REFERENCES": "29, pp. 3451–3460, 2021.",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": ""
        },
        {
          "6. REFERENCES": "",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "Tzu-Quan Lin,\nJiatong\nShi, Kai-Wei Chang,\nZili Huang,"
        },
        {
          "6. REFERENCES": "[4]\nSanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shu-",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "Haibin Wu, Xuankai Chang, Shinji Watanabe, Abdelrahman"
        },
        {
          "6. REFERENCES": "jie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yosh-",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "Mohamed, Shang-Wen Li, and Hung-yi Lee,\n“SUPERB @"
        },
        {
          "6. REFERENCES": "ioka, Xiong Xiao, et al., “WavLM: Large-scale self-supervised",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "SLT 2022: Challenge on Generalization and Efﬁciency of Self-"
        },
        {
          "6. REFERENCES": "pre-training for full stack speech processing,” IEEE Journal of",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "Supervised Speech Representation Learning,”\nin IEEE-SLT"
        },
        {
          "6. REFERENCES": "Selected Topics in Signal Processing, 2022.",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "Workshop, 2022."
        },
        {
          "6. REFERENCES": "[5] Heming Wang, Yao Qian, Xiaofei Wang, Yiming Wang,",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "[18] David\nSnyder, Guoguo Chen,\nand Daniel\nPovey,\n“Mu-"
        },
        {
          "6. REFERENCES": "Chengyi Wang, Shujie Liu, Takuya Yoshioka,\nJinyu Li, and",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "arXiv preprint\nsan:\nA music,\nspeech,\nand noise\ncorpus,”"
        },
        {
          "6. REFERENCES": "DeLiang Wang,\n“Improving noise robustness of contrastive",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "arXiv:1510.08484, 2015."
        },
        {
          "6. REFERENCES": "speech representation learning with speech reconstruction,” in",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": ""
        },
        {
          "6. REFERENCES": "",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "[19] Gordon\nWichern,\nJoe\nAntognini,\nMichael\nFlynn,"
        },
        {
          "6. REFERENCES": "ICASSP. IEEE, 2022, pp. 6062–6066.",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": ""
        },
        {
          "6. REFERENCES": "",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "Licheng Richard\nZhu,\nEmmett McQuinn,\nDwight Crow,"
        },
        {
          "6. REFERENCES": "[6] Yiming Wang,\nJinyu Li, Heming Wang, Yao Qian, Chengyi",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "Ethan Manilow, and Jonathan Le Roux,\n“Wham!: Extending"
        },
        {
          "6. REFERENCES": "Wang, and Yu Wu,\n“Wav2vec-switch: Contrastive learning",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "arXiv preprint\nspeech separation to noisy environments,”"
        },
        {
          "6. REFERENCES": "from original-noisy speech pairs\nfor\nrobust\nspeech recogni-",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "arXiv:1907.01160, 2019."
        },
        {
          "6. REFERENCES": "tion,” in ICASSP. IEEE, 2022, pp. 7097–7101.",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": ""
        },
        {
          "6. REFERENCES": "",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "[20] Harishchandra Dubey, Vishak Gopal, Ross Cutler, Ashkan"
        },
        {
          "6. REFERENCES": "",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "Aazami, Sergiy Matusevych, Sebastian Braun, Seﬁk Emre Es-"
        },
        {
          "6. REFERENCES": "[7] Kuan Po Huang, Yu-Kuan Fu, Yu Zhang, and Hung yi Lee,",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": ""
        },
        {
          "6. REFERENCES": "",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "kimez, Manthan Thakker, Takuya Yoshioka, Hannes Gamper,"
        },
        {
          "6. REFERENCES": "“Improving Distortion Robustness of Self-supervised Speech",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": ""
        },
        {
          "6. REFERENCES": "",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "et al., “ICASSP 2022 Deep Noise Suppression Challenge,” in"
        },
        {
          "6. REFERENCES": "Inter-\nProcessing Tasks with Domain Adaptation,”\nin Proc.",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": ""
        },
        {
          "6. REFERENCES": "",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "ICASSP. IEEE, 2022, pp. 9271–9275."
        },
        {
          "6. REFERENCES": "speech 2022, 2022, pp. 2193–2197.",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": ""
        },
        {
          "6. REFERENCES": "",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "[21]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren"
        },
        {
          "6. REFERENCES": "[8] Heng-Jui Chang, Shu-wen Yang, and Hung-yi Lee, “DistilHu-",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": ""
        },
        {
          "6. REFERENCES": "",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "Jansen, Wade Lawrence, R Channing Moore, Manoj Plakal,"
        },
        {
          "6. REFERENCES": "BERT: Speech representation learning by layer-wise distilla-",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": ""
        },
        {
          "6. REFERENCES": "",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "and Marvin Ritter, “Audio set: An ontology and human-labeled"
        },
        {
          "6. REFERENCES": "tion of hidden-unit BERT,” in ICASSP. IEEE, 2022, pp. 7087–",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": ""
        },
        {
          "6. REFERENCES": "",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "dataset\nfor audio events,”\nin ICASSP.\nIEEE, 2017, pp. 776–"
        },
        {
          "6. REFERENCES": "7091.",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": ""
        },
        {
          "6. REFERENCES": "",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "780."
        },
        {
          "6. REFERENCES": "[9] Yeonghyeon Lee, Kangwook Jang, Jahyun Goo, Youngmoon",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": ""
        },
        {
          "6. REFERENCES": "",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "[22]\nJoachim Thiemann,\nNobutaka\nIto,\nand\nEmmanuel Vin-"
        },
        {
          "6. REFERENCES": "Jung,\nand Hoi Rin Kim,\n“FitHuBERT: Going Thinner and",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": ""
        },
        {
          "6. REFERENCES": "",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "cent,\n“The diverse environments multi-channel acoustic noise"
        },
        {
          "6. REFERENCES": "Deeper for Knowledge Distillation of Speech Self-Supervised",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": ""
        },
        {
          "6. REFERENCES": "",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "database (demand): A database of multichannel environmen-"
        },
        {
          "6. REFERENCES": "Models,” in Proc. Interspeech 2022, 2022, pp. 3588–3592.",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": ""
        },
        {
          "6. REFERENCES": "",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "tal noise recordings,”\nin Proceedings of Meetings on Acous-"
        },
        {
          "6. REFERENCES": "[10] Takanori Ashihara, Takafumi Moriya, Kohei Matsuura,\nand",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "tics ICA2013. Acoustical Society of America, 2013, vol. 19, p."
        },
        {
          "6. REFERENCES": "Tomohiro Tanaka,\n“Deep versus Wide: An Analysis of Stu-",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "035081."
        },
        {
          "6. REFERENCES": "dent Architectures for Task-Agnostic Knowledge Distillation",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": ""
        },
        {
          "6. REFERENCES": "",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "[23]\nJon Barker, Ricard Marxer, Emmanuel Vincent,\nand Shinji"
        },
        {
          "6. REFERENCES": "of Self-Supervised Speech Models,” in Proc. Interspeech 2022,",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": ""
        },
        {
          "6. REFERENCES": "",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "Watanabe,\n“The third ‘CHiME’speech separation and recog-"
        },
        {
          "6. REFERENCES": "2022, pp. 411–415.",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": ""
        },
        {
          "6. REFERENCES": "",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "nition challenge: Dataset,\ntask and baselines,”\nin 2015 IEEE"
        },
        {
          "6. REFERENCES": "[11] Kuan-Po Huang, Yu-Kuan Fu, Tsu-Yuan Hsu, Fabian Rit-",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "Workshop on Automatic Speech Recognition and Understand-"
        },
        {
          "6. REFERENCES": "ter Gutierrez, Fan-Lin Wang, Liang-Hsuan Tseng, Yu Zhang,",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "ing (ASRU). IEEE, 2015, pp. 504–511."
        },
        {
          "6. REFERENCES": "et al.,\n“Improving generalizability of distilled self-supervised",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": ""
        },
        {
          "6. REFERENCES": "",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "[24] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev"
        },
        {
          "6. REFERENCES": "speech processing models under distorted settings,”\nin IEEE-",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": ""
        },
        {
          "6. REFERENCES": "",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "Khudanpur,\n“Librispeech:\nan ASR corpus based on public"
        },
        {
          "6. REFERENCES": "SLT Workshop, 2022.",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": ""
        },
        {
          "6. REFERENCES": "",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": "domain audio books,” in ICASSP. IEEE, 2015, pp. 5206–5210."
        },
        {
          "6. REFERENCES": "[12] A Arunkumar, Vrunda Nileshkumar\nSukhadia,\nand\nSrini-",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": ""
        },
        {
          "6. REFERENCES": "vasan Umesh,\n“Investigation of Ensemble features of Self-",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": ""
        },
        {
          "6. REFERENCES": "Supervised Pretrained Models for Automatic Speech Recog-",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": ""
        },
        {
          "6. REFERENCES": "nition,” in Proc. Interspeech 2022, 2022, pp. 5145–5149.",
          "[13] Yevgen Chebotar and Austin Waters,\n“Distilling Knowledge": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Selfsupervised speech representation learning: A review",
      "authors": [
        "Abdelrahman Mohamed",
        "Hung-Yi Lee",
        "Lasse Borgholt",
        "Jakob Havtorn",
        "Joakim Edin",
        "Christian Igel",
        "Katrin Kirchhoff",
        "Shang-Wen",
        "Karen Li",
        "Lars Livescu",
        "Maaløe"
      ],
      "year": "2022",
      "venue": "Selfsupervised speech representation learning: A review",
      "arxiv": "arXiv:2205.10643"
    },
    {
      "citation_id": "3",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "4",
      "title": "HuBERT: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "5",
      "title": "WavLM: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "Sanyuan Chen",
        "Chengyi Wang",
        "Zhengyang Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Jinyu Li",
        "Naoyuki Kanda",
        "Takuya Yoshioka",
        "Xiong Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "6",
      "title": "Improving noise robustness of contrastive speech representation learning with speech reconstruction",
      "authors": [
        "Heming Wang",
        "Yao Qian",
        "Xiaofei Wang",
        "Yiming Wang",
        "Chengyi Wang",
        "Shujie Liu",
        "Takuya Yoshioka",
        "Jinyu Li",
        "Deliang Wang"
      ],
      "year": "2022",
      "venue": "ICASSP"
    },
    {
      "citation_id": "7",
      "title": "Wav2vec-switch: Contrastive learning from original-noisy speech pairs for robust speech recognition",
      "authors": [
        "Yiming Wang",
        "Jinyu Li",
        "Heming Wang",
        "Yao Qian",
        "Chengyi Wang",
        "Yu Wu"
      ],
      "year": "2022",
      "venue": "ICASSP"
    },
    {
      "citation_id": "8",
      "title": "Improving Distortion Robustness of Self-supervised Speech Processing Tasks with Domain Adaptation",
      "authors": [
        "Po Kuan",
        "Yu-Kuan Huang",
        "Yu Fu",
        "Hung Zhang",
        "Lee Yi"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "9",
      "title": "DistilHu-BERT: Speech representation learning by layer-wise distillation of hidden-unit BERT",
      "authors": [
        "Heng-Jui Chang",
        "Shu-Wen Yang",
        "Hung-Yi Lee"
      ],
      "year": "2022",
      "venue": "ICASSP"
    },
    {
      "citation_id": "10",
      "title": "FitHuBERT: Going Thinner and Deeper for Knowledge Distillation of Speech Self-Supervised Models",
      "authors": [
        "Yeonghyeon Lee",
        "Kangwook Jang",
        "Jahyun Goo",
        "Youngmoon Jung",
        "Hoi Rin Kim"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "11",
      "title": "Deep versus Wide: An Analysis of Student Architectures for Task-Agnostic Knowledge Distillation of Self-Supervised Speech Models",
      "authors": [
        "Takanori Ashihara",
        "Takafumi Moriya",
        "Kohei Matsuura",
        "Tomohiro Tanaka"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "12",
      "title": "Improving generalizability of distilled self-supervised speech processing models under distorted settings",
      "authors": [
        "Kuan-Po Huang",
        "Yu-Kuan Fu",
        "Tsu-Yuan Hsu",
        "Fabian Gutierrez",
        "Fan-Lin Wang",
        "Liang-Hsuan Tseng",
        "Yu Zhang"
      ],
      "year": "2022",
      "venue": "Improving generalizability of distilled self-supervised speech processing models under distorted settings"
    },
    {
      "citation_id": "13",
      "title": "Investigation of Ensemble features of Self-Supervised Pretrained Models for Automatic Speech Recognition",
      "authors": [
        "Arunkumar",
        "Nileshkumar Vrunda",
        "Srinivasan Sukhadia",
        "Umesh"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "14",
      "title": "Distilling Knowledge from Ensembles of Neural Networks for Speech Recognition",
      "authors": [
        "Yevgen Chebotar",
        "Austin Waters"
      ],
      "year": "2016",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "15",
      "title": "Distilling knowledge from ensembles of acoustic models for joint ctc-attention end-to-end speech recognition",
      "authors": [
        "Yan Gao",
        "Titouan Parcollet",
        "Nicholas Lane"
      ],
      "year": "2021",
      "venue": "2021 IEEE Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "16",
      "title": "One teacher is enough? pre-trained language model distillation from multiple teachers",
      "authors": [
        "Chuhan Wu",
        "Fangzhao Wu",
        "Yongfeng Huang"
      ],
      "year": "2021",
      "venue": "One teacher is enough? pre-trained language model distillation from multiple teachers",
      "arxiv": "arXiv:2106.01023"
    },
    {
      "citation_id": "17",
      "title": "Unified and effective ensemble knowledge distillation",
      "authors": [
        "Chuhan Wu",
        "Fangzhao Wu",
        "Tao Qi",
        "Yongfeng Huang"
      ],
      "year": "2022",
      "venue": "Unified and effective ensemble knowledge distillation",
      "arxiv": "arXiv:2204.00548"
    },
    {
      "citation_id": "18",
      "title": "SUPERB @ SLT 2022: Challenge on Generalization and Efficiency of Self-Supervised Speech Representation Learning",
      "authors": [
        "Annie Tzu-Hsun Feng",
        "Ching-Feng Dong",
        "Shu-Wen Yeh",
        "Tzu-Quan Yang",
        "Jiatong Lin",
        "Kai-Wei Shi",
        "Zili Chang",
        "Haibin Huang",
        "Xuankai Wu",
        "Shinji Chang",
        "Abdelrahman Watanabe",
        "Mohamed",
        "Shang-Wen",
        "Hung-Yi Li",
        "Lee"
      ],
      "year": "2022",
      "venue": "IEEE-SLT Workshop"
    },
    {
      "citation_id": "19",
      "title": "Musan: A music, speech, and noise corpus",
      "authors": [
        "David Snyder",
        "Guoguo Chen",
        "Daniel Povey"
      ],
      "year": "2015",
      "venue": "Musan: A music, speech, and noise corpus",
      "arxiv": "arXiv:1510.08484"
    },
    {
      "citation_id": "20",
      "title": "Wham!: Extending speech separation to noisy environments",
      "authors": [
        "Gordon Wichern",
        "Joe Antognini",
        "Michael Flynn",
        "Licheng Richard Zhu",
        "Emmett Mcquinn",
        "Dwight Crow",
        "Ethan Manilow",
        "Jonathan Roux"
      ],
      "year": "2019",
      "venue": "Wham!: Extending speech separation to noisy environments",
      "arxiv": "arXiv:1907.01160"
    },
    {
      "citation_id": "21",
      "title": "ICASSP 2022 Deep Noise Suppression Challenge",
      "authors": [
        "Harishchandra Dubey",
        "Vishak Gopal",
        "Ross Cutler",
        "Ashkan Aazami",
        "Sergiy Matusevych",
        "Sebastian Braun",
        "Sefik Eskimez",
        "Manthan Thakker",
        "Takuya Yoshioka",
        "Hannes Gamper"
      ],
      "year": "2022",
      "venue": "ICASSP"
    },
    {
      "citation_id": "22",
      "title": "Audio set: An ontology and human-labeled dataset for audio events",
      "authors": [
        "Jort F Gemmeke",
        "P Daniel",
        "Dylan Ellis",
        "Aren Freedman",
        "Wade Jansen",
        "R Channing Lawrence",
        "Manoj Moore",
        "Marvin Plakal",
        "Ritter"
      ],
      "year": "2017",
      "venue": "ICASSP"
    },
    {
      "citation_id": "23",
      "title": "The diverse environments multi-channel acoustic noise database (demand): A database of multichannel environmental noise recordings",
      "authors": [
        "Joachim Thiemann",
        "Nobutaka Ito",
        "Emmanuel Vincent"
      ],
      "year": "2013",
      "venue": "Proceedings of Meetings on Acoustics ICA2013"
    },
    {
      "citation_id": "24",
      "title": "The third 'CHiME'speech separation and recognition challenge: Dataset, task and baselines",
      "authors": [
        "Jon Barker",
        "Ricard Marxer",
        "Emmanuel Vincent",
        "Shinji Watanabe"
      ],
      "year": "2015",
      "venue": "2015 IEEE Workshop on Automatic Speech Recognition and Understanding"
    },
    {
      "citation_id": "25",
      "title": "Librispeech: an ASR corpus based on public domain audio books",
      "authors": [
        "Vassil Panayotov",
        "Guoguo Chen",
        "Daniel Povey",
        "Sanjeev Khudanpur"
      ],
      "year": "2015",
      "venue": "ICASSP"
    }
  ]
}