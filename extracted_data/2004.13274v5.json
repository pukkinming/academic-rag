{
  "paper_id": "2004.13274v5",
  "title": "Exploring The Contextual Factors Affecting Multimodal Emotion Recognition In Videos",
  "published": "2020-04-28T04:02:08Z",
  "authors": [
    "Prasanta Bhattacharya",
    "Raj Kumar Gupta",
    "Yinping Yang"
  ],
  "keywords": [
    "Affective computing",
    "Affect sensing and analysis",
    "Modelling human emotions",
    "Multi-modal recognition",
    "Sentiment analysis",
    "Technology & devices for affective computing"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotional expressions form a key part of user behavior on today's digital platforms. While multimodal emotion recognition techniques are gaining research attention, there is a lack of deeper understanding on how visual and non-visual features can be used to better recognize emotions in certain contexts, but not others. This study analyzes the interplay between the effects of multimodal emotion features derived from facial expressions, tone and text in conjunction with two key contextual factors: i) gender of the speaker, and ii) duration of the emotional episode. Using a large public dataset of 2,176 manually annotated YouTube videos, we found that while multimodal features consistently outperformed bimodal and unimodal features, their performance varied significantly across different emotions, gender and duration contexts. Multimodal features performed particularly better for male speakers in recognizing most emotions. Furthermore, multimodal features performed particularly better for shorter than for longer videos in recognizing neutral and happiness, but not sadness and anger. These findings offer new insights towards the development of more context-aware emotion recognition and empathetic systems.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "or long, the interest surrounding emotion recognition has been a key focus area within the field of affective computing  [1] -  [3] . Recent advances in computing infrastructure and datasets have led to new developments in detecting emotions using multiple types of data modalities like facial and acoustic expressions, linguistic and semantic patterns, body movements, eye gaze patterns and electroencephalography signals  [2] . Recent studies have proposed different design approaches to perform automatic recognition of affective outcomes like valence, arousal, dominance, and emotion types  [1] ,  [2] .\n\nThe ability to automatically recognize emotions has valuable implications for a range of applications. These include the design of empathetic agents  [4] , understanding consumer behavior at scale from observational data  [5] , and aiding healthcare professionals in a range of activities from diagnosing depressive symptoms in patients, to tracking the onset and progression of autism  [6] ,  [7] .\n\nThis paper focuses on extracting high-level emotion features from visual, audio and text data modalities by leveraging a number of recent emotion analysis technologies. We tested the efficacy of these features in classifying finegrained emotions from a real-world dataset comprising 2,176 labelled video clips, and sought to explore the conditions under which multimodal features outperformed bimodal and unimodal features.\n\nMore importantly, we performed an in-depth analysis of the effects from two important contextual factors that impact the performance of multimodal emotion recognition systems: i) gender of the speaker, and ii) duration of the emotion expression episode. First, we investigated whether the speaker's gender was linked to the relative effectiveness of visual, audio and text features in recognizing different emotions. To date, despite extensive research aimed at uncovering gender differences in how emotion is experienced and perceived (e.g. McDuff et al.  [8] ), little is known about how these differences affect the predictive performance of affective computing systems. We found that while multimodal features tend to outperform unimodal features for both male and female speakers, the relative improvement of using multimodal over unimodal was higher for male speakers. A trimodal classifier (i.e. combining visual, audio, and text features) was found to be the best performing multimodal classifier for both genders. For the unimodal features alone, we found that visual features performed best for both male and female speakers. With respect to the emotion categories in the overall sample, we found that while visual features performed best for predicting happy, sad, and neutral emotions, the audio features performed best for anger. Through these insights, the current study aims to be among the first to emphasize the importance of developing gender-aware multimodal systems, as the empirical performance of these systems can be largely contingent on inherent gender differences among speakers.\n\nIn addition to gender, we examined if the duration of the emotion being expressed affected the way a particular emotion was perceived and recognized across various modalities. Psychological studies in the past have examined the relationship between emotions and emotion duration using subjects' recalled emotional experiences  [9] -  [11] .\n\nHowever, to the best of our knowledge, the relationship between episodic duration of emotions and classifier performance in videos, as well as the associated underpinnings, are still not well understood. Our analyses show that duration played an important role in affecting classifier performance. Specifically, while multimodal classifiers outperformed unimodal classifiers for videos of all durations, this relative improvement was higher in shorter episodic durations for only the negative emotions like sadness and anger. In contrast, the relative improvement from using multiple modalities was higher in longer episodic durations for neutral and happiness emotions.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Concepts And Related Work",
      "text": "Expressing and perceiving emotions is central to human experience and is a key contributor to the sustenance of interpersonal communication  [12] -  [14] . Emotions also affect how we interpret our environment, develop opinions and form judgments about the surrounding individuals and situations, and even drive behaviors  [15] .\n\nWithin the overarching category of 'affect', it is generally accepted that 'emotion' is distinct from related states like mood and temperament. Emotions might involve a range of feelings which may be transient, like relief, or last for relatively long periods of time, as is the case with mood, or really long spans of time, as with temperament  [16] -  [18] . Some of the earliest definitions about emotions came independently from psychologist William James and physiologist Carl Lange who contended that emotions arise naturally as a result of environmental stimulus, and that our interpretation of the physiological sensation of these changes constitutes emotion  [19] . Contrary to James' contention that emotions could be a response to environmental changes, W.B. Cannon treated emotions as being felt first but expressed outwardly in certain behaviors much later  [20] . There is also debate over how emotions can be perceived or described  [13] ,  [14] ,  [21] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion Type, Valence, Arousal And Intensity",
      "text": "For the purpose of this paper, we adopt three pertinent concepts from the emotion literature to facilitate the subsequent emotion feature extraction and analysis.\n\nFirst, we refer to the stream of work on categorization of emotions which views emotions as discrete and distinct types such as happiness (or joy), anger, sadness and fear  [22] -  [26] . This view, while \"simplistic\"  [27] ,  [28] , helps to focus the present study in terms of differentiating the specific effects due to data modality, duration and gender for various distinct emotion types. Second, we leverage the dimensional view of emotion that has received growing popularity in recent years  [29] -  [33] . In this study, we specifically considered two dimensions: valence and arousal. Valence describes the degree of pleasantness or unpleasantness of a signal and is generally measured on a continuous scale ranging from positive (i.e. pleasant) to negative (i.e. unpleasant). Arousal, on the other hand, refers to the extent of physical activation, ranging from no arousal to very high arousal. Thus, various emotional states like happy, angry, sad and afraid can be mapped to a two-dimensional valence-arousal scale, known as the circumplex model of affect  [34]  . For instance, in the circumplex model  [34]  , \"happy\" is associated with a positive valence and moderate to high arousal. In contrast, both \"tensed\" and \"angry\" have a negative valence with moderate to high arousal, while \"sad\" is associated with a negative valence and low arousal. Third, we also consider emotion intensity as an important concept. This aspect of emotion concerns the degree or \"depth\" of emotional experience, ranging from barely noticeable to most intense imaginable  [26] ,  [35] ,  [36] . For instance, the anger emotions can range from low-intensity annoyance to high-intensity rage, and this difference represents a relatively new, less explored aspect of emotional information that can benefit the development of computational methods (e.g.,  [37] ).",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Multimodal Vs Unimodal Recognition Systems",
      "text": "Early research into emotion recognition systems largely focused on single data modalities (e.g., only facial expressions or tone of voice) and on emotions extracted from enacted sequences by trained actors  [38] . More recently, studies have attempted to combine signals from multiple modalities such as facial expressions and audio  [39] ,  [40] , audio and written text  [41] ,  [42] , physiological signals  [43] , and various combinations of these modalities  [44] . Overall, studies in this field have started to emphasize the importance of multimodal signals of emotions in more naturalistic scenarios (See  [2] ,  [3]  for detailed reviews).\n\nHowever, despite notable efforts, the existing literature on emotion recognition is limited in three ways. First, while there is growing evidence that multimodal systems generally outperform unimodal systems in relevant tasks  [2] ,  [40] ,  [45] , recent results have shown more complex patterns. For example, a number of studies have found that multimodal systems can often exhibit negligible improvements or even reductions in performance  [39] ,  [46] . Second, a vast majority of existing analyses on multimodal systems were based on relatively small samples (i.e. fewer than 50 participants), and using mostly bimodal classifiers. Third, there is also scant work on understanding the role of various kinds of contextual factors that might directly or indirectly influence model performance (e.g. de Gelder et al.  [47] ). With the exception of a few studies, the extant literature does not offer an adequate understanding of why certain data modalities work better for specific emotions, but not others  [48] . It is therefore important to address the fundamental question of which data modalities are best suited for certain emotions, and the role of contextual factors in affecting the performance of these multimodal systems.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Contextual Factors: Gender And Duration",
      "text": "How does the performance of multimodal emotion recognition systems change in different contexts? A few recent studies, such as  [39] ,  [47] ,  [49] ,  [50] , have started to explore how contextual factors can potentially affect the performance of emotion related classifiers. For instance, de Gelder et al. investigated how the presence of information on the visual and auditory context (e.g. information about natural scene, vocal expressions etc.) can benefit the recognition of facial expressions  [47] . Contextual information can also come from the same modality. For instance, Metallinou et al. showed that emotions within as well as across utterances in a dialog sequence can be leveraged to improve emotion recognition  [39] . These studies have focused primarily on external contexts (e.g., speaker's environment, emotion eliciting factors).\n\nA relatively new direction is the interplay of internal contexts manifested through individual differences, like gender, and emotions. For example, Brebner analyzed two samples comprising Australian and International participants for differences in the frequency and intensity of selfreported emotions by gender  [51] . The results highlighted significant differences for affection and sadness emotions, where females scored higher on both frequency and intensity. Males, on the other hand, scored higher on pride in both frequency and intensity. This is not entirely surprising, and a number of studies have attempted to explain gender differences in how emotions are experienced and perceived in others  [8] ,  [52] ,  [53] . For instance, it has been shown that females generally perform better than men at perceiving and exhibiting negative emotions like sadness and fear  [54] . The reasons for this range from evolutionary (e.g., Babchuck et al.'s primary caretaker hypothesis or Hampson et al.'s fitness threat hypothesis  [52] ,  [55] ) to biological  [54]  to social and normative factors  [56] . However, it remains to be understood whether and how the performance of multimodal emotion features varies as a result of such gender differences.\n\nAnother contextual factor that has attracted recent attention, and motivates the present study, is the role of temporal contexts. Recent studies have shown that the temporal length of the specific emotion being recorded is likely to influence the effectiveness of the classifier  [48] ,  [49] . This is likely due to a combination of three major factors. First, the duration of an emotion expression episode is likely to vary as a function of the data modality  [49] . For example, Lingenfesler et al.  [48]  showed that an asynchronous fusion algorithm that takes into account different onset and offset times for emotions in various modalities, outperformed synchronous fusion algorithms. This is particularly useful since certain modalities, like language, spoken or written, take longer to manifest emotions, while others like facial expressions, take shorter time. Second, the nature of the platform or device through which the emotions are being recorded often influences the intensity and duration of the emotion. For instance, users recording a vlog on YouTube are likely to express emotions over longer durations, as compared to users uploading a much shorter Vine video. Lastly, the specific types of emotion might also be associated with the duration of the episode. In this study, we investigate how the duration of the videos affects the performance of multimodal features across various emotions.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Data",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Data Source",
      "text": "We make use of the One-Minute-Gradual (OMG) Emotion Dataset  [57]  for our analyses. The dataset was released as part of the 2018 OMG Emotional Behavior Challenge 1  where the original task was to implement an emotionrecognition system to accurately predict arousal and valence scores using an annotated dataset. The original OMG train dataset 2  contains YouTube links of 2,444 public video clips. We were able to retrieve 2,176 clips as of May 2018 using the links provided, as some links became unavailable over time. Each video clip's duration ranged around the one-minute mark in general, but with some shorter and longer duration videos. Most of the videos were of actors and actresses who were practicing lines or monologues probably in preparation for auditions. Through these monologues, the actors and actresses exhibited a number of discrete emotions (e.g., happiness, sadness, surprise), of varying arousals. Fig.  1  shows screenshots of six sample videos from this dataset.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Emotion Ground Truth Labels",
      "text": "The dataset also provides utterance-level (approximately 10 seconds per utterance) majority-voted ground truth labels (\"EmotionMaxVote\") for seven discrete emotion categories, namely anger, fear, happiness, neutral, sadness, disgust, and surprise 3  . For each clip, the majority vote label was calculated by having five different annotations from crowdsourced Amazon Mechanical Turk workers  [57] .\n\nTable  1  presents the basic descriptive statistics of emotion categories, sorted by the most balanced to the most imbalanced classes. We note that neutral and happiness had acceptable levels of imbalance, while sadness and anger exhibited higher class imbalance. Disgust, fear and surprise classes had the highest class imbalance. The relatively small number of disgust, fear and surprise cases in the dataset is an important limitation. It is for this reason that we choose to perform the subgroup analyses using only the neutral, happiness, sadness, and anger emotion categories. Here, it is useful to note that the class labels refer to the emotion ground truth of the speakers as perceived by the multiple independent annotators. It is different from selfreported or recalled emotional experiences, as used in traditional psychological studies.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Gender And Duration Labels",
      "text": "For this research, we manually annotated the gender of the speaker. We coded a discrete gender variable with three categories: \"female\", \"male\", and \"others\", for when it was either not possible to assess the gender or when there were multiple speakers in one utterance. We calculated the duration of each video using the start and end time of the video clips mentioned in the dataset.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Emotion Feature Extraction",
      "text": "As the purpose of our study is to understand the effects of contextual factors affecting multimodal emotion recognition, our feature extraction strategy focuses on extracting theoretically explainable, high-level emotion features from the visual, audio and text channels, respectively. Hence, even though it was possible to extract a large number of lowerlevel visual and non-visual features (e.g., facial action units, word embeddings), we chose to leverage a set of pretrained emotion analysis systems that are grounded in data and key emotion concepts (see Section 2.1) to generate high-level emotion features.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Visual-Based Emotion Feature Extraction",
      "text": "Among all modalities, facial expressions (e.g., frown, smiling) provide arguably the most intuitive emotion cues. We used a face emotion analysis system called FEA 4 to extract the visual features. FEA was trained with visual profiling techniques from static images  [58] , and was later extended to process a set of image frames in videos using deep learning techniques (see details in  [58] -  [62] ). The system has been tested on AffectNet test set  [63] , and achieved Concordance Correlation Coefficients (CCC) of 0.527 and 0.569 for valence and arousal prediction respectively.\n\nFor each video, the FEA system generates vValence and vArousal, which indicate the degree of valence (from very unpleasant to very pleasant) and the degree of arousal (from no to very high physical activation) detected from the speaker's facial expressions. A third feature, vIntensity, measures the distance to the neutral state, calculated as the square root of the sum of squares of vValence and vArousal. The outputs also include  25   indicating whether, or not, a face is expressing a particular emotional state aligned with the circumplex emotion model  [34] . Taken together, we extracted a total of 28 visual-based emotion features, namely vValence,  vArousal, vIntensity, vAfraid, vAlarmed, vAnnoyed, vAroused, vAstonished, vBored, vCalm, vContent, vDelighted, vDepressed, vDistressed, vDroopy, vExcited, vFrustrated, vGloomy, vHappy, vMiserable, vNeutral, vPleased, vSad, vSatisfied, vSerene, vSleepy, vTensed , and vTired.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Audio-Based Emotion Feature Extraction",
      "text": "In addition to visual features, the acoustic patterns of speech (e.g., speaking speed, pitch, intonation) provide different sources of emotion cues. For pre-processing, we first applied FFMPEG  5  to extract the audio component from the videos. We then extracted the audio-based emotion features using the AcousEmo 6  system, which was trained with acoustic analysis of emotion signals using deep Boltzmann machines (DBM) ([64]-  [69] ). The DBM classifier showed an effective predictive accuracy, measured as unweighted average recall, of 53.6, 64.4, 56.3 and 53.1 over valence, arousal, power and expectancy dimensions, respectively, when evaluated using the 2011 Audio/Visual Emotion Challenge dataset  [70]  (see details described in  [69] ).\n\nFor a given audio, AcousEmo produces two main outputs: aValence and aArousal, indicating the degree of valence (from very unpleasant to very pleasant) and the degree of arousal (from no to very high physical activation) detected from the acoustic signals. The system also generates two additional emotion dimensions: aPower (which subsumes two related concepts of the power and control that a speaker is expressing) and aExpectancy (which reflects whether the speaker is anticipating or expecting things to happen) (see theoretical discussion of the two additional dimensions  [71]  and the AVEC2011 challenge  [70]  for details). For the present OMG data, we extracted a total of ten audio-based emotion features: aValence, aArousal, aPower, aExpectancy, aIntensity, aJoy, aAnger, aFear, aSadness, aNeutral.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Text-Based Emotion Feature Extraction",
      "text": "In addition to the visual and acoustic features, the language content of a speaker's speech in a video (e.g., word choice, linguistic meaning) can also carry valuable emotion cues. Before extracting the text-based emotion features, we first obtained the speech transcript from the audio using an Automatic Speech Recognition (ASR) engine developed by I2R  7  . This engine incorporates a range of speech processing techniques including sub-harmonic ratio based voice activity detection  [72] , mismatched crowdsourcing for acoustic modeling  [73] , multi-task learning for pronunciation modeling  [74] , and multi-task adversarial training for unsupervised adaptation  [75] ,  [76] . It has exhibited robust performance at various speech processing tasks including transcribing, retrieving, and indexing speech in English, Mandarin, Malay, Tamil, and other Southeast Asian spoken languages  [77] -  [80] . The base engine we used is trained using a deep neural network (DNN), based on Kaldi speech recognition toolkit. Tested on dev-clean and test-clean conditions  8  of LibriSpeech data, the system achieved 9.32% and 9.49% word error rate (WER), respectively.\n\nNext, we adopted CrystalFeel 9  to extract the emotion intensity features from the speech transcript. CrystalFeel is a collection of five SVM-based algorithms independently trained with tweets labelled with intensity scores for the overall valence, joy, anger, sadness and fear, respectively  [37] . Evaluated on the SemEval-18 affect in tweets dataset  [81] , CrystalFeel achieved high Pearson correlation coefficients of 0.816 (overall valence), 0.708 (joy), 0.740 (anger), 0.700 (fear) and 0.720 (sadness) with human labelled emotion intensity scores  [37] . CrystalFeel has also been useful in generating effective emotion features for predicting and understanding happiness in crowdsourced text  [82]  and predicting news popularity on Facebook  [83] .\n\nFor a given text such as a tweet or a speech transcript, CrystalFeel generates text-based emotion features in terms of the intensity value (from 0: barely noticeable to 1: extremely intense) of the five analytic dimensions. We were able to extract and use the following five text-based emotion intensity features from the speech transcript for each video: tValence, tJoy, tAnger, tFear, tSadness.\n\nDetails on the individual feature description and a bivariate correlational analysis, showing to what extent each feature is correlated with each of the OMG ground truth emotion labels, are provided in Appendix A.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Experiments And Analyses",
      "text": "In order to analyze and compare the effectiveness of different feature sets, we trained a support vector machine (SVM) with a polynomial kernel  10  and tuned the cost function using a grid search approach over the range of [0.01, 10]. This choice of classifier is consistent with recent work in this field (see  [84]  for an example). For the classification models (i.e. classifying finegrained emotion classes), the relevant accuracy measures considered were the raw accuracy and the area under curve (AUC). For the regression models (i.e. predicting arousal and valence scores), the relevant accuracy measure considered was the Pearson correlation coefficient.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Modality Effects: Comparing Unimodal, Bimodal And Trimodal Features",
      "text": "We first tested our multimodal features on the task of predicting continuous-valued arousal and valence scores provided by the OMG dataset. We developed a SVM regression model with a linear kernel and grid-search tuned cost function. Fig.  2  demonstrates the Pearson correlation coefficient from the regression models implemented for various feature combinations.\n\nThe results show that the trimodal model, denoted by V+A+T, performed the best for both valence and arousal.\n\nSpecifically, the best multimodal model outperformed the best unimodal model in Pearson correlation scores by over 23% and 20% for arousal and valence respectively. Interestingly, while the V+A model was found to be the most effective bimodal model in predicting arousal, the V+T and A+T models performed better in predicting valence. Similarly, while the audio-only (A) model was the best performing unimodal model in predicting arousal, the textonly (T) model performed best in predicting valence.\n\nWe then evaluated the multimodal features in classifying the seven emotion categories using a tuned binary SVM, and a 10-fold cross-validation. We applied class weights proportional to the class distribution within each fold to account for the inherent class imbalance.\n\nTable  2  below presents the raw accuracy and AUC scores from this analysis. Our results show that the bimodal and trimodal classifiers exhibited significant improvement in raw accuracy and AUC scores, over the unimodal classifiers to varying extents, across the different emotions. We tested the statistical significance of these improvements using a combination of a paired t-test as well  as a Wilcoxon signed-rank test, and the differences were statistically significant at the 95% confidence level.\n\nTo better quantify the improvements in accuracy, we use a version of the commonly used MM1 score which is specified as follows:\n\nHere, ğ‘€ğ‘ğ‘¥(ğ‘ ğ‘€ğ‘€ ) denotes the accuracy of the best multimodal classifier (A+T or V+T or V+A or V+A+T), ğ‘€ğ‘ğ‘¥(ğ‘ ğ‘ˆğ‘€ ) denotes the accuracy of the best unimodal classifier (A or V or T). The effect size of the MM1 score has been used in a number of recent studies and offers a conservative estimate of the relative improvements from using a multimodal classifier  [2] . Since MM1 measures improvement over a baseline score, the measure is not sensitive to the specific type of classification accuracy measure being used, and hence, offers a flexible and more disciplined way of comparing classifier performance across models.\n\nTable  3  below presents the list of best unimodal and multimodal classifiers together with the associated MM1 effects. The estimates of MM1 scores show a number of interesting patterns. First, the average MM1 scores across the seven emotion classes is 20.2 for the cross-validated model, implying that the multimodal classifiers outperformed unimodal classifiers across all emotions. The highest improvements for multimodal classifiers were observed for the disgust and fear emotions.\n\nThe best performing unimodal classifier for neutral, happiness and sadness was the visual-only (V) classifier, while the best performing unimodal classifier for the anger, disgust, fear, and surprise was the audio-only (A) classifier. Among the multimodal classifiers, the trimodal (V+A+T) classifier outperformed bimodal classifiers across all emotions. The best performing bimodal classifier across all emotion classes was the V+A classifier.\n\nGiven that disgust, fear, and surprise have high class imbalance, for rest of the analyses, we focus our attention on the emotion categories of neutral, happiness, sadness, and anger. We tested the robustness of the multimodal features using a random forest for these four emotion categories. The results from this analysis, detailed in Tables B1 and B2 in Appendix B, exhibit smaller effect sizes but are consistent with our key findings discussed here.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Contextual Effects: Gender And Duration",
      "text": "To enhance our understanding of the how gender of the speaker influences the inference of different emotions using a multimodal system, we performed an analysis of two gender categories (i.e. male vs. female). Using two human annotators, the dataset (2,176 videos) was gender-annotated into 738 male-only and 1,004 female-only videos. Around 434 videos could not be annotated with a specific gender as the gender of the speaker was either not discernible from the video even after inter-rater consultations, or was not uniquely identifiable on account of multiple speakers. For the successful gender annotations, the interrater agreement was 100%. Table  B3  in Appendix B provides the sample distribution across gender and emotion categories.\n\nTable  4  below highlights the MM1 scores for the maleonly and female-only subgroups extracted from the data. Our results show that while multimodal classifiers offered significant 11 improvements over unimodal classifiers for both gender groups, there exists significant variance in the strength of this improvement, as discussed in detail in the next section. As mentioned earlier, we focus our attention only on the neutral, happiness, sadness, and anger emotions for this analysis, as the extent of class imbalance in these categories can be reasonably addressed using class weights and other techniques, e.g. SMOTE analysis  [85] . Similar to the overall data, we tested the robustness of these findings for our gender subgroups using a random forest classifier. These additional results are presented in Tables  B4  and B5  in Appendix B, and are consistent with our key findings discussed next.\n\nThe duration of an episode when an emotion is expressed can vary widely. As described earlier, the duration of episode might be longer or shorter depending on i) the modality e.g., emotions expressed through text take longer to express than through visuals, ii) the nature of medium e.g., short tweets and Vine videos vs. longer YouTube videos, and iii) the specific type of emotions, e.g., happiness takes shorter time to fully express than disgust.\n\nIn order to test the accuracy of the classifiers for the various emotion classes across short and long duration videos, we performed a quartile split on the training set based on the duration of videos, and categorized the first and fourth duration quartiles as short and long duration videos respectively. The first quartile comprised a total of 544 videos ranging in duration from 0.6 seconds to 4.4 seconds, while the fourth quartile comprised a total of 543 videos ranging in duration from 10.5 seconds to 30.7 seconds. Table  B3  in the Appendix B illustrates the sample distribution across duration and emotion categories.\n\nWe then trained an SVM classifier, similar to the one in the previous section, on these two groups of videos. We report the resulting MM1 scores for the two subgroups in Table  5  below. Similar to the gender subgroups, we tested and confirmed the robustness of this analysis using a random forest classifier; the additional results are presented in Tables B6 and B7 as part of Appendix B.\n\nThis analysis shows that multimodal classifiers outperformed unimodal classifiers across emotions for both shorter duration as well as longer duration videos. The only exception to this was the neutral class, where the best multimodal classifier trained on shorter duration videos performed at the same level as the best unimodal classifier (i.e. MM1 = 0). For neutral and happiness, classifiers trained on longer duration videos reported higher MM1 than those trained on shorter videos. However, for sadness and anger, classifiers trained on shorter duration videos reported a higher MM1 than those trained on longer videos.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Discussions",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Key Findings",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Multimodal Emotion Features Perform Significantly Better Than Unimodal Features, But The Modality Effects Differ Across Emotion Types.",
      "text": "One of the key insights from our analyses was the difficulty in classifying the neutral class. For the cross-validated model, the accuracy of classifying the neutral emotion was the lowest for the best unimodal (V, AUC=0.583), best bimodal (V+A, AUC=0.660) and trimodal (V+A+T, AUC=0.688) classifiers, as compared to all other emotions. Furthermore, the MM1 score (ğ‘€ğ‘€1 ğ‘›ğ‘’ğ‘¢ğ‘¡ğ‘Ÿğ‘ğ‘™ = 18.0) for the neutral emotion was lower than the average MM1 score across all emotions (ğ‘€ğ‘€1 ğ‘ğ‘£ğ‘” = 20.2). The same pattern is also consistent across all the gender (i.e. male vs. female) and duration (i.e. shorter vs. longer) subgroups, where the MM1 scores for neutral emotion was found to be smaller than the MM1 scores for other emotions. These findings imply that for classifying neutral states, multimodal classifiers do not offer substantial improvements over unimodal classifiers, as compared to other emotions. More importantly, this reduction in improvement was most salient for shorter duration videos where the best multimodal classifier showed no improvement over the best unimodal classifier (ğ‘€ğ‘€1 ğ‘›ğ‘’ğ‘¢ğ‘¡ğ‘Ÿğ‘ğ‘™ = 0). We found converging evidence for this using a random forest classifier as well (ğ‘€ğ‘€1 ğ‘›ğ‘’ğ‘¢ğ‘¡ğ‘Ÿğ‘ğ‘™ = 3.3). This is consistent with recent work in affect recognition which emphasizes the complexity in conceptualizing what constitutes a neutral class and the problems with decoupling neutral-only videos from neutral segments of all other videos  [84] ,  [86] .  Barros et al.  reported that the neutral class exhibited the highest number of misclassifications in their analysis  [86] . Similarly, Soleymani et al. reported high variance in pupillary response for participants when watching neutral scenes in videos  [84] . Some past work has also shown that it is plausible for classifiers to confuse neutral state with emotions like happiness and sadness  [87] ,  [88] . In our analyses too, we found that the neutral and happiness emotion classes were the most difficult to classify, and showed lower improvements with multiple modalities as compared to other emotions. For all other emotions, multimodal accuracies were significantly higher than the unimodal accuracies for the training set.\n\nInterestingly, we also observed that multimodal classifiers tend to work substantially better than unimodal classifiers for negatively valenced emotions like anger (ğ‘€ğ‘€1 ğ‘ğ‘›ğ‘”ğ‘’ğ‘Ÿ = 20.3 > ğ‘€ğ‘€1 ğ‘ğ‘£ğ‘” = 20.2), disgust (ğ‘€ğ‘€1 ğ‘‘ğ‘–ğ‘ ğ‘”ğ‘¢ğ‘ ğ‘¡ = 29.6 > ğ‘€ğ‘€1 ğ‘ğ‘£ğ‘” = 20.2), and fear (ğ‘€ğ‘€1 ğ‘“ğ‘’ğ‘ğ‘Ÿ = 24.1 > ğ‘€ğ‘€1 ğ‘ğ‘£ğ‘” = 20.2), although there is a possibility that these findings are specific to our data context, particularly given the imbalanced class distribution in disgust and fear categories. However, we found that, for both SVM and random forest classifiers, negative emotions like sadness and anger exhibited better MM1 scores than positively valenced emotions like happiness. This can be partly because these emotions are complex or compound  [26]  and are unlikely to be fully learned through any one modality. In contrast, emotions like happiness have clear markers in at least one modality (e.g., facial smile) and can therefore be learned with reasonable accuracy through unimodal classifiers. This is consistent with recent work by  [89]  which found that facial expressions conveyed happiness the clearest, while vocal features conveyed anger better than other emotions. In our analyses too, we found that the most effective unimodal classifier for the happiness emotion was visual-only (V, AUC = 0.639).\n\nSimilar to fine-grained emotions, the improvement in multimodal performance was substantial in predicting the continuous dimensions like arousal and valence. This is consistent with recent studies that have shown high efficacy of multimodal systems in predicting valence and polarity  [1] ,  [3] ,  [84] . In our analyses, we found that across emotions, bimodal and trimodal regression models outperformed unimodal regression models for both arousal and valence. Among the multimodal models, the trimodal model (V+A+T) outperformed all bimodal models. Further, we found that while audio-related features (V+A and A) performed better than text-related features in predicting arousal, text-related features (A+T, V+T, T) performed better than audio features in predicting valence.\n\nThe gender of the speaker plays a moderating role on the multimodal features' performance. Our analyses on the OMG dataset also uncovered significant gender differences in emotion classification across various emotion classes. For instance, we found that multimodal classifiers outperformed unimodal classifiers for all emotions across the two genders (i.e. MM1 > 0). However, the average MM1 scores for male speakers (ğ‘€ğ‘€1 ğ‘ğ‘£ğ‘”,ğ‘šğ‘ğ‘™ğ‘’ = 9.0) outperformed those for female speakers (ğ‘€ğ‘€1 ğ‘ğ‘£ğ‘”,ğ‘“ğ‘’ğ‘šğ‘ğ‘™ğ‘’ = 5.9) across emotions. One possible explanation for this is that females, as young as 4 to 6 year old, have been found to express complex emotions  [90] , like sadness and anxiety, through a combination of subtle actions that are harder to detect, than for men who generally show limited reactions when expressing complex emotions. This is probably due to a mix of evolutionary limitations and restrictive social norms. For emotions like anger, where reactions like loud vocal expressions are relatively common for both men and women, the classifiers are less likely to show any difference. Recent studies looking at gender differences in emotion experience and perception have also found that women tend to be better than men at picking up certain emotions, and particularly negative emotions, as well as experiencing these emotions themselves  [52] ,  [53] ,  [55] . However, our findings hint that, with the exception of anger, it might be challenging for multimodal classifiers to 12 The improvements in AUC scores were statistically significant at the correctly classify other emotions expressed by female speakers.\n\nEven though, in this study, we do not focus on gender effects in other negative emotions like fear and disgust due to the class imbalance in the available dataset, there have been recent studies that discuss relevant findings. For instance, and with disgust specifically, Al-Shawaf et al. provide a set of evolutionary-functional reasons to explain why women consistently experience higher levels of disgust than men  [54] . It is possible, therefore, that female speakers might be expressing this emotion more vividly than males, making it easier for classifiers to learn this difference. Similarly, for anger, an emotion closely related to disgust, our analyses show that unimodal and multimodal classifiers perform reasonably well for female speakers. Furthermore, for male speakers, the visual-only classifier was found to be the best performing unimodal classifier for happiness, sadness, and anger emotions, while the audioonly classifier performed better for neutral. For female speakers, however, the visual-only classifier was found to be better performing for all emotion classes.\n\nMultimodal emotion features' effectiveness varies across episodic durations. The other contextual factor that we focused on was the duration of the emotion expression episode. We conjectured that the duration of a recorded emotion episode would be contingent on i) data modality, ii) nature of the medium, and iii) the type of emotion. While certain modalities, like text, require longer to fully onset a specific emotion (i.e. we might need to wait for a sentence to end to decipher the full meaning and intent from the speaker), other modalities like visual are quicker and more direct (e.g. a smile to convey happiness). Videos from certain online video sharing websites like YouTube, as is the case with the current dataset, tend to be longer in duration than videos from other platforms like Vine.\n\nOur analyses on classifier performance for shorter vs. longer duration videos show that the average MM1 scores across emotions was comparable for the two duration groups ( ğ‘€ğ‘€1 ğ‘ğ‘£ğ‘”,ğ‘ â„ğ‘œğ‘Ÿğ‘¡ğ‘’ğ‘Ÿ = 8.6, ğ‘€ğ‘€1 ğ‘ğ‘£ğ‘”,ğ‘™ğ‘œğ‘›ğ‘”ğ‘’ğ‘Ÿ = 9.7). However, we noticed variance across the specific emotion categories. For instance, and as mentioned earlier, multimodal classifiers failed to offer any improvement over unimodal classifiers for the neutral emotion when trained with shorter duration videos (ğ‘€ğ‘€1 ğ‘›ğ‘’ğ‘¢ğ‘¡ğ‘Ÿğ‘ğ‘™,ğ‘ â„ğ‘œğ‘Ÿğ‘¡ğ‘’ğ‘Ÿ = 0). For longer duration videos however, there was an improvement (ğ‘€ğ‘€1 ğ‘›ğ‘’ğ‘¢ğ‘¡ğ‘Ÿğ‘ğ‘™,ğ‘™ğ‘œğ‘›ğ‘”ğ‘’ğ‘Ÿ = 6.2). We also found that for neutral and positively valenced emotions, like happiness, the improvements in multimodal classification over unimodal classification were higher, as reflected by a higher MM1 score 12 . However, for negatively valenced emotions like sadness and anger, the opposite was true. This might partly be due to the nature of the specific emotions being expressed. For instance, while negative emotions like sadness and disgust might take longer to fully onset and peak, others such as happiness and surprise tend to be more immediate. More research is needed to fully understand the underlying mechanisms of why these duration effects manifest.\n\n95% confidence level on a Wilcoxon signed -rank test.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Implications, Limitations And Future Research",
      "text": "Taken together, our analyses provide clear and converging empirical evidence supporting the multimodal approach to recognizing and analyzing emotions. More importantly, our findings highlight significant variances based on important contextual factors, like the gender of the speaker and the duration of the emotion episode.\n\nNotably, our findings add to a growing stream of literature that cautions about the prevalence of biases in machine learning datasets and models  [91] ,  [92] . Our findings highlight that the relative performance of emotion recognition systems might vary substantially across genders, durations, emotions, and plausibly, the underlying feature extraction systems. Researchers and developers need to be aware of such variances especially if such systems are being used in a gender-sensitive context (e.g. employee selection based on video interviews).\n\nThere are a number of limitations in the present study that warrants future research. First, the current study uses text, speech and visual as the three data modalities, and uses all of these to compare the performance of the multimodal emotion recognition systems. We note that the feature combination technique used is platform-agnostic, i.e., while these modalities are common sources of emotion data, the relative importance and richness of these modalities might vary across platforms, e.g. Instagram content is likely to be richer in visual than text.\n\nSecond, and as pointed out in  [48] , the various modalities are not temporally aligned within the duration of the emotion episode. This might introduce errors in feature fusion, leading to higher misclassifications.\n\nThird, while our study offers robust analytical insights based on real-world videos, the findings can be limited to the specific dataset we used. As we noted, the OMG dataset, though with a relatively data size and a good gender and duration diversity, included relatively fewer representations of disgust, fear and surprise which prevented us from extending our subgroup analyses to these classes. We also did not have access to the annotators' demographic distribution, which prevented us from studying the sensitivity of our findings, if any, to the annotators' gender.\n\nLast but not the least, in this study, we selectively focused on two important contextual factors: gender and duration. Other factors like i) surrounding scenes, ii) physiological state (e.g., body temperature), iii) conversational contexts, etc., may also affect classifier performance. Similarly, the results are also plausibly sensitive to the performance and biases associated with specific tools, though reasonably effective, that we selected to generate the features. The relative performance of the classifiers might, therefore, also vary as a function of the emotion features extraction technologies, and inherit their innate biases. As technologies advance, future work in this direction shall further explore the effects of these factors on multimodal classifier accuracy.\n\nFuture research can look into building richer datasets, applying more advanced emotion analysis technologies, and studying abstractions that can explain the psychological underpinnings driving the associations between emo-tions, data modalities, genders and other contextual factors. A theoretically sound interpretation of multimodal emotion recognition can benefit not just the development of better systems, but also help preempt biases and explain several inconsistent findings reported in prior work.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusion",
      "text": "This study leverages state-of-the-art visual, audio and text based emotion recognition systems, and presents an empirical analysis to further our understanding of the contextual factors affecting multimodal emotion recognition. While multimodal features consistently outperformed unimodal features, our findings revealed significant variance in improvement across different emotions. This study is also an early attempt at examining the effect of contextual factors like the gender of the speaker or the duration of the emotional episode, on classifier accuracy across modalities and emotion classes.\n\nFrom an applied perspective, the relative strength of the unimodal or bimodal features in comparison to full, multimodal emotion recognition can provide a valuable reference to practitioners. In today's digital platforms, emotion cues are largely extracted from text-based unimodal (e.g., Tweets, Facebook comments), audio-text bimodal (e.g., call center recordings), or visual-text bimodal (e.g., Instagram stories and posts) content. Practitioners can derive a sense of confidence from our multimodal analysis to make an informed assessment of the relative predictive performance of the unimodal and bimodal data sources.\n\nEmotions will continue to play a key role in shaping human experience and communication. The exploratory findings from this study offer valuable empirical insights to help understand and inform how future affective systems leveraging multimodal emotions can be improved with multiple data and communication modalities, different emotions of interest, as well as a wide range of contextual factors including but not limited to gender and duration of the expressive episode. of majority class observations that were randomly selected to be retained in the final sample was equal to the number of newly synthesized minority class observations (i.e., under-sampling parameter = 1) 3  . Tables B4-B7 present performance measures for gender (male vs. female) and duration (short duration vs. long duration) subgroups using a random forest classifier.",
      "page_start": 10,
      "page_end": 18
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows screenshots of six sample vid-",
      "page": 4
    },
    {
      "caption": "Figure 2: demonstrates the Pearson correlation coef-",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Preprint Notice:": "Â© 2021 IEEE.  Personal use of this material is permitted.  Permission from IEEE must be obtained for all"
        },
        {
          "Preprint Notice:": "other uses, in any current or future media, including reprinting/republishing this material for advertising"
        },
        {
          "Preprint Notice:": "or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or"
        },
        {
          "Preprint Notice:": "reuse of any copyrighted component of this work in other works."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Prasanta Bhattacharya, Raj Kumar Gupta, and Yinping Yang": "Abstractâ€” Emotional expressions form a key part of user behavior on todayâ€™s digital platforms. While multimodal emotion"
        },
        {
          "Prasanta Bhattacharya, Raj Kumar Gupta, and Yinping Yang": "recognition techniques are gaining research attention, there is a lack of deeper understanding on how visual and non-visual"
        },
        {
          "Prasanta Bhattacharya, Raj Kumar Gupta, and Yinping Yang": "features can be used to better recognize emotions in certain contexts, but not others. This study analyzes the interplay between"
        },
        {
          "Prasanta Bhattacharya, Raj Kumar Gupta, and Yinping Yang": "the effects of multimodal emotion features derived from facial expressions, tone and text in conjunction with two key contextual"
        },
        {
          "Prasanta Bhattacharya, Raj Kumar Gupta, and Yinping Yang": "factors: i) gender of the speaker, and ii) duration of the emotional episode. Using a large public dataset of 2,176 manually"
        },
        {
          "Prasanta Bhattacharya, Raj Kumar Gupta, and Yinping Yang": "annotated YouTube videos, we found that while multimodal features consistently outperformed bimodal and unimodal features,"
        },
        {
          "Prasanta Bhattacharya, Raj Kumar Gupta, and Yinping Yang": "their performance varied significantly across different emotions, gender and duration contexts. Multimodal features performed"
        },
        {
          "Prasanta Bhattacharya, Raj Kumar Gupta, and Yinping Yang": "particularly better for male speakers in recognizing most emotions. Furthermore, multimodal features performed particularly"
        },
        {
          "Prasanta Bhattacharya, Raj Kumar Gupta, and Yinping Yang": "better for shorter than for longer videos in recognizing neutral and happiness, but not sadness and anger. These findings offer"
        },
        {
          "Prasanta Bhattacharya, Raj Kumar Gupta, and Yinping Yang": "new insights towards the development of more context-aware emotion recognition and empathetic systems."
        },
        {
          "Prasanta Bhattacharya, Raj Kumar Gupta, and Yinping Yang": "Index Termsâ€” Affective computing, Affect sensing and analysis, Modelling human emotions, Multi-modal recognition,"
        },
        {
          "Prasanta Bhattacharya, Raj Kumar Gupta, and Yinping Yang": "Sentiment analysis, Technology & devices for affective computing"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "1 \nINTRODUCTION"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "or  long,  the  interest  surrounding  emotion  recognition"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "has been a key focus area within the field of affective \nF"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "computing  [1]â€“[3].  Recent  advances  in  computing  infra-"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "structure  and  datasets  have  led  to  new  developments  in"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "detecting emotions using multiple types of data modalities"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "like facial and acoustic expressions, linguistic and semantic"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "patterns, body movements, eye gaze patterns and electro-"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "encephalography  signals  [2].  Recent  studies  have  pro-"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "posed  different  design  approaches  to  perform  automatic"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "recognition  of  affective  outcomes  like  valence,  arousal,"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "dominance, and emotion types [1], [2]."
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "The ability to automatically recognize emotions has val-"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "uable  implications  for  a  range  of  applications.  These  in-"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "clude the design of empathetic agents  [4], understanding"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "consumer  behavior  at  scale  from  observational  data  [5],"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "and aiding healthcare professionals in a range of activities"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "from  diagnosing  depressive  symptoms \nin  patients, \nto"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "tracking the onset and progression of autism [6], [7]."
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "This paper focuses on extracting high-level emotion fea-"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "tures from visual, audio and text data modalities by lever-"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "aging  a  number  of  recent  emotion  analysis  technologies."
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "We tested the efficacy of these features in classifying fine-"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "grained  emotions  from  a  real-world  dataset  comprising"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "2,176 labelled video clips, and sought to explore the condi-"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "tions under which multimodal features outperformed bi-"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "modal and unimodal features."
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": ""
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": ""
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "ï‚·  P. Bhattacharya is with the Institute of High Performance Computing,"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": ""
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "Agency for Science, Technology and Research (A*STAR), #16-16 Connexis"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": ""
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "North, 1 Fusionopolis Way, Singapore 138632 E-mail:"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "prasanta_bhattacharya@ihpc.a-star.edu.sg."
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "ï‚·  R.K. Gupta is with the Institute of High Performance Computing, Agency"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": ""
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "for Science, Technology and Research, #16-16 Connexis North, 1 Fu-"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": ""
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "sionopolis Way, Singapore 138632 E-mail: gupta-rk@ihpc.a-star.edu.sg."
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "ï‚·  Y. Yang is with the Institute of High Performance Computing, Agency for"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "Science, Technology and Research (A*STAR), #16-16 Connexis North, 1"
        },
        {
          "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”": "Fusionopolis Way, Singapore 138632 E-mail: yangyp@ihpc.a-star.edu.sg."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2": "However,  to  the  best  of  our  knowledge,  the  relationship",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "valence-arousal scale, known as the circumplex model of"
        },
        {
          "2": "between episodic duration of emotions and classifier per-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "affect  [34]  .  For  instance,  in  the  circumplex  model  [34]  ,"
        },
        {
          "2": "formance  in  videos,  as  well  as  the  associated  underpin-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "â€œhappyâ€ is associated with a positive valence and moder-"
        },
        {
          "2": "nings,  are  still  not  well  understood.  Our  analyses  show",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "ate to high arousal. In contrast, both â€œtensedâ€ and â€œangryâ€"
        },
        {
          "2": "that duration played an important role in affecting classi-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "have  a  negative  valence  with  moderate  to  high  arousal,"
        },
        {
          "2": "fier performance. Specifically, while multimodal classifiers",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "while â€œsadâ€ is associated with a negative valence and low"
        },
        {
          "2": "outperformed unimodal classifiers for videos of all dura-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "arousal. Third, we also consider emotion intensity as an im-"
        },
        {
          "2": "tions, this relative improvement was higher in shorter epi-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "portant  concept.  This  aspect  of  emotion  concerns  the  de-"
        },
        {
          "2": "sodic durations for only the negative emotions like sadness",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "gree  or  â€œdepthâ€  of  emotional  experience,  ranging  from"
        },
        {
          "2": "and anger. In contrast, the relative improvement from us-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "barely noticeable to most intense imaginable [26], [35], [36]."
        },
        {
          "2": "ing multiple modalities was higher in longer episodic du-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "For instance, the anger emotions can range from low-inten-"
        },
        {
          "2": "rations for neutral and happiness emotions.",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "sity annoyance to high-intensity rage, and this difference"
        },
        {
          "2": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "represents  a  relatively  new,  less  explored  aspect  of  emo-"
        },
        {
          "2": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "tional  information  that  can  benefit  the  development  of"
        },
        {
          "2": "2  CONCEPTS AND RELATED WORK",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "computational methods (e.g., [37])."
        },
        {
          "2": "Expressing  and  perceiving  emotions  is  central  to  human",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "2.2 Multimodal vs Unimodal Recognition Systems"
        },
        {
          "2": "experience and is a key contributor to the sustenance of in-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "2": "terpersonal communication [12]â€“[14]. Emotions also affect",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Early research into emotion recognition systems largely fo-"
        },
        {
          "2": "how we interpret our environment, develop opinions and",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "cused  on  single  data  modalities  (e.g.,  only  facial  expres-"
        },
        {
          "2": "form  judgments  about  the  surrounding  individuals  and",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "sions or tone of voice) and on emotions extracted from en-"
        },
        {
          "2": "situations, and even drive behaviors [15].",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "acted sequences by trained actors [38]. More recently, stud-"
        },
        {
          "2": "Within the overarching category of â€˜affectâ€™, it is gener-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "ies have attempted to combine signals from multiple mo-"
        },
        {
          "2": "ally accepted that â€˜emotionâ€™ is distinct from related states",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "dalities such as facial expressions and audio [39], [40], au-"
        },
        {
          "2": "like  mood  and  temperament.  Emotions  might  involve  a",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "dio  and  written  text  [41],  [42],  physiological  signals  [43],"
        },
        {
          "2": "range of feelings which may be transient, like relief, or last",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "and various combinations of these modalities [44]. Overall,"
        },
        {
          "2": "for relatively long periods of time, as is the case with mood,",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "studies  in  this  field  have  started  to  emphasize  the  im-"
        },
        {
          "2": "or  really  long  spans  of  time,  as  with  temperament  [16]â€“",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "portance of multimodal signals of emotions in more natu-"
        },
        {
          "2": "[18]. Some of the earliest definitions about emotions came",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "ralistic scenarios (See [2], [3] for detailed reviews)."
        },
        {
          "2": "independently from psychologist William James and phys-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "However, despite notable efforts, the existing literature"
        },
        {
          "2": "iologist Carl Lange who contended that emotions arise nat-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "on  emotion  recognition  is  limited  in  three  ways.  First,"
        },
        {
          "2": "urally as a result of environmental stimulus, and that our",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "while there is growing evidence that multimodal systems"
        },
        {
          "2": "interpretation  of \nthe  physiological \nsensation  of \nthese",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "generally outperform unimodal systems in relevant tasks"
        },
        {
          "2": "changes constitutes emotion [19]. Contrary to Jamesâ€™ con-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "[2], [40], [45], recent results have shown more complex pat-"
        },
        {
          "2": "tention that emotions could be a response to environmen-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "terns.  For  example,  a  number  of  studies  have  found  that"
        },
        {
          "2": "tal  changes,  W.B.  Cannon  treated  emotions  as  being  felt",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "multimodal systems can often exhibit negligible improve-"
        },
        {
          "2": "first  but  expressed  outwardly  in  certain  behaviors  much",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "ments or even reductions in performance [39], [46]. Second,"
        },
        {
          "2": "later [20]. There is also debate over how emotions can be",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "a vast majority of existing analyses on multimodal systems"
        },
        {
          "2": "perceived or described [13], [14], [21].",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "were based on relatively small samples (i.e. fewer than 50"
        },
        {
          "2": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "participants), and using mostly bimodal classifiers. Third,"
        },
        {
          "2": "2.1 Emotion Type, Valence, Arousal and Intensity",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "there is also scant work on understanding the role of vari-"
        },
        {
          "2": "For  the  purpose  of  this  paper,  we  adopt  three  pertinent",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "ous kinds of contextual factors that might directly or indi-"
        },
        {
          "2": "concepts from the emotion literature to facilitate the subse-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "rectly  influence  model  performance  (e.g.  de  Gelder  et  al."
        },
        {
          "2": "quent emotion feature extraction and analysis.",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "[47]). With the exception of a few studies, the extant litera-"
        },
        {
          "2": "First, we refer to the stream of work on categorization of",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "ture does not offer an adequate understanding of why cer-"
        },
        {
          "2": "emotions  which  views  emotions  as  discrete  and  distinct",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "tain data modalities work better for specific emotions, but"
        },
        {
          "2": "types  such as happiness (or  joy), anger, sadness and fear",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "not others [48]. It is therefore important to address the fun-"
        },
        {
          "2": "[22]â€“[26].  This  view,  while  â€œsimplisticâ€[27],  [28],  helps  to",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "damental question of which data modalities are best suited"
        },
        {
          "2": "focus the present study in terms of differentiating the spe-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "for certain emotions, and the role of contextual factors in"
        },
        {
          "2": "cific effects due to data modality, duration and gender for",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "affecting the performance of these multimodal systems."
        },
        {
          "2": "various distinct emotion types. Second, we leverage the di-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "2.3  Contextual Factors: Gender and Duration"
        },
        {
          "2": "mensional view of emotion that has received growing pop-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "2": "ularity in recent years [29]â€“[33]. In this study, we specifi-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "How does the performance of multimodal emotion recog-"
        },
        {
          "2": "cally considered two dimensions: valence and arousal. Va-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "nition systems change in different contexts? A few recent"
        },
        {
          "2": "lence  describes  the  degree  of  pleasantness  or  unpleasant-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "studies, such as [39], [47], [49], [50], have started to explore"
        },
        {
          "2": "ness of a signal and is generally measured on a continuous",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "how  contextual  factors  can  potentially  affect  the  perfor-"
        },
        {
          "2": "scale ranging from positive (i.e. pleasant) to negative (i.e.",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "mance  of  emotion  related  classifiers.  For \ninstance,  de"
        },
        {
          "2": "unpleasant). Arousal, on the other hand, refers to the extent",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Gelder et al. investigated how the presence of information"
        },
        {
          "2": "of  physical  activation,  ranging  from  no  arousal  to  very",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "on the visual and auditory context (e.g. information about"
        },
        {
          "2": "high  arousal.  Thus,  various  emotional  states  like  happy,",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "natural scene, vocal expressions etc.) can benefit the recog-"
        },
        {
          "2": "angry, sad and afraid can be mapped to a two-dimensional",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "nition  of  facial  expressions  [47].  Contextual  information"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: presents the basic descriptive statistics of emo-",
      "data": [
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "can also come from the same modality. For instance, Met-\n3  DATA"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "allinou et al. showed that emotions within as well as across"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "3.1 Data Source"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "utterances  in  a  dialog  sequence  can  be  leveraged  to  im-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "We make use of the One-Minute-Gradual (OMG) Emotion"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "prove  emotion  recognition  [39].  These  studies  have  fo-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "Dataset [57] for our analyses. The dataset was released as"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "cused  primarily on external  contexts (e.g., speakerâ€™s envi-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "part  of  the  2018  OMG  Emotional  Behavior  Challenge 1"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "ronment, emotion eliciting factors)."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "where  the  original  task  was  to  implement  an  emotion-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "A relatively new direction is the interplay of internal con-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "recognition  system  to  accurately  predict  arousal  and  va-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "texts manifested through individual differences, like gen-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "lence scores using an annotated dataset.  The original OMG"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "der,  and  emotions.  For  example,  Brebner  analyzed  two"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "train dataset2 contains YouTube links of 2,444 public video"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "samples  comprising  Australian  and  International  partici-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "clips. We were able to retrieve 2,176 clips as of May 2018"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "pants for differences in the frequency and intensity of self-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "using the links provided, as some links became unavaila-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "reported emotions by gender [51]. The results highlighted \nble  over  time.  Each  video  clipâ€™s  duration  ranged  around"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "significant differences for affection and sadness emotions, \nthe one-minute mark in general, but with some shorter and"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "longer duration videos. Most of the videos were of actors \nwhere females scored higher on both frequency and inten-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "and  actresses  who  were  practicing  lines  or  monologues \nsity.  Males,  on  the  other  hand,  scored  higher  on  pride  in"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "probably in preparation for auditions. Through these mon-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "both frequency and intensity. This is not entirely surpris-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "ologues, the actors and actresses exhibited a number of dis-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "ing,  and  a  number  of  studies  have  attempted  to  explain"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "crete emotions (e.g., happiness, sadness, surprise), of var-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "gender  differences  in  how  emotions  are  experienced  and"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "ying arousals. Fig. 1 shows screenshots of six sample vid-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "perceived in others [8], [52], [53]. For instance, it has been"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "eos from this dataset."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "shown that females generally perform better than men at"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "perceiving and exhibiting negative emotions like sadness"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "and fear [54]. The reasons for this range from evolutionary"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "(e.g.,  Babchuck  et  al.â€™s  primary  caretaker  hypothesis  or"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "Hampson et al.â€™s fitness threat hypothesis [52], [55]) to bi-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "ological [54] to social and normative factors [56]. However,"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "it remains to be understood whether and how the perfor-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "mance of multimodal emotion features varies as a result of"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "such gender differences."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "Another contextual factor that has attracted recent atten-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "tion, and motivates the present study, is the role of temporal"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "contexts.  Recent  studies  have  shown  that  the  temporal"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "Fig.1. OMG-Emotion dataset (left to right, top to bottom:  Actor/actress"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "length  of  the  specific  emotion  being  recorded  is  likely  to"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "demonstrating happiness, sadness, anger, disgust, fear and surprise)"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "influence the effectiveness of the classifier [48], [49]. This is"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "likely due to a combination of three major factors. First, the"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "3.2 Emotion Ground Truth Labels"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "duration of an emotion expression episode is likely to vary"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "The  dataset  also  provides  utterance-level  (approximately"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "as a function of the data modality [49]. For example, Lin-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "10 seconds per utterance) majority-voted ground truth la-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "genfesler  et  al.  [48]  showed  that  an  asynchronous  fusion"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "bels (â€œEmotionMaxVoteâ€) for seven discrete emotion cate-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "algorithm that takes into account different onset and offset"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "gories,  namely  anger,  fear,  happiness,  neutral,  sadness,"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "times  for  emotions  in  various  modalities,  outperformed"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "disgust, and surprise3. For each clip, the majority vote label"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "synchronous fusion algorithms. This is particularly useful"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "was  calculated by  having  five  different  annotations  from"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "since certain modalities, like language, spoken or written, \ncrowdsourced Amazon Mechanical Turk workers [57]."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "take  longer  to  manifest  emotions,  while  others  like  facial \nTable 1 presents the basic descriptive statistics of emo-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "tion  categories,  sorted  by  the  most  balanced  to  the  most \nexpressions,  take  shorter  time.  Second,  the  nature  of  the"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "imbalanced  classes.  We  note  that  neutral  and  happiness \nplatform or device through which the emotions are being"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "had acceptable levels of imbalance, while sadness and an-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "recorded often influences the intensity and duration of the"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "ger exhibited higher class imbalance. Disgust, fear and sur-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "emotion. For instance, users recording a vlog on YouTube"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "prise classes had the highest class imbalance. The relatively"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "are  likely  to  express  emotions  over  longer  durations,  as"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "small number of disgust, fear and surprise cases in the da-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "compared to users uploading a much shorter Vine video."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "taset is an important limitation. It is for this reason that we"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "Lastly, the specific types of emotion might also be associ-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "choose  to  perform  the  subgroup  analyses  using  only  the"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "ated with the duration of the episode. In this study, we in-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "neutral, happiness, sadness, and anger emotion categories."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "vestigate how the duration of the videos affects the perfor-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "mance of multimodal features across various emotions."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "1 The OMG-Emotion Challenge: https://www2.informatik.uni-ham-\n3 The 2018 OMG Challenge required participants to predict valence and"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "burg.de/wtm/omgchallenges/omg_emotion.html \narousal scores only, and not discrete emotion categories."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "2 The data is downloaded from:  https://github.com/knowledgetechnolo-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n3": "gyuhh/OMGEmotionChallenge/blob/master/omg_TrainVideos.csv"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: indicating whether, or not, a face is expressing a particular",
      "data": [
        {
          "4": "TABLE 1",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "indicating whether, or not, a face is expressing a particular"
        },
        {
          "4": "DESCRIPTIVE STATISTICS OF THE DATASET",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "emotional  state  aligned  with \nthe  circumplex  emotion"
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "model [34]. Taken together, we extracted a total of 28 vis-"
        },
        {
          "4": "#, count of videos",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "ual-based  emotion  features,  namely  vValence,  vArousal,"
        },
        {
          "4": "%, out of the total of 2,176 videos",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "vIntensity,  vAfraid,  vAlarmed,  vAnnoyed,  vAroused,  vAston-"
        },
        {
          "4": "Neu-\nHappi-\nSad-\nAn-\nDis-\nSur-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "ished, vBored, vCalm, vContent, vDelighted, vDepressed, vDis-"
        },
        {
          "4": "Fear",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "4": "tral \nness \nness \nger \ngust \nprise",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "tressed,  vDroopy,  vExcited,  vFrustrated,  vGloomy,  vHappy,"
        },
        {
          "4": "772 \n652 \n315  \n260  \n100  \n53  \n24",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "vMiserable,  vNeutral,  vPleased,  vSad,  vSatisfied,  vSerene,"
        },
        {
          "4": "35.5% \n14.5% \n12.0% \n4.6% \n2.4% \n1.1% \n30.0%",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "vSleepy, vTensed, and vTired."
        },
        {
          "4": "Here, it is useful to note that the class labels refer to the",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "4.2 Audio-based Emotion Feature Extraction"
        },
        {
          "4": "emotion  ground  truth  of  the  speakers  as  perceived  by  the",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "In  addition  to  visual  features,  the  acoustic  patterns  of"
        },
        {
          "4": "multiple independent annotators. It is different from  self-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "speech  (e.g.,  speaking  speed,  pitch,  intonation)  provide"
        },
        {
          "4": "reported or recalled emotional experiences, as used in tradi-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "different sources of emotion cues. For pre-processing, we"
        },
        {
          "4": "tional psychological studies.",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "first  applied  FFMPEG 5  to  extract  the  audio  component"
        },
        {
          "4": "3.3 Gender and Duration Labels",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "from the videos. We then extracted the audio-based emo-"
        },
        {
          "4": "For this research, we manually annotated the gender of the",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "tion \nfeatures  using \nthe  AcousEmo 6  system,  which  was"
        },
        {
          "4": "speaker. We coded a discrete gender variable with three cat-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "trained  with  acoustic  analysis  of  emotion  signals  using"
        },
        {
          "4": "egories:  â€œfemaleâ€,  â€œmaleâ€,  and  â€œothersâ€,  for  when  it  was",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "deep  Boltzmann  machines  (DBM)  ([64]â€“[69]).  The  DBM"
        },
        {
          "4": "either not possible to assess the gender or when there were",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "classifier  showed  an  effective  predictive  accuracy,  meas-"
        },
        {
          "4": "multiple speakers in one utterance. We calculated the du-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "ured as unweighted average recall, of 53.6, 64.4, 56.3 and"
        },
        {
          "4": "ration  of  each  video  using  the  start  and  end  time  of  the",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "53.1 over valence, arousal, power and expectancy dimen-"
        },
        {
          "4": "video clips mentioned in the dataset.",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "sions,  respectively,  when  evaluated  using  the  2011  Au-"
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "dio/Visual Emotion Challenge dataset [70] (see details de-"
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "scribed in [69])."
        },
        {
          "4": "4  EMOTION FEATURE EXTRACTION",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "For a given audio, AcousEmo produces two main out-"
        },
        {
          "4": "As the purpose of our study is to understand the effects of",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "puts:  aValence  and  aArousal,  indicating  the  degree  of  va-"
        },
        {
          "4": "contextual  factors  affecting  multimodal  emotion  recogni-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "lence (from very unpleasant to very pleasant) and the de-"
        },
        {
          "4": "tion, our feature extraction strategy focuses on extracting",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "gree of arousal (from no to very high physical activation)"
        },
        {
          "4": "theoretically explainable, high-level emotion features from the",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "detected from the acoustic signals. The system also gener-"
        },
        {
          "4": "visual, audio and text channels, respectively. Hence, even",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "ates  two  additional  emotion  dimensions:  aPower  (which"
        },
        {
          "4": "though it was possible to extract a large number of lower-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "subsumes two  related concepts of the  power and control"
        },
        {
          "4": "level  visual  and  non-visual  features  (e.g.,  facial  action",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "that  a  speaker  is  expressing)  and  aExpectancy  (which  re-"
        },
        {
          "4": "units, word embeddings), we chose to leverage a set of pre-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "flects  whether  the  speaker  is  anticipating  or  expecting"
        },
        {
          "4": "trained  emotion  analysis  systems  that  are  grounded  in",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "things to happen) (see theoretical discussion of the two ad-"
        },
        {
          "4": "data and key emotion concepts (see Section 2.1) to generate",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "ditional dimensions [71] and the AVEC2011 challenge [70]"
        },
        {
          "4": "high-level emotion features.",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "for details). For the present OMG data, we extracted a total"
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "of  ten  audio-based  emotion  features:  aValence,  aArousal,"
        },
        {
          "4": "4.1 Visual-based Emotion Feature Extraction",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "aPower, aExpectancy, aIntensity, aJoy, aAnger, aFear, aSadness,"
        },
        {
          "4": "Among all modalities, facial expressions (e.g., frown, smil-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "aNeutral."
        },
        {
          "4": "ing) provide arguably the most intuitive emotion cues. We",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "4.3 Text-based Emotion Feature Extraction"
        },
        {
          "4": "used a face emotion analysis system called FEA4 to extract",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "4": "the visual features. FEA was trained with visual profiling",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "In  addition  to  the  visual  and  acoustic  features,  the  lan-"
        },
        {
          "4": "techniques from static images [58], and was later extended",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "guage content of a speakerâ€™s speech in a video (e.g., word"
        },
        {
          "4": "to process a set of image frames in videos using deep learn-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "choice, linguistic meaning) can also carry valuable emotion"
        },
        {
          "4": "ing  techniques  (see  details  in  [58]â€“[62]).  The  system  has",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "cues. Before extracting the text-based emotion features, we"
        },
        {
          "4": "been tested on AffectNet test set  [63], and achieved Con-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "first  obtained  the  speech  transcript  from  the  audio  using"
        },
        {
          "4": "cordance Correlation Coefficients (CCC) of 0.527 and 0.569",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "an Automatic Speech Recognition (ASR) engine developed"
        },
        {
          "4": "for valence and arousal prediction respectively.",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "by  I2R7.  This  engine  incorporates  a  range  of  speech  pro-"
        },
        {
          "4": "For each video, the FEA system generates vValence and",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "cessing  techniques \nincluding  sub-harmonic  ratio  based"
        },
        {
          "4": "vArousal, which indicate the degree of valence (from very",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "voice  activity  detection  [72],  mismatched  crowdsourcing"
        },
        {
          "4": "unpleasant  to  very  pleasant)  and  the  degree  of  arousal",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "for acoustic modeling [73],  multi-task learning for pronun-"
        },
        {
          "4": "(from  no  to  very  high  physical  activation)  detected  from",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "ciation modeling [74], and multi-task adversarial training"
        },
        {
          "4": "the speakerâ€™s facial expressions. A third feature, vIntensity,",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "for unsupervised adaptation [75], [76]. It has exhibited ro-"
        },
        {
          "4": "measures the distance to the neutral state, calculated as the",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "bust  performance  at  various  speech  processing  tasks  in-"
        },
        {
          "4": "square root of the sum of squares of vValence and vArousal.",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "cluding  transcribing,  retrieving,  and  indexing  speech  in"
        },
        {
          "4": "The outputs also include 25 binary features (1: yes; 0: no)",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "English,  Mandarin,  Malay,  Tamil,  and  other  Southeast"
        },
        {
          "4": "4\n FEA \nis \naccessible \nvia \nthe \nauthors \n[58]â€™ \n \npage",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "5 FFMPEG is accessed via https://github.com/FFmpeg/FFmpeg"
        },
        {
          "4": "https://sites.google.com/site/vonikakis/software-code/appeal-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "6 AcousEmo is accessible by contacting the authors [65]."
        },
        {
          "4": "ing_slideshows or via https://opsis.sg (commercial version).",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "7 The I2R ASR engine is accessible by contacting the authors [75]."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 2: below presents the raw accuracy and AUC",
      "data": [
        {
          "CLASSIFIER BASED ON 10-FOLD CROSS-VALIDATION": ""
        },
        {
          "CLASSIFIER BASED ON 10-FOLD CROSS-VALIDATION": ""
        },
        {
          "CLASSIFIER BASED ON 10-FOLD CROSS-VALIDATION": ""
        },
        {
          "CLASSIFIER BASED ON 10-FOLD CROSS-VALIDATION": ""
        },
        {
          "CLASSIFIER BASED ON 10-FOLD CROSS-VALIDATION": "57.3"
        },
        {
          "CLASSIFIER BASED ON 10-FOLD CROSS-VALIDATION": "50.9"
        },
        {
          "CLASSIFIER BASED ON 10-FOLD CROSS-VALIDATION": ""
        },
        {
          "CLASSIFIER BASED ON 10-FOLD CROSS-VALIDATION": "42.1"
        },
        {
          "CLASSIFIER BASED ON 10-FOLD CROSS-VALIDATION": ""
        },
        {
          "CLASSIFIER BASED ON 10-FOLD CROSS-VALIDATION": "62.0"
        },
        {
          "CLASSIFIER BASED ON 10-FOLD CROSS-VALIDATION": ""
        },
        {
          "CLASSIFIER BASED ON 10-FOLD CROSS-VALIDATION": "59.5"
        },
        {
          "CLASSIFIER BASED ON 10-FOLD CROSS-VALIDATION": "56. 6"
        },
        {
          "CLASSIFIER BASED ON 10-FOLD CROSS-VALIDATION": "64.5"
        },
        {
          "CLASSIFIER BASED ON 10-FOLD CROSS-VALIDATION": ""
        },
        {
          "CLASSIFIER BASED ON 10-FOLD CROSS-VALIDATION": ".583"
        },
        {
          "CLASSIFIER BASED ON 10-FOLD CROSS-VALIDATION": ".569"
        },
        {
          "CLASSIFIER BASED ON 10-FOLD CROSS-VALIDATION": ""
        },
        {
          "CLASSIFIER BASED ON 10-FOLD CROSS-VALIDATION": ".536"
        },
        {
          "CLASSIFIER BASED ON 10-FOLD CROSS-VALIDATION": ""
        },
        {
          "CLASSIFIER BASED ON 10-FOLD CROSS-VALIDATION": ".660"
        },
        {
          "CLASSIFIER BASED ON 10-FOLD CROSS-VALIDATION": ""
        },
        {
          "CLASSIFIER BASED ON 10-FOLD CROSS-VALIDATION": ".624"
        },
        {
          "CLASSIFIER BASED ON 10-FOLD CROSS-VALIDATION": ""
        },
        {
          "CLASSIFIER BASED ON 10-FOLD CROSS-VALIDATION": ".631"
        },
        {
          "CLASSIFIER BASED ON 10-FOLD CROSS-VALIDATION": ""
        },
        {
          "CLASSIFIER BASED ON 10-FOLD CROSS-VALIDATION": ".688"
        },
        {
          "CLASSIFIER BASED ON 10-FOLD CROSS-VALIDATION": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 2: below presents the raw accuracy and AUC",
      "data": [
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "Asian  spoken  languages  [77]â€“[80].  The  base  engine  we \nSpecifically, the best multimodal model outperformed the"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "used is trained using a deep neural network (DNN), based \nbest unimodal model in Pearson correlation scores by over"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "on  Kaldi  speech  recognition  toolkit.  Tested  on  dev-clean \n23% and 20% for arousal and valence respectively.  Inter-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "and test-clean conditions8 of LibriSpeech data, the system \nestingly, while the V+A model was found to be the most"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "achieved 9.32% and 9.49% word error rate (WER), respec-\neffective bimodal model in predicting arousal, the V+T and"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "tively.  \nA+T models performed better in predicting valence. Simi-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "Next, we adopted CrystalFeel9 to extract the emotion in-\nlarly,  while  the  audio-only  (A)  model  was  the  best  per-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "tensity features from the speech transcript. CrystalFeel is a"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "collection  of  five  SVM-based  algorithms  independently"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "trained  with  tweets  labelled  with  intensity  scores  for  the"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "overall valence, joy, anger, sadness and fear, respectively"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "[37]. Evaluated on the SemEval-18 affect in tweets dataset"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "[81], CrystalFeel achieved high Pearson correlation coeffi-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "cients of 0.816 (overall valence), 0.708 (joy), 0.740 (anger),"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "0.700 (fear) and 0.720 (sadness) with human labelled emo-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "tion intensity scores [37]. CrystalFeel has also been useful"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "in generating effective emotion features for predicting and"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "understanding  happiness  in  crowdsourced  text  [82]  and"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "Fig.2. Pearson correlation coefficients for cross-validated (CV) models"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "predicting news popularity on Facebook [83]."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "For a given text such as a tweet or a speech transcript,"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "forming  unimodal  model  in  predicting  arousal,  the  text-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "CrystalFeel generates text-based emotion features in terms"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "only (T) model performed best in predicting valence."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "of  the  intensity  value  (from  0:  barely  noticeable  to  1:  ex-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "We then evaluated the multimodal features in classify-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "tremely intense) of the five analytic dimensions. We were"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "ing  the  seven  emotion  categories  using  a  tuned  binary"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "able to extract and use the following five text-based emo-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "SVM,  and  a  10-fold  cross-validation.  We  applied  class"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "tion intensity features from the speech transcript for each"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "weights proportional to the class distribution within each"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "video: tValence, tJoy, tAnger, tFear, tSadness."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "fold to account for the inherent class imbalance."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "Details on the individual feature description and a biva-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "Table  2  below  presents  the  raw  accuracy  and  AUC"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "riate  correlational  analysis,  showing  to  what  extent  each"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "scores  from  this  analysis.  Our  results  show  that  the  bi-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "feature is correlated with  each of the  OMG ground truth"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "modal  and  trimodal  classifiers  exhibited  significant  im-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "emotion labels, are provided in Appendix A."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "provement in raw accuracy and AUC scores, over the uni-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "modal  classifiers  to  varying  extents,  across  the  different"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "emotions. We tested the statistical significance of these im-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "5 \nEXPERIMENTS AND ANALYSES"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "provements using a combination of a paired t-test as well"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "In order to analyze and compare the effectiveness of differ-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "ent  feature  sets,  we  trained  a  support  vector  machine"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "TABLE 2"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "(SVM) with a polynomial kernel10 and tuned the cost func-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "RAW ACCURACY AND AUC RESULTS FOR EMOTION"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "tion using a grid search approach over the range of [0.01,"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "CLASSIFIER BASED ON 10-FOLD CROSS-VALIDATION"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "10]. This choice of classifier is consistent with recent work"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "Neutral  Happiness Sadness \nAnger \nDisgust \nFear \nSurprise"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "Modality / \nin this field (see [84] for an example)."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "Emotion \nFor \nthe \nclassification  models \n(i.e. \nclassifying \nfine-\nRaw Accuracy"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "grained emotion classes), the relevant accuracy  measures \nVisual \n57.3 \n71.6 \n75.8 \n61.8 \n56.8 \n53.1 \n73.9"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "considered  were  the  raw  accuracy  and  the  area  under \nAudio \n50.9 \n54.2 \n68.5 \n79.0 \n77.5 \n76.8 \n86.0"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "curve  (AUC).  For  the  regression  models  (i.e.  predicting"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "Text \n42.1 \n66.6 \n84.3 \n79.5 \n85.3 \n43.1 \n44.4"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "arousal and valence scores), the relevant accuracy measure"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "V + A \n62.0 \n74.9 \n76.2 \n76.5 \n82.6 \n72.9 \n89.6"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "considered was the Pearson correlation coefficient."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "V + T \n59.5 \n71.1 \n77.8 \n65.6 \n72.9 \n63.0 \n78.6"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "A + T \n56. 6 \n59.1 \n78.2 \n79.7 \n89.2 \n86.4 \n92.5 \n5.1 Modality Effects: Comparing Unimodal,"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "Bimodal and Trimodal Features  \nV + A + T \n64.5 \n75.6 \n82.4 \n80.9 \n88.3 \n84.1 \n95.6"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "We first tested our multimodal features on the task of pre-\n \nAUC"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "dicting continuous-valued arousal and valence scores pro-\nVisual \n.583 \n.639 \n.674 \n.608 \n.593 \n.649 \n.724"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "vided by the OMG dataset. We developed a SVM regres-\nAudio \n.569 \n.598 \n.635 \n.651 \n.673 \n.725 \n.785"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "sion model with a linear kernel and grid-search tuned cost"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "Text \n.536 \n.598 \n.599 \n.571 \n.581 \n.598 \n.616"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "function. Fig. 2 demonstrates the Pearson correlation coef-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "V + A \n.660 \n.695 \n.735 \n.720 \n.790 \n.842 \n.927"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "ficient from the regression  models implemented for vari-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "V + T \n.624 \n.665 \n.724 \n.661 \n.758 \n.746 \n.851"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "ous feature combinations."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "A + T \n.631 \n.642 \n.714 \n.703 \n.758 \n.802 \n.880"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "The results show that the trimodal model, denoted by"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "V + A + T \n.688 \n.714 \n.799 \n.783 \n.872 \n.900 \n.937"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "V+A+T, performed the best for both valence and arousal."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "8 Kaldi training under different conditions:  https://github.com/kaldi-\n9 CrystalFeel is accessible via the authorsâ€™ [37] page https://socialanalyt-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "asr/kaldi/blob/master/egs/librispeech/s5/RESULTS  \nicsplus.net/crystalfeel."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n5": "10 The subgroup analyses were performed using a linear kernel."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 3: below presents the list of best unimodal and tated into 738 male-only and 1,004 female-only videos.",
      "data": [
        {
          "6": "as  a  Wilcoxon  signed-rank  test,  and  the  differences  were",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "(V+A+T) classifier outperformed bimodal classifiers across"
        },
        {
          "6": "statistically significant at the 95% confidence level.",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "all emotions. The best performing bimodal classifier across"
        },
        {
          "6": "To  better  quantify  the  improvements  in  accuracy,  we",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "all emotion classes was the V+A classifier."
        },
        {
          "6": "use a version of the commonly used MM1 score which is",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Given that disgust, fear, and surprise have high class"
        },
        {
          "6": "specified as follows:",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "imbalance, for rest of the analyses, we focus our attention"
        },
        {
          "6": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "on the emotion categories of neutral, happiness, sadness,"
        },
        {
          "6": "(1) \nMM1=100*[ğ‘€ğ‘ğ‘¥(ğ‘ğ‘€ğ‘€) âˆ’  ğ‘€ğ‘ğ‘¥(ğ‘ğ‘ˆğ‘€)] ğ‘€ğ‘ğ‘¥(ğ‘ğ‘ˆğ‘€)",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "and anger. We tested the robustness of the multimodal fea-"
        },
        {
          "6": "Here, ğ‘€ğ‘ğ‘¥(ğ‘ğ‘€ğ‘€) denotes the accuracy of the best multi-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "tures using a random forest for these four emotion catego-"
        },
        {
          "6": "modal \nclassifier \n(A+T  or  V+T  or  V+A  or  V+A+T),",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "ries.  The  results  from  this  analysis,  detailed  in  Tables  B1"
        },
        {
          "6": "ğ‘€ğ‘ğ‘¥(ğ‘ğ‘ˆğ‘€) denotes the accuracy of the best unimodal clas-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "and  B2  in  Appendix  B,  exhibit  smaller  effect  sizes  but  are"
        },
        {
          "6": "sifier  (A or  V or  T). The effect size of the  MM1 score has",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "consistent with our key findings discussed here."
        },
        {
          "6": "been used in a number of recent studies and offers a con-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "5.2 Contextual Effects: Gender and Duration"
        },
        {
          "6": "servative estimate of the relative improvements from using",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "To  enhance  our  understanding  of  the  how  gender  of  the"
        },
        {
          "6": "a multimodal classifier [2]. Since MM1 measures improve-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "speaker influences the inference of different emotions us-"
        },
        {
          "6": "ment over a baseline score, the measure is not sensitive to",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "ing a multimodal system, we performed an analysis of two"
        },
        {
          "6": "the specific type of classification accuracy  measure being",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "gender categories (i.e. male vs. female). Using two human"
        },
        {
          "6": "used, and hence, offers a flexible and more disciplined way",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "annotators,  the  dataset  (2,176  videos)  was  gender-anno-"
        },
        {
          "6": "of comparing classifier performance across models.",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "tated  into  738  male-only  and  1,004  female-only  videos."
        },
        {
          "6": "Table  3  below  presents  the  list  of  best  unimodal  and",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Around 434 videos could not be annotated with a specific"
        },
        {
          "6": "multimodal  classifiers  together  with  the  associated  MM1",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "gender as the gender of the speaker was either not discern-"
        },
        {
          "6": "effects. The estimates of MM1 scores show a number of in-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "ible from the video even after inter-rater consultations, or"
        },
        {
          "6": "teresting patterns. First, the average MM1 scores across the",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "was  not  uniquely \nidentifiable  on  account  of  multiple"
        },
        {
          "6": "seven emotion classes is 20.2 for the cross-validated model,",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "speakers. For the successful gender annotations, the inter-"
        },
        {
          "6": "implying  that  the  multimodal  classifiers  outperformed",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "rater agreement was 100%. Table B3 in Appendix B provides"
        },
        {
          "6": "unimodal  classifiers  across  all  emotions.  The  highest  im-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "the sample distribution across gender and emotion catego-"
        },
        {
          "6": "provements  for  multimodal  classifiers  were  observed  for",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "ries."
        },
        {
          "6": "the disgust and fear emotions.",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Table 4 below highlights the MM1 scores for the male-"
        },
        {
          "6": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "only and female-only subgroups extracted from the data."
        },
        {
          "6": "TABLE 3",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "6": "MM1 SCORES BASED ON 10-FOLD CROSS-VALI-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Our results show that while multimodal classifiers offered"
        },
        {
          "6": "DATED AUC SCORES",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "significant 11 improvements  over  unimodal  classifiers  for"
        },
        {
          "6": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "both gender groups, there exists significant variance in the"
        },
        {
          "6": "MM1 Score (%)",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "6": "Modality",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "strength of this improvement, as discussed in detail in the"
        },
        {
          "6": "Neutral  Happiness  Sadness  Anger  Disgust \nFear \nSurprise",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "next section. As mentioned earlier, we focus our attention"
        },
        {
          "6": "18.0 \n11.7 \n18.5 \n20.3 \n29.6 \n24.1 \n19.4",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "only  on  the  neutral,  happiness,  sadness,  and  anger  emo-"
        },
        {
          "6": "Best MM  V+A+T \nV+A+T \nV+A+T  V+A+T  V+A+T  V+A+T  V+A+T",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "tions  for  this  analysis,  as  the  extent  of  class  imbalance  in"
        },
        {
          "6": "Best UM \nV \nV \nV \nA \nA \nA \nA",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "these  categories  can  be  reasonably  addressed  using  class"
        },
        {
          "6": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "weights  and  other  techniques,  e.g.  SMOTE  analysis  [85]."
        },
        {
          "6": "The  best  performing  unimodal  classifier  for  neutral,",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Similar  to  the  overall  data,  we  tested  the  robustness  of"
        },
        {
          "6": "happiness  and  sadness  was  the  visual-only  (V)  classifier,",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "these findings for our gender subgroups using a random"
        },
        {
          "6": "while  the best  performing  unimodal  classifier  for  the  an-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "forest  classifier. These additional results are  presented in"
        },
        {
          "6": "ger, disgust, fear, and surprise was the audio-only (A) clas-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Tables B4 and B5 in Appendix B, and are consistent with our"
        },
        {
          "6": "sifier.  Among \nthe  multimodal  classifiers, \nthe \ntrimodal",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "key findings discussed next."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 3: below presents the list of best unimodal and tated into 738 male-only and 1,004 female-only videos.",
      "data": [
        {
          "MM1 SCORES FOR MALE VS. FEMALE SPEAKERS BASED ON 10-FOLD CROSS-VALIDATED AUC SCORES": ""
        },
        {
          "MM1 SCORES FOR MALE VS. FEMALE SPEAKERS BASED ON 10-FOLD CROSS-VALIDATED AUC SCORES": "Modality"
        },
        {
          "MM1 SCORES FOR MALE VS. FEMALE SPEAKERS BASED ON 10-FOLD CROSS-VALIDATED AUC SCORES": ""
        },
        {
          "MM1 SCORES FOR MALE VS. FEMALE SPEAKERS BASED ON 10-FOLD CROSS-VALIDATED AUC SCORES": "Male Speakers"
        },
        {
          "MM1 SCORES FOR MALE VS. FEMALE SPEAKERS BASED ON 10-FOLD CROSS-VALIDATED AUC SCORES": "Overall"
        },
        {
          "MM1 SCORES FOR MALE VS. FEMALE SPEAKERS BASED ON 10-FOLD CROSS-VALIDATED AUC SCORES": "Best MM"
        },
        {
          "MM1 SCORES FOR MALE VS. FEMALE SPEAKERS BASED ON 10-FOLD CROSS-VALIDATED AUC SCORES": "Best UM"
        },
        {
          "MM1 SCORES FOR MALE VS. FEMALE SPEAKERS BASED ON 10-FOLD CROSS-VALIDATED AUC SCORES": "Female Speakers"
        },
        {
          "MM1 SCORES FOR MALE VS. FEMALE SPEAKERS BASED ON 10-FOLD CROSS-VALIDATED AUC SCORES": "Overall"
        },
        {
          "MM1 SCORES FOR MALE VS. FEMALE SPEAKERS BASED ON 10-FOLD CROSS-VALIDATED AUC SCORES": "Best MM"
        },
        {
          "MM1 SCORES FOR MALE VS. FEMALE SPEAKERS BASED ON 10-FOLD CROSS-VALIDATED AUC SCORES": "Best UM"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 5: below. Similar to the gender subgroups, we tested",
      "data": [
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n7": "The  duration  of  an  episode  when  an  emotion  is  ex-\nvideos respectively. The first quartile comprised a total of"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n7": "pressed can vary widely. As described earlier, the duration \n544 videos ranging in duration from 0.6 seconds to 4.4 sec-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n7": "of episode might be longer or shorter depending on i) the \nonds,  while  the  fourth  quartile  comprised  a  total  of  543"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n7": "modality e.g., emotions expressed through text take longer \nvideos ranging in duration from 10.5 seconds to 30.7 sec-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n7": "to express than through visuals, ii) the nature of medium \nonds.  Table B3 in the Appendix B illustrates the sample dis-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n7": "e.g., short tweets and Vine videos vs. longer YouTube vid-\ntribution across duration and emotion categories."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n7": "eos, and iii) the specific type of emotions, e.g., happiness \nWe then trained an SVM classifier, similar to the one in"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n7": "takes shorter time to fully express than disgust.   \nthe  previous  section,  on  these  two  groups  of  videos.  We"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n7": "In  order  to  test  the  accuracy  of  the  classifiers  for  the \nreport the resulting MM1 scores for the two subgroups in"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n7": "various  emotion  classes  across  short  and  long  duration \nTable 5 below. Similar to the gender subgroups, we tested"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n7": "videos,  we  performed  a  quartile  split  on  the  training  set \nand confirmed the robustness of this analysis using a ran-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n7": "based on the duration of videos, and categorized the first \ndom  forest  classifier;  the  additional  results  are  presented"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n7": "and  fourth  duration  quartiles  as  short  and  long  duration \nin Tables B6 and B7 as part of Appendix B."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 5: below. Similar to the gender subgroups, we tested",
      "data": [
        {
          "MM1 SCORES FOR SHORTER VS. LONGER DURATION VIDEOS BASED ON 10-FOLD CROSS-VALIDATION": ""
        },
        {
          "MM1 SCORES FOR SHORTER VS. LONGER DURATION VIDEOS BASED ON 10-FOLD CROSS-VALIDATION": "Modality"
        },
        {
          "MM1 SCORES FOR SHORTER VS. LONGER DURATION VIDEOS BASED ON 10-FOLD CROSS-VALIDATION": ""
        },
        {
          "MM1 SCORES FOR SHORTER VS. LONGER DURATION VIDEOS BASED ON 10-FOLD CROSS-VALIDATION": "Shorter Duration"
        },
        {
          "MM1 SCORES FOR SHORTER VS. LONGER DURATION VIDEOS BASED ON 10-FOLD CROSS-VALIDATION": "Overall"
        },
        {
          "MM1 SCORES FOR SHORTER VS. LONGER DURATION VIDEOS BASED ON 10-FOLD CROSS-VALIDATION": "Best MM"
        },
        {
          "MM1 SCORES FOR SHORTER VS. LONGER DURATION VIDEOS BASED ON 10-FOLD CROSS-VALIDATION": "Best UM"
        },
        {
          "MM1 SCORES FOR SHORTER VS. LONGER DURATION VIDEOS BASED ON 10-FOLD CROSS-VALIDATION": "Longer Duration"
        },
        {
          "MM1 SCORES FOR SHORTER VS. LONGER DURATION VIDEOS BASED ON 10-FOLD CROSS-VALIDATION": "Overall"
        },
        {
          "MM1 SCORES FOR SHORTER VS. LONGER DURATION VIDEOS BASED ON 10-FOLD CROSS-VALIDATION": "Best MM"
        },
        {
          "MM1 SCORES FOR SHORTER VS. LONGER DURATION VIDEOS BASED ON 10-FOLD CROSS-VALIDATION": "Best UM"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "8": "= 20.2), although there is a possibility that these findings",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "correctly  classify  other  emotions  expressed  by \nfemale"
        },
        {
          "8": "are specific to our data context, particularly given the im-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "speakers."
        },
        {
          "8": "balanced class distribution in disgust and fear categories.",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Even though, in this study, we do not focus on gender"
        },
        {
          "8": "However, we found that, for both SVM and random forest",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "effects in other negative emotions like fear and disgust due"
        },
        {
          "8": "classifiers,  negative  emotions  like  sadness  and  anger  ex-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "to the class imbalance in the available dataset, there have"
        },
        {
          "8": "hibited better MM1 scores than positively valenced emo-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "been recent studies that discuss relevant findings. For in-"
        },
        {
          "8": "tions like happiness.  This can be partly because these emo-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "stance, and with disgust specifically, Al-Shawaf et al. pro-"
        },
        {
          "8": "tions are complex or compound [26] and are unlikely to be",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "vide  a  set  of  evolutionary-functional  reasons  to  explain"
        },
        {
          "8": "fully learned through any one modality. In contrast, emo-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "why  women  consistently  experience  higher  levels  of  dis-"
        },
        {
          "8": "tions like happiness have clear markers in at least one mo-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "gust  than  men  [54].  It  is  possible,  therefore,  that  female"
        },
        {
          "8": "dality (e.g., facial smile) and can therefore be learned with",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "speakers  might  be  expressing  this  emotion  more  vividly"
        },
        {
          "8": "reasonable accuracy through unimodal classifiers. This is",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "than males, making it easier for classifiers to learn this dif-"
        },
        {
          "8": "consistent with recent work by [89] which found that facial",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "ference. Similarly, for anger, an emotion closely related to"
        },
        {
          "8": "expressions conveyed happiness the clearest, while vocal",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "disgust, our analyses show that unimodal and multimodal"
        },
        {
          "8": "features conveyed anger better than other emotions. In our",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "classifiers  perform  reasonably  well  for  female  speakers."
        },
        {
          "8": "analyses  too,  we  found  that  the  most  effective  unimodal",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Furthermore, for male  speakers, the visual-only classifier"
        },
        {
          "8": "classifier  for  the  happiness  emotion  was  visual-only  (V,",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "was found to be the best performing unimodal classifier for"
        },
        {
          "8": "AUC = 0.639).",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "happiness, sadness, and anger emotions, while the audio-"
        },
        {
          "8": "Similar  to  fine-grained  emotions,  the  improvement  in",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "only  classifier  performed  better  for  neutral.  For  female"
        },
        {
          "8": "multimodal performance was substantial in predicting the",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "speakers, however, the visual-only classifier was found to"
        },
        {
          "8": "continuous  dimensions  like  arousal  and  valence.  This  is",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "be better performing for all emotion classes."
        },
        {
          "8": "consistent  with  recent  studies  that  have  shown  high  effi-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Multimodal  emotion \nfeaturesâ€™  effectiveness  varies"
        },
        {
          "8": "cacy of multimodal systems in predicting valence and po-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "across episodic durations. The other contextual factor that"
        },
        {
          "8": "larity  [1],  [3],  [84].  In  our  analyses,  we  found  that  across",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "we focused on was the duration of the emotion expression"
        },
        {
          "8": "emotions,  bimodal  and  trimodal  regression  models  out-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "episode.  We  conjectured  that  the  duration  of  a  recorded"
        },
        {
          "8": "performed  unimodal  regression  models  for  both  arousal",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "emotion episode would be contingent on i) data modality,"
        },
        {
          "8": "and valence. Among the multimodal models, the trimodal",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "ii)  nature  of  the  medium,  and  iii)  the  type  of  emotion."
        },
        {
          "8": "model  (V+A+T)  outperformed  all  bimodal  models.  Fur-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "While certain modalities, like text, require longer to fully"
        },
        {
          "8": "ther, we found that while audio-related features (V+A and",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "onset a specific emotion (i.e. we might need to wait for a"
        },
        {
          "8": "A) performed better than text-related features in predicting",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "sentence  to  end  to  decipher  the  full  meaning  and  intent"
        },
        {
          "8": "arousal, text-related features (A+T, V+T, T) performed bet-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "from the speaker), other modalities like visual are quicker"
        },
        {
          "8": "ter than audio features in predicting valence.",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "and more direct (e.g. a smile to convey happiness). Videos"
        },
        {
          "8": "The gender of the speaker plays a moderating role on",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "from certain online video sharing websites like YouTube,"
        },
        {
          "8": "the  multimodal  featuresâ€™  performance.  Our  analyses  on",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "as is the case with the current dataset, tend to be longer in"
        },
        {
          "8": "the OMG dataset also uncovered significant gender differ-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "duration than videos from other platforms like Vine."
        },
        {
          "8": "ences in emotion classification across various emotion clas-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Our  analyses  on  classifier  performance  for  shorter  vs."
        },
        {
          "8": "ses. For instance, we found that multimodal classifiers out-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "longer duration videos show that the average MM1 scores"
        },
        {
          "8": "performed unimodal classifiers for all emotions across the",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "across  emotions  was  comparable  for  the  two  duration"
        },
        {
          "8": "two  genders  (i.e.  MM1  >  0).  However,  the  average  MM1",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "groups  ( ğ‘€ğ‘€1ğ‘ğ‘£ğ‘”,ğ‘ â„ğ‘œğ‘Ÿğ‘¡ğ‘’ğ‘Ÿ  =  8.6, ğ‘€ğ‘€1ğ‘ğ‘£ğ‘”,ğ‘™ğ‘œğ‘›ğ‘”ğ‘’ğ‘Ÿ  =  9.7).  How-"
        },
        {
          "8": "scores for male speakers (ğ‘€ğ‘€1ğ‘ğ‘£ğ‘”,ğ‘šğ‘ğ‘™ğ‘’ = 9.0) outperformed",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "ever, we noticed variance across the specific emotion cate-"
        },
        {
          "8": "those for female speakers (ğ‘€ğ‘€1ğ‘ğ‘£ğ‘”,ğ‘“ğ‘’ğ‘šğ‘ğ‘™ğ‘’ = 5.9) across emo-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "gories. For instance, and as mentioned earlier, multimodal"
        },
        {
          "8": "tions. One possible explanation for this is that females, as",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "classifiers failed to offer any improvement over unimodal"
        },
        {
          "8": "young as 4 to 6 year old, have been found to express com-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "classifiers \nfor \nthe  neutral  emotion  when \ntrained  with"
        },
        {
          "8": "plex  emotions  [90],  like  sadness  and  anxiety,  through  a",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "shorter duration videos (ğ‘€ğ‘€1ğ‘›ğ‘’ğ‘¢ğ‘¡ğ‘Ÿğ‘ğ‘™,ğ‘ â„ğ‘œğ‘Ÿğ‘¡ğ‘’ğ‘Ÿ = 0). For longer"
        },
        {
          "8": "combination  of  subtle  actions  that  are  harder  to  detect,",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "duration  videos  however, \nthere  was  an \nimprovement"
        },
        {
          "8": "than for men who generally show limited reactions when",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "(ğ‘€ğ‘€1ğ‘›ğ‘’ğ‘¢ğ‘¡ğ‘Ÿğ‘ğ‘™,ğ‘™ğ‘œğ‘›ğ‘”ğ‘’ğ‘Ÿ = 6.2). We also found that for neutral and"
        },
        {
          "8": "expressing  complex  emotions.  This  is  probably  due  to  a",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "positively valenced emotions, like happiness, the improve-"
        },
        {
          "8": "mix  of  evolutionary \nlimitations  and \nrestrictive \nsocial",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "ments in multimodal classification over unimodal classifi-"
        },
        {
          "8": "norms. For emotions like anger, where reactions like loud",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "cation were higher, as reflected by a higher MM1 score12."
        },
        {
          "8": "vocal expressions are relatively common for both men and",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "However,  for  negatively  valenced  emotions  like  sadness"
        },
        {
          "8": "women,  the  classifiers  are  less  likely  to  show  any  differ-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "and anger, the opposite was true. This might partly be due"
        },
        {
          "8": "ence. Recent studies looking at gender differences in emo-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "to the nature of the specific emotions being expressed. For"
        },
        {
          "8": "tion  experience  and  perception  have  also \nfound \nthat",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "instance, while negative emotions like sadness and disgust"
        },
        {
          "8": "women  tend  to  be  better  than  men  at  picking  up  certain",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "might take longer to fully onset and peak, others such as"
        },
        {
          "8": "emotions,  and  particularly  negative  emotions,  as  well  as",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "happiness and surprise tend to be more immediate. More"
        },
        {
          "8": "experiencing  these  emotions  themselves  [52],  [53],  [55].",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "research  is  needed  to  fully  understand  the  underlying"
        },
        {
          "8": "However, our findings hint that, with the exception of an-",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "mechanisms of why these duration effects manifest."
        },
        {
          "8": "ger,  it  might  be  challenging  for  multimodal  classifiers  to",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "8": "12 The improvements in AUC scores were statistically significant at the",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "95% confidence level on a Wilcoxon signed â€“rank test."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "6.2  Implications, Limitations and Future Research \ntions,  data  modalities,  genders  and  other  contextual  fac-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "tors.  A  theoretically  sound  interpretation  of  multimodal \nTaken together, our analyses provide clear and converging"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "emotion recognition can benefit not just the development \nempirical  evidence  supporting  the  multimodal  approach"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "of better systems, but also help preempt biases and explain \nto recognizing and analyzing emotions. More importantly,"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "several inconsistent findings reported in prior work.  \nour  findings  highlight  significant  variances  based  on  im-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "portant  contextual  factors,  like  the  gender  of  the  speaker"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "and the duration of the emotion episode."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "7  CONCLUSION"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "Notably, our findings add to a growing stream of liter-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "This study leverages state-of-the-art visual, audio and text"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "ature that cautions about the prevalence of biases in ma-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "based  emotion  recognition  systems,  and  presents  an  em-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "chine learning datasets and models [91], [92]. Our findings"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "pirical analysis to further our understanding of the contex-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "highlight that the relative performance of emotion recog-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "tual \nfactors  affecting  multimodal  emotion  recognition."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "nition  systems  might  vary  substantially  across  genders,"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "While multimodal features consistently outperformed uni-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "durations, emotions, and plausibly, the underlying feature"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "modal features, our findings revealed significant variance"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "extraction systems. Researchers and developers need to be"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "in  improvement  across  different  emotions.  This  study  is"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "aware of such variances especially if such systems are be-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "also an early attempt at examining the effect of contextual"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "ing used in a gender-sensitive context (e.g. employee selec-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "factors like the gender of the speaker or the duration of the"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "tion based on video interviews)."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "emotional episode, on classifier accuracy across modalities"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "There are a number of limitations in the present study"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "and emotion classes."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "that warrants future research. First, the current study uses"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "From an applied perspective, the relative strength of the"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "text,  speech  and  visual  as  the  three  data  modalities,  and"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "unimodal or bimodal features in comparison to full, multi-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "uses all of these to compare the performance of the multi-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "modal  emotion  recognition  can  provide  a  valuable  refer-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "modal emotion recognition systems. We note that the fea-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "ence to practitioners. In todayâ€™s digital platforms, emotion"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "ture combination technique used is platform-agnostic, i.e.,"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "cues are largely extracted from text-based unimodal (e.g.,"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "while  these  modalities  are  common  sources  of  emotion"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "Tweets, Facebook comments), audio-text bimodal (e.g., call"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "data, the relative importance and richness of these modal-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "center recordings), or visual-text bimodal (e.g., Instagram"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "ities might vary across platforms, e.g. Instagram content is"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "stories and posts) content. Practitioners can derive a sense"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "likely to be richer in visual than text."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "of confidence from our multimodal analysis to make an in-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "Second, and as pointed out in [48], the various modal-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "formed assessment of the relative predictive performance"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "ities are not temporally aligned within the duration of the"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "of the unimodal and bimodal data sources."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "emotion episode. This might introduce errors in feature fu-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "Emotions  will  continue  to  play  a  key  role  in  shaping"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "sion, leading to higher misclassifications."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "human  experience  and  communication.  The  exploratory"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "Third, while our study offers robust analytical insights"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "findings from this study offer valuable empirical insights"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "based on real-world videos, the findings can be limited to"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "to  help  understand  and  inform  how  future  affective  sys-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "the  specific  dataset  we  used.  As  we  noted,  the  OMG  da-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "tems  leveraging  multimodal  emotions  can  be  improved"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "taset, though with a relatively data size and a good gender"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "with multiple data and communication modalities, differ-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "and duration diversity, included relatively fewer represen-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "ent emotions of interest, as well as a wide range of contex-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "tations  of  disgust,  fear  and  surprise  which  prevented  us"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "tual factors including but not limited to gender and dura-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "from extending our subgroup analyses to these classes. We"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "tion of the expressive episode."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "also  did  not  have  access  to  the  annotatorsâ€™  demographic"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "distribution, which prevented us from studying the sensi-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "ACKNOWLEDGMENT \ntivity of our findings, if any, to the annotatorsâ€™ gender."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "Last but not the least, in this study, we selectively fo-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "This research is supported by the Agency for Science, Tech-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "cused on two important contextual factors: gender and du-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "nology  and  Research  (A*STAR)  under  its  SERC  Strategic"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "ration. Other factors like i) surrounding scenes, ii) physio-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "Fund (a1718g0046) and A*ccelerate Gap Fund (accl180222)."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "logical  state  (e.g.,  body  temperature),  iii)  conversational"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "We thank the Project Digital Emotions team for the valua-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "contexts, etc., may also affect classifier performance. Simi-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "ble teamwork, and in particular,  Nur Atiqah Othman for"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "larly, the results are also plausibly sensitive to the perfor-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "her annotation assistance on the gender labels used in the"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "mance  and  biases  associated  with  specific  tools,  though"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "analysis. The authors are grateful for the helpful comments"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "reasonably effective, that we selected to generate the fea-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "from Desmond Ong, and the advices from Stefan Winkler,"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "tures.  The  relative  performance  of  the  classifiers  might,"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "Huang Dongyan and Nancy Chen  on applying their sys-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "therefore, also vary as a function of the emotion features"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "tems  in  processing  and  extracting  visual  and  speech  fea-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "extraction technologies, and inherit their innate biases. As"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "tures used in this study. All errors that remain are our sole"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "technologies  advance,  future  work  in  this  direction  shall"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "responsibility."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "further explore the effects of these factors on multimodal"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "classifier accuracy."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "REFERENCES"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "Future research can look into building richer datasets,"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "[1] \nS. K. Dâ€™Mello, N. Dowell, and A. Graesser, â€œUnimodal and"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "applying  more  advanced  emotion  analysis  technologies,"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "Multimodal  Human  Perception  of  Naturalistic  Non-Basic"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "and studying abstractions that can explain the psychologi-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "Affective  States  during  Human-Computer \nInteractions,â€"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "cal underpinnings driving the associations between emo-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n9": "IEEE Trans. Affect. Comput., vol. 4, no. 4, pp. 452â€“465, 2013."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "10": "[2]",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "audio features,â€ Vis. Comput., vol. 29, no. 12, pp. 1269â€“1275,"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "2013."
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "G.  Krell  et  al.,  â€œFusion  of  Fragmentary  Classifier  Decisions"
        },
        {
          "10": "[3]",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "IAPR  Workshop  on \nfor  Affective  State  Recognition,â€ \nin"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Multimodal  Pattern  Recognition  of  Social  Signals  in  Human-"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Computer Interaction, 2012, pp. 116â€“130."
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "S. K. Dâ€™mello and A. Graesser, â€œMultimodal semi-automated"
        },
        {
          "10": "[4]",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "affect  detection \nfrom \nconversational \ncues,  gross  body"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "language,  and \nfacial \nfeatures,â€  User  Model.  User-adapt."
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Interact., vol. 20, no. 2, pp. 147â€“187, 2010."
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "P. Ekman, â€œAn Argument for Basic Emotions,â€ Cogn. Emot.,"
        },
        {
          "10": "[5]",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "vol. 6, no. 3â€“4, pp. 169â€“200, 1992."
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "A. Ortony, G. L. Clore, and A. Collins, The cognitive structure"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "of emotions. Cambridge university press, 1988."
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "A.  Ortony  and  T.  J.  Turner,  â€œWhatâ€™s  Basic  About  Basic"
        },
        {
          "10": "[6]",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Emotions?,â€ Psychol. Rev., vol. 97, pp. 315â€“331, 1990."
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "T. J. Turner and A. Ortony, â€œBasic Emotions: Can Conflicting"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Criteria Converge?,â€ Psychol. Rev., vol. 99, pp. 566â€“571, 1992."
        },
        {
          "10": "[7]",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "S. Wang, Y. Zhu, G. Wu, and Q. Ji, â€œHybrid Video Emotional"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Tagging  using  Usersâ€™  EEG  and  Video  Content,â€  Multimed."
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Tools Appl., vol. 72, no. 2, pp. 1257â€“1283, 2014."
        },
        {
          "10": "[8]",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "T.  BaltruÅ¡aitis,  N.  Banda,  and  P.  Robinson,  â€œDimensional"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "affect  recognition  using  continuous  conditional  random"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "IEEE \nInternational  Conference \nand \nfields,â€ \nin  2013  10th"
        },
        {
          "10": "[9]",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Workshops  on  Automatic  Face  and  Gesture  Recognition  (FG),"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "2013, pp. 1â€“8."
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "M. S. Hussain, H. Monkaresi, and R. A. Calvo, â€œCombining"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Classifiers in Multimodal Affect Detection,â€ in Proceedings of"
        },
        {
          "10": "[10]",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "the Tenth Australasian Data Mining Conference, 2012, pp. 103â€“"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "108."
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "K.  Lu  and  Y.  Jia,  â€œAudio-visual  Emotion  Recognition  with"
        },
        {
          "10": "[11]",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Proceedings \nof \nthe \n21st \nBoosted  Coupled  HMM,â€ \nin"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "International  Conference  on  Pattern  Recognition  (ICPR2012),"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "2012, pp. 1148â€“1151."
        },
        {
          "10": "[12]",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "D. Glowinski, N. Dael, A. Camurri, G. Volpe, M. Mortillaro,"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "and  K.  Scherer,  â€œToward  a  minimal \nrepresentation  of"
        },
        {
          "10": "[13]",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "affective gestures,â€ IEEE Trans. Affect. Comput., vol. 2, no. 2,"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "pp. 106â€“118, 2011."
        },
        {
          "10": "[14]",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "J.  A.  Russell,  â€œA  circumplex  model  of  affect,â€  J.  Pers.  Soc."
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Psychol., vol. 39, no. 6, pp. 1161â€“1178, Dec. 1980."
        },
        {
          "10": "[15]",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "J. Sonnemans and N. H. Frijda, â€œThe structure of subjective"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "emotional intensity,â€ Cogn. Emot., vol. 8, no. 4, pp. 329â€“350,"
        },
        {
          "10": "[16]",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "1994."
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "N. H. Frijda and others, The emotions. Cambridge University"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Press, 1986."
        },
        {
          "10": "[17]",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "R. K. Gupta and Y. Yang, â€œCrystalFeel at SemEval-2018 Task"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "1:  Understanding  and  Detecting  Emotion  Intensity  using"
        },
        {
          "10": "[18]",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Affective Lexicons,â€ in In Proceedings of The 12th International"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Workshop on Semantic Evaluation, 2018, pp. 256â€“263."
        },
        {
          "10": "[19]",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "M. Pantic and L. J. Rothkrantz, â€œToward an Affect-sensitive"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Multimodal Human-computer Interaction,â€ Proc. IEEE, vol."
        },
        {
          "10": "[20]",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "91, no. 9, pp. 1370â€“1390, 2003."
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "A.  Metallinou,  M.  Wollmer,  A.  Katsamanis,  F.  Eyben,  B."
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Schuller, and S. Narayanan, â€œContext-sensitive Learning for"
        },
        {
          "10": "[21]",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Enhanced Audiovisual Emotion Classification,â€ IEEE Trans."
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Affect. Comput., vol. 3, no. 2, pp. 184â€“198, 2012."
        },
        {
          "10": "[22]",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "J. C. Lin, C. H. Wu, and W. L. Wei, â€œError Weighted Semi-"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "coupled  Hidden  Markov  Model  for  Audio-visual  Emotion"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "Recognition,â€ IEEE Trans. Multimed., vol. 14, no. 1, pp. 142â€“\nFour Levels of Analysis,â€ Cogn. Emot., vol. 13, no. 5, pp. 505â€“"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "156, 2012. \n521, 1999."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "[41] \nB. Schuller, â€œRecognizing Affect from Linguistic Information \n[57] \nP.  Barros,  N.  Churamani,  E.  Lakomkin,  H.  Siqueira,  A."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "in 3D Continuous Space,â€ IEEE Trans. Affect. Comput., vol. 2, \nSutherland,  and  S.  Wermter,  â€œThe  Omg-emotion  Behavior"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "no. 4, pp. 192â€“205, 2011. \nDataset.,â€  in  2018  International  Joint  Conference  on  Neural"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "[42] \nC.  H.  Wu  and  W.  B.  Liang,  â€œEmotion  Recognition  of \nNetworks (IJCNN), 2018, pp. 1â€“7."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "Affective  Speech  Based  on  Multiple  Classifiers  Using \n[58] \nV.  Vonikakis,  R.  Subramanian,  J.  Arnfred,  and  S.  Winkler,"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "Acoustic-prosodic Information and Semantic Labels,â€ IEEE \nâ€œProbabilistic  Approach  to  People-centric  Photo  selection"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "Trans. Affect. Comput., vol. 2, no. 1, pp. 10â€“21, 2011. \nand Sequencing,â€ IEEE Trans. Multimed., vol. 19, no. 11, pp."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "[43] \nO.  AlZoubi,  S.  K.  Dâ€™Mello,  and  R.  A.  Calvo,  â€œDetecting \n2609â€“2624, 2017."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "naturalistic \nexpressions \nof \nnonbasic \naffect \nusing \n[59] \nS.  Peng,  L.  Zhang,  S.  Winkler,  and  M.  Winslett,  â€œGive  me"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "physiological signals,â€ IEEE Trans. Affect. Comput., vol. 3, no. \nOne  Portrait \nImage, \nI  will \ntell  you  your  Emotion  and"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "3, pp. 298â€“310, 2012. \nPersonality.,â€  in  Proceedings  of  the  26th  ACM  International"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "[44] \nL.  Kessous,  G.  Castellano,  and  G.  Caridakis,  â€œMultimodal \nConference on Multimedia, 2018, pp. 1226â€“1227."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "emotion recognition in speech-based interaction using facial \n[60] \nV.  Vonikakis  and  S.  Winkler,  â€œEmotion-based  Sequence  of"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "J. \nexpression, \nbody \ngesture \nand \nacoustic \nanalysis,â€ \nFamily Photos,â€ in Proceedings of the 20th ACM international"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "Multimodal User Interfaces, vol. 3, no. 1â€“2, pp. 33â€“48, 2010. \nconference on Multimedia, 2012, pp. 1371â€“1372."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "[45] \nM. Wollmer, M. Kaiser, F. Eyben, B. Schuller, and G. Rigoll, \n[61] \nV.  Vonikakis,  Y.  Yazici,  V.  D.  Nguyen,  and  S.  Winkler,"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "â€œLSTM-Modeling of continuous emotions in an audiovisual \nâ€œGroup  Happiness  Assessment  using  Geometric  Features"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "affect  recognition  framework,â€  Image  Vis.  Comput.,  vol.  31, \nand  Dataset  Balancing,â€ \nin  Proceedings  of  the  18th  ACM"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "no. 2, pp. 153â€“163, 2013. \nInternational  Conference  on  Multimodal  Interaction,  2016,  pp."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "[46] \nM.  Glodek \net  al.,  â€œMultiple  Classifier  Systems \nfor \nthe \n479â€“486."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "Classification  of  Audio-visual  Emotional  States,â€  Affect. \n[62] \nH.  W.  Ng,  V.  D.  Nguyen,  V.  Vonikakis,  and  S.  Winkler,"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "Comput. Intell. Interact. Springer, pp. 359â€“368, 2011. \nâ€œDeep Learning for Emotion Recognition on Small Datasets"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "of \nthe  ACM \n[47] \nB. de Gelder, H. K. M. Meeren, R. Righart, J. den Stock, W. \nusing  Transfer  Learning,â€ \nin  Proceedings"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "A. C. de Riet, and M. Tamietto, â€œBeyond the face: exploring \nInternational  Conference  on  Multimodal  Interaction,  2015,  pp."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "rapid influences of context on face processing,â€ Prog. Brain \n443â€“449."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "Res., vol. 155, pp. 37â€“48, 2006. \n[63] \nA. Mollahosseini, B. Hasani, and M. H. Mahoor, â€œAffectnet:"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "[48] \nF. Lingenfelser, J. Wagner, J. Deng, R. Brueckner, B. Schuller, \nA  database \nfor \nfacial  expression,  valence,  and  arousal"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "and  E.  AndrÃ©,  â€œAsynchronous  and  event-based \nfusion \ncomputing in the wild,â€ IEEE Trans. Affect. Comput., vol. 10,"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "systems \nfor \naffect \nrecognition  on  naturalistic  data \nin \nno. 1, pp. 18â€“31, 2017."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "comparison to conventional approaches,â€ IEEE Trans. Affect. \n[64] \nD.-Y.  Huang  et  al.,  â€œVisual  Speech  Emotion  Conversion"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "Comput., vol. 9, no. 4, pp. 410â€“423, 2016. \nusing Deep Learning for 3D Talking Head,â€ in Proceedings of"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "the  Joint  Workshop  of  the  4th  Workshop  on  Affective  Social \n[49] \nJ.  C.  Kim \nand  M.  A.  Clements, \nâ€œMultimodal  Affect"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "Multimedia \nComputing \nand \nFirst  Multi-Modal  Affective \nClassification  at  Various  Temporal  Lengths,â€  IEEE  Trans."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "Affect. Comput., vol. 6, no. 4, pp. 371â€“384, 2015. \nComputing of Large-Scale Multimedia Data, 2018, pp. 7â€“13."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "[50] \nH. Aviezer et al., â€œAngry, Disgusted, or Afraid? Studies on \n[65] \nD.  Y.  Huang  et  al.,  â€œMultimodal  Prediction  Of  Affective"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "the Malleability of Emotion Perception,â€ Psychol. Sci., vol. 19, \nDimensions  Fusing  Multiple  Regression  Techniques,â€ \nin"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "no. 7, pp. 724â€“732, 2008. \nINTERSPEECH 2017, 2017, pp. 162â€“165."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "[51] \nJ. Brebner, â€œGender and emotions,â€ Pers. Individ. Dif., vol. 34, \n[66] \nX. Ouyang et al., â€œAudio-visual Emotion Recognition Using"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "no. 3, pp. 387â€“394, 2003. \nDeep Transfer Learning and Multiple Temporal Models,â€ in"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "[52] \nE. Hampson, S. M. van Anders, and L. I. Mullin, â€œA female \n19th ACM International Conference on Multimodal Interaction,"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "advantage in the recognition of emotional facial expressions: \n2017, pp. 577â€“582."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "Test of an evolutionary hypothesis,â€ Evol. Hum. Behav., vol. \n[67] \nW. Ding et al., â€œAudio and Face Video Emotion Recognition"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "27, no. 6, pp. 401â€“416, 2006. \nin \nthe  Wild  using  Deep  Neural  Networks  and  Small"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "of \nthe  18th  ACM \nInternational \n[53] \nE. B. McClure, â€œA Meta-analytic Review of Sex Differences \nDatasets,â€ \nin  Proceedings"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "in  Facial  Expression  Processing  and  their  Development  in \nConference on Multimodal Interaction, 2016, pp. 506â€“513."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "Infants, Children, and Adolescents,â€ Psychol. Bull., vol. 126, \n[68] \nW.  Y.  Quck,  D.  Y.  Huang,  W.  Lin,  H.  Li,  and  M.  Dong,"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "pp. 424â€“453, 2000. \nâ€œMobile Acoustic Emotion Recognition,â€ in IEEE Region 10"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "[54] \nL.  Al-Shawaf,  D.  M.  G.  Lewis,  and  D.  M.  Buss,  â€œSex \nConference (TENCON), 2016, pp. 170â€“174."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "differences \nin  disgust:  Why \nare  women  more \neasily \n[69] \nK. Poon-Feng, D.-Y. Huang, M. Dong, and H. Li, â€œAcoustic"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "disgusted than men?,â€ Emot. Rev., vol. 10, no. 2, pp. 149â€“160, \nemotion  recognition  based  on  fusion  of  multiple  feature-"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "The \n9th \n2018. \ndependent \ndeep \nBoltzmann  machines,â€ \nin"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "International \nSymposium \non \nChinese \nSpoken \nLanguage \n[55] \nW.  A.  Babchuk,  R.  B.  Hames,  and  R.  A.  Thompson,  â€œSex"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "differences in the Recognition of Infant Facial Expressions of \nProcessing, 2014, pp. 584â€“588."
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "Ethol. \nEmotion: \nThe \nPrimary  Caretaker  Hypothesis,â€ \n[70] \nB.  Schuller,  M.  Valstar,  F.  Eyben,  G.  McKeown,  R.  Cowie,"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "Sociobiol., vol. 6, pp. 89â€“101, 1985. \nand  M. \nPantic, \nâ€œAvec \n2011--the \nfirst \ninternational"
        },
        {
          "BHATTACHARYA  ET AL.: THE CONTEXTUAL FACTORS OF MULTIMODAL EMOTION RECOGNITION IN VIDEOS  \n11": "[56] \nD.  Keltner  and  J.  Haidt,  â€œSocial  Functions  of  Emotions  at \naudio/visual emotion challenge,â€ in International Conference"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "[84] \nM. Soleymani, M. Pantic, and T. Pun, â€œMultimodal Emotion"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Recognition \nin  Response  to  Videos,â€  IEEE  Trans.  Affect."
        },
        {
          "12": "[71]",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Comput., vol. 3, no. 2, pp. 211â€“223, 2012."
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "[85] \nN.  V  Chawla,  K.  W.  Bowyer,  L.  O.  Hall,  and  W.  P."
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Kegelmeyer,  â€œSMOTE:  synthetic  minority  over-sampling"
        },
        {
          "12": "[72]",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "technique,â€ J. Artif. Intell. Res., vol. 16, pp. 321â€“357, 2002."
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "[86] \nP. Barros, D. Jirak, C. Weber, and S. Wermter, â€œMultimodal"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Emotional  State  Recognition  Using  Sequence-dependent"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Deep  Hierarchical  Features,â€  Neural  Networks,  vol.  72,  pp."
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "140â€“151, 2015."
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "[87] \nC.  Busso  et  al.,  â€œAnalysis  of  Emotion  Recognition  using"
        },
        {
          "12": "[73]",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Facial Expressions, Speech and Multimodal Information,â€ in"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Proceedings  of  the  6th  International  Conference  on  Multimodal"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Interfaces, 2004, pp. 205â€“211."
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "[88] \nC.  M.  Lee  et  al.,  â€œEmotion  Recognition  Based  on  Phoneme"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Classes,â€ in Eighth International Conference on Spoken Language"
        },
        {
          "12": "[74]",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Processing, 2004, pp. 889â€“892."
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "[89] \nH.  Cao,  D.  G.  Cooper,  M.  K.  Keutmann,  R.  C.  Gur,  A."
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Nenkova, \nand  R.  Verma, \nâ€œCREMA-D:  Crowd-sourced"
        },
        {
          "12": "[75]",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "emotional  multimodal  actors  dataset,â€  IEEE  Trans.  Affect."
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Comput., vol. 5, no. 4, pp. 377â€“390, 2014."
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "[90] \nL. R. Brody and J. A. Hall, â€œGender and emotion in context,â€"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Handb. Emot., vol. 3, pp. 395â€“408, 2008."
        },
        {
          "12": "[76]",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "[91] \nJ. P. Bajorek, â€œVoice recognition still has significant race and"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "gender  biases,â€  Harvard  Business  Review,  2019. \n[Online]."
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Available: https://hbr.org/2019/05/voice-recognition-still-"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "has-significant-race-and-gender-biases."
        },
        {
          "12": "[77]",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "[92] \nM.  R.  Costa-jussÃ ,  â€œAn  analysis  of  gender  bias  studies  in"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "natural language processing,â€ Nat. Mach. Intell., vol. 1, no. 11,"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "pp. 495â€“496, 2019."
        },
        {
          "12": "[78]",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "P"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "rasanta  Bhattacharya  is  a  Scientist  at  the  Social  and  Cognitive"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Computing Department of the Institute of High Performance Compu-"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "ting, A*STAR, Singapore. His research interests cover social network"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "analyses and behavioral analytics. He graduated with a PhD in Infor-"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "mation Systems  from  the  National  University  of  Singapore.  He  also"
        },
        {
          "12": "[79]",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "serves as an Adjunct Assistant Professor with the Department of An-"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "alytics and Operations at NUS Business School."
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "R"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "aj  Kumar  Gupta is  a Senior  Scientist  at the  Social  and  Cognitive"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Computing Department of the Institute of High Performance Compu-"
        },
        {
          "12": "[80]",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "ting, A*STAR, Singapore. His research interests include natural lan-"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "guage  processing,  sentiment  and  emotion  analysis,  and multimodal"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "analysis. He received his PhD in Computer Science from the Nanyang"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Technological University, Singapore. He has more than five yearsâ€™ in-"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "dustry experience on software development, visual analytics and em-"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "bedded systems."
        },
        {
          "12": "[81]",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Y"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "inping Yang is a Senior Scientist and Principal Investigator of the"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "A*STAR Digital Emotions programme at Institute of High Performance"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "Computing, A*STAR, Singapore. Her research interests cover intelli-"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "gent negotiation systems, emotion recognition and sentiment analy-"
        },
        {
          "12": "[82]",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "sis, and strategic foresight. She obtained her Ph.D. in Information Sys-"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "tems from the National University of Singapore. Besides research, she"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "also manages industry projects and advises tech start-ups in emotion"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": "analytics and e-negotiation technologies."
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "12": "[83]",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING": ""
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Appendix A": "",
          "Multimodal Emotion Features and Correlation Analyses with OMG": "Emotion Ground Truth Labels"
        },
        {
          "Appendix A": "Appendix A provides a detailed summary of the 43 visual, audio and text based emotion features,",
          "Multimodal Emotion Features and Correlation Analyses with OMG": ""
        },
        {
          "Appendix A": "and reports results from a correlation analysis. This supplemental material complements the feature",
          "Multimodal Emotion Features and Correlation Analyses with OMG": ""
        },
        {
          "Appendix A": "extraction description reported in Section 4 of the main paper.",
          "Multimodal Emotion Features and Correlation Analyses with OMG": ""
        },
        {
          "Appendix A": "",
          "Multimodal Emotion Features and Correlation Analyses with OMG": "Prior  to  our  classification  and  regression  experiments,  we  performed  a  bivariate  correlation"
        },
        {
          "Appendix A": "analysis  to  gain  a  preliminary  understanding  of  how  each  of  the  extracted  visual,  audio  and  text",
          "Multimodal Emotion Features and Correlation Analyses with OMG": ""
        },
        {
          "Appendix A": "features is associated with the ground truth emotion labels in the OMG videos dataset. As the emotion",
          "Multimodal Emotion Features and Correlation Analyses with OMG": ""
        },
        {
          "Appendix A": "classes and most of the features are binary or dichotomous variables, Kendallâ€™s correlation coefficient",
          "Multimodal Emotion Features and Correlation Analyses with OMG": ""
        },
        {
          "Appendix A": "Ï„ was used for all correlations, except for the text-based features which are continuous variables, for",
          "Multimodal Emotion Features and Correlation Analyses with OMG": ""
        },
        {
          "Appendix A": "which Pearsonâ€™s r was used.",
          "Multimodal Emotion Features and Correlation Analyses with OMG": ""
        },
        {
          "Appendix A": "Table A1 below presents the results from the bivariate correlation analysis. The results suggest a",
          "Multimodal Emotion Features and Correlation Analyses with OMG": ""
        },
        {
          "Appendix A": "clear  pattern:  at  the  individual  feature  level,  almost  all  the  extracted  visual,  audio,  and  text  based",
          "Multimodal Emotion Features and Correlation Analyses with OMG": ""
        },
        {
          "Appendix A": "emotion features were significantly associated with at least one of the seven emotion classes.",
          "Multimodal Emotion Features and Correlation Analyses with OMG": ""
        },
        {
          "Appendix A": "Among the visual features, vValence (Ï„ = .343**) and vIntensity (Ï„ = .252**) were found to have the",
          "Multimodal Emotion Features and Correlation Analyses with OMG": ""
        },
        {
          "Appendix A": "strongest  correlations  with  happiness.  For  audio  features,  aArousal  (Ï„  =  .141**)  and  aIntensity  (Ï„  =",
          "Multimodal Emotion Features and Correlation Analyses with OMG": ""
        },
        {
          "Appendix A": ".143**)  were  significantly  correlated  with  anger.  For  text  based  emotion  features,  tValence,  i.e.,  the",
          "Multimodal Emotion Features and Correlation Analyses with OMG": ""
        },
        {
          "Appendix A": "intensity of the overall valence detected in the speech transcript, was found to be highly associated",
          "Multimodal Emotion Features and Correlation Analyses with OMG": ""
        },
        {
          "Appendix A": "with happiness (r = .163**). Meanwhile, tSadness, i.e., the intensity of sadness detected in the speech",
          "Multimodal Emotion Features and Correlation Analyses with OMG": ""
        },
        {
          "Appendix A": "transcript, was highly associated with sadness (r = .164**).",
          "Multimodal Emotion Features and Correlation Analyses with OMG": ""
        },
        {
          "Appendix A": "The correlation analysis also indicated that gender and duration were indeed associated with some",
          "Multimodal Emotion Features and Correlation Analyses with OMG": ""
        },
        {
          "Appendix A": "emotions, but not others in this dataset. For example, female speakers were negatively associated with",
          "Multimodal Emotion Features and Correlation Analyses with OMG": ""
        },
        {
          "Appendix A": "neutral (Ï„ = -.214**) expressions, but positively associated with sadness (Ï„ = .256**) and anger (Ï„ = .046*)",
          "Multimodal Emotion Features and Correlation Analyses with OMG": ""
        },
        {
          "Appendix A": "expressions. Longer videos were positively associated with happiness (Ï„ = .089**) and surprise (Ï„ =",
          "Multimodal Emotion Features and Correlation Analyses with OMG": ""
        },
        {
          "Appendix A": ".044*) but were negatively associated with sadness (Ï„ = -.072**).",
          "Multimodal Emotion Features and Correlation Analyses with OMG": ""
        },
        {
          "Appendix A": "",
          "Multimodal Emotion Features and Correlation Analyses with OMG": "The correlation results revealed interesting preliminary patterns, but most of the above-illustrated"
        },
        {
          "Appendix A": "correlations have relatively small magnitudes. In order to understand the predictive performance of",
          "Multimodal Emotion Features and Correlation Analyses with OMG": ""
        },
        {
          "Appendix A": "these features, we used machine learning based classifiers, as reported in the main paper, to analyze",
          "Multimodal Emotion Features and Correlation Analyses with OMG": ""
        },
        {
          "Appendix A": "the combined effects of these various individual visual, audio and text based features.",
          "Multimodal Emotion Features and Correlation Analyses with OMG": ""
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table A1. Results of Bivariate Correlation Analysis": ""
        },
        {
          "Table A1. Results of Bivariate Correlation Analysis": ""
        },
        {
          "Table A1. Results of Bivariate Correlation Analysis": ""
        },
        {
          "Table A1. Results of Bivariate Correlation Analysis": ""
        },
        {
          "Table A1. Results of Bivariate Correlation Analysis": "-.152**"
        },
        {
          "Table A1. Results of Bivariate Correlation Analysis": ""
        },
        {
          "Table A1. Results of Bivariate Correlation Analysis": ""
        },
        {
          "Table A1. Results of Bivariate Correlation Analysis": ""
        },
        {
          "Table A1. Results of Bivariate Correlation Analysis": ""
        },
        {
          "Table A1. Results of Bivariate Correlation Analysis": ".051*"
        },
        {
          "Table A1. Results of Bivariate Correlation Analysis": ""
        },
        {
          "Table A1. Results of Bivariate Correlation Analysis": ""
        },
        {
          "Table A1. Results of Bivariate Correlation Analysis": ""
        },
        {
          "Table A1. Results of Bivariate Correlation Analysis": ""
        },
        {
          "Table A1. Results of Bivariate Correlation Analysis": ""
        },
        {
          "Table A1. Results of Bivariate Correlation Analysis": ""
        },
        {
          "Table A1. Results of Bivariate Correlation Analysis": "-.080**"
        },
        {
          "Table A1. Results of Bivariate Correlation Analysis": ""
        },
        {
          "Table A1. Results of Bivariate Correlation Analysis": ""
        },
        {
          "Table A1. Results of Bivariate Correlation Analysis": ""
        },
        {
          "Table A1. Results of Bivariate Correlation Analysis": ""
        },
        {
          "Table A1. Results of Bivariate Correlation Analysis": ".061**"
        },
        {
          "Table A1. Results of Bivariate Correlation Analysis": ".025"
        },
        {
          "Table A1. Results of Bivariate Correlation Analysis": ".052*"
        },
        {
          "Table A1. Results of Bivariate Correlation Analysis": ".014"
        },
        {
          "Table A1. Results of Bivariate Correlation Analysis": ".013"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotion": "Feature description",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": ""
        },
        {
          "Emotion": "",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": ""
        },
        {
          "Emotion": "",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": "-.007"
        },
        {
          "Emotion": "indicating the degree to which",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": ""
        },
        {
          "Emotion": "the audio expresses [-1:  least",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": ""
        },
        {
          "Emotion": "sense of expectancy; 1: most",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": ""
        },
        {
          "Emotion": "sense of  expectancy]",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": ""
        },
        {
          "Emotion": "A continuous variable",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": "-.141**"
        },
        {
          "Emotion": "indicating the degree to which",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": ""
        },
        {
          "Emotion": "the audio expresses [0:  least",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": ""
        },
        {
          "Emotion": "intense feelings; 1:  most",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": ""
        },
        {
          "Emotion": "intense feelings]",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": ""
        },
        {
          "Emotion": "A group of binary variables",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": ".066**"
        },
        {
          "Emotion": "indicating whether or not the",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": ".007"
        },
        {
          "Emotion": "audio expresses this emotion",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": ".026"
        },
        {
          "Emotion": "[0:  no; 1: yes]",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": "-.044*"
        },
        {
          "Emotion": "",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": "-.049*"
        },
        {
          "Emotion": "",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": ""
        },
        {
          "Emotion": "A continuous variable",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": "-.130**"
        },
        {
          "Emotion": "indicating the degree to which",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": ""
        },
        {
          "Emotion": "the text expresses [0: highly",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": ""
        },
        {
          "Emotion": "intense unpleasant feelings; 1:",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": ""
        },
        {
          "Emotion": "highly intense pleasant",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": ""
        },
        {
          "Emotion": "feelings]",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": ""
        },
        {
          "Emotion": "A group of continuous",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": "-.098"
        },
        {
          "Emotion": "variables indicating the degree",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": "-.006"
        },
        {
          "Emotion": "to which the text expresses [0:",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": ".107**"
        },
        {
          "Emotion": "barely noticeable amount of",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": ".164**"
        },
        {
          "Emotion": "joy/anger/fear/sadness; 1:",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": ""
        },
        {
          "Emotion": "extremely high intensity of",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": ""
        },
        {
          "Emotion": "joy/anger/fear/sadness]",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": ""
        },
        {
          "Emotion": "",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": ""
        },
        {
          "Emotion": "Binary variable indicating",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": ".256**"
        },
        {
          "Emotion": "whether or not the speaker in",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": ""
        },
        {
          "Emotion": "the video is female [1: yes; 0:",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": ""
        },
        {
          "Emotion": "no]",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": ""
        },
        {
          "Emotion": "Binary variable indicating",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": "-.083**"
        },
        {
          "Emotion": "whether or not the speaker in",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": ""
        },
        {
          "Emotion": "the video is male [1: yes; 0: no]",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": ""
        },
        {
          "Emotion": "Binary variable indicating",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": "-.192**"
        },
        {
          "Emotion": "whether or not the speaker in",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": ""
        },
        {
          "Emotion": "the video is specific gender-",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": ""
        },
        {
          "Emotion": "identifiable [1: yes; 0: no]",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": ""
        },
        {
          "Emotion": "The duration of emotion",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": "-.072**"
        },
        {
          "Emotion": "expression episode measured",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": ""
        },
        {
          "Emotion": "in seconds",
          "Correlation Coefficients ( Kendallâ€™s Ï„)": ""
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Appendix B \n \nAdditional Classifier Experiment Results": ""
        },
        {
          "Appendix B \n \nAdditional Classifier Experiment Results": ""
        },
        {
          "Appendix B \n \nAdditional Classifier Experiment Results": ""
        },
        {
          "Appendix B \n \nAdditional Classifier Experiment Results": "ii)  number  of  variables  randomly  sampled  at  each  split  (i.e.  the  ntree  and  mtry"
        },
        {
          "Appendix B \n \nAdditional Classifier Experiment Results": ""
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table B1. Performance analysis using a random forest classifier with class weights (cross-": ""
        },
        {
          "Table B1. Performance analysis using a random forest classifier with class weights (cross-": ""
        },
        {
          "Table B1. Performance analysis using a random forest classifier with class weights (cross-": "Modality"
        },
        {
          "Table B1. Performance analysis using a random forest classifier with class weights (cross-": ""
        },
        {
          "Table B1. Performance analysis using a random forest classifier with class weights (cross-": "Visual"
        },
        {
          "Table B1. Performance analysis using a random forest classifier with class weights (cross-": "Audio"
        },
        {
          "Table B1. Performance analysis using a random forest classifier with class weights (cross-": "Text"
        },
        {
          "Table B1. Performance analysis using a random forest classifier with class weights (cross-": "V + A"
        },
        {
          "Table B1. Performance analysis using a random forest classifier with class weights (cross-": "V + T"
        },
        {
          "Table B1. Performance analysis using a random forest classifier with class weights (cross-": "A + T"
        },
        {
          "Table B1. Performance analysis using a random forest classifier with class weights (cross-": "V + A + T"
        },
        {
          "Table B1. Performance analysis using a random forest classifier with class weights (cross-": "MM1 Score"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "to the number of newly synthesized minority class observations (i.e., under-sampling parameter = 1)3.": ""
        },
        {
          "to the number of newly synthesized minority class observations (i.e., under-sampling parameter = 1)3.": ""
        },
        {
          "to the number of newly synthesized minority class observations (i.e., under-sampling parameter = 1)3.": "Modality"
        },
        {
          "to the number of newly synthesized minority class observations (i.e., under-sampling parameter = 1)3.": ""
        },
        {
          "to the number of newly synthesized minority class observations (i.e., under-sampling parameter = 1)3.": "Visual"
        },
        {
          "to the number of newly synthesized minority class observations (i.e., under-sampling parameter = 1)3.": "Audio"
        },
        {
          "to the number of newly synthesized minority class observations (i.e., under-sampling parameter = 1)3.": "Text"
        },
        {
          "to the number of newly synthesized minority class observations (i.e., under-sampling parameter = 1)3.": "V + A"
        },
        {
          "to the number of newly synthesized minority class observations (i.e., under-sampling parameter = 1)3.": "V + T"
        },
        {
          "to the number of newly synthesized minority class observations (i.e., under-sampling parameter = 1)3.": "A + T"
        },
        {
          "to the number of newly synthesized minority class observations (i.e., under-sampling parameter = 1)3.": "V + A + T"
        },
        {
          "to the number of newly synthesized minority class observations (i.e., under-sampling parameter = 1)3.": "MM1 Score (%)"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "duration vs. long duration) subgroups using a random forest classifier.": ""
        },
        {
          "duration vs. long duration) subgroups using a random forest classifier.": ""
        },
        {
          "duration vs. long duration) subgroups using a random forest classifier.": "Modality"
        },
        {
          "duration vs. long duration) subgroups using a random forest classifier.": ""
        },
        {
          "duration vs. long duration) subgroups using a random forest classifier.": "Visual"
        },
        {
          "duration vs. long duration) subgroups using a random forest classifier.": "Audio"
        },
        {
          "duration vs. long duration) subgroups using a random forest classifier.": "Text"
        },
        {
          "duration vs. long duration) subgroups using a random forest classifier.": "V + A"
        },
        {
          "duration vs. long duration) subgroups using a random forest classifier.": "V + T"
        },
        {
          "duration vs. long duration) subgroups using a random forest classifier.": "A + T"
        },
        {
          "duration vs. long duration) subgroups using a random forest classifier.": "V + A + T"
        },
        {
          "duration vs. long duration) subgroups using a random forest classifier.": "MM1 Score"
        },
        {
          "duration vs. long duration) subgroups using a random forest classifier.": "MM1 Score (Overall)"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table B5. Performance analysis for female subgroup using a random forest classifier": ""
        },
        {
          "Table B5. Performance analysis for female subgroup using a random forest classifier": "Modality"
        },
        {
          "Table B5. Performance analysis for female subgroup using a random forest classifier": ""
        },
        {
          "Table B5. Performance analysis for female subgroup using a random forest classifier": "Visual"
        },
        {
          "Table B5. Performance analysis for female subgroup using a random forest classifier": "Audio"
        },
        {
          "Table B5. Performance analysis for female subgroup using a random forest classifier": "Text"
        },
        {
          "Table B5. Performance analysis for female subgroup using a random forest classifier": "V + A"
        },
        {
          "Table B5. Performance analysis for female subgroup using a random forest classifier": "V + T"
        },
        {
          "Table B5. Performance analysis for female subgroup using a random forest classifier": "A + T"
        },
        {
          "Table B5. Performance analysis for female subgroup using a random forest classifier": "V + A + T"
        },
        {
          "Table B5. Performance analysis for female subgroup using a random forest classifier": "MM1 Score"
        },
        {
          "Table B5. Performance analysis for female subgroup using a random forest classifier": "MM1 Score (Overall)"
        }
      ],
      "page": 19
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Unimodal and Multimodal Human Perception of Naturalistic Non-Basic Affective States during Human-Computer Interactions",
      "authors": [
        "S D'mello",
        "N Dowell",
        "A Graesser"
      ],
      "year": "2013",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "2",
      "title": "A review and meta-analysis of multimodal affect detection systems",
      "authors": [
        "J Kory"
      ],
      "year": "2015",
      "venue": "ACM Comput. Surv"
    },
    {
      "citation_id": "3",
      "title": "Affect detection: An interdisciplinary review of models, methods, and their applications",
      "authors": [
        "R Calvo",
        "S Mello"
      ],
      "year": "2010",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "4",
      "title": "Should Machines Express Sympathy and Empathy? Experiments with a Health Advice Chatbot",
      "authors": [
        "B Liu",
        "S Sundar"
      ],
      "year": "2018",
      "venue": "Cyberpsychology, Behav. Soc. Netw"
    },
    {
      "citation_id": "5",
      "title": "Comparative experiments on sentiment classification for online product reviews",
      "authors": [
        "H Cui",
        "V Mittal",
        "M Datar"
      ],
      "year": "2006",
      "venue": "Proc. of 21st National Conference on Artificial Intelligence, AAAI, MA"
    },
    {
      "citation_id": "6",
      "title": "Recognition of Emotions in Autism: A Formal Meta-analysis",
      "authors": [
        "M Uljarevic",
        "A Hamilton"
      ],
      "year": "2013",
      "venue": "J. Autism Dev. Disord"
    },
    {
      "citation_id": "7",
      "title": "Cross-Cultural Depression Recognition from Vocal Biomarkers",
      "authors": [
        "S Alghowinem",
        "R Goecke",
        "J Epps",
        "M Wagner",
        "J Cohn"
      ],
      "year": "2016",
      "venue": "Cross-Cultural Depression Recognition from Vocal Biomarkers"
    },
    {
      "citation_id": "8",
      "title": "A Large-scale Analysis of Sex Differences in Facial Expressions",
      "authors": [
        "D Mcduff",
        "E Kodra",
        "R El Kaliouby",
        "M Lafrance"
      ],
      "year": "2017",
      "venue": "PLoS One"
    },
    {
      "citation_id": "9",
      "title": "The duration of affective phenomena or emotions, sentiments and passions",
      "authors": [
        "N Frijda",
        "B Mesquita",
        "J Sonnemans",
        "S Van Goozen"
      ],
      "year": "1991",
      "venue": "Wiley Int. Rev. Stud. Emot"
    },
    {
      "citation_id": "10",
      "title": "Intensity and duration of negative emotions: Comparing the role of appraisals and regulation strategies",
      "authors": [
        "K Brans",
        "P Verduyn"
      ],
      "year": "2014",
      "venue": "PLoS One"
    },
    {
      "citation_id": "11",
      "title": "Which emotions last longest and why: The role of event importance and rumination",
      "authors": [
        "P Verduyn",
        "S Lavrijsen"
      ],
      "year": "2015",
      "venue": "Motiv. Emot"
    },
    {
      "citation_id": "12",
      "title": "The Expression of the Emotions in Man and Animals",
      "authors": [
        "C Darwin",
        "P Prodger"
      ],
      "year": "1998",
      "venue": "The Expression of the Emotions in Man and Animals"
    },
    {
      "citation_id": "13",
      "title": "What is Emotion?",
      "authors": [
        "E Zemach"
      ],
      "year": "2001",
      "venue": "Am. Philos. Q"
    },
    {
      "citation_id": "14",
      "title": "What is an Emotion?",
      "authors": [
        "W James"
      ],
      "year": "1884",
      "venue": "Mind"
    },
    {
      "citation_id": "15",
      "title": "Affect as Information",
      "authors": [
        "G Clore",
        "K Gasper",
        "E Garvin"
      ],
      "year": "2001",
      "venue": "Handb. Affect Soc. Cogn"
    },
    {
      "citation_id": "16",
      "title": "Emotion and Moods",
      "authors": [
        "S Robbins",
        "T Judge"
      ],
      "year": "2017",
      "venue": "Organizational Behavior, 17"
    },
    {
      "citation_id": "17",
      "title": "Human Emotions: A Reader",
      "authors": [
        "J Jenkins",
        "K Oatley",
        "N Stein"
      ],
      "year": "1998",
      "venue": "Human Emotions: A Reader"
    },
    {
      "citation_id": "18",
      "title": "Moods, Emotion Episodes, and Emotions",
      "authors": [
        "N Frijda"
      ],
      "year": "1993",
      "venue": "Handb. Emot"
    },
    {
      "citation_id": "19",
      "title": "The Principles of Psychology",
      "authors": [
        "W James",
        "F Burkhardt",
        "F Bowers",
        "I Skrupskelis"
      ],
      "year": "1890",
      "venue": "The Principles of Psychology"
    },
    {
      "citation_id": "20",
      "title": "The James-Lange Theory of Emotions: A Critical Examination and an Alternative Theory",
      "authors": [
        "W Cannon"
      ],
      "year": "1927",
      "venue": "Am. J. Psychol"
    },
    {
      "citation_id": "21",
      "title": "What is Emotion?: History, Measures, and Meanings",
      "authors": [
        "J Kagan"
      ],
      "year": "2007",
      "venue": "What is Emotion?: History, Measures, and Meanings"
    },
    {
      "citation_id": "22",
      "title": "Human emotion recognition from videos using spatio-temporal and audio features",
      "authors": [
        "M Rashid",
        "S Abu-Bakar",
        "M Mokji"
      ],
      "year": "2013",
      "venue": "Vis. Comput"
    },
    {
      "citation_id": "23",
      "title": "Fusion of Fragmentary Classifier Decisions for Affective State Recognition",
      "authors": [
        "G Krell"
      ],
      "year": "2012",
      "venue": "IAPR Workshop on Multimodal Pattern Recognition of Social Signals in Human-Computer Interaction"
    },
    {
      "citation_id": "24",
      "title": "Multimodal semi-automated affect detection from conversational cues, gross body language, and facial features",
      "authors": [
        "A Graesser"
      ],
      "year": "2010",
      "venue": "User Model. User-adapt. Interact"
    },
    {
      "citation_id": "25",
      "title": "An Argument for Basic Emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cogn. Emot"
    },
    {
      "citation_id": "26",
      "title": "The cognitive structure of emotions",
      "authors": [
        "A Ortony",
        "G Clore",
        "A Collins"
      ],
      "year": "1988",
      "venue": "The cognitive structure of emotions"
    },
    {
      "citation_id": "27",
      "title": "What's Basic About Basic Emotions?",
      "authors": [
        "A Ortony",
        "T Turner"
      ],
      "year": "1990",
      "venue": "Psychol. Rev"
    },
    {
      "citation_id": "28",
      "title": "Basic Emotions: Can Conflicting Criteria Converge?",
      "authors": [
        "T Turner",
        "A Ortony"
      ],
      "year": "1992",
      "venue": "Psychol. Rev"
    },
    {
      "citation_id": "29",
      "title": "Hybrid Video Emotional Tagging using Users' EEG and Video Content",
      "authors": [
        "S Wang",
        "Y Zhu",
        "G Wu",
        "Q Ji"
      ],
      "year": "2014",
      "venue": "Multimed. Tools Appl"
    },
    {
      "citation_id": "30",
      "title": "Dimensional affect recognition using continuous conditional random fields",
      "authors": [
        "T BaltruÅ¡aitis",
        "N Banda",
        "P Robinson"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "31",
      "title": "Combining Classifiers in Multimodal Affect Detection",
      "authors": [
        "M Hussain",
        "H Monkaresi",
        "R Calvo"
      ],
      "year": "2012",
      "venue": "Proceedings of the Tenth Australasian Data Mining Conference"
    },
    {
      "citation_id": "32",
      "title": "Audio-visual Emotion Recognition with Boosted Coupled HMM",
      "authors": [
        "K Lu",
        "Y Jia"
      ],
      "year": "2012",
      "venue": "Proceedings of the 21st International Conference on Pattern Recognition"
    },
    {
      "citation_id": "33",
      "title": "Toward a minimal representation of affective gestures",
      "authors": [
        "D Glowinski",
        "N Dael",
        "A Camurri",
        "G Volpe",
        "M Mortillaro",
        "K Scherer"
      ],
      "year": "2011",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "34",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "J. Pers. Soc. Psychol"
    },
    {
      "citation_id": "35",
      "title": "The structure of subjective emotional intensity",
      "authors": [
        "J Sonnemans",
        "N Frijda"
      ],
      "year": "1994",
      "venue": "Cogn. Emot"
    },
    {
      "citation_id": "36",
      "title": "Frijda and others, The emotions",
      "year": "1986",
      "venue": "Frijda and others, The emotions"
    },
    {
      "citation_id": "37",
      "title": "CrystalFeel at SemEval-2018 Task 1: Understanding and Detecting Emotion Intensity using Affective Lexicons",
      "authors": [
        "R Gupta",
        "Y Yang"
      ],
      "year": "2018",
      "venue": "Proceedings of The 12th International Workshop on Semantic Evaluation"
    },
    {
      "citation_id": "38",
      "title": "Toward an Affect-sensitive Multimodal Human-computer Interaction",
      "authors": [
        "M Pantic",
        "L Rothkrantz"
      ],
      "year": "2003",
      "venue": "Proc. IEEE"
    },
    {
      "citation_id": "39",
      "title": "Context-sensitive Learning for Enhanced Audiovisual Emotion Classification",
      "authors": [
        "A Metallinou",
        "M Wollmer",
        "A Katsamanis",
        "F Eyben",
        "B Schuller",
        "S Narayanan"
      ],
      "year": "2012",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "40",
      "title": "Error Weighted Semicoupled Hidden Markov Model for Audio-visual Emotion",
      "authors": [
        "J Lin",
        "C Wu",
        "W Wei"
      ],
      "venue": "Error Weighted Semicoupled Hidden Markov Model for Audio-visual Emotion"
    },
    {
      "citation_id": "41",
      "title": "IEEE Trans. Multimed",
      "year": "2012",
      "venue": "IEEE Trans. Multimed"
    },
    {
      "citation_id": "42",
      "title": "Recognizing Affect from Linguistic Information in 3D Continuous Space",
      "authors": [
        "B Schuller"
      ],
      "year": "2011",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "43",
      "title": "Emotion Recognition of Affective Speech Based on Multiple Classifiers Using Acoustic-prosodic Information and Semantic Labels",
      "authors": [
        "C Wu",
        "W Liang"
      ],
      "year": "2011",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "44",
      "title": "Detecting naturalistic expressions of nonbasic affect using physiological signals",
      "authors": [
        "O Alzoubi",
        "S Mello",
        "R Calvo"
      ],
      "year": "2012",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "45",
      "title": "Multimodal emotion recognition in speech-based interaction using facial expression, body gesture and acoustic analysis",
      "authors": [
        "L Kessous",
        "G Castellano",
        "G Caridakis"
      ],
      "year": "2010",
      "venue": "J. Multimodal User Interfaces"
    },
    {
      "citation_id": "46",
      "title": "LSTM-Modeling of continuous emotions in an audiovisual affect recognition framework",
      "authors": [
        "M Wollmer",
        "M Kaiser",
        "F Eyben",
        "B Schuller",
        "G Rigoll"
      ],
      "year": "2013",
      "venue": "Image Vis. Comput"
    },
    {
      "citation_id": "47",
      "title": "Multiple Classifier Systems for the Classification of Audio-visual Emotional States",
      "authors": [
        "M Glodek"
      ],
      "year": "2011",
      "venue": "Affect. Comput. Intell. Interact"
    },
    {
      "citation_id": "48",
      "title": "Beyond the face: exploring rapid influences of context on face processing",
      "authors": [
        "B De Gelder",
        "H Meeren",
        "R Righart",
        "J Stock",
        "W De Riet",
        "M Tamietto"
      ],
      "year": "2006",
      "venue": "Prog. Brain Res"
    },
    {
      "citation_id": "49",
      "title": "Asynchronous and event-based fusion systems for affect recognition on naturalistic data in comparison to conventional approaches",
      "authors": [
        "F Lingenfelser",
        "J Wagner",
        "J Deng",
        "R Brueckner",
        "B Schuller",
        "E AndrÃ©"
      ],
      "year": "2016",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "50",
      "title": "Multimodal Affect Classification at Various Temporal Lengths",
      "authors": [
        "J Kim",
        "M Clements"
      ],
      "year": "2015",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "51",
      "title": "Angry, Disgusted, or Afraid? Studies on the Malleability of Emotion Perception",
      "authors": [
        "H Aviezer"
      ],
      "year": "2008",
      "venue": "Psychol. Sci"
    },
    {
      "citation_id": "52",
      "title": "Gender and emotions",
      "authors": [
        "J Brebner"
      ],
      "year": "2003",
      "venue": "Pers. Individ. Dif"
    },
    {
      "citation_id": "53",
      "title": "A female advantage in the recognition of emotional facial expressions: Test of an evolutionary hypothesis",
      "authors": [
        "E Hampson",
        "S Van Anders",
        "L Mullin"
      ],
      "year": "2006",
      "venue": "Evol. Hum. Behav"
    },
    {
      "citation_id": "54",
      "title": "A Meta-analytic Review of Sex Differences in Facial Expression Processing and their Development in Infants, Children, and Adolescents",
      "authors": [
        "E Mcclure"
      ],
      "year": "2000",
      "venue": "Psychol. Bull"
    },
    {
      "citation_id": "55",
      "title": "Sex differences in disgust: Why are women more easily disgusted than men?",
      "authors": [
        "L Al-Shawaf",
        "D Lewis",
        "D Buss"
      ],
      "year": "2018",
      "venue": "Emot. Rev"
    },
    {
      "citation_id": "56",
      "title": "Sex differences in the Recognition of Infant Facial Expressions of Emotion: The Primary Caretaker Hypothesis",
      "authors": [
        "W Babchuk",
        "R Hames",
        "R Thompson"
      ],
      "year": "1985",
      "venue": "Ethol. Sociobiol"
    },
    {
      "citation_id": "57",
      "title": "Social Functions of Emotions at Four Levels of Analysis",
      "authors": [
        "D Keltner",
        "J Haidt"
      ],
      "year": "1999",
      "venue": "Cogn. Emot"
    },
    {
      "citation_id": "58",
      "title": "The Omg-emotion Behavior Dataset",
      "authors": [
        "P Barros",
        "N Churamani",
        "E Lakomkin",
        "H Siqueira",
        "A Sutherland",
        "S Wermter"
      ],
      "year": "2018",
      "venue": "2018 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "59",
      "title": "Probabilistic Approach to People-centric Photo selection and Sequencing",
      "authors": [
        "V Vonikakis",
        "R Subramanian",
        "J Arnfred",
        "S Winkler"
      ],
      "year": "2017",
      "venue": "IEEE Trans. Multimed"
    },
    {
      "citation_id": "60",
      "title": "Give me One Portrait Image, I will tell you your Emotion and Personality",
      "authors": [
        "S Peng",
        "L Zhang",
        "S Winkler",
        "M Winslett"
      ],
      "year": "2018",
      "venue": "Proceedings of the 26th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "61",
      "title": "Emotion-based Sequence of Family Photos",
      "authors": [
        "V Vonikakis",
        "S Winkler"
      ],
      "year": "2012",
      "venue": "Proceedings of the 20th ACM international conference on Multimedia"
    },
    {
      "citation_id": "62",
      "title": "Group Happiness Assessment using Geometric Features and Dataset Balancing",
      "authors": [
        "V Vonikakis",
        "Y Yazici",
        "V Nguyen",
        "S Winkler"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "63",
      "title": "Deep Learning for Emotion Recognition on Small Datasets using Transfer Learning",
      "authors": [
        "H Ng",
        "V Nguyen",
        "V Vonikakis",
        "S Winkler"
      ],
      "year": "2015",
      "venue": "Proceedings of the ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "64",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "65",
      "title": "Visual Speech Emotion Conversion using Deep Learning for 3D Talking Head",
      "authors": [
        "D.-Y Huang"
      ],
      "year": "2018",
      "venue": "Proceedings of the Joint Workshop of the 4th Workshop on Affective Social Multimedia Computing and First Multi-Modal Affective Computing of Large-Scale Multimedia Data"
    },
    {
      "citation_id": "66",
      "title": "Multimodal Prediction Of Affective Dimensions Fusing Multiple Regression Techniques",
      "authors": [
        "D Huang"
      ],
      "year": "2017",
      "venue": "Multimodal Prediction Of Affective Dimensions Fusing Multiple Regression Techniques"
    },
    {
      "citation_id": "67",
      "title": "Audio-visual Emotion Recognition Using Deep Transfer Learning and Multiple Temporal Models",
      "authors": [
        "X Ouyang"
      ],
      "year": "2017",
      "venue": "19th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "68",
      "title": "Audio and Face Video Emotion Recognition in the Wild using Deep Neural Networks and Small Datasets",
      "authors": [
        "W Ding"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "69",
      "title": "Mobile Acoustic Emotion Recognition",
      "authors": [
        "W Quck",
        "D Huang",
        "W Lin",
        "H Li",
        "M Dong"
      ],
      "year": "2016",
      "venue": "IEEE Region 10 Conference"
    },
    {
      "citation_id": "70",
      "title": "Acoustic emotion recognition based on fusion of multiple featuredependent deep Boltzmann machines",
      "authors": [
        "K Poon-Feng",
        "D.-Y Huang",
        "M Dong",
        "H Li"
      ],
      "year": "2014",
      "venue": "The 9th International Symposium on Chinese Spoken Language Processing"
    },
    {
      "citation_id": "71",
      "title": "Avec 2011--the first international audio/visual emotion challenge",
      "authors": [
        "B Schuller",
        "M Valstar",
        "F Eyben",
        "G Mckeown",
        "R Cowie",
        "M Pantic"
      ],
      "year": "2011",
      "venue": "International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "72",
      "title": "The world of emotions is not two-dimensional",
      "authors": [
        "J Fontaine",
        "K Scherer",
        "E Roesch",
        "P Ellsworth"
      ],
      "year": "2007",
      "venue": "Psychol. Sci"
    },
    {
      "citation_id": "73",
      "title": "Single and multi-channel approaches for distant speech recognition under noisy reverberant conditions: I2R'S system description for the ASpIRE challenge",
      "authors": [
        "J Dennis",
        "T Dat"
      ],
      "year": "2015",
      "venue": "2015 IEEE Workshop on Automatic Speech Recognition and Understanding"
    },
    {
      "citation_id": "74",
      "title": "Multi-task learning using mismatched transcription for under-resourced speech recognition",
      "authors": [
        "V Do",
        "N Chen",
        "B Lim",
        "M Hasegawa-Johnson"
      ],
      "year": "2017",
      "venue": "Proceedings of the Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "75",
      "title": "Multi-Task Learning for Mispronunciation Detection on Singapore Children's Mandarin Speech",
      "authors": [
        "R Tong",
        "N Chen",
        "B Ma"
      ],
      "year": "2017",
      "venue": "Multi-Task Learning for Mispronunciation Detection on Singapore Children's Mandarin Speech"
    },
    {
      "citation_id": "76",
      "title": "Unsupervised Feature Adaptation Using Adversarial Multi-Task Training for Automatic Evaluation of Children's Speech",
      "authors": [
        "R Duan",
        "N Chen"
      ],
      "year": "2020",
      "venue": "Unsupervised Feature Adaptation Using Adversarial Multi-Task Training for Automatic Evaluation of Children's Speech"
    },
    {
      "citation_id": "77",
      "title": "Senone-aware Adversarial Multitask Training for Unsupervised Child to Adult Speech Adaptation",
      "authors": [
        "R Duan",
        "N Chen"
      ],
      "venue": "International Conference on Acoustics, Speech, & Signal Processing"
    },
    {
      "citation_id": "78",
      "title": "Strategies for Vietnamese keyword search",
      "authors": [
        "N Chen"
      ],
      "year": "2014",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "79",
      "title": "Multimodal neural pronunciation modeling for spoken languages with logographic origin",
      "authors": [
        "M Nguyen",
        "G Ngo",
        "N Chen"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "80",
      "title": "Hierarchical Character Embeddings: Learning Phonological and Semantic Representations in Languages of Logographic Origin Using Recursive Neural Networks",
      "authors": [
        "M Nguyen",
        "G Ngo",
        "N Chen"
      ],
      "year": "2019",
      "venue": "IEEE/ACM Trans. Audio, Speech, Lang. Process"
    },
    {
      "citation_id": "81",
      "title": "Unsupervised data selection and word-morph mixed language model for tamil low-resource keyword search",
      "authors": [
        "C Ni",
        "C.-C Leung",
        "L Wang",
        "N Chen",
        "B Ma"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "82",
      "title": "Emotion Intensities in Tweets",
      "authors": [
        "S Mohammad",
        "F Bravo-Marquez"
      ],
      "year": "2017",
      "venue": "Proceedings of the Sixth Joint Conference on Lexical and Computational Semantics"
    },
    {
      "citation_id": "83",
      "title": "What Constitutes Happiness? Predicting and Characterizing the Ingredients of Happiness Using Emotion Intensity Analysis",
      "authors": [
        "R Gupta",
        "P Bhattacharya",
        "Y Yang"
      ],
      "year": "2019",
      "venue": "The AAAI-19 Workshop on Affective Content Analysis"
    },
    {
      "citation_id": "84",
      "title": "Predicting and Understanding News Social Popularity with Emotional Salience Features",
      "authors": [
        "R Gupta",
        "Y Yang"
      ],
      "year": "2019",
      "venue": "Proceedings of the 27th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "85",
      "title": "Multimodal Emotion Recognition in Response to Videos",
      "authors": [
        "M Soleymani",
        "M Pantic",
        "T Pun"
      ],
      "year": "2012",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "86",
      "title": "SMOTE: synthetic minority over-sampling technique",
      "authors": [
        "N Chawla",
        "K Bowyer",
        "L Hall",
        "W Kegelmeyer"
      ],
      "year": "2002",
      "venue": "J. Artif. Intell. Res"
    },
    {
      "citation_id": "87",
      "title": "Multimodal Emotional State Recognition Using Sequence-dependent Deep Hierarchical Features",
      "authors": [
        "P Barros",
        "D Jirak",
        "C Weber",
        "S Wermter"
      ],
      "year": "2015",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "88",
      "title": "Analysis of Emotion Recognition using Facial Expressions, Speech and Multimodal Information",
      "authors": [
        "C Busso"
      ],
      "year": "2004",
      "venue": "Proceedings of the 6th International Conference on Multimodal Interfaces"
    },
    {
      "citation_id": "89",
      "title": "Emotion Recognition Based on Phoneme Classes",
      "authors": [
        "C Lee"
      ],
      "year": "2004",
      "venue": "Eighth International Conference on Spoken Language Processing"
    },
    {
      "citation_id": "90",
      "title": "CREMA-D: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "91",
      "title": "Gender and emotion in context",
      "authors": [
        "L Brody",
        "J Hall"
      ],
      "year": "2008",
      "venue": "Handb. Emot"
    },
    {
      "citation_id": "92",
      "title": "Voice recognition still has significant race and gender biases",
      "authors": [
        "J Bajorek"
      ],
      "year": "2019",
      "venue": "Harvard Business Review"
    },
    {
      "citation_id": "93",
      "title": "An analysis of gender bias studies in natural language processing",
      "authors": [
        "M Costa-JussÃ "
      ],
      "year": "2019",
      "venue": "Nat. Mach. Intell"
    }
  ]
}