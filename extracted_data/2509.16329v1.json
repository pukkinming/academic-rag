{
  "paper_id": "2509.16329v1",
  "title": "Investigating Polyglot Speech Foundation Models For Learning Collective Emotion From Crowds",
  "published": "2025-09-19T18:13:28Z",
  "authors": [
    "Orchid Chetia Phukan",
    "Girish",
    "Mohd Mujtaba Akhtar",
    "Panchal Nayak",
    "Priyabrata Mallick",
    "Swarup Ranjan Behera",
    "Parabattina Bhagath",
    "Pailla Balakrishna Reddy",
    "Arun Balaji Buduru"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper investigates the polyglot (multilingual) speech foundation models (SFMs) for Crowd Emotion Recognition (CER). We hypothesize that polyglot SFMs, pre-trained on diverse languages, accents, and speech patterns, are particularly adept at navigating the noisy and complex acoustic environments characteristic of crowd settings, thereby offering a significant advantage for CER. To substantiate this, we perform a comprehensive analysis, comparing polyglot, monolingual, and speaker recognition SFMs through extensive experiments on a benchmark CER dataset across varying audio durations (1 sec, 500 ms, and 250 ms). The results consistently demonstrate the superiority of polyglot SFMs, outperforming their counterparts across all audio lengths and excelling even with extremely short-duration inputs. These findings pave the way for adaptation of SFMs in setting up new benchmarks for CER.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Crowd Emotion Recognition (CER) involves the intricate task of predicting collective emotional states in large groups, where emotions are conveyed vocally through cheering, booing, and clapping, and visually through gestures such as waving and synchronized movements. These emotional expressions are prevalent at events like sports matches, concerts, political rallies, and social movements, where they profoundly shape the overall atmosphere and influence both participants and performers. Accurately capturing and interpreting these emotions is crucial across fields such as public safety, event management, and social research. Research in CER has predominantly centered around visual modalities, such as facial expressions and body movements  [1] ,  [2] ,  [3] ,  [4] ,  [5] ,  [6] . However, audiobased approaches remain relatively unexamined, highlighting a significant gap in the field.\n\nCER through audio poses distinct challenges, largely due to the noisy, spontaneous, and overlapping nature of crowd expressions, particularly in audio streams. Crowds express emotions collectively, but the complexity and dynamism of these environments have left CER underexplored compared to speech emotion recognition (SER), that focuses on recognizing a single individual emotions. Additionally, the limited availability of labeled data also hinders the research into CER through audio cues. Overcoming these obstacles necessitates innovative approaches capable of managing both the linguistic diversity and the noisy, dynamic environments typical of large crowds.\n\nTo solve this gap, Franzoni et al.  [7]  presented the first audiobased CER dataset. Adding on this, Faisal et al.  [8]  used MFCC with Random Foreset classifier and Vision-based foundation models like MobileNetV2 with spectrograms for CER. Anand et al.  [9]  used CLAP representations with a downstream neural network-based approach for predicting crowd excitement score. In this work, we focus on CER through audio.\n\nUnlike CER, SER has sufficient development particularly due to recent advancements in speech foundation models (SFMs), such as Wav2Vec, HuBERT, pre-trained on large-scale diverse speech datasets, offer promising solutions for performance benefit, data scarcity as well as prevention of training models from scratch  [10] ,  [11] ,  [12] . These models have also excelent in various other speech processing tasks such as speech recognition  [13] , speaker segmentation  [14] , shout intensity prediction  [15] , and so on. Applying such SFMs to CER holds the potential to significantly improve data efficiency and enhance performance in CER, especially within the noisy and diverse environments that characterize real-world crowd scenarios.\n\nTo bridge this gap, we investigate the potential of stateof-the-art (SOTA) SFMs for recognizing crowd emotions and we hypothesize that polyglot (multilingual) SFMs will prove highly effective for CER by navigating the noisy and complex environments of crowded environments. This can be attributed to their capacity to capture a wide range of pitches, tones, and emotional nuances owing to their pre-training on diverse speech data that encompasses various languages, accents, and speaking styles. To validate, our hypothesis, we carry out a large-scale comparison of various SOTA SFMs with different downstreams such as SVM, Random Foreset, Fully Connected Network (FCN), and CNN. To the best of our knowledge, we are the first study to explore SFMs for CER. To summarize, the major contributions of this study are as follows:",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Speech Foundation Models",
      "text": "We leverage SOTA SFMs that excels across various tasks in speech processing across different benchmarks. Each SFM with their distinct technical strengths, provides critical capabilities aligned with our investigation into CER. By integrating these advanced SFMs, our approach harnesses their scalability, robustness, and versatility, making them invaluable for effective CER. Detailed explanation of the Detailed explanation regarding the SFM considered in our study are given below: WavLM 1    [16]  It is a SOTA SFM that outperforms all the other SFMs in various speech procesing tasks in SUPERB benchmark. It learns both speech denoising and masked prediction during training and trained on 960 hours of english librispeech data. We use the base version of 94.70 million parameters. UniSpeech-SAT 2    [17]  employs a contrastive objective alongside multitask learning. Its pre-training follows a speaker-aware approach and is conducted using 960 hours of Librispeech English speech data. We utilize the base variant, which comprises 94.68 million parameters. Wav2vec2 3    [18]  employs a self-supervised learning approach, transforming raw audio into latent speech representations through a convolutional feature encoder and Transformer blocks. We consider the base version of 95.04 million parameters trained on 960 hours of Librispeech data in english. HuBERT 4    [19]  employs a BERT-like masked prediction framework to learn both acoustic and linguistic features effectively. It shows SOTA performance on multiple speech recognition benchmarks in comparison to previous wav2vec2. We use the base version of 94.68 million parameters trained on librispeech 960 hour english data. XLS-R 5    [20]  is a multi-lingual representation learning model based on wav2vec2 architecture and trained on 128 languages. The training datasets comprises of BABEL, Voxlingua107, Commonvoice, MLS, and VoxPopuli. We use the 300 million parameters variant in our work. Whisper 6    [21]  is a multi-task learning SFM based on vanilla transformer encoder-decoder architecture. Pre-training is carried out in 680k hours of multilingual data in 96 languages and in a weakly supervised manner. Whisper shows improve performance than XLS-R for multilingual speech recognition. We utilize the base version of 74 millions parameters. Massively Multilingual Speech (MMS) 7    [22]  is based on the wav2vec2 architecture and pre-trained on apporximately 1400 languages. It uses around 500k hours of data for its pretraining including FLEURS, MMS-lab, BABEL and solves constrastive learning objective. We use the openly available 1 billion parameters variant.\n\nx-vector  [23]  is specifically designed for speaker recognition and it is a time-delay neural network. However, x-vector has shown its effectiveness in related applications such as SER  [11] , shout intensity prediction  [15] , so we thought it might be helpful for CER and so, we include it in our experiments. It consists of 4.2 million parameters. We use frozen SFMs as we want to understand their implicit capacity of understanding CER. We extract representations from the last hidden state through the use of mean pooling with dimensional size of 768 for WavlM, Unispeech-SAT, wav2vec2, HuBERT and 1280 for XLS-R, MMS. For Whisper and x-vector, the dimensions are 512, however, for Whisper, we extract it from the last hidden state of the encoder and discard the decoder.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Experiments A. Benchmark Dataset",
      "text": "We utilize the only openly accesible dataset for CER, to the best of our knowledge given by Franzoni et al.  [7]  in our study. It was meticulously curated to capture the rich emotional expressions of crowds during high-attendance events, such as sports matches, concerts, political rallies, and public gatherings. We segment high-quality audio clips into 1-second blocks with a 0.25 second overlap, yielding a total of 9515 blocks from 69 original clips following Franzoni et al.  [7] . Each block was assigned with emotional labels assigned to three distinct categories: Approval (cheering, clapping), Disapproval (booing, hissing), and Neutral (background chatter) as the original audioclip label. More detailed statistics are presented in Table  I . Further, we segment these 1-sec blocks into 500 milliseconds (ms), and 250 ms. However, before splitting into ms duration audios, we remove the silence as otherwise some silent audio might be present. We segment into short segments to understand the capability of SFMs for short-segment CER. All the audios were resampled to 16 KHz before passing it through the SFMs.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Downstream Modeling",
      "text": "We include both classical ML and DL networks as downstream networks with the SFMs. This includes SVM, Random Forest Classifier (RFC), Fully Connected Network (FCN), and    For CNN, it consists of three 1D-CNN layers: the first with 32 filters, the second with 64 filters, and the third with 128 filters, each with a filter size of 3x3, ReLU activation, and a stride of 1. After each convolutional layer, batch normalization and max-pooling with a pool size of 2x2 were applied. The output was flattened and passed through two fully connected layers, the first with 512 neurons and the second with 128 neurons, both using ReLU activation. A dropout layer with a rate of 0.5 was added between the layers to prevent overfitting. The final output layer used softmax activation with 3 neurons for classification. Similarly, the FCN model followed a similar structure, consisting of fully connected layers with 512, 256, and 128 neurons, each using ReLU activation, and a dropout rate of 0.5 was applied to prevent overfitting. The output layer in the FCN model also had 3 neurons with softmax activation. The design of our methodology is depicted in Figure  1 . Detailed architectural details of FCN and CNN are given in Figure  2a  and 2b. FCN models trainable parameters are from 1 to 3 millions followed by the CNN models with 2.5 to 4 millions.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "C. Training Details",
      "text": "We use Adam as the optimizer and learning rate of 0.001. We employ categorical cross-entropy as loss. Training was conducted over 50 epochs with a batch size of 32. We use 5-fold cross-validation for training and testing. Here, 4 folds are used for training and one fold for testing. Additionally, we make use of early stopping for preventing overfitting.  Table  II  provides the evaluation scores of different downstream models trained on top of SFMs representations. For all the audio durations (1 sec, 500 ms and 250 ms), we can see that polyglot SFMs (XLS-R, Whisper, MMS) completely dominate other SFMs including monolingual and speaker recognition SFMs for CER. This validates our hypothesis that polyglot SFMs will excel in CER due to their ability to capture diverse pitches, tones, and emotional variations. This stems from their pre-training on a broad spectrum of speech data, covering multiple languages, accents, and speaking styles. Overall CNN models shows superior performance in comparison to other downstreams (SVM, RFC, FCN) with different SFMs in all the audio durations. For the 1-sec audio segments, MMS demonstrated the topmost performance, achieving an accuracy of 99.11% and an F1 score of 96.85% with CNN. Among the polyglot SFMs also, MMS is top and this can be attributed to its larger size of 1 billion parameters allowing it capture the acoustic characteristics required for CER in a much better manner. For 500 ms and 250 ms also, we observe clear dominance of MMS over other SFMs including polyglot SFMs. MMS reported accuracy of 99.19%, 99.20% and F1 score of 96.66%, 96.65% for 500 ms and 250 ms respectively with CNN.\n\nAmong the monolingual SFMs (Wav2vec2, Unispeech-SAT, WavLM, HuBERT), HuBERT showed the best results for all the audio durations. It reported accuracy of 98.09%, 98.23%, 97.88% and F1 score of 93.29%, 94.71%, 92.38% for 1 sec, 500 ms, and 250 ms respectively. This could point towards its ability in capturing the diverse crowd sounds in a superior manner. One interesting observation is the performance of xvector. Despite being a very small SFM comprising of only 4.2 million paramters, it shows comparative performance in comparison with monolingual PTMs in some instances. This behavior can be traced to its speaker recognition providing ability to capture diverse pitches, tones, and so on speech characteristics for effective CER. We visualize t-SNE plots of the raw representations from the last hidden state of the SFMs in Figure  3 . These plots reveal clearer and more distinct clustering for polyglot SFMs across different emotional categories, amplifying the superior performance observed in the results. We also plot the confusion matrices of MMS and HuBERT in Figure  4  for 1 sec duration. These findings underscore the potential of leveraging polyglot SFMs to significantly enhance the performance of CER systems and establishing a solid foundation for future research. Comparison with MFCC Baseline: As MFCC is one of the most used input representation for speech and audio processing and used by research works as a baseline for evaluating their proposed methods performance  [24] ,  [25] , we also give a comparison of the best models with baseline MFCC features to understand the effectiveness of polyglot SFMs. We keep the modeling and training details same for experiments with MFCC as set for experiments with SFMs representations. The comparison is presented in Figure  5 . The best model CNN(MMS) shows superior performance in comparison to baseline MFCC feature.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iv. Conclusion",
      "text": "In this study, we investigated the effectiveness of polyglot SFMs for CER. We hypothesized that polyglot SFMs, pretrained on diverse languages, accents, and speech patterns, would be particularly adept at handling the noisy and complex acoustic environments of crowds, offering a distinct advantage for CER. Through extensive experiments on a benchmark CER dataset with varying audio durations (1 sec, 500 ms, and 250 ms), our analysis confirmed that polyglot SFMs consistently outperformed monolingual and speaker recognition SFMs, demonstrating superior performance even with short-duration inputs. These findings reinforce the potential of polyglot SFMs in CER and set a foundation for future research. Our study will also act as a guide for selection of SFMs for CER and related applications.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: â€¢ We perform extensive experiments on a benchmark CER",
      "page": 1
    },
    {
      "caption": "Figure 1: Overview of the proposed system architecture for CER, illustrating the SFM, representation extraction, and classification",
      "page": 3
    },
    {
      "caption": "Figure 2: Model Architecture",
      "page": 3
    },
    {
      "caption": "Figure 1: . Detailed",
      "page": 3
    },
    {
      "caption": "Figure 3: t-SNE plot visualization of different SFMs: (a) Wav2vec2, (b) Unispeech-SAT, (c) WavLM, (d) x-vector, (e) Whisper, (f)",
      "page": 4
    },
    {
      "caption": "Figure 4: Confusion matrices for 1 sec duration: (a) MMS (b)",
      "page": 4
    },
    {
      "caption": "Figure 5: Comparison of the best models with baseline MFCC;",
      "page": 4
    },
    {
      "caption": "Figure 3: These plots reveal clearer and more",
      "page": 5
    },
    {
      "caption": "Figure 4: for 1 sec duration. These findings",
      "page": 5
    },
    {
      "caption": "Figure 5: The best model",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "96.94": "95.35",
          "90.14": "79.80",
          "95.34": "95.24",
          "84.83": "80.67",
          "93.08": "93.38",
          "73.28": "76.03",
          "97.87": "94.07",
          "92.03": "77.55",
          "95.80": "95.63",
          "84.55": "85.24",
          "98.61": "98.71",
          "95.88": "94.96",
          "98.75": "97.45",
          "95.59": "92.51",
          "98.35": "96.48",
          "94.04": "88.81"
        },
        {
          "96.94": "97.29",
          "90.14": "91.62",
          "95.34": "96.00",
          "84.83": "85.75",
          "93.08": "94.19",
          "73.28": "78.60",
          "97.87": "97.87",
          "92.03": "92.03",
          "95.80": "97.64",
          "84.55": "92.80",
          "98.61": "98.93",
          "95.88": "96.10",
          "98.75": "99.06",
          "95.59": "96.81",
          "98.35": "98.35",
          "94.04": "94.04"
        },
        {
          "96.94": "97.51",
          "90.14": "92.73",
          "95.34": "96.40",
          "84.83": "87.17",
          "93.08": "95.19",
          "73.28": "84.32",
          "97.87": "98.09",
          "92.03": "93.29",
          "95.80": "98.23",
          "84.55": "94.61",
          "98.61": "98.97",
          "95.88": "96.64",
          "98.75": "99.11",
          "95.59": "96.85",
          "98.35": "98.71",
          "94.04": "95.39"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "96.93": "96.32",
          "89.97": "86.32",
          "95.69": "95.71",
          "85.71": "82.19",
          "92.67": "92.85",
          "66.04": "71.57",
          "97.28": "94.17",
          "90.26": "75.63",
          "95.56": "94.57",
          "86.69": "76.80",
          "98.99": "97.69",
          "96.47": "92.49",
          "99.00": "96.99",
          "97.60": "90.78",
          "98.41": "97.00",
          "94.85": "90.13"
        },
        {
          "96.93": "97.17",
          "89.97": "90.02",
          "95.69": "98.61",
          "85.71": "94.43",
          "92.67": "93.45",
          "66.04": "77.08",
          "97.28": "98.07",
          "90.26": "92.52",
          "95.56": "96.48",
          "86.69": "88.17",
          "98.99": "98.99",
          "96.47": "95.81",
          "99.00": "99.10",
          "97.60": "96.61",
          "98.41": "98.68",
          "94.85": "94.94"
        },
        {
          "96.93": "97.44",
          "89.97": "91.67",
          "95.69": "95.52",
          "85.71": "84.87",
          "92.67": "94.22",
          "66.04": "81.00",
          "97.28": "98.23",
          "90.26": "94.71",
          "95.56": "97.58",
          "86.69": "91.67",
          "98.99": "98.90",
          "96.47": "96.17",
          "99.00": "99.19",
          "97.60": "96.66",
          "98.41": "98.90",
          "94.85": "95.63"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "95.38": "92.93",
          "82.83": "72.87",
          "94.96": "92.22",
          "83.01": "70.74",
          "92.48": "91.16",
          "58.18": "64.46",
          "97.57": "95.24",
          "91.86": "80.68",
          "93.62": "93.44",
          "81.57": "72.37",
          "98.81": "97.98",
          "95.61": "93.65",
          "98.84": "97.57",
          "96.34": "91.86",
          "97.91": "98.20",
          "92.92": "93.55"
        },
        {
          "95.38": "95.66",
          "82.83": "84.86",
          "94.96": "94.83",
          "83.01": "81.77",
          "92.48": "93.25",
          "58.18": "74.70",
          "97.57": "97.69",
          "91.86": "92.05",
          "93.62": "94.08",
          "81.57": "79.66",
          "98.81": "98.95",
          "95.61": "96.21",
          "98.84": "98.87",
          "96.34": "96.47",
          "97.91": "98.44",
          "92.92": "94.63"
        },
        {
          "95.38": "96.20",
          "82.83": "87.25",
          "94.96": "95.20",
          "83.01": "83.14",
          "92.48": "94.08",
          "58.18": "79.66",
          "97.57": "97.88",
          "91.86": "92.38",
          "93.62": "95.02",
          "81.57": "83.17",
          "98.81": "98.97",
          "95.61": "96.58",
          "98.84": "99.20",
          "96.34": "96.65",
          "97.91": "98.44",
          "92.92": "94.63"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Crowd emotion detection using dynamic probabilistic models",
      "authors": [
        "M Baig",
        "E Barakova",
        "L Marcenaro",
        "M Rauterberg",
        "C Regazzoni"
      ],
      "year": "2014",
      "venue": "From Animals to Animats 13: 13th International Conference on Simulation of Adaptive Behavior, SAB 2014"
    },
    {
      "citation_id": "2",
      "title": "Non-volume preserving-based fusion to grouplevel emotion recognition on crowd videos",
      "authors": [
        "K Quach",
        "N Le",
        "C Duong",
        "I Jalata",
        "K Roy",
        "K Luu"
      ],
      "year": "2022",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "3",
      "title": "A selffusion network based on contrastive learning for group emotion recognition",
      "authors": [
        "X Wang",
        "D Zhang",
        "H.-Z Tan",
        "D.-J Lee"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "4",
      "title": "Semisupervised group emotion recognition based on contrastive learning",
      "authors": [
        "J Zhang",
        "X Wang",
        "D Zhang",
        "D.-J Lee"
      ],
      "year": "2022",
      "venue": "Electronics"
    },
    {
      "citation_id": "5",
      "title": "Towards a robust group-level emotion recognition via uncertainty-aware learning",
      "authors": [
        "Q Zhu",
        "Q Mao",
        "J Zhang",
        "X Huang",
        "W Zheng"
      ],
      "year": "2023",
      "venue": "Towards a robust group-level emotion recognition via uncertainty-aware learning",
      "arxiv": "arXiv:2310.04306"
    },
    {
      "citation_id": "6",
      "title": "Crowd emotion recognition based on causal spatiotemporal structure",
      "authors": [
        "M Wu",
        "L Wang",
        "G Li"
      ],
      "year": "2022",
      "venue": "Proceedings of the 8th International Conference on Computing and Artificial Intelligence"
    },
    {
      "citation_id": "7",
      "title": "Emotional sounds of crowds: Spectrogram-based analysis using deep learning",
      "authors": [
        "V Franzoni",
        "G Biondi",
        "A Milani"
      ],
      "year": "2020",
      "venue": "Multimedia tools and applications"
    },
    {
      "citation_id": "8",
      "title": "Eslce: A dataset of emotional sounds from large crowd events",
      "authors": [
        "M Faisal",
        "M Ahmed",
        "M Ahad"
      ],
      "year": "2021",
      "venue": "2021 Joint 10th International Conference on Informatics, Electronics & Vision (ICIEV) and 2021 5th International Conference on Imaging, Vision & Pattern Recognition (icIVPR)"
    },
    {
      "citation_id": "9",
      "title": "Pulse of the crowd: Quantifying crowd energy through audio and video analysis",
      "authors": [
        "A Anand"
      ],
      "year": "2024",
      "venue": "2024 IEEE 7th International Conference on Multimedia Information Processing and Retrieval (MIPR)"
    },
    {
      "citation_id": "10",
      "title": "Evaluating self-supervised speech representations for speech emotion recognition",
      "authors": [
        "B Atmaja",
        "A Sasou"
      ],
      "year": "2022",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "11",
      "title": "Transforming the embeddings: A lightweight technique for speech emotion recognition tasks",
      "authors": [
        "O Chetia Phukan",
        "A Balaji",
        "R Buduru",
        "Sharma"
      ],
      "venue": "Transforming the embeddings: A lightweight technique for speech emotion recognition tasks",
      "doi": "10.21437/Interspeech.2023-2561"
    },
    {
      "citation_id": "12",
      "title": "Exploring wav2vec 2.0 fine tuning for improved speech emotion recognition",
      "authors": [
        "L.-W Chen",
        "A Rudnicky"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "Investigation of ensemble features of self-supervised pretrained models for automatic speech recognition",
      "authors": [
        "A Arunkumar",
        "V Sukhadia",
        "S Umesh"
      ],
      "venue": "Investigation of ensemble features of self-supervised pretrained models for automatic speech recognition",
      "doi": "10.21437/Interspeech.2022-11376"
    },
    {
      "citation_id": "14",
      "title": "Specializing self-supervised speech representations for speaker segmentation",
      "authors": [
        "S Baroudi",
        "T Pellegrini",
        "H Bredin"
      ],
      "year": "2024",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "15",
      "title": "Investigating the effectiveness of speaker embeddings for shout intensity prediction",
      "authors": [
        "T Fukumori",
        "T Ishida",
        "Y Yamashita"
      ],
      "year": "2023",
      "venue": "2023 Asia Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "16",
      "title": "Wavlm: Large-scale self-supervised pretraining for full stack speech processing",
      "authors": [
        "S Chen"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "Unispeech-sat: Universal speech representation learning with speaker aware pre-training",
      "authors": [
        "S Chen"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/ICASSP43922.2022.9747077"
    },
    {
      "citation_id": "18",
      "title": "Wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "19",
      "title": "Hubert: Selfsupervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "20",
      "title": "Xls-r: Self-supervised cross-lingual speech representation learning at scale",
      "authors": [
        "A Babu"
      ],
      "venue": "Xls-r: Self-supervised cross-lingual speech representation learning at scale",
      "doi": "10.21437/Interspeech.2022-143"
    },
    {
      "citation_id": "21",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "22",
      "title": "Scaling speech technology to 1,000+ languages",
      "authors": [
        "V Pratap"
      ],
      "year": "2024",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "23",
      "title": "X-vectors: Robust dnn embeddings for speaker recognition",
      "authors": [
        "D Snyder",
        "D Garcia-Romero",
        "G Sell",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2018",
      "venue": "2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "24",
      "title": "Whisper features for dysarthric severity-level classification",
      "authors": [
        "S Rathod",
        "M Charola",
        "A Vora",
        "Y Jogi",
        "H Patil"
      ],
      "venue": "Whisper features for dysarthric severity-level classification",
      "doi": "10.21437/Interspeech.2023-1891"
    },
    {
      "citation_id": "25",
      "title": "Whisper encoder features for infant cry classification",
      "authors": [
        "M Charola",
        "A Kachhi",
        "H Patil"
      ],
      "venue": "Whisper encoder features for infant cry classification",
      "doi": "10.21437/Interspeech.2023-1916"
    }
  ]
}