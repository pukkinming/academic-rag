{
  "paper_id": "2406.15119v2",
  "title": "Breaking Resource Barriers In Speech Emotion Recognition Via Data Distillation",
  "published": "2024-06-21T13:10:46Z",
  "authors": [
    "Yi Chang",
    "Zhao Ren",
    "Zhonghao Zhao",
    "Thanh Tam Nguyen",
    "Kun Qian",
    "Tanja Schultz",
    "Björn W. Schuller"
  ],
  "keywords": [
    "speech recognition",
    "human-computer interaction",
    "computational paralinguistics"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition (SER) plays a crucial role in human-computer interaction. The emergence of edge devices in the Internet of Things (IoT) presents challenges in constructing intricate deep learning models due to constraints in memory and computational resources. Moreover, emotional speech data often contains private information, raising concerns about privacy leakage during the deployment of SER models. To address these challenges, we propose a data distillation framework to facilitate efficient development of SER models in IoT applications using a synthesised, smaller, and distilled dataset. Our experiments demonstrate that the distilled dataset can be effectively utilised to train SER models with fixed initialisation, achieving performances comparable to those developed using the original full emotional speech dataset.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech, a distinctive human capability, facilitates the conveyance of a wide range of emotional nuances  [1]  (e. g., happiness, sadness, anger, fear, etc.). Identifying emotions from speech is a meaningful and challenging task  [2, 3] , as internal emotional states are often hidden in speech and difficult for humans to recognise  [4] . Over the past decades, speech emotion recognition (SER) aims to automatically identify emotional states from speech signals through signal processing and artificial intelligence approaches. An escalating number of human-computer interaction applications, such as intelligent call centres  [5]  and computer games  [6] , are evidencing a burgeoning need for SER.\n\nWith the advancement of computing resources, numerous deep learning (DL) methodologies have been successfully applied to the task of SER  [7] [8] [9] [10] , including convolutional neural networks (CNNs) with the input of time-frequency representations  [8, 9]  and end-to-end models (e. g., wav2vec 2.0  [11] , HuBERT  [12] , and WavLM  [13] ) fed with raw audio samples. Currently, the integration of DL techniques for SER into the Internet of Things (IoT) has spurred the development of numerous innovative personalised applications, such as smart homes  [14] , health states monitoring  [15] , and intelligent transportation systems  [16] . In this context, applying DL to IoT frameworks shows two major challenges. First, the constraints of DL models imposed by memory consumption continue to impede their advancement on edge devices, which often have limited onboard resources  [17] . Second, considering that speech data encompasses a wide array of personal information  [18] , recent research findings have demonstrated the potential vulnerability of DL algorithms to privacy breaches  [19, 20] .\n\nTo address the aforementioned two challenges, data distilla-tion emerges as a promising solution by extracting representative information to form a refined, smaller dataset from a larger one. Specifically, data distillation generates a compact dataset synthesised from the entirety of the original data, enabling that models trained on the smaller dataset perform on par with those on the complete one  [21] . Additionally, leveraging synthesised smallerscale datasets for subsequent model training further diminishes the likelihood of privacy leakage  [22] . Data distillation has emerged as a vibrant research area in the field of machine learning with diverse applications (e. g., image classification  [23] ).\n\nHowever, the application of data distillation remains limited in SER. To the best of our knowledge, this is the first attempt to construct a data distillation framework specifically for SER.\n\nIn this paper, we present a data distillation framework to synthesise a compact dataset for SER by constructing a similar model parameters with fewer iterations  [23] . The main contributions of this work can be summarised as follows: First, the proposed approach can effectively reduce the dataset size to approximately 15 % of the original one, conserving memory space while maintaining performance. Second, the required training iterations on the distilled dataset are less than those on the original data. Third, our framework has potential in protecting user identification against malicious exploitation in the SER domain. The code will be available online upon the paper's acceptance.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Our work addresses memory constraints on edge devices by reducing model parameters. For instance, model pruning can effectively reduce model size and computational complexity by strategically removing some model parameters  [24] . Parameter quantisation reduces the storage space of a model by decreasing the number of bits needed for each parameter  [25] . Model distillation  [26]  is a method that transfers knowledge from a complex teacher model to a lightweight student model, making it well-suited for edge computing and IoT applications  [26] . Compared to the aforementioned methods for model size reduction, data distillation entails transferring knowledge from a large-scale dataset to a smaller one, enabling models trained on the compact dataset to match the performance of those trained on the original dataset  [21] . Data distillation has been widely researched within ML (e. g., continual learning  [27] , federated learning  [28] , and neural architecture search  [29] ). Most existing research focuses on image datasets, like MNIST  [30]  and CIFAR  [31] . Studies  [32]  and  [33]  have sought to apply dataset distillation techniques in the domains of speech recognition and question-answering systems, respectively. Inspired by the these studies, data distillation  [23]  is investigated herein to reduce data size for SER in our work.\n\nTo protect users' privacy in the context of IoT, federated learning aims to keep users' data on local clients (i. e., edge devices) while only uploading model parameters to the cloud server. This approach helps to safeguard privacy by minimising exposure during client-server communication as well as in the cloud, though it requires the exchange of complex model parameters  [22] . Data distillation can protect users' privacy by generating dataset summaries from few number of data samples  [22] . Moreover, data distillation has been shown to be effective in differential privacy  [34] . Furthermore, integrating federated learning with data distillation, which involves uploading synthesized data rather than model parameters, not only reduces the communication load between clients and the server but has also been shown to outperform traditional federated learning with model synchronisation  [35, 36] .",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "Based on the original, complete and full SER dataset Dorg, dataset distillation in this work aims at generating a smaller but effective, synthetic training dataset Dsyn. As depicted as in Figure  1 , log Mel spectrograms extracted from the audio samples in Dorg are fed into a convolutional neural network (CNN) for emotion classification, where the snapshot of model parameters after each iteration are stored into a teacher trajectory. When conducting the dataset distillation, we first initialise the trainable Dsyn by randomly selecting a certain number of log Mel spectrograms per class (IPC) from Dorg. Second, a student trajectory is initialised with a randomly chosen iteration (i. e., i in Figure  1 ) from a teacher trajectory. Afterwards, the student trajectory is trained on the Dsyn for several iterations (i. e., Q in Figure  1 ). During the dataset distillation, the values of the log Mel spectrograms are updated according to the distance between θi+P and θQ, where P is a pre-defined number of epochs along the teacher trajectory. In this way, the knowledge of all emotional information in the full dataset is transferred from the teacher trajectory to the generated Dsyn. In Section 3.1 and 3.2, we introduce the SER models applied for detailed trajectories construction. In Section 3.3, the loss function in the training of Dsyn is described.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Teacher Trajectories",
      "text": "In this work, the trajectory means a sequence of a model's parameters. When training the CNN models on Dorg, we store the snapshot parameters of CNN models after each epoch as θi, where i represents the current iteration number. A teacher trajectory is defined as Θ = {θ1, θ2, . . . , θe}, where e denotes the number total iterations. In this work, we generate a set of teacher trajectories T = {Θ1, Θ2, . . . , Θt} for robustness.\n\nIn considering deployment on edge devices, log Mel spectrograms are preferred over raw audio samples due to their lower storage resource requirements  [37] . Furthermore, CNNs have demonstrated their proficiency in extracting effective representations from log Mel spectrograms  [38, 39] . Therefore, we evaluate the effectiveness of our proposed framework for SER using three classic CNN models: CNN-6, ResNet-9, and VGG-15. These CNN models are also suitable for deployment on devices with limited computing resources  [37] . The detailed model architectures are described in Section 4.2.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Student Trajectories",
      "text": "Similarly, a student trajectory denotes also a sequence of model's parameters trained on the Dsyn. For faster distillation process and robustness, at each distillation step, we first randomly choose one teacher trajectory Θj from T and then randomly chose the i-th snapshot of a model's parameters θi from Θj. However, due to the diminishing informational value (i. e., model parameter changes) in later iterations during the training, we set an upper bound e + for the value i in the θi for the student trajectory initialisation.\n\nAt each distillation step, following the student trajectory's initialisation, there are in total of Q numbers of updates based on the classification loss of the synthetic data. Specifically, considering the memory constraints, the synthetic dataset is partitioned into batches, and gradient descent updates are performed at each time according to the cross-entropy loss L ′ calculated from the current batch of synthetic data, indicated as follows: θn+1 = θn -α∇L ′ ((bn); θn),\n\nwhere n ∈ {0, 1, . . . , Q -1} indicates the current iteration number and bn is the sampled batch of synthetic data, and α is a trainable learning rate to automatically adjust the magnitude of iterations P and Q for the student trajectories at each distillation step. After a certain number of distillation steps, the final distilled data Dsyn is generated.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Training Of Distilled Data",
      "text": "After Q updates to the student trajectory, with the snapshot of parameters θi+P retrieved from Θ and the parameters θQ, the distilled log Mel spectrograms are updated according to the normalised squared l-norm distance as follows:\n\nwhere Q ≪ P , showing the efficiency of the distillation process. This loss function directly encourages a similar student trajectory θQ to the teacher trajectory θi+P trained on the Dorg in the parameter space with much less iteration steps. The normalisation in the above loss function ensures the continuity and effectiveness of the learning signal across different training stages by maintaining the relative scale of updates and preventing the potentially diminishing gradient issue, especially in the later training epochs  [23] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Experiments And Results",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Experimental Settings",
      "text": "Evaluation Metrics. The Unweighted Average Recall (UAR), alongside accuracy (i. e., weighted average recall), serves as the primary evaluation metric in alignment with previous studies on the DEMoS  [37, 42]  corpus, aiming to mitigate the issue of class imbalance. Models Architecture. The CNN-6 model consists of four convolutional layers with output channels of 64, 128, 256, and 512, each with a kernel size of 3 × 3. Each convolutional layer is followed by a local max pooling layer with a kernel size of 2 × 2. The VGG-15 model features five convolutional blocks: the first two blocks each contain two convolutional layers, while the latter three blocks each comprise three convolutional layers. The number of output channels in these blocks is set as 64, 128, 256, 512, and 512, with all convolutional layers utilising 3 × 3 kernels. Following each convolutional block, a max pooling layer with a 2 × 2 kernel is employed. The ResNet-9 model initiates with a convolutional layer of 64 output channels with a 7 × 7 kernel and a stride of 2, followed by a max pooling layer with a 3 × 3 kernel. Subsequent to this are three ResNet blocks with output channel numbers 128, 256, and 512. These blocks leverage 'shortcut connections' that combine identity mappings with the outputs from two consecutive 3 × 3 convolutional layers  [38] . For classification purposes, a global max pooling layer is utilised, followed by two fully connected (FC) layers to discern the contribution of each time-frequency bin. In this way, CNN-6, ResNet-9, and VGG-15 contain 4.44 M, 4.96 M, and 14.86 M parameters, respectively. Implementation Details. In this work, all audio samples in the DEMoS dataset are re-sampled into 16 kHz for faster processing. Moreover, we unify the duration of all audio samples to match that of the longest clip by trimming excessive portions and self-repeating segments of shorter ones. We extract log Mel spectrograms from DEMoS audio samples as features for models development. Specifically, the sliding window, overlap, and Mel bins are set as 512, 256, and 64 time frames, respectively. As a result, the obtained log Mel spectrograms are shaped as (373, 64), with the 373 representing the time axis and 64 describing the number of Mel frequency bins. The developing of the models employs an Adam optimiser with an initial learning rate of 1e-3 and stops after 50 epochs. The learning rate decreases by 30 % after every 5 epochs.\n\nIn our distillation setup, we define the number of teacher trajectories as NΘ = 5 to ensure robustness against random initialisation of the student trajectories. We empirically set the maximum starting iteration e + to 30 epochs when randomly selecting a parameter snapshot from Θ. For the initialisation of trainable distilled data, we sample {1, 5, 10, 50, 100, 150} log Mel spectrograms per class (IPC). The iteration number Q applied to the student trajectory at each distillation step is fixed at 20, while P is set to 3 epochs of iterations. The synthetic data is updated using an Stochastic Gradient Descent (SGD) optimiser with an initial learning rate set to 1000. Concurrently, the learning rate α, used in the gradient updates (as defined in Equation  1 ), starts at 1e -3 and is also adjusted using an SGD optimiser, with an initial learning rate of 1e -3. The l is set as 2 in Equation  2 . The entire distillation process is iterated for 60 cycles empirically. During the distillation process, the synthetic data is utilised every 10 distillation steps to develop three CNN models introduced in Section 3.1 from scratch, which are subsequently evaluated on the validation and test datasets of DEMoS. Given the reduced size of the synthetic dataset, the total number of training epochs is reduced to 30, while all other settings remain consistent with the development of the teacher trajectories.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results And Analysis",
      "text": "Models developed on full dataset. To ensure the efficacy of teachers networks in data distillation, we compare our performance with those of all state-of-the-art (SOTA) approaches on the DEMoS dataset, as shown in Table  2 . CNN-6 achieves best validation UAR and best test UAR. Moreover, our approach outperforms most prior works on DEMoS dataset. Please note that the data split in  [42]  is different from our work and the work in  [42]  applies a more advanced wav2vec 2.0 model. Models developed on the distilled dataset. Considering the memory constraints on IoT devices, we experiment with various sizes of synthetic datasets. While a higher IPC can enhance model performance, it also necessitates a larger dataset. Thus, we set the IPC values to a maximum of 150 to ensure the distilled  datasets remain sufficiently small. Specifically, certain numbers of Log Mel spectrograms per class (IPC) are randomly chosen from the real original dataset and updated throughout the distillation process, with IPC values set as {1, 5, 10, 50, 100, 150}. These IPC values represents 0.1 %, 0.5 %, 1.0 %, 5.0 %, 9.9 %, 14.9 % of the training plus validation data samples and 0.2 %, 0.9 %, 1.9 %, 9.4 %, 18.8 %, 28.2 % of the training samples.\n\nFor each IPC, we apply the DEMoS validation dataset to test the effectiveness of the distilled data after every 10 distillation steps, up to 100. Figure  2  presents the results when IP C = 150. First, the performance of CNN-6 remains stable across the distillation steps, regardless of whether the synthetic data is guided by CNN-6, ResNet-9, or VGG-15, which may be attributed to its relatively simpler architecture and the adequacy of 150 IPC for training CNN-6. Second, VGG-15 shows improved performance when trained on synthetic data distilled by VGG-15 itself, likely due to its more complex architecture. Third, as evidenced by Figure  2 , apart from the case with IPC = 100, we observe a distillation step of 60 appears optimal. Further distillation leads to no improvement or even a decrease in performance, possibly indicative of overfitting.\n\nAfter fixing the total number of distillation steps to 60, we examine the impact of IPC on the efficacy of the synthetic data used for training from scratch. In Figure  3 , there is a discernible trend showing that the performance of models trained on the synthetic distilled data improves with the increase of IPC. Notably, CNN-6 exhibits a more pronounced and rapid improvement, with considerable gains observed post an IPC threshold of 10. Upon reaching an IPC of 150, the performance of VGG-15 closely aligns with that of CNN-6 and their UARs exceeds 70 %, outperforming one SOTA work  [37]  in Table  2 . An IPC of 150 means we train the model with 28.2 % of the original validation dataset. A comparable performance on the distilled data with IPC= 150 has verified the effectiveness of the proposed approach, which is another reason we do not increase the IPC further.\n\nIn Table  3 , we present the model performances trained on distilled data guided by their respective model architectures as well as others. With IPC set at 150, the train dataset constitutes 28.2 % and the combined train plus validation set accounts for 14.9 % of the original corresponding dataset. On the validation dataset, CNN-6 achieves a UAR of 88.8 % when trained on the full train dataset. Interestingly, even when trained on merely 28.2 % of the data samples, its UAR remains above 72.0 %, peaking at 74.9 % when guided by its own architecture during the data distillation process. Similarly, ResNet-9 maintains a consistent validation UAR of around 40.0 %. VGG-15, on the other hand, exhibits more variation in validation UAR when distilled datasets are guided by different models, achieving its peak at 74.1 % UAR under the guidance of its own architecture. This performance significantly surpasses the work presented in  [37]  (p < 0.01 in a one-tailed z-test) and is comparable to ResNet-9's performance trained on the full train dataset. Moreover, when  trained on the distilled train plus validation dataset, representing a mere 14.9 % of the data samples, the test UAR experiences a decrease compared to the validation UAR. For instance, CNN-6, when trained with data distilled under ResNet-9's guidance, yields a highest test UAR of 67.1 %. Additionally, transferability of the distilled dataset has been observed. Specifically, CNN-6 and ResNet-9 perform similarly regardless of the guiding model for the distilled dataset, whereas VGG-15, when trained on data distilled by its own architectural style, outperforms the other models. This could be attributed to the inherent complexity and representational capacity of VGG-15, which may benefit more distinctly from data distilled within its architectural paradigm.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Conclusions And Future Work",
      "text": "In this work, we proposed a data distillation framework for Speech Emotion Recognition that leverages knowledge transfer from pre-trained teacher trajectories to generate a compact, synthetic dataset. Our experimental results demonstrated that the framework could achieve comparable performance to models trained on the full dataset, with the highest test unweighted average recall reaching 67.1 % using only 14.9 % of the data samples. Additionally, we observed notable transferability in the distilled dataset. In future work, we will investigate the application of more complex models (e. g., wav2vec 2.0) to guide the data distillation process. We also intend to conduct an in-depth analysis of the characteristics of the generated distilled data to understand its underpinnings and potential for SER.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of the proposed data distillation framework for SER. The blue arrows indicate the teacher networks’ training on the",
      "page": 2
    },
    {
      "caption": "Figure 1: , log Mel spectrograms extracted from the audio samples",
      "page": 2
    },
    {
      "caption": "Figure 1: ) from a teacher trajectory. Afterwards, the student",
      "page": 2
    },
    {
      "caption": "Figure 1: ). During the dataset distillation, the values of the",
      "page": 2
    },
    {
      "caption": "Figure 2: Comparison of the performance (UAR [%]) of the models on the DEMoS validation datasets, when IPC is set as 150.",
      "page": 4
    },
    {
      "caption": "Figure 2: presents the results when IPC = 150.",
      "page": 4
    },
    {
      "caption": "Figure 2: , apart from the case with IPC = 100, we observe a",
      "page": 4
    },
    {
      "caption": "Figure 3: , there is a discernible",
      "page": 4
    },
    {
      "caption": "Figure 3: Analysis of the image per class (IPC) on the DEMoS",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 2: Comparison of the performances [UAR %] on the",
      "data": [
        {
          "#": "Speaker",
          "Train\nVal\nTest": "27\n25\n16",
          "(cid:80)": "68",
          "F / M": "23 /45"
        },
        {
          "#": "Anger\nDisgust\nFear\nGuilt\nHappiness\nSadness\nSurprise",
          "Train\nVal\nTest": "586\n531\n360\n666\n608\n404\n461\n404\n291\n453\n395\n281\n561\n471\n363\n606\n543\n381\n396\n358\n246",
          "(cid:80)": "1,477\n1,678\n1,156\n1,129\n1,395\n1,530\n1,000",
          "F / M": "400 / 729\n596 / 1,082\n524 / 871\n415 / 741\n516 / 961\n349 / 651\n532 / 998"
        },
        {
          "#": "(cid:80)",
          "Train\nVal\nTest": "3,729\n3,310\n2,326",
          "(cid:80)": "9,365",
          "F / M": "3,332 / 6,033"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "A comprehensive review of speech emotion recognition systems",
      "authors": [
        "T Wani"
      ],
      "year": "2021",
      "venue": "IEEE access"
    },
    {
      "citation_id": "3",
      "title": "Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "4",
      "title": "STAA-Net: A Sparse and Transferable Adversarial Attack for Speech Emotion Recognition",
      "authors": [
        "Y Chang",
        "Z Ren",
        "Z Zhang",
        "X Jing",
        "K Qian",
        "X Shao",
        "B Hu",
        "T Schultz",
        "B Schuller"
      ],
      "year": "2024",
      "venue": "STAA-Net: A Sparse and Transferable Adversarial Attack for Speech Emotion Recognition"
    },
    {
      "citation_id": "5",
      "title": "Emotion recognition from speech: tools and challenges",
      "authors": [
        "A Al-Talabani",
        "H Sellahewa",
        "S Jassim"
      ],
      "year": "2015",
      "venue": "Proc. MMIPSA"
    },
    {
      "citation_id": "6",
      "title": "Emotion recognition method for call/contact centre systems",
      "authors": [
        "M Płaza",
        "R Kazała",
        "Z Koruba",
        "M Kozłowski",
        "M Lucińska",
        "K Sitek",
        "J Spyrka"
      ],
      "year": "2022",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "7",
      "title": "Audiovisual analysis for recognising frustration during Game-Play: Introducing the multimodal game frustration database",
      "authors": [
        "M Song",
        "Z Yang",
        "A Baird",
        "E Parada-Cabaleiro",
        "Z Zhang",
        "Z Zhao",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "Proc. ACII"
    },
    {
      "citation_id": "8",
      "title": "Speech emotion recognition using machine learning -A systematic review",
      "authors": [
        "S Madanian"
      ],
      "venue": "Speech emotion recognition using machine learning -A systematic review"
    },
    {
      "citation_id": "9",
      "title": "Speech emotion recognition using deep 1D & 2D CNN LSTM networks",
      "authors": [
        "J Zhao",
        "X Mao",
        "L Chen"
      ],
      "year": "2019",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "10",
      "title": "Speech emotion recognition from 3D log-Mel spectrograms with deep learning network",
      "authors": [
        "H Meng",
        "T Yan",
        "F Yuan",
        "H Wei"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "11",
      "title": "Dual-TBNet: Improving the robustness of speech features via dual-transformer-bilstm for speech emotion recognition",
      "authors": [
        "Z Liu",
        "X Kang",
        "F Ren"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "12",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Proc. NIPS"
    },
    {
      "citation_id": "13",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "14",
      "title": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "S Chen"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "Real-Time Speech Emotion Analysis for Smart Home Assistants",
      "authors": [
        "R Chatterjee",
        "S Mazumdar",
        "R Sherratt",
        "R Halder",
        "T Maitra",
        "D Giri"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Consumer Electronics"
    },
    {
      "citation_id": "16",
      "title": "Speech technology for healthcare: Opportunities, challenges, and state of the art",
      "authors": [
        "S Latif"
      ],
      "year": "2021",
      "venue": "IEEE Reviews in Biomedical Engineering"
    },
    {
      "citation_id": "17",
      "title": "Deep learning in the industrial internet of things: Potentials, challenges, and emerging applications",
      "authors": [
        "R Khalil",
        "N Saeed",
        "M Masood",
        "Y Fard",
        "M.-S Alouini",
        "T Al-Naffouri"
      ],
      "year": "2021",
      "venue": "IEEE Internet of Things Journal"
    },
    {
      "citation_id": "18",
      "title": "Deep learning on computational-resource-limited platforms: A survey",
      "authors": [
        "C Chen",
        "P Zhang",
        "H Zhang",
        "J Dai",
        "Y Yi",
        "H Zhang",
        "Y Zhang"
      ],
      "year": "2020",
      "venue": "Mobile Information Systems"
    },
    {
      "citation_id": "19",
      "title": "Speech emotion recognition approaches: A systematic review",
      "authors": [
        "A Hashem",
        "M Arif",
        "M Alghamdi"
      ],
      "venue": "Speech Communication"
    },
    {
      "citation_id": "20",
      "title": "A survey of machine and deep learning methods for internet of things (IoT) security",
      "authors": [
        "M Al-Garadi",
        "A Mohamed",
        "A Al-Ali",
        "X Du",
        "I Ali",
        "M Guizani"
      ],
      "year": "2020",
      "venue": "IEEE Communications Surveys & Tutorials"
    },
    {
      "citation_id": "21",
      "title": "Mobi-sage: A sparse additive generative model for mobile app recommendation",
      "authors": [
        "H Yin",
        "L Chen",
        "W Wang",
        "X Du",
        "Q Nguyen",
        "X Zhou"
      ],
      "year": "2017",
      "venue": "Proc. ICDE"
    },
    {
      "citation_id": "22",
      "title": "Dataset distillation",
      "authors": [
        "T Wang",
        "J.-Y Zhu",
        "A Torralba",
        "A Efros"
      ],
      "year": "2018",
      "venue": "Dataset distillation"
    },
    {
      "citation_id": "23",
      "title": "Data distillation: A survey",
      "authors": [
        "N Sachdeva",
        "J Mcauley"
      ],
      "venue": "Proc. TMLR, 2023"
    },
    {
      "citation_id": "24",
      "title": "Dataset distillation by matching training trajectories",
      "authors": [
        "G Cazenavette",
        "T Wang",
        "A Torralba",
        "A Efros",
        "J.-Y Zhu"
      ],
      "year": "2022",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "25",
      "title": "Learning both weights and connections for efficient neural network",
      "authors": [
        "S Han",
        "J Pool",
        "J Tran",
        "W Dally"
      ],
      "year": "2015",
      "venue": "Proc. NIPS"
    },
    {
      "citation_id": "26",
      "title": "A survey of quantization methods for efficient neural network inference",
      "authors": [
        "A Gholami",
        "S Kim",
        "Z Dong",
        "Z Yao",
        "M Mahoney",
        "K Keutzer"
      ],
      "year": "2022",
      "venue": "Proc. LPCV"
    },
    {
      "citation_id": "27",
      "title": "Distilling the knowledge in a neural network",
      "authors": [
        "G Hinton",
        "O Vinyals",
        "J Dean"
      ],
      "year": "2014",
      "venue": "Distilling the knowledge in a neural network"
    },
    {
      "citation_id": "28",
      "title": "Memory efficient data-free distillation for continual learning",
      "authors": [
        "X Li",
        "S Wang",
        "J Sun",
        "Z Xu"
      ],
      "year": "2023",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "29",
      "title": "Soft-label dataset distillation and text dataset distillation",
      "authors": [
        "I Sucholutsky",
        "M Schonlau"
      ],
      "venue": "Proc. IJCNN, 2021"
    },
    {
      "citation_id": "30",
      "title": "Dataset distillation with infinitely wide convolutional networks",
      "authors": [
        "T Nguyen",
        "R Novak",
        "L Xiao",
        "J Lee"
      ],
      "year": "2021",
      "venue": "Proc. NIPS"
    },
    {
      "citation_id": "31",
      "title": "The MNIST Database of Handwritten Digit Images for Machine Learning Research [Best of the Web]",
      "authors": [
        "L Deng"
      ],
      "year": "2012",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "32",
      "title": "Learning multiple layers of features from tiny images",
      "authors": [
        "A Krizhevsky",
        "G Hinton"
      ],
      "year": "2009",
      "venue": "Learning multiple layers of features from tiny images"
    },
    {
      "citation_id": "33",
      "title": "Speech disfluency detection with contextual representation and data distillation",
      "authors": [
        "P Mohapatra",
        "A Pandey",
        "B Islam",
        "Q Zhu"
      ],
      "year": "2022",
      "venue": "Proc. IASA"
    },
    {
      "citation_id": "34",
      "title": "Towards data distillation for end-to-end spoken conversational question answering",
      "authors": [
        "C You",
        "N Chen",
        "F Liu",
        "D Yang",
        "Y Zou"
      ],
      "venue": "Proc. ICLR, 2021"
    },
    {
      "citation_id": "35",
      "title": "Privacy for free: How does dataset condensation help privacy",
      "authors": [
        "T Dong",
        "B Zhao",
        "L Lyu"
      ],
      "year": "2022",
      "venue": "Proc. ICML"
    },
    {
      "citation_id": "36",
      "title": "Fedsynth: Gradient compression via synthetic data in federated learning",
      "authors": [
        "S Hu",
        "J Goetz",
        "K Malik",
        "H Zhan",
        "Z Liu",
        "Y Liu"
      ],
      "year": "2022",
      "venue": "Fedsynth: Gradient compression via synthetic data in federated learning"
    },
    {
      "citation_id": "37",
      "title": "Meta knowledge condensation for federated learning",
      "authors": [
        "P Liu",
        "X Yu",
        "J Zhou"
      ],
      "year": "2022",
      "venue": "Meta knowledge condensation for federated learning"
    },
    {
      "citation_id": "38",
      "title": "Knowledge transfer for on-device speech emotion recognition with neural structured learning",
      "authors": [
        "Y Chang",
        "Z Ren",
        "T Nguyen",
        "K Qian",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "39",
      "title": "Generating and protecting against adversarial attacks for deep speech-based emotion recognition models",
      "authors": [
        "Z Ren",
        "A Baird",
        "J Han",
        "Z Zhang",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "40",
      "title": "CovNet: A transfer learning framework for automatic COVID-19 detection from crowd-sourced cough sounds",
      "authors": [
        "Y Chang",
        "X Jing",
        "Z Ren",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "Frontiers in Digital Health"
    },
    {
      "citation_id": "41",
      "title": "DEMoS: An Italian emotional speech corpus",
      "authors": [
        "E Parada-Cabaleiro",
        "G Costantini",
        "A Batliner",
        "M Schmitt",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "DEMoS: An Italian emotional speech corpus"
    },
    {
      "citation_id": "42",
      "title": "Enhancing transferability of black-box adversarial attacks via lifelong learning for speech emotion recognition models",
      "authors": [
        "Z Ren",
        "J Han",
        "N Cummins",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "43",
      "title": "Fast yet effective speech emotion recognition with self-distillation",
      "authors": [
        "Z Ren",
        "T Nguyen",
        "Y Chang",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "Proc. ICASSP"
    }
  ]
}