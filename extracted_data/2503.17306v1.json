{
  "paper_id": "2503.17306v1",
  "title": "Exploring The Temporal Dynamics Of Facial Mimicry In Emotion Processing Using Action Units",
  "published": "2025-03-21T16:54:51Z",
  "authors": [
    "Meisam Jamshidi Seikavandi",
    "Jostein Fimland",
    "Maria Jung Barrett",
    "Paolo Burelli"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Facial mimicry-the automatic, unconscious imitation of others' expressions-is vital for emotional understanding. This study investigates how mimicry differs across emotions using Face Action Units from videos and participants' responses. Dynamic Time Warping quantified the temporal alignment between participants' and stimuli's facial expressions, revealing significant emotional variations. Post-hoc tests indicated greater mimicry for 'Fear' than 'Happy' and reduced mimicry for 'Anger' compared to 'Fear'. The mimicry correlations with personality traits like Extraversion and Agreeableness were significant, showcasing subtle yet meaningful connections. These findings suggest specific emotions evoke stronger mimicry, with personality traits playing a secondary role in emotional alignment. Notably, our results highlight how personality-linked mimicry mechanisms extend beyond interpersonal communication to affective computing applications, such as remote humanhuman interactions and human-virtual-agent scenarios. Insights from temporal facial mimicry-e.g., designing digital agents that adaptively mirror user expressions-enable developers to create empathetic, personalized systems, enhancing emotional resonance and user engagement.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Facial mimicry, the automatic imitation of others' expressions, plays a crucial role in empathy and social interaction  [15] ,  [10] . Often termed the 'social glue'  [24] , it fosters emotional connection and social cohesion. This subtle, unconscious reaction is central to emotion processing  [29] ,  [31]  and offers valuable insights for enhancing human-computer interaction  [23] .\n\nWhile facial expression research is extensive  [21] ,  [36] ,  [46] ,  [37] , the temporal dynamics of mimicry remain underexplored. Studies have primarily focused on static aspects, overlooking how expressions align and evolve over time-a key factor in emotional contagion, where individuals unconsciously synchronize expressions.\n\nPersonality traits like Extraversion and Agreeableness influence mimicry responses  [16] , yet their impact on temporal dynamics needs further study.\n\nTo bridge these gaps, this study examines the temporal dynamics of facial mimicry during emotion processing using Action Units (AUs)  [13] . Dynamic Time Warping (DTW) enables us to analyze temporal alignment, revealing nuanced patterns missed in static analyses  [33] .\n\nThese findings can inform affective computing systems-such as virtual agents or telepresence applications-by leveraging personality-linked mimicry patterns to mirror users' expressions in real time. This approach enhances emotional resonance and engagement, particularly in remote interactions without full physical presence.\n\nOur research questions are:\n\n1) How do personality traits influence the temporal dynamics of facial mimicry and emotion recognition? 2) What is the relationship between the temporal alignment of facial mimicry and the congruence between perceived and felt emotions? 3) How do different emotions affect the temporal patterns of facial mimicry and recognition accuracy?\n\nBy addressing these questions, we aim to advance the understanding of facial mimicry in emotion processing, with implications for social cognition, clinical interventions, and technology design.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Background",
      "text": "Facial mimicry plays a vital role in human communication as a nonverbal channel for emotional exchange. It involves the automatic imitation of others' facial expressions, enhancing emotion recognition and empathy  [29] ,  [31] ,  [2] . Studying facial mimicry provides insights into social cognition, affective processes, and neural mechanisms.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Facial Mimicry And Emotion Recognition",
      "text": "The matched motor hypothesis suggests that observing facial expressions activates corresponding motor representations, aiding emotion recognition through embodied simulation  [28] . This supports the action-perception loop, where perceiving expressions trigger motor responses that refine emotional understanding  [7] . Mimicry deficits are linked to impaired recognition, especially in negative moods  [15] ,  [7] . The facial feedback hypothesis proposes that facial actions amplify emotional experiences, forming a feedback loop between expression and emotion  [44] ,  [20] . Mimicry is largely automatic, integrating cues like voice and movement for complete emotional representation  [3] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "B. Neural Mechanisms Of Facial Mimicry",
      "text": "Facial mimicry involves the mirror neuron system, which activates during both observation and execution of expressions, facilitating emotional understanding  [18] . Brain imaging links emotional expression observation to empathy and emotion processing areas  [35] , though the underlying neural pathways remain partially understood. Personality traits from the Big Five Inventory-Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism-affect emotion recognition and mimicry. Extraverts, attuned to social cues, exhibit greater mimicry  [38] ,  [26] , while Neuroticism is associated with a bias toward negative emotions, impairing accuracy  [11] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "D. Action Units In Emotion Recognition",
      "text": "Facial mimicry influences emotional communication, with AUs from the Facial Action Coding System (FACS)  [13]  indicating facial expressions  [43] . AUs, which categorize muscle movements linked to emotions, are detectable via electromyography (EMG) or image-based techniques. They capture subtle changes, revealing micro-expressions that expose concealed emotions  [40] .Recent advances in AU detection, like Zhi et al.  [49] , highlight deep learning models. Automatic AU imitation enhances empathy and recognition  [17] . Research by Pfister et al.  [30]  and Song et al.  [39]  underscores micro-expression recognition's importance, with technological advances improving AU classification  [41] ,  [47] ,  [12] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "E. Dynamic Time Warping And Facial Expressions",
      "text": "DTW aligns time-dependent sequences, such as facial expressions and AUs. Raducanu and Dornaika  [32]  showed that dynamic recognition of facial expressions using DTW outperforms static image analysis. Khan et al.  [22]  applied DTW to measure AU similarities, enhancing emotion classification. Zhao et al.  [48]  optimized K-Nearest Neighbor classification with DTW, improving facial expression analysis. Yang et al.  [45]  demonstrated DTW's efficacy in emotion recognition, boosting classification accuracy.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "F. Research Gap",
      "text": "Despite extensive research on static mimicry, temporal dynamics remain underexplored. AUs provide a granular approach to mimicry analysis. This study bridges gaps by using AUs and DTW to examine the temporal alignment of expressions. We analyze AU divergence and its correlation with personality traits and discrepancies between felt and perceived emotions, offering deeper insights into individual differences in emotion recognition.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Methods",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Participants",
      "text": "We recruited 73 participants (52 males, 21 females; mean age 27.4 ± 6 years) with normal or corrected-to-normal vision and neurotypicality ( diagnosed with neurodivergent conditions such as autism, ADHD, or dyslexia). Informed consent was obtained per university ethical guidelines.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Experimental Design And Procedure",
      "text": "Participants completed 88 trials (4 practice, 84 main) using videos from the CREMA-D dataset  [6] , which features actors expressing six emotions at varying intensities.\n\nThe CREMA-D dataset provides demographic diversity with 91 actors (48 male, 43 female) aged 20-74, covering six basic emotions (Anger, Disgust, Fear, Happy, Neutral, Sad) at four intensity levels. We selected 84 clips representing diverse combinations to ensure a balanced emotional and demographic context, supporting generalizability.\n\nTo simulate face-to-face dialogue, a written scenario was provided before each video to enhance engagement and emotional alignment. Participants imagined conversing with the individuals in the videos, and responding to their emotions.\n\nFacial expressions were recorded at up to 40 fps using a webcam, synchronized with stimuli via Lab Streaming Layer (LSL is a framework for real-time data synchronization and integration across devices). AUs were extracted using OpenFace 2.0  [4] . Before trials, participants completed the Big Five Inventory (BFI-44)  [19]  to assess personality traits. After each video, they rated their perceived and felt emotions on 9-point Likert scales for valence and arousal.\n\nThe 9-point scale was chosen for its higher resolution and sensitivity in capturing subtle emotional variations, enhancing reliability in analyzing complex affective states  [25] ,  [5] .\n\nThe scales are shown in Figure  1 .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Data Preprocessing And Feature Extraction",
      "text": "Facial Action Units (AUs) were extracted from both participant and stimulus videos using OpenFace 2.0  [4] . This process captured 17 key AUs critical for analyzing facial mimicry and emotional expressions. These AUs, listed in Table  I , provide a comprehensive representation of facial muscle movements, enabling a detailed and nuanced evaluation of mimicry dynamics.\n\nDTW  [27]  was applied to align AU intensity sequences, quantifying temporal mimicry between participants and stimuli.\n\nDTW handles potential delays in mimicry reactions, capturing subtle dynamic patterns missed by static methods.  DTW distance, representing dissimilarity between participant and stimulus AUs, is calculated as:\n\nwhere A and B are AU intensity sequences, n is the length of A, ϕ is the warping function mapping indices of A to B, and |A(i) -B(ϕ(i))| is the absolute difference between the i-th element of A and the ϕ(i)-th element of B.\n\nMean and standard deviation of DTW distances were calculated for each trial to capture individual variability.\n\nThe Davies-Bouldin Index (DBI)  [9]  was used to evaluate the clustering quality of perceived emotions in the valencearousal space. Lower DBI values indicate better clustering, assessing the alignment of perceived and felt emotions.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iv. Statistical Analysis",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Variables And Measures",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Key Variables Analyzed:",
      "text": "• Sympathy: Euclidean distance between perceived and felt emotions in the valence-arousal space, indicating alignment.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Statistical Analyses",
      "text": "Pearson's correlation and multiple linear regression models were employed to analyze relationships between variables. Statistical significance was set at α = 0.05, and Bonferroni corrections were applied for multiple comparisons.\n\n1) Correlation Analysis: Pearson's correlations (r) were calculated between:\n\n• Sympathy and personality traits (E, C, N, O, A).\n\n• Sympathy and facial mimicry divergence (DTW scores).\n\n• Emotion recognition performance, DBI and facial mimicry divergence. • Relationships between personality traits and facial mimicry divergence. C. Results 1) Sympathy, Personality, and facial mimicry divergence: Table  II  shows which variables correlate significantly with sympathy. Pearson's correlation revealed significant positive correlations between sympathy and perceived emotion arousal (r = 0.24, p < 0.001), as well as facial mimicry divergence (r = 0.12, p = 0.001). Sympathy was negatively correlated with perceived emotion valence (r = -0.17, p < 0.001), indicating higher sympathy for more negative emotions. Positive correlations with personality traits were observed for Agreeableness (r = 0.10, p = 0.005) and Extraversion (r = 0.08, p = 0.032). 2) Emotion Recognition Performance and facial mimicry divergence: Correlation analyses (Table  III ) showed that DBI scores were negatively correlated with perceived emotion valence (r = -0.30, p < 0.001) and felt emotion valence (r = -0.25, p < 0.001), indicating better performance with more positive emotions. No significant correlation was found between DBI scores and facial mimicry (r = 0.05, p = 0.12).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Anova And Post-Hoc Tests",
      "text": "An ANOVA revealed significant effects of emotion labels on both DTW divergences (F (5, 357) = 7.12, p < 0.001) and DBI scores (F (5, 357) = 12.36, p < 0.001). Posthoc Tukey HSD tests (Table  IV  and Table V ) showed that 'Happy' emotions elicited higher DTW divergence compared to 'Fear' and lower divergence for 'Fear' compared to 'Anger' (Figure  3 ). Similarly, DBI scores indicated better emotion recognition for 'Happy' compared to negative emotions like 'Anger' and 'Disgust'. These findings highlight that positive emotions, such as 'Happy,' lead to stronger mimicry and better recognition performance compared to negative emotions.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "V. Discussion",
      "text": "This study provides insights into the dynamic aspects of facial mimicry and its links to personality traits and emotional alignment. Using DTW to analyze AU sequences, we identified temporal alignment patterns that were not evident in static analyses.\n\nThe positive correlation between facial mimicry divergence and sympathy support the facial feedback hypothesis  [44] , indicating closer mimicry enhances emotional alignment. This aligns with prior findings that mimicry boosts empathy and emotional understanding  [15] ,  [10] .\n\nPersonality traits like Extraversion and Agreeableness positively correlated with sympathy and mimicry divergence. Extraverts' stronger mimicry and emotion recognition likely stem from their social responsiveness  [26] ,  [38] . Similarly, Agreeableness, associated with empathy and prosocial behavior  [14] , showed positive links with mimicry and sympathy.\n\nNo significant relationship was found between Neuroticism and mimicry or emotion recognition, contrasting studies suggesting heightened sensitivity to negative stimuli  [11] , possibly due to limited Neuroticism variability or contextual factors.\n\nEmotion recognition was more accurate for positive emotions (e.g., happiness) than negative ones (e.g., fear, anger), supporting findings that positive emotions are easier to identify  [42] . The lack of a significant link between mimicry divergence and DBI scores suggests that mimicry enhances emotional alignment but may not universally improve emotion recognition across categories.\n\nBy integrating personality traits and temporal facial expression analysis, our study underscores the importance of temporal dynamics in emotion research  [34] .\n\nThese findings highlight mimicry's role in social cognition and empathy, with potential applications for improving emotion recognition in populations with deficits (e.g., autism spectrum disorders)  [1] . Considering personality differences could enhance social interaction models and inform the design of affective computing and social robotics systems for natural interactions  [8] .\n\nLimitations include a homogeneous sample, potential biases in self-reported emotions, and AU measure validity. The gender imbalance in our sample may affect generalizability, as gender influences empathic attitudes and mimicry  [16] . Future studies should include more balanced samples to explore gender differences in emotional processing. Further research should investigate diverse populations, incorporate physiological measures like EMG, and explore faster AU detection methods.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "We demonstrated that greater facial mimicry similarity is associated with higher sympathy, supporting the facial feedback hypothesis. Extraversion and Agreeableness positively correlated with sympathy and mimicry, highlighting individual differences.\n\nDespite modest variance explained, these findings advance understanding of mimicry's temporal dynamics in emotion processing.\n\nBy integrating dynamic analysis like DTW with detailed facial measures, we uncover intricate mechanisms underlying emotion recognition and social interaction. insights can guide the development of more empathetic and adaptive virtual agents, which could simulate facial mimicry to enhance human-agent interactions. .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Vii. Ethical Impact Statement",
      "text": "This research investigates the temporal dynamics of facial mimicry in emotion processing by analyzing AUs, which capture subtle facial muscle movements related to emotions. While AU analysis provides valuable insights into emotional alignment and social interaction, it also raises ethical considerations regarding privacy and the autonomy of participants.\n\nInformed consent was obtained from all participants, who were briefed on the study's objectives, data usage, and their right to withdraw at any time without consequences. To protect participant privacy, we extracted AUs from face videos and only used anonymized AU data for analysis. This approach preserves essential aspects of emotional expression while significantly reducing the sensitivity and identifiability of the data compared to raw video footage.\n\nAUs capture fine-grained facial movements, including subtle and often unconscious expressions known as microexpressions, which can involuntarily convey an individual's emotional state. Although these micro-expressions were not the primary focus of our analysis, their potential to reveal private emotional information underscores a need for caution in interpreting such data, particularly in applications where individuals cannot consciously control the emotional cues they display. Using mimicry for behavioral predictions, such as assessing emotional alignment or social intentions, raises concerns about potential overreach, misinterpretation, or misuse in sensitive contexts like surveillance or biased decisionmaking. Like any technology, facial mimicry analysis can drive positive innovation when used responsibly or pose risks if misused. We emphasize the importance of robust ethical guidelines to ensure transparent, fair, and responsible applications that prioritize individual rights and societal wellbeing.\n\nThe findings from this study have potential applications in enhancing human-computer interaction, social robotics, and clinical interventions. However, given the automatic and largely unconscious nature of many facial expressions, there is a risk of misuse in scenarios that might limit an individual's ability to manage the emotional signals they disclose. Additionally, automated frameworks like OpenFace, while powerful, may have inherent biases that could affect AU predictions across diverse demographic groups. Acknowledging these biases is crucial to ensure fairness and reliability in emotion recognition applications. Careful, ethically aligned deployment of these findings is therefore essential to uphold participants' rights to privacy and autonomy.\n\nWe have prioritized ethical data handling and anonymization protocols throughout this study, ensuring that the analysis of facial mimicry is conducted with respect for individual privacy and ethical standards. This commitment aligns with our objective to advance affective computing while maintaining a balanced approach to innovation and privacy protection.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The 9-point Likert scales for valence and arousal ratings.",
      "page": 2
    },
    {
      "caption": "Figure 2: Framework for similarity detection using DTW on AUs. DTW",
      "page": 3
    },
    {
      "caption": "Figure 3: Mean DTW Divergence Across Emotion Labels. The boxplot shows",
      "page": 4
    },
    {
      "caption": "Figure 3: ). Similarly, DBI scores indicated better",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "AU": "AU14",
          "Description": "Dimpler"
        },
        {
          "AU": "AU15",
          "Description": "Lip Corner Depressor"
        },
        {
          "AU": "AU17",
          "Description": "Chin Raiser"
        },
        {
          "AU": "AU20",
          "Description": "Lip Stretcher"
        },
        {
          "AU": "AU23",
          "Description": "Lip Tightener"
        },
        {
          "AU": "AU25",
          "Description": "Lips Part"
        },
        {
          "AU": "AU26",
          "Description": "Jaw Drop"
        },
        {
          "AU": "AU45",
          "Description": "Blink"
        },
        {
          "AU": "",
          "Description": ""
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion recognition in children with autism spectrum disorders: Relations to eye gaze and autonomic state",
      "authors": [
        "E Bal",
        "E Harden",
        "D Lamb",
        "A Van Hecke",
        "J Denver",
        "S Porges"
      ],
      "year": "2010",
      "venue": "Journal of Autism and Developmental Disorders"
    },
    {
      "citation_id": "2",
      "title": "Is empathy necessary to comprehend the emotional faces? the empathic effect on attentional mechanisms (eye movements), cortical correlates (n200 event-related potentials) and facial behaviour (electromyography) in face processing",
      "authors": [
        "M Balconi",
        "Y Canavesio"
      ],
      "year": "2016",
      "venue": "Cognition and Emotion"
    },
    {
      "citation_id": "3",
      "title": "Neuroscientific approaches to emotional expressions",
      "authors": [
        "M Balconi",
        "C Lucchiari"
      ],
      "year": "2007",
      "venue": "Journal of Psychology"
    },
    {
      "citation_id": "4",
      "title": "Openface: An open source facial behavior analysis toolkit",
      "authors": [
        "T Baltrušaitis",
        "P Robinson",
        "L.-P Morency"
      ],
      "year": "2016",
      "venue": "2016 IEEE Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "5",
      "title": "Improving the measurement of emotional responses with fine-grained likert scales",
      "authors": [
        "C Benitez-Quiroz",
        "R Wilbur",
        "A Martinez"
      ],
      "year": "2022",
      "venue": "Emotion Review"
    },
    {
      "citation_id": "6",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "7",
      "title": "The chameleon effect: The perceptionbehavior link and social interaction",
      "authors": [
        "T Chartrand",
        "J Bargh"
      ],
      "year": "1999",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "8",
      "title": "The acceptability of social robots: A scoping review of the recent literature",
      "authors": [
        "D David",
        "P Thérouanne",
        "I Milhabet"
      ],
      "year": "2022",
      "venue": "Computers in Human Behavior"
    },
    {
      "citation_id": "9",
      "title": "A cluster separation measure",
      "authors": [
        "D Davies",
        "D Bouldin"
      ],
      "year": "1979",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "10",
      "title": "Emotional empathy's role in facial mimicry: A review of current research",
      "year": "2000",
      "venue": "Journal of Nonverbal Behavior"
    },
    {
      "citation_id": "11",
      "title": "Investigating the relationship between facial mimicry and empathy",
      "authors": [
        "B Doe",
        "D Green",
        "E Parker"
      ],
      "year": "2019",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "12",
      "title": "Action unit classification for facial expression recognition using active learning and svm",
      "authors": [
        "E Edwards",
        "F Foster"
      ],
      "year": "2022",
      "venue": "Journal of Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "13",
      "title": "Facial Action Coding System: The Manual on CD ROM",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "2002",
      "venue": "Facial Action Coding System: The Manual on CD ROM"
    },
    {
      "citation_id": "14",
      "title": "Agreeableness, empathy, and helping: a person× situation perspective",
      "authors": [
        "W Graziano",
        "M Habashi",
        "B Sheese",
        "R Tobin"
      ],
      "year": "2007",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "15",
      "title": "Emotional contagion: A review of the literature on emotional mimicry",
      "authors": [
        "E Hatfield"
      ],
      "year": "1994",
      "venue": "Emotion"
    },
    {
      "citation_id": "16",
      "title": "Emotional mimicry as social regulation",
      "authors": [
        "U Hess",
        "A Fischer"
      ],
      "year": "2013",
      "venue": "Emotion Review"
    },
    {
      "citation_id": "17",
      "title": "Facial mimicry, empathy, and emotion recognition: a meta-analysis of correlations",
      "authors": [
        "A Holland",
        "G O'connell",
        "I Dziobek"
      ],
      "year": "2021",
      "venue": "Cognition and Emotion"
    },
    {
      "citation_id": "18",
      "title": "Grasping the intentions of others with one's own mirror neuron system",
      "authors": [
        "M Iacoboni"
      ],
      "year": "2005",
      "venue": "Proceedings of the National Academy of Sciences USA"
    },
    {
      "citation_id": "19",
      "title": "The big five trait taxonomy: History, measurement, and theoretical perspectives. Handbook of Personality: Theory and Research",
      "authors": [
        "O John",
        "S Srivastava"
      ],
      "year": "1999",
      "venue": "The big five trait taxonomy: History, measurement, and theoretical perspectives. Handbook of Personality: Theory and Research"
    },
    {
      "citation_id": "20",
      "title": "Social functions of emotions",
      "authors": [
        "D Keltner",
        "J Haidt"
      ],
      "year": "1999",
      "venue": "Emotion"
    },
    {
      "citation_id": "21",
      "title": "Emotion recognition using featurelevel fusion of facial expressions and body gestures",
      "authors": [
        "T Keshari",
        "S Palaniswamy"
      ],
      "year": "2019",
      "venue": "2019 international conference on communication and electronics systems (ICCES)"
    },
    {
      "citation_id": "22",
      "title": "Facial expression recognition based on facial action unit",
      "authors": [
        "Y Khan",
        "S Ahmad"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE International Conference on Image Processing (ICIP)"
    },
    {
      "citation_id": "23",
      "title": "Human-agent and human-robot interaction theory: Similarities to and differences from human-human interaction. Human-computer interaction: The agency perspective",
      "authors": [
        "N Krämer",
        "A Der Pütten",
        "S Eimler"
      ],
      "year": "2012",
      "venue": "Human-agent and human-robot interaction theory: Similarities to and differences from human-human interaction. Human-computer interaction: The agency perspective"
    },
    {
      "citation_id": "24",
      "title": "The chameleon effect as social glue: Evidence for the evolutionary significance of nonconscious mimicry",
      "authors": [
        "J Lakin",
        "V Jefferis",
        "C Cheng",
        "T Chartrand"
      ],
      "year": "2003",
      "venue": "Journal of nonverbal behavior"
    },
    {
      "citation_id": "25",
      "title": "Affective norms for english words (anew): Affective ratings of words and instructions for use",
      "authors": [
        "P Lang",
        "M Bradley",
        "B Cuthbert"
      ],
      "year": "2019",
      "venue": "Behavior Research Methods"
    },
    {
      "citation_id": "26",
      "title": "The five-factor theory of personality. Handbook of Personality: Theory and Research",
      "authors": [
        "R Mccrae",
        "P Costa"
      ],
      "year": "2008",
      "venue": "The five-factor theory of personality. Handbook of Personality: Theory and Research"
    },
    {
      "citation_id": "27",
      "title": "Information retrieval for music and motion",
      "authors": [
        "M Müller"
      ],
      "year": "2007",
      "venue": "Information retrieval for music and motion"
    },
    {
      "citation_id": "28",
      "title": "Embodied emotion perception",
      "year": "2006",
      "venue": "Psychological Science"
    },
    {
      "citation_id": "29",
      "title": "Face to face: Blocking facial mimicry can selectively impair recognition of emotional expressions",
      "authors": [
        "L Oberman",
        "P Winkielman",
        "V Ramachandran"
      ],
      "year": "2007",
      "venue": "Social neuroscience"
    },
    {
      "citation_id": "30",
      "title": "Recognizing spontaneous facial micro-expressions",
      "authors": [
        "T Pfister",
        "X Li",
        "G Zhao",
        "M Pietikäinen"
      ],
      "year": "2011",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "31",
      "title": "Mapping correspondence between facial mimicry and emotion recognition in healthy subjects",
      "authors": [
        "M Ponari",
        "M Conson",
        "N D'amico",
        "D Grossi",
        "L Trojano"
      ],
      "year": "2012",
      "venue": "Emotion"
    },
    {
      "citation_id": "32",
      "title": "Dynamic vs. static recognition of facial expressions",
      "authors": [
        "B Raducanu",
        "F Dornaika"
      ],
      "year": "2008",
      "venue": "Ambient Intelligence"
    },
    {
      "citation_id": "33",
      "title": "Facial expression recognition based on multi-view observations with dynamic time warping",
      "authors": [
        "B Raducanu"
      ],
      "year": "2014",
      "venue": "Computer Vision and Image Understanding"
    },
    {
      "citation_id": "34",
      "title": "Emotions are emergent processes: they require a dynamic computational architecture",
      "authors": [
        "K Scherer"
      ],
      "year": "1535",
      "venue": "Philosophical Transactions of the Royal Society B: Biological Sciences"
    },
    {
      "citation_id": "35",
      "title": "Mirror neuron system activity in response to emotional facial expressions: An fmri study",
      "authors": [
        "M Schulte-Rüther",
        "H Markowitsch",
        "N Shah",
        "G Fink"
      ],
      "year": "2008",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "36",
      "title": "Gaze reveals emotion perception: Insights from modelling naturalistic face viewing",
      "authors": [
        "M Seikavandi",
        "M Barret"
      ],
      "year": "2023",
      "venue": "2023 International Conference on Machine Learning and Applications (ICMLA)"
    },
    {
      "citation_id": "37",
      "title": "Modeling face emotion perception from naturalistic face viewing: Insights from fixational events and gaze strategies",
      "authors": [
        "M Seikavandi",
        "M Barrett",
        "P Burelli"
      ],
      "year": "2024",
      "venue": "Recent Advances in Deep Learning Applications: New Techniques and Practical Examples"
    },
    {
      "citation_id": "38",
      "title": "Exploring the effects of personality traits on the perception of emotions",
      "authors": [
        "A Smith",
        "B Johnson",
        "C Lee"
      ],
      "year": "2021",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "39",
      "title": "Micro-expression recognition model based on video modalities",
      "authors": [
        "Y Song"
      ],
      "year": "2022",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "40",
      "title": "Spontaneous facial expressions and micro-expressions coding: From brain to face",
      "authors": [
        "D Taylor",
        "M Evans",
        "S Carter"
      ],
      "year": "2021",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "41",
      "title": "Micro-expression recognition based on deep learning",
      "authors": [
        "Y Wang"
      ],
      "year": "2020",
      "venue": "Journal of Visual Communication and Image Representation"
    },
    {
      "citation_id": "42",
      "title": "The increasing instance of negative emotion reduce the performance of emotion recognition",
      "authors": [
        "X Wang",
        "S Zhao",
        "Y Pei",
        "Z Luo",
        "L Xie",
        "Y Yan",
        "E Yin"
      ],
      "year": "2023",
      "venue": "Frontiers in Human Neuroscience"
    },
    {
      "citation_id": "43",
      "title": "Measuring facial mimicry: Affdex vs. emg",
      "authors": [
        "J.-F Westermann",
        "R Schäfer",
        "M Nordmann",
        "P Richter",
        "T Müller",
        "M Franz"
      ],
      "year": "2024",
      "venue": "Plos one"
    },
    {
      "citation_id": "44",
      "title": "A multi-lab test of the facial feedback hypothesis by the many smiles collaboration",
      "authors": [
        "J Wheatley"
      ],
      "year": "2022",
      "venue": "Nature Human Behaviour"
    },
    {
      "citation_id": "45",
      "title": "Improving emotion recognition using k-nearest neighbor classification based on dtw",
      "authors": [
        "Y Yang"
      ],
      "year": "2020",
      "venue": "International Journal of Machine Learning and Computing"
    },
    {
      "citation_id": "46",
      "title": "Survey of deep emotion recognition in dynamic data using facial, speech, and textual cues",
      "authors": [
        "L Zhang",
        "M Wang"
      ],
      "year": "2023",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "47",
      "title": "Real-time microexpression recognition using 3d convolutional neural networks",
      "authors": [
        "Z Zhang",
        "Y Liu",
        "Y Wu",
        "C Xu",
        "S Wang"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Information Forensics and Security"
    },
    {
      "citation_id": "48",
      "title": "An optimization of the k-nearest neighbor using dynamic time warping as a measurement similarity for facial expressions recognition",
      "authors": [
        "Y Zhao"
      ],
      "year": "2018",
      "venue": "Proceedings of the ACM International Conference on Multimedia"
    },
    {
      "citation_id": "49",
      "title": "A comprehensive survey on automatic facial action unit analysis",
      "authors": [
        "R Zhi",
        "M Liu",
        "D Zhang"
      ],
      "year": "2020",
      "venue": "The Visual Computer"
    }
  ]
}