{
  "paper_id": "2202.08430v1",
  "title": "Emotion Recognition Among Couples: A Survey",
  "published": "2022-02-17T03:19:23Z",
  "authors": [
    "George Boateng",
    "Elgar Fleisch",
    "Tobias Kowatsch"
  ],
  "keywords": [
    "Couples",
    "emotion recognition",
    "affective computing",
    "literature survey Moffit Center Cancer Conversation Acoustic",
    "Lexical; Fusion: Feature level",
    "Decision level Acoustic: Prosodic and spectral eGeMAPS fea-turesLexical: Deep senstence embedding No No DNN LOCO CV UAR 3 Positive vs Negative vs Neutral: 57.42% Chakravarthula et al.",
    "2021 [20] UCLA / UW Couples Therapy Lexical N-gram",
    "ELMo No No ML",
    "GRU 88.9%",
    "Negative affect: 86.7%",
    "Sadness: 61.6%"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Couples' relationships affect the physical health and emotional well-being of partners. Automatically recognizing each partner's emotions could give a better understanding of their individual emotional well-being, enable interventions and provide clinical benefits. In the paper, we summarize and synthesize works that have focused on developing and evaluating systems to automatically recognize the emotions of each partner based on couples' interaction or conversation contexts. We identified 28 articles from IEEE, ACM, Web of Science, and Google Scholar that were published between 2010 and 2021. We detail the datasets, features, algorithms, evaluation, and results of each work as well as present main themes. We also discuss current challenges, research gaps and propose future research directions. In summary, most works have used audio data collected from the lab with annotations done by external experts and used supervised machine learning approaches for binary classification of positive and negative affect. Performance results leave room for improvement with significant research gaps such as no recognition using data from daily life. This survey will enable new researchers to get an overview of this field and eventually enable the development of emotion recognition systems to inform interventions to improve the emotional well-being of couples. CCS Concepts: • General and reference → Surveys and overviews; • Human-centered computing → Human computer interaction (HCI); • Applied computing → Psychology.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The emotions experienced by romantic partners are linked with relationship quality and the management of chronic diseases. Couples' emotions experienced during conflicts predict if these couples stay together in the long term (  [41] ).\n\nFor example, couples heading for break-up show more negative emotions and less positive emotions than happy couples  [19, 40] . For couples with one partner having a chronic disease, the burden of the disease management is shared by both partners and it takes a toll on the emotional well-being of not just the patient but also on the supporting partner. Furthermore, social support from partners in chronic disease management has been shown to either have positive or negative effects on the emotional well-being of patients  [15, 50, 76] . Because of the importance of emotions among couples, researchers are working towards understanding the emotional processes that take place in intimate relationships (e.g.,  [34, 90] ) and the link between emotions and social support in couples' dyadic management of chronic diseases  [64] . Consequently, being able to automatically recognize each partner's emotions could enable the research of social and health psychologists, and also inform the development of dyadic interventions (where partners are both involved e.g.,  [54] ) to improve the emotional well-being, relationship quality, and chronic disease management of couples.\n\nEmotion recognition among couples entails the recognition of the emotions of each romantic partner based on the context of their interaction. This task has a number of differences and similarities with other kinds of emotion recognition tasks. Standard emotion recognition attempts to recognize the emotion of each individual and uses various kinds of stimuli to induce an emotional reaction such as driving  [104] , listening to music or watching a movie  [1] , giving a speech  [85]  and engaging in a conversation with another person  [75] .\n\nAuthors' addresses: George Boateng, gboateng@ethz.ch, ETH Zürich, Zurich, Switzerland; Elgar Fleisch, efleisch@ethz.ch, ETH Zürich, Zurich, Switzerland and University of St. Gallen, St. Gallen, Switzerland; Tobias Kowatsch, tkowatsch@ethz.ch, ETH Zürich, Zurich, Switzerland and University of St. Gallen, St. Gallen, Switzerland. The stimuli used for couples' emotion recognition is a conversational context and hence, it has some similarities to emotion recognition tasks based on individuals having a conversation. The conversational context has the unique challenge of turn-taking dynamics which requires the system to correctly identify who is speaking for each speech segment, termed speaker diarization, consequently making this kind of emotion recognition task more challenging than others that employ stimuli such as listening to music or watching a video.\n\nMost emotion recognition tasks using conversational contexts tend to use actors acting out hypothetical scenarios  [17, 18, 66] . Actors tend to exaggerate their emotional expressions and the models trained with that type of data have been shown to perform better than models trained on real-world data  [30] . It is not clear if such emotion recognition systems will perform well on data from non-actors. On the other hand, some emotion recognition tasks employ people (e.g., 2 strangers) having real conversations  [43, 71] . This type of emotion recognition is the closest to couples' emotion recognition in terms of the conversational context and the dyadic and realistic nature of the interaction. However, for the couples' context, because the two individuals involved are in a romantic relationship, various insights from psychology research about couples can be leveraged to better recognize the emotion of each partner. For example, partners tend to influence each other's emotions throughout an interaction  [3] , and hence various interpersonal dynamics could be leveraged to adequately recognize each partner's emotions (e.g.,  [11, 70] ).\n\nBecause of the uniqueness of couples' emotion recognition and the potential clinical utility, there is a need to synthesize emotion recognition approaches focused on the couples' context. Several works have developed systems to automatically recognize the emotions of couples. In this paper, we describe and discuss these works and give a comprehensive overview of this research field. We surveyed 28 articles published over the past 11 years (2010 -2021) given the first set of works on this topic were published in 2010. We detail the datasets, features, algorithms, evaluation, and results of each work as well as present main themes. We also discuss current challenges, research gaps and propose future research directions. There are surveys of emotion recognition works focused on specific modalities such as visual and speech modalities (  [103] ), wearables (  [86] ), multimodality (  [30, 74] ) and contexts such as driving (  [104] ) and conversation (  [75] ). However, this is the first survey of works that focus on emotion recognition within the context of couples' interactions or conversations.\n\nThe rest of this paper is organized as follows. In Section 2, we describe the scope of the survey and the approach used to select the papers. In Section 3, we give an overview of the surveyed works. In Section 4, we describe emotion models, elicitation, and annotation approaches that have been used. In Section 5, we give an overview of the datasets that have been used in the surveyed works. In Section 6, we describe the modalities used, how they have been preprocessed, and the features extracted from them. In Section 7, we discuss the algorithms that have been used, modeling approaches considering the unique context of couples' interactions, evaluation approaches, and the results obtained. In Section 8, we discuss research gaps, challenges, and future directions. We conclude in Section 9.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Survey Scope And Methodology",
      "text": "Our methodology is similar to those of Zepf et al.  [104] . We sought to survey papers that used data to automatically recognize the emotions of each romantic partner based on the couple's interaction or conversation context. We developed a list of search terms that covered three concepts: (1) emotions/emotional behavior (emotion, affect, affective, moods, behavior), (2) recognition (recognition, prediction, classification, behavior signal processing, affective computing, machine learning, deep learning, neural networks) and (3) couples (couples, dyad, spouse, married). We entered the search terms into the following databases: IEEE, ACM, Web of Science, and Google Scholar. We also looked through the references of relevant papers.\n\nTo be included, the papers had to perform automatic recognition of each partner's emotions (e.g., positive or negative valence), emotional behavior or emotional states (e.g., positive affect/positivity, negative affect/negativity, sadness), employed statistical or machine learning approaches, and used data collected from the context of couples' interaction or conversations. We included papers that were not peer-reviewed yet but were on archival databases such as arXiv for completeness.\n\nWe excluded papers that used data from interacting partners that are not real couples, i.e., individuals acting out dyadic interactions either using a script or engaging in spontaneous sessions such as the following datasets  [17, 18, 66, 69] . We also excluded papers that recognized couple behavior which are not emotional states such as level of blame  [7] , conflict  [92] , suicidal risk  [23] . In particular, we excluded one paper  [2]  that focused on recognizing stressful conversations from other kinds of stressful situations among couples. We also excluded papers that recognized couples' relationship state (e.g., happy vs sad couple)  [97]  rather than each partner's emotions. Another related paper that we excluded is  [9]  which proposed a research plan for emotion recognition but does not present performed analysis and results in the paper. After using these inclusion and exclusion criteria, we had 28 relevant articles published between 2010 and 2021 (Table  2 ).",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Overview Of Works",
      "text": "Out of the 28 surveyed papers, a majority of the works (n=24) have been done primarily by the Signal Analysis and Interpretation Laboratory (SAIL) team at the University of Southern California which published the first set of works on this topic in 2010  [6, 56] . Subsequent works extended or built upon previous works from the research group. Few works have been done by researchers outside this research lab and include the following  [5, 11, 13, 26] .\n\nTogether, these works have used 5 datasets that were collected from a laboratory setting (Table  1 ). Most of these works used emotion labels from external raters with few using self-reported data from the couples themselves. Only three modalities have been used -acoustic, lexical, and visual -with acoustic being the most used modality. Multimodal fusion has been done mostly for acoustic and lexical modalities using feature-level and decision-level fusion. Support vector machines are the most used algorithm. Various intrapersonal considerations (e.g., saliency) and interpersonal considerations (e.g., synchrony) have been leveraged (details in a future section). Evaluations have mostly been done with leave-one-couple-out cross-validation with accuracy being used as the metric.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Background",
      "text": "In this section, we describe various emotion models and approaches to eliciting and annotating emotion data from couples.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Emotion Models",
      "text": "There are mainly two models of emotions used in the literature in emotion recognition: categorical and dimensional.\n\nCategorical emotions are based on the six basic emotions proposed by Ekman: happiness, sadness, fear, anger, disgust, and surprise  [31] . Over time, additional emotion categories have been included and used in literature such as anxiety, frustration, etc. Dimensional approaches mainly use two dimensions: valence (pleasure) and arousal (activation) which are based on Russell's circumplex model of emotions  [82] . Valence refers to how negative to positive the person feels and arousal refers to how sleepy to active a person feels. Using these two dimensions, several categorical emotions can be placed and grouped into the four quadrants: high arousal and negative valence (e.g., stressed, angry), low arousal and negative valence (e.g., depressed), low arousal and positive valence (e.g., relaxed), and high arousal and positive valence (e.g., excited).",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Elicitation",
      "text": "Approaches for eliciting emotions in couples have generally happened in the lab/controlled settings and in daily life.\n\nIn the lab, couples are asked to have emotionally charged conversations that are videotaped  [80] . Some of these conversations center on topics that cause distress in their relationship. These conversations elicit emotions during and after the conversations which are then annotated. In daily life, sensor data (e.g., audio) is collected from couples periodically  [79]  or when conversation moments are detected  [9] .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Annotation",
      "text": "Two approaches are used for emotion annotations by social psychologists: self-report and observer reports, and two scales: global and local (continuous, utterance-level). For self-reports, each partner provides emotion ratings right after the whole interaction/conversation (global/session ratings) with validated instruments such as the Affect Grid questionnaire  [83]  and Multidimensional Mood questionnaire  [91]  or they are asked to watch a video recording of the conversation while providing continuous (moment-by-moment) emotion ratings using a joystick (e.g.,  [80] ). In the case of daily life, couples are periodically asked to complete self-reports such as the PANAS  [100] , Affect Grid questionnaire  [83]  and Affective Slider  [4]  at random time  [9, 87, 89]  or after sensor data recording  [9] . Additionally, the dyadic nature of couples' interactions enables the collection of partner-perceived emotions where each partner (e.g., partner A) is asked to provide a rating of their perception of the emotion of their partner (e.g., partner B) emotion after an interaction in the lab (e.g., sels2019a, sels2019b) or in daily life (e.g., sels2020). All these types of self-reports enable the collection of the subjective and perceived emotions of each partner. However, these ratings could be biased and may not reflect each partner's actual emotions about the interaction.\n\nFor observer reports, people are trained to watch the video recordings (e.g., in the case of lab data) and use a coding scheme to rate the interaction on specific emotional behaviors (e.g., SPAFF  [25] ) using continuous or utterance-level ratings (e.g., every 10 seconds or speaker turn) or global ratings (of the whole interaction). Such coding is also done for example, for audio data collected from couples' daily life interactions  [79] . This manual coding process is costly and time-consuming as multiple coders need to be trained for this task  [53]  and suffers from inter-rater reliability issues  [47, 67] . Furthermore, these ratings reflect the observers' perceived emotions of the partners and they do not represent the subjective emotions of the partners.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Studies And Datasets",
      "text": "In this section, we describe all the datasets that have been used in the surveyed emotion recognition works, how they were collected and annotated. The surveyed papers used five (5) datasets, all of which were collected in the lab. Three\n\n(3) were observer annotated, one (1) was self annotated, and one (1) had both self and observer annotations (Table  1 ). The distribution of papers that have used the five datasets is as follows: UCLA/UW Couples Therapy  (23) , Cancer\n\nConversation (1), KU Leuven Dyadic Interaction (1), Stanford Psychotherapy (1), and UZH Couples' Interactions (2).",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Ucla/Uw Couples Therapy",
      "text": "Researchers conducted a longitudinal lab study at the University of California, Los Angeles, and the University of Washington in the U.S. with 134 seriously and chronically distressed heterosexual married couples  [6] . Their age statistics were as follows: the range is 22 to 72 years, the median age for men was 43 years (SD = 8.8), and the median age for women was 42 years (SD = 8.7). They were, on average, college-educated (median level of education for both men and women was 17 years, SD = 3.2). The sample was largely Caucasian (77%), with 8% African American, 5% Asian or Pacific Islander, 5% Latino/Latina, 1% Native American, and 4% Other. Couples were married for an average of 10.0 years (SD = 7.7)  [24] .\n\nCouples received couples therapy for 1 year. They had conversations and discussed a problem in their relationship with no therapist or research staff present. They discussed the wife's chosen topic for 10 minutes and the husband's chosen topic for 10 minutes which were considered separate sessions. The sessions were recorded at three points in time: before the therapy, 26 weeks into it, and two years after the therapy sessions ended. There were 96 hours of data across 574 sessions. The sessions were videotaped and later transcribed and annotated by 3-4 trained coders. The annotators assigned 33 session-level (global) behavioral codes for each spouse on a scale of 1 -9 using two coding schemes. The coding schemes are the Social Support Interaction Rating System (SSIRS) which consists of 20 codes that measure the emotional component of the interaction and the topic of conversation  [51]  and the Couples Interaction\n\nRating System 2 (CIRS2) which consist of 13 codes and were specifically designed for conversations involving a problem in a relationship  [44] . There were no local (utterance-or speaker-turn-level) annotations.\n\nAuthors that used this dataset for emotion recognition tasks used only six codes in experiments due to low interevaluator agreement for the other codes. The codes that were used are level of blame, level of acceptance towards the other spouse, global positive affect, global negative affect, level of sadness, use of humor. They used the manual transcript of the data to automatically create word and speaker turn alignments which resulted in a smaller number of sessions and unique couples data used for the recognition experiments: 293 sessions. Also, they computed the mean values across raters and then selected data whose ratings were in the top 20% and bottom 20% for each of the codes.\n\nConsequently, the data used for analysis was from 60 -85 unique husband/wife pairs. The task was cast as a binary classification for the two extremes for each code. In this survey, we consider works that used the affect-related codes:\n\nglobal positive affect, global negative affect, level of sadness, anxiety, and anger.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Moffit Center Cancer Conversation",
      "text": "Researchers conducted a study in which they collected data from 85 couples in the U.S. who were coping with advanced cancer (one spouse having cancer and the other being a caregiver)  [22, 77, 78] . Here is the demographic data of the 82 couples whose data was eventually used: 29.3% of patients were female and 70.1% of caregivers were female, the mean age for patients was 66.8 years (SD = 9.2), and the mean age for caregivers was 64.8 years (SD = 9.4). On average, they had college or vocational education. The patient sample was largely Caucasian (92.7%), with 6.1% African American, 3.7% Latino/Latina, 1.2% Native American, and 0% Other. The caregiver sample was also largely Caucasian (90.2%), with 4.9% African American, 3.7% Latino/Latina, 2.4% Native American, and 1.2% Other. Couples were married for an average of 35 years (SD = 15.8)  [77] .\n\nThey engaged in a 10-minute neutral discussion (daily routine) and a 10-minute stressor discussion about an issue related to cancer management in controlled settings (e.g., clinic consult rooms, participant homes) with an experimenter present without facilitating. The issue was decided based on their ratings on Cancer Inventory of Problem Situations,  [46]  in which a list of 20 common cancer concerns (e.g., lack of energy, finances, over-protection) are rated as being not a problem, somewhat of a problem, or a severe problem. The interactions were audio-recorded. We estimated that a total of 27 hours of data was collected.\n\nThe audio was annotated on an utterance / speaker-turn level by 2 trained coders using the Rapid Marital Interaction\n\nCoding System, 2nd Edition  [48, 49]  with inter-rater reliability scores of Kappas above 0.88 for 20% of all codes). Each utterance was assigned one behavioral code out of 7 codes which were then grouped into three: positive (low and high positive), hostile/negative (low and high hostile), neutral/constructive (constructive problem discussion). Additional codes were dysphoric affect and other which were not used for the recognition task. Hence, the task was framed as a 3-class classification problem. They used the manual transcripts to automatically create word, speaker turn and label alignments. This dataset has been used by  [22] .",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Ku Leuven Dyadic Interaction",
      "text": "Researchers conducted a Dyadic Interaction lab study in Leuven, Belgium with 101 heterosexual, Dutch-speaking couples  [13, 87, 88] . The majority (n=96) cohabited and 7 were married. The average age was 26 years (SD=5), ranging from 18 to 53 years. The partners were together for 4.5 years (SD=2.8), ranging from 7 months to 21 years. There was no information about the ethnicity and education levels of participants.\n\nThese couples were first asked to have a neutral 10-minute conversation, then a 10-minute conversation about a negative topic (a characteristic of their partner that annoys them the most), followed by a 10-minute conversation about a positive topic (a characteristic of their partner that they value the most)  [27, 87, 88] .\n\nAfter each conversation, each partner completed self-reports on various emotion labels such as anger, sadness, anxiety, relaxation, and happiness using a 7-point Likert scale ranging from strongly disagree (1) to strongly agree  (7) .\n\nAdditionally, each partner watched the video recording of the conversation separately on a computer and rated his or her emotion on a moment-by-moment basis by continuously adjusting a joystick to the left (very negative) and the right (very positive), so that it closely matched their feelings, resulting in valence scores on a continuous scale from -1 to 1  [42, 81] . Additionally, each partner reported how they felt after the interaction and how they thought their partner felt, using the Affect Grid questionnaire  [83]  which captures the valence and arousal dimensions of Russell's circumplex model of emotions  [82]  resulting in values between 0 and 8 each for pleasure and arousal.\n\nTrained research assistants (5) listened and visually inspected the audios, and annotated the exact start and end of each talking turn for each partner. Authors that used this data categorized the valence scores into two classes, negative (0-4) and positive valence (5-8) for males and females, consequently framing the task as a binary classification task.\n\nThis dataset was used by  [13]",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Stanford Psychotherapy",
      "text": "Researchers collected audio, and video data from 3 heterosexual couples at Stanford University in the U.S. over 18hour-long couple therapy sessions (18 sessions, 1 hour each for each couple A, B, and C) undergoing therapy over a period of 2 years  [26] . We estimated that a total of 18 hours of data was collected. No demographic information about the couples was available.\n\nThe audio was first transcribed manually, including the start and end times of each word. Then trained coders watched the video and used the transcript to code the data by marking start and end times of any of the following 4 emotions: anger, sadness, joy, tension, and neutral (defined as the absence of the 4 emotions). Each label was given a rating of low, medium, or high but the levels were not used in the analysis. These codes were adapted from Gottman's 19 SPAFF affective codes  [25] . To assign labels, annotators had to mark the start and end times of the occurrence of one of the emotions. Hence, no two emotion-labeled segments could overlap. Only audio data was used for recognition which was framed as a 5-class classification task. This dataset has been used by the work  [26] .",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Uzh Couples Interactions",
      "text": "Researchers collected data from 368 heterosexual German-speaking, Swiss couples (N=736 participants; age 20-80) at the University of Zurich, Switzerland over 10 years  [55, 98] . The longitudinal study sought to investigate the impact of stress on the relationship development of couples and children across their lifespan. The average age was 47 (SD = 18.4)\n\nfor women and 49 (SD = 18.2) for men, the mean relationship duration was 21 years (SD = 17.9) with 66% being married.\n\nFor women, 6% attended the mandatory school years (9 years), 40% completed vocational training, 21% completed high school, and 32% completed college or university. For men, 3% attended the mandatory school years, 35% completed vocational training, 12% completed high school, and 49% completed an academic degree.\n\nCouples participated in three videotaped conversations in the lab each for 8 minutes -one conflict and two mutual support conversations from years 1 to 6 (one session per year). Video-recorded data from 3 couples were not available resulting in 365 couples' data. The number of couples that took part reduced over the years with the details available at  [98] . Based on the data collected over the years, our estimate of the total amount of data is 637 hours.\n\nFor the conflict interaction which was used for emotion recognition, couples had to choose one problematic topic for the conflict interaction from a list of common problems (PAQ A;  [45] ), and participants were then videotaped as they discussed the selected issue for 8 minutes. After each conversation, each partner provided self-report responses to the Multidimensional Mood questionnaire  [91]  of their emotions on four bipolar dimensions -namely \"good mood versus bad mood, \" \"relaxed versus angry, \" \"happy versus sad\" and \"calm versus stressed\" -with the scale: 1 -very much, 2 -much, 3 -a little, 4 -a little, 5 -much, 6 -very much. The authors that used the dataset preprocessed these responses by averaging the \"good mood versus bad mood\" and \"happy versus sad\" scales and then binarized the averaged values such that values greater than or equal to 3.5 were negative (0) and the rest were positive (1).\n\nAdditionally, two research assistants were trained to code communication behaviors (interobserver agreement, k = 0.9) using an adapted version of the Specific Affect Coding System (SPAFF)  [25] . The most prevalent code from the list was assigned every 10 seconds resulting in 48 sequences for each interaction. The codes were then grouped into positive and negative for emotion recognition.\n\nThe speech was manually annotated with the start and end of each speaker's turn, along with pauses and noise. The speech was manually transcribed in 15-second chunks separately for each partner. Given that Swiss German is mostly spoken with different dialects across Switzerland, the spoken words were written as the corresponding German word equivalent. The following two works -  [11]  and  [5]  -used this data to recognize global and local emotions respectively, all framed as binary classification tasks.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Modalities, Data Preprocessing And Feature Extraction",
      "text": "In this section, we describe the modalities of the data used for emotion recognition in the surveyed works along with preprocessing approaches and features that were extracted from each modality. The surveyed papers used three distinct modalities with acoustic being the most represented modality: acoustic  (19) , lexical  (9) , and visual (2).",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Acoustic",
      "text": "Given that the audio data is collected in the context of conversations, it is generally annotated manually or automatically with the segments that contain speech vs no speech (voice activity detection), and additionally, those segments are annotated to correspond to the speech of each partner, which is known as speaker diarization.\n\nNext, various features are extracted either using feature engineering or transfer learning. Feature engineering entails using handcrafted features that have been shown to be discriminative for the recognition task. For feature engineering, standard acoustics features such as prosodic (e.g., pitch, energy, speaking rate), spectral (e.g., mel frequency cepstral coefficients) and voice quality (shimmer and jitter) are extracted over frames of short durations (e.g., 25 ms) known as low-level descriptors (LLDs) using a sliding window (e.g., 10 ms) which may or may not be overlapping  [5, 6, 8, 11, 13, 22, 26, 38, 39, 52, 56-59, 61-63, 96, 101] . Various statistics called functionals (e.g., mean, median, percentiles, etc) are computed over these frames to get features for a segment (e.g., 2 seconds) or the whole audio (8-10 mins). In particular, a set of 88 features called eGeMAPS  [32]  have been shown to be a minimalist feature set that is effective for emotion recognition tasks and have been used in the following works  [5, 11, 22] . The openSMILE toolkit  [33]  has been mostly used for acoustic feature extraction. Other tools such as Praat  [14]  have been used. Due to the likelihood of having a lot of features, various features selection methods such as forward feature selection has been used  [6] . Additionally, various approaches have been used to remove the speaker, microphone, and environmental variability of the audio signal by performing mean normalizing of the LLDs for the whole session audio (e.g.,  [8] ).\n\nTransfer learning is an approach used to circumvent the need to develop hand-crafted features and entails using a model pre-trained on a different but related task (  [35] ). This process entails using the model for feature extraction or fine-tuning in which the whole model or later layers are retrained. For example, Boateng et al. took a CNN model called the YAMNET that was pretrained on an audio event classification task and used it to extract feature embeddings from spectrograms over 1-second time windows (  [13] ). Also, Li et al pretrained a deep learning model (CNN) on an emotion recognition task that used acted data and then used the model to extract acoustic embeddings for the recognition task  [63] .",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Lexical",
      "text": "The audio is generally transcribed automatically or manually in order to use the content of the speech for emotion recognition. Various linguistic features ranging from simple features (bag of words and TF-IDF), dictionary-based features used in psychology such as LIWC  [72] , more advanced ones such as word embeddings (word2vec  [68]  and ELMo  [73] ) to deep learning models such as BERT  [28]  which are currently the state-of-the-art for computing linguistic features.\n\nHere are examples of works that have used those lexical features: bag-of-words (unigram  [21, 37] , ngram  [5, 20, 70] ), TF-IDF  [39, 52, 60] , LIWC  [5] , word embeddings (word2vec  [94, 95] , ELMo  [20] ), deep sentence embeddings (seq-to-seq models  [22, 93, 94, 96] , BERT and Sentence-BERT  [5, 11] ). Transfer learning has also been used for the lexical data. For example, various sentence embeddings have been computed using pretrained models  [5, 11, 22, 93, 94, 96] .",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Visual",
      "text": "Few works used the visual modality and in particular head movements in the videos that were recorded. The following features have been extracted: line spectral frequencies/power spectral density of head motion vectors to capture the vertical and horizontal directions of the head motion  [39, 102] . Facial expressions were not used in those works because the quality of the video was not good enough to compute features from the face (e.g., varying sitting positions, camera distance/angle, and lighting conditions)  [39] .",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Data Analysis And Evaluation",
      "text": "In this section, we describe various algorithms that have been used for emotion recognition, multimodal fusion approaches, intrapersonal and interpersonal considerations, evaluation, and results.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Algorithms",
      "text": "The surveyed works used mostly supervised learning approaches with a few using semi-supervised  [21, 38, 39, 52, 59, 60, 101]  and unsupervised learning  [62, 93] . The algorithms used range from simple statistical algorithms and traditional machine learning to deep learning methods. Support vector machines (SVM) have been the most used algorithm.\n\nHere are the algorithms used by various works: SVM  [5, 6, 8, 11, 13, 38, 39, 52, 58, 61, 94, 95, 101, 102] , linear discriminant analysis (LDA)  [6, 101] , markov models  [21, 56, 57, 70, 101] , multiple instance learning (diversity density  [39, 59, 60] , diversity density SVM  [38, 52] ), maximum likelihood  [20, 21, 37, 70] , sequential probability ratio test  [60] ,\n\nlogistic regression  [8] , perceptron  [101] , gaussian mixture model (GMM)  [102] , deep neural networks  [22, 61, 62, 96] ,\n\nLSTM  [93] [94] [95] [96] , GRU  [20, 63] , random forest  [11, 26] , CNN  [63] .",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Multimodal Fusion",
      "text": "Modalities that have been combined include acoustic and lexical data  [11, 22, 52, 96] , and acoustic, lexical, and visual  [39] . Various fusion methods have been used such as feature-level fusion in which the features of each modality are concatenated and decision-level fusion in which each modality is trained with a separate model and the predictions from the models are combined using various approaches such as majority vote. The following papers used feature-level fusion  [11, 22, 39, 96]  and decision-level fusion  [22, 39, 96] . Additionally, knowledge-driven expert fusion approaches have been explored by Tseng et al. as follows: gender-based and therapy-stage fusion  [96] .",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Intrapersonal Considerations",
      "text": "One challenge with recognizing global emotion labels -one emotion label for a long interaction duration such as 8-10 minutes -is that there is a whole range of emotions experienced and expressed throughout the interaction with different intensities. One naive approach to address this challenge is to assign every segment (e.g., 2 seconds) with the label of the whole audio and then train the model with this modified data-label pairings. This approach is used in various fields such as physical activity recognition  [10]  since an activity label (e.g., walking) is consistent over different segments. However, such an approach is error-prone for the context of emotion recognition as it erroneously assumes that the emotion label is the same for all segments. The standard approach used in various works is to compute statistics such as mean, median, etc., over the features that have been computed over short windows as previously seen in the approach used for extracting acoustic features.\n\nHowever, the emotion recognition task may benefit from more creative modeling approaches. One such approach relates to the concept of saliency. Some interaction segments might be more salient for recognizing that one label assigned to the whole audio. Some works have leveraged some methods to identify those salient segments. One such saliency-based method is multiple instance learning which automatically identifies salient instances from a bag of instances in a semi-supervised learning fashion  [29] . For example, the following works leveraged multiple instance learning to identify salient instances to use for recognition using acoustic features  [38, 59] , lexical features  [60] , both acoustic and lexical features  [52] , and all three modalities -acoustic, lexical, and visual  [39] . Another saliency-based method used the concept of the peak-end rule which posits that how people feel after an emotional experience is predicted by the emotional extremes and the end of that experience  [36] . The theory was leveraged to identify salient segments, extract features from those segments, and then perform the recognition task  [13] . Another modeling approach leveraged dynamic modeling of all segments with a Markov model using acoustic features  [101]  and lexical features  [21, 70] .",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Interpersonal Considerations",
      "text": "The dyadic nature of couples' interactions offers the opportunity to leverage various interaction dynamics to perform recognition of emotions. One major dyadic dynamic that has been used is synchrony/entrainment which refers to how similar/aligned/synchronized partners are when interacting. Various quantitative measures for synchrony have been computed for various modalities. For acoustic, some examples include prosodic entrainment measures computed with the following similarity measures (1) square of correlation coefficient, (2) mutual information, and (3) mean of spectral coherence over pitch and energy between the sequential turns of partner A and partner B when there is turn change  [56] . Another approach leverages principal component analysis (PCA) to compute both prosodic and spectral entrainment while providing information about the directionality of the entrainment  [57, 58] . For the visual modality, the Kullback-Leibler (KL) divergence of the features extracted from the head motion of the partners was used as the similarity measure for synchrony  [102] . Another dyadic dynamic derives from the idea that partners generally influence each other while interacting. Dyadic influence has been modeled using lexical features from both partners  [70]  and both acoustic and lexical features from both partners  [11] .",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Evaluation",
      "text": "Three works performed regression  [20, 94, 96]  with the rest performing classification. All works that performed classification trained models to perform binary classification except  [22]  and  [26]  which performed 3-class and 5-class classification respectively. All works performed global emotion recognition except  [22]  and  [5]  which performed utterance-level recognition for every speaker turn and every 10 seconds respectively. All works have used accuracy as the evaluation metric except the following which used unweighted average recall (UAR)  [5, 11, 13, 22] , Spearman correlation  [20]  and mean absolute error (MAE)  [94, 96] . It is important to note that all the works that used accuracy as the metric had balanced classes (except  [26] ) and hence, there should not be any concern about it not being an appropriate metric.\n\nMost evaluations have been done with leave-one-couple-out (LOCO) cross-validation which is a robust evaluation approach as it gives a sense of how the model will perform on an unseen couple. With this approach, models are trained using data from all couples but one, and then the prediction is done on the remaining couple's data as the test set. This process is repeated till each couple has been used as a test set. Hence, if there are 300 couples, the evaluation is done 300 times. In the end, the predictions of each test couple are combined either by computing the evaluation metric (e.g., accuracy) separately for each couple and then computing the mean and standard deviation of the accuracies, or concatenating all the predictions and computing one accuracy value for all the combined predictions. LOCO is a variation of the standard leave-one-subject-out cross-validation but more robust for the context of couples data as it ensures that there is no data leakage from the same audio (as an example) being in both train and test sets. One challenge with LOCO is that the evaluation could take a long time when there are a lot of couples as it is done as many times as there are couples.\n\nOther similarly robust evaluation approaches that have been used which also ensure that there is no data leakage but reduces the amount of time relatively are leave-N-couples-out cross-validation (LNCO) (e.g.,  [63]  with n=4) and K-fold cross-validation (CV) couple disjoint (k=10:  [5, 11, 38, 52, 60] , k=6:  [20] ). The \"couple disjoint\" refers to the fact that a couple is never in both the train and test set for the same evaluation run.\n\nAnother approach that has been used is the standard hold out (train test split) evaluation  [26] . That work also performed couple-dependent evaluation  [26] . That is, the authors trained and evaluated models within each couple separately. Hence, the concept of \"couple disjoint\" does not apply. The results from such an evaluation could be inflated as it could leverage particularities of the data to produce high results and does not give a sense of how the model will perform on an unseen couple. Nonetheless, it gives a sense of how well the model may perform if personalized models are trained.\n\nFurthermore, several works have performed gender-specific evaluations where the model is trained and evaluated separately for male and female partners  [6, 8, 11, 13, 21, 22, 26, 38, 96, 101] . The motivation for this approach is that gender differences affect how people express their emotions  [16]  and in particular how they speak and hence training approaches may benefit from using models separately for each gender.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Results",
      "text": "In this section, we summarize the main results across the works and also provide some context to enable the correct interpretation of the results.\n\nThe best result for the work that performed 3-class classification (positive, negative, neutral) is 57.4% UAR  [22] . The best result for the 5-class classification (anger, sadness, joy, and tension, neutral) which also used couple-dependent evaluation ranged from 78% to 95% for different couples and genders  [26] .\n\nFor binary classification, we provide results for 2 groups of works -  (1)  works that used the UCLA/UW Couples Therapy dataset in which they only considered ratings at the 2 extremes and (2) works that used other datasets without only considering the extreme ratings. We separate the two because the first task is easier than the second since only extremes are being considered rather than all ratings regardless of the intensity.\n\nFor the first group, here are the best accuracies for each emotion task and gender with the corresponding modality shown:\n\n• Positive affect: 82% (female)  [6]  (acoustic), 78% (male)  [101]  (acoustic), 93% (combined male and female)  [52]  (lexical)\n\n• Negative affect: 88.57% (female)  [21]  (lexical), 85.7% (male)  [8]  (acoustic), 95%  [52]  (combined male and female) (lexical)\n\n• Sadness: 66.4% (female)  [38]  (acoustic), 63.6% (male)  [38]  (acoustic), 80%  [52]  (combined male and female)(lexical)\n\n• Positive vs negative: 76%  [56]  (combined male and female) (lexical)\n\nIn light of these high accuracy results, it is worth noting that they are not reflective of true emotion recognition performance since the data was partitioned into two extreme ratings (top 20% and bottom 20%). Consequently, the performance would likely be much lower if all the data were used.\n\nThe best results for the second group are the following UAR for positive vs negative: 74.8% (female)  [13]  (acoustic), 56.1% (male)  [11]  (acoustic and lexical), 69.2% (combined male and female)  [5]  (lexical).\n\nFor the regression tasks, the best results are 1.22 MAE for negative affect  [96]  (acoustic and lexical) and Spearman correlation of 0.5 for positive affect, 0.58 for negative affect, 0.18 for anxiety, 0.52 for anger and 0.28 for sadness  [20]  (lexical).\n\nFor works that use gender-specific evaluations, performance for female partners tends to be better than for male partners. These results might suggest that it is more difficult recognizing the emotions of male partners and consistent with insights from psychology that suggest that female partners are more emotionally expressive  [16] . Also, in works that consider multiple modalities, lexical modality tends to outperform other modalities including multimodal ones  [5, 22, 39, 52] . Considering the different evaluation contexts (e.g., different number of classes, data subsamples, etc.), it is difficult to compare results directly. Nonetheless, excluding results from UCLA/UW Couples Therapy dataset (because of the data selection bias issue) and the Stanford Lab dataset (because of the use of couple-dependent evaluation) both of which produce inflated results, it is clear that all of the best accuracy results are below 75% with most below 70%. As a reference, the partner-perceived result reported in Boateng et al (  [13] ) -how well partner A could tell the emotions of their partner B -were 73.2% (male) and 74.3% (female). Hence, there is more room for improvement to have performance results that are on par with or exceed how well, for example, husbands or wives could tell the emotions of their wives or husbands.",
      "page_start": 15,
      "page_end": 16
    },
    {
      "section_name": "Discussion: Research Gap, Challenges And Future Direction",
      "text": "Despite the contributions of these works, there are still significant research gaps. In this section, we discuss these research gaps, challenges, and future directions in this area of research.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Unexplored Modalities",
      "text": "Only two modalities -acoustic and lexical -have been mostly used for recognition with the visual modality explored only superficially. Several modalities such as physiological data (heart rate, heart rate variability, skin temperature, skin conductance), body and hand gestures using accelerometer, gyroscope, or even the visual modality, and facial expression are unexplored. Additionally, only standard and simple multimodal fusion approaches have been used.\n\nMore complex fusion approaches such as model-level and hybrid  [74]  could be explored in the future.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Cross-Lingual And Cross-Cultural Evaluation",
      "text": "None of these works have performed cross-lingual and cross-cultural evaluations. Models in these works have been developed in lingual silos (English, German, and Dutch language speakers) and cultural silos (North Americans, Western Europeans). More effort would be needed to develop and evaluate recognition systems that work across languages since multilingual language models would need to be used. Furthermore, culture affects how people experience and express emotions  [65, 84] . Currently, it is not clear how well the recognition systems would work across cultural contexts. Hence, more work is needed to perform these kinds of evaluations as it is important for building systems that are easily generalizable to other contexts.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Intrapersonal And Interpersonal Modeling",
      "text": "Further intrapersonal and interpersonal modeling approaches could be explored. For example, attention mechanisms  [99]  could be leveraged to automatically learn the salient segments as part of the training process. Also, synchrony measures have only been computed for individual modalities. Computing and using synchrony measures multimodally is a possible future direction. Additionally, more complex dyadic influence modeling could be used such as on a turnby-turn basis rather than only including the features of the interacting partner as was done in  [11] . Other kinds of dyadic dynamics from  [23]  can be used such as the ratio of both partners' counts of positive and negative words and turn-taking patterns (e.g., the ratio of partners' speaker turn duration, pauses, number of words, etc).",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Observed Vs Self-Reported Emotion Data",
      "text": "Most of these works have used observed emotion labels from external raters with only a few works using self-reported labels from the partners themselves  [11, 13] . One challenge with observed labels is that they are based on the perceptions of external individuals and consequently, do not reflect the subjective emotions of the partners. Though similar, performing recognition of these two groups is distinct and important to be mindful of depending on the downstream use case of the system. For example, if the intended use case and intervention is that partner A shows empathy to partner B based on how partner A is feeling, for example, a recognition system that only looks at emotional behavior or emotional expression will not be the best to use but rather, one that can adequately quantify partner A's subjective emotions. More work is needed to be done using self-reported labels.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Data From Daily Life",
      "text": "Currently, there is no work that has performed emotion recognition using data collected from couples' interactions in uncontrolled settings in daily life. Data from the wild tend to be noisy and could have more potential confounders such as increased heart rate arising from physical activity rather than from high emotional arousal such as stress or anger which could be the likely reason in a controlled lab conversation setting. Hence, the recognition task would be more challenging than the context of the datasets used in these works, which are couples sitting at one place, with limited mobility, and having an 8-10 minute conversation. Consequently, models developed with lab data will likely not perform well on data from daily life. Future work is needed to collect this kind of data and perform recognition with it. This work  [12]  is a step in that direction.\n\nThough not unique to the context of data from daily life, it is more critical that performance evaluations go beyond the standard accuracy metrics and include detailed error analyses and assessments of conditions under which the model performs poorly. For example, the model might perform poorly when the signal-to-noise ratio is above a certain threshold (as was considered by  [6] ) or if the transcript has way too few words per speaker turn. The model could be preempted from performing recognition when these conditions are encountered to reduce the likelihood of the model performing poorly. Error analyses would reveal more detailed information such as these. These are key requirements for building robust systems that work using data from uncontrolled settings.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Real-Time Recognition Systems",
      "text": "Furthermore, none of the works surveyed have actually implemented systems that perform real-time recognition either in the lab or in the real world which is the holy grail of a system for couples' emotion recognition. The real test of such a machine learning system is its deployment and evaluation in the contexts in which they are to be used.\n\nKey challenges related to the turn-taking nature of couples' interactions need to be addressed towards accomplishing this goal especially for the two most used modalities -acoustic and lexical. These include having automatic speech preprocessing pipelines -voice activity detection, speaker diarization, and speech recognition systems -that work accurately on the fly without time lags for audio data. Several of the surveyed works used manually annotated data in the preprocessing stage and hence it is a key challenge for future works to address.\n\nAdditionally, the recognition algorithm would also need to work accurately and without lag, on the systems that they are deployed on. For ubiquitous devices such as smartphones, smartwatches, or edge devices, the model would have to be small enough to fit on the device and perform computation without hoarding all the compute resources.\n\nMost of the works have used simple algorithms such as SVM which will work well for such contexts. But for the deep learning models, there might be potential challenges because of their size and compute requirements. This issue is more pertinent for the lexical modality since current start-of-the-art language models (e.g., BERT) are huge and might be impossible to fit on edge devices in their original form. Various approaches to compress, distill and quantize large models would need to be explored.",
      "page_start": 18,
      "page_end": 19
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we survey 28 works that have developed and evaluated systems for emotion recognition using data collected from couples' interactions or conversations. Overall, the works in this survey have mostly used one specific data set -UCLA/UW Couples Therapy data. All works have used data collected from lab contexts. Most works used the acoustic modality and the SVM algorithm for binary classification of positive and negative affect. Various multimodal fusion and intrapersonal and interpersonal modeling approaches have been explored. Robust evaluation approaches (e.g., LOCO, LNCO, and 10-fold CV couple disjoint) and metrics (UAR and accuracy with balanced data) have been used. Performance results leave room for improvement. Substantial research gaps remain with several opportunities for future research directions such as exploring more modalities and advanced fusion approaches, performing crosslingual and cross-cultural evaluations, leveraging other intrapersonal and interpersonal modeling approaches, using data from daily life, and performing real-time and real-world deployment and evaluation of the recognition system.\n\nInsights from this survey would enable future research towards having better couples' emotion recognition system that would enable social and health psychology research and the development of interventions to improve the emotional well-being, relationship quality, and chronic disease management of couples.",
      "page_start": 18,
      "page_end": 18
    }
  ],
  "figures": [],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "UCLA\n/\nUW\nCouples\nTher-\napy",
          "No\nof\nCou-\nples": "134",
          "Couples Context": "English-speaking,\nChronically\ndis-\ntressed couples",
          "Data": "Video,\naudio,\ntranscripts",
          "Hours\nof\nData": "96",
          "Session Type": "Relationship\nproblem\n(1)",
          "Mins\nper\nSes-\nsion": "10",
          "Annotation\nType": "Observer",
          "Annotation\nScope": "Global",
          "Emotion\nModel": "Dimensional,\nCategorical",
          "Emotions": "Positive\naﬀect, Negative\naf-\nfect, Sadness, Anger, Anxiety"
        },
        {
          "Dataset": "Moﬃt\nCen-\nter\nCancer\nConversation",
          "No\nof\nCou-\nples": "85",
          "Couples Context": "English-speaking,\nCouples managing\ncancer",
          "Data": "Video,\naudio,\ntranscripts",
          "Hours\nof\nData": "27",
          "Session Type": "Neutral\n(1),\nCancer\nmanagement issue (1)",
          "Mins\nper\nSes-\nsion": "10",
          "Annotation\nType": "Observer",
          "Annotation\nScope": "Local\n(ut-\nterance\n/\nspeaker-turn)",
          "Emotion\nModel": "Categorical",
          "Emotions": "Hostile\n(Negative),\nPositive,\nConstructive (Neutral)"
        },
        {
          "Dataset": "KU\nLeuven\nDyadic\nInterac-\ntion",
          "No\nof\nCou-\nples": "101",
          "Couples Context": "Dutch-speaking\nCouples",
          "Data": "Video, audio",
          "Hours\nof\nData": "51",
          "Session Type": "Neutral (1), Positive (1),\nNegative (1)",
          "Mins\nper\nSes-\nsion": "10",
          "Annotation\nType": "Self",
          "Annotation\nScope": "Global, Local\n(continuous)",
          "Emotion\nModel": "Categorical,\nDimensional",
          "Emotions": "Categorical:\nanger,\nsadness,\nanxiety,\nrelaxation,\nhappi-\nness, Dimensional:\nValence\nand Arousal"
        },
        {
          "Dataset": "Stanford\nPsy-\nchotherapy",
          "No\nof\nCou-\nples": "3",
          "Couples Context": "English-speaking\nCouples Therapy",
          "Data": "Video,\naudio,\ntranscripts",
          "Hours\nof\nData": "18",
          "Session Type": "Relationship\ndiscus-\nsion (1)",
          "Mins\nper\nSes-\nsion": "60",
          "Annotation\nType": "Observer",
          "Annotation\nScope": "Local\n(utter-\nance)",
          "Emotion\nModel": "Categorical",
          "Emotions": "Anger,\nsadness,\njoy,\ntension,\nneutral"
        },
        {
          "Dataset": "UZH\nCouples\nInteraction",
          "No\nof\nCou-\nples": "368",
          "Couples Context": "German-speaking\ncouples in Switzer-\nland",
          "Data": "Video,\naudio,\ntranscripts",
          "Hours\nof\nData": "637",
          "Session Type": "Conﬂict discussion (1),\nMutual support discus-\nsion (2)",
          "Mins\nper\nSes-\nsion": "8",
          "Annotation\nType": "Self,\nOb-\nserver",
          "Annotation\nScope": "Global, Local\n(utterance)",
          "Emotion\nModel": "Dimensional",
          "Emotions": "Positive, negative, happy vs\nsad, good mood vs bad mood,\nrelaxed\nvs\nangry,\ncalm vs\nstressed"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Ref": "Biggiogera\net\nal., 2021 [5]",
          "Dataset": "UZH\nCouples\nInteraction",
          "Modalities": "Acoustic\n(A),\nLexical\n(L)",
          "Features": "A:\nProsodic\nand\nspectral\neGeMAPS\nfeatures,\nL:\nNgram\n+\nTFIDF,\nLIWC,\nDeep sentence embeddings",
          "Interpersonal": "No",
          "Intraper-\nsonal": "No",
          "Algorithms": "Linear SVM",
          "Evalua-\ntion": "10-fold\nCV\ncouple\ndisjoint",
          "Met-\nric": "UAR",
          "Class": "2",
          "Main Best Results": "Negative\nvs\nPositive\naﬀect:\n69.2%"
        },
        {
          "Ref": "Black et al., 2010\n[6]",
          "Dataset": "UCLA\n/\nUW\nCouples\nTher-\napy",
          "Modalities": "Acoustic",
          "Features": "Prosodic and spectral",
          "Interpersonal": "No",
          "Intraper-\nsonal": "No",
          "Algorithms": "Linear SVM, LDA",
          "Evalua-\ntion": "LOCO\nCV",
          "Met-\nric": "ACC",
          "Class": "2",
          "Main Best Results": "Positive\naﬀect:\n82% (female),\n75% (male), Negative\naﬀect:\n77% (female), 76% (male)"
        },
        {
          "Ref": "Black et al., 2013\n[8]",
          "Dataset": "UCLA\n/\nUW\nCouples\nTher-\napy",
          "Modalities": "Acoustic",
          "Features": "Prosodic and spectral",
          "Interpersonal": "No",
          "Intraper-\nsonal": "No",
          "Algorithms": "Linear SVM, LR",
          "Evalua-\ntion": "LOCO\nCV",
          "Met-\nric": "ACC",
          "Class": "2",
          "Main Best Results": "Positive aﬀect: 77.9% (female),\n72.9\n(male), Negative\naﬀect:\n80% (female), 85.7% (male)"
        },
        {
          "Ref": "Boateng\net\nal.,\n2020 [13]",
          "Dataset": "KU\nLeuven\nDyadic\nInterac-\ntion",
          "Modalities": "Acoustic",
          "Features": "Deep acoustic\nembeddings\n(Spectrograms + Pretrained\nCNN)",
          "Interpersonal": "No",
          "Intraper-\nsonal": "Yes",
          "Algorithms": "Linear SVM",
          "Evalua-\ntion": "LOCO\nCV",
          "Met-\nric": "UAR",
          "Class": "2",
          "Main Best Results": "Negative vs Positive valence:\n74.8% (female), 53.3% (male)"
        },
        {
          "Ref": "Boateng\net\nal.,\n2021 [11]",
          "Dataset": "UZH\nCouples\nInteraction",
          "Modalities": "Acoustic\n(A),\nLexical\n(L);\nFusion:\nFeature\nlevel",
          "Features": "A:\nProsodic\nand\nspectral\neGeMAPS features, L: Deep\nsentence embeddings",
          "Interpersonal": "Yes\n(Dyadic\nIn-\nﬂuence)",
          "Intraper-\nsonal": "No",
          "Algorithms": "Linear\nSVM, RBF\nSVM, RF",
          "Evalua-\ntion": "10-fold\nCV\ncouple\ndisjoint",
          "Met-\nric": "UAR",
          "Class": "2",
          "Main Best Results": "Negative vs Positive valence:\n64.8% (female), 56.1% (male)"
        },
        {
          "Ref": "Chakravarthula\net al., 2015 [21]",
          "Dataset": "UCLA\n/\nUW\nCouples\nTher-\napy",
          "Modalities": "Lexical",
          "Features": "Unigram",
          "Interpersonal": "No",
          "Intraper-\nsonal": "Yes (Dynamic\nmodeling)",
          "Algorithms": "ML\nas\nbaseline,\nHMM",
          "Evalua-\ntion": "LOCO\nCV",
          "Met-\nric": "ACC",
          "Class": "2",
          "Main Best Results": "Negative\naﬀect:\n88.57%\n(fe-\nmale), 83.57% (male)"
        },
        {
          "Ref": "Chakravarthula\net al., 2018 [70]",
          "Dataset": "UCLA\n/\nUW\nCouples\nTher-\napy",
          "Modalities": "Lexical",
          "Features": "Ngram",
          "Interpersonal": "Yes\n(Dyadic\nIn-\nﬂuence)",
          "Intraper-\nsonal": "Yes (Dynamic\nmodeling)",
          "Algorithms": "ML\nas\nbaseline,\nHMM",
          "Evalua-\ntion": "LOCO\nCV",
          "Met-\nric": "ACC",
          "Class": "2",
          "Main Best Results": "Negative aﬀect: 88.93%"
        },
        {
          "Ref": "Chakravarthula\net al., 2019 [22]",
          "Dataset": "Moﬃt\nCen-\nter\nCancer\nConversation",
          "Modalities": "Acoustic,\nLexical;\nFu-\nsion: Feature level, De-\ncision level",
          "Features": "Acoustic:\nProsodic\nand\nspectral\neGeMAPS\nfea-\nturesLexical:\nDeep\nsens-\ntence embedding",
          "Interpersonal": "No",
          "Intraper-\nsonal": "No",
          "Algorithms": "DNN",
          "Evalua-\ntion": "LOCO\nCV",
          "Met-\nric": "UAR",
          "Class": "3",
          "Main Best Results": "Positive vs Negative vs Neu-\ntral: 57.42%"
        },
        {
          "Ref": "Chakravarthula\net al., 2021 [20]",
          "Dataset": "UCLA\n/\nUW\nCouples\nTher-\napy",
          "Modalities": "Lexical",
          "Features": "N-gram, ELMo",
          "Interpersonal": "No",
          "Intraper-\nsonal": "No",
          "Algorithms": "ML, GRU",
          "Evalua-\ntion": "6-fold\nCV\ncouple\ndisjoint",
          "Met-\nric": "Corr",
          "Class": "N/A",
          "Main Best Results": "Positive\naﬀect:\n0.5,\nNega-\ntive\naﬀect:\n0.58,\nAnxiety:\n0.18,Anger:\n0.52,\nSadness:\n0.34"
        },
        {
          "Ref": "Crangle\net\nal.,\n2019 [26]",
          "Dataset": "Stanford\nPsy-\nchotherapy",
          "Modalities": "Acoustic",
          "Features": "Prosodic\nand\nspectral\nfea-\ntures",
          "Interpersonal": "No",
          "Intraper-\nsonal": "No",
          "Algorithms": "RF",
          "Evalua-\ntion": "Hold out",
          "Met-\nric": "ACC",
          "Class": "5",
          "Main Best Results": "Anger,\nsadness,\njoy,\ntension,\nneutral, Couple A: 87% (male),\n90% (female), Couple B: 78%\n(male), 84% (female), Couple\nC: 95% (male), 88% (female)"
        },
        {
          "Ref": "Georgiou et\nal.,\n2011 [37]",
          "Dataset": "UCLA\n/\nUW\nCouples\nTher-\napy",
          "Modalities": "Lexical",
          "Features": "Unigram",
          "Interpersonal": "No",
          "Intraper-\nsonal": "No",
          "Algorithms": "ML",
          "Evalua-\ntion": "LOCO\nCV",
          "Met-\nric": "ACC",
          "Class": "2",
          "Main Best Results": "Positive\naﬀect:\n88.9%,\nNeg-\native\naﬀect:\n86.7%,\nSadness:\n61.6%"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Ref": "Gibson\net\nal.,\n2011 [38]",
          "Dataset": "UCLA\n/\nUW\nCouples\nTher-\napy",
          "Modalities": "Acoustic",
          "Features": "Spectral",
          "Interpersonal": "No",
          "Intraper-\nsonal": "Yes (Sailency)",
          "Algorithms": "RBF SVM as base-\nline, DD SVM",
          "Evalua-\ntion": "10-fold\nCV\ncouple\ndisjoint",
          "Met-\nric": "ACC",
          "Class": "2",
          "Main Best Results": "Positive aﬀect: 74.3% (female),\n58.6\n(male), Negative\naﬀect:\n77.9% (female),\n71.4% (male),\nSadness: 66.4% (female), 63.6%\n(male)"
        },
        {
          "Ref": "Gibson\net\nal.,\n2015 [39]",
          "Dataset": "UCLA\n/\nUW\nCouples\nTher-\napy",
          "Modalities": "Acoustic\n(A),\nLexical\n(L), Visual\n(V); Fusion:\nFeature level, Decision\nlevel",
          "Features": "A: Prosodic and spectral, L:\nTFIDF, V:\nPower\nspectral\ndensity (PSD) of head mo-\ntion vectors",
          "Interpersonal": "No",
          "Intraper-\nsonal": "Yes (Sailency)",
          "Algorithms": "DD, Linear SVM",
          "Evalua-\ntion": "LOCO\nCV",
          "Met-\nric": "ACC",
          "Class": "2",
          "Main Best Results": "Positive\naﬀect:\n65.13%\n(all\nmodalities),\nNegative\naﬀect:\n70.34% (lexical)"
        },
        {
          "Ref": "Katsamanis\net\nal., 2011 [52]",
          "Dataset": "UCLA\n/\nUW\nCouples\nTher-\napy",
          "Modalities": "Acoustic,\nLexical;\nFu-\nsion: Unclear",
          "Features": "MFCC, TFIDF",
          "Interpersonal": "No",
          "Intraper-\nsonal": "Yes (Sailency)",
          "Algorithms": "RBF SVM as base-\nline, DD SVM.",
          "Evalua-\ntion": "10-fold\nCV\ncouple\ndisjoint",
          "Met-\nric": "ACC",
          "Class": "2",
          "Main Best Results": "Positive\naﬀect:\n93% (lexical),\nNegative aﬀect: 95% (lexical),\nSadness: 80% (lexical)"
        },
        {
          "Ref": "Lee\net al., 2010\n[56]",
          "Dataset": "UCLA\n/\nUW\nCouples\nTher-\napy",
          "Modalities": "Acoustic",
          "Features": "Prosodic",
          "Interpersonal": "Yes (Synchrony)",
          "Intraper-\nsonal": "No",
          "Algorithms": "MM",
          "Evalua-\ntion": "LOCO\nCV",
          "Met-\nric": "ACC",
          "Class": "2",
          "Main Best Results": "Positive\nvs\nnegative\naﬀect:\n76%"
        },
        {
          "Ref": "Lee et al., 2011a\n[58]",
          "Dataset": "UCLA\n/\nUW\nCouples\nTher-\napy",
          "Modalities": "Acoustic",
          "Features": "Prosodic and spectral",
          "Interpersonal": "Yes (Synchrony)",
          "Intraper-\nsonal": "No",
          "Algorithms": "RBF SVM",
          "Evalua-\ntion": "LOCO\nCV",
          "Met-\nric": "ACC",
          "Class": "2",
          "Main Best Results": "Positive\nvs\nnegative\naﬀect:\n51.79%"
        },
        {
          "Ref": "Lee et al., 2011b\n[59]",
          "Dataset": "UCLA\n/\nUW\nCouples\nTher-\napy",
          "Modalities": "Acoustic",
          "Features": "Prosodic and spectral",
          "Interpersonal": "Yes (Synchrony)",
          "Intraper-\nsonal": "Yes (Sailency)",
          "Algorithms": "DD",
          "Evalua-\ntion": "LOCO\nCV",
          "Met-\nric": "ACC",
          "Class": "2",
          "Main Best Results": "Positive\nvs\nnegative\naﬀect:\n53.93%"
        },
        {
          "Ref": "Lee\net al., 2012\n[60]",
          "Dataset": "UCLA\n/\nUW\nCouples\nTher-\napy",
          "Modalities": "Lexical",
          "Features": "TFIDF",
          "Interpersonal": "No",
          "Intraper-\nsonal": "Yes (Sailency)",
          "Algorithms": "DD, SPRT",
          "Evalua-\ntion": "10-fold\nCV\ncouple\ndisjoint",
          "Met-\nric": "ACC",
          "Class": "2",
          "Main Best Results": "Positive\naﬀect:\n76.1%,\nNeg-\native\naﬀect:\n74.2%,\nSadness:\n54.2%"
        },
        {
          "Ref": "Lee\net al., 2014\n[57]",
          "Dataset": "UCLA\n/\nUW\nCouples\nTher-\napy",
          "Modalities": "Acoustic",
          "Features": "Prosodic and spectral",
          "Interpersonal": "Yes (Synchrony)",
          "Intraper-\nsonal": "No",
          "Algorithms": "HMM,\nFactorial\nHMM",
          "Evalua-\ntion": "LOCO\nCV",
          "Met-\nric": "ACC",
          "Class": "2",
          "Main Best Results": "Positive\nvs\nnegative\naﬀect:\n62.86%"
        },
        {
          "Ref": "Li\net\nal.,\n2016\n[61]",
          "Dataset": "UCLA\n/\nUW\nCouples\nTher-\napy",
          "Modalities": "Acoustic",
          "Features": "Prosodic\nand\nspectral\nfea-\ntures",
          "Interpersonal": "No",
          "Intraper-\nsonal": "No",
          "Algorithms": "SVM as\nbaseline,\nDNN",
          "Evalua-\ntion": "LOCO\nCV",
          "Met-\nric": "ACC",
          "Class": "2",
          "Main Best Results": "Negative aﬀect: 77.14%"
        },
        {
          "Ref": "Li\net\nal.,\n2017\n[62]",
          "Dataset": "UCLA\n/\nUW\nCouples\nTher-\napy",
          "Modalities": "Acoustic",
          "Features": "Prosodic and spectral",
          "Interpersonal": "No",
          "Intraper-\nsonal": "No",
          "Algorithms": "DNN\n(autoen-\ncoder)",
          "Evalua-\ntion": "LOCO\nCV",
          "Met-\nric": "ACC",
          "Class": "2",
          "Main Best Results": "Negative aﬀect: 69.64%, Posi-\ntive aﬀect: 66.43 %"
        },
        {
          "Ref": "Li\net\nal.,\n2020\n[63]",
          "Dataset": "UCLA\n/\nUW\nCouples\nTher-\napy",
          "Modalities": "Acoustic",
          "Features": "Prosodic and spectral, Deep\nacoustic embeddings",
          "Interpersonal": "No",
          "Intraper-\nsonal": "No",
          "Algorithms": "CNN, GRU",
          "Evalua-\ntion": "LNCO\n(n=4)",
          "Met-\nric": "ACC",
          "Class": "2",
          "Main Best Results": "Positive\naﬀect:\n65.36%, Neg-\native aﬀect:\n76.07%, Sadness:\n59.29%"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Ref": "Tseng\net\nal.,\n2016 [95]",
          "Dataset": "UCLA\n/\nUW\nCouples\nTher-\napy",
          "Modalities": "Lexical",
          "Features": "word2vec",
          "Interpersonal": "No",
          "Intraper-\nsonal": "No",
          "Algorithms": "ML\nas\nbaseline,\nLSTM + RBF SVR",
          "Evalua-\ntion": "LOCO\nCV",
          "Met-\nric": "ACC",
          "Class": "2",
          "Main Best Results": "Negative aﬀect: 88.93%"
        },
        {
          "Ref": "Tseng\net\nal.,\n2017 [94]",
          "Dataset": "UCLA\n/\nUW\nCouples\nTher-\napy",
          "Modalities": "Lexical",
          "Features": "word2vec, Deep\nsentence\nembeddings",
          "Interpersonal": "No",
          "Intraper-\nsonal": "No",
          "Algorithms": "LSTM + RBF SVR",
          "Evalua-\ntion": "LOCO\nCV",
          "Met-\nric": "MAE",
          "Class": "N/A",
          "Main Best Results": "Negative aﬀect: 1.37"
        },
        {
          "Ref": "Tseng\net\nal.,\n2018 [96]",
          "Dataset": "UCLA\n/\nUW\nCouples\nTher-\napy",
          "Modalities": "Acoustic\n(A),\nLexical\n(L);\nFusion:\nFeature\nlevel,\nDecision\nlevel,\nGender based, Therapy\nstage",
          "Features": "A:\nProsodic\nand\nspectral\nfeatures, L: Deep sentence\nembeddings",
          "Interpersonal": "No",
          "Intraper-\nsonal": "No",
          "Algorithms": "LSTM, DNN",
          "Evalua-\ntion": "LOCO\nCV",
          "Met-\nric": "MAE",
          "Class": "2",
          "Main Best Results": "Negative aﬀect: 1.22"
        },
        {
          "Ref": "Tseng\net\nal.,\n2019 [93]",
          "Dataset": "UCLA\n/\nUW\nCouples\nTher-\napy",
          "Modalities": "Lexical",
          "Features": "Deep sentence embeddings",
          "Interpersonal": "No",
          "Intraper-\nsonal": "No",
          "Algorithms": "LSTM",
          "Evalua-\ntion": "LOCO\nCV",
          "Met-\nric": "ACC",
          "Class": "2",
          "Main Best Results": "Positive aﬀect: 86.8%Negative\naﬀect: 87.9%"
        },
        {
          "Ref": "Xia\nel\nal., 2015\n[101]",
          "Dataset": "UCLA\n/\nUW\nCouples\nTher-\napy",
          "Modalities": "Acoustic",
          "Features": "Prosodic and spectral",
          "Interpersonal": "No",
          "Intraper-\nsonal": "Yes (Dynamic\nmodeling)",
          "Algorithms": "SVM, LDA, Voted\nPerceptron\nas\nbaseline, HMM +\nSVM, LDA, Voted\nPerceptron",
          "Evalua-\ntion": "LOCO\nCV",
          "Met-\nric": "ACC",
          "Class": "2",
          "Main Best Results": "Positive\naﬀect:\n81% (female),\n78% (male), Negative\naﬀect:\n79% (female), 84% (male), Sad-\nness: 65% (female), 61% (male)"
        },
        {
          "Ref": "Xiao et al., 2015\n[102]",
          "Dataset": "UCLA\n/\nUW\nCouples\nTher-\napy",
          "Modalities": "Visual",
          "Features": "Line\nSpectral\nFrequencies\nof head motion vectors",
          "Interpersonal": "Yes (Synchrony)",
          "Intraper-\nsonal": "No",
          "Algorithms": "GMM\n+\nLinear\nSVM",
          "Evalua-\ntion": "LOCO\nCV",
          "Met-\nric": "ACC",
          "Class": "2",
          "Main Best Results": "Positive aﬀect: 63%, Negative\naﬀect: 57%"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "DECAF: MEG-based multimodal database for decoding affective physiological responses",
      "authors": [
        "Mojtaba Khomami",
        "Ramanathan Subramanian",
        "Seyed Mostafa Kia",
        "Paolo Avesani"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "2",
      "title": "Automated Detection of Stressful Conversations Using Wearable Physiological and Inertial Sensors",
      "authors": [
        "Rummana Bari",
        "Md Mahbubur Rahman",
        "Nazir Saleheen",
        "Megan Parsons",
        "Eugene Buder",
        "Santosh Kumar"
      ],
      "year": "2020",
      "venue": "Proceedings of the ACM on interactive, mobile, wearable and ubiquitous technologies"
    },
    {
      "citation_id": "3",
      "title": "Emotional experience in close relationships. Blackwell handbook of social psychology: Interpersonal processes",
      "authors": [
        "Ellen Berscheid",
        "Hilary Ammazzalorso"
      ],
      "year": "2001",
      "venue": "Emotional experience in close relationships. Blackwell handbook of social psychology: Interpersonal processes"
    },
    {
      "citation_id": "4",
      "title": "The affective slider: A digital self-assessment scale for the measurement of human emotions",
      "authors": [
        "Alberto Betella",
        "Paul",
        "Verschure"
      ],
      "year": "2016",
      "venue": "PloS one"
    },
    {
      "citation_id": "5",
      "title": "BERT Meets LIWC: Exploring State-of-the-Art Language Models for Predicting Communication Behavior in Couples' Conflict Interactions",
      "authors": [
        "Jacopo Biggiogera",
        "George Boateng",
        "Peter Hilpert",
        "Matthew Vowels",
        "Guy Bodenmann",
        "Mona Neysari",
        "Fridtjof Nussbeck",
        "Tobias Kowatsch"
      ],
      "year": "2021",
      "venue": "BERT Meets LIWC: Exploring State-of-the-Art Language Models for Predicting Communication Behavior in Couples' Conflict Interactions",
      "doi": "10.1145/3461615.3485423"
    },
    {
      "citation_id": "6",
      "title": "Automatic classification of married couples' behavior using audio features",
      "authors": [
        "Matthew Black",
        "Athanasios Katsamanis",
        "Chi-Chun Lee",
        "Adam Lammert",
        "Brian Baucom",
        "Andrew Christensen",
        "Panayiotis Georgiou",
        "Shrikanth S Narayanan"
      ],
      "year": "2010",
      "venue": "Automatic classification of married couples' behavior using audio features"
    },
    {
      "citation_id": "7",
      "title": "Classification of Blame in Married Couples' Interactions by Fusing Automatically Derived Speech and Language Information",
      "authors": [
        "Matthew P Black",
        "G Panayiotis",
        "Athanasios Georgiou",
        "Brian Katsamanis",
        "Shrikanth Baucom",
        "Narayanan"
      ],
      "year": "2011",
      "venue": "Twelfth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "8",
      "title": "Toward automating a human behavioral coding system for married couples' interactions using speech acoustic features",
      "authors": [
        "Athanasios Matthew P Black",
        "Brian Katsamanis",
        "Chi-Chun Baucom",
        "Adam Lee",
        "Andrew Lammert",
        "Panayiotis Christensen",
        "Georgiou",
        "Shrikanth S Narayanan"
      ],
      "year": "2013",
      "venue": "Speech communication"
    },
    {
      "citation_id": "9",
      "title": "Towards Real-Time Multimodal Emotion Recognition among Couples",
      "authors": [
        "George Boateng"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "10",
      "title": "ActivityAware: an app for real-time daily activity level monitoring on the amulet wrist-worn device",
      "authors": [
        "George Boateng",
        "John Batsis",
        "Ryan Halter",
        "David Kotz"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops)"
    },
    {
      "citation_id": "11",
      "title": "You Made Me Feel This Way\": Investigating Partners' Influence in Predicting Emotions in Couples' Conflict Interactions Using Speech Data",
      "authors": [
        "George Boateng",
        "Peter Hilpert",
        "Guy Bodenmann",
        "Mona Neysari",
        "Tobias Kowatsch"
      ],
      "year": "2021",
      "venue": "Companion Publication of the 2021 International Conference on Multimodal Interaction",
      "doi": "10.1145/3461615.3485424"
    },
    {
      "citation_id": "12",
      "title": "Speech Emotion Recognition among Elderly Individuals using Transfer Learning",
      "authors": [
        "George Boateng",
        "Tobias Kowatsch"
      ],
      "year": "2020",
      "venue": "Companion Publication of the 2020 International Conference on Multimodal Interaction (ICMI '20 Companion)"
    },
    {
      "citation_id": "13",
      "title": "Speech Emotion Recognition among Couples using the Peak-End Rule and Transfer Learning",
      "authors": [
        "George Boateng",
        "Laura Sels",
        "Peter Kuppens",
        "Peter Hilpert",
        "Urte Scholz",
        "Tobias Kowatsch"
      ],
      "year": "2020",
      "venue": "Companion Publication of the 2020 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "15",
      "title": "Praat, a system for doing phonetics by computer",
      "authors": [
        "Paul Boersma"
      ],
      "year": "2001",
      "venue": "Glot. Int"
    },
    {
      "citation_id": "16",
      "title": "Effects of social support visibility on adjustment to stress: Experimental evidence",
      "authors": [
        "Niall Bolger",
        "David Amarel"
      ],
      "year": "2007",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "17",
      "title": "On understanding gender differences in the expression of emotion",
      "authors": [
        "Leslie R Brody"
      ],
      "year": "1993",
      "venue": "Human feelings: Explorations in affect development and meaning"
    },
    {
      "citation_id": "18",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "19",
      "title": "MSP-IMPROV: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "Carlos Busso",
        "Srinivas Parthasarathy",
        "Alec Burmania",
        "Mohammed Abdelwahab",
        "Najmeh Sadoughi",
        "Emily Provost"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "20",
      "title": "Emotional behavior in long-term marriage",
      "authors": [
        "John Laura L Carstensen",
        "Robert Gottman",
        "Levenson"
      ],
      "year": "1995",
      "venue": "Psychology and aging"
    },
    {
      "citation_id": "21",
      "title": "An analysis of observation length requirements for machine understanding of human behaviors from spoken language",
      "authors": [
        "Brian Sandeep Nallan Chakravarthula",
        "Shrikanth Baucom",
        "Panayiotis Narayanan",
        "Georgiou"
      ],
      "year": "2021",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "22",
      "title": "A language-based generative model framework for behavioral analysis of couples' therapy",
      "authors": [
        "Rahul Sandeep Nallan Chakravarthula",
        "Brian Gupta",
        "Panayiotis Baucom",
        "Georgiou"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "23",
      "title": "Predicting Behavior in Cancer-Afflicted Patient and Spouse Interactions Using Speech and Language",
      "authors": [
        "Haoqi Sandeep Nallan Chakravarthula",
        "Li",
        "Shao-Yen",
        "Maija Tseng",
        "Panayiotis Reblin",
        "Georgiou"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "24",
      "title": "Automatic Prediction of Suicidal Risk in Military Couples Using Multimodal Interaction Cues from Couples Conversations",
      "authors": [
        "Md Sandeep Nallan Chakravarthula",
        "Nasir",
        "Shao-Yen",
        "Haoqi Tseng",
        "Li",
        "Jin Tae",
        "Brian Park",
        "Craig Baucom",
        "Shrikanth Bryan",
        "Panayiotis Narayanan",
        "Georgiou"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "25",
      "title": "Traditional versus integrative behavioral couple therapy for significantly and chronically distressed married couples",
      "authors": [
        "Andrew Christensen",
        "David Atkins",
        "Sara Berns",
        "Jennifer Wheeler",
        "Donald Baucom",
        "Lorelei Simpson"
      ],
      "year": "2004",
      "venue": "Journal of consulting and clinical psychology"
    },
    {
      "citation_id": "26",
      "title": "The specific affect coding system (SPAFF). Handbook of emotion elicitation and assessment",
      "authors": [
        "A James",
        "John Coan",
        "Gottman"
      ],
      "year": "2007",
      "venue": "The specific affect coding system (SPAFF). Handbook of emotion elicitation and assessment"
    },
    {
      "citation_id": "27",
      "title": "Machine learning for the recognition of emotion in the speech of couples in psychotherapy using the Stanford Suppes Brain Lab Psychotherapy Dataset",
      "authors": [
        "Colleen Crangle",
        "Rui Wang",
        "Marcos Perreau-Guimaraes",
        "Michelle Nguyen",
        "Duc Nguyen",
        "Patrick Suppes"
      ],
      "year": "2019",
      "venue": "Machine learning for the recognition of emotion in the speech of couples in psychotherapy using the Stanford Suppes Brain Lab Psychotherapy Dataset",
      "arxiv": "arXiv:1901.04110"
    },
    {
      "citation_id": "28",
      "title": "Complex affect dynamics add limited information to the prediction of psychological well-being",
      "authors": [
        "Egon Dejonckheere",
        "Merijn Mestdagh",
        "Marlies Houben",
        "Isa Rutten",
        "Laura Sels",
        "Peter Kuppens",
        "Francis Tuerlinckx"
      ],
      "year": "2019",
      "venue": "Nature human behaviour"
    },
    {
      "citation_id": "29",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "30",
      "title": "Solving the multiple instance problem with axis-parallel rectangles",
      "authors": [
        "Richard Thomas G Dietterich",
        "Tomás Lathrop",
        "Lozano-Pérez"
      ],
      "year": "1997",
      "venue": "Artificial intelligence"
    },
    {
      "citation_id": "31",
      "title": "A review and meta-analysis of multimodal affect detection systems",
      "authors": [
        "K Sidney",
        "Jacqueline Kory"
      ],
      "year": "2015",
      "venue": "ACM Computing Surveys (CSUR)"
    },
    {
      "citation_id": "32",
      "title": "Universal facial expressions of emotion",
      "authors": [
        "Paul Ekman",
        "Dacher Keltner"
      ],
      "year": "1997",
      "venue": "Nonverbal communication: Where nature meets culture"
    },
    {
      "citation_id": "33",
      "title": "The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing",
      "authors": [
        "Florian Eyben",
        "Klaus Scherer",
        "Björn Schuller",
        "Johan Sundberg",
        "Elisabeth André",
        "Carlos Busso",
        "Laurence Devillers",
        "Julien Epps",
        "Petri Laukka",
        "Shrikanth S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "34",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin Wöllmer",
        "Björn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "35",
      "title": "Affective processes as mediators of links between close relationships and physical health",
      "authors": [
        "Ledina Allison K Farrell",
        "Sarah Ce Imami",
        "Richard Stanton",
        "Slatcher"
      ],
      "year": "2018",
      "venue": "Social and Personality Psychology Compass"
    },
    {
      "citation_id": "36",
      "title": "A Review of Generalizable Transfer Learning in Automatic Emotion Recognition",
      "authors": [
        "Kexin Feng",
        "Theodora Chaspari"
      ],
      "year": "2020",
      "venue": "Frontiers in Computer Science"
    },
    {
      "citation_id": "37",
      "title": "Extracting meaning from past affective experiences: The importance of peaks, ends, and specific emotions",
      "authors": [
        "Barbara L Fredrickson"
      ],
      "year": "2000",
      "venue": "Cognition & Emotion"
    },
    {
      "citation_id": "38",
      "title": "That's aggravating, very aggravating\": is it possible to classify behaviors in couple interactions using automatically derived lexical features",
      "authors": [
        "G Panayiotis",
        "Matthew Georgiou",
        "Adam Black",
        "Brian Lammert",
        "Baucom",
        "Shrikanth S Narayanan"
      ],
      "year": "2011",
      "venue": "International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "39",
      "title": "Automatic identification of salient acoustic instances in couples' behavioral interactions using diverse density support vector machines",
      "authors": [
        "James Gibson",
        "Athanasios Katsamanis",
        "Matthew Black",
        "Shrikanth Narayanan"
      ],
      "year": "2011",
      "venue": "Twelfth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "40",
      "title": "Multiple instance learning for behavioral coding",
      "authors": [
        "James Gibson",
        "Athanasios Katsamanis",
        "Francisco Romero",
        "Bo Xiao",
        "Panayiotis Georgiou",
        "Shrikanth Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "41",
      "title": "The mathematics of marriage: Dynamic nonlinear models",
      "authors": [
        "John Mordechai"
      ],
      "year": "2005",
      "venue": "The mathematics of marriage: Dynamic nonlinear models"
    },
    {
      "citation_id": "42",
      "title": "What predicts divorce?",
      "authors": [
        "John Mordechai"
      ],
      "year": "2014",
      "venue": "The relationship between marital processes and marital outcomes"
    },
    {
      "citation_id": "43",
      "title": "A valid procedure for obtaining self-report of affect in marital interaction",
      "authors": [
        "M John",
        "Robert Gottman",
        "Levenson"
      ],
      "year": "1985",
      "venue": "Journal of consulting and clinical psychology"
    },
    {
      "citation_id": "44",
      "title": "The Vera am Mittag German audio-visual emotional speech database",
      "authors": [
        "Michael Grimm",
        "Kristian Kroschel",
        "Shrikanth Narayanan"
      ],
      "year": "2008",
      "venue": "2008 IEEE international conference on multimedia and expo"
    },
    {
      "citation_id": "45",
      "title": "Couples interaction rating system 2 (CIRS2)",
      "authors": [
        "Heavey",
        "Gill",
        "Christensen"
      ],
      "year": "2002",
      "venue": "Couples interaction rating system 2 (CIRS2)"
    },
    {
      "citation_id": "46",
      "title": "The longitudinal impact of demand and withdrawal during marital conflict",
      "authors": [
        "Andrew Christopher L Heavey",
        "Neil Christensen",
        "Malamuth"
      ],
      "year": "1995",
      "venue": "Journal of consulting and clinical psychology"
    },
    {
      "citation_id": "47",
      "title": "Living with cancer: The cancer inventory of problem situations",
      "authors": [
        "Cyndie Richard L Heinrich",
        "Patricia Coscarelli Schag",
        "Ganz"
      ],
      "year": "1984",
      "venue": "Journal of clinical psychology"
    },
    {
      "citation_id": "48",
      "title": "Observation of couple conflicts: Clinical assessment applications, stubborn truths, and shaky foundations",
      "authors": [
        "Richard E Heyman"
      ],
      "year": "2001",
      "venue": "Observation of couple conflicts: Clinical assessment applications, stubborn truths, and shaky foundations"
    },
    {
      "citation_id": "49",
      "title": "Rapid marital interaction coding system (RMICS)",
      "authors": [
        "Richard E Heyman"
      ],
      "year": "2004",
      "venue": "Rapid marital interaction coding system (RMICS)"
    },
    {
      "citation_id": "50",
      "title": "Marital interaction coding system: Revision and empirical evaluation",
      "authors": [
        "Robert Richard E Heyman",
        "J Mark Weiss",
        "Eddy"
      ],
      "year": "1995",
      "venue": "Behaviour research and therapy"
    },
    {
      "citation_id": "51",
      "title": "When the going gets tough, does support get going? Determinants of spousal support provision to type 2 diabetic patients",
      "authors": [
        "Masumi Iida",
        "Ann Stephens",
        "Karen Rook",
        "Melissa Franks",
        "James Salem"
      ],
      "year": "2010",
      "venue": "Personality and Social Psychology Bulletin"
    },
    {
      "citation_id": "52",
      "title": "Couples interaction study: Social support interaction rating system",
      "authors": [
        "J Jones",
        "Christensen"
      ],
      "year": "1998",
      "venue": "Couples interaction study: Social support interaction rating system"
    },
    {
      "citation_id": "53",
      "title": "Multiple instance learning for classification of human behavior observations",
      "authors": [
        "Athanasios Katsamanis",
        "James Gibson",
        "Matthew Black",
        "Shrikanth S Narayanan"
      ],
      "year": "2011",
      "venue": "International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "54",
      "title": "Couple observational coding systems",
      "authors": [
        "K Patricia",
        "Donald Kerig",
        "Baucom"
      ],
      "year": "2004",
      "venue": "Couple observational coding systems"
    },
    {
      "citation_id": "55",
      "title": "The Me in We dyadic communication intervention is feasible and acceptable among advanced cancer patients and their family caregivers",
      "authors": [
        "Dana Ketcher",
        "Casidee Thompson",
        "Amy Otto",
        "Maija Reblin",
        "Kristin Cloyes",
        "Margaret Clayton",
        "Brian Baucom",
        "Lee Ellington"
      ],
      "year": "2021",
      "venue": "Palliative Medicine"
    },
    {
      "citation_id": "56",
      "title": "Avoidance orientation and the escalation of negative communication in intimate relationships",
      "authors": [
        "Monika Kuster",
        "Katharina Bernecker",
        "Sabine Backes",
        "Veronika Brandstätter",
        "W Fridtjof",
        "Thomas Nussbeck",
        "Mike Bradbury",
        "Dorothee Martin",
        "Guy Sutter-Stickel",
        "Bodenmann"
      ],
      "year": "2015",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "57",
      "title": "Quantification of prosodic entrainment in affective spontaneous spoken interactions of married couples",
      "authors": [
        "Chi-Chun Lee",
        "Matthew Black",
        "Athanasios Katsamanis",
        "Adam Lammert",
        "Brian Baucom",
        "Andrew Christensen",
        "Panayiotis Georgiou",
        "Shrikanth S Narayanan"
      ],
      "year": "2010",
      "venue": "Eleventh Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "58",
      "title": "Computing vocal entrainment: A signal-derived PCA-based quantification scheme with application to affect analysis in married couple interactions",
      "authors": [
        "Chi-Chun Lee",
        "Athanasios Katsamanis",
        "Matthew Black",
        "Brian Baucom",
        "Andrew Christensen",
        "Panayiotis Georgiou",
        "Shrikanth S Narayanan"
      ],
      "year": "2014",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "59",
      "title": "An analysis of PCA-based vocal entrainment measures in married couples' affective spoken interactions",
      "authors": [
        "Chi-Chun Lee",
        "Athanasios Katsamanis",
        "Matthew Black",
        "Brian Baucom",
        "Panayiotis Georgiou",
        "Shrikanth Narayanan"
      ],
      "year": "2011",
      "venue": "Twelfth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "60",
      "title": "Affective state recognition in married couples' interactions using PCA-based vocal entrainment measures with multiple instance learning",
      "authors": [
        "Chi-Chun Lee",
        "Athanasios Katsamanis",
        "Matthew Black",
        "Brian Baucom",
        "Panayiotis Georgiou",
        "Shrikanth S Narayanan"
      ],
      "year": "2011",
      "venue": "International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "61",
      "title": "Based on isolated saliency or causal integration? toward a better understanding of human annotation process using multiple instance learning and sequential probability ratio test",
      "authors": [
        "Chi-Chun Lee",
        "Athanasios Katsamanis",
        "G Panayiotis",
        "Georgiou",
        "Shrikanth S Narayanan"
      ],
      "year": "2012",
      "venue": "Thirteenth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "62",
      "title": "Sparsely Connected and Disjointly Trained Deep Neural Networks for Low Resource Behavioral Annotation: Acoustic Classification in Couples' Therapy",
      "authors": [
        "Haoqi Li",
        "Brian Baucom",
        "Panayiotis Georgiou"
      ],
      "year": "2016",
      "venue": "Proc. Interspeech",
      "doi": "10.21437/Interspeech.2016-1217"
    },
    {
      "citation_id": "63",
      "title": "Unsupervised latent behavior manifold learning from acoustic features: Audio2behavior",
      "authors": [
        "Haoqi Li",
        "Brian Baucom",
        "Panayiotis Georgiou"
      ],
      "year": "2017",
      "venue": "2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "64",
      "title": "Linking emotions to behaviors through deep transfer learning",
      "authors": [
        "Haoqi Li",
        "Brian Baucom",
        "Panayiotis Georgiou"
      ],
      "year": "2020",
      "venue": "PeerJ Computer Science"
    },
    {
      "citation_id": "65",
      "title": "Social Support and Common Dyadic Coping in Couples' Dyadic Management of Type II Diabetes: Protocol for an Ambulatory Assessment Application",
      "authors": [
        "Janina Lüscher",
        "Tobias Kowatsch",
        "George Boateng",
        "Prabhakaran Santhanam",
        "Guy Bodenmann",
        "Urte Scholz"
      ],
      "year": "2019",
      "venue": "JMIR research protocols"
    },
    {
      "citation_id": "66",
      "title": "American-Japanese cultural differences in intensity ratings of facial expressions of emotion",
      "authors": [
        "David Matsumoto",
        "Paul Ekman"
      ],
      "year": "1989",
      "venue": "Motivation and Emotion"
    },
    {
      "citation_id": "67",
      "title": "The USC CreativeIT database: A multimodal database of theatrical improvisation. Multimodal Corpora: Advances in Capturing",
      "authors": [
        "Angeliki Metallinou",
        "Chi-Chun Lee",
        "Carlos Busso",
        "Sharon Carnicke",
        "Shrikanth Narayanan"
      ],
      "year": "2010",
      "venue": "Coding and Analyzing Multimodality"
    },
    {
      "citation_id": "68",
      "title": "Annotation and processing of continuous emotional attributes: Challenges and opportunities",
      "authors": [
        "Angeliki Metallinou",
        "Shrikanth Narayanan"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG)"
    },
    {
      "citation_id": "69",
      "title": "Distributed representations of words and phrases and their compositionality",
      "authors": [
        "Tomas Mikolov",
        "Ilya Sutskever",
        "Kai Chen",
        "Greg Corrado",
        "Jeff Dean"
      ],
      "year": "2013",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "70",
      "title": "Emotion recognition from embedded bodily expressions and speech during dyadic interactions",
      "authors": [
        "Sikandar Philipp M Müller",
        "Prateek Amin",
        "Mykhaylo Verma",
        "Andreas Andriluka",
        "Bulling"
      ],
      "year": "2015",
      "venue": "2015 International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "71",
      "title": "Modeling Interpersonal Influence of Verbal Behavior in Couples Therapy Dyadic Interactions",
      "authors": [
        "Brian Sandeep Nallan Chakravarthula",
        "Panayiotis Baucom",
        "Georgiou"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech",
      "doi": "10.21437/Interspeech.2018-1562"
    },
    {
      "citation_id": "72",
      "title": "",
      "authors": [
        "Young Cheul",
        "Narae Park",
        "Soowon Cha",
        "Auk Kang",
        "Ahsan Kim",
        "Leontios Habib Khandoker",
        "Alice Hadjileontiadis",
        "Yong Oh",
        "Uichin Jeong",
        "Lee"
      ],
      "year": "2020",
      "venue": ""
    },
    {
      "citation_id": "73",
      "title": "a multimodal sensor dataset for continuous emotion recognition in naturalistic conversations",
      "year": "2020",
      "venue": "Scientific Data"
    },
    {
      "citation_id": "74",
      "title": "Linguistic inquiry and word count: LIWC",
      "authors": [
        "Martha James W Pennebaker",
        "Roger Francis",
        "Booth"
      ],
      "year": "2001",
      "venue": "Linguistic inquiry and word count: LIWC"
    },
    {
      "citation_id": "75",
      "title": "Deep contextualized word representations",
      "authors": [
        "Mark Matthew E Peters",
        "Mohit Neumann",
        "Matt Iyyer",
        "Christopher Gardner",
        "Kenton Clark",
        "Luke Lee",
        "Zettlemoyer"
      ],
      "year": "2018",
      "venue": "Proceedings of NAACL-HLT"
    },
    {
      "citation_id": "76",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Rajiv Bajpai",
        "Amir Hussain"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "77",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "Soujanya Poria",
        "Navonil Majumder",
        "Rada Mihalcea",
        "Eduard Hovy"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "78",
      "title": "The relation of perceived and received social support to mental health among first responders: a meta-analytic review",
      "authors": [
        "Gabriele Prati",
        "Luca Pietrantoni"
      ],
      "year": "2010",
      "venue": "Journal of Community Psychology"
    },
    {
      "citation_id": "79",
      "title": "Everyday couples' communication research: Overcoming methodological barriers with technology",
      "authors": [
        "Maija Reblin",
        "Richard Heyman",
        "Lee Ellington",
        "Brian Baucom",
        "Panayiotis Georgiou",
        "Susan Vadaparampil"
      ],
      "year": "2018",
      "venue": "Patient education and counseling"
    },
    {
      "citation_id": "80",
      "title": "Behind closed doors: How advanced cancer couples communicate at home",
      "authors": [
        "Maija Reblin",
        "Susan Steven K Sutton",
        "Richard Vadaparampil",
        "Lee Heyman",
        "Ellington"
      ],
      "year": "2019",
      "venue": "Journal of psychosocial oncology"
    },
    {
      "citation_id": "81",
      "title": "Cancer conversations in context: naturalistic observation of couples coping with breast cancer",
      "authors": [
        "Megan Robbins",
        "Ana López",
        "Karen Weihs",
        "Matthias Mehl"
      ],
      "year": "2014",
      "venue": "Journal of Family Psychology"
    },
    {
      "citation_id": "82",
      "title": "Emotion elicitation using dyadic interaction tasks. Handbook of emotion elicitation and assessment",
      "authors": [
        "Nicole Roberts",
        "Jeanne Tsai",
        "James Coan"
      ],
      "year": "2007",
      "venue": "Emotion elicitation using dyadic interaction tasks. Handbook of emotion elicitation and assessment"
    },
    {
      "citation_id": "83",
      "title": "Continuous measurement of emotion. Handbook of emotion elicitation and assessment",
      "authors": [
        "Anna Marie",
        "Robert Levenson"
      ],
      "year": "2007",
      "venue": "Continuous measurement of emotion. Handbook of emotion elicitation and assessment"
    },
    {
      "citation_id": "84",
      "title": "A circumplex model of affect",
      "authors": [
        "Russell James"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "85",
      "title": "Affect grid: a single-item scale of pleasure and arousal",
      "authors": [
        "Anna James A Russell",
        "Gerald Weiss",
        "Mendelsohn"
      ],
      "year": "1989",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "86",
      "title": "Emotional experience in cultural context: A comparison between Europe, Japan and the United States. Faces of emotion: recent research",
      "authors": [
        "K Scherer",
        "H Wallbott",
        "D Matsumoto",
        "Tsutomu"
      ],
      "year": "1988",
      "venue": "Emotional experience in cultural context: A comparison between Europe, Japan and the United States. Faces of emotion: recent research"
    },
    {
      "citation_id": "87",
      "title": "Introducing wesad, a multimodal dataset for wearable stress and affect detection",
      "authors": [
        "Philip Schmidt",
        "Attila Reiss",
        "Robert Duerichen",
        "Claus Marberger",
        "Kristof Van Laerhoven"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "88",
      "title": "Wearable-Based Affect Recognition-A Review",
      "authors": [
        "Philip Schmidt",
        "Attila Reiss",
        "Robert Dürichen",
        "Kristof Van Laerhoven"
      ],
      "year": "2019",
      "venue": "Sensors"
    },
    {
      "citation_id": "89",
      "title": "The occurrence and correlates of emotional interdependence in romantic relationships",
      "authors": [
        "Laura Sels",
        "Jed Cabrieto",
        "Emily Butler",
        "Harry Reis",
        "Eva Ceulemans",
        "Peter Kuppens"
      ],
      "year": "2019",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "90",
      "title": "All's well that ends well? A test of the peak-end rule in couples' conflict discussions",
      "authors": [
        "Laura Sels",
        "Eva Ceulemans",
        "Peter Kuppens"
      ],
      "year": "2019",
      "venue": "European Journal of Social Psychology"
    },
    {
      "citation_id": "91",
      "title": "Actual and perceived emotional similarity in couples' daily lives",
      "authors": [
        "Laura Sels",
        "Yan Ruan",
        "Peter Kuppens",
        "Eva Ceulemans",
        "Harry Reis"
      ],
      "year": "2020",
      "venue": "Social Psychological and Personality Science"
    },
    {
      "citation_id": "92",
      "title": "Emotion, social relationships, and physical health: concepts, methods, and evidence for an integrative perspective",
      "authors": [
        "Timothy Smith",
        "Karen Weihs"
      ],
      "year": "2019",
      "venue": "Psychosomatic medicine"
    },
    {
      "citation_id": "93",
      "title": "Der Mehrdimensionale Befindlichkeitsfragebogen MDBF",
      "authors": [
        "Rolf Steyer",
        "Peter Schwenkmezger",
        "Peter Notz",
        "Michael Eid"
      ],
      "year": "1997",
      "venue": "Der Mehrdimensionale Befindlichkeitsfragebogen MDBF"
    },
    {
      "citation_id": "94",
      "title": "",
      "authors": [
        "Germany Göttingen"
      ],
      "year": "1997",
      "venue": ""
    },
    {
      "citation_id": "95",
      "title": "Using multimodal wearable technology to detect conflict among couples",
      "authors": [
        "Adela Timmons",
        "Theodora Chaspari",
        "C Sohyun",
        "Laura Han",
        "Perrone",
        "Gayla Shrikanth S Narayanan",
        "Margolin"
      ],
      "year": "2017",
      "venue": "Computer"
    },
    {
      "citation_id": "96",
      "title": "Unsupervised online multitask learning of behavioral sentence embeddings",
      "authors": [
        "Shao-Yen",
        "Brian Tseng",
        "Panayiotis Baucom",
        "Georgiou"
      ],
      "year": "0200",
      "venue": "PeerJ Computer Science"
    },
    {
      "citation_id": "97",
      "title": "Approaching Human Performance in Behavior Estimation in Couples Therapy Using Deep Sentence Embeddings",
      "authors": [
        "Shao-Yen",
        "Brian Tseng",
        "Panayiotis G Baucom",
        "Georgiou"
      ],
      "year": "2017",
      "venue": "Approaching Human Performance in Behavior Estimation in Couples Therapy Using Deep Sentence Embeddings"
    },
    {
      "citation_id": "98",
      "title": "Couples Behavior Modeling and Annotation Using Low-Resource LSTM Language Models",
      "authors": [
        "Shao-Yen",
        "Sandeep Tseng",
        "Brian Nallan Chakravarthula",
        "Panayiotis G Baucom",
        "Georgiou"
      ],
      "year": "2016",
      "venue": "Couples Behavior Modeling and Annotation Using Low-Resource LSTM Language Models"
    },
    {
      "citation_id": "99",
      "title": "Multimodal Fusion for Behavior Analysis",
      "authors": [
        "Shao-Yen",
        "Haoqi Tseng",
        "Brian Li",
        "Panayiotis Baucom",
        "Georgiou"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "100",
      "title": "DeepConnection: classifying momentary relationship state from images of romantic couples",
      "authors": [
        "Maximiliane Uhlich",
        "Daniel Bojar"
      ],
      "year": "2021",
      "venue": "Journal of Computational Social Science"
    },
    {
      "citation_id": "101",
      "title": "PASEZ Project-Impact of stress on relationship development of couples and children",
      "year": "2020",
      "venue": "PASEZ Project-Impact of stress on relationship development of couples and children"
    },
    {
      "citation_id": "102",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "103",
      "title": "Development and validation of brief measures of positive and negative affect: the PANAS scales",
      "authors": [
        "David Watson",
        "Anna Clark",
        "Auke Tellegen"
      ],
      "year": "1988",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "104",
      "title": "A dynamic model for behavioral analysis of couple interactions using acoustic features",
      "authors": [
        "Wei Xia",
        "James Gibson",
        "Bo Xiao",
        "Brian Baucom",
        "Panayiotis G Georgiou"
      ],
      "year": "2015",
      "venue": "Sixteenth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "105",
      "title": "Head motion modeling for human behavior analysis in dyadic interaction",
      "authors": [
        "Bo Xiao",
        "Panayiotis Georgiou",
        "Brian Baucom",
        "Shrikanth S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE transactions on multimedia"
    },
    {
      "citation_id": "106",
      "title": "A survey of affect recognition methods: Audio, visual, and spontaneous expressions",
      "authors": [
        "Zhihong Zeng",
        "Maja Pantic",
        "Thomas Glenn I Roisman",
        "Huang"
      ],
      "year": "2008",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "107",
      "title": "Driver emotion recognition for intelligent vehicles: a survey",
      "authors": [
        "Sebastian Zepf",
        "Javier Hernandez",
        "Alexander Schmitt",
        "Wolfgang Minker",
        "Rosalind Picard"
      ],
      "year": "2020",
      "venue": "ACM Computing Surveys (CSUR)"
    }
  ]
}