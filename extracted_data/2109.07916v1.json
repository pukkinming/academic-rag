{
  "paper_id": "2109.07916v1",
  "title": "Fser: Deep Convolutional Neural Networks For Speech Emotion Recognition",
  "published": "2021-09-15T05:03:24Z",
  "authors": [
    "Bonaventure F. P. Dossou",
    "Yeno K. S. Gbenou"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Using mel-spectrograms over conventional MFCCs features, we assess the abilities of convolutional neural networks to accurately recognize and classify emotions from speech data. We introduce FSER, a speech emotion recognition model trained on four valid speech databases, achieving a high-classification accuracy of 95,05%, over 8 different emotion classes: anger, anxiety, calm, disgust, happiness, neutral, sadness, surprise. On each benchmark dataset, FSER outperforms the best models introduced so far, achieving a state-of-the-art performance. We show that FSER stays reliable, independently of the language, sex identity, and any other external factor. Additionally, we describe how FSER could potentially be used to improve mental and emotional health care and how our analysis and findings serve as guidelines and benchmarks for further works in the same direction.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotions are integral parts of daily communication between humans. Whether oral or written, properly understanding the interlocutor is key for a good communication. Even though emotions can be easily concealed in written communication, several psychological studies have shown that it is more difficult for humans to hide their feelings physically or vocally. More generally, facial expressions and voice tones are very good indicators of one's emotional state. Many psychological and physiological studies  [2, 6, 8, 10, 12, 16]  (not limited to the ones cited here) have proved, that emotions make us feel and act; stimulating and influencing both our facial expressions and voice tone. For instance, adrenaline is released in fearful situations to help us run away from danger, as excitement or joy can be expressed while we are talking with friends, family, cuddling our pets or practicing risky sports or activities such as mountaineering or skydiving.\n\nThe recognition of emotional states in speech, so called Speech Emotion Recognition (SER) is not a new concept in the fields of Artificial Intelligence and Machine Learn-ing, but still is a very challenging task, that deserves more attention.\n\nConvolutional neural networks (CNNs) are types of artificial neural networks, one of the most popular that have helped make breakthroughs in face recognition, segmentation and object recognition, handwriting recognition, and of course speech recognition. CNNs get their name from a well-known mathematical operation called convolution, and in most cases are used for efficient patterns recognition in data, notably including accurate images classification.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Structure Of The Paper:",
      "text": "In sections 2 and 3, we respectively describe the speech corpora used in this study, and the preprocessing pipeline. In section 4, after introducing our model architecture, we describe our experiments, results and provide a use case of FSER. The last section is dedicated to the conclusion.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Datasets, And Emotion Classes Creation",
      "text": "For the current study, we collected, united (i.e. speech samples representatives of the same emotion were combined) and used speeches from 4 existing datasets: Emodb 1  , Emovo 2  , Savee 3  , RAVDESS  [9] . The speech files across all datasets are sampled at 48 kHz.\n\nEmodb is a publicly available German speech database containing speech recordings of seven emotions: sadness, anger, happiness, fear, disgust, boredom and neutrality. The recordings were made by five men and five women, all actors, creating 10 statements for each emotion that was tested and evaluated. The entire dataset contains 535 speeches.\n\nEmovo is a publicly available Italian speech database with seven emotions: happiness, sadness, anger, fear, disgust, surprise and neutral. Six Italian actors (three women and three men) generated 14 samples for each emotion. The entire dataset contains 588 speeches. Savee is a British English language database of public speaking, made of speech recordings with seven emotions: happiness, sadness, anger, fear, disgust, surprise and neutral. The recordings were made by four English male actors, generating 15 samples for each emotion. The entire dataset contains 480 speeches.\n\nRavdess has been created by 24 authors: 12 men and 12 women with eight emotions: neutral, calm, happy, sad, angry, scared, disgusted, surprised. The entire dataset contains 1440 speeches.\n\nDespite the fact that speeches across the datasets are sampled at the same frequency, they are structured in different and specific ways. Hence, we had to classify them appropriately and combine the same emotions from all datasets. The average length of speech samples is 3-4 seconds.\n\nAfter the collection and classification of all emotions from the four different datasets, we got 10 classes of emotions: anger, anxiety, fear, boredom, calm, disgust, happiness, neutral, sadness, surprise. Due to the small number of f ear and boredom emotions samples, we decided to add them respectively to anxiety and calm emotion classes. This was also motivated by the fact that, those speech samples have similar pitches and amplitudes. This results in 8 classes of emotion.\n\nWe chose to disregard gender information and to focus only on emotions. The final distribution of emotion classes with the numbers of speech samples of each class, are stated in Table  1 .",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Data Processing And Visualisation",
      "text": "Figure  1  shows the amplitudes of a random speech sample along the time measured in seconds (s). Speech signals are made up of amplitudes and frequencies. To get more information from our speech samples, we decided to map them into the frequency domain using the Fast Fourier Transformation (FFT)  [3, 4] .  Applying the FFT to a sample using the scipy.fft and librosa packages, gives Figure  2 , which shows the initial speech plot, the double-side (negative and positive) FFT spectrum, and the positive FFT spectrum.\n\nHowever, this is still lacking of time information. To remedy this, and make sure we preserve frequencies, time and amplitudes information about the speech samples, in reasonable and adequate range, we decided then to use mel- spectrograms.\n\nIn a mel-spectrogram 4  the abscissa represents time, the ordinate axis represents frequency, and amplitudes are showed by the darkness of a precise frequency at a particular time: low amplitudes are represented with a light-blue color, and very high amplitudes are represented by dark red (see Figure  3 ).\n\nThere are two types of spectrograms: broad-band spectrograms and narrow-band spectrograms. In our study, we used narrow-band spectrograms because they have higher frequency resolution, and larger time interval for every spectrum than broad-band spectrograms: this allows the detection of very small differences in frequencies. Moreover, they show individual harmonic structures, which are vibration frequency folds of the speech, as horizontal striations.\n\nIn Figure  4  are provided examples of mel-spectrograms of speakers speaking a sentence with anger 4(a) and anxiety 4(b). We converted all original speech signals to melspectrograms using the librosa melspectrogram 5  module of the librosa library, and transformed the SER task into a pattern recognition and image classification problem.\n\nWe used 512 as length of the FFT window, 512 as the hop-length (number of samples between successive frames) and a hanning windows size is set to the length of FFT window.\n\nDataset Splitting: 2434 images (80%) have been allocated for the training phase. The remaining 609 (20%) have been used as testing set, to evaluate the model performance. As a standard in the machine learning field, we then split",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Model Architecture And Training",
      "text": "Figure  5  presents the architecture of FSER. The input is an RGB image of shape (64, 64, 3). Our model is made of four blocks of convolutional layers (CLs) with reLU activation  [11] , maximum 2D pooling layers (MPLs)  [19]  and Dropouts  [5]  interposed between them. The output of the four CLs is flattened and fed into a series of three fullyconnected layers, which is finally fed into a softmax activation unit  [7]  to output the probability of the input to belong to each class of emotion.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Hyper-Parameters And Training:",
      "text": "We used a batch size of 64, a learning rate of 0.001 and Stochastic Gradient Descent  [14, 15]   ical cross-entropy.\n\nOur hyper-parameters (stated above), including the number of layers, the number of filters in each layer, the dropout probability, the size of the kernel, padding and stride, have been selected after many manual fine-tuning trials.\n\nFSER has been trained for 400 epochs, during 8 days on a 16GB Tesla K80 GPU, using the platform PaperSpace.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results And Potential Use Case",
      "text": "Tables  2  and 3  present respectively the classification report of FSER on the SER task and the normalized confusion matrix. As shown in Tables  2  and 3 , and in Figure  6 , FSER has a nearly-perfect score for every emotion class.\n\nAdditionally, Table  4  shows the comparison of our work to other works done on the SER task, with the datasets of interest. We can interpret Table  4  as follow:\n\n• On Emodb, our FSER (97.44) outperformed the melspectrograms-based approach proposed by Zhao et al.  [20]  (95.53).\n\n• On Savee, our FSER (99.10) outperformed the traditional MFCCs-based approach proposed by Qayyum et al.  [1]  (83.61)\n\n• On Ravdess, our FSER (98.67) outperformed both mel-spectrograms and traditional MFCCs approaches proposed by Kannan, V. and Haresh, R.  [17]  (86 and 53 respectively).\n\nTo test the robustness of FSER, we collected real-life audio samples from French, Fon, and Igbo speakers. Each of them was asked to imitate as much as they can those emotions, without providing any guidelines. In total, we had 24 audios (3 samples per emotion class), out of which FSER predicted correctly 20 audios. The 4 misclassed audios were found to belong to the classes f ear and boredom that we had to respectively add and consider as anxiety and calm classes. Recalling that FSER has been trained in English, German, and Italian, its performance on these real-time audios, shows that FSER stays reliable, independently of the language, sex identity, and any other external factor.\n\nPotential use case: For humans, emotions constitute an important factor to personal and global health. Being emotionally unstable can affect not only mental wellness but also physical health. During this pandemic, many health reports demonstrated a significant increase in feelings of loneliness and emotional instability leading to and increased rate of suicide for example. Being emotionally healthy is hence crucial for our well-being. As AI-based systems are nowadays helping doctors to quickly diagnose diseases such as cancers, FSER-like systems could also be useful to doctors, particularly psychologists, in identifying the emotional state of their patient. It is no news that some people have trouble expressing their emotions with words, so FSER brings a potential solution to help physicians to better understand their patients, and provide them more efficient treatments. They could also be useful to organizations providing mental care and support to families, clients, or communities at large.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "In the current study, we evaluated the abilities of CNNs to effectively recognize and classify emotions from speech data. We introduced FSER, which outperformed all models that have been introduced for SER task. We showed that FSER is reliable, with no regards to the language spoken, the sex identity of the individual, or any other external factor. FSER-like systems could be beneficial to the healthcare system, as they can help to better understand patients, to provide them better and quicker treatments. It is worth mentioning the limitations of this work, notably the data augmentation we had to perform to deal with the limited number of samples.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows the amplitudes of a random speech sam-",
      "page": 2
    },
    {
      "caption": "Figure 1: Speech sample visualisation using the Librosa",
      "page": 2
    },
    {
      "caption": "Figure 2: Speech sample characteristics using Librosa, and",
      "page": 2
    },
    {
      "caption": "Figure 3: Speech sample converted into mel-spectrogram,",
      "page": 2
    },
    {
      "caption": "Figure 2: , which shows the initial",
      "page": 2
    },
    {
      "caption": "Figure 4: Mel-Spectrograms of speakers speaking a sentence with anger (a) and anxiety (b).",
      "page": 3
    },
    {
      "caption": "Figure 4: are provided examples of mel-spectrograms",
      "page": 3
    },
    {
      "caption": "Figure 5: FSER Model Architecture with: 1) CL: Convolutional Layer + Relu Activation function, 2) MPL: Maximum 2D",
      "page": 4
    },
    {
      "caption": "Figure 6: FSER ROC-AUC on 8-emotions Recognition and",
      "page": 4
    },
    {
      "caption": "Figure 5: presents the architecture of FSER. The input is",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotion class": "Anger\nAnxiety\nCalm\nDisgust\nHappiness\nNeutral\nSadness\nSurprise",
          "Amount of data (%)": "463 (15.22%)\n405 (13.31%)\n273 (8.97%)\n382 (12.55%)\n407 (13.37%)\n379 (12.45%)\n398 (13.09%)\n336 (11.04%)"
        },
        {
          "Emotion class": "Total",
          "Amount of data (%)": "3043 (100%)"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Predicted Emotions": "Anger"
        },
        {
          "Predicted Emotions": "96.92\n0.53\n0.00\n0.88\n1.72\n0.37\n0.41\n0.43"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Speech Dataset": "Emodb",
          "Features": "Mel-Spectrogram (*)",
          "Accuracies (%)": "*"
        },
        {
          "Speech Dataset": "-\n-\n+\n✓\n-\n-",
          "Features": "-\n+\n+\n✓\n✓\n✓",
          "Accuracies (%)": "-\n86\n95.33\n97.74\n99.10\n98.67"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Convolutional neural network (cnn) based speech-emotion recognition",
      "authors": [
        "A Abdul Qayyum",
        "A Arefeen",
        "C Shahnaz"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Signal Processing, Information, Communication Systems (SPIC-SCON)"
    },
    {
      "citation_id": "2",
      "title": "Perception of the emotional content of speech: A comparison of two canadian groups",
      "authors": [
        "C Daniel",
        "Ken Albas",
        "Cheryl Mccluskey",
        "Albas"
      ],
      "year": "1976",
      "venue": "Journal of Cross-Cultural Psychology"
    },
    {
      "citation_id": "3",
      "title": "Time-Frequency Signal Analysis and Processing: A Comprehensive Reference",
      "authors": [
        "B Boashash"
      ],
      "year": "2003",
      "venue": "Time-Frequency Signal Analysis and Processing: A Comprehensive Reference"
    },
    {
      "citation_id": "4",
      "title": "The Fourier Transform and Its Applications",
      "authors": [
        "R Bracewell"
      ],
      "year": "2000",
      "venue": "The Fourier Transform and Its Applications"
    },
    {
      "citation_id": "5",
      "title": "Franc ¸ois Chollet. Deep Learning with Python. Manning",
      "year": "2017",
      "venue": "Franc ¸ois Chollet. Deep Learning with Python. Manning"
    },
    {
      "citation_id": "6",
      "title": "The relationship between the inner speech and emotions: Revisiting the study of passions in psychology",
      "authors": [
        "Pablo Fossa",
        "Raymond Madrigal Pérez",
        "Camila Marcotti"
      ],
      "year": "2020",
      "venue": "The relationship between the inner speech and emotions: Revisiting the study of passions in psychology"
    },
    {
      "citation_id": "7",
      "title": "Deep Learning",
      "authors": [
        "Ian Goodfellow",
        "Yoshua Bengio",
        "Aaron Courville"
      ],
      "year": "2016",
      "venue": "Deep Learning"
    },
    {
      "citation_id": "8",
      "title": "The effect of emotion on voice production and speech acoustics",
      "authors": [
        "Tom Johnstone"
      ],
      "year": "2017",
      "venue": "The effect of emotion on voice production and speech acoustics"
    },
    {
      "citation_id": "9",
      "title": "Funding Information Natural Sciences and Engineering Research Council of Canada: 2012-341583 Hear the world research chair in music and emotional speech from Phonak",
      "authors": [
        "Steven Livingstone",
        "Frank Russo"
      ],
      "year": "2018",
      "venue": "Funding Information Natural Sciences and Engineering Research Council of Canada: 2012-341583 Hear the world research chair in music and emotional speech from Phonak"
    },
    {
      "citation_id": "10",
      "title": "Sound frequency affects speech emotion perception: results from congenital amusia",
      "authors": [
        "Sydney Lolli",
        "Ari Lewenstein",
        "Julian Basurto",
        "Sean Winnik",
        "Psyche Loui"
      ],
      "year": "2015",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "11",
      "title": "Rectified linear units improve restricted boltzmann machines",
      "authors": [
        "Vinod Nair",
        "Geoffrey Hinton"
      ],
      "year": "2010",
      "venue": "Johannes Fürnkranz and Thorsten Joachims"
    },
    {
      "citation_id": "12",
      "title": "Perception of emotions in speech. a review of psychological and physiological research",
      "authors": [
        "O Kislova",
        "M Rusalova"
      ],
      "year": "2013",
      "venue": "Usp Fiziol Nauk"
    },
    {
      "citation_id": "13",
      "title": "The effectiveness of data augmentation in image classification using deep learning",
      "authors": [
        "Luis Perez",
        "Jason Wang"
      ],
      "year": "2017",
      "venue": "The effectiveness of data augmentation in image classification using deep learning"
    },
    {
      "citation_id": "14",
      "title": "A stochastic approximation method",
      "authors": [
        "Herbert Robbins",
        "Sutton Monro"
      ],
      "venue": "Ann. Math. Statist"
    },
    {
      "citation_id": "15",
      "title": "An overview of gradient descent optimization algorithms",
      "authors": [
        "Sebastian Ruder"
      ],
      "year": "2016",
      "venue": "An overview of gradient descent optimization algorithms"
    },
    {
      "citation_id": "16",
      "title": "Personality, emotion, psychopathology and speech",
      "authors": [
        "K Scherer"
      ],
      "year": "1980",
      "venue": "Language"
    },
    {
      "citation_id": "17",
      "title": "Emotion recognition from speech",
      "authors": [
        "Kannan Venkataramanan",
        "Haresh Rengaraj Rajamohan"
      ],
      "year": "2019",
      "venue": "Emotion recognition from speech"
    },
    {
      "citation_id": "18",
      "title": "Understanding data augmentation for classification: when to warp?",
      "authors": [
        "Sebastien Wong",
        "Adam Gatt",
        "Victor Stamatescu",
        "Mark Mcdonnell"
      ],
      "year": "2016",
      "venue": "Understanding data augmentation for classification: when to warp?"
    },
    {
      "citation_id": "19",
      "title": "Max-pooling dropout for regularization of convolutional neural networks",
      "authors": [
        "Haibing Wu",
        "Xiaodong Gu"
      ],
      "year": "2015",
      "venue": "Max-pooling dropout for regularization of convolutional neural networks"
    },
    {
      "citation_id": "20",
      "title": "Speech emotion recognition using deep 1d and 2d cnn lstm networks",
      "authors": [
        "Jianfeng Zhao",
        "Xia Mao",
        "Lijiang Chen"
      ],
      "year": "2019",
      "venue": "Biomedical Signal Processing and Control"
    }
  ]
}