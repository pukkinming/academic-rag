{
  "paper_id": "2412.01860v1",
  "title": "Pairwise Discernment Of Affectnet Expressions With Arcface",
  "published": "2024-12-01T10:18:55Z",
  "authors": [
    "Dylan Waldner",
    "Shyamal Mitra"
  ],
  "keywords": [
    "Computer Vision",
    "Affective Computing",
    "Human-Computer Interaction"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This study takes a preliminary step toward teaching computers to recognize human emotions through Facial Emotion Recognition (FER). Transfer learning is applied using ResNeXt, EfficientNet models, and an ArcFace model originally trained on the facial verification task, leveraging the AffectNet database-a collection of human face images annotated with corresponding emotions. The findings highlight the value of congruent domain transfer learning, the challenges posed by imbalanced datasets in learning facial emotion patterns, and the effectiveness of pairwise learning in addressing class imbalances to enhance model performance on the FER task.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "D EEP learning for visual classification tasks takes ad- vantage of contemporary explosions in computing power. As computing power has grown over the past 40 years, so have the applications for computer vision tasks, including but not limited to Facial Recognition, Object Detection, Handwriting Authentication, Autonomous Driving, and Image Generation. We will focus on the growing field of Facial Emotion Recognition (FER), which has made significant strides in controlled settings (models posing) but still faces substantial challenges in tasks \"in the wild,\" where lighting, camera angles, and resolution are not optimized. While FER certainly has applications in surveillance and security, this project is the first stepping stone towards work in communicating emotion to machines. This section will review the history of Image classification and Facial Emotion Recognition. Section 2 will describe the strengths and limitations of our dataset and the manipulation tactics we employed. Section 3 will explain the results of our experiments. Section 4 will describe the results of our experiments. Section 5 will detail sources of error and the short and long-term trajectories of Facial Emotion Recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "History",
      "text": "Deep learning for image classification traces its origins to the Neocognitron  [1] , developed by Kunihiko Fukushima in 1980. The Neocognitron was modeled after Hubel and Wiesel's studies on human visual processing  [2]  [3]  [4] , which described simple and complex cells that capture patterns across multiple layers of abstraction. It implemented a network that processed small patches of input images and passed data from simple to complex cells repeatedly to form S-layers, or \"feature maps,\" which represent specific patterns or features and ultimately produced an image classification output.\n\nWhile the Neocognitron is widely considered the first Convolutional Neural Network (CNN), Yann LeCun's LeNet  [5]  expanded on its design to establish the modern CNN architecture. LeNet processed 32x32 input images of hand-drawn numbers [0-9] and classified them as the correct integer with 99% accuracy. LeNet accomplished this by introducing a sliding filter with shared weights that \"convolved\" over the input data, enhancing pattern recognition robustness through invariant feature detection. This sliding layer has become the foundation for the modern CNN. Building on the Neocognitron's data abstraction, LeNet incorporated pooling layers, where subsampling captured more general patterns in the image and reduced overfitting. LeNet's most lasting contribution was the addition of a fully connected layer-a dense matrix operation where input weights are connected to all output weights. LeCun included the fully connected layer in anticipation of advances in computing power, which he believed would enable dense layers to detect intricate patterns. However, the limitation at the time was that contemporary computing power remained relatively constrained, and with it, so did LeNet's broader capabilities.\n\nAs computing power increased, GPU performance and availability breakthroughs led to an explosion of deep learning models for image classification in the mid-2010s. At the forefront was AlexNet  [6] , which processed 224x224 input images from ImageNet and achieved a top-5 classification accuracy rate of 85%. AlexNet introduced several innovations that became foundational in contemporary deep learning, including the ReLU activation layer, which enabled non-linear decision boundaries; Local Response Normalization, a precursor to Batch Normalization that enhanced contrast in feature maps; Dropout, a regularization technique that randomly deactivated neurons during training to reduce overfitting; and data augmentation techniques combined with multi-GPU architectures that set new standards for scalability and performance.\n\nHowever, AlexNet falls short compared to modern im-age classification models due to the computing limitations of its time, which restricted the network to 8 layers and constrained its ability to identify complex patterns. Despite these limitations, AlexNet paved the way for a series of models that competed for top performance on the ImageNet data set  [7] , processing 224x224 input images and classifying them into 1 of 5,247 categories. Network in Network  [8]  introduced non-linearity within the filters by incorporating Multi-Layer Perceptrons (MLPs), enhancing the network's expressiveness. VGGNets  [9]  revamped AlexNet's hyperparameters and restructured the GPU architecture into an ensemble, doubling the network's depth while maintaining the same number of parameters. GoogLeNet  [10] , named in homage to LeNet, introduced the inception block, which improved computational efficiency and pattern recognition by leveraging dimension reduction and sparse matrices, enabling neurons to focus on capturing the most relevant features. GoogLeNet also introduced softmax classification blocks at intermediate stages of the network architecture to reinforce gradient strength and address the vanishing gradient problem.\n\nResNet  [11]  revolutionized deep learning by leveraging identity mapping, which adds the input of a block directly to its output. This technique allowed the model to focus on the residual-or the change-within an image as it passes through the model, leading to more focused and manageable learning. A key benefit of residual mapping was improved gradient strength, which, combined with GoogLeNet's intermediary classification blocks, effectively mitigated the vanishing gradient issue. The overarching theme of these mid-decade models was achieving greater network depth while maintaining a fixed number of parameters, resulting in higher accuracies. However, despite their strong performance in classification tasks, these models were so complex and memory-intensive that deployment costs and test speeds remained prohibitive for widespread use.\n\nThe next wave of CNNs for image classification prioritized optimization to address the high memory and computational costs of earlier models. Like their predecessors, these models were trained on ImageNet, processing 224x224 input images, but they achieved comparable accuracy with significantly reduced memory usage and computational demands. Notable examples include SqueezeNet  [12] , Squeeze-and-Excite  [13] , MobileNet  [14]    [15] , and Effi-cientNet  [16] . These models focused on compressing model size and complexity to make CNNs more practical for realworld applications by optimizing the relationship between model architecture and the hardware they operate on.\n\nSqueezeNet introduced techniques such as 8-bit quantization on sparse matrices, optimizing the sparse matrices introduced by GoogLeNet for GPUs, which specialize in dense matrix multiplication. MobileNet leveraged the relationship between parameter count and kernel dimension by splitting standard convolutional layers into Depthwise and Pointwise filters. This innovation maintained accuracy while cutting memory usage by a factor of ( 1\n\n), where N is the number of output channels and D K is the kernel dimension. These advances marked a shift toward creating lightweight, efficient architectures suitable for deployment on devices with limited computational resources, such as smartphones and embedded systems.\n\nIn the modern era, the focus of image classification has shifted from relying solely on CNNs to incorporating Vision Transformers (ViTs) and hybrid models. This transition was pioneered by the seminal paper \"Attention Is All You Need\"  [17] , which introduced transformers as a framework for handling sequential data. Building on this foundation, Google repurposed transformers for image classification in their groundbreaking work  [18] , driven by the intuition that the transformer's superior computational efficiency and scalability could allow it to perform equivalently to, or even surpass, CNNs in visual recognition tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Research Statement",
      "text": "The goal of our research was to differentiate between emotions in images of peoples' faces. We used the AffectNet dataset and three different Convolutional Neural Networks. We employed transfer learning on the CNNs to classify images into one of the eight AffectNet classes. Our study has implications for research in facial emotion recognition, human-computer interaction, cognitive behavioral therapy, image and video generation, and security.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Data",
      "text": "We used the AffectNet Database  [19]  to train our models and run experiments. More specifically, we selected a subset of the AffectNet database consisting of ≈ 281,000 images, each annotated in four different ways: Expression, Arousal, Valence, and Landmarks. Expression assigned a label corresponding to the AffectNet emotion classes: [Neutral, Happy, Sad, Disgust, Fear, Anger, Surprise, and Contempt]. Arousal assigned a value in [-1, +1], reflecting whether an expression is exciting/agitating or calm/soothing. Valence assigned a value in [-1 + 1], reflecting how positive or negative an expression is (e.g., happy vs. sad, angry vs. neutral). Landmark annotated the features on each image's face, providing a numerical representation of facial features. The larger AffectNet dataset included classes \"Non-Face,\" \"None,\" and \"Uncertain.\" We cleaned our dataset of these classes, yielding a much more refined dataset on which to perform classification.\n\nWe chose the AffectNet database for its size, its annotations, and the diversity of people represented. The AffectNet database was constructed from images scraped from Google Images, meaning they were photos posted by ordinary people online, rather than professionally taken images. This type of photo is referred to as \"in the wild.\" This placed work using the database at the forefront of the FER challenge due to the inherent noise in the dataset caused by variations in picture angle and lighting. The challenge was further compounded by the imbalance of emotional representation \"in the wild\"; people were more likely to post happy photos of themselves and others online than photos depicting sadness or disgust. As a result of this trend, the cleaned AffectNet dataset we worked with had the class distribution summarized in Table  1 .\n\nDealing with this significant skew was challenging because the dataset accurately represented the distribution of images on the internet. A generalized model deployed on the images \"in the wild\" needed to account for this imbalance. However, this class imbalance disrupts the model from learning the underlying facial emotion feature patterns, preventing true Facial Emotion Recognition. In Section 4 we discuss our solution to this contradiction. A significant limitation of the dataset was training a model to sufficiently differentiate between minorityrepresented classes and between a minority represented class and a majority class. For example, differentiating between fear, disgust, and contempt posed a major challenge, as these three classes combined to just 14,000 photos, representing only ≈ 5% of the dataset. This meant that a model trained on this dataset had a bias towards ≈ 95% of its data that from the five major classes and needed to learn to differentiate between three similar classes that comprised only ≈ 5% of the dataset. In practice, the model tended to over-predict the majority classes, often classifying minority classes as one of the majority classes. In this paper, we aimed to generalize the model to the underlying patterns associated with each emotion independent of the noise and imbalance that results from \"in the wild\" data.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Resnext And Efficientnet",
      "text": "The ResNeXt101-32x4d  [20]  and EfficientNet-b0  [16]  models we used were originally trained on the ImageNet dataset  [7] . ImageNet consisted of 3.2 million images from the internet, spanning 5,247 classes. This was achieved through a massive collection of internet images, which humans then verified using Amazon Mechanical Turk. Multiple humans verified each image to reach a consensus, particularly for niche classes. We applied transfer learning to these models, transitioning from a general image classification task to the facial emotion recognition task.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Arcface",
      "text": "The ArcFace model we used was trained on Microsoft's MS1MV2 facial verification dataset  [21] . Composed of ≈ 10 million images of ≈ 100,000 celebrities, the dataset included individuals from 2,000 different professions across 200 countries, with a wide range of ages and a higher proportion of women compared to men 1  . Images were scraped from the web using keyword searches and were labeled by the MS1M authors. We applied transfer learning to this model, facilitating a more natural transition from the facial verification task to the facial emotion recognition task.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Related Works",
      "text": "Our work followed the trend of Facial Recognition tasks in Image Classification. We built on previous experiments that utilized the AffectNet database, including studies by Ngo and Yoon  [22] , Deng et al.  [23] , and Siddiqui et al.  [24] .\n\nNgo and Yoon introduced the weighted-cluster loss function, which aimed to improve upon weighted loss functions designed for imbalanced datasets. Among these weighted loss functions was weighted softmax, which penalized misclassifications of minority classes more than majority classes. However, weighted softmax fell short because it could not handle high inter-class similarities and intraclass variations. Ngo and Yoon demonstrated that center loss could be incorporated alongside softmax to reduce intra-class variation by drawing samples closer to their class center. Center loss also fell short, however, as major classes were updated more frequently than minor classes, limiting its ability to tighten minority clusters effectively.\n\nNgo and Yoon showed that weighted-cluster loss addressed the imbalanced class problem left by center loss by adding a new term that simultaneously pulled the centers of each cluster apart while drawing each sample within the cluster closer to its center. Their paper demonstrated that weighted-cluster loss made each cluster more compact and distinct, which should have resulted in improved classification. They deployed a weighted-softmax function on the rearranged cluster feature space to classify the logits into a final prediction.\n\nBuilding on Ngo and Yoon's weighted-cluster loss function, we extended our research to ArcFace and Angular Additive Margin  [23] . Angular Additive Margin followed the precedent set by Triplet loss and Center loss, enhancing intra-class compactness and increasing inter-class separation in the feature space. Deng et al. demonstrated that Angular Additive Margin rearranged the feature space before classification, simplifying the drawing of decision boundaries by creating more distinct and well-defined features.\n\nAngular Additive Margin (AAM) is derived from the softmax function:\n\nand defined by:\n\nIn Softmax, the term W T j x i measures the similarity between the weight vector W j and the feature vector x i . In ArcFace, this term is modified to ||W j ||||x i || cos(θ j ), where both W j and x i are normalized to 1, simplifying to cos(θ j ). ArcFace introduced two additional parameters: m, an additive angular margin that increased the separation between classes in the embedding space, and s, the scaling factor that controlled the radius of the hypersphere to which the features are normalized and projected onto. Normalizing the features to the surface of the hypersphere allowed predictions to depend only on the angle between the weights W and x the features, measured by cos(θ j ). When this feature space was passed on to the final classification, the feature vectors were more distinct from each other, making decision boundaries easier to draw.\n\nFinally, we followed the baselines set by the AffectNet paper  [19] , which we will discuss in Section 3.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Tools For Analysis",
      "text": "We aimed to improve the baselines established by the AffectNet paper by implementing and combining various approaches into a single model. Initially, we conducted a series of experiments with a ResNeXt101-32x4d CNN  [20]  and then transitioned to an EfficientNet-B0 CNN  [16]  from TorchHub, which demonstrated higher efficiency and accuracy compared to ResNeXt. We removed the final fully connected layers from both models and replaced them with four layers in a multi-output module, each employing a fully connected layer for a specific annotation: arousal, valence, expression, and landmarks. We froze the weights for the entire model except for the expression classifier. The losses from the four output layers were added together to train a fully connected classification layer for expression. We found that model accuracy decreased when only the expression classifier's loss was used. In our initial experiments, we tested three different loss functions as outlined in the original AffectNet paper: Mean Squared Error (MSE), Pearson Correlation Loss, and Signed MSE Loss.  [19]  Both models were pre-trained on ImageNet, so we normalized the images based on ImageNet's standards of mean and standard deviation (std): mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]. To match the expected input dimensions, we resized the images to 224x224 for the ResNeXt and EfficientNet models. We applied random horizontal flips as well as random horizontal and vertical translations to augment the data for minority classes. We used the ADAM optimizer with a Reduce On Plateau (ROP) learning rate scheduler. For ResNeXt and EfficientNet, the learning rate scheduler was configured to start at 0.128 and decrease by a factor of 10 when the training loss plateaued for five epochs.\n\nAfter the initial experiments, we switched to a ResNet100 model pre-trained on the facial recognition dataset MS1MV2. This model was trained using Angular Additive Margin, which was specifically designed for the facial recognition task-a task inherently more complex than facial emotion recognition. For this model, we initially set the learning rate scheduler to the same parameters as the ResNeXt and EfficientNet models. However, through experimentation, we adjusted to an initial learning rate of 0.01 with a patience of 5 and a factor of 0.25. We once again experimented with data augmentation techniques, including color jitter, random transformations, rotations, and reflections. Input images were scaled to 112x112 to match the expected input dimensions.\n\nWe experimented with calculating the valence, landmark, and arousal loss using Signed Mean Squared Error and Pearson Correlation Loss. Signed Mean Squared Error accounted for the direction of the error, which was critical, particularly for valence and arousal, as their direction determined the quality of the expression (e.g., excitement, boredom) and the magnitude of the assigned value represented the intensity of the emotion. For this reason, for an actual value of 0.3, a predicted value of -0.1 was not as accurate as a predicted value of 0.7, even though the absolute distance between the points was identical. Pearson Correlation Loss took a similar approach by measuring the correlation between the predicted and ground truth values. To function as a loss metric, we subtracted the Pearson Correlation from 1 to maximize the correlation. We determined Signed MSE yielded slightly better results, and so all of our reported results used Signed MSE error.\n\nThe ResNeXt model was accessed from TorchHub; the EfficientNet model was accessed from Torchvision. The ArcFace backbone and weights were downloaded from the InsightFace GitHub Repo. All training and test runs were done on the UT Servers using HTCondor.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Initial Results",
      "text": "The initial results of the training loops run on the AffectNet dataset were very promising. Starting at ≈ 50% validation accuracy for the ResNeXt model, we got up to ≈ 60% validation accuracy with the EfficientNet model. We moved on from these ImageNet-trained, stock CNNs, and repurposed the pretrained ArcFace model available from InsightFace to leverage its success in Facial Recognition into the Facial Emotion Recognition task. Initial tests were run with 40 epoch training loops and a 0.01 LR that was reduced 10x after five epochs of plateau.\n\nHowever, what became apparent very early on was the issue the class imbalance in the dataset posed to the model's robustness. While the largest class, Happiness, boasted a recall > 92%, the three least represented classes (Fear, Disgust, and Contempt) regularly had precision, recall, and F1-scores < 12%. Table  2  illustrates these disparities that manifested during training and validation time. To balance the data, we experimented with a weighted random sampler in the dataloader to even out class representations by using the reciprocal of the class count for weights: 1/size(class). We also applied transformations to the minority classes to maintain data diversity, utilizing the Torchvision transforms library to implement these changes.\n\nWe tested affine transformations with random 10% horizontal and vertical translations and random 2-degree rotations.\n\nColor jitters were configured with a 10% random brightness change, 20% random contrast change, 15% random saturation change, and 5% random hue change. Random horizontal flips occurred with a 50% probability. However, despite these efforts, the imbalanced dataset persisted in the model's representations of facial emotions.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Capturing Facial Emotion Patterns",
      "text": "The various approaches we employed were unsuccessful in enabling the model to accurately capture the underlying facial emotion patterns, primarily due to significant class imbalances in the training data. This limitation was evident in Table  2 , where the recall scores for majority classes were disproportionately high, highlighting the model's tendency to overpredict these classes. While the model achieved high accuracies for majority classes, this performance was driven by a strong bias toward those classes rather than a genuine understanding of the facial features associated with emotions like happiness or neutrality. This bias became particularly apparent during testing, where we evaluated the model using the balanced working test set 2 from the AffectNet dataset, as summarized in Table  3 .\n\nThe training dataset exhibited a significant class imbalance, reflecting the biases inherent in how people share images of themselves and others online. However, the test dataset did not account for this imbalance. By equalizing all classes, the test dataset shifted its focus away from evaluating the model's ability to discern between facial expressions \"in the wild\" and instead assessed its ability to distinguish facial emotions in isolation. As a result, a significant drop in performance was observed when comparing the training and validation accuracies, where the model learned the \"in the wild\" task, to the test accuracies, where the model was evaluated on true emotion discernment. These results are summarized in Table  4 .\n\nTable  4  presents two distinct accuracy metrics. The balanced test dataset accuracy represents the model's performance on a dataset with equal representation of all classes, designed to assess its ability to distinguish emotions under 2. The official test set is withheld for competitions standardized conditions. In contrast, the skewed test dataset accuracy evaluates the model's performance on a dataset with the class distribution shown in Table  3 , which was adjusted to reflect the natural class imbalance found in the training data and \"in the wild.\" This skewed test set was specifically designed to measure the model's effectiveness on the \"in the wild\" FER task, rather than on purely balanced FER discernment.\n\nTo conduct this analysis, we trained and optimized two models. One model was trained and tuned on an artificially balanced AffectNet dataset using a weighted random sampler to equalize class sampling probabilities. A maximum of 2 × size(smallestclass) samples were selected with replacement. To enhance the diversity of minority classes (Fear, Disgust, and Contempt) and mitigate issues with duplicate samples, these classes were augmented with random affine transformations, including up to 10% horizontal and vertical translations and random rotations. Additionally, a 50% probability of horizontal flipping was applied. These augmentations were carefully designed to introduce meaningful diversity without distorting the features of the original images. Our approach maximized the size of the dataset while retaining balanced class representation.\n\nThe model performed significantly better on the \"in the wild\" test set, confirming its ability to generalize to realworld emotions. However, its performance on the balanced test dataset was similarly poor regardless of whether it was trained on the natural dataset or an artificially balanced dataset. This lack of improvement indicates that broadly re-balancing the training set is insufficient for training the model effectively on the FER task. Re-balancing efforts failed because reducing the training data to equally represent minority classes resulted in an overly condensed dataset, leaving insufficient data for the model to generalize effectively to facial emotion features. Addressing these discrepancies between different facial emotion features required a more targeted and radical approach.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Pairwise Discernment",
      "text": "Due to the massive class imbalances, we determined that a model trained on the AffectNet dataset was best suited to handle pure emotion discernment by distinguishing between pairs of emotions at a time. The motivation for pairwise discernment is inspired by the way people can more accurately identify or describe a musical note when comparing it directly to another, highlighting the power of relative distinctions over isolated recognition. We grouped emotions into pairs (e.g., happy and neutral, happy and disgusted) and evaluated the model trained with the ArcFace architecture fine-tuned on the AffectNet dataset. To balance the classes within each pair, we weighed them equally and performed random sampling 2 × min(class1size, class2size) times, ensuring equal representation between classes in each pair. While the results still reflected the biases of the generally trained model toward the larger classes, pairwise discernment proved to be a more effective task for the model. The results are summarized in Table  5 . To further optimize pairwise discernment, we implemented a dictionary of fully connected (FC) layers, each assigned to a specific pair, to learn the nuances of the differences between pairs. The first implementation utilized the generally trained model by appending the pair-specific FC layer to the general FC layer. The second implementation removed the FC layer fine-tuned on AffectNet, operating under the intuition that the general model's learned biases could influence the pairwise FC layers. Both implementations were trained for 30 epochs with a learning rate of 0.0001, a learning rate scheduler that reduced the rate by a factor of 0.25 after a five-epoch plateau, a weight decay of 0.0005, a batch size of 256, and the ADAM optimizer.\n\nTheoretically, using a dictionary of FC layers-one dedicated to each class pair-should result in a better architecture for detecting true facial emotion features. This design focuses exclusively on the two classes being compared, enhancing the model's ability to distinguish between them. In contrast, the single FC layer model performs a standard forward pass, extracting logits for all classes but only considers the logits for the relevant pair during evaluation. However, this approach does not guarantee that the model is learning to differentiate based on meaningful facial emotion features; it could simply produce higher logits for the correct class by chance, even if both logits are low.\n\nIn practice, the fully connected layer dictionary approach performed worse than the general model. Table  5  summarizes the comparison, showing that while highly represented classes benefited from the FC dictionary approach, the pairwise dictionary struggled to effectively learn minorityrepresented classes. This limitation arose because the dense layers in the pairwise dictionary lacked sufficient data for training on minority class pairs, whereas the general model was trained on the entire AffectNet dataset. As a result, performance dropped by up to 27%, underscoring the challenges of this approach in addressing class imbalances.\n\nTable  6  summarizes the general model's class accuracies for each pair. Since the classes are balanced against each other, high metrics for both classes suggest that the model effectively recognizes genuine facial emotions, reducing the influence of \"in the wild\" biases. However, not all pairs performed equally well. The pairwise discernment scale provides insight into which emotion pairs the model can effectively differentiate.\n\nNotably, the model consistently distinguished between Happy + Sad faces as well as Happy + Angry faces, achieving strong performance even on new data. An unexpected result was the model's ability to discern between the three minority classes: Fear + Contempt, Fear + Disgust, and Contempt + Disgust. The high metrics for these pairs suggest that the model can reliably differentiate between the minority classes despite the limited data.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "Our work introduced a novel approach by applying pairwise discernment to the \"in the wild\" FER AffectNet dataset, achieving accuracies as high as 92%. This approach highlighted the challenges of distinguishing multiple emotions simultaneously in images sourced from the internet, where natural biases influence the types of photos people post of themselves and others. Pairwise discernment enabled the model to effectively differentiate between three minority classes (Fear, Contempt, and Disgust) that were indistinguishable during general classification tasks. This enhanced ability to distinguish minority classes suggests that pairwise discernment allows the model to learn underlying facial emotion features that remain elusive in broader classification tasks. We conclude that pairwise discernment is a powerful tool for addressing data set imbalances and capturing meaningful patterns by reducing task complexity and improving model generalization.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Sources Of Error",
      "text": "The models performed very well on majority classes but struggled significantly with minority classes. In this section, we discuss the potential sources of error contributing to this discrepancy and analyze where our approach fell short.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Imbalanced Data Set",
      "text": "An early source of error in the project was the heavy class imbalance in the dataset. Nearly half of the training data belonged to one of the eight classes (see Table  1 ) (Happy), and 80% of the data fell into the two majority classes (Happiness and Neutral). Consequently, the three minority classes combined to account for only 5% of the training data. This imbalance occurred naturally due to the proportion of photos published \"in the wild\"; people were more likely to post photos of themselves when they were happy than when they were, for example, afraid.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Subjectivity Of Facial Expressions",
      "text": "The annotations provided by AffectNet were invaluable for training the model to recognize underlying patterns and quantify various facial expressions. However, a key challenge lies in the subjectivity of these annotations. According to the AffectNet paper: \"To measure the agreement between the annotators, 36,000 images were annotated by two annotators. The annotations were performed fully blind and independently, i.e., the annotators were unaware of the intended query or the other annotator's response. The results showed that the annotators agreed on 60.7% of the images.\"  [19]  This relatively low agreement rate highlights inconsistencies in how annotations were assigned, suggesting that the ground truth labels may not adhere to a consistent standard. Consequently, these inconsistencies could have disrupted the model's ability to reliably identify patterns in the data.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Short Term Facial Emotion Recognition",
      "text": "In the short term, Facial Emotion Recognition (FER) is expected to continue advancing significantly, driven by larger datasets with standardized annotations. While this project demonstrated relative success with a refined and intentional dataset, its performance \"in the wild\" remains unclear. Datasets like the CFP-FP dataset  [25] , which is used for facial verification from both frontal and profile angles, could be adapted for facial emotion recognition. Similarly, video datasets, such as the facial verification dataset Labeled Faces in the Wild (LFW)  [26] , could be developed for FER, enabling models to predict emotions dynamically. Realtime emotion detection has the potential to bring significant benefits, such as enhancing behavioral therapy by allowing researchers to process patient emotions quickly and autonomously.\n\nAdditionally, datasets and experiments based on annotations like Valence and Arousal, rather than rigid classes such as emotion expressions (e.g., happy, sad), would better capture the range of human emotions. Expression classes, as used in this project, remain too broad; for instance, an embarrassed laugh and a gleeful laugh are both categorized as \"happy,\" yet they represent significantly different mental states.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Long-Term Future For Machine Learning Emotion",
      "text": "Image classification represents only the first step in enabling computers to understand and communicate emotion more broadly. The disparity between observable behavior and internal mental states remains substantial, and teaching models to classify behavior does not equate to teaching them emotion. Bayesian Neural Networks, with their capacity for world modeling, offer promising progress toward representing and communicating more complex mental and emotional states.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "age classification models due to the computing limitations",
          "2": "with limited computational resources, such as smartphones"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "of\nits\ntime, which restricted the network to 8 layers and",
          "2": "and embedded systems."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "constrained its ability to identify complex patterns. Despite",
          "2": "In the modern era,\nthe focus of\nimage classification has"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "these\nlimitations, AlexNet paved the way for a series of",
          "2": "shifted from relying solely on CNNs\nto incorporating Vi-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "models that competed for top performance on the ImageNet",
          "2": "sion Transformers (ViTs) and hybrid models. This transition"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "data set [7], processing 224x224 input images and classifying",
          "2": "was pioneered by the seminal paper ”Attention Is All You"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "them into 1 of 5,247 categories. Network in Network [8]",
          "2": "Need” [17], which introduced transformers as a framework"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "introduced non-linearity within the filters by incorporating",
          "2": "for handling sequential data. Building on this foundation,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Multi-Layer Perceptrons (MLPs), enhancing the network’s",
          "2": "Google repurposed transformers for image classification in"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "expressiveness. VGGNets\n[9]\nrevamped AlexNet’s hyper-",
          "2": "their groundbreaking work [18], driven by the\nintuition"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "parameters and restructured the GPU architecture into an",
          "2": "that the transformer’s superior computational efficiency and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "ensemble, doubling the network’s depth while maintaining",
          "2": "scalability could allow it to perform equivalently to, or even"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "the same number of parameters. GoogLeNet\n[10], named",
          "2": "surpass, CNNs in visual recognition tasks."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "in homage to LeNet,\nintroduced the inception block, which",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "improved computational efficiency and pattern recognition",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "2": "1.2\nResearch Statement"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "by leveraging dimension reduction and sparse matrices,",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "enabling neurons to focus on capturing the most\nrelevant",
          "2": "The goal of our research was to differentiate between emo-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "features. GoogLeNet also introduced softmax classification",
          "2": "tions\nin images of peoples’\nfaces. We used the AffectNet"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "blocks at\nintermediate stages of\nthe network architecture",
          "2": "dataset and three different Convolutional Neural Networks."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "to reinforce gradient\nstrength and address\nthe vanishing",
          "2": "We\nemployed transfer\nlearning on the CNNs\nto classify"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "gradient problem.",
          "2": "images into one of\nthe eight AffectNet classes. Our study"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "2": "has implications for research in facial emotion recognition,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "ResNet\n[11] revolutionized deep learning by leveraging",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "2": "human-computer interaction, cognitive behavioral\ntherapy,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "identity mapping, which adds the input of a block directly",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "2": "image and video generation, and security."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "to its output. This\ntechnique allowed the model\nto focus",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "on\nthe\nresidual—or\nthe\nchange—within\nan\nimage\nas\nit",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "passes\nthrough the model,\nleading to more\nfocused and",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "2": "2\nDATA"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "manageable\nlearning. A key benefit of\nresidual mapping",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "2": "We used the AffectNet Database [19]\nto train our models"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "was\nimproved gradient\nstrength, which,\ncombined with",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "2": "and run experiments. More specifically, we selected a subset"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "GoogLeNet’s intermediary classification blocks, effectively",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "2": "of\nthe AffectNet database consisting of ≈ 281,000 images,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "mitigated the vanishing gradient\nissue. The\noverarching",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "2": "each annotated in four different ways: Expression, Arousal,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "theme of\nthese mid-decade models was achieving greater",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "2": "Valence, and Landmarks. Expression assigned a label corre-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "network depth while maintaining a fixed number of param-",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "2": "sponding to the AffectNet emotion classes: [Neutral, Happy,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "eters, resulting in higher accuracies. However, despite their",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "2": "Sad, Disgust, Fear, Anger, Surprise, and Contempt]. Arousal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "strong performance\nin\nclassification\ntasks,\nthese models",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "2": "assigned a\nvalue\nin [−1, +1],\nreflecting whether\nan ex-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "were so complex and memory-intensive that deployment",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "2": "pression is\nexciting/agitating\nor\ncalm/soothing. Valence"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "costs and test speeds remained prohibitive for widespread",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "2": "assigned a value\nin [−1 + 1],\nreflecting how positive or"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "use.",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "2": "negative an expression is\n(e.g., happy vs.\nsad, angry vs."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "The next wave of CNNs\nfor\nimage\nclassification pri-",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "2": "neutral). Landmark annotated the features on each image’s"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "oritized optimization\nto\naddress\nthe\nhigh memory\nand",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "2": "face, providing a numerical representation of facial features."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "computational costs of earlier models. Like their predeces-",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "2": "The larger AffectNet dataset\nincluded classes ”Non-Face,”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "sors,\nthese models were trained on ImageNet, processing",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "2": "”None,” and ”Uncertain.” We cleaned our dataset of\nthese"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "224x224 input\nimages, but\nthey achieved comparable ac-",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "2": "classes, yielding a much more refined dataset on which to"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "curacy with significantly reduced memory usage and com-",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "2": "perform classification."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "putational demands. Notable examples include SqueezeNet",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "2": "We\nchose\nthe AffectNet database\nfor\nits\nsize,\nits\nan-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "[12], Squeeze-and-Excite [13], MobileNet [14] [15], and Effi-",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "2": "notations,\nand the diversity\nof people\nrepresented. The"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "cientNet [16]. These models focused on compressing model",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "2": "AffectNet database was constructed from images\nscraped"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "size and complexity to make CNNs more practical for real-",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "2": "from Google\nImages, meaning they were photos posted"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "world applications by optimizing the relationship between",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "2": "by ordinary people online, rather than professionally taken"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "model architecture and the hardware they operate on.",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "2": "images. This type of photo is referred to as ”in the wild.”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "SqueezeNet introduced techniques such as 8-bit quanti-",
          "2": "This placed work using the database\nat\nthe\nforefront of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "zation on sparse matrices, optimizing the sparse matrices in-",
          "2": "the FER challenge due to the inherent noise in the dataset"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "troduced by GoogLeNet for GPUs, which specialize in dense",
          "2": "caused by variations\nin picture\nangle\nand lighting. The"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "matrix multiplication. MobileNet leveraged the relationship",
          "2": "challenge was\nfurther\ncompounded by the\nimbalance of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "between parameter count and kernel dimension by splitting",
          "2": "emotional representation ”in the wild”; people were more"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "standard convolutional\nlayers into Depthwise and Pointwise",
          "2": "likely to post happy photos of themselves and others online"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "filters. This innovation maintained accuracy while cutting",
          "2": "than photos depicting sadness or disgust. As a result of this"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "1D\n( 1\n), where N is the",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "2K\nmemory usage by a factor of\nN +",
          "2": "trend,\nthe cleaned AffectNet dataset we worked with had"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "number of output channels and DK is the kernel dimension.",
          "2": "the class distribution summarized in Table 1."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "These advances marked a shift toward creating lightweight,",
          "2": "Dealing with this significant skew was challenging be-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "efficient architectures\nsuitable for deployment on devices",
          "2": "cause the dataset accurately represented the distribution of"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "TABLE 1",
          "3": "2.3\nRelated Works"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "AffectNet Expression Classes, Counts, and Proportions",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "3": "Our work followed the trend of Facial Recognition tasks in"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "3": "Image Classification. We built on previous experiments that"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Expression Class\nCount\nProportion (%)",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "3": "utilized the AffectNet database,\nincluding studies by Ngo"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Neutral\n74874\n26.02",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "3": "and Yoon [22], Deng et al. [23], and Siddiqui et al. [24]."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Happy\n134415\n46.72",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "3": "Ngo\nand Yoon\nintroduced\nthe weighted-cluster\nloss"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Sad\n25459\n8.85",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Surprise\n14090\n4.90",
          "3": "function, which\naimed\nto\nimprove\nupon weighted\nloss"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Fear\n6378\n2.22",
          "3": "functions designed for\nimbalanced datasets. Among these"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Disgust\n3803\n1.32",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "3": "weighted loss functions was weighted softmax, which pe-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Anger\n24882\n8.65",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "3": "nalized misclassifications of minority classes more than ma-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Contempt\n3750\n1.30",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "3": "jority classes. However, weighted softmax fell short because"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "3": "it could not handle high inter-class\nsimilarities and intra-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "images on the internet. A generalized model deployed on",
          "3": "class variations. Ngo and Yoon demonstrated that\ncenter"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "the images ”in the wild” needed to account\nfor this imbal-",
          "3": "loss\ncould be\nincorporated alongside\nsoftmax\nto\nreduce"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "ance. However, this class imbalance disrupts the model from",
          "3": "intra-class variation by drawing samples closer to their class"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "learning\nthe underlying\nfacial\nemotion\nfeature patterns,",
          "3": "center. Center loss also fell short, however, as major classes"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "preventing true Facial Emotion Recognition. In Section 4 we",
          "3": "were updated more frequently than minor classes,\nlimiting"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "discuss our solution to this contradiction.",
          "3": "its ability to tighten minority clusters effectively."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "A significant\nlimitation\nof\nthe\ndataset was\ntraining",
          "3": "Ngo and Yoon showed that weighted-cluster\nloss ad-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "a model\nto\nsufficiently\ndifferentiate\nbetween minority-",
          "3": "dressed the imbalanced class problem left by center loss by"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "represented classes\nand between a minority represented",
          "3": "adding a new term that simultaneously pulled the centers"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "class and a majority class. For example, differentiating be-",
          "3": "of each cluster apart while drawing each sample within the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "tween fear, disgust, and contempt posed a major challenge,",
          "3": "cluster closer\nto its center. Their paper demonstrated that"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "as these three classes combined to just 14,000 photos, repre-",
          "3": "weighted-cluster loss made each cluster more compact and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "senting only ≈ 5% of\nthe dataset. This meant\nthat a model",
          "3": "distinct, which should have resulted in improved classifi-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "trained on this dataset had a bias\ntowards ≈ 95% of\nits",
          "3": "cation. They deployed a weighted-softmax function on the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "data that\nfrom the five major classes and needed to learn",
          "3": "rearranged cluster feature space to classify the logits into a"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "to differentiate between three similar classes that comprised",
          "3": "final prediction."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "only ≈ 5% of\nthe dataset.\nIn practice,\nthe model\ntended to",
          "3": "Building on Ngo and Yoon’s weighted-cluster loss func-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "over-predict the majority classes, often classifying minority",
          "3": "tion, we\nextended our\nresearch to ArcFace\nand Angular"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "classes\nas one of\nthe majority classes.\nIn this paper, we",
          "3": "Additive Margin [23]. Angular Additive Margin followed"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "aimed to generalize the model\nto the underlying patterns",
          "3": "the precedent set by Triplet loss and Center loss, enhancing"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "associated with each emotion independent of the noise and",
          "3": "intra-class\ncompactness and increasing inter-class\nsepara-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "imbalance that results from ”in the wild” data.",
          "3": "tion in the feature space. Deng et al. demonstrated that An-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "3": "gular Additive Margin rearranged the feature space before"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "2.1\nResNeXt and EfficientNet",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "3": "classification,\nsimplifying the drawing of decision bound-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "The ResNeXt101-32x4d [20] and EfficientNet-b0 [16] models",
          "3": "aries by creating more distinct and well-defined features."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "we used were originally trained on the ImageNet dataset",
          "3": "Angular Additive Margin (AAM)\nis derived from the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "[7].\nImageNet\nconsisted of\n3.2 million images\nfrom the",
          "3": "softmax function:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "internet, spanning 5,247 classes. This was achieved through",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "a massive collection of internet images, which humans then",
          "3": "xi+byi\neW T"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "3": "(1)\nLSof tM ax = − log"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "verified using Amazon Mechanical Turk. Multiple humans",
          "3": "(cid:80)N"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "3": "j xi+bj\nj=1 eW T"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "verified each image to reach a consensus, particularly for",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "niche classes. We applied transfer learning to these models,",
          "3": "and defined by:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "transitioning from a general\nimage classification task to the",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "facial emotion recognition task.",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "3": "es cos(θyi +m)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "3": "(2)\nLAAM = − log"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "2.2\nArcFace",
          "3": "es cos θj\nes cos(θyi +m) + (cid:80)N"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "3": "j=1,j̸=yi"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "The ArcFace model we used was\ntrained on Microsoft’s",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "3": "In Softmax,\nthe\nterm W T\nthe\nsimilarity\nj xi measures"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "MS1MV2 facial verification dataset [21]. Composed of ≈ 10",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "3": "between the weight vector Wj\nand the feature vector xi."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "million images of ≈ 100,000 celebrities, the dataset included",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "3": "In ArcFace,\nthis\nterm is modified to\n||Wj||||xi|| cos(θj),"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "individuals from 2,000 different professions across 200 coun-",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "3": "where both Wj and xi are normalized to 1, simplifying to"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "tries, with a wide range of ages and a higher proportion of",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "3": "cos(θj). ArcFace introduced two additional parameters: m,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "women compared to men1.\nImages were scraped from the",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "3": "an additive angular margin that\nincreased the separation"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "web using keyword searches and were labeled by the MS1M",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "3": "between classes in the embedding space, and s, the scaling"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "authors. We applied transfer learning to this model,\nfacili-",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "3": "factor that controlled the radius of the hypersphere to which"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "tating a more natural\ntransition from the facial verification",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "3": "the features are normalized and projected onto. Normalizing"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "task to the facial emotion recognition task.",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "3": "the features to the surface of\nthe hypersphere allowed pre-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "3": "dictions to depend only on the angle between the weights W"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "1. The paper dismisses the imbalance claiming the genders can be",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "balanced manually by keyword search.",
          "3": "and x the features, measured by cos(θj). When this feature"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: illustrates these disparities that manifested",
      "data": [
        {
          "facial\nrecognition task—a": "",
          "task\ninherently more\ncomplex": ""
        },
        {
          "facial\nrecognition task—a": "than facial emotion recognition. For this model, we initially",
          "task\ninherently more\ncomplex": ""
        },
        {
          "facial\nrecognition task—a": "set\nthe learning rate scheduler",
          "task\ninherently more\ncomplex": "to the same parameters as"
        },
        {
          "facial\nrecognition task—a": "the ResNeXt and EfficientNet models. However,",
          "task\ninherently more\ncomplex": "through"
        },
        {
          "facial\nrecognition task—a": "experimentation, we adjusted to an initial",
          "task\ninherently more\ncomplex": "learning rate of"
        },
        {
          "facial\nrecognition task—a": "",
          "task\ninherently more\ncomplex": ""
        },
        {
          "facial\nrecognition task—a": "0.01 with a patience of\n5",
          "task\ninherently more\ncomplex": "and a\nfactor of\n0.25. We once"
        },
        {
          "facial\nrecognition task—a": "",
          "task\ninherently more\ncomplex": ""
        },
        {
          "facial\nrecognition task—a": "again experimented with data augmentation techniques, in-",
          "task\ninherently more\ncomplex": ""
        },
        {
          "facial\nrecognition task—a": "",
          "task\ninherently more\ncomplex": ""
        },
        {
          "facial\nrecognition task—a": "cluding color jitter, random transformations, rotations, and",
          "task\ninherently more\ncomplex": ""
        },
        {
          "facial\nrecognition task—a": "",
          "task\ninherently more\ncomplex": ""
        },
        {
          "facial\nrecognition task—a": "reflections.\nInput",
          "task\ninherently more\ncomplex": "images were scaled to 112x112 to match"
        },
        {
          "facial\nrecognition task—a": "",
          "task\ninherently more\ncomplex": ""
        },
        {
          "facial\nrecognition task—a": "the expected input dimensions.",
          "task\ninherently more\ncomplex": ""
        },
        {
          "facial\nrecognition task—a": "",
          "task\ninherently more\ncomplex": ""
        },
        {
          "facial\nrecognition task—a": "We\nexperimented with calculating",
          "task\ninherently more\ncomplex": "the valence,\nland-"
        },
        {
          "facial\nrecognition task—a": "mark, and arousal",
          "task\ninherently more\ncomplex": "loss using Signed Mean Squared Error"
        },
        {
          "facial\nrecognition task—a": "and Pearson Correlation Loss. Signed Mean Squared Error",
          "task\ninherently more\ncomplex": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: illustrates these disparities that manifested",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "space was passed on to the final classification,\nthe feature",
          "4": "the intensity of\nthe emotion. For this reason,\nfor an actual"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "vectors were more distinct from each other, making decision",
          "4": "value of 0.3, a predicted value of\n-0.1 was not as accurate"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "boundaries easier to draw.",
          "4": "as a predicted value of 0.7, even though the absolute dis-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Finally, we followed the baselines set by the AffectNet",
          "4": "tance between the points was identical. Pearson Correlation"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "paper [19], which we will discuss in Section 3.",
          "4": "Loss took a similar approach by measuring the correlation"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "4": "between the predicted and ground truth values. To function"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "4": "as a loss metric, we subtracted the Pearson Correlation from"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "3\nTOOLS FOR ANALYSIS",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "4": "1 to maximize the correlation. We determined Signed MSE"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "We\naimed\nto\nimprove\nthe\nbaselines\nestablished by\nthe",
          "4": "yielded slightly better\nresults, and so all of our\nreported"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "AffectNet paper by implementing and combining various",
          "4": "results used Signed MSE error."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "approaches\ninto\na\nsingle model.\nInitially, we\nconducted",
          "4": "The ResNeXt model was accessed from TorchHub;\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "a\nseries\nof\nexperiments with a ResNeXt101-32x4d CNN",
          "4": "EfficientNet model was\naccessed from Torchvision. The"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "[20] and then transitioned to an EfficientNet-B0 CNN [16]",
          "4": "ArcFace backbone and weights were downloaded from the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "from TorchHub, which demonstrated higher efficiency and",
          "4": "InsightFace GitHub Repo. All\ntraining and test\nruns were"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "accuracy compared to ResNeXt. We removed the final fully",
          "4": "done on the UT Servers using HTCondor."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "connected layers from both models and replaced them with",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "four\nlayers\nin a multi-output module,\neach employing a",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "4": "4\nRESULTS"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "fully\nconnected layer\nfor\na\nspecific\nannotation:\narousal,",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "valence, expression, and landmarks. We froze the weights",
          "4": "4.1\nInitial Results"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "for the entire model except for the expression classifier. The",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "4": "The initial results of the training loops run on the AffectNet"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "losses from the four output\nlayers were added together to",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "4": "dataset were very promising. Starting at ≈ 50% validation"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "train a fully connected classification layer\nfor expression.",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "4": "accuracy for the ResNeXt model, we got up to ≈ 60% vali-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "We found that model accuracy decreased when only the",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "4": "dation accuracy with the EfficientNet model. We moved on"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "expression classifier’s loss was used.\nIn our\ninitial experi-",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "4": "from these ImageNet-trained, stock CNNs, and repurposed"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "ments, we tested three different\nloss functions as outlined",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "4": "the pretrained ArcFace model available from InsightFace to"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "in the original AffectNet paper: Mean Squared Error (MSE),",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "4": "leverage\nits\nsuccess\nin Facial Recognition into the Facial"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Pearson Correlation Loss, and Signed MSE Loss. [19]",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "4": "Emotion Recognition task.\nInitial\ntests were\nrun with 40"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Both models were\npre-trained\non\nImageNet,\nso we",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "4": "epoch training loops and a 0.01 LR that was reduced 10x"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "normalized the images based on ImageNet’s standards of",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "4": "after five epochs of plateau."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "mean and standard deviation (std): mean = [0.485, 0.456,",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "4": "However, what became apparent very early on was the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "0.406] and std = [0.229, 0.224, 0.225]. To match the expected",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "4": "issue the class imbalance in the dataset posed to the model’s"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "input dimensions, we\nresized the\nimages\nto 224x224\nfor",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "4": "robustness. While the largest class, Happiness, boasted a re-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "the ResNeXt and EfficientNet models. We applied random",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "4": "call > 92%, the three least represented classes (Fear, Disgust,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "horizontal flips as well as random horizontal and vertical",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "4": "and Contempt) regularly had precision, recall, and F1-scores"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "translations\nto augment\nthe data for minority classes. We",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "4": "< 12%. Table 2 illustrates these disparities that manifested"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "used the ADAM optimizer with a Reduce On Plateau (ROP)",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "4": "during training and validation time."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "learning rate scheduler. For ResNeXt and EfficientNet,\nthe",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "learning rate scheduler was configured to start at 0.128 and",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "4": "TABLE 2"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "decrease by a factor of 10 when the training loss plateaued",
          "4": "ArcFace Training L2=0.0005 Metrics"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "for five epochs.",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "After\nthe\ninitial\nexperiments,\nwe\nswitched\nto\na",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "4": "Metric\nTraining\nValidation"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "ResNet100 model\npre-trained\non\nthe\nfacial\nrecognition",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "4": "Loss\n21685.281\n21655.095"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "dataset MS1MV2. This model was\ntrained using Angular",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "4": "Accuracy (%)\n79.130\n74.226"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Additive Margin, which was specifically designed for\nthe",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "facial\nrecognition task—a\ntask\ninherently more\ncomplex",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "4": "Overall Metric\nPrecision\nRecall\nF1 Score"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "than facial emotion recognition. For this model, we initially",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "set\nthe learning rate scheduler\nto the same parameters as",
          "4": "Values\n0.724\n0.742\n0.724"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "the ResNeXt and EfficientNet models. However,\nthrough",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "experimentation, we adjusted to an initial\nlearning rate of",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "4": "Class\nPrecision\nRecall\nF1 Score"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "0.01 with a patience of\n5\nand a\nfactor of\n0.25. We once",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "4": "Neutral\n0.631\n0.788\n0.701"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "again experimented with data augmentation techniques, in-",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "4": "Happy\n0.869\n0.925\n0.896"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "cluding color jitter, random transformations, rotations, and",
          "4": "Sad\n0.680\n0.420\n0.519"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "4": "Surprise\n0.486\n0.452\n0.468"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "reflections.\nInput\nimages were scaled to 112x112 to match",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "4": "Fear\n0.363\n0.115\n0.175"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "the expected input dimensions.",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "4": "Disgust\n0.248\n0.100\n0.142"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "We\nexperimented with calculating\nthe valence,\nland-",
          "4": "Anger\n0.660\n0.481\n0.556"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "mark, and arousal\nloss using Signed Mean Squared Error",
          "4": "Contempt\n0.074\n0.013\n0.022"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "and Pearson Correlation Loss. Signed Mean Squared Error",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "accounted for the direction of\nthe error, which was critical,",
          "4": "To balance the data, we experimented with a weighted"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "particularly for valence and arousal, as their direction deter-",
          "4": "random sampler\nin the dataloader\nto even out\nclass\nrep-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "mined the quality of\nthe expression (e.g., excitement, bore-",
          "4": "resentations by using the reciprocal of\nthe class count\nfor"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "dom) and the magnitude of the assigned value represented",
          "4": "weights: 1/size(class). We also applied transformations to"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 3: , which was",
      "data": [
        {
          "TABLE 3": "",
          "TABLE 4": ""
        },
        {
          "TABLE 3": "Balanced Test Datasets.",
          "TABLE 4": ""
        },
        {
          "TABLE 3": "",
          "TABLE 4": "Balanced"
        },
        {
          "TABLE 3": "Balanced Count",
          "TABLE 4": "Test Data Set"
        },
        {
          "TABLE 3": "",
          "TABLE 4": "Accuracy (%)"
        },
        {
          "TABLE 3": "",
          "TABLE 4": ""
        },
        {
          "TABLE 3": "",
          "TABLE 4": ""
        },
        {
          "TABLE 3": "",
          "TABLE 4": ""
        },
        {
          "TABLE 3": "",
          "TABLE 4": "44"
        },
        {
          "TABLE 3": "",
          "TABLE 4": ""
        },
        {
          "TABLE 3": "",
          "TABLE 4": ""
        },
        {
          "TABLE 3": "",
          "TABLE 4": ""
        },
        {
          "TABLE 3": "",
          "TABLE 4": ""
        },
        {
          "TABLE 3": "",
          "TABLE 4": "40.8"
        },
        {
          "TABLE 3": "",
          "TABLE 4": ""
        },
        {
          "TABLE 3": "",
          "TABLE 4": ""
        },
        {
          "TABLE 3": "",
          "TABLE 4": ""
        },
        {
          "TABLE 3": "",
          "TABLE 4": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 3: , which was",
      "data": [
        {
          "Trained on": ""
        },
        {
          "Trained on": "Naturally\n40.8\n74.7"
        },
        {
          "Trained on": "Skewed Dataset"
        },
        {
          "Trained on": ""
        },
        {
          "Trained on": ""
        },
        {
          "Trained on": "standardized conditions. In contrast, the skewed test dataset"
        },
        {
          "Trained on": "accuracy evaluates\nthe model’s performance on a dataset"
        },
        {
          "Trained on": "with the\nclass distribution shown in Table 3, which was"
        },
        {
          "Trained on": "adjusted to reflect\nthe natural class imbalance found in the"
        },
        {
          "Trained on": "training data and ”in the wild.” This skewed test set was"
        },
        {
          "Trained on": "specifically designed to measure the model’s effectiveness"
        },
        {
          "Trained on": "on the ”in the wild” FER task,\nrather\nthan on purely bal-"
        },
        {
          "Trained on": "anced FER discernment."
        },
        {
          "Trained on": "To conduct this analysis, we trained and optimized two"
        },
        {
          "Trained on": "models. One model was trained and tuned on an artificially"
        },
        {
          "Trained on": "balanced AffectNet dataset using a weighted random sam-"
        },
        {
          "Trained on": "pler\nto equalize class sampling probabilities. A maximum"
        },
        {
          "Trained on": "2 × size(smallestclass)\nof\nsamples were\nselected with"
        },
        {
          "Trained on": "replacement. To enhance the diversity of minority classes"
        },
        {
          "Trained on": ""
        },
        {
          "Trained on": "(Fear, Disgust,\nand Contempt)\nand mitigate\nissues with"
        },
        {
          "Trained on": ""
        },
        {
          "Trained on": "duplicate samples, these classes were augmented with ran-"
        },
        {
          "Trained on": ""
        },
        {
          "Trained on": "dom affine transformations, including up to 10% horizontal"
        },
        {
          "Trained on": ""
        },
        {
          "Trained on": "and vertical\ntranslations and random rotations. Addition-"
        },
        {
          "Trained on": ""
        },
        {
          "Trained on": "ally, a 50% probability of horizontal flipping was applied."
        },
        {
          "Trained on": ""
        },
        {
          "Trained on": "These augmentations were carefully designed to introduce"
        },
        {
          "Trained on": ""
        },
        {
          "Trained on": "meaningful diversity without distorting the features of\nthe"
        },
        {
          "Trained on": ""
        },
        {
          "Trained on": "original\nimages. Our approach maximized the size of\nthe"
        },
        {
          "Trained on": ""
        },
        {
          "Trained on": "dataset while retaining balanced class representation."
        },
        {
          "Trained on": ""
        },
        {
          "Trained on": "The model performed significantly better on the ”in the"
        },
        {
          "Trained on": ""
        },
        {
          "Trained on": "wild” test\nset, confirming its ability to generalize to real-"
        },
        {
          "Trained on": ""
        },
        {
          "Trained on": "world emotions. However,\nits performance on the balanced"
        },
        {
          "Trained on": ""
        },
        {
          "Trained on": "test dataset was similarly poor regardless of whether it was"
        },
        {
          "Trained on": ""
        },
        {
          "Trained on": "trained on the natural dataset or an artificially balanced"
        },
        {
          "Trained on": ""
        },
        {
          "Trained on": "dataset. This\nlack of\nimprovement\nindicates\nthat broadly"
        },
        {
          "Trained on": ""
        },
        {
          "Trained on": "re-balancing the training set\nis insufficient\nfor training the"
        },
        {
          "Trained on": ""
        },
        {
          "Trained on": "model\neffectively\non\nthe\nFER task. Re-balancing\nefforts"
        },
        {
          "Trained on": ""
        },
        {
          "Trained on": "failed because reducing the training data to equally rep-"
        },
        {
          "Trained on": ""
        },
        {
          "Trained on": "resent minority classes\nresulted in an overly condensed"
        },
        {
          "Trained on": ""
        },
        {
          "Trained on": "dataset,\nleaving insufficient data for the model\nto general-"
        },
        {
          "Trained on": ""
        },
        {
          "Trained on": "ize effectively to facial emotion features. Addressing these"
        },
        {
          "Trained on": ""
        },
        {
          "Trained on": "discrepancies between different\nfacial emotion features re-"
        },
        {
          "Trained on": ""
        },
        {
          "Trained on": "quired a more targeted and radical approach."
        },
        {
          "Trained on": ""
        },
        {
          "Trained on": ""
        },
        {
          "Trained on": "4.3\nPairwise Discernment"
        },
        {
          "Trained on": ""
        },
        {
          "Trained on": "Due to the massive class imbalances, we determined that"
        },
        {
          "Trained on": "a model\ntrained on the AffectNet dataset was best suited"
        },
        {
          "Trained on": "to handle pure emotion discernment by distinguishing be-"
        },
        {
          "Trained on": ""
        },
        {
          "Trained on": "tween pairs of emotions at a time. The motivation for pair-"
        },
        {
          "Trained on": ""
        },
        {
          "Trained on": "wise discernment\nis inspired by the way people can more"
        },
        {
          "Trained on": ""
        },
        {
          "Trained on": "accurately identify or describe a musical note when compar-"
        },
        {
          "Trained on": ""
        },
        {
          "Trained on": "ing it directly to another, highlighting the power of relative"
        },
        {
          "Trained on": "distinctions over isolated recognition. We grouped emotions"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "",
          "TABLE 6": ""
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "and evaluated the model\ntrained with the ArcFace archi-",
          "TABLE 6": ""
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "tecture fine-tuned on the AffectNet dataset. To balance the",
          "TABLE 6": ""
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "classes within each pair, we weighed them equally and per-",
          "TABLE 6": "Precision"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "formed random sampling 2 × min(class1size, class2size)",
          "TABLE 6": "0.9310"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "times,\nensuring\nequal\nrepresentation\nbetween\nclasses\nin",
          "TABLE 6": "0.9190"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "each pair. While the results still reflected the biases of\nthe",
          "TABLE 6": "0.8513"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "generally trained model toward the larger classes, pairwise",
          "TABLE 6": "0.9952"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "discernment proved to\nbe\na more\neffective\ntask for\nthe",
          "TABLE 6": "0.8426"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "model. The results are summarized in Table 5.",
          "TABLE 6": "0.9951"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "",
          "TABLE 6": "0.9489"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "TABLE 5",
          "TABLE 6": "0.8673"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "Difference in Accuracy Between Single Fully Connected Layer Pair",
          "TABLE 6": ""
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "",
          "TABLE 6": "0.8644"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "Discernment Model and Fully Connected Pair Dictionary Test Results",
          "TABLE 6": ""
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "",
          "TABLE 6": "0.8778"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "",
          "TABLE 6": "0.8778"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "Class\nOne FC Layer\nFC Pair Dict\nDifference",
          "TABLE 6": ""
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "",
          "TABLE 6": "0.8644"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "Pair\nAccuracy (%)\nAccuracy (%)\n(%)",
          "TABLE 6": ""
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "",
          "TABLE 6": "0.8697"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "Fear + Contempt\n92.5\n70.6\n-21.9",
          "TABLE 6": ""
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "",
          "TABLE 6": "0.8683"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "Happy + Sad\n91.1\n92.3\n+1.2",
          "TABLE 6": ""
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "Happy + Anger\n90.5\n92.1\n+1.6",
          "TABLE 6": "0.8755"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "Neutral + Happy\n90.4\n90.3\n-0.1",
          "TABLE 6": "0.8605"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "Surprise + Anger\n87.1\n73.8\n-13.3",
          "TABLE 6": ""
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "",
          "TABLE 6": "0.7755"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "Fear + Disgust\n87.1\n65.4\n-21.7",
          "TABLE 6": ""
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "",
          "TABLE 6": "0.9835"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "Sad + Surprise\n86.9\n77.7\n-9.2",
          "TABLE 6": ""
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "Disgust + Contempt\n86.8\n59.5\n-27.3",
          "TABLE 6": "0.8288"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "Happy + Surprise\n85.1\n88.8\n+3.7",
          "TABLE 6": "0.8023"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "Sad + Anger\n81.5\n67.6\n-13.9",
          "TABLE 6": ""
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "",
          "TABLE 6": "0.7267"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "Happy + Fear\n81.2\n80.7\n-0.5",
          "TABLE 6": ""
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "",
          "TABLE 6": "1.0000"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "Surprise + Disgust\n76.4\n52.0\n-24.4",
          "TABLE 6": ""
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "Neutral + Surprise\n75.5\n80.2\n+4.7",
          "TABLE 6": ""
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "",
          "TABLE 6": "0.6891"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "Fear + Anger\n75.5\n59.8\n-15.7",
          "TABLE 6": ""
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "",
          "TABLE 6": "0.9371"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "Neutral + Sad\n75.3\n76.6\n+1.3",
          "TABLE 6": ""
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "Surprise + Contempt\n75.3\n52.3\n-23.0",
          "TABLE 6": "0.6803"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "Neutral + Anger\n75.1\n75.6\n+0.5",
          "TABLE 6": "0.9352"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "Happy + Disgust\n74.1\n71.3\n-2.8",
          "TABLE 6": ""
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "",
          "TABLE 6": "0.9443"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "Sad + Contempt\n73.9\n50.9\n-23.0",
          "TABLE 6": ""
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "",
          "TABLE 6": "0.6788"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "Anger + Contempt\n72.9\n52.7\n-20.2",
          "TABLE 6": ""
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "Neutral + Fear\n69.1\n67.1\n-2.0",
          "TABLE 6": "0.6779"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "Neutral + Disgust\n67.9\n63.2\n-4.7",
          "TABLE 6": "0.9377"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "Sad + Fear\n67.1\n55.5\n-11.6",
          "TABLE 6": ""
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "",
          "TABLE 6": "0.6759"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "Sad + Disgust\n66.6\n50.9\n-15.7",
          "TABLE 6": ""
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "",
          "TABLE 6": "0.9500"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "Surprise + Fear\n61.1\n50.5\n-10.6",
          "TABLE 6": ""
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "Disgust + Anger\n59.5\n52.3\n-7.2",
          "TABLE 6": "0.6765"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "Neutral + Contempt\n54.1\n53.4\n-0.7",
          "TABLE 6": "0.9343"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "Happy + Contempt\n52.7\n59.2\n+6.5",
          "TABLE 6": ""
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "",
          "TABLE 6": "0.6588"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "",
          "TABLE 6": "1.0000"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "To further optimize pairwise discernment, we\nimple-",
          "TABLE 6": ""
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "",
          "TABLE 6": "0.6648"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "mented a dictionary of\nfully connected (FC)\nlayers,\neach",
          "TABLE 6": "0.9343"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "assigned to\na\nspecific pair,\nto\nlearn the nuances\nof\nthe",
          "TABLE 6": ""
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "",
          "TABLE 6": "0.5543"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "differences between pairs. The first implementation utilized",
          "TABLE 6": "0.9254"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "the generally trained model by appending the pair-specific",
          "TABLE 6": ""
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "",
          "TABLE 6": "0.6195"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "FC layer to the general FC layer. The second implementation",
          "TABLE 6": "0.9751"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "removed the FC layer fine-tuned on AffectNet, operating",
          "TABLE 6": ""
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "",
          "TABLE 6": "0.6095"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "under the intuition that\nthe general model’s learned biases",
          "TABLE 6": "0.9891"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "could influence the pairwise FC layers. Both implementa-",
          "TABLE 6": ""
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "",
          "TABLE 6": "0.6084"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "tions were\ntrained for 30 epochs with a learning rate of",
          "TABLE 6": "0.9052"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "0.0001, a learning rate scheduler\nthat\nreduced the rate by",
          "TABLE 6": ""
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "",
          "TABLE 6": "0.6043"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "a factor of 0.25 after a five-epoch plateau, a weight decay of",
          "TABLE 6": "0.9069"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "0.0005, a batch size of 256, and the ADAM optimizer.",
          "TABLE 6": ""
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "",
          "TABLE 6": "0.5668"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "Theoretically, using a dictionary of FC layers—one ded-",
          "TABLE 6": "0.8284"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "icated to each class pair—should result in a better architec-",
          "TABLE 6": ""
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "",
          "TABLE 6": "0.8800"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "ture for detecting true facial emotion features. This design",
          "TABLE 6": "0.5543"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "focuses\nexclusively\non the\ntwo\nclasses\nbeing\ncompared,",
          "TABLE 6": ""
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "",
          "TABLE 6": "0.5216"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "enhancing the model’s ability to distinguish between them.",
          "TABLE 6": "0.9000"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "In contrast,\nthe single FC layer model performs a standard",
          "TABLE 6": ""
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "",
          "TABLE 6": "0.5139"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "forward pass, extracting logits for all classes but only con-",
          "TABLE 6": "1.0000"
        },
        {
          "into pairs (e.g., happy and neutral, happy and disgusted)": "siders the logits for the relevant pair during evaluation.",
          "TABLE 6": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "into pairs (e.g., happy and neutral, happy and disgusted)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "and evaluated the model\ntrained with the ArcFace archi-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "tecture fine-tuned on the AffectNet dataset. To balance the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "classes within each pair, we weighed them equally and per-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "formed random sampling 2 × min(class1size, class2size)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "times,\nensuring\nequal\nrepresentation\nbetween\nclasses\nin"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "each pair. While the results still reflected the biases of\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "generally trained model toward the larger classes, pairwise"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "discernment proved to\nbe\na more\neffective\ntask for\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "model. The results are summarized in Table 5."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "TABLE 5"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Difference in Accuracy Between Single Fully Connected Layer Pair"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Discernment Model and Fully Connected Pair Dictionary Test Results"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Class\nOne FC Layer\nFC Pair Dict\nDifference"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Pair\nAccuracy (%)\nAccuracy (%)\n(%)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Fear + Contempt\n92.5\n70.6\n-21.9"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Happy + Sad\n91.1\n92.3\n+1.2"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Happy + Anger\n90.5\n92.1\n+1.6"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Neutral + Happy\n90.4\n90.3\n-0.1"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Surprise + Anger\n87.1\n73.8\n-13.3"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Fear + Disgust\n87.1\n65.4\n-21.7"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Sad + Surprise\n86.9\n77.7\n-9.2"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Disgust + Contempt\n86.8\n59.5\n-27.3"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Happy + Surprise\n85.1\n88.8\n+3.7"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Sad + Anger\n81.5\n67.6\n-13.9"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Happy + Fear\n81.2\n80.7\n-0.5"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Surprise + Disgust\n76.4\n52.0\n-24.4"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Neutral + Surprise\n75.5\n80.2\n+4.7"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Fear + Anger\n75.5\n59.8\n-15.7"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Neutral + Sad\n75.3\n76.6\n+1.3"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Surprise + Contempt\n75.3\n52.3\n-23.0"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Neutral + Anger\n75.1\n75.6\n+0.5"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Happy + Disgust\n74.1\n71.3\n-2.8"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Sad + Contempt\n73.9\n50.9\n-23.0"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Anger + Contempt\n72.9\n52.7\n-20.2"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Neutral + Fear\n69.1\n67.1\n-2.0"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Neutral + Disgust\n67.9\n63.2\n-4.7"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Sad + Fear\n67.1\n55.5\n-11.6"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Sad + Disgust\n66.6\n50.9\n-15.7"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Surprise + Fear\n61.1\n50.5\n-10.6"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Disgust + Anger\n59.5\n52.3\n-7.2"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Neutral + Contempt\n54.1\n53.4\n-0.7"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Happy + Contempt\n52.7\n59.2\n+6.5"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "To further optimize pairwise discernment, we\nimple-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "mented a dictionary of\nfully connected (FC)\nlayers,\neach"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "assigned to\na\nspecific pair,\nto\nlearn the nuances\nof\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "differences between pairs. The first implementation utilized"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "the generally trained model by appending the pair-specific"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "FC layer to the general FC layer. The second implementation"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "removed the FC layer fine-tuned on AffectNet, operating"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "under the intuition that\nthe general model’s learned biases"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "could influence the pairwise FC layers. Both implementa-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "tions were\ntrained for 30 epochs with a learning rate of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "0.0001, a learning rate scheduler\nthat\nreduced the rate by"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "a factor of 0.25 after a five-epoch plateau, a weight decay of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "0.0005, a batch size of 256, and the ADAM optimizer."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Theoretically, using a dictionary of FC layers—one ded-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "icated to each class pair—should result in a better architec-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "ture for detecting true facial emotion features. This design"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "focuses\nexclusively\non the\ntwo\nclasses\nbeing\ncompared,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "enhancing the model’s ability to distinguish between them."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "In contrast,\nthe single FC layer model performs a standard"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "forward pass, extracting logits for all classes but only con-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "siders the logits for the relevant pair during evaluation."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 1: ) (Happy),",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "However,\nthis\napproach does not\nguarantee\nthat\nthe",
          "7": "belonged to one of\nthe eight classes (see Table 1)\n(Happy),"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "model is learning to differentiate based on meaningful facial",
          "7": "and 80% of\nthe data\nfell\ninto\nthe\ntwo majority\nclasses"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "emotion features;\nit could simply produce higher logits for",
          "7": "(Happiness and Neutral). Consequently,\nthe three minority"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "the correct class by chance, even if both logits are low.",
          "7": "classes combined to account for only 5% of the training data."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "In practice, the fully connected layer dictionary approach",
          "7": "This imbalance occurred naturally due to the proportion of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "performed worse than the general model. Table 5 summa-",
          "7": "photos published ”in the wild”; people were more likely"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "rizes the comparison, showing that while highly represented",
          "7": "to post photos of\nthemselves when they were happy than"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "classes\nbenefited\nfrom the\nFC dictionary\napproach,\nthe",
          "7": "when they were, for example, afraid."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "pairwise dictionary struggled to effectively learn minority-",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "7": "Subjectivity of Facial Expressions"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "represented classes. This limitation arose because the dense",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "layers in the pairwise dictionary lacked sufficient data for",
          "7": "The\nannotations provided by AffectNet were\ninvaluable"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "training on minority class pairs, whereas the general model",
          "7": "for\ntraining the model\nto\nrecognize underlying patterns"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "was\ntrained on the\nentire AffectNet dataset. As a result,",
          "7": "and quantify various\nfacial\nexpressions. However,\na key"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "performance dropped by up to 27%, underscoring the chal-",
          "7": "challenge lies in the subjectivity of\nthese annotations. Ac-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "lenges of this approach in addressing class imbalances.",
          "7": "cording to the AffectNet paper: ”To measure the agreement"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Table 6 summarizes the general model’s class accuracies",
          "7": "between the annotators, 36,000 images were annotated by"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "for each pair. Since the classes are balanced against each",
          "7": "two annotators. The annotations were performed fully blind"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "other, high metrics for both classes suggest\nthat\nthe model",
          "7": "and independently,\ni.e.,\nthe\nannotators were unaware of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "effectively recognizes genuine facial emotions, reducing the",
          "7": "the intended query or the other annotator’s response. The"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "influence of ”in the wild” biases. However, not all pairs",
          "7": "results\nshowed that\nthe\nannotators\nagreed on\n60.7% of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "performed equally well. The pairwise discernment\nscale",
          "7": "the images.” [19] This relatively low agreement rate high-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "provides\ninsight\ninto which emotion pairs\nthe model can",
          "7": "lights\ninconsistencies\nin how annotations were\nassigned,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "effectively differentiate.",
          "7": "suggesting that\nthe ground truth labels may not adhere to"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Notably,\nthe model consistently distinguished between",
          "7": "a consistent\nstandard. Consequently,\nthese inconsistencies"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Happy + Sad faces as well as Happy + Angry faces, achiev-",
          "7": "could have disrupted the model’s ability to reliably identify"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "ing strong performance even on new data. An unexpected",
          "7": "patterns in the data."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "result was the model’s ability to discern between the three",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "minority classes: Fear + Contempt, Fear + Disgust,\nand",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "7": "5.2\nShort Term Facial Emotion Recognition"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Contempt + Disgust. The high metrics for these pairs sug-",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "7": "In\nthe\nshort\nterm,\nFacial Emotion Recognition\n(FER)\nis"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "gest\nthat\nthe model can reliably differentiate between the",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "7": "expected to\ncontinue\nadvancing\nsignificantly, driven\nby"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "minority classes despite the limited data.",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "7": "larger datasets with standardized annotations. While this"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "7": "project demonstrated relative\nsuccess with a refined and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "5\nCONCLUSION",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "7": "intentional dataset,\nits performance ”in the wild” remains"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Our work introduced a novel approach by applying pair-",
          "7": "unclear. Datasets like the CFP-FP dataset [25], which is used"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "wise discernment to the ”in the wild” FER AffectNet dataset,",
          "7": "for facial verification from both frontal and profile angles,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "achieving accuracies as high as 92%. This approach high-",
          "7": "could be adapted for facial emotion recognition. Similarly,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "lighted the challenges of distinguishing multiple emotions",
          "7": "video datasets, such as the facial verification dataset Labeled"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "simultaneously in images sourced from the internet, where",
          "7": "Faces in the Wild (LFW)\n[26], could be developed for FER,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "natural biases influence the types of photos people post of",
          "7": "enabling models\nto predict\nemotions dynamically. Real-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "themselves and others. Pairwise discernment enabled the",
          "7": "time emotion detection has the potential to bring significant"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "model\nto effectively differentiate between three minority",
          "7": "benefits,\nsuch as enhancing behavioral\ntherapy by allow-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "classes\n(Fear, Contempt, and Disgust)\nthat were indistin-",
          "7": "ing researchers\nto process patient\nemotions quickly and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "guishable during general classification tasks. This enhanced",
          "7": "autonomously."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "ability to distinguish minority classes suggests that pairwise",
          "7": "Additionally, datasets and experiments based on anno-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "discernment\nallows\nthe model\nto learn underlying facial",
          "7": "tations like Valence and Arousal,\nrather\nthan rigid classes"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "emotion features\nthat\nremain elusive\nin broader\nclassifi-",
          "7": "such as emotion expressions (e.g., happy, sad), would better"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "cation tasks. We conclude that pairwise discernment\nis a",
          "7": "capture the range of human emotions. Expression classes,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "powerful\ntool\nfor addressing data set\nimbalances and cap-",
          "7": "as used in this project,\nremain too broad;\nfor\ninstance, an"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "turing meaningful patterns by reducing task complexity and",
          "7": "embarrassed laugh and a gleeful laugh are both categorized"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "improving model generalization.",
          "7": "as ”happy,” yet they represent significantly different mental"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "7": "states."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "5.1\nSources of Error",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "7": "5.3\nLong-Term Future for Machine Learning Emotion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "The models performed very well on majority classes but",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "struggled significantly with minority classes. In this section,",
          "7": "Image classification represents only the first step in enabling"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "we discuss the potential sources of error contributing to this",
          "7": "computers to understand and communicate emotion more"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "discrepancy and analyze where our approach fell short.",
          "7": "broadly. The disparity between observable behavior\nand"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "7": "internal mental\nstates\nremains\nsubstantial,\nand teaching"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Imbalanced Data Set",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "7": "models\nto classify behavior does not\nequate\nto teaching"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "An early source of error in the project was the heavy class",
          "7": "them emotion. Bayesian Neural Networks, with their\nca-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "imbalance in the dataset. Nearly half of\nthe training data",
          "7": "pacity for world modeling, offer promising progress toward"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "representing and communicating more complex mental and",
          "8": "S. Xie, R. Girshick, P. Doll´ar, Z. Tu,\nand K. He,\n“Aggregated"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "8": "residual transformations for deep neural networks,” 2017."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "emotional states.",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "8": "Guo,\nL.\nZhang,\nY.\nHu,\nX.\nHe,\nand\nJ.\nGao,\n“Ms-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "8": "celeb-1m:\nA\ndataset\nand\nbenchmark\nfor\nlarge-scale\nface"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "8": "recognition,” CoRR, vol. abs/1607.08221, 2016. [Online]. Available:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "ACKNOWLEDGMENTS",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "8": "http://arxiv.org/abs/1607.08221"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "8": "and S. Yoon,\n“Facial\nexpression recognition based"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "We\nare grateful\nto Dr. Mohammad Mehdi Hosseini\nand",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "8": "on weighted-cluster\nloss\nand\ndeep\ntransfer\nlearning\nusing\na"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "the University of Denver Computer Science department for",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "8": "highly imbalanced dataset,” Sensors, vol. 20, no. 9, 2020. [Online]."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "granting us access to the AffectNet database.",
          "8": "Available: https://www.mdpi.com/1424-8220/20/9/2639"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "8": "J. Deng,\nJ. Guo, N. Xue,\nand S. Zafeiriou,\n“Arcface: Additive"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "We appreciate the technical assistance provided by Amy",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "8": "angular margin loss for deep face recognition,” in Proceedings of the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Bush and the UTCS help staff, who gave us access to the",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "8": "IEEE Conference on Computer Vision and Pattern Recognition, 2019,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "compute and resources necessary for this paper.",
          "8": "pp. 4690–4699."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "8": "[24] N. Siddiqui, R. Dave, T. Bauer, T. Reither, D. Black, and M. Han-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "8": "son, “A robust\nframework for deep learning approaches to facial"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "REFERENCES",
          "8": "emotion recognition and evaluation,” 2022."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "",
          "8": "S. Sengupta, J.-C. Chen, C. Castillo, V. M. Patel, R. Chellappa, and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "[1]\nK. Fukushima, “Neocognitron: A self-organizing neural network",
          "8": "D. W.\nJacobs, “Frontal\nto profile face verification in the wild,”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "model for a mechanism of pattern recognition unaffected by shift",
          "8": "in 2016 IEEE Winter Conference on Applications of Computer Vision"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "in position,” Biological Cybernetics, vol. 36, pp. 193–202, 1980.",
          "8": "(WACV), 2016, pp. 1–9."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "[2]\nD. H. Hubel and T. N. Wiesel, “Receptive fields, binocular inter-",
          "8": "[26] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller, “La-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "action and functional architecture in the cat’s visual cortex,” The",
          "8": "beled faces in the wild: A database for studying face recognition"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Journal of Physiology, vol. 160, pp. 106–154, 1962.",
          "8": "in unconstrained environments,” University\nof Massachusetts,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "[3]\n——, “Receptive fields and functional architecture in two nonstri-",
          "8": "Amherst, Tech. Rep. 07-49, October 2007."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "ate visual areas (18 and 19) of the cat,” Journal of Neurophysiology,",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "vol. 28, pp. 229–289, 1965.",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "[4]\n——, “Functional architecture of macaque monkey visual cortex,”",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "Proceedings of the Royal Society of London. Series B. Biological Sciences,",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "vol. 198, pp. 1–59, 1977.",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "[5]\nY. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "the\nlearning applied to document\nrecognition,” in Proceedings of",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "IEEE, vol. 86, no. 11, 1998, pp. 2278–2324. [Online]. Available: http:",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.42.7665",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "[6]\nA.\nKrizhevsky,\nI.\nSutskever,\nand\nG.\nE. Hinton,\n“Imagenet",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH YEAR": "classification\nwith\ndeep\nconvolutional\nneural\nnetworks,”",
          "8": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[12]\nF. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, and": "K. Keutzer, “Squeezenet: Alexnet-level accuracy with 50x fewer"
        },
        {
          "[12]\nF. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, and": "parameters and ¡0.5mb model size,” 2016."
        },
        {
          "[12]\nF. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, and": "[13]\nJ. Hu, L. Shen, S. Albanie, G. Sun, and E. Wu, “Squeeze-and-"
        },
        {
          "[12]\nF. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, and": "excitation networks,” 2019."
        },
        {
          "[12]\nF. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, and": "[14] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,"
        },
        {
          "[12]\nF. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, and": "T. Weyand, M. Andreetto, and H. Adam, “Mobilenets: Efficient"
        },
        {
          "[12]\nF. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, and": "convolutional neural networks\nfor mobile vision applications,”"
        },
        {
          "[12]\nF. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, and": "2017."
        },
        {
          "[12]\nF. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, and": "[15] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen,"
        },
        {
          "[12]\nF. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, and": "“Mobilenetv2: Inverted residuals and linear bottlenecks,” 2019."
        },
        {
          "[12]\nF. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, and": "[16] M. Tan and Q. V. Le, “Efficientnet: Rethinking model scaling for"
        },
        {
          "[12]\nF. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, and": "convolutional neural networks,” 2020."
        },
        {
          "[12]\nF. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, and": "[17] A. Vaswani, N. Shazeer, N. Parmar,\nJ. Uszkoreit, L.\nJones, A. N."
        },
        {
          "[12]\nF. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, and": "Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,”"
        },
        {
          "[12]\nF. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, and": "2023."
        },
        {
          "[12]\nF. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, and": "[18] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position",
      "authors": [
        "K Fukushima"
      ],
      "year": "1980",
      "venue": "Biological Cybernetics"
    },
    {
      "citation_id": "2",
      "title": "Receptive fields, binocular interaction and functional architecture in the cat's visual cortex",
      "authors": [
        "D Hubel",
        "T Wiesel"
      ],
      "year": "1962",
      "venue": "The Journal of Physiology"
    },
    {
      "citation_id": "3",
      "title": "Receptive fields and functional architecture in two nonstriate visual areas (18 and 19) of the cat",
      "year": "1965",
      "venue": "Journal of Neurophysiology"
    },
    {
      "citation_id": "4",
      "title": "Functional architecture of macaque monkey visual cortex",
      "year": "1977",
      "venue": "Proceedings of the Royal Society of London. Series B. Biological Sciences"
    },
    {
      "citation_id": "5",
      "title": "Gradient-based learning applied to document recognition",
      "authors": [
        "Y Lecun",
        "L Bottou",
        "Y Bengio",
        "P Haffner"
      ],
      "year": "1998",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "6",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2012",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "7",
      "title": "Imagenet: A large-scale hierarchical image database",
      "authors": [
        "J Deng",
        "W Dong",
        "R Socher",
        "L.-J Li",
        "K Li",
        "L Fei-Fei"
      ],
      "year": "2009",
      "venue": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "8",
      "title": "Network in network",
      "authors": [
        "M Lin",
        "Q Chen",
        "S Yan"
      ],
      "year": "2014",
      "venue": "Network in network"
    },
    {
      "citation_id": "9",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2015",
      "venue": "Very deep convolutional networks for large-scale image recognition"
    },
    {
      "citation_id": "10",
      "title": "Going deeper with convolutions",
      "authors": [
        "C Szegedy",
        "W Liu",
        "Y Jia",
        "P Sermanet",
        "S Reed",
        "D Anguelov",
        "D Erhan",
        "V Vanhoucke",
        "A Rabinovich"
      ],
      "year": "2014",
      "venue": "Going deeper with convolutions"
    },
    {
      "citation_id": "11",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2015",
      "venue": "Deep residual learning for image recognition"
    },
    {
      "citation_id": "12",
      "title": "Squeezenet: Alexnet-level accuracy with 50x fewer parameters and ¡0.5mb model size",
      "authors": [
        "F Iandola",
        "S Han",
        "M Moskewicz",
        "K Ashraf",
        "W Dally",
        "K Keutzer"
      ],
      "year": "2016",
      "venue": "Squeezenet: Alexnet-level accuracy with 50x fewer parameters and ¡0.5mb model size"
    },
    {
      "citation_id": "13",
      "title": "Squeeze-andexcitation networks",
      "authors": [
        "J Hu",
        "L Shen",
        "S Albanie",
        "G Sun",
        "E Wu"
      ],
      "year": "2019",
      "venue": "Squeeze-andexcitation networks"
    },
    {
      "citation_id": "14",
      "title": "Mobilenets: Efficient convolutional neural networks for mobile vision applications",
      "authors": [
        "A Howard",
        "M Zhu",
        "B Chen",
        "D Kalenichenko",
        "W Wang",
        "T Weyand",
        "M Andreetto",
        "H Adam"
      ],
      "year": "2017",
      "venue": "Mobilenets: Efficient convolutional neural networks for mobile vision applications"
    },
    {
      "citation_id": "15",
      "title": "Mobilenetv2: Inverted residuals and linear bottlenecks",
      "authors": [
        "M Sandler",
        "A Howard",
        "M Zhu",
        "A Zhmoginov",
        "L.-C Chen"
      ],
      "year": "2019",
      "venue": "Mobilenetv2: Inverted residuals and linear bottlenecks"
    },
    {
      "citation_id": "16",
      "title": "Efficientnet: Rethinking model scaling for convolutional neural networks",
      "authors": [
        "M Tan",
        "Q Le"
      ],
      "year": "2020",
      "venue": "Efficientnet: Rethinking model scaling for convolutional neural networks"
    },
    {
      "citation_id": "17",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2023",
      "venue": "Attention is all you need"
    },
    {
      "citation_id": "18",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly",
        "J Uszkoreit",
        "N Houlsby"
      ],
      "year": "2021",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale"
    },
    {
      "citation_id": "19",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2017.2740923"
    },
    {
      "citation_id": "20",
      "title": "Aggregated residual transformations for deep neural networks",
      "authors": [
        "S Xie",
        "R Girshick",
        "P Dollár",
        "Z Tu",
        "K He"
      ],
      "year": "2017",
      "venue": "Aggregated residual transformations for deep neural networks"
    },
    {
      "citation_id": "21",
      "title": "Msceleb-1m: A dataset and benchmark for large-scale face recognition",
      "authors": [
        "Y Guo",
        "L Zhang",
        "Y Hu",
        "X He",
        "J Gao"
      ],
      "year": "2016",
      "venue": "CoRR"
    },
    {
      "citation_id": "22",
      "title": "Facial expression recognition based on weighted-cluster loss and deep transfer learning using a highly imbalanced dataset",
      "authors": [
        "Q Ngo",
        "S Yoon"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "23",
      "title": "Arcface: Additive angular margin loss for deep face recognition",
      "authors": [
        "J Deng",
        "J Guo",
        "N Xue",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "24",
      "title": "A robust framework for deep learning approaches to facial emotion recognition and evaluation",
      "authors": [
        "N Siddiqui",
        "R Dave",
        "T Bauer",
        "T Reither",
        "D Black",
        "M Hanson"
      ],
      "year": "2022",
      "venue": "A robust framework for deep learning approaches to facial emotion recognition and evaluation"
    },
    {
      "citation_id": "25",
      "title": "Frontal to profile face verification in the wild",
      "authors": [
        "S Sengupta",
        "J.-C Chen",
        "C Castillo",
        "V Patel",
        "R Chellappa",
        "D Jacobs"
      ],
      "year": "2016",
      "venue": "2016 IEEE Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "26",
      "title": "Labeled faces in the wild: A database for studying face recognition in unconstrained environments",
      "authors": [
        "G Huang",
        "M Ramesh",
        "T Berg",
        "E Learned-Miller"
      ],
      "year": "2007",
      "venue": "Labeled faces in the wild: A database for studying face recognition in unconstrained environments"
    }
  ]
}