{
  "paper_id": "2206.05687v2",
  "title": "Drnet: Decomposition And Reconstruction Network For Remote Physiological Measurement",
  "published": "2022-06-12T07:40:10Z",
  "authors": [
    "Yuhang Dong",
    "Gongping Yang",
    "Yilong Yin"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Remote photoplethysmography (rPPG) based physiological measurement has great application values in affective computing, non-contact health monitoring, telehealth monitoring, etc, which has become increasingly important especially during the COVID-19 pandemic. Existing methods are generally divided into two groups. The first focuses on mining the subtle blood volume pulse (BVP) signals from face videos, but seldom explicitly models the noises that dominate face video content. They are susceptible to the noises and may suffer from poor generalization ability in unseen scenarios. The second focuses on modeling noisy data directly, resulting in suboptimal performance due to the lack of regularity of these severe random noises. In this paper, we propose a Decomposition and Reconstruction Network (DRNet) focusing on the modeling of physiological features rather than noisy data. A novel cycle loss is proposed to constrain the periodicity of physiological information. Besides, a plug-and-play Spatial Attention Block (SAB) is proposed to enhance features along with the spatial location information. Furthermore, an efficient Patch Cropping (PC) augmentation strategy is proposed to synthesize augmented samples with different noise and features. Extensive experiments on different public datasets as well as the cross-database testing demonstrate the effectiveness of our approach.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Computer vision based remote physiological measurement has been gaining a tremendous interest, which has significant advantages compared with traditional contactbased measurements, particularly during the COVID-19 pandemic. Firstly, these methods can achieve reliable contactless vitals measurement such as heart rate (HR), Respiration Frequency (RF) and heart rate variability (HRV), * Corresponding author. but don't require any customized equipment. These methods rely only on the video feed recorded from a commonly accessible camera such as a commodity smartphone camera. Secondly, conventional contact-based measurements such as electrocardiography (ECG) and photoplethysmography (PPG) require dedicated skin-contact devices for data collection, which may cause discomfort and inconvenience for subjects. Hence, these computer vision based methods are more patient-friendly, which can achieve non-contact human health monitoring. Thirdly, these methods have broader applications than contact-based methods, including telehealth monitoring, deep forgery detection, affective computing, human behavior understanding and sports.\n\nRemote photoplethysmography (rPPG) is one of the most studied computer vision based measurement methods, which aims to extract physiological signals from video sequences. The principle of rPPG based physiological measurement is the fact that optical absorption of a local tissue varies periodically with the blood volume due to the human heartbeat. Nevertheless, the subtle optical absorption variation (not visible for human eyes) can be easily affected by the noises like head movements, lighting variations and device noises. To overcome the above challenges, a lot of conventional methods have been proposed to solve these strong random noises using color space projection  [26, 20, 11]  or certain skin reflection models  [4, 28] . However, these methods are built upon the shallow and coarse assumptions, which do not always hold in handling the complicated scenes, such as large head movement or dim lighting condition. Besides, the real-world samples are usually too complex to be modeled with multiple simple mathematical models. Therefore, these methods will easily fail in realworld samples.\n\nIn recent years, deep learning has achieved significant breakthroughs in various computer vision tasks, and many scholars have also tried to utilize the strong modeling ability of deep neural networks for remote physiological signals prediction. Most of these methods focus on learning a network mapping from different manual representations of face videos (e.g., cropped video frames  [22] , difference of video frames  [3] , spatial-temporal map  [16, 17] ). On one hand, these manual representations consist of physiological and non-physiological information, and the estimation performance can be easily affected by non-physiological information such as head movements and lighting variations. On the other hand, the key challenge of rPPG-based physiological measurement is how to effectively extract physiological information and suppress the adverse effects of non-physiological information. However, existing methods either seldom explicitly model the serious random noises, or model noisy data directly. Due to the lack of sufficient data and the lack of regularity of these strong noises, both of them lead to suboptimal performance.\n\nMotivated by the above discussions, rather than modeling noisy data with the severe lack of regularity, it is better to directly model regular, periodic physiological features. In other words, unlike previous studies, our approach focuses on modeling physiological information rather than modeling noisy data. In this way, we propose the Decomposition and Reconstruction Network (DRNet), which adopts a novel decomposition and reconstruction strategy (models physiological features directly, decomposes non-physiological noise indirectly, and then uses the decomposed noise and features to reconstruct new synthetic samples). Furthermore, we propose a novel cycle loss to constrain the periodicity of physiological information.\n\nOur contribution can be summarized as follows:\n\n• We propose the first rPPG-dedicated data augmentation method, Patch Cropping (PC), to generate data with different degrees of noise influences and multiscale physiological information, which is able to plug and play in not only DRNet but also other existing frameworks for performance improvement.\n\n• To the best of our knowledge, our approach is the first to focus on modeling physiological information. A novel cycle loss is proposed to constrain the periodicity of physiological features. Moreover, a new easyto-hard decomposition and reconstruction strategy is proposed.\n\n• We propose a lightweight and efficient Spatial Attention Block (SAB) plugged in our physiological estimator, which can adaptively recalibrate the varying importance of different channels and spatial regions.\n\n• Our proposed method achieves state-of-the-art performance on three public benchmark datasets with intradataset and cross-dataset testing protocols.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Traditional Methods",
      "text": "rPPG is the monitoring of blood volume pulse (BVP) from a camera at a distance.  [26]  proved, for the first time, that PPG signals can be measured remotely (>1m) from face videos using ambient light. After that, many scholars have devoted their efforts in this challenging task.  [20]  introduced a new methodology which applied independent component analysis (ICA) to reconstruct rPPG signals from face videos. Similarly,  [11]  proposed a new rPPG signals estimation method based on principal component analysis (PCA). In comparison to ICA, PCA can reduce computational complexity greatly. For improving motion robustness,  [4]  proposed a CHROM method, which linearly combines the RGB channels to separate pulse signal from motion-induced distortion. Besides,  [28]  proposed a \"plane-orthogonal-to-skin\" (POS) method. Both CHROM and POS are based on the skin reflection model. These hand-crafted methods are straightforward to understand but limited in specific situations, and the estimation performance will drop significantly in complex and unconstrained scenarios.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Deep-Learning Based Methods",
      "text": "To overcome the limitations of traditional methods, many scholars have tried to employ deep learning (DL) technology for remote physiological measurement in recent years. The first DL-based method is DeepPhys  [3] , which computed the difference of frames and used an endto-end convolutional neural network (CNN) to extract physiological signals.  [22]  proposed the HR-CNN which predicts remote HR from aligned face images using a two-step CNN. Due to lots of irrelevant background content in raw face videos, some researchers tried to design efficient manual representations for physiological information.  [16]  designed a novel and efficient spatial-temporal map, which is mapped by a CNN to its HR value, and used a CNN-RNN structure to predict average HR values. In addition,  [17]  attempted to remove the noise via cross-verified feature disentangling.  [13]  tried to use Dual-GAN to model noise distribution and physiological estimator directly.  [31]  proposes an end-to-end video transformer based architecture, to adaptively aggregate both local and global spatiotemporal features for rPPG representation enhancement. These methods using deep networks can be divided into two categories. Methods in the first group (e.g.,  [3, 16, 31] ), focus on mining the subtle BVP signals from face videos, but seldom explicitly model the noises that dominate face video content. They are susceptible to the noises and may suffer from poor generalization ability in unseen scenarios. The second category of methods, which are more close to our approach, focus on directly disentangling the\n\nmc ← me(area); 9: end if 10: return Cropped STMap mc.\n\nphysiological information with non-physiological representations  [17, 13] . However, because of the lack of regularity of these serious random noises, these methods are difficult to converge, which leads to suboptimal performance.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "We denote the input video as v, and the corresponding ground-truth BVP signal as s gt . The goal of remote physiological measurement is to learn a mapping:",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Spatial Temporal Map",
      "text": "Many previous methods  [22, 24]  focused on direct applying CNNs to the face videos with good results. However, due to the low PSNR of rPPG signals in face videos, these methods are inefficient, expensive and time-consuming. Hence, in order to avoid high computational complexity and time-consuming, we choose to use Spatial Temporal Map (STMap) like  [16] , which establishes a preliminary representation of the physiological signal by discarding most of the irrelevant background content.\n\nAn illustration of STMap generation procedure can be found in Fig.  1 . Firstly, a face detector is applied to the face video to obtain the face position and 68 facial landmarks. Secondly, the face is cropped from each frame of the video and converted to YUV color space. Thirdly, the face images is divided into n grids according to the predefined ROI. The average of the pixel values of each grid is calculated and then concatenated into a sequence of T for C channels. The n grids are directly placed into rows. We denote the combined signal sequences as PixelMap. We denote the PixelMap and STMap computed from v as pm and m, respectively. Then our goal is to establish a mapping:\n\nUnlike the previous design of STMap  [16] , we have made the following improvements: (1) We detect faces using RetinaFace  [6]  with MobileNet  [7]  backbone, which can get more precise face landmarks faster. (  2 ) We appropriately reduced the ROI area by discarding non-skin facial areas such as eyes and mouth region. An example of our ROI definition consisting of n = 32 ROIs is shown in the Fig.  1 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Patch Cropping Augmentation",
      "text": "Due to the high collection cost for remote physiological measurement, there are limited data scale and diversity in public datasets. Besides, since the amplitude of the optical absorption color change is very small, applying traditional augmentation methods (e.g., random cropping or flipping) directly may destroy the subtle physiological information. To address these issues, we propose an rPPG-dedicated data",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Drnet",
      "text": "Overall Architecture In our task, noisy data consists of physiological features and non-physiological noise. Specifically, the creation of a PixelMap pm can be generally for-mulated by a linear model:\n\nwhere pm p represents the physiological information which is a noise-free, regular, periodic optical absorption color variation purely caused by the rPPG signal s gt , and pm np denotes the non-physiological information which is the summation of all noise sources physically caused by external environment such as head movements and device noises.\n\nExisting methods either seldom explicitly model the noise pm np , or model the noisy data pm or the noise pm np directly. Due to the lack of sufficient data and the lack of regularity of these strong noises, both of them lead to suboptimal performance.\n\nTo address these problems, DRNet follows the strategy of decomposition and reconstruction. To achieve indirect feature disentangling, we focus on modeling the noise-free and periodic physiological information pm p , in which its generation method is fixed, regular and easy to fit, and then decompose the noise pm np from the noisy data pm using Equation 3. After that, we cross-generate the synthetic data using the decomposed features and noise for reconstructing new samples.\n\nSpecifically, as shown in Fig.  3 , with pairwise input face video clips v 1 , v 2 and the corresponding ground-truth BVP signals s 1  gt , s 2 gt , we first generate the corresponding Pix-elMaps pm 1 and pm 2 computed from v 1 , v 2 respectively. Then, we use the Physiological Generator G p to generate the PixelMaps pm 1 p , pm 2 p from the ground-truth BVP signals s 1 gt , s 2 gt respectively. After that, we apply subtraction operation of subtracting pm 1 p , pm 2 p from pm 1 , pm 2 to separate out the nonphysiological information pm 1 np , pm 2 np respectively. Moreover, pseudo PixelMaps pm 1 pse , pm 2 pse are generated by using different combinations of noises and features, i.e., pseudo PixelMap pm Finally, the STMaps which is generated by the groundtruth PixelMaps pm 1 , pm 2 and the synthetic PixelMaps pm 1 p , pm 2 p , pm 1 pse , pm 2 pse through Magnifying Operation are both fed to the physiological estimator E p for physiological signal predictions.\n\nLoss function For rPPG signal prediction, we use a Pearson correlation based loss to define the similarity between the predicted signal and ground truth. Specifically, the loss function is given by\n\nwhere x is the ground-truth rPPG signals, y is the estimated rPPG signals, x and y denote the mean values of x and y.\n\nIn addition, we propose a cycle loss to constrain the periodicity of the generated PixelMap pm p which represents periodic optical absorption color change. The cycle loss averages randomly selected n c = Rand(1, n) rows from pm p . We denote the averaged result as pm avg with dimension of 1 × T × c. Finally, we constrain the periodicity of c channels of pm avg respectively.\n\nwhere pm i avg denotes the i-th channel of pm avg , HR gt indicates the ground-truth HR, P SD(•) indicates the power spectral density of pm i avg , and CE(•) indicates the crossentropy loss. The overall loss function of our DRNet is",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Spatial Attention Block",
      "text": "For remote physiological signal estimation, what the network really needs to focus on is the faint color variation caused by rPPG. However, noise such as head movements, non-skin regions, occlusion, illumination changes can destroy precise rPPG signal predictions. One idea is to use heavy preprocessing such as skin segmentation or ROI selection algorithms on face videos to reduce the impact of noise. However, this solution is inefficient and timeconsuming. In summary, it is very important to use attention mechanism to effectively extract physiological features. Therefore, we introduce a plug-and-play Spatial Attention Block (SAB) to efficiently utilize physiological information using lightweight computation and memory.\n\nUnlike previous methods (e.g.,  [13] ) using highly redundant attention mechanisms, SAB contains very few parameters. Each row of STMap m represents the raw temporal signal for one ROI on the face, and the input STMap is split by rows. Therefore, as shown in Fig.  4 , 2D-Conv with kernel size 1 × 3 is performed. Inspired by SENet  [8] , our attention module is added to the input feature map in",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Datasets",
      "text": "In this section, we evaluate the effectiveness of the proposed method on three public-domain datasets.  [15]  is a challenging largescale multi-modal database, which contains 2,378 visible light facial videos of 107 subjects. In order to simulate real world conditions as realistic as possible, this dataset was collected under less-constrained scenarios, which contains various variations such as different head movements, illumination condition variations, and acquisition device changes.  [23]  is a public available database for remote heart rate estimation, which comprises RGB videos from 10 subjects(8 male, 2 female) in 6 different setups. The ground-truth rPPG signals were captured using a finger clip pulse oximeter (pulox CMS50E).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Vipl-Hr Vipl-Hr Dataset",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Pure Pure Dataset",
      "text": "UBFC-rPPG UBFC-rPPG dataset  [1]  is a database for remote heart rate estimation, which contains 42 uncompressed RGB videos. In order to make this dataset cover a wider range of heart rate values, all subjects were asked to play a time sensitive mathematical game that supposedly raises their heart rate.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "We perform HR estimation on VIPL-HR, PURE and UBFC-rPPG, and cross-database HR estimation on UBFC-rPPG with training on PURE. For HR estimation, we follow  [17] , and standard deviation of the error (Std), mean absolute error (MAE), root mean square error (RMSE), mean error rate percentage (MER), and Pearson's correlation coefficient (r) are employed for performance evaluation.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Implementation Details",
      "text": "Firstly, our pipeline was implemented using PyTorch framework and trained on one NVIDIA GeForce RTX 3090 GPU. We train the DRNet for 40 epochs, using random initialization. Adam optimizer  [10]  is used while learning rate is set to 0.0001 and batch size is set to 32. Secondly, in all experiments, the length of each video clip T is set to 256 frames, and the step between clips is 10 frames. Besides, we pre-processed the ground truth rPPG signal using a 4th-order Butterworth bandpass filter with cutoff frequency [0.6, 3] Hz for restricting outliers like  [27] . We follow the previous studies  [24, 17, 13]  to compute HR. Finally, all face videos and the corresponding rPPG signals were resampled to 30 fps using cubic spline interpolation like  [13]  before generating STMap. In order to simplify the training process, we pre-train E s and D s as a rPPG signal autoencoder on two public PPG datasets (BIDMC  [19]  and Cuff-less  [9] ), and fix their weights when training the DR-Net.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results",
      "text": "HR estimation on VIPL-HR Following the original protocol in  [16]  for a fair comparison, a subject-exclusive 5fold cross-validation protocol is used on the large-scale VIPL-HR dataset. We compare our method with several baseline methods on VIPL-HR, in which the performance of these baseline methods are directly from  [31] . As illustrated in Table  1 , all three traditional methods (SAMC  [25] , POS  [28]  and CHROM  [4] ) perform quite poorly, struggling to handle the unconstrained complex scenarios (e.g., large head movement and various lighting condition). Similarly, the end-to-end methods (e.g., DeepPhys  [3]  and Phys-Net  [30] ) using raw face videos as input also perform poorly due to the lack of efficient data representation. Directly processing raw facial video is not only time-consuming but also computationally intensive. Moreover, its low signalto-noise ratio is not conducive to the extraction of rPPG signals. The proposed approach is far outperforms all the remaining methods (e.g., RhythmNet  [16]  and Dual-GAN  [13] ) under all measures because DRNet incorporates a novel and powerful data augmentation strategy and an efficient attention module. Such results also demonstrate the superiority of our approach, which focuses not on noisy data modeling like previous methods but on physiological information modeling.\n\nHR estimation on PURE and UBFC-rPPG We further evaluate the effectiveness of the proposed approach by performing HR estimation on two small datasets (PURE and UBFC-rPPG). For PURE dataset, we follow the same testing protocol in  [22]  for a fair comparison. The performance of 2SR, CHROM, HR-CNN, Dual-GAN are available in  [13] . As shown in Table  2 , the proposed DRNet outperforms all the baseline methods under all measures. For UBFC-rPPG dataset, we follow the same testing protocol in  [13] , in which the videos of the first 30 subjects are used for training, and the videos of the remaining 12 subjects are used for testing. The results of POS, CHROM, GREEN, SysRhythm, Dual-GAN are directly from  [13] , and the results of PulseGAN are directly from  [21] . As illustrated in Table  3 , our method still outperforms the baseline methods.\n\nThese results show that our method can not only perform well on large-scale datasets, but also on small datasets.\n\nCross-database HR estimation Cross-dataset evaluations are conducted to testify the generalization ability of our solution under unseen scenarios. We follow PulseGAN  [21]  and Dual-GAN  [13]  to train our model on PURE and valid it on UBFC-rPPG. As shown in Table  4 , the results of GREEN, ICA, POS, CHROM, PulseGAN are from  [21] . Besides, the results of Siamese-rPPG and Dual-GAN are from  [24]  and  [13]  respectively. It can be seen from Table  4  that the DRNet is still very effective for HR estimation under unseen scenarios. These results indicate that the proposed method has a better generalization ability under new scenarios with unknown noises.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ablation Study",
      "text": "In this subsection, all ablation studies are conducted for the proposed method for HR estimation on the large-scale and challenging VIPL-HR database. We train our network using just the physiological estimator E p and the groundtruth STMaps as the baseline method. Effectiveness of PC As shown in Table  5 , PC can improve the performance of HR estimation on VIPL-HR dataset greatly. The vast majority of previous studies lack effective data augmentation strategies. And compared with other existing augmentation methods, PC for the first time realizes the utilization of multi-scale features without destroying the very weak physiological information. It is worth noting that the augmented samples contain different degrees of noise influences and multi-scale physiological information, which is why PC can make remarkable performance gains.\n\nEffectiveness of SAB It can be seen from the second and third rows of Table  5  that the baseline method without SAB achieves worse when without enhancing features along with the spatial location information. In contrast, compared with the baseline method without SAB, the baseline method assembled with SAB obtains 5.09% MAE decrease, which indicates the effectiveness of SAB. Video processing is a time-sensitive task, and the low-parameter SAB can effectively and efficiently extract physiological signals by adaptively adjusting the weights of different channels and face ROIs.\n\nEffectiveness of DRNet As illustrated in Table  5 , with the help of SAB and PC, DRNet makes the prediction results further improved and can reduce the MAE and RMSE errors by 0.29 and 0.51. In addition, as shown in Fig.  5 , we visualize the synthetic noise-free STMap generated by our method and the real noise-less STMap, and we can see that the synthetic noise-free STMap is very similar to the real noise-less STMap. This shows that our network can decompose and model physiological information well, which is a problem not well addressed by previous methods. The advantages of DRNet are three-fold:  (1)  Generating training examples for supervised tasks is a long sought after goal in AI, and DRNet presents a novel data synthesis strategy without modeling noise or noisy data directly. (2) DRNet separates out non-physiological noise by explicitly modeling physiological features, and uses the disentangled features and noise to cross-generate the pseudo data. After that, the pseudo data are used to improve the robustness of the network. (3) The serious random noise and noisy data are difficult or even impossible to model explicitly. Thus, DRNet focuses on: how to obtain a physiological estimator that can extract noise-free features, instead of wasting on direct modeling noise or noisy data.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose an effective end-to-end network for remote physiological sensing using a decomposition and reconstruction strategy to reduce the influences of non-physiological signals and enhance feature disentanglement. Moreover, we design a lightweight and efficient Spatial Attention Block. Besides, a novel Patch Cropping augmentation strategy is proposed for enriching the training data. Extensive experiments are performed to verify the effectiveness of the proposed methods. In the future, we will explore the self-supervised learning technologies for remote physiological measurement.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Spatial Temporal Map.",
      "page": 3
    },
    {
      "caption": "Figure 1: Firstly, a face detector is applied to the face",
      "page": 3
    },
    {
      "caption": "Figure 1: 3.2. Patch Cropping Augmentation",
      "page": 3
    },
    {
      "caption": "Figure 2: Patch Cropping Augmentation.",
      "page": 4
    },
    {
      "caption": "Figure 2: and Algorithm 1 respectively. An enlarged STMap",
      "page": 4
    },
    {
      "caption": "Figure 3: , with pairwise input face",
      "page": 4
    },
    {
      "caption": "Figure 3: Framework of the DRNet. Pairwise face video chips are used for training. We first generate the corresponding PixelMaps pm1,",
      "page": 5
    },
    {
      "caption": "Figure 4: The architecture of the (a) physiological estimator Ep, (b) physiological generator Gp, (c) spatial attention block. “BN” denotes",
      "page": 6
    },
    {
      "caption": "Figure 5: (a) The STMap with real and small noises computed",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "School of Software, Shandong University, China": "dongyuhang42@qq.com,"
        },
        {
          "School of Software, Shandong University, China": "Abstract"
        },
        {
          "School of Software, Shandong University, China": ""
        },
        {
          "School of Software, Shandong University, China": "Remote photoplethysmography (rPPG) based physiolog-"
        },
        {
          "School of Software, Shandong University, China": "ical measurement has great application values in affective"
        },
        {
          "School of Software, Shandong University, China": "computing, non-contact health monitoring, telehealth mon-"
        },
        {
          "School of Software, Shandong University, China": "itoring, etc, which has become increasingly important es-"
        },
        {
          "School of Software, Shandong University, China": "pecially during the COVID-19 pandemic. Existing methods"
        },
        {
          "School of Software, Shandong University, China": "are generally divided into two groups. The first\nfocuses on"
        },
        {
          "School of Software, Shandong University, China": "mining the subtle blood volume pulse (BVP) signals from"
        },
        {
          "School of Software, Shandong University, China": "face videos, but\nseldom explicitly models\nthe noises\nthat"
        },
        {
          "School of Software, Shandong University, China": "dominate face video content.\nThey are susceptible to the"
        },
        {
          "School of Software, Shandong University, China": "noises and may suffer from poor generalization ability in"
        },
        {
          "School of Software, Shandong University, China": "unseen scenarios.\nThe second focuses on modeling noisy"
        },
        {
          "School of Software, Shandong University, China": "data directly,\nresulting in suboptimal performance due to"
        },
        {
          "School of Software, Shandong University, China": ""
        },
        {
          "School of Software, Shandong University, China": "the lack of\nregularity of\nthese severe random noises.\nIn"
        },
        {
          "School of Software, Shandong University, China": ""
        },
        {
          "School of Software, Shandong University, China": "this paper, we propose a Decomposition and Reconstruc-"
        },
        {
          "School of Software, Shandong University, China": ""
        },
        {
          "School of Software, Shandong University, China": "tion Network (DRNet) focusing on the modeling of physio-"
        },
        {
          "School of Software, Shandong University, China": ""
        },
        {
          "School of Software, Shandong University, China": "logical\nfeatures rather than noisy data. A novel cycle loss"
        },
        {
          "School of Software, Shandong University, China": ""
        },
        {
          "School of Software, Shandong University, China": "is proposed to constrain the periodicity of physiological in-"
        },
        {
          "School of Software, Shandong University, China": ""
        },
        {
          "School of Software, Shandong University, China": "formation. Besides, a plug-and-play Spatial Attention Block"
        },
        {
          "School of Software, Shandong University, China": ""
        },
        {
          "School of Software, Shandong University, China": "(SAB) is proposed to enhance features along with the spatial"
        },
        {
          "School of Software, Shandong University, China": ""
        },
        {
          "School of Software, Shandong University, China": "location information. Furthermore, an efficient Patch Crop-"
        },
        {
          "School of Software, Shandong University, China": ""
        },
        {
          "School of Software, Shandong University, China": "ping (PC) augmentation strategy is proposed to synthesize"
        },
        {
          "School of Software, Shandong University, China": ""
        },
        {
          "School of Software, Shandong University, China": "augmented samples with different noise and features. Ex-"
        },
        {
          "School of Software, Shandong University, China": ""
        },
        {
          "School of Software, Shandong University, China": "tensive experiments on different public datasets as well as"
        },
        {
          "School of Software, Shandong University, China": ""
        },
        {
          "School of Software, Shandong University, China": "the cross-database testing demonstrate the effectiveness of"
        },
        {
          "School of Software, Shandong University, China": ""
        },
        {
          "School of Software, Shandong University, China": "our approach."
        },
        {
          "School of Software, Shandong University, China": ""
        },
        {
          "School of Software, Shandong University, China": ""
        },
        {
          "School of Software, Shandong University, China": ""
        },
        {
          "School of Software, Shandong University, China": "1. Introduction"
        },
        {
          "School of Software, Shandong University, China": ""
        },
        {
          "School of Software, Shandong University, China": "Computer vision based remote physiological measure-"
        },
        {
          "School of Software, Shandong University, China": ""
        },
        {
          "School of Software, Shandong University, China": "ment has been gaining a tremendous\ninterest, which has"
        },
        {
          "School of Software, Shandong University, China": ""
        },
        {
          "School of Software, Shandong University, China": "significant advantages compared with traditional contact-"
        },
        {
          "School of Software, Shandong University, China": ""
        },
        {
          "School of Software, Shandong University, China": "based measurements,\nparticularly\nduring\nthe COVID-19"
        },
        {
          "School of Software, Shandong University, China": ""
        },
        {
          "School of Software, Shandong University, China": "pandemic. Firstly,\nthese methods can achieve reliable con-"
        },
        {
          "School of Software, Shandong University, China": ""
        },
        {
          "School of Software, Shandong University, China": "tactless vitals measurement such as heart\nrate (HR), Res-"
        },
        {
          "School of Software, Shandong University, China": ""
        },
        {
          "School of Software, Shandong University, China": "piration Frequency (RF) and heart\nrate variability (HRV),"
        },
        {
          "School of Software, Shandong University, China": ""
        },
        {
          "School of Software, Shandong University, China": "*Corresponding author."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "constrain the periodicity of physiological information.": ""
        },
        {
          "constrain the periodicity of physiological information.": "Our contribution can be summarized as follows:"
        },
        {
          "constrain the periodicity of physiological information.": ""
        },
        {
          "constrain the periodicity of physiological information.": ""
        },
        {
          "constrain the periodicity of physiological information.": "• We propose the first\nrPPG-dedicated data augmenta-"
        },
        {
          "constrain the periodicity of physiological information.": "tion method, Patch Cropping (PC),\nto generate data"
        },
        {
          "constrain the periodicity of physiological information.": "with different degrees of noise influences and multi-"
        },
        {
          "constrain the periodicity of physiological information.": "scale physiological\ninformation, which is able to plug"
        },
        {
          "constrain the periodicity of physiological information.": "and play in not only DRNet but also other existing"
        },
        {
          "constrain the periodicity of physiological information.": "frameworks for performance improvement."
        },
        {
          "constrain the periodicity of physiological information.": ""
        },
        {
          "constrain the periodicity of physiological information.": ""
        },
        {
          "constrain the periodicity of physiological information.": "• To the best of our knowledge, our approach is the first"
        },
        {
          "constrain the periodicity of physiological information.": ""
        },
        {
          "constrain the periodicity of physiological information.": "to focus on modeling physiological\ninformation.\nA"
        },
        {
          "constrain the periodicity of physiological information.": ""
        },
        {
          "constrain the periodicity of physiological information.": "novel cycle loss is proposed to constrain the period-"
        },
        {
          "constrain the periodicity of physiological information.": ""
        },
        {
          "constrain the periodicity of physiological information.": "icity of physiological features. Moreover, a new easy-"
        },
        {
          "constrain the periodicity of physiological information.": ""
        },
        {
          "constrain the periodicity of physiological information.": "to-hard decomposition and reconstruction strategy is"
        },
        {
          "constrain the periodicity of physiological information.": ""
        },
        {
          "constrain the periodicity of physiological information.": "proposed."
        },
        {
          "constrain the periodicity of physiological information.": ""
        },
        {
          "constrain the periodicity of physiological information.": ""
        },
        {
          "constrain the periodicity of physiological information.": "• We propose a lightweight and efficient Spatial Atten-"
        },
        {
          "constrain the periodicity of physiological information.": ""
        },
        {
          "constrain the periodicity of physiological information.": "tion Block (SAB) plugged in our physiological estima-"
        },
        {
          "constrain the periodicity of physiological information.": ""
        },
        {
          "constrain the periodicity of physiological information.": "tor, which can adaptively recalibrate the varying im-"
        },
        {
          "constrain the periodicity of physiological information.": ""
        },
        {
          "constrain the periodicity of physiological information.": "portance of different channels and spatial regions."
        },
        {
          "constrain the periodicity of physiological information.": ""
        },
        {
          "constrain the periodicity of physiological information.": ""
        },
        {
          "constrain the periodicity of physiological information.": "• Our proposed method achieves state-of-the-art perfor-"
        },
        {
          "constrain the periodicity of physiological information.": ""
        },
        {
          "constrain the periodicity of physiological information.": "mance on three public benchmark datasets with intra-"
        },
        {
          "constrain the periodicity of physiological information.": ""
        },
        {
          "constrain the periodicity of physiological information.": "dataset and cross-dataset testing protocols."
        },
        {
          "constrain the periodicity of physiological information.": ""
        },
        {
          "constrain the periodicity of physiological information.": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "face videos (e.g., cropped video frames [22], difference of": "video frames [3], spatial-temporal map [16, 17]). On one",
          "2. Related Work": ""
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "",
          "2. Related Work": "2.1. Traditional Methods"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "hand,\nthese manual\nrepresentations consist of physiologi-",
          "2. Related Work": ""
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "cal and non-physiological\ninformation, and the estimation",
          "2. Related Work": ""
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "",
          "2. Related Work": "rPPG is\nthe monitoring of blood volume pulse (BVP)"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "performance can be easily affected by non-physiological in-",
          "2. Related Work": ""
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "",
          "2. Related Work": "from a\ncamera\nat\na distance.\n[26] proved,\nfor\nthe first"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "formation such as head movements and lighting variations.",
          "2. Related Work": ""
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "",
          "2. Related Work": "time,\nthat PPG signals can be measured remotely (>1m)"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "On the other hand,\nthe key challenge of rPPG-based phys-",
          "2. Related Work": ""
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "",
          "2. Related Work": "from face videos using ambient\nlight.\nAfter\nthat, many"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "iological measurement\nis how to effectively extract phys-",
          "2. Related Work": ""
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "",
          "2. Related Work": "scholars have devoted their efforts in this challenging task."
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "iological\ninformation and suppress\nthe adverse effects of",
          "2. Related Work": ""
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "",
          "2. Related Work": "[20] introduced a new methodology which applied indepen-"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "non-physiological information. However, existing methods",
          "2. Related Work": ""
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "",
          "2. Related Work": "dent component analysis (ICA) to reconstruct rPPG signals"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "either seldom explicitly model\nthe serious random noises,",
          "2. Related Work": ""
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "",
          "2. Related Work": "from face videos.\nSimilarly,\n[11] proposed a new rPPG"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "or model noisy data directly. Due to the lack of sufficient",
          "2. Related Work": ""
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "",
          "2. Related Work": "signals estimation method based on principal component"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "data and the lack of regularity of these strong noises, both",
          "2. Related Work": ""
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "",
          "2. Related Work": "analysis\n(PCA).\nIn comparison to ICA, PCA can reduce"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "of them lead to suboptimal performance.",
          "2. Related Work": ""
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "",
          "2. Related Work": "computational complexity greatly.\nFor\nimproving motion"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "Motivated by the above discussions, rather than model-",
          "2. Related Work": "robustness,\n[4] proposed a CHROM method, which lin-"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "ing noisy data with the severe lack of regularity,\nit is better",
          "2. Related Work": "early combines the RGB channels to separate pulse signal"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "to directly model\nregular, periodic physiological\nfeatures.",
          "2. Related Work": "from motion-induced distortion. Besides,\n[28] proposed a"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "In other words, unlike previous studies, our approach fo-",
          "2. Related Work": "“plane-orthogonal-to-skin” (POS) method. Both CHROM"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "cuses on modeling physiological\ninformation rather\nthan",
          "2. Related Work": "and POS are based on the skin reflection model.\nThese"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "modeling noisy data.\nIn this way, we propose\nthe De-",
          "2. Related Work": "hand-crafted methods are straightforward to understand but"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "composition and Reconstruction Network (DRNet), which",
          "2. Related Work": "limited in specific\nsituations,\nand the\nestimation perfor-"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "adopts\na\nnovel\ndecomposition\nand\nreconstruction\nstrat-",
          "2. Related Work": "mance will drop significantly in complex and unconstrained"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "egy (models physiological\nfeatures directly,\ndecomposes",
          "2. Related Work": "scenarios."
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "non-physiological noise indirectly,\nand then uses\nthe de-",
          "2. Related Work": ""
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "composed noise and features to reconstruct new synthetic",
          "2. Related Work": "2.2. Deep-learning based Methods"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "samples).\nFurthermore, we propose a novel cycle loss to",
          "2. Related Work": ""
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "",
          "2. Related Work": "To\novercome\nthe\nlimitations\nof\ntraditional methods,"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "constrain the periodicity of physiological information.",
          "2. Related Work": ""
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "",
          "2. Related Work": "many scholars have\ntried to employ deep learning (DL)"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "Our contribution can be summarized as follows:",
          "2. Related Work": ""
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "",
          "2. Related Work": "technology for\nremote physiological measurement\nin re-"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "",
          "2. Related Work": "cent years.\nThe first DL-based method is DeepPhys\n[3],"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "• We propose the first\nrPPG-dedicated data augmenta-",
          "2. Related Work": "which computed the difference of frames and used an end-"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "tion method, Patch Cropping (PC),\nto generate data",
          "2. Related Work": "to-end convolutional neural network (CNN) to extract phys-"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "with different degrees of noise influences and multi-",
          "2. Related Work": "iological signals.\n[22] proposed the HR-CNN which pre-"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "scale physiological\ninformation, which is able to plug",
          "2. Related Work": "dicts remote HR from aligned face images using a two-step"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "and play in not only DRNet but also other existing",
          "2. Related Work": "CNN. Due to lots of irrelevant background content\nin raw"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "frameworks for performance improvement.",
          "2. Related Work": "face videos, some researchers tried to design efficient man-"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "",
          "2. Related Work": "ual representations for physiological\ninformation.\n[16] de-"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "",
          "2. Related Work": "signed a novel and efficient\nspatial-temporal map, which"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "• To the best of our knowledge, our approach is the first",
          "2. Related Work": ""
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "",
          "2. Related Work": "is mapped by a CNN to its HR value, and used a CNN-"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "to focus on modeling physiological\ninformation.\nA",
          "2. Related Work": ""
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "",
          "2. Related Work": "RNN structure to predict average HR values.\nIn addition,"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "novel cycle loss is proposed to constrain the period-",
          "2. Related Work": ""
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "",
          "2. Related Work": "[17] attempted to remove the noise via cross-verified fea-"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "icity of physiological features. Moreover, a new easy-",
          "2. Related Work": ""
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "",
          "2. Related Work": "ture disentangling.\n[13]\ntried to use Dual-GAN to model"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "to-hard decomposition and reconstruction strategy is",
          "2. Related Work": ""
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "",
          "2. Related Work": "noise distribution and physiological estimator directly. [31]"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "proposed.",
          "2. Related Work": ""
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "",
          "2. Related Work": "proposes an end-to-end video transformer based architec-"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "",
          "2. Related Work": "ture,\nto adaptively aggregate both local and global spatio-"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "• We propose a lightweight and efficient Spatial Atten-",
          "2. Related Work": ""
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "",
          "2. Related Work": "temporal\nfeatures\nfor\nrPPG representation enhancement."
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "tion Block (SAB) plugged in our physiological estima-",
          "2. Related Work": ""
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "",
          "2. Related Work": "These methods using deep networks can be divided into two"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "tor, which can adaptively recalibrate the varying im-",
          "2. Related Work": ""
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "",
          "2. Related Work": "categories. Methods in the first group (e.g.,\n[3, 16, 31]),"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "portance of different channels and spatial regions.",
          "2. Related Work": ""
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "",
          "2. Related Work": "focus on mining the subtle BVP signals from face videos,"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "",
          "2. Related Work": "but seldom explicitly model\nthe noises that dominate face"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "• Our proposed method achieves state-of-the-art perfor-",
          "2. Related Work": ""
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "",
          "2. Related Work": "video content. They are susceptible to the noises and may"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "mance on three public benchmark datasets with intra-",
          "2. Related Work": ""
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "",
          "2. Related Work": "suffer\nfrom poor generalization ability in unseen scenar-"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "dataset and cross-dataset testing protocols.",
          "2. Related Work": ""
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "",
          "2. Related Work": "ios.\nThe\nsecond category of methods, which are more"
        },
        {
          "face videos (e.g., cropped video frames [22], difference of": "",
          "2. Related Work": "close to our approach,\nfocus on directly disentangling the"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 1. Spatial Temporal Map.": "Algorithm 1 Patch Cropping Augmentation"
        },
        {
          "Figure 1. Spatial Temporal Map.": "Input: Original STMap m; Enlarged STMap me; Cropping prob-"
        },
        {
          "Figure 1. Spatial Temporal Map.": "ability ρ, Original STMap size C × n × T ; Enlarged STMap size"
        },
        {
          "Figure 1. Spatial Temporal Map.": "C × ne × T ."
        },
        {
          "Figure 1. Spatial Temporal Map.": "1: ρ1 ← Rand(0, 1)."
        },
        {
          "Figure 1. Spatial Temporal Map.": "2:\nif ρ1 ≥ ρ then"
        },
        {
          "Figure 1. Spatial Temporal Map.": ""
        },
        {
          "Figure 1. Spatial Temporal Map.": "3:\nmc ← m;"
        },
        {
          "Figure 1. Spatial Temporal Map.": ""
        },
        {
          "Figure 1. Spatial Temporal Map.": "4:\nelse"
        },
        {
          "Figure 1. Spatial Temporal Map.": ""
        },
        {
          "Figure 1. Spatial Temporal Map.": "5:\nx ← 0, y ← Rand(0, ne − n);"
        },
        {
          "Figure 1. Spatial Temporal Map.": ""
        },
        {
          "Figure 1. Spatial Temporal Map.": "6:\nW ← T, H ← n"
        },
        {
          "Figure 1. Spatial Temporal Map.": ""
        },
        {
          "Figure 1. Spatial Temporal Map.": "7:\narea ← (x, y, x + W, y + H);"
        },
        {
          "Figure 1. Spatial Temporal Map.": ""
        },
        {
          "Figure 1. Spatial Temporal Map.": "8:\nmc ← me(area);"
        },
        {
          "Figure 1. Spatial Temporal Map.": ""
        },
        {
          "Figure 1. Spatial Temporal Map.": "9:\nend if"
        },
        {
          "Figure 1. Spatial Temporal Map.": ""
        },
        {
          "Figure 1. Spatial Temporal Map.": "10:\nreturn Cropped STMap mc."
        },
        {
          "Figure 1. Spatial Temporal Map.": ""
        },
        {
          "Figure 1. Spatial Temporal Map.": ""
        },
        {
          "Figure 1. Spatial Temporal Map.": "physiological information with non-physiological represen-"
        },
        {
          "Figure 1. Spatial Temporal Map.": "tations [17, 13]. However, because of the lack of regularity"
        },
        {
          "Figure 1. Spatial Temporal Map.": "of these serious random noises,\nthese methods are difficult"
        },
        {
          "Figure 1. Spatial Temporal Map.": "to converge, which leads to suboptimal performance."
        },
        {
          "Figure 1. Spatial Temporal Map.": ""
        },
        {
          "Figure 1. Spatial Temporal Map.": "3. Methodology"
        },
        {
          "Figure 1. Spatial Temporal Map.": ""
        },
        {
          "Figure 1. Spatial Temporal Map.": "We denote the input video as v, and the corresponding"
        },
        {
          "Figure 1. Spatial Temporal Map.": ""
        },
        {
          "Figure 1. Spatial Temporal Map.": "ground-truth BVP signal as sgt. The goal of remote physi-"
        },
        {
          "Figure 1. Spatial Temporal Map.": ""
        },
        {
          "Figure 1. Spatial Temporal Map.": "ological measurement is to learn a mapping:"
        },
        {
          "Figure 1. Spatial Temporal Map.": ""
        },
        {
          "Figure 1. Spatial Temporal Map.": ""
        },
        {
          "Figure 1. Spatial Temporal Map.": "(1)\nF : v → sgt"
        },
        {
          "Figure 1. Spatial Temporal Map.": ""
        },
        {
          "Figure 1. Spatial Temporal Map.": "3.1. Spatial Temporal Map"
        },
        {
          "Figure 1. Spatial Temporal Map.": "Many previous methods [22, 24]\nfocused on direct ap-"
        },
        {
          "Figure 1. Spatial Temporal Map.": ""
        },
        {
          "Figure 1. Spatial Temporal Map.": "plying CNNs to the face videos with good results. However,"
        },
        {
          "Figure 1. Spatial Temporal Map.": "due to the low PSNR of rPPG signals in face videos,\nthese"
        },
        {
          "Figure 1. Spatial Temporal Map.": "methods\nare\ninefficient,\nexpensive\nand\ntime-consuming."
        },
        {
          "Figure 1. Spatial Temporal Map.": "Hence, in order to avoid high computational complexity and"
        },
        {
          "Figure 1. Spatial Temporal Map.": "time-consuming, we choose to use Spatial Temporal Map"
        },
        {
          "Figure 1. Spatial Temporal Map.": "(STMap)\nlike [16], which establishes a preliminary repre-"
        },
        {
          "Figure 1. Spatial Temporal Map.": "sentation of the physiological signal by discarding most of"
        },
        {
          "Figure 1. Spatial Temporal Map.": "the irrelevant background content."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Video Clip v\nEnlarged STMap me": "Figure 2. Patch Cropping Augmentation."
        },
        {
          "Video Clip v\nEnlarged STMap me": "augmentation method, named Patch Cropping (PC), to syn-\nmulated by a linear model:"
        },
        {
          "Video Clip v\nEnlarged STMap me": "thesize new samples with different noise levels."
        },
        {
          "Video Clip v\nEnlarged STMap me": "(3)\npm = pmp + pmnp"
        },
        {
          "Video Clip v\nEnlarged STMap me": "The illustration and algorithm of PC are summarized in"
        },
        {
          "Video Clip v\nEnlarged STMap me": "Fig.\n2 and Algorithm 1 respectively. An enlarged STMap"
        },
        {
          "Video Clip v\nEnlarged STMap me": "where pmp represents the physiological information which"
        },
        {
          "Video Clip v\nEnlarged STMap me": "can be generated by subdividing each face ROI\ninto\nme"
        },
        {
          "Video Clip v\nEnlarged STMap me": "is a noise-free,\nregular, periodic optical absorption color"
        },
        {
          "Video Clip v\nEnlarged STMap me": "γ × γ sub-ROIs. Besides,\nthe enlarged STMap me has the"
        },
        {
          "Video Clip v\nEnlarged STMap me": "variation purely caused by the rPPG signal sgt, and pmnp"
        },
        {
          "Video Clip v\nEnlarged STMap me": "dimension of C × ne × T ,\nin which ne = n × γ × γ."
        },
        {
          "Video Clip v\nEnlarged STMap me": "denotes\nthe\nnon-physiological\ninformation which\nis\nthe"
        },
        {
          "Video Clip v\nEnlarged STMap me": "Specifically, a total of 32 face ROIs are defined in the pro-"
        },
        {
          "Video Clip v\nEnlarged STMap me": "summation of all noise sources physically caused by exter-"
        },
        {
          "Video Clip v\nEnlarged STMap me": "cess of generating the original STMap m. When γ is set"
        },
        {
          "Video Clip v\nEnlarged STMap me": "nal environment such as head movements and device noises."
        },
        {
          "Video Clip v\nEnlarged STMap me": "to 2, each face ROI\nis divided into 2 × 2 sub-ROIs to get"
        },
        {
          "Video Clip v\nEnlarged STMap me": "Existing methods either seldom explicitly model\nthe noise"
        },
        {
          "Video Clip v\nEnlarged STMap me": "a new ROI definition consisting of 128 ROIs,\nin which"
        },
        {
          "Video Clip v\nEnlarged STMap me": "the noisy data pm or\npmnp, or model\nthe noise pmnp di-"
        },
        {
          "Video Clip v\nEnlarged STMap me": "128 = 32 × 2 × 2. After\nthat,\nthese two ROI definitions"
        },
        {
          "Video Clip v\nEnlarged STMap me": "rectly. Due to the lack of sufficient data and the lack of"
        },
        {
          "Video Clip v\nEnlarged STMap me": "are used to generate the original STMap m of dimension"
        },
        {
          "Video Clip v\nEnlarged STMap me": "regularity of these strong noises, both of them lead to sub-"
        },
        {
          "Video Clip v\nEnlarged STMap me": "3 × 32 × 256 and the enlarged STMap me of dimension"
        },
        {
          "Video Clip v\nEnlarged STMap me": "optimal performance."
        },
        {
          "Video Clip v\nEnlarged STMap me": "3 × 128 × 256, respectively. Finally, without direct interpo-"
        },
        {
          "Video Clip v\nEnlarged STMap me": "To address these problems, DRNet follows the strategy"
        },
        {
          "Video Clip v\nEnlarged STMap me": "lating,\nlossless data augmentation is achieved by randomly"
        },
        {
          "Video Clip v\nEnlarged STMap me": "of decomposition and reconstruction.\nTo achieve indirect"
        },
        {
          "Video Clip v\nEnlarged STMap me": "cropping the enlarged STMap me. The two hyperparame-"
        },
        {
          "Video Clip v\nEnlarged STMap me": "feature disentangling, we focus on modeling the noise-free"
        },
        {
          "Video Clip v\nEnlarged STMap me": "ters γ and ρ control\nthe enlared ratio and the augmentation"
        },
        {
          "Video Clip v\nEnlarged STMap me": "and periodic physiological\nin which its\ninformation pmp,"
        },
        {
          "Video Clip v\nEnlarged STMap me": "intensity, respectively. As a tradeoff, we use empirical set-"
        },
        {
          "Video Clip v\nEnlarged STMap me": "generation method is fixed, regular and easy to fit, and then"
        },
        {
          "Video Clip v\nEnlarged STMap me": "tings γ = 2 and ρ = 0.5 for experiments."
        },
        {
          "Video Clip v\nEnlarged STMap me": "decompose the noise pmnp from the noisy data pm using"
        },
        {
          "Video Clip v\nEnlarged STMap me": "There are three advantages for PC: (1) In PC, the cropped"
        },
        {
          "Video Clip v\nEnlarged STMap me": "Equation 3. After that, we cross-generate the synthetic data"
        },
        {
          "Video Clip v\nEnlarged STMap me": "STMap mc and the original STMap m describe the sub-\nusing the decomposed features and noise for reconstructing"
        },
        {
          "Video Clip v\nEnlarged STMap me": "ject’s physiological\ninformation from detailed and general"
        },
        {
          "Video Clip v\nEnlarged STMap me": "new samples."
        },
        {
          "Video Clip v\nEnlarged STMap me": "aspects respectively.\n(2) In PC, a more detailed STMap me"
        },
        {
          "Video Clip v\nEnlarged STMap me": "Specifically, as shown in Fig. 3, with pairwise input face"
        },
        {
          "Video Clip v\nEnlarged STMap me": "is obtained by subdividing each face ROI, which avoids the"
        },
        {
          "Video Clip v\nEnlarged STMap me": "video clips v1, v2 and the corresponding ground-truth BVP"
        },
        {
          "Video Clip v\nEnlarged STMap me": "computational error of other existing methods (e.g., random"
        },
        {
          "Video Clip v\nEnlarged STMap me": "signals s1\ngt, s2\ngt, we first generate the corresponding Pix-"
        },
        {
          "Video Clip v\nEnlarged STMap me": "cropping, upsampling and downsampling [18]) caused by"
        },
        {
          "Video Clip v\nEnlarged STMap me": "elMaps pm1 and pm2 computed from v1, v2 respectively."
        },
        {
          "Video Clip v\nEnlarged STMap me": "direct cropping or\ninterpolating.\n(3) Compared with the"
        },
        {
          "Video Clip v\nEnlarged STMap me": "Then, we use the Physiological Generator Gp to generate"
        },
        {
          "Video Clip v\nEnlarged STMap me": "original STMap m,\nthe cropped STMap mc contains sim-"
        },
        {
          "Video Clip v\nEnlarged STMap me": "the PixelMaps pm1\np, pm2\np from the ground-truth BVP sig-"
        },
        {
          "Video Clip v\nEnlarged STMap me": "ilar and multi-scale physiological information and different"
        },
        {
          "Video Clip v\nEnlarged STMap me": "nals s1\ngt, s2\ngt respectively."
        },
        {
          "Video Clip v\nEnlarged STMap me": "levels of noise influences, not achieved by other existing"
        },
        {
          "Video Clip v\nEnlarged STMap me": "After\nthat, we apply subtraction operation of subtract-"
        },
        {
          "Video Clip v\nEnlarged STMap me": "methods (e.g., random horizontal and vertical flipping [17])."
        },
        {
          "Video Clip v\nEnlarged STMap me": "ing pm1\nfrom pm1, pm2\nto separate out\nthe non-\np, pm2"
        },
        {
          "Video Clip v\nEnlarged STMap me": "physiological information pm1\nnp, pm2\nnp respectively. More-"
        },
        {
          "Video Clip v\nEnlarged STMap me": "3.3. DRNet"
        },
        {
          "Video Clip v\nEnlarged STMap me": "over, pseudo PixelMaps pm1\nare generated by\npse, pm2\npse"
        },
        {
          "Video Clip v\nEnlarged STMap me": "Overall Architecture\nIn our task, noisy data consists of\nusing different combinations of noises and features,\ni.e.,"
        },
        {
          "Video Clip v\nEnlarged STMap me": "physiological features and non-physiological noise. Specif-\npseudo PixelMap pm1\nis generated by adding pm1\nto\npse\np"
        },
        {
          "Video Clip v\nEnlarged STMap me": "pm2\nically,\nthe creation of a PixelMap pm can be generally for-\nnp, and pm2\npse is generated by adding pm2\np to pm1\nnp."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2": "Ground-turth BVP sgt",
          "Physiological Estimator Ep": ""
        },
        {
          "2": "PixelMap pm2",
          "Physiological Estimator Ep": ""
        },
        {
          "2": "",
          "Physiological Estimator Ep": "STMap m2"
        },
        {
          "2": "Decomposition and Reconstruction",
          "Physiological Estimator Ep": ""
        },
        {
          "2": "Video v2",
          "Physiological Estimator Ep": ""
        },
        {
          "2": "Figure 3. Framework of the DRNet. Pairwise face video chips are used for training. We first generate the corresponding PixelMaps pm1,",
          "Physiological Estimator Ep": ""
        },
        {
          "2": "pm2 of the input face video clips. Then we feed the ground-truth BVP signals into the Physiological Generator Gp to generate the noise-",
          "Physiological Estimator Ep": ""
        },
        {
          "2": "free PixelMaps. After that, we cross-generate the pseudo PixelMaps pm1",
          "Physiological Estimator Ep": "pse, pm2\npse using different combinations of noises and features"
        },
        {
          "2": "from different original PixelMaps. The physiological estimator Ep takes the STMaps which is generated by the ground-truth PixelMaps",
          "Physiological Estimator Ep": ""
        },
        {
          "2": "pm1, pm2 and the synthetic PixelMaps pm1\np, pm2\np, pm1\npse, pm2",
          "Physiological Estimator Ep": "pse through magnifying operation as input for rPPG signal predictions. The"
        },
        {
          "2": "modules of the same type in our network use shared weights.",
          "Physiological Estimator Ep": ""
        },
        {
          "2": "Finally,\nthe STMaps which is generated by the ground-",
          "Physiological Estimator Ep": "where pmi\nin-\navg denotes the i-th channel of pmavg, HRgt"
        },
        {
          "2": "truth PixelMaps pm1, pm2\nand the synthetic PixelMaps",
          "Physiological Estimator Ep": "dicates the ground-truth HR, P SD(·) indicates the power"
        },
        {
          "2": "pm1\nthrough Magnifying Operation\npse\npse, pm2\np, pm1\np, pm2",
          "Physiological Estimator Ep": "spectral density of pmi\navg, and CE(·) indicates the cross-"
        },
        {
          "2": "are both fed to the physiological estimator Ep for physio-",
          "Physiological Estimator Ep": "entropy loss. The overall loss function of our DRNet is"
        },
        {
          "2": "logical signal predictions.",
          "Physiological Estimator Ep": ""
        },
        {
          "2": "",
          "Physiological Estimator Ep": "(6)\nL = Lphy + Lcyc"
        },
        {
          "2": "",
          "Physiological Estimator Ep": "3.4. Spatial Attention Block"
        },
        {
          "2": "Loss function\nFor rPPG signal prediction, we use a Pear-",
          "Physiological Estimator Ep": ""
        },
        {
          "2": "son correlation based loss to define the similarity between",
          "Physiological Estimator Ep": "For\nremote physiological\nsignal\nestimation, what\nthe"
        },
        {
          "2": "the predicted signal and ground truth. Specifically, the loss",
          "Physiological Estimator Ep": "network really needs to focus on is the faint color varia-"
        },
        {
          "2": "function is given by",
          "Physiological Estimator Ep": "tion caused by rPPG. However, noise such as head move-"
        },
        {
          "2": "",
          "Physiological Estimator Ep": "ments, non-skin regions, occlusion,\nillumination changes"
        },
        {
          "2": "(cid:80)T",
          "Physiological Estimator Ep": ""
        },
        {
          "2": "t=1(xi − x)(yi − y)",
          "Physiological Estimator Ep": "can destroy precise rPPG signal predictions. One idea is"
        },
        {
          "2": "(4)\nLphy = 1 −",
          "Physiological Estimator Ep": ""
        },
        {
          "2": "(cid:113)(cid:80)T\n(cid:113)(cid:80)T",
          "Physiological Estimator Ep": "to use heavy preprocessing such as\nskin segmentation or"
        },
        {
          "2": "t=1(xi − x)2\nt=1(yi − y)2",
          "Physiological Estimator Ep": ""
        },
        {
          "2": "",
          "Physiological Estimator Ep": "ROI selection algorithms on face videos to reduce the im-"
        },
        {
          "2": "",
          "Physiological Estimator Ep": "pact of noise. However, this solution is inefficient and time-"
        },
        {
          "2": "where x is the ground-truth rPPG signals, y is the estimated",
          "Physiological Estimator Ep": ""
        },
        {
          "2": "",
          "Physiological Estimator Ep": "consuming.\nIn summary,\nit\nis very important\nto use at-"
        },
        {
          "2": "rPPG signals, x and y denote the mean values of x and y.",
          "Physiological Estimator Ep": ""
        },
        {
          "2": "",
          "Physiological Estimator Ep": "tention mechanism to effectively extract physiological fea-"
        },
        {
          "2": "In addition, we propose a cycle loss to constrain the pe-",
          "Physiological Estimator Ep": ""
        },
        {
          "2": "",
          "Physiological Estimator Ep": "tures. Therefore, we introduce a plug-and-play Spatial At-"
        },
        {
          "2": "riodicity of the generated PixelMap pmp which represents",
          "Physiological Estimator Ep": ""
        },
        {
          "2": "",
          "Physiological Estimator Ep": "tention Block (SAB) to efficiently utilize physiological\nin-"
        },
        {
          "2": "periodic optical absorption color change.\nThe cycle loss",
          "Physiological Estimator Ep": ""
        },
        {
          "2": "",
          "Physiological Estimator Ep": "formation using lightweight computation and memory."
        },
        {
          "2": "averages randomly selected nc = Rand(1, n) rows from",
          "Physiological Estimator Ep": ""
        },
        {
          "2": "",
          "Physiological Estimator Ep": "Unlike previous methods (e.g., [13]) using highly redun-"
        },
        {
          "2": "pmp. We denote the averaged result as pmavg with dimen-",
          "Physiological Estimator Ep": ""
        },
        {
          "2": "",
          "Physiological Estimator Ep": "dant attention mechanisms, SAB contains very few param-"
        },
        {
          "2": "sion of 1 × T × c. Finally, we constrain the periodicity of c",
          "Physiological Estimator Ep": ""
        },
        {
          "2": "",
          "Physiological Estimator Ep": "eters.\nEach row of STMap m represents the raw tempo-"
        },
        {
          "2": "channels of pmavg respectively.",
          "Physiological Estimator Ep": ""
        },
        {
          "2": "",
          "Physiological Estimator Ep": "ral signal\nfor one ROI on the face, and the input STMap"
        },
        {
          "2": "",
          "Physiological Estimator Ep": "is split by rows. Therefore, as shown in Fig.\n4, 2D-Conv"
        },
        {
          "2": "",
          "Physiological Estimator Ep": "with kernel size 1 × 3 is performed. Inspired by SENet [8],"
        },
        {
          "2": "1 c\nc(cid:88) i\nCE(P SD(pmi\n(5)\nLcyc =\navg), HRgt)",
          "Physiological Estimator Ep": ""
        },
        {
          "2": "",
          "Physiological Estimator Ep": "our attention module is added to the input\nfeature map in"
        },
        {
          "2": "=1",
          "Physiological Estimator Ep": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Conv, BN": "ReLU",
          "STMap": "",
          "BVP": ""
        },
        {
          "Conv, BN": "",
          "STMap": "DownBlock, 32",
          "BVP": "DownBlock, 32"
        },
        {
          "Conv, BN": "Conv, BN",
          "STMap": "",
          "BVP": ""
        },
        {
          "Conv, BN": "",
          "STMap": "",
          "BVP": ""
        },
        {
          "Conv, BN": "ReLU",
          "STMap": "",
          "BVP": ""
        },
        {
          "Conv, BN": "",
          "STMap": "DownBlock, 64",
          "BVP": "DownBlock, 64"
        },
        {
          "Conv, BN": "",
          "STMap": "",
          "BVP": ""
        },
        {
          "Conv, BN": "",
          "STMap": "DownBlock, 128",
          "BVP": "DownBlock, 128"
        },
        {
          "Conv, BN": "",
          "STMap": "",
          "BVP": ""
        },
        {
          "Conv, BN": "",
          "STMap": "DownBlock, 256",
          "BVP": "DownBlock, 256"
        },
        {
          "Conv, BN": "",
          "STMap": "",
          "BVP": ""
        },
        {
          "Conv, BN": "",
          "STMap": "DownBlock, 512",
          "BVP": "DownBlock, 512"
        },
        {
          "Conv, BN": "",
          "STMap": "",
          "BVP": ""
        },
        {
          "Conv, BN": "",
          "STMap": "UpBlock, 256",
          "BVP": "UpBlock, 256"
        },
        {
          "Conv, BN": "",
          "STMap": "",
          "BVP": ""
        },
        {
          "Conv, BN": "",
          "STMap": "UpBlock, 128",
          "BVP": "UpBlock, 128"
        },
        {
          "Conv, BN": "",
          "STMap": "",
          "BVP": ""
        },
        {
          "Conv, BN": "",
          "STMap": "UpBlock, 64",
          "BVP": "UpBlock, 64"
        },
        {
          "Conv, BN": "",
          "STMap": "",
          "BVP": ""
        },
        {
          "Conv, BN": "TransConv",
          "STMap": "",
          "BVP": ""
        },
        {
          "Conv, BN": "",
          "STMap": "UpBlock, 32",
          "BVP": "UpBlock, 32"
        },
        {
          "Conv, BN": "Conv, BN",
          "STMap": "",
          "BVP": ""
        },
        {
          "Conv, BN": "",
          "STMap": "",
          "BVP": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Scale": ""
        },
        {
          "Scale": "C x H x W"
        },
        {
          "Scale": ""
        },
        {
          "Scale": ""
        },
        {
          "Scale": "（c) Spatial Attention Block"
        },
        {
          "Scale": ""
        },
        {
          "Scale": ""
        },
        {
          "Scale": "4.2. Evaluation Metrics"
        },
        {
          "Scale": ""
        },
        {
          "Scale": "We perform HR estimation on VIPL-HR, PURE and"
        },
        {
          "Scale": ""
        },
        {
          "Scale": "UBFC-rPPG, and cross-database HR estimation on UBFC-"
        },
        {
          "Scale": ""
        },
        {
          "Scale": "rPPG with training on PURE. For HR estimation, we follow"
        },
        {
          "Scale": ""
        },
        {
          "Scale": "[17], and standard deviation of the error (Std), mean abso-"
        },
        {
          "Scale": "lute error\n(MAE),\nroot mean square error\n(RMSE), mean"
        },
        {
          "Scale": "error rate percentage (MER), and Pearson’s correlation co-"
        },
        {
          "Scale": "efficient (r) are employed for performance evaluation."
        },
        {
          "Scale": ""
        },
        {
          "Scale": "4.3. Implementation Details"
        },
        {
          "Scale": ""
        },
        {
          "Scale": "Firstly,\nour pipeline was\nimplemented using PyTorch"
        },
        {
          "Scale": ""
        },
        {
          "Scale": "framework and trained on one NVIDIA GeForce RTX 3090"
        },
        {
          "Scale": "GPU. We train the DRNet for 40 epochs, using random ini-"
        },
        {
          "Scale": "tialization. Adam optimizer [10] is used while learning rate"
        },
        {
          "Scale": "is set\nto 0.0001 and batch size is set\nto 32.\nSecondly,\nin"
        },
        {
          "Scale": "all experiments,\nthe length of each video clip T is set\nto"
        },
        {
          "Scale": "256 frames, and the step between clips is 10 frames. Be-"
        },
        {
          "Scale": "sides, we pre-processed the ground truth rPPG signal us-"
        },
        {
          "Scale": "ing a 4th-order Butterworth bandpass filter with cutoff fre-"
        },
        {
          "Scale": "quency [0.6, 3] Hz for restricting outliers like [27]. We fol-"
        },
        {
          "Scale": "low the previous studies [24, 17, 13]\nto compute HR. Fi-"
        },
        {
          "Scale": "nally, all\nface videos and the corresponding rPPG signals"
        },
        {
          "Scale": "were resampled to 30 fps using cubic spline interpolation"
        },
        {
          "Scale": "like [13] before generating STMap. In order to simplify the"
        },
        {
          "Scale": "training process, we pre-train Es and Ds as a rPPG signal"
        },
        {
          "Scale": "autoencoder on two public PPG datasets (BIDMC [19] and"
        },
        {
          "Scale": "Cuff-less [9]), and fix their weights when training the DR-"
        },
        {
          "Scale": "Net."
        },
        {
          "Scale": "4.4. Results"
        },
        {
          "Scale": ""
        },
        {
          "Scale": "HR estimation on VIPL-HR\nFollowing the original pro-"
        },
        {
          "Scale": "tocol\nin [16]\nfor a fair comparison, a subject-exclusive 5-"
        },
        {
          "Scale": "fold cross-validation protocol\nis used on the\nlarge-scale"
        },
        {
          "Scale": "VIPL-HR dataset. We compare our method with several"
        },
        {
          "Scale": "baseline methods on VIPL-HR,\nin which the performance"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 4: Cross-database HR estimation (training on PURE and",
      "data": [
        {
          "Table 1. HR estimation results by our method and several state-of-": "the-art methods on the VIPL-HR database.",
          "Table 3. HR estimation results by our method and several state-of-": "the-art methods on the UBFC-rPPG database."
        },
        {
          "Table 1. HR estimation results by our method and several state-of-": "Method",
          "Table 3. HR estimation results by our method and several state-of-": "MAE↓ RMSE↓ MER↓ r↑"
        },
        {
          "Table 1. HR estimation results by our method and several state-of-": "SAMC [25]",
          "Table 3. HR estimation results by our method and several state-of-": "8.35"
        },
        {
          "Table 1. HR estimation results by our method and several state-of-": "POS [28]",
          "Table 3. HR estimation results by our method and several state-of-": "8.20"
        },
        {
          "Table 1. HR estimation results by our method and several state-of-": "CHROM [4]",
          "Table 3. HR estimation results by our method and several state-of-": "Opt. Express 6.01"
        },
        {
          "Table 1. HR estimation results by our method and several state-of-": "I3D [2]",
          "Table 3. HR estimation results by our method and several state-of-": "5.59"
        },
        {
          "Table 1. HR estimation results by our method and several state-of-": "DeepPhys [3]",
          "Table 3. HR estimation results by our method and several state-of-": "1.19"
        },
        {
          "Table 1. HR estimation results by our method and several state-of-": "PhysNet [30]",
          "Table 3. HR estimation results by our method and several state-of-": "0.44"
        },
        {
          "Table 1. HR estimation results by our method and several state-of-": "AutoHR [29]",
          "Table 3. HR estimation results by our method and several state-of-": "0.42"
        },
        {
          "Table 1. HR estimation results by our method and several state-of-": "ST-attention [18]",
          "Table 3. HR estimation results by our method and several state-of-": ""
        },
        {
          "Table 1. HR estimation results by our method and several state-of-": "RhythmNet [16]",
          "Table 3. HR estimation results by our method and several state-of-": ""
        },
        {
          "Table 1. HR estimation results by our method and several state-of-": "",
          "Table 3. HR estimation results by our method and several state-of-": "Table 4. Cross-database HR estimation (training on PURE and"
        },
        {
          "Table 1. HR estimation results by our method and several state-of-": "NAS-HR [12]",
          "Table 3. HR estimation results by our method and several state-of-": ""
        },
        {
          "Table 1. HR estimation results by our method and several state-of-": "",
          "Table 3. HR estimation results by our method and several state-of-": "testing on UBFC-rPPG) by our DRNet and baseline methods."
        },
        {
          "Table 1. HR estimation results by our method and several state-of-": "CVD [17]",
          "Table 3. HR estimation results by our method and several state-of-": ""
        },
        {
          "Table 1. HR estimation results by our method and several state-of-": "",
          "Table 3. HR estimation results by our method and several state-of-": ""
        },
        {
          "Table 1. HR estimation results by our method and several state-of-": "PhysFormer [31]",
          "Table 3. HR estimation results by our method and several state-of-": ""
        },
        {
          "Table 1. HR estimation results by our method and several state-of-": "",
          "Table 3. HR estimation results by our method and several state-of-": "Opt. Express 8.29"
        },
        {
          "Table 1. HR estimation results by our method and several state-of-": "Dual-GAN [13]",
          "Table 3. HR estimation results by our method and several state-of-": ""
        },
        {
          "Table 1. HR estimation results by our method and several state-of-": "",
          "Table 3. HR estimation results by our method and several state-of-": "Opt. Express 4.39"
        },
        {
          "Table 1. HR estimation results by our method and several state-of-": "DRNet(Ours)",
          "Table 3. HR estimation results by our method and several state-of-": ""
        },
        {
          "Table 1. HR estimation results by our method and several state-of-": "",
          "Table 3. HR estimation results by our method and several state-of-": "3.52"
        },
        {
          "Table 1. HR estimation results by our method and several state-of-": "",
          "Table 3. HR estimation results by our method and several state-of-": "3.10"
        },
        {
          "Table 1. HR estimation results by our method and several state-of-": "Table 2. HR estimation results by our method and several state-of-",
          "Table 3. HR estimation results by our method and several state-of-": ""
        },
        {
          "Table 1. HR estimation results by our method and several state-of-": "",
          "Table 3. HR estimation results by our method and several state-of-": "2.09"
        },
        {
          "Table 1. HR estimation results by our method and several state-of-": "the-art methods on the PURE database.",
          "Table 3. HR estimation results by our method and several state-of-": ""
        },
        {
          "Table 1. HR estimation results by our method and several state-of-": "",
          "Table 3. HR estimation results by our method and several state-of-": "1.29"
        },
        {
          "Table 1. HR estimation results by our method and several state-of-": "Method",
          "Table 3. HR estimation results by our method and several state-of-": ""
        },
        {
          "Table 1. HR estimation results by our method and several state-of-": "",
          "Table 3. HR estimation results by our method and several state-of-": "0.74"
        },
        {
          "Table 1. HR estimation results by our method and several state-of-": "2SR [5]",
          "Table 3. HR estimation results by our method and several state-of-": ""
        },
        {
          "Table 1. HR estimation results by our method and several state-of-": "",
          "Table 3. HR estimation results by our method and several state-of-": "0.65"
        },
        {
          "Table 1. HR estimation results by our method and several state-of-": "CHROM [4]",
          "Table 3. HR estimation results by our method and several state-of-": ""
        },
        {
          "Table 1. HR estimation results by our method and several state-of-": "HR-CNN [22]",
          "Table 3. HR estimation results by our method and several state-of-": ""
        },
        {
          "Table 1. HR estimation results by our method and several state-of-": "Dual-GAN [13]",
          "Table 3. HR estimation results by our method and several state-of-": ""
        },
        {
          "Table 1. HR estimation results by our method and several state-of-": "DRNet(Ours)",
          "Table 3. HR estimation results by our method and several state-of-": "UBFC-rPPG dataset, we follow the same testing protocol"
        },
        {
          "Table 1. HR estimation results by our method and several state-of-": "",
          "Table 3. HR estimation results by our method and several state-of-": "in [13], in which the videos of the first 30 subjects are used"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 5: The ablation study of DRNet for HR estimation on the",
      "data": [
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": ""
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "VIPL-HR database."
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "Method\nPC\nSAB\nStd↓\nMAE↓\nRMSE↓\nMER↓\nr↑"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "×\n×\nBaseline\n8.70\n5.66\n8.77\n6.93%\n0.74"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "√"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "×\n7.53\n4.71\n7.56\n5.81%\n0.81"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "√\n√"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "7.18\n4.47\n7.29\n5.45%\n0.83"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "√\n√"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "6.75\n4.18\n6.78\n5.14%\n0.85\nDRNet"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": ""
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": ""
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "DRNet focuses on: how to obtain a physiological estimator"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "that can extract noise-free features,\ninstead of wasting on"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": ""
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "direct modeling noise or noisy data."
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": ""
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": ""
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "5. Conclusion"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": ""
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": ""
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "In this paper, we propose an effective end-to-end net-"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": ""
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "work for\nremote physiological\nsensing using a decompo-"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": ""
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "sition and reconstruction strategy to reduce the influences"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": ""
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "of non-physiological signals and enhance feature disentan-"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": ""
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "glement. Moreover, we design a lightweight and efficient"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": ""
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "Spatial Attention Block. Besides, a novel Patch Cropping"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": ""
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "augmentation strategy is proposed for enriching the training"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "data. Extensive experiments are performed to verify the ef-"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "fectiveness of the proposed methods.\nIn the future, we will"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": ""
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "explore the self-supervised learning technologies for remote"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": ""
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "physiological measurement."
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": ""
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": ""
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "Acknowledgments"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": ""
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "This\nwork\nwas\nsupported\nin\npart\nby\nthe\nNSFC-"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": ""
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "Xinjiang\nJoint\nFund\nunder\nGrant\nU1903127\nand"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": ""
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "in\npart\nby\nthe\nNatural\nScience\nFoundation\nof"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "Shandong\nProvince\nunder\nGrant\nZR2020MF052."
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": ""
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": ""
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "References"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": ""
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "[1]\nSerge Bobbia, Richard Macwan, Yannick Benezeth, Alamin"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "Mansouri,\nand Julien Dubois.\nUnsupervised skin tissue"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "Pattern\nsegmentation for\nremote photoplethysmography."
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "Recognition Letters, 124:82–90, 2019. 6"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "[2]\nJoao Carreira and Andrew Zisserman.\nQuo vadis,\naction"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "recognition?\na new model\nand the kinetics dataset.\nIn"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "proceedings of\nthe IEEE Conference on Computer Vision"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": ""
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "and Pattern Recognition, pages 6299–6308, 2017. 7"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": ""
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "[3] Weixuan Chen and Daniel McDuff.\nDeepphys:\nVideo-"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": ""
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "based physiological measurement using convolutional atten-"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": ""
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "tion networks.\nIn Proceedings of the European Conference"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": ""
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "on Computer Vision (ECCV), pages 349–365, 2018. 2, 7"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": ""
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "[4] Gerard De Haan and Vincent Jeanne. Robust pulse rate from"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": ""
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "chrominance-based rppg.\nIEEE Transactions on Biomedical"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": ""
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "Engineering, 60(10):2878–2886, 2013. 1, 2, 7"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": ""
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "[5] Gerard De Haan and Arno Van Leest.\nImproved motion ro-"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": ""
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "bustness of remote-ppg by using the blood volume pulse sig-"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "nature. Physiological measurement, 35(9):1913, 2014. 7"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "[6]\nJiankang Deng,\nJia Guo, Evangelos Ververas,\nIrene Kot-"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "sia, and Stefanos Zafeiriou. Retinaface: Single-shot multi-"
        },
        {
          "Table 5. The ablation study of DRNet\nfor HR estimation on the": "level\nface localisation in the wild.\nIn Proceedings of\nthe"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "Recognition, pages 5203–5212, 2020. 3",
          "David A Clifton. Toward a robust estimation of respiratory": "rate from pulse oximeters. IEEE Transactions on Biomedical"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "[7] Andrew G Howard, Menglong Zhu,\nBo Chen, Dmitry",
          "David A Clifton. Toward a robust estimation of respiratory": "Engineering, 64(8):1914–1923, 2016. 6"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "Kalenichenko, Weijun Wang, Tobias Weyand, Marco An-",
          "David A Clifton. Toward a robust estimation of respiratory": "[20] Ming-Zher Poh, Daniel J McDuff, and Rosalind W Picard."
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "dreetto, and Hartwig Adam. Mobilenets: Efficient convolu-",
          "David A Clifton. Toward a robust estimation of respiratory": "Non-contact, automated cardiac pulse measurements using"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "tional neural networks for mobile vision applications. arXiv",
          "David A Clifton. Toward a robust estimation of respiratory": "video imaging and blind source separation. Optics express,"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "preprint arXiv:1704.04861, 2017. 3",
          "David A Clifton. Toward a robust estimation of respiratory": "18(10):10762–10774, 2010. 1, 2, 7"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "[8]\nJie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-",
          "David A Clifton. Toward a robust estimation of respiratory": "[21] Rencheng Song, Huan Chen,\nJuan Cheng, Chang Li, Yu"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "works.\nIn Proceedings of the IEEE conference on computer",
          "David A Clifton. Toward a robust estimation of respiratory": "Liu, and Xun Chen. Pulsegan: Learning to generate realis-"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "vision and pattern recognition, pages 7132–7141, 2018. 5",
          "David A Clifton. Toward a robust estimation of respiratory": "tic pulse waveforms in remote photoplethysmography. IEEE"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "[9] Mohamad Kachuee, Mohammad Mahdi Kiani, Hoda Mo-",
          "David A Clifton. Toward a robust estimation of respiratory": "Journal of Biomedical and Health Informatics, 25(5):1373–"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "hammadzade, and Mahdi Shabany. Cuff-less high-accuracy",
          "David A Clifton. Toward a robust estimation of respiratory": "1384, 2021. 7"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "calibration-free blood pressure estimation using pulse transit",
          "David A Clifton. Toward a robust estimation of respiratory": "[22] Radim ˇSpetl´ık, Vojtech\nFranc,\nand\nJir´ı Matas.\nVisual"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "time.\nIn 2015 IEEE international symposium on circuits and",
          "David A Clifton. Toward a robust estimation of respiratory": "heart\nrate\nestimation with\nconvolutional\nneural\nnetwork."
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "systems (ISCAS), pages 1006–1009. IEEE, 2015. 6",
          "David A Clifton. Toward a robust estimation of respiratory": "In Proceedings of\nthe british machine vision conference,"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "[10] Diederik P Kingma and Jimmy Ba. Adam: A method for",
          "David A Clifton. Toward a robust estimation of respiratory": "Newcastle, UK, pages 3–6, 2018. 2, 3, 7"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "stochastic optimization.\narXiv preprint\narXiv:1412.6980,",
          "David A Clifton. Toward a robust estimation of respiratory": "[23] Ronny\nStricker,\nSteffen\nM¨uller,\nand\nHorst-Michael"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "2014. 6",
          "David A Clifton. Toward a robust estimation of respiratory": "Gross.\nNon-contact\nvideo-based\npulse\nrate measure-"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "[11] Magdalena Lewandowska, Jacek Rumi´nski, Tomasz Koce-",
          "David A Clifton. Toward a robust estimation of respiratory": "ment\non\na mobile\nservice\nrobot.\nIn The\n23rd\nIEEE"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "jko,\nand Jkedrzej Nowak.\nMeasuring pulse\nrate with a",
          "David A Clifton. Toward a robust estimation of respiratory": "International Symposium on Robot and Human Interactive"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "webcam—a non-contact method for evaluating cardiac ac-",
          "David A Clifton. Toward a robust estimation of respiratory": "Communication, pages 1056–1062. IEEE, 2014. 6"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "tivity.\nIn 2011 federated conference on computer\nscience",
          "David A Clifton. Toward a robust estimation of respiratory": "[24] Yun-Yun Tsou, Yi-An Lee, Chiou-Ting Hsu,\nand Shang-"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "and information systems (FedCSIS), pages 405–410. IEEE,",
          "David A Clifton. Toward a robust estimation of respiratory": "Hung Chang.\nSiamese-rppg\nnetwork:\nRemote\nphoto-"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "2011. 1, 2",
          "David A Clifton. Toward a robust estimation of respiratory": "plethysmography signal\nestimation from face videos.\nIn"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "",
          "David A Clifton. Toward a robust estimation of respiratory": "Proceedings of the 35th annual ACM symposium on applied"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "[12] Hao Lu and Hu Han. Nas-hr: Neural architecture search for",
          "David A Clifton. Toward a robust estimation of respiratory": ""
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "",
          "David A Clifton. Toward a robust estimation of respiratory": "computing, pages 2066–2073, 2020. 3, 6, 7"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "Virtual Reality &\nheart\nrate estimation from face videos.",
          "David A Clifton. Toward a robust estimation of respiratory": ""
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "Intelligent Hardware, 3(1):33–42, 2021. 7",
          "David A Clifton. Toward a robust estimation of respiratory": "[25]\nSergey Tulyakov, Xavier Alameda-Pineda, Elisa Ricci, Lijun"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "",
          "David A Clifton. Toward a robust estimation of respiratory": "Yin, Jeffrey F Cohn, and Nicu Sebe.\nSelf-adaptive matrix"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "[13] Hao Lu, Hu Han, and S Kevin Zhou. Dual-gan:\nJoint bvp",
          "David A Clifton. Toward a robust estimation of respiratory": ""
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "",
          "David A Clifton. Toward a robust estimation of respiratory": "completion for heart rate estimation from face videos under"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "and noise modeling for remote physiological measurement.",
          "David A Clifton. Toward a robust estimation of respiratory": ""
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "",
          "David A Clifton. Toward a robust estimation of respiratory": "realistic conditions.\nIn Proceedings of the IEEE conference"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "In Proceedings of\nthe IEEE/CVF Conference on Computer",
          "David A Clifton. Toward a robust estimation of respiratory": ""
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "",
          "David A Clifton. Toward a robust estimation of respiratory": "on computer vision and pattern recognition, pages 2396–"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "Vision and Pattern Recognition, pages 12404–12413, 2021.",
          "David A Clifton. Toward a robust estimation of respiratory": ""
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "",
          "David A Clifton. Toward a robust estimation of respiratory": "2404, 2016. 7"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "2, 3, 5, 6, 7",
          "David A Clifton. Toward a robust estimation of respiratory": ""
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "",
          "David A Clifton. Toward a robust estimation of respiratory": "[26] Wim Verkruysse, Lars O Svaasand, and J Stuart Nelson. Re-"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "[14] Xuesong Niu, Hu Han, Shiguang Shan, and Xilin Chen. Syn-",
          "David A Clifton. Toward a robust estimation of respiratory": ""
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "",
          "David A Clifton. Toward a robust estimation of respiratory": "mote plethysmographic imaging using ambient light. Optics"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "rhythm: Learning a deep heart rate estimator from general to",
          "David A Clifton. Toward a robust estimation of respiratory": ""
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "",
          "David A Clifton. Toward a robust estimation of respiratory": "express, 16(26):21434–21445, 2008. 1, 2, 7"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "specific.\nIn 2018 24th International Conference on Pattern",
          "David A Clifton. Toward a robust estimation of respiratory": ""
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "Recognition (ICPR), pages 3580–3585. IEEE, 2018. 7",
          "David A Clifton. Toward a robust estimation of respiratory": "[27] Wenjin Wang,\nAlbertus\nC\nDen\nBrinker,\nand\nGerard"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "",
          "David A Clifton. Toward a robust estimation of respiratory": "De Haan.\nSingle-element\nremote-ppg.\nIEEE Transactions"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "[15] Xuesong Niu, Hu Han, Shiguang Shan,\nand Xilin Chen.",
          "David A Clifton. Toward a robust estimation of respiratory": ""
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "",
          "David A Clifton. Toward a robust estimation of respiratory": "on Biomedical Engineering, 66(7):2032–2043, 2018. 6"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "Vipl-hr: A multi-modal database for pulse estimation from",
          "David A Clifton. Toward a robust estimation of respiratory": ""
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "on\nless-constrained\nface\nvideo.\nIn Asian Conference",
          "David A Clifton. Toward a robust estimation of respiratory": "[28] Wenjin Wang, Albertus C den Brinker, Sander Stuijk, and"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "Computer Vision, pages 562–576. Springer, 2018. 6",
          "David A Clifton. Toward a robust estimation of respiratory": "Gerard De Haan.\nAlgorithmic principles of\nremote ppg."
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "",
          "David A Clifton. Toward a robust estimation of respiratory": "IEEE Transactions on Biomedical Engineering, 64(7):1479–"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "[16] Xuesong Niu, Shiguang Shan, Hu Han,\nand Xilin Chen.",
          "David A Clifton. Toward a robust estimation of respiratory": ""
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "",
          "David A Clifton. Toward a robust estimation of respiratory": "1491, 2016. 1, 2, 7"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "Rhythmnet:\nEnd-to-end\nheart\nrate\nestimation\nfrom face",
          "David A Clifton. Toward a robust estimation of respiratory": ""
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "via spatial-temporal\nrepresentation.\nIEEE Transactions on",
          "David A Clifton. Toward a robust estimation of respiratory": "[29] Zitong Yu, Xiaobai Li, Xuesong Niu, Jingang Shi, and Guoy-"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "Image Processing, 29:2409–2423, 2019. 2, 3, 6, 7",
          "David A Clifton. Toward a robust estimation of respiratory": "ing Zhao. Autohr: A strong end-to-end baseline for remote"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "",
          "David A Clifton. Toward a robust estimation of respiratory": "IEEE Signal\nheart rate measurement with neural searching."
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "[17] Xuesong Niu, Zitong Yu, Hu Han, Xiaobai Li, Shiguang",
          "David A Clifton. Toward a robust estimation of respiratory": ""
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "",
          "David A Clifton. Toward a robust estimation of respiratory": "Processing Letters, 27:1245–1249, 2020. 7"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "Shan, and Guoying Zhao. Video-based remote physiologi-",
          "David A Clifton. Toward a robust estimation of respiratory": ""
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "",
          "David A Clifton. Toward a robust estimation of respiratory": "[30] Zitong\nYu,\nXiaobai\nLi,\nand\nGuoying\nZhao.\nRe-"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "cal measurement via cross-verified feature disentangling.\nIn",
          "David A Clifton. Toward a robust estimation of respiratory": ""
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "",
          "David A Clifton. Toward a robust estimation of respiratory": "mote\nphotoplethysmograph\nsignal measurement\nfrom fa-"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "European Conference on Computer Vision, pages 295–310.",
          "David A Clifton. Toward a robust estimation of respiratory": ""
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "",
          "David A Clifton. Toward a robust estimation of respiratory": "cial videos using spatio-temporal networks.\narXiv preprint"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "Springer, 2020. 2, 3, 4, 6, 7",
          "David A Clifton. Toward a robust estimation of respiratory": ""
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "",
          "David A Clifton. Toward a robust estimation of respiratory": "arXiv:1905.02419, 2019. 7"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "[18] Xuesong Niu, Xingyuan Zhao, Hu Han, Abhijit Das, Antitza",
          "David A Clifton. Toward a robust estimation of respiratory": ""
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "",
          "David A Clifton. Toward a robust estimation of respiratory": "[31] Zitong Yu, Yuming Shen,\nJingang Shi, Hengshuang Zhao,"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "Dantcheva, Shiguang Shan, and Xilin Chen. Robust remote",
          "David A Clifton. Toward a robust estimation of respiratory": ""
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "",
          "David A Clifton. Toward a robust estimation of respiratory": "Philip Torr, and Guoying Zhao.\nPhysformer: Facial video-"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "heart rate estimation from face utilizing spatial-temporal at-",
          "David A Clifton. Toward a robust estimation of respiratory": ""
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "",
          "David A Clifton. Toward a robust estimation of respiratory": "based physiological measurement with temporal difference"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "tention.\nIn 2019 14th IEEE International Conference on",
          "David A Clifton. Toward a robust estimation of respiratory": ""
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "",
          "David A Clifton. Toward a robust estimation of respiratory": "transformer. arXiv preprint arXiv:2111.12082, 2021. 2, 7"
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "Automatic Face & Gesture Recognition (FG 2019), pages",
          "David A Clifton. Toward a robust estimation of respiratory": ""
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "1–8. IEEE, 2019. 4, 7",
          "David A Clifton. Toward a robust estimation of respiratory": ""
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "[19] Marco AF Pimentel, Alistair EW Johnson, Peter H Charlton,",
          "David A Clifton. Toward a robust estimation of respiratory": ""
        },
        {
          "IEEE/CVF Conference\non Computer Vision\nand Pattern": "Drew Birrenkott, Peter J Watkinson, Lionel Tarassenko, and",
          "David A Clifton. Toward a robust estimation of respiratory": ""
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Unsupervised skin tissue segmentation for remote photoplethysmography",
      "authors": [
        "Serge Bobbia",
        "Richard Macwan",
        "Yannick Benezeth",
        "Alamin Mansouri",
        "Julien Dubois"
      ],
      "year": "2019",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "2",
      "title": "Quo vadis, action recognition? a new model and the kinetics dataset",
      "authors": [
        "Joao Carreira",
        "Andrew Zisserman"
      ],
      "year": "2017",
      "venue": "proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "3",
      "title": "Deepphys: Videobased physiological measurement using convolutional attention networks",
      "authors": [
        "Weixuan Chen",
        "Daniel Mcduff"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "4",
      "title": "Robust pulse rate from chrominance-based rppg",
      "authors": [
        "Gerard De",
        "Vincent Jeanne"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "5",
      "title": "Improved motion robustness of remote-ppg by using the blood volume pulse signature",
      "authors": [
        "Gerard De",
        "Arno Van Leest"
      ],
      "year": "1913",
      "venue": "Physiological measurement"
    },
    {
      "citation_id": "6",
      "title": "Retinaface: Single-shot multilevel face localisation in the wild",
      "authors": [
        "Jiankang Deng",
        "Jia Guo",
        "Evangelos Ververas",
        "Irene Kotsia",
        "Stefanos Zafeiriou"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "7",
      "title": "Mobilenets: Efficient convolutional neural networks for mobile vision applications",
      "authors": [
        "Menglong Andrew G Howard",
        "Bo Zhu",
        "Dmitry Chen",
        "Weijun Kalenichenko",
        "Tobias Wang",
        "Marco Weyand",
        "Hartwig Andreetto",
        "Adam"
      ],
      "year": "2017",
      "venue": "Mobilenets: Efficient convolutional neural networks for mobile vision applications",
      "arxiv": "arXiv:1704.04861"
    },
    {
      "citation_id": "8",
      "title": "Squeeze-and-excitation networks",
      "authors": [
        "Jie Hu",
        "Li Shen",
        "Gang Sun"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "9",
      "title": "Cuff-less high-accuracy calibration-free blood pressure estimation using pulse transit time",
      "authors": [
        "Mohamad Kachuee",
        "Mohammad Mahdi Kiani",
        "Hoda Mohammadzade",
        "Mahdi Shabany"
      ],
      "year": "2015",
      "venue": "2015 IEEE international symposium on circuits and systems (ISCAS)"
    },
    {
      "citation_id": "10",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "11",
      "title": "Measuring pulse rate with a webcam-a non-contact method for evaluating cardiac activity",
      "authors": [
        "Magdalena Lewandowska",
        "Jacek Rumiński",
        "Tomasz Kocejko",
        "Jkedrzej Nowak"
      ],
      "year": "2011",
      "venue": "2011 federated conference on computer science and information systems (FedCSIS)"
    },
    {
      "citation_id": "12",
      "title": "Nas-hr: Neural architecture search for heart rate estimation from face videos",
      "authors": [
        "Hao Lu",
        "Hu Han"
      ],
      "year": "2021",
      "venue": "Virtual Reality & Intelligent Hardware"
    },
    {
      "citation_id": "13",
      "title": "Dual-gan: Joint bvp and noise modeling for remote physiological measurement",
      "authors": [
        "Hao Lu",
        "Hu Han",
        "Kevin Zhou"
      ],
      "year": "2007",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "14",
      "title": "Synrhythm: Learning a deep heart rate estimator from general to specific",
      "authors": [
        "Xuesong Niu",
        "Hu Han",
        "Shiguang Shan",
        "Xilin Chen"
      ],
      "year": "2018",
      "venue": "2018 24th International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "15",
      "title": "Vipl-hr: A multi-modal database for pulse estimation from less-constrained face video",
      "authors": [
        "Xuesong Niu",
        "Hu Han",
        "Shiguang Shan",
        "Xilin Chen"
      ],
      "year": "2018",
      "venue": "Asian Conference on Computer Vision"
    },
    {
      "citation_id": "16",
      "title": "Rhythmnet: End-to-end heart rate estimation from face via spatial-temporal representation",
      "authors": [
        "Xuesong Niu",
        "Shiguang Shan",
        "Hu Han",
        "Xilin Chen"
      ],
      "year": "2007",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "17",
      "title": "Video-based remote physiological measurement via cross-verified feature disentangling",
      "authors": [
        "Xuesong Niu",
        "Zitong Yu",
        "Hu Han",
        "Xiaobai Li",
        "Shiguang Shan",
        "Guoying Zhao"
      ],
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "18",
      "title": "",
      "authors": [
        "Springer"
      ],
      "year": "2007",
      "venue": ""
    },
    {
      "citation_id": "19",
      "title": "Robust remote heart rate estimation from face utilizing spatial-temporal attention",
      "authors": [
        "Xuesong Niu",
        "Xingyuan Zhao",
        "Hu Han",
        "Abhijit Das",
        "Antitza Dantcheva",
        "Shiguang Shan",
        "Xilin Chen"
      ],
      "year": "2019",
      "venue": "Gesture Recognition (FG 2019)"
    },
    {
      "citation_id": "20",
      "title": "Toward a robust estimation of respiratory rate from pulse oximeters",
      "authors": [
        "A Marco",
        "Alistair Ew Pimentel",
        "Johnson",
        "Drew Peter H Charlton",
        "Birrenkott",
        "Lionel Peter J Watkinson",
        "David Tarassenko",
        "Clifton"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "21",
      "title": "Non-contact, automated cardiac pulse measurements using video imaging and blind source separation",
      "authors": [
        "Ming-Zher Poh",
        "Daniel Mcduff",
        "Rosalind Picard"
      ],
      "year": "2007",
      "venue": "Optics express"
    },
    {
      "citation_id": "22",
      "title": "Pulsegan: Learning to generate realistic pulse waveforms in remote photoplethysmography",
      "authors": [
        "Rencheng Song",
        "Huan Chen",
        "Juan Cheng",
        "Chang Li",
        "Yu Liu",
        "Xun Chen"
      ],
      "year": "2021",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "23",
      "title": "Visual heart rate estimation with convolutional neural network",
      "authors": [
        "Radim Špetlík",
        "Vojtech Franc",
        "Jirí Matas"
      ],
      "year": "2018",
      "venue": "Proceedings of the british machine vision conference"
    },
    {
      "citation_id": "24",
      "title": "Non-contact video-based pulse rate measurement on a mobile service robot",
      "authors": [
        "Ronny Stricker",
        "Steffen Müller",
        "Horst-Michael Gross"
      ],
      "year": "2014",
      "venue": "The 23rd IEEE International Symposium on Robot and Human Interactive Communication"
    },
    {
      "citation_id": "25",
      "title": "Siamese-rppg network: Remote photoplethysmography signal estimation from face videos",
      "authors": [
        "Yun-Yun Tsou",
        "Yi-An Lee",
        "Chiou-Ting Hsu",
        "Shang-Hung Chang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 35th annual ACM symposium on applied computing"
    },
    {
      "citation_id": "26",
      "title": "Self-adaptive matrix completion for heart rate estimation from face videos under realistic conditions",
      "authors": [
        "Sergey Tulyakov",
        "Xavier Alameda-Pineda",
        "Elisa Ricci",
        "Lijun Yin",
        "Jeffrey Cohn",
        "Nicu Sebe"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "27",
      "title": "Remote plethysmographic imaging using ambient light",
      "authors": [
        "Wim Verkruysse",
        "Lars Svaasand",
        "J Stuart"
      ],
      "year": "2007",
      "venue": "Optics express"
    },
    {
      "citation_id": "28",
      "title": "Single-element remote-ppg",
      "authors": [
        "Wenjin Wang",
        "Albertus C Den",
        "Gerard Brinker",
        "De Haan"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "29",
      "title": "Algorithmic principles of remote ppg",
      "authors": [
        "Wenjin Wang",
        "Albertus C Den",
        "Sander Brinker",
        "Gerard Stuijk",
        "De Haan"
      ],
      "year": "2007",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "30",
      "title": "Autohr: A strong end-to-end baseline for remote heart rate measurement with neural searching",
      "authors": [
        "Zitong Yu",
        "Xiaobai Li",
        "Xuesong Niu",
        "Jingang Shi",
        "Guoying Zhao"
      ],
      "year": "2020",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "31",
      "title": "Remote photoplethysmograph signal measurement from facial videos using spatio-temporal networks",
      "authors": [
        "Zitong Yu",
        "Xiaobai Li",
        "Guoying Zhao"
      ],
      "year": "2019",
      "venue": "Remote photoplethysmograph signal measurement from facial videos using spatio-temporal networks",
      "arxiv": "arXiv:1905.02419"
    },
    {
      "citation_id": "32",
      "title": "Physformer: Facial videobased physiological measurement with temporal difference transformer",
      "authors": [
        "Zitong Yu",
        "Yuming Shen",
        "Jingang Shi",
        "Hengshuang Zhao",
        "Philip Torr",
        "Guoying Zhao"
      ],
      "year": "2021",
      "venue": "Physformer: Facial videobased physiological measurement with temporal difference transformer",
      "arxiv": "arXiv:2111.12082"
    }
  ]
}