{
  "paper_id": "2107.08361v1",
  "title": "An Improved Stargan For Emotional Voice Conversion: Enhancing Voice Quality And Data Augmentation",
  "published": "2021-07-18T04:28:47Z",
  "authors": [
    "Xiangheng He",
    "Junjie Chen",
    "Georgios Rizos",
    "Björn W. Schuller"
  ],
  "keywords": [
    "emotional voice conversion",
    "generative adversarial network",
    "data augmentation"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotional Voice Conversion (EVC) aims to convert the emotional style of a source speech signal to a target style while preserving its content and speaker identity information. Previous emotional conversion studies do not disentangle emotional information from emotion-independent information that should be preserved, thus transforming it all in a monolithic manner and generating audio of low quality, with linguistic distortions. To address this distortion problem, we propose a novel StarGAN framework along with a two-stage training process that separates emotional features from those independent of emotion by using an autoencoder with two encoders as the generator of the Generative Adversarial Network (GAN). The proposed model achieves favourable results in both the objective evaluation and the subjective evaluation in terms of distortion, which reveals that the proposed model can effectively reduce distortion. Furthermore, in data augmentation experiments for end-to-end speech emotion recognition, the proposed StarGAN model achieves an increase of 2 % in Micro-F1 and 5 % in Macro-F1 compared to the baseline StarGAN model, which indicates that the proposed model is more valuable for data augmentation.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotional Voice Conversion (EVC) aims to alter the emotional style of a source speech signal to a target style while preserving other emotion-independent information. A major problem of this task is that high-quality parallel data is hard to collect due to substantial labor costs  [1] . Recent work proposed to utilise generative adversarial network (GAN) frameworks  [2, 3, 4]  to convert the spectrum features of a source audio signal to the features of the target  [5, 6, 7, 8, 9] , thus eliminating the need for parallel data. In addition, evidence shows that the GAN-generated audio signals, as a source of augmenting data, are valuable for improving the predictive performance of speech emotion recognition (SER) models  [6, 10] , which not only indicates that the GAN-generated audio signals indeed carry effective emotional information, but also provides a promising data augmentation solution for the SER task  [11, 12, 13, 14] .\n\nHowever, comparing the source audio signal waveform with the waveform of the audio signal converted using the existing StarGAN-based EVC model  [6] , we observe that some silent frames in the original waveform were given very high amplitude values in the converted waveform. An example is shown in Figure  1 . Since these silent frames represent silence between discrete words or sentences, excessive amplitude values should not be given to these frames in any conversion between emotions. This indicates that the existing StarGAN framework is prone to generating artefacts; we believe that this is because it transforms the input clip in a monolithic manner, instead of focusing on the emotional indices, as desired. This monolithic conversion manner tends to cause the distortion of the linguistic characteristics (e. g., cracked voices, or jitter) and damages the overall audio quality. Experimental results of existing EVC models also show that the mean opinion scores (MOS) for audio quality before and after conversion suffer from a significant drop  [8] .\n\nWe believe that a means to address this problem is to separate emotional features from emotion-independent features. Autoencoder (AE)-or variational autoencoder (VAE)-based feature disentanglement methods provide a possible solution and have been successfully applied to the voice conversion (VC) task  [15, 16, 17, 18, 19, 20] . In  [17] , by providing the speaker identity features as a condition to the decoder, the encoder learns to encode only the speaker-independent information in the process of minimising the reconstruction loss. Experimental results from  [20]  show that the degree of disentanglement is positively correlated with the performance of the VC model and can be enhanced by both GANs and the speaker classifier. Inspired by these feature disentanglement methods, we propose to utilise an autoencoder with two encoders as the generator of the StarGAN as well as a two stage training process for the model.\n\nUnlike the existing StarGAN-based EVC model  [6] , we explicitly separate emotion-independent features from emotional features during conversion. When providing the target emotion label to StarGAN, we use continuous emotional features arXiv:2107.08361v1 [cs.SD] 18 Jul 2021 extracted by a pre-trained emotion classifier rather than using one-hot vectors to represent each emotion. Similar continuous emotional features have also been used in  [8] . However, their model is based on VAW-GAN in the context of one-to-many emotional conversion, while our model is based on StarGAN and able to complete the many-to-many emotional conversion. Besides, their model simply reconstructs the source spectral envelope without attempting to convert it during the whole training process. The authors only consider the emotion conversion when testing, which causes a mismatch between training and testing. Our model eliminates this mismatch by using a proposed two-stage training process.\n\nThe main contributions of this paper are: 1) We propose a novel model structure as well as a two-stage training process based on StarGAN, which explicitly separate emotionindependent features from emotional features during conversion 1  ; 2) we perform both the objective evaluation using Melcepstral distortion (MCD)  [21]  and the subjective evaluation in terms of distortion to ascertain whether this separation of features can indeed reduce the distortion of generated audio signals and improve their overall quality; 3) we replicate the data augmentation experiments of an existing StarGAN-based EVC model  [6]  to showcase that our audio quality improvements are not at the expense of emotional conversion performance.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Improved Stargan",
      "text": "Since the harmonic spectral envelope (SP) and the fundamental frequency (F0) contour contain emotion-related cues  [22, 23] , we use the WORLD vocoder  [24]  to decompose the raw audio signal into SP, F0, and aperiodicity parameters (AP) before converting. We then apply the proposed StarGAN framework to generate the SP with target emotion. We next introduce the two training stages, the model architecture and the final conversion process.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Training Stage1: Autoencoder Training",
      "text": "In the first training stage, we propose to utilise an autoencoder with two encoders as the GAN generator G, which aims to learn an emotion-independent encoder capable of separating emotional features from emotion-independent features.\n\nAs shown in the left part of Figure  2 , the autoencoder is trained by simply reconstructing the source spectral envelope SPC1 with emotion C1 without attempting to convert it, which means that the target emotion is not provided in this stage. The emotion encoder aims to encode emotional information, where its latent code E(SPC1) is provided by a pre-trained emotion classifier (fixed during training stage 1) shown in Figure  3 . By providing the emotion features of the source SPC1 as a condition, the emotion-independent encoder learns to eliminate emotional information from the input, thus making its latent code I(SPC1) emotion independent. The decoder learns to reconstruct SPC1 using the concatenated latent codes, which can be formulated as:\n\nTo make the reconstructed SP C1 more realistic, we train the autoencoder based on adversarial learning.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Training Stage2: Stargan Training",
      "text": "In the second training stage, the typical StarGAN training is implemented to eliminate the training-testing mismatch in  [8]  and enable the many-to-many emotion conversion.\n\nAs shown in the right part of Figure  2 , the emotionindependent encoder and the decoder are the generator of the StarGAN, denoted as G. We extract the emotion features of audio signals with the same emotion by using the pre-trained emotion classifier and average them as the feature representations of target emotion labels. We denote EC2 as the representation of target emotion C2. The generator G first reconstructs the SP C2 from EC2 and I(SPC1) and then reconstructs the SP C1 from EC1 and I(SP C2 ):\n\nThe generator G tries to minimise the reconstruction loss and fool the discriminator D by generating a more realistic SP. The D tries to maximise the loss between the real SP and the fake SP. The domain classifier C learns to predict the emotion label and helps to enable the many-to-many emotion conversion.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Conversion Process At Test Time",
      "text": "We follow the relative logarithm Gaussian normalised transformation (LGNT)  [25]  proposed by  [6]  to convert the F0 contour of an audio signal from emotion C1 to the target emotion C2.\n\nwhere µ(F 0) and σ(F 0) are the mean and variance of log(F 0). ∆µC 1 , C 2 and ∆σC 1 , C 2 are the average difference in the mean log(F 0) and the average change in the variance of log(F 0) between emotion C1 and C2.\n\nAt test time, with the spectral envelope converted by our improved StarGAN model and the F0 contour converted by the relative LGNT, we synthesise the audio signal with the target emotion via the WORLD vocoder.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset And Training Details",
      "text": "We use the Interactive Emotional Dyadic Motion Capture (IEMOCAP) Database  [26]  for all our experiments. This dataset has been recorded from ten actors in sessions 1-5, including emotional scripts and improvised hypothetical scenarios. It contains approximately 12 hours of recordings with 9 emotions. The sampling rate of all recordings is 16 kHz. We utilise three emotions including angry, sad, and happy from both scripted and improvised patterns which contains 2 435 recordings in totally.\n\nFor more details of our improved StarGAN framework, we use the same discriminator and the domain classifier structures as the existing StarGAN-based model  [6, 27] . The architecture of our emotion-independent encoder and decoder is the same as the generator in  [6, 27] . The pre-trained classifier used in the first training stage is shown in the left part of Figure  3 . It has the same structure as the domain classifier in the second stage. We extract 36 cepstral coefficients as an approximation to the whole spectral envelope for each training sample. In both the first and the second training stages, we set the batch size to 4 and use the Adam optimiser  [28]  with the learning rate (for G, D and C), β1 and β2 set to 1 × 10 -4 , 0.5, and 0.999. We perform 1 generator update after 5 discriminator and domain classifier updates in the StarGAN training stage as in  [29] . The first training stage lasts for 400 epochs, and we manually stop the second training stage when the loss reaches a plateau (approximately 380 epochs).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Audio Distortion Evaluation",
      "text": "To ascertain whether the separation of emotional features from emotion-independent features can indeed reduce the distortion of the generated audio signals and improve their overall quality, we perform an objective evaluation using the MCD and a subjective evaluation via the human preference test and the mean opinion scores (MOS) test in terms of distortion.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Subjective Evaluation Experimental Design",
      "text": "We conduct two listening tests for our human perception experiments, the human preference test and the MOS test in audio distortion. 26 fluent English-speaking subjects participated in all experiments. Each of them was asked to evaluate 60 audio samples (30 pairs), which contains 30 audio samples generated by the three-class StarGAN-based EVC model  [6]  (denoted as baseline-StarGAN-EVC) and 30 audio samples generated by our proposed StarGAN-based EVC model (denoted as StarGANbased EVC). Both models were trained on the IEMOCAP dataset with the same train/validation/test split. Two audio samples of each pair were generated from one source sample in IEMOCAP to a randomly selected target emotion.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Subjective Evaluation Results And Discussion",
      "text": "For the MOS test in terms of distortion, participants were shown a pair of samples for each question and were asked to rate each sample on the MOS scale shown in Table  1 . The MOS test results in terms of distortion with 95 % confidence interval are shown in Table  2 . We observe that the improved-StarGAN-EVC model outperforms the baseline-StarGAN-EVC model with a higher MOS score with no confidence interval overlap.\n\nFor the audio preference test, participants were asked to compare samples from each pair and choose the one they believe in having less perceived distortion (e. g., cracked voices, or jitter). The audio preference test results are summarised in Figure  4 . We observe that our proposed model obtained a preference of up to 58.21 %, which is almost three times the figure for the baseline-StarGAN-EVC model, indicating that humans clearly favored samples converted by our improved-StarGAN-EVC model.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Objective Evaluation Experimental Design",
      "text": "We also perform an objective evaluation using MCD, which measures the distortion between two sequences of Mel-cepstral coefficients. It is a commonly used metric for assessing the   [30, 31, 32, 33]  and EVC. We calculate the MCD between the mel cepstra of reference audio signals and that of audio signals converted by both the baseline-StarGAN-EVC and the improved-StarGAN-EVC model for all six source-to-target emotion pairs.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Objective Evaluation Results And Discussion",
      "text": "Table  3  reports the MCD values of source-target emotion pairs and the overall MCD values for all converted audio signals regardless of emotions. We note that our proposed improved-StarGAN-EVC model outperforms the baseline-StarGAN-EVC model in MCD for all terms, revealing that our proposed model can effectively reduce distortion. Besides, for the sad-happy and sad-angry emotion conversion, our model achieves a distortion reduction of up to and 1.192, which indicates that the proposed feature separation method has a much better suppression effect on distortion when the targets are high arousal emotions.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Data Augmentation Evaluation",
      "text": "Although the audio distortion evaluation experiments show that our proposed feature separation method can effectively reduce distortion, we still need to determine whether or not this improvement in audio quality is at the expense of our model's emotional conversion performance. We perform the data augmentation experiments to validate its emotional conversion performance.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Design",
      "text": "We use an end-to-end speech emotion recognition (SER) model  [34] , which directly takes the waveform as its input to accomplish the data augmentation experiments. The architecture is shown in the right part of Figure  3 . We first standardised the raw audio signal by using the mean and standard deviation from the training set. The raw signal was then sampled at an interval of 40 ms with a step size of 10 ms. Given the 16 kHz sampling rate of raw signals, the waveform for each frame is of dimension 640. We randomly selected 16 continuous frames for each audio signal and got the fixed-size input waveform of dimension 16 × 640. Note that the SER model here is entirely different from the pre-trained SER model used in our improved StarGAN training. Also, note that the SER model here is slightly different from the model used in  [6]  in terms of kernel sizes and filter numbers; that is why the data augmentation results we report for the baseline-StarGAN-EVC are different from the results shown in  [6] . Our SER model was trained using the Adam optimiser with the learning rate, β1 and β2 set to 1 × 10 -5 , 0.5, and 0.999. The batch size was set to 4. We employ sessions 1-3 of the IEMOCAP dataset as the original training set, session 4 as the validation set, and session 5 as the testing set. We consider Micro-F1 and Macro-F1 scores to report the performance of the SER model on the testing set. All results are averaged across 10 trials to reduce the effect of initialisation biases.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results And Discussion",
      "text": "We report the data augmentation results in Table  4 . We trained the SER model on three datasets, namely the IEMOCAP dataset of sessions 1-3, IEMOCAP sessions 1-3 augmented with audio signals generated by our proposed improved-StarGAN-EVC model, and IEMOCAP sessions 1-3 augmented with audio signals generated by the baseline-StarGAN-EVC model. We refer to them as sessions1-3, sessions1-3+improved-StarGAN, and sessions1-3+baseline-StarGAN, respectively. We observe that the model trained on sessions1-3+improved-StarGAN achieves higher Micro-F1 and Macro-F1 scores than the model trained only on sessions1-3, which indicates that the audio signals converted by our proposed model indeed carry effective emotional information and are valuable for data augmentation. We also note that the model trained on sessions1-3+improved-StarGAN achieves an absolute increase of 2 % in Micro-F1 and 5 % in Macro-F1 compared to the model trained on sessions1-3+baseline-StarGAN, which indicates that our model's improvement in audio quality is not at the expense of its emotional conversion performance; on the contrary, the audio signals generated by our proposed model are more consistent with the distribution of the original dataset and are more valuable for data augmentation.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "We proposed a novel StarGAN-based framework and a two-stage training process that explicitly separates the emotional features from emotion-independent features to reduce the audio distortion and improve the emotion conversion performance. Experimental results in the objective evaluation and the subjective evaluation in terms of audio distortion validated that the proposed model can effectively reduce the distortion and improve the voice quality, especially when the targets are high arousal emotions. Results of data augmentation experiments for end-to-end SER indicated that the improvement in audio quality obtained by the proposed model is not at the expense of its emotional conversion performance. The proposed model is more valuable for data augmentation. Future approaches should also consider verification of pertained speech intelligibility subsequent to conversion, such as by ASR similar to the here shown SER.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Since these silent frames represent silence between",
      "page": 1
    },
    {
      "caption": "Figure 1: Example waveforms of source and converted audio",
      "page": 1
    },
    {
      "caption": "Figure 2: The model structure used in the ﬁrst (left) and the second (right) stage of training.",
      "page": 2
    },
    {
      "caption": "Figure 2: , the autoencoder is",
      "page": 2
    },
    {
      "caption": "Figure 3: The structure of the pre-trained emotion classiﬁer (left)",
      "page": 2
    },
    {
      "caption": "Figure 2: , the emotion-",
      "page": 2
    },
    {
      "caption": "Figure 4: Results of audio preference test.",
      "page": 3
    },
    {
      "caption": "Figure 3: It has the",
      "page": 3
    },
    {
      "caption": "Figure 3: We ﬁrst standardised the",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Explanation of MOS Scores.",
      "page": 3
    },
    {
      "caption": "Table 2: Results of the MOS test with 95 % conﬁdence interval.",
      "page": 3
    },
    {
      "caption": "Table 1: The MOS test",
      "page": 3
    },
    {
      "caption": "Table 2: We observe that the improved-StarGAN-EVC",
      "page": 3
    },
    {
      "caption": "Table 3: Results of the MCD for source-target emotion pairs and overall converted audio signals regardless of emotions.",
      "page": 4
    },
    {
      "caption": "Table 4: Results of data augmentation experiments. Standard deviations for 10 trials are indicated in parentheses.",
      "page": 4
    },
    {
      "caption": "Table 3: reports the MCD values of source-target emotion pairs",
      "page": 4
    },
    {
      "caption": "Table 4: We trained",
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Towards intelligent crowdsourcing for audio data annotation: Integrating active learning in the real world",
      "authors": [
        "S Hantke",
        "Z Zhang",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "Towards intelligent crowdsourcing for audio data annotation: Integrating active learning in the real world"
    },
    {
      "citation_id": "3",
      "title": "Generative adversarial networks",
      "authors": [
        "I Goodfellow",
        "J Pouget-Abadie",
        "M Mirza",
        "B Xu",
        "D Warde-Farley",
        "S Ozair",
        "A Courville",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Generative adversarial networks",
      "arxiv": "arXiv:1406.2661"
    },
    {
      "citation_id": "4",
      "title": "Unpaired image-toimage translation using cycle-consistent adversarial networks",
      "authors": [
        "J.-Y Zhu",
        "T Park",
        "P Isola",
        "A Efros"
      ],
      "year": "2017",
      "venue": "Proceedings"
    },
    {
      "citation_id": "5",
      "title": "Stargan: Unified generative adversarial networks for multi-domain imageto-image translation",
      "authors": [
        "Y Choi",
        "M Choi",
        "M Kim",
        "J.-W Ha",
        "S Kim"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "6",
      "title": "Emotional voice conversion using dual supervised adversarial networks with continuous wavelet transform f0 features",
      "authors": [
        "Z Luo",
        "J Chen",
        "T Takiguchi",
        "Y Ariki"
      ],
      "year": "2019",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "7",
      "title": "Stargan for emotional speech conversion: Validated by data augmentation of endto-end emotion recognition",
      "authors": [
        "G Rizos",
        "A Baird",
        "M Elliott",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "8",
      "title": "Converting anyone's emotion: Towards speaker-independent emotional voice conversion",
      "authors": [
        "K Zhou",
        "B Sisman",
        "M Zhang",
        "H Li"
      ],
      "year": "2020",
      "venue": "Converting anyone's emotion: Towards speaker-independent emotional voice conversion",
      "arxiv": "arXiv:2005.07025"
    },
    {
      "citation_id": "9",
      "title": "Seen and unseen emotional style transfer for voice conversion with a new emotional speech dataset",
      "authors": [
        "K Zhou",
        "B Sisman",
        "R Liu",
        "H Li"
      ],
      "year": "2020",
      "venue": "Seen and unseen emotional style transfer for voice conversion with a new emotional speech dataset",
      "arxiv": "arXiv:2010.14794"
    },
    {
      "citation_id": "10",
      "title": "Non-parallel emotion conversion using a deep-generative hybrid network and an adversarial pair discriminator",
      "authors": [
        "R Shankar",
        "J Sager",
        "A Venkataraman"
      ],
      "year": "2020",
      "venue": "Non-parallel emotion conversion using a deep-generative hybrid network and an adversarial pair discriminator",
      "arxiv": "arXiv:2007.12932"
    },
    {
      "citation_id": "11",
      "title": "Can deep generative audio be emotional? towards an approach for personalised emotional audio generation",
      "authors": [
        "A Baird",
        "S Amiriparian",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "2019 IEEE 21st International Workshop on Multimedia Signal Processing (MMSP)"
    },
    {
      "citation_id": "12",
      "title": "Augmenting generative adversarial networks for speech emotion recognition",
      "authors": [
        "S Latif",
        "M Asim",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "Augmenting generative adversarial networks for speech emotion recognition",
      "arxiv": "arXiv:2005.08447"
    },
    {
      "citation_id": "13",
      "title": "Data augmentation using gans for speech emotion recognition",
      "authors": [
        "A Chatziagapi",
        "G Paraskevopoulos",
        "D Sgouropoulos",
        "G Pantazopoulos",
        "M Nikandrou",
        "T Giannakopoulos",
        "A Katsamanis",
        "A Potamianos",
        "S Narayanan"
      ],
      "year": "2019",
      "venue": "Data augmentation using gans for speech emotion recognition"
    },
    {
      "citation_id": "14",
      "title": "Modelling sample informativeness for deep affective computing",
      "authors": [
        "G Rizos",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "An evolutionary-based generative approach for audio data augmentation",
      "authors": [
        "S Mertes",
        "A Baird",
        "D Schiller",
        "B Schuller",
        "E André"
      ],
      "year": "2020",
      "venue": "2020 IEEE 22nd International Workshop on Multimedia Signal Processing (MMSP)"
    },
    {
      "citation_id": "16",
      "title": "Voice conversion from non-parallel corpora using variational autoencoder",
      "authors": [
        "C.-C Hsu",
        "H.-T Hwang",
        "Y.-C Wu",
        "Y Tsao",
        "H.-M Wang"
      ],
      "year": "2016",
      "venue": "2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "17",
      "title": "Voice conversion across arbitrary speakers based on a single target-speaker utterance",
      "authors": [
        "S Liu",
        "J Zhong",
        "L Sun",
        "X Wu",
        "X Liu",
        "H Meng"
      ],
      "year": "2018",
      "venue": "Interspeech"
    },
    {
      "citation_id": "18",
      "title": "Investigation of f0 conditioning and fully convolutional networks in variational autoencoder based voice conversion",
      "authors": [
        "W.-C Huang",
        "Y.-C Wu",
        "C.-C Lo",
        "P Tobing",
        "T Hayashi",
        "K Kobayashi",
        "T Toda",
        "Y Tsao",
        "H.-M Wang"
      ],
      "year": "2019",
      "venue": "Investigation of f0 conditioning and fully convolutional networks in variational autoencoder based voice conversion",
      "arxiv": "arXiv:1905.00615"
    },
    {
      "citation_id": "19",
      "title": "Autovc: Zero-shot voice style transfer with only autoencoder loss",
      "authors": [
        "K Qian",
        "Y Zhang",
        "S Chang",
        "X Yang",
        "M Hasegawa-Johnson"
      ],
      "year": "2019",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "20",
      "title": "F0consistent many-to-many non-parallel voice conversion via conditional autoencoder",
      "authors": [
        "K Qian",
        "Z Jin",
        "M Hasegawa-Johnson",
        "G Mysore"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "21",
      "title": "Unsupervised representation disentanglement using cross domain features and adversarial learning in variational autoencoder based voice conversion",
      "authors": [
        "W.-C Huang",
        "H Luo",
        "H.-T Hwang",
        "C.-C Lo",
        "Y.-H Peng",
        "Y Tsao",
        "H.-M Wang"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Emerging Topics in Computational Intelligence"
    },
    {
      "citation_id": "22",
      "title": "Mel-cepstral distance measure for objective speech quality assessment",
      "authors": [
        "R Kubichek"
      ],
      "year": "1993",
      "venue": "Proceedings of IEEE Pacific Rim Conference on Communications Computers and Signal Processing"
    },
    {
      "citation_id": "23",
      "title": "Computational Paralinguistics: Emotion, Affect and Personality in Speech and Language Processing",
      "authors": [
        "B Schuller",
        "A Batliner"
      ],
      "year": "2013",
      "venue": "Computational Paralinguistics: Emotion, Affect and Personality in Speech and Language Processing"
    },
    {
      "citation_id": "24",
      "title": "An acoustic study of emotions expressed in speech",
      "authors": [
        "S Yildirim",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "Z Deng",
        "S Lee",
        "S Narayanan",
        "C Busso"
      ],
      "year": "2004",
      "venue": "Eighth International Conference on Spoken Language Processing"
    },
    {
      "citation_id": "25",
      "title": "World: a vocoder-based high-quality speech synthesis system for real-time applications",
      "authors": [
        "M Morise",
        "F Yokomori",
        "K Ozawa"
      ],
      "year": "2016",
      "venue": "IEICE TRANSACTIONS on Information and Systems"
    },
    {
      "citation_id": "26",
      "title": "High quality voice conversion through phoneme-based linear mapping functions with straight for mandarin",
      "authors": [
        "K Liu",
        "J Zhang",
        "Y Yan"
      ],
      "year": "2007",
      "venue": "Fourth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD 2007)"
    },
    {
      "citation_id": "27",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "28",
      "title": "Stargan-vc: Non-parallel many-to-many voice conversion using star generative adversarial networks",
      "authors": [
        "H Kameoka",
        "T Kaneko",
        "K Tanaka",
        "N Hojo"
      ],
      "year": "2018",
      "venue": "2018 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "29",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "30",
      "title": "Improved training of wasserstein gans",
      "authors": [
        "I Gulrajani",
        "F Ahmed",
        "M Arjovsky",
        "V Dumoulin",
        "A Courville"
      ],
      "year": "2017",
      "venue": "Improved training of wasserstein gans",
      "arxiv": "arXiv:1704.00028"
    },
    {
      "citation_id": "31",
      "title": "Voice conversion using sequence-to-sequence learning of context posterior probabilities",
      "authors": [
        "H Miyoshi",
        "Y Saito",
        "S Takamichi",
        "H Saruwatari"
      ],
      "year": "2017",
      "venue": "Voice conversion using sequence-to-sequence learning of context posterior probabilities",
      "arxiv": "arXiv:1704.02360"
    },
    {
      "citation_id": "32",
      "title": "Average modeling approach to voice conversion with non-parallel data",
      "authors": [
        "X Tian",
        "J Wang",
        "H Xu",
        "E Chng",
        "H Li"
      ],
      "year": "2018",
      "venue": "Odyssey"
    },
    {
      "citation_id": "33",
      "title": "Non-parallel voice conversion with cyclic variational autoencoder",
      "authors": [
        "P Tobing",
        "Y.-C Wu",
        "T Hayashi",
        "K Kobayashi",
        "T Toda"
      ],
      "year": "2019",
      "venue": "Non-parallel voice conversion with cyclic variational autoencoder",
      "arxiv": "arXiv:1907.10185"
    },
    {
      "citation_id": "34",
      "title": "Cyclic spectral modeling for unsupervised unit discovery into voice conversion with excitation and waveform modelingh",
      "authors": [
        "P Tobing",
        "T Hayashi",
        "Y.-C Wu",
        "K Kobayashi",
        "T Toda"
      ],
      "year": "2020",
      "venue": "Proceedings of INTERSPEECH"
    },
    {
      "citation_id": "35",
      "title": "Attention-augmented end-toend multi-task learning for emotion prediction from speech",
      "authors": [
        "Z Zhang",
        "B Wu",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    }
  ]
}