{
  "paper_id": "2502.03979v2",
  "title": "Towards Unified Music Emotion Recognition Across Dimensional And Categorical Models",
  "published": "2025-02-06T11:20:22Z",
  "authors": [
    "Jaeyong Kang",
    "Dorien Herremans"
  ],
  "keywords": [
    "music emotion recognition",
    "multitask learning",
    "deep learning",
    "knowledge distillation",
    "music",
    "affective computing"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "One of the most significant challenges in Music Emotion Recognition (MER) comes from the fact that emotion labels can be heterogeneous across datasets with regard to the emotion representation, including categorical (e.g., happy, sad) versus dimensional labels (e.g., valence-arousal). In this paper, we present a unified multitask learning framework that combines these two types of labels and is thus able to be trained on multiple datasets. This framework uses an effective input representation that combines musical features (i.e., key and chords) and MERT embeddings. Moreover, knowledge distillation is employed to transfer the knowledge of teacher models trained on individual datasets to a student model, enhancing its ability to generalize across multiple tasks. To validate our proposed framework, we conducted extensive experiments on a variety of datasets, including MTG-Jamendo, DEAM, PMEmo, and EmoMusic. According to our experimental results, the inclusion of musical features, multitask learning, and knowledge distillation significantly enhances performance. In particular, our model outperforms the state-of-the-art models on the MTG-Jamendo dataset. Our work makes a significant contribution to MER by allowing the combination of categorical and dimensional emotion labels in one unified framework, thus enabling training across datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Music plays an essential role in influencing human emotions  [36] . In the past decades, numerous Music Emotion Recognition (MER) models been developed. MER has found applications in various domains, e.g., music recommendation systems  [51] , generative systems  [32] , and music therapy  [1] . However, the field faces significant challenges due to the diversity of available datasets and their inconsistent labeling schemes  [28] .\n\nMost MER datasets can be categorized into two types: categorical labels, which represent discrete emotions such as happy or sad, and dimensional labels, such as Russell's circumplex model of affect  [44] , which describes emotions on continuous scales of valence and arousal. While these datasets provide valuable resources, their heterogeneous nature complicates efforts to combine them effectively. Previous approaches  [5] ,  [9] ,  [10] ,  [26] ,  [34] ,  [40] ,  [43] ,  [48]  have often focused on a single type of label, limiting their ability as they can only train on one (often small) dataset.\n\nTo address these challenges, we introduce a unified multitask learning framework that incorporates both categorical and dimensional emotion labels within a shared architecture. This approach enables us to train on multiple datasets. Multitask learning has been shown to be successful on many other tasks such as speech emotion recognition  [6] , symbolic music emotion recognition  [41] , genre classification and mood detection  [18] , to financial portfolio prediction  [37] . These studies demonstrate the benefits of shared representations across tasks.\n\nTo properly represent the input music, our proposed framework combines three types of features: 1) Music undERstanding model with large-scale self-supervised Training (MERT) embeddings  [54] , 2) harmonic representations of chord progressions, and 3) musical key. The MERT embeddings are able to provide a rich representation of timbre, rhythm, and high-level musical semantics. Chord progressions and musical key features are able to encode harmonic and tonal structures, which are equally vital for understanding emotion in music.\n\nAn important innovation of our framework is the use of knowledge distillation (KD)  [21]  to unify learning across datasets with disparate label types. Pre-trained teacher models, optimized for either categorical or dimensional labels, guide the multitask student model by providing soft target logits, which are probability distributions over the possible labels. These logits help the student model learn more effectively by transferring the teacher model's knowledge, thereby improving generalization and performance across datasets. Additionally, the framework incorporates data augmentation strategies during feature extraction to introduce variability and mitigate overfitting, ensuring robustness across diverse audio inputs.\n\nTo validate our proposed framework, we conducted several experiments on multiple datasets, including MTG-Jamendo  [4]  for categorical labels and DEAM  [3] , PMEmo  [55] , and Emo-Music  [46]  for dimensional labels. Our experimental results show that integration of MERT embeddings with high-level musical feature (i.e., chord progression and key) significantly enhances performance. In addition, training the network on heterogeneous datasets with multitask learning can further improve performance on single datasets, demonstrating the ability to bridge the gap between diverse label types. On the MTG-Jamendo dataset, our model achieves state-of-the-art performance, surpassing the best-performing model, lileonardo  [5] , from the MediaEval 2021 competition  [50] , as well as more recent approaches  [18] ,  [19] , with the highest PR-AUC of 0.1543 and ROC-AUC of 0.7810. These results highlight the effectiveness of our approach in advancing MER.\n\nIn summary, our contributions are listed as follows:\n\n• We developed a unified multitask learning framework for MER that facilitates training on datasets with both categorical and dimensional emotion labels. • We combined the high-level musical features with MERT embeddings for a richer input representation. • We conducted several experiments and showed the effectiveness of our proposed framework. In addition, our proposed model achieves state-of-the-art performance on the MTG-Jamendo dataset  [4] . In the rest of the paper, we first describe the related work in Section II. This is followed by a description of our proposed framework in Section III. After that, we describe our experimental setup and its results in Section IV. Finally, Section VI offers conclusions from this work.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Below we will discuss some of the existing literature in MER research. For a more comprehensive overview of the literature, the reader is referred to  [28] ,  [45] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Single-Task Mer Models",
      "text": "Most MER models mainly concentrate on single-task learning, which involves training models on individual datasets which use particular emotion labeling schemes, like categorical labels (e.g., happy, sad) or dimensional labels (e.g., valence-arousal). Convolutional neural network (CNN)-based methods have demonstrated strong performance in MER across various datasets. For instance, Liu et al.  [31]  proposed a spectrogram-based CNN model which captures temporal and spectral features. This model has a macro F1-score of 0.472 and a micro F1-score of 0.534 on CAL500 dataset  [52] , and a macro F1-score of 0.596 and a micro F1-score of 0.709 on CAL500exp dataset  [53] , outperforming traditional approaches. Bour et al.  [5]  introduced the frequency-dependent convolutions in a CNN model, achieving the highest performance at MediaEval 2021 with a PR-AUC-macro of 0.1509 and a ROC-AUC-macro of 0.7748 on MTG-Jamendo dataset  [4] . Recently, Jia  [26]  introduced a CNN-based model which combines MFCCs with residual phase features, achieving a recognition accuracy of 92.06% on a dataset comprising 2,906 songs categorized into four emotion classes: anger, happiness, relaxation, and sadness.\n\nGiven the temporal nature of music, many researchers use recurrent neural network (RNN)-based architectures, such as Long-Short Term Memory networks (LSTMs) and Gated Recurrent Units (GRUs). For instance, Rajesh et al.  [43]  used LSTMs with MFCC features to predict emotion on the DEAM dataset  [3] , achieving an accuracy of 89.3%, which is better than SVM's 85.7%. Chaki et al.  [9]  added attention mechanisms to LSTMs to concentrate on emotionally significant segments, resulting in R 2 scores of 0.53 for valence and 0.75 for arousal on the EmoMusic dataset  [46] , surpassing the performance of the LSTM variant that lacked attention mechanisms.\n\nRecently, Transformer-based methods have seen a rise in popularity. For instance, Suresh et al.  [47]  proposed the multimodal model with Transformer-based architecture for classifying the mood of music by leveraging the features from audio and lyrics. Their proposed model was evaluated on a subset of the MoodyLyrics dataset  [7]  which contains audio and lyrics of 680 songs. Their proposed model outperforms the unimodal model with an accuracy of 77.94%.\n\nThe absence of official train/test splits in many datasets, such as the PMEmo dataset  [55] , makes it difficult to compare models directly, except for the MTG-Jamendo dataset  [4] , which provides official splits. This enables us to compare our model with those from MediaEval 2021  [5] ,  [34] ,  [40] ,  [48]  as well as more recent methods  [18] ,  [19] ,  [29]  in our experiments. Despite these achievements, single-task models frequently face challenges in generalizing across various datasets, underscoring the need for research that integrates multiple datasets.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Mer Models With Multi-Dataset Integration",
      "text": "Integrating multiple datasets is essential for improving generalization but is challenging due to inconsistencies in labeling schemes, especially between categorical and dimensional labels. Liu et al.  [30]  addressed this issue by leveraging a large language model (LLM) to align categorical labels from various datasets into a common semantic space. They used three disjoint datasets with categorical labels, which include MTG-Jamendo  [4] , CAL500  [52] , and Emotify  [2] . They showed the effectiveness of their approach by performing zero-shot inference on a new dataset.\n\nMazzetta et al.  [35]  introduced a multi-source learning framework that integrates features and labels from heterogeneous datasets, including 4Q  [38] , PMEmo  [55] , EmoMusic  [46] , and the Bi-Modal Emotion Dataset  [33]  to enhance the model's robustness. These datasets use Russell's Circumplex Model of Affect, and focus on the valence and arousal dimensions. While this framework effectively utilizes dimensional labels to improve robustness, it does not incorporate categorical labels, thus limiting its ability to fully leverage the diversity of available datasets  [28] . Developing frameworks that integrate both label types remains an open challenge, which we address in this work.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Multitask Learning In Mer",
      "text": "Multitask learning (MTL)  [8]  aims to enhance the performance of multiple related tasks by leveraging shared knowledge among them. Qiu et al.  [41]  introduced an MTL framework for Symbolic Music Emotion Recognition (SMER). They combines emotion recognition task with auxiliary tasks such as key and velocity classification. The idea is that by forcing the model to learn key and velocity information, it will also better understand the resulting emotion. Evaluated on the EMOPIA  [24]  and VGMIDI  [17]  datasets, their approach achieved accuracy improvements of 4.17% and 1.97%, respectively, compared to single-task models. Ji et al.  [23]  introduced an attention-based deep feature fusion (ADFF) method for valence-arousal prediction within a multitask learning framework. They integrate spatial feature extraction using an adapted VGGNet with a squeeze-and-excitation (SE) attentionbased temporal module to enhance affect-salient features. Their model achieved R 2 scores of 0.4575 for valence and 0.6394 for arousal on the PMEmo dataset  [55] .\n\nExisting MTL frameworks in MER are typically still trained on datasets with one type of emotion label. In this research, we explore how we can leverage an MTL architecture to enable predicting across heterogeneous emotion datasets.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Knowledge Distillation For Mer",
      "text": "Knowledge distillation (KD) is a technique where a smaller student model learns to replicate the behavior of a larger teacher model by aligning its soft predictions with those of the teacher, rather than solely relying on the actual labels  [21] . In contrast to transfer learning, which usually involves adjusting a pre-trained model for a new task or dataset, KD emphasizes the use of the teacher model's outputs (soft labels) to guide the training of the student model. Tong  [49]  proposed a KDbased multimodal MER framework that uses a teacher-student model, where a pre-trained genre classification model transfers knowledge to an emotion recognition model. Knowledge transfer is guided by Exponential Moving Average (EMA) analysis that refines the student model's parameters without backpropagation. The training minimizes a combined KL divergence and cross-entropy loss, which improves emotion recognition in terms of both labeled and unlabeled data while maintaining efficiency. Jeong et al.  [25]  demonstrated the potential of KD in a multitask setting for valence-arousal prediction, facial expression recognition, and action unit prediction. Despite its promise, the use of KD in multitask frameworks for integrating heterogeneous labels remains underexplored.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "E. Feature Engineering For Mer",
      "text": "Feature engineering is a key component in Music Emotion Recognition (MER). Earlier works relied heavily on handcrafted features such as MFCCs, chroma features, and rhythmic descriptors  [22] , which capture low-level signal properties but fall short in representing higher-level musical semantics  [39] . Other approaches leverage embeddings from audio models trained on large-scale datasets. For example, VGGish  [20] , a convolutional model trained in a supervised fashion for audio classification, and CLAP  [16] , which is trained with a contrastive audio-text alignment objective, have been used to extract richer representations of audio content.\n\nPre-trained encoders have also gained traction in MER due to their ability to learn from unlabeled audio data. Among these, MERT  [54]  has demonstrated strong performance relative to other pre-trained encoders. For instance, MERT-95M achieves a PR-AUC of 0.134 and an ROC-AUC of 0.764 on the MTG-Jamendo dataset, while the larger MERT-330M model improves these scores slightly to a PR-AUC of 0.14 and an ROC-AUC of 0.765. These results highlight the potential of pre-trained encoders for emotion prediction tasks.\n\nBeyond embeddings, high-level musical features such as key signatures and chord progressions have also been shown to play a significant role in emotion prediction  [11] . In this study, we aim to enhance MER by integrating large-scale audio embeddings with these symbolic, high-level musical descriptors.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Proposed Method",
      "text": "In this section, we present our proposed unified multitask learning framework for Music Emotion Recognition (MER), which trains on both categorical and dimensional emotion labels. The overall framework of our proposed MER system is shown in Figure  1 . First, we extract three complementary features from audio signals: 1) embeddings from the MERT model  [54] , 2) harmonic representations of chord progressions, and 3) musical key. These features are then concatenated and fed into classifiers in our multitask learning architecture to predict both categorical and dimensional emotion labels simultaneously. Furthermore, we leverage knowledge distillation technique, where teacher models are first trained separately on individual datasets-MTG-Jamendo for categorical labels and DEAM, PMEmo, and EmoMusic for dimensional labels-without multitask-specific branches. These trained teacher models generate soft logits to guide the student model during multitask learning, thereby improving performance across datasets with different label types.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Feature Extraction",
      "text": "To effectively capture music emotion, our proposed framework has two distinct feature extraction pipelines: 1) pretrained embeddings from MERT and 2) musical features.\n\n1) Pre-trained embeddings (MERT): The Music undERstanding model with large-scale self-supervised Training (MERT)  [54]  is employed for extracting embeddings from audio files. MERT is able to learn rich musical representations, capturing various musical features that make its embeddings highly effective for downstream tasks such as music emotion recognition (MER). To optimize performance for emotion recognition tasks, we derive embeddings by concatenating the outputs from the 5th and 6th layers of MERT, as these layers have demonstrated superior performance for music emotion prediction through preliminary experimentation.\n\nTo improve the model's robustness, the data augmentation step is incorporated within the MERT feature extraction pipeline as illustrated in Figure  2 . The data augmentation step is outlined as follows:\n\n1) The audio track is divided into fixed-length segments (i.e., 30 seconds each). 2) MERT embeddings are obtained for each segment from the 5th and 6th layers. 3) These embeddings from both layers are then combined to create the feature vector for each segment. 4) During training, we randomly select a series of consecutive segments. The starting index and the number of segments are chosen uniformly at random (e.g., Seg =  [2, 3]  in Figure  2 ). 5) The final feature vector is formed by averaging the embeddings of the selected segments. The intuition behind this augmentation strategy is to expose the model to varying temporal contexts of the same track across training epochs. Emotion in music can vary subtly throughout a piece-different sections might convey slightly different moods due to changes in melody, harmony, rhythm, or instrumentation. By randomly sampling and averaging different combinations of segments, the model learns to generalize across these intra-track variations, rather than overfitting to a single static representation. This not only improves robustness to local variations in musical content but also better reflects the dynamic and evolving nature of emotional expression in music.\n\n2) Musical Features (Chord and Key): Unlike the segmented and augmented MERT features, musical features (such as chord progressions and key signatures) are calculated over the whole song, representing global harmonic and tonal information. The framework integrates harmonic and tonal information by extracting musical features such as chord progressions and key signatures. A Transformer-based chord recognition model  [27]  is used for extracting the chord progressions of the song. The chord recognition model was evaluated on the following datasets: a subset of 221 songs from Isophonics 1  (171 songs by the Beatles, 12 songs by Carole King, 20 songs by Queen and 18 songs by Zweieck), Robbie Williams  [14]  (65 songs by Robbie Williams), and a subset of 185 songs from UsPop2002 2  . The chord recognition model is evaluated using mir_eval metrics  [42] , achieving Weighted Chord Symbol Recall (WCSR) scores of 83.5% for Root, 80.8% for Thirds, 75.9% for Triads, 71.8% for Sevenths, 65.5% for Tetrads, 82.3% for Maj-min, and 80.8% for the MIREX categories. These scores are considered acceptable for this study, as most errors are minor, such as misclassifying A minor as C major.\n\nIn our model, each chord is encoded by its root (e.g., D) and quality (e.g., sus4), representing the chord type, and mapped into an embedding space to capture harmonic patterns. These chord progressions are then converted into MIDI representations based on music theory. For instance, a C major chord comprises the notes C, E, G, while a C minor 7th chord comprises the notes C, E, G, B. The start and end times of each chord are mapped to define its duration in the MIDI file. This MIDI representation serves as input to a key detection model.\n\nKey detection is performed using the music21 library  [12] . To ensure consistency in the representation of chords, we 'normalize' them based on the key. Extracted chords from songs in major keys are transposed to C major, while those from minor keys are transposed to A minor. The extracted chord sequences have 13 different chord types such as 'major', 'minor', 'diminished', 'augmented', 'suspended', and 'seventh' chords. The key is encoded based on its mode. For instance, 'major' corresponds to C major and 'minor' corresponds to A minor. These harmonic features-key (as a scalar) and chords (as sequences of embedded tokens)-are integrated with MERT features and used as input to the model.\n\n3) Temporal Modeling of Chord Progressions: The output of the chord detection model gives us a long sequence of chords. To capture the temporal dependencies and relationships in these harmonic progressions, we model these sequences a Transformer-based encoder architecture, (marked as 'Chord Transformer' in Figure  1 ). Each chord in the t-th position of the sequence is represented as a concatenation of its root embedding, C (t) root ∈ R droot , and quality embedding,\n\nquality ∈ R dquality . The combined embedding for the t-th chord is expressed as follows:\n\nwhere ⊕ denotes vector concatenation, resulting in C (t) ∈ R droot+dquality .\n\nTo incorporate the sequential structure of the chord progression, a positional encoding P (t) ∈ R droot+dquality is added to the embeddings:\n\nThe encoded chord embeddings,\n\nenc , . . . , C (T ) enc ], represent the sequence of chords for the song, where T is the total number of chords in the sequence. A special CLS token, CLS ∈ R droot+dquality , is prepended to this sequence to aggregate global information:\n\nThe input sequence is processed by a Transformer encoder with two layers and eight attention heads, generating output embeddings C out , where the first token corresponds to the CLS representation:\n\nThe output C CLS-out , taken as the first element of the Transformer output, serves as a global representation of the chord progression. Rather than merely reflecting its positional placement, the CLS token aggregates information from all chord embeddings via the self-attention mechanism.\n\nIt is concatenated with the MERT embeddings, f MERT , and key embeddings, K, to form the final combined feature vector:\n\nThe combined feature vector is projected into a latent space using a feedforward layer with 512 units and ReLU activation before being passed to task-specific branches for mood classification and valence-arousal regression. The classification and regression branches each consist of two feedforward layers with 256 hidden units.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Classification With Multitask Learning",
      "text": "In our proposed multitask learning framework, we have two different branches to handle both categorical and dimensional music emotion prediction tasks. We use a Binary Cross-Entropy (BCE) loss function when training the network on the dataset with categorical labels, such as \"happy\" or \"sad\". To address the class imbalance issue, the BCE loss is weighted based on the frequency of the positive class for each label, which is defined as follows:\n\nwhere x is the predicted probabilities for each emotion category, and y is the corresponding ground-truth binary label (1 for presence and 0 for absence of the category), c denotes the total number of emotion labels, ensuring that the loss is averaged across all categories, and the term w i adjusts the contribution of the positive class, while wi scales the contribution of the negative class, which can be defined as:\n\nwhere p i is the frequency of the positive class for label i.\n\nFor dimensional labels, the model predicts continuous valence and arousal (VA) values using a Mean Squared Error (MSE) loss. The MSE loss is given by:\n\nwhere y v and y a are the ground-truth valence and arousal values, and x v and x a are their respective predictions. We use a selective update strategy to mitigate task interference in our multitask learning framework. For the dataset with categorical labels such as MTG-Jamendo, we update only the categorical branch's parameter. Likewise, for the datasets with dimensional labels (e.g., DEAM or PMEmo), we update only the parameters of the dimensional branch. By taking a selective update strategy, the model can effectively learn from heterogeneous datasets with preserved task-specific performance.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "C. Knowledge Distillation",
      "text": "We use the knowledge distillation technique to efficiently train our multitask models by transferring knowledge from the pre-trained teacher models to the multitask student models  [21] . Separate teacher models are trained on categorical and dimensional datasets, each optimized for its respective task. For instance, a teacher model trained on the MTG-Jamendo dataset generates soft labels for categorical labels, while teacher models trained on DEAM, PMEmo, and EmoMusic generate soft labels for dimensional predictions.\n\nThe student model learns from both hard labels (ground truth) and soft logits through the Kullback-Leibler (KL) divergence loss, which is defined as:\n\nwhere s and t denote the predicted and teacher-generated soft labels, respectively, c is the number of output dimensions (i.e., emotion categories for classification, or 2 for valence and arousal in regression), and s i , t i represent the i-th elements of the student and teacher distributions, respectively.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "D. Total Loss Function",
      "text": "The total loss function combines the task-specific loss with the KL divergence loss derived from the teacher models. Specifically, for categorical and dimensional datasets, the total loss functions are defined as:\n\nwhere α and β are hyperparameters that control the balance between the task-specific loss and the KL divergence loss for classification and regression, respectively. L KD,Cat and L KD,VA refer to the KL divergence losses computed using the soft targets generated by the classification and regression teacher models, respectively. L Total,Cat and L Total,VA represent the final loss values used to update the classification and regression branches of the multitask model, respectively.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Iv. Experimental Setup",
      "text": "In this section, we discuss the experimental setup of the proposed unified multitask learning framework.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Dataset",
      "text": "In our experiment, we use four different datasets for either categorical emotion recognition or dimensional emotion prediction tasks. For categorical emotion recognition, we use the MTG-Jamendo dataset  [4] , which consists of 18,486 full-length tracks labeled with 56 mood/theme tags such as \"happy\" and \"sad\".\n\nFor dimensional emotion prediction, we use three different datasets with valence-arousal (VA) labels which are normalized within the range of 1 to 9:\n\n• DEAM dataset  [3] : This dataset contains 1,802 songs from Western pop and rock. It has both dynamic and static VA annotations. We use the static VA annotations. • PMEmo dataset  [55] : This dataset consists of 794 songs from Western pop. It has both dynamic and static VA annotations. We use the static VA annotations.\n\n• EmoMusic dataset  [46] : This dataset contains 744 songs spanning diverse genres. It has both dynamic and static VA annotations. We use the static VA annotations.\n\nTo ensure consistency, we follow the official train, validation, and test splits for the MTG-Jamendo dataset. For the other datasets which do not provide official training, validation, and test splits, we randomly split the data into 70% for the training set, 15% for the validation set, and 15% for the test set. The details of the datasets used in our experiments are shown in Table  I .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Implementation Details",
      "text": "To extract MERT features, the audio tracks are segmented into 30-second clips. We leverage the popular MERT-v1-95M model, accessible via Hugging Face 3  , to obtain the embeddings. Knowledge distillation (KD) is employed, where teacher models pre-trained on each dataset guide the student model during multitask learning. The teacher models are trained separately on each dataset using the same architecture as the student. For feature extraction, we concatenate the 5th and 6th layer embeddings of MERT and process chord/key features with the Chord Transformer with positional encoding. The hyperparameters for the total loss function (as defined in Equations 10 and 11) are set to α = 0.2 and β = 0.2. During training, only the relevant loss components are updated based on the dataset type (categorical or dimensional). The student and teacher models are trained for 200 epochs with a batch size of 8, a learning rate of 0.0001, and the Adam optimizer. All models, including both the student and teacher models, are trained on a cluster of four NVIDIA Tesla V100 DGXS GPUs, each with 32 GB of memory. The code is implemented in PyTorch and is available online  4  .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Performance Metrics",
      "text": "Depending on the labeling schemes of datasets, we employ different metrics for evaluating our proposed models. For instance, we use both Precision-Recall AUC (PR-AUC) and Receiver Operating Characteristic AUC (ROC-AUC) for MTG-Jamendo dataset with categorical labels. On the other hand, we use the R 2 scores for valence (R 2 V ) and arousal (R 2 A ) for other three datasets with dimensional labels. 1) PR-AUC and ROC-AUC: PR-AUC  [13]  measures precision-recall trade-offs to assess classification performance. This is especially useful for unbalanced datasets. On the other hand, ROC-AUC  [13]  assesses how well a model performs by analyzing true positive and false positive rates across various decision thresholds.\n\n2) R",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Training Datasets",
      "text": "MTG-Jamendo (J.) DEAM (D.) EmoMusic (E.) PMEmo (P.)  Model PR-AUC ROC-AUC lileonardo  [5]  0.1508 0.7747 SELAB-HCMUS  [40]  0.1435 0.7599 Mirable  [48]  0.1356 0.7687 UIBK-DBIS  [34]  0.1087 0.7046 Hasumi et al.  [19]  0.0730 0.7750 Greer et al.  [18]  0.1082 0.7354 MERT-95M  [54]  0.1340 0.7640 MERT-330M  [54]  0.1400 0.7650 Proposed (Ours) 0.1543 0.7810",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "D. Baseline Models On Mtg-Jamendo",
      "text": "To evaluate the effectiveness of our approach, we compare it against several representative models developed for the MTG-Jamendo dataset, as shown in Table  IV : 1) lileonardo  [5] , a deep CNN that applies frequency-dependent convolutions to mel spectrograms, achieving the top score in the MediaEval 2021 competition; 2) SELAB-HCMUS  [40] , which uses a co-teaching training strategy and short audio segments to enhance performance and reduce training time; 3) Mirable  [48] , which explores semi-supervised learning through noisy student training and harmonic pitch class profiles (HPCP); 4) UIBK-DBIS  [34] , an ensemble-based model that clusters emotion labels to train specialized classifiers; 5) Hasumi et al.  [19] , which proposes classifier group chains to capture tag dependencies for improved music tagging; 6) Greer et al.  [18] , who introduce M3BERT, a multi-task, self-supervised transformer trained via masked reconstruction and fine-tuned for emotion recognition; and 7) MERT  [29] , a large-scale acoustic music understanding model (with 95M and 330M parameters) pre-trained using pseudo labels from acoustic and musical teachers, demonstrating strong performance across several music understanding tasks.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "V. Results",
      "text": "In a first experiment, we removed the high-level musical features (i.e., key and chords) from our proposed framework and trained the network using the MERT features only. The performance of the models with these different input feature configurations is shown in Table  II . As can be seen from the table, incorporating both the MERT and high-level musical features significantly enhanced the performance of our model. The model with high-level musical features includes the Chord Transformer model. This achieves the best results, including a PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-Jamendo dataset. Furthermore, the framework achieves higher R 2\n\nV and R 2 A scores for valence and arousal on the dimensional datasets (DEAM, EmoMusic, and PMEmo), demonstrating the importance of including the musical features.\n\nIn a second experiment, we explore how training the network on heterogeneous datasets can improve performance on single datasets. Table  III  shows the comparison of performance metrics when training on multiple datasets. We trained the network on multiple datasets as indicated in the leftmost column. When comparing our results without data fusion to the results for our model trained on each of the datasets separately (row 1), we notice that the performance clearly increases when adding a second dataset. However, the best performance is reached when training on all datasets (MTG-Jamendo + DEAM + EmoMusic + PMEmo), with a PR-AUC of 0.1543 and an ROC-AUC of 0.7810 on the MTG-Jamendo dataset, as well as the best R 2\n\nV and R 2 A scores across the dimensional datasets. These results show the importance of leveraging diverse datasets in a unified multitask framework.\n\nLastly, we evaluated our proposed model on the MTG-Jamendo dataset using its official train-validation-test split, which enables direct comparison with prior work. As shown in Table  IV , our model achieves strong performance compared to both earlier baselines such as lileonardo  [5]  from the MediaEval 2021 competition  [50] , as well as more recent representative methods  [18] ,  [19] ,  [54] , which reflect current directions in MER modeling. The substantial improvement over these models suggests that our proposed framework is a promising approach for categorical emotion recognition in music. In contrast, other MER datasets do not offer official splits, which makes direct performance comparison less consistent across studies.\n\nThese results confirm our proposed framework as a tool to unify categorical and dimensional emotion recognition without sacrificing state-of-the-art performance. The integration of MERT embeddings, musical features, multitask learning, and knowledge distillation proves to be a highly effective method that enhances music emotion recognition on diverse datasets and labeling schemes.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "In this study, we propose a unified multitask learning framework for Music Emotion Recognition (MER) that facilitates training on datasets with both categorical and dimensional emotion labels. Our proposed architecture incorporates knowledge distillation and takes both high-level musical features such as chords and key signatures, as well as pre-trained embeddings from MERT as input, thus enabling it to effectively capture emotional nuances in music. The experimental results demonstrate that our framework outperforms state-of-the-art models on the MTG-Jamendo dataset, including the winners of the MediaEval 2021 competition. Our best model achieves a PR-AUC of 0.1543 and an ROC-AUC of 0.7810. This model is made available open-source online. Our results show that our multitask learning approach enables generalization across diverse datasets, while knowledge distillation facilitates efficient knowledge transfer from teacher to student models.\n\nIn summary, our work provides a robust solution for training on multiple datasets with different types of emotion labels. In future research, we may explore the addition of different musical features as input to the model, refine data augmentation techniques, and expand the framework's applicability to other affective computing domains.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overall architecture of our proposed framework.",
      "page": 3
    },
    {
      "caption": "Figure 2: MERT feature extraction and data augmentation workflow.",
      "page": 4
    },
    {
      "caption": "Figure 1: First, we extract three complementary",
      "page": 4
    },
    {
      "caption": "Figure 2: The data augmentation step",
      "page": 4
    },
    {
      "caption": "Figure 1: ). Each chord",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "jaeyong\nkang@sutd.edu.sg": "Abstract—One\nof\nthe most\nsignificant\nchallenges\nin Music",
          "dorien herremans@sutd.edu.sg": "To address these challenges, we introduce a unified multi-"
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "Emotion Recognition (MER) comes\nfrom the fact\nthat emotion",
          "dorien herremans@sutd.edu.sg": ""
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "",
          "dorien herremans@sutd.edu.sg": "task learning framework that incorporates both categorical and"
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "labels\ncan\nbe\nheterogeneous\nacross\ndatasets with\nregard\nto",
          "dorien herremans@sutd.edu.sg": ""
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "",
          "dorien herremans@sutd.edu.sg": "dimensional emotion labels within a shared architecture. This"
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "the\nemotion representation,\nincluding\ncategorical\n(e.g., happy,",
          "dorien herremans@sutd.edu.sg": ""
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "",
          "dorien herremans@sutd.edu.sg": "approach enables us\nto train on multiple datasets. Multitask"
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "sad)\nversus\ndimensional\nlabels\n(e.g.,\nvalence-arousal).\nIn\nthis",
          "dorien herremans@sutd.edu.sg": ""
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "paper, we present a unified multitask learning framework that",
          "dorien herremans@sutd.edu.sg": "learning\nhas\nbeen\nshown\nto\nbe\nsuccessful\non many\nother"
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "combines these two types of labels and is thus able to be trained",
          "dorien herremans@sutd.edu.sg": "tasks such as speech emotion recognition [6], symbolic music"
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "on multiple datasets. This\nframework uses\nan effective\ninput",
          "dorien herremans@sutd.edu.sg": ""
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "",
          "dorien herremans@sutd.edu.sg": "emotion recognition [41], genre classification and mood detec-"
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "representation\nthat\ncombines musical\nfeatures\n(i.e.,\nkey\nand",
          "dorien herremans@sutd.edu.sg": ""
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "",
          "dorien herremans@sutd.edu.sg": "tion [18],\nto financial portfolio prediction [37]. These studies"
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "chords) and MERT embeddings. Moreover, knowledge distillation",
          "dorien herremans@sutd.edu.sg": ""
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "",
          "dorien herremans@sutd.edu.sg": "demonstrate the benefits of shared representations across tasks."
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "is employed to transfer the knowledge of teacher models trained",
          "dorien herremans@sutd.edu.sg": ""
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "on individual datasets\nto a student model, enhancing its ability",
          "dorien herremans@sutd.edu.sg": "To properly represent\nthe input music, our proposed frame-"
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "to\ngeneralize\nacross multiple\ntasks. To\nvalidate\nour proposed",
          "dorien herremans@sutd.edu.sg": "work combines three types of features: 1) Music undERstand-"
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "framework, we\nconducted extensive\nexperiments\non a\nvariety",
          "dorien herremans@sutd.edu.sg": ""
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "",
          "dorien herremans@sutd.edu.sg": "ing model with large-scale self-supervised Training (MERT)"
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "of\ndatasets,\nincluding MTG-Jamendo, DEAM,\nPMEmo,\nand",
          "dorien herremans@sutd.edu.sg": ""
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "",
          "dorien herremans@sutd.edu.sg": "embeddings\n[54], 2) harmonic representations of chord pro-"
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "EmoMusic. According to our experimental results,\nthe inclusion",
          "dorien herremans@sutd.edu.sg": ""
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "",
          "dorien herremans@sutd.edu.sg": "gressions,\nand 3) musical key. The MERT embeddings\nare"
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "of musical features, multitask learning, and knowledge distillation",
          "dorien herremans@sutd.edu.sg": ""
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "significantly\nenhances\nperformance.\nIn\nparticular,\nour model",
          "dorien herremans@sutd.edu.sg": "able to provide a rich representation of\ntimbre,\nrhythm, and"
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "outperforms\nthe\nstate-of-the-art models on the MTG-Jamendo",
          "dorien herremans@sutd.edu.sg": "high-level musical semantics. Chord progressions and musical"
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "dataset. Our work makes a significant contribution to MER by",
          "dorien herremans@sutd.edu.sg": ""
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "",
          "dorien herremans@sutd.edu.sg": "key features are able to encode harmonic and tonal structures,"
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "allowing the combination of categorical and dimensional emotion",
          "dorien herremans@sutd.edu.sg": ""
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "",
          "dorien herremans@sutd.edu.sg": "which are equally vital\nfor understanding emotion in music."
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "labels\nin one unified framework,\nthus\nenabling training across",
          "dorien herremans@sutd.edu.sg": ""
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "",
          "dorien herremans@sutd.edu.sg": "An important\ninnovation of our\nframework is\nthe use of"
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "datasets.",
          "dorien herremans@sutd.edu.sg": ""
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "Index Terms—music emotion recognition, multitask learning,",
          "dorien herremans@sutd.edu.sg": "knowledge\ndistillation\n(KD)\n[21]\nto\nunify\nlearning\nacross"
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "deep learning, knowledge distillation, music, affective computing",
          "dorien herremans@sutd.edu.sg": "datasets with disparate label types. Pre-trained teacher models,"
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "",
          "dorien herremans@sutd.edu.sg": "optimized for either categorical or dimensional\nlabels, guide"
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "",
          "dorien herremans@sutd.edu.sg": "the multitask student model by providing soft\ntarget\nlogits,"
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "I.\nINTRODUCTION",
          "dorien herremans@sutd.edu.sg": ""
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "",
          "dorien herremans@sutd.edu.sg": "which are probability distributions over\nthe possible\nlabels."
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "Music plays\nan essential\nrole\nin influencing human emo-",
          "dorien herremans@sutd.edu.sg": "These logits help the student model\nlearn more effectively by"
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "tions\n[36].\nIn\nthe\npast\ndecades,\nnumerous Music Emotion",
          "dorien herremans@sutd.edu.sg": "transferring the teacher model’s knowledge, thereby improving"
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "Recognition (MER) models been developed. MER has found",
          "dorien herremans@sutd.edu.sg": "generalization and performance across datasets. Additionally,"
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "applications in various domains, e.g., music recommendation",
          "dorien herremans@sutd.edu.sg": "the framework incorporates data augmentation strategies dur-"
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "systems [51], generative systems [32], and music therapy [1].",
          "dorien herremans@sutd.edu.sg": "ing\nfeature\nextraction\nto\nintroduce\nvariability\nand mitigate"
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "However,\nthe\nfield\nfaces\nsignificant\nchallenges\ndue\nto\nthe",
          "dorien herremans@sutd.edu.sg": "overfitting, ensuring robustness across diverse audio inputs."
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "diversity of available datasets and their\ninconsistent\nlabeling",
          "dorien herremans@sutd.edu.sg": "To validate our proposed framework, we conducted several"
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "schemes [28].",
          "dorien herremans@sutd.edu.sg": "experiments on multiple datasets, including MTG-Jamendo [4]"
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "Most MER datasets can be categorized into two types: cate-",
          "dorien herremans@sutd.edu.sg": "for categorical\nlabels and DEAM [3], PMEmo [55], and Emo-"
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "gorical labels, which represent discrete emotions such as happy",
          "dorien herremans@sutd.edu.sg": "Music [46]\nfor dimensional\nlabels. Our experimental\nresults"
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "or sad, and dimensional\nlabels, such as Russell’s circumplex",
          "dorien herremans@sutd.edu.sg": "show that\nintegration of MERT embeddings with high-level"
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "model of affect [44], which describes emotions on continuous",
          "dorien herremans@sutd.edu.sg": "musical\nfeature (i.e., chord progression and key) significantly"
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "scales of valence\nand arousal. While\nthese datasets provide",
          "dorien herremans@sutd.edu.sg": "enhances performance.\nIn addition,\ntraining the network on"
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "valuable\nresources,\ntheir\nheterogeneous\nnature\ncomplicates",
          "dorien herremans@sutd.edu.sg": "heterogeneous\ndatasets with multitask\nlearning\ncan\nfurther"
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "efforts to combine them effectively. Previous approaches [5],",
          "dorien herremans@sutd.edu.sg": "improve\nperformance\non\nsingle\ndatasets,\ndemonstrating\nthe"
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "[9],\n[10],\n[26],\n[34],\n[40],\n[43],\n[48] have often focused on a",
          "dorien herremans@sutd.edu.sg": "ability\nto\nbridge\nthe\ngap\nbetween\ndiverse\nlabel\ntypes. On"
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "single type of label, limiting their ability as they can only train",
          "dorien herremans@sutd.edu.sg": "the MTG-Jamendo dataset, our model achieves state-of-the-art"
        },
        {
          "jaeyong\nkang@sutd.edu.sg": "on one (often small) dataset.",
          "dorien herremans@sutd.edu.sg": "performance, surpassing the best-performing model, lileonardo"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "more recent approaches [18],\n[19], with the highest PR-AUC",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "the performance of\nthe LSTM variant\nthat\nlacked attention"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "of 0.1543 and ROC-AUC of 0.7810. These results highlight",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "mechanisms."
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "the effectiveness of our approach in advancing MER.",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "Recently, Transformer-based methods have\nseen a\nrise\nin"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "In summary, our contributions are listed as follows:",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "popularity.\nFor\ninstance,\nSuresh\net\nal.\n[47]\nproposed\nthe"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "• We\ndeveloped\na\nunified multitask\nlearning\nframework",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "multimodal model with Transformer-based\narchitecture\nfor"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "for MER that\nfacilitates\ntraining on datasets with both",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "classifying the mood of music by leveraging the features from"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "categorical and dimensional emotion labels.",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "audio and lyrics. Their proposed model was evaluated on a"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "• We combined the high-level musical features with MERT",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "subset of\nthe MoodyLyrics dataset\n[7] which contains audio"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "embeddings for a richer\ninput\nrepresentation.",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "and lyrics of 680 songs. Their proposed model outperforms"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "• We\nconducted several\nexperiments\nand showed the\nef-",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "the unimodal model with an accuracy of 77.94%."
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "fectiveness of our proposed framework.\nIn addition, our",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "The\nabsence of official\ntrain/test\nsplits\nin many datasets,"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "proposed model achieves state-of-the-art performance on",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "such as the PMEmo dataset [55], makes it difficult\nto compare"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "the MTG-Jamendo dataset\n[4].",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "models\ndirectly,\nexcept\nfor\nthe MTG-Jamendo\ndataset\n[4],"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "which\nprovides\nofficial\nsplits. This\nenables\nus\nto\ncompare"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "In\nthe\nrest\nof\nthe\npaper, we\nfirst\ndescribe\nthe\nrelated",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": ""
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "our model with those from MediaEval 2021 [5],\n[34],\n[40],"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "work in Section II. This\nis\nfollowed by a description of our",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": ""
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "[48] as well as more recent methods\n[18],\n[19],\n[29]\nin our"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "proposed framework in Section III. After\nthat, we describe",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": ""
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "experiments. Despite\nthese\nachievements,\nsingle-task mod-"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "our experimental setup and its results in Section IV. Finally,",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": ""
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "els\nfrequently face challenges\nin generalizing across various"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "Section VI offers conclusions from this work.",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": ""
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "datasets,\nunderscoring\nthe\nneed\nfor\nresearch\nthat\nintegrates"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "II. RELATED WORK",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "multiple datasets."
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "Below we will discuss\nsome of\nthe\nexisting literature\nin",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": ""
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "B. MER Models with Multi-Dataset\nIntegration"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "MER research. For\na more\ncomprehensive overview of\nthe",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": ""
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "literature,\nthe reader\nis referred to [28],\n[45].",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "Integrating multiple datasets is essential for improving gen-"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "eralization but is challenging due to inconsistencies in labeling"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "A.\nSingle-Task MER Models",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": ""
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "schemes, especially between categorical and dimensional\nla-"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "Most MER models mainly concentrate on single-task learn-",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "bels. Liu et al. [30] addressed this issue by leveraging a large"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "ing, which\ninvolves\ntraining models\non\nindividual\ndatasets",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "language model (LLM) to align categorical labels from various"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "which\nuse\nparticular\nemotion\nlabeling\nschemes,\nlike\ncate-",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "datasets\ninto\na\ncommon\nsemantic\nspace. They\nused\nthree"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "gorical\nlabels\n(e.g., happy,\nsad) or dimensional\nlabels\n(e.g.,",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "disjoint datasets with categorical\nlabels, which include MTG-"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "valence-arousal). Convolutional neural network (CNN)-based",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "Jamendo [4], CAL500 [52],\nand Emotify [2]. They showed"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "methods\nhave\ndemonstrated\nstrong\nperformance\nin MER",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "the\neffectiveness of\ntheir\napproach by performing zero-shot"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "across various datasets. For\ninstance, Liu et al.\n[31] proposed",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "inference on a new dataset."
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "a\nspectrogram-based CNN model which\ncaptures\ntemporal",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "Mazzetta\net\nal.\n[35]\nintroduced\na multi-source\nlearning"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "and spectral\nfeatures. This model has\na macro F1-score of",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "framework that\nintegrates\nfeatures and labels\nfrom heteroge-"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "0.472\nand\na micro F1-score\nof\n0.534\non CAL500\ndataset",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "neous datasets,\nincluding 4Q [38], PMEmo [55], EmoMusic"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "[52], and a macro F1-score of 0.596 and a micro F1-score of",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "[46], and the Bi-Modal Emotion Dataset\n[33]\nto enhance the"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "0.709 on CAL500exp dataset\n[53], outperforming traditional",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "model’s\nrobustness. These datasets use Russell’s Circumplex"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "approaches. Bour et al. [5] introduced the frequency-dependent",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "Model\nof Affect,\nand\nfocus\non\nthe\nvalence\nand\narousal"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "convolutions\nin a CNN model, achieving the highest perfor-",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "dimensions. While this framework effectively utilizes dimen-"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "mance at MediaEval 2021 with a PR-AUC-macro of 0.1509",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "sional\nlabels\nto improve\nrobustness,\nit does not\nincorporate"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "and a ROC-AUC-macro of 0.7748 on MTG-Jamendo dataset",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "categorical\nlabels,\nthus limiting its ability to fully leverage the"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "[4]. Recently, Jia [26]\nintroduced a CNN-based model which",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "diversity of\navailable datasets\n[28]. Developing frameworks"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "combines MFCCs with residual phase\nfeatures,\nachieving a",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "that\nintegrate\nboth\nlabel\ntypes\nremains\nan\nopen\nchallenge,"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "recognition accuracy of 92.06% on a dataset comprising 2,906",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "which we address in this work."
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "songs categorized into four emotion classes: anger, happiness,",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": ""
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "C. Multitask Learning in MER"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "relaxation, and sadness.",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": ""
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "Given\nthe\ntemporal\nnature\nof music, many\nresearchers",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "Multitask learning (MTL)\n[8] aims\nto enhance the perfor-"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "use recurrent neural network (RNN)-based architectures, such",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "mance of multiple related tasks by leveraging shared knowl-"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "as Long-Short Term Memory networks\n(LSTMs) and Gated",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "edge among them. Qiu et al.\n[41]\nintroduced an MTL frame-"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "Recurrent Units\n(GRUs).\nFor\ninstance, Rajesh\net\nal.\n[43]",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "work for Symbolic Music Emotion Recognition (SMER). They"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "used LSTMs with MFCC features\nto\npredict\nemotion\non",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "combines emotion recognition task with auxiliary tasks\nsuch"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "the DEAM dataset\n[3],\nachieving\nan\naccuracy\nof\n89.3%,",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "as key and velocity classification. The idea is that by forcing"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "which is better\nthan SVM’s 85.7%. Chaki\net\nal.\n[9]\nadded",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "the model\nto learn key and velocity information,\nit will also"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "attention mechanisms to LSTMs to concentrate on emotionally",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "better\nunderstand\nthe\nresulting\nemotion. Evaluated\non\nthe"
        },
        {
          "[5],\nfrom the MediaEval 2021 competition [50],\nas well\nas": "significant segments, resulting in R2 scores of 0.53 for valence",
          "and 0.75 for arousal on the EmoMusic dataset [46], surpassing": "EMOPIA [24]\nand VGMIDI\n[17]\ndatasets,\ntheir\napproach"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 1. Overall architecture of our proposed framework.": "promise, the use of KD in multitask frameworks for integrating"
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": "heterogeneous labels remains underexplored."
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": ""
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": "E. Feature Engineering for MER"
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": ""
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": "Feature engineering is a key component\nin Music Emotion"
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": ""
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": "Recognition\n(MER). Earlier works\nrelied\nheavily\non\nhand-"
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": ""
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": "crafted features such as MFCCs, chroma features, and rhyth-"
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": ""
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": "mic descriptors [22], which capture low-level signal properties"
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": ""
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": "but\nfall\nshort\nin representing higher-level musical\nsemantics"
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": ""
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": "[39]. Other approaches leverage embeddings from audio mod-"
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": ""
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": "els trained on large-scale datasets. For example, VGGish [20],"
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": ""
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": "a\nconvolutional model\ntrained\nin\na\nsupervised\nfashion\nfor"
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": ""
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": "audio classification,\nand CLAP [16], which is\ntrained with"
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": ""
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": "a contrastive audio-text alignment objective, have been used"
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": "to extract\nricher\nrepresentations of audio content."
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": "Pre-trained encoders have also gained traction in MER due"
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": "to their\nability to learn from unlabeled audio data. Among"
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": ""
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": "these, MERT [54] has demonstrated strong performance rel-"
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": ""
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": "ative to other pre-trained encoders. For\ninstance, MERT-95M"
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": ""
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": "achieves a PR-AUC of 0.134 and an ROC-AUC of 0.764 on the"
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": ""
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": "MTG-Jamendo dataset, while the larger MERT-330M model"
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": ""
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": "improves\nthese scores\nslightly to a PR-AUC of 0.14 and an"
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": ""
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": "ROC-AUC of 0.765. These results highlight\nthe potential of"
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": ""
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": "pre-trained encoders for emotion prediction tasks."
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": ""
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": "Beyond\nembeddings,\nhigh-level musical\nfeatures\nsuch\nas"
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": ""
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": "key signatures and chord progressions have also been shown"
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": ""
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": "to play a significant\nrole in emotion prediction [11].\nIn this"
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": ""
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": "study, we\naim to\nenhance MER by\nintegrating\nlarge-scale"
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": ""
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": "audio\nembeddings with\nthese\nsymbolic,\nhigh-level musical"
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": ""
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": "descriptors."
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": ""
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": "III. PROPOSED METHOD"
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": ""
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": "In this\nsection, we present our proposed unified multitask"
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": "learning framework for Music Emotion Recognition (MER),"
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": "which\ntrains\non\nboth\ncategorical\nand\ndimensional\nemotion"
        },
        {
          "Fig. 1. Overall architecture of our proposed framework.": "labels."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "is outlined as follows:"
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "1) The\naudio track is divided into fixed-length segments"
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "(i.e., 30 seconds each)."
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "2) MERT embeddings are obtained for each segment from"
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "the 5th and 6th layers."
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "3) These embeddings from both layers are then combined"
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "to create the feature vector\nfor each segment."
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "4) During training, we randomly select a series of consec-"
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "utive segments. The starting index and the number of"
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "segments are chosen uniformly at\nrandom (e.g., Seg ="
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "[2,3]\nin Figure 2)."
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "5) The\nfinal\nfeature\nvector\nis\nformed\nby\naveraging\nthe"
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "embeddings of\nthe selected segments."
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "The\nintuition behind this\naugmentation strategy is\nto ex-"
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "pose\nthe model\nto\nvarying\ntemporal\ncontexts\nof\nthe\nsame"
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "track\nacross\ntraining\nepochs. Emotion\nin music\ncan\nvary"
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "subtly\nthroughout\na\npiece—different\nsections might\nconvey"
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "slightly different moods due to changes in melody, harmony,"
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": ""
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "rhythm, or\ninstrumentation. By randomly sampling and aver-"
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "aging different\ncombinations of\nsegments,\nthe model\nlearns"
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "to generalize\nacross\nthese\nintra-track variations,\nrather\nthan"
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": ""
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "overfitting\nto\na\nsingle\nstatic\nrepresentation. This\nnot\nonly"
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": ""
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "improves\nrobustness\nto\nlocal\nvariations\nin musical\ncontent"
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": ""
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "but\nalso better\nreflects\nthe dynamic\nand evolving nature of"
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": ""
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "emotional expression in music."
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": ""
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "2) Musical Features\n(Chord\nand Key):\nUnlike\nthe\nseg-"
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": ""
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "mented and augmented MERT features, musical features (such"
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": ""
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "as chord progressions and key signatures) are calculated over"
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": ""
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "the whole song, representing global harmonic and tonal\ninfor-"
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": ""
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "mation. The framework integrates harmonic and tonal informa-"
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": ""
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "tion by extracting musical features such as chord progressions"
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": ""
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "and key signatures. A Transformer-based chord recognition"
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": ""
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "model [27] is used for extracting the chord progressions of the"
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": ""
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "song. The chord recognition model was evaluated on the fol-"
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": ""
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "lowing datasets: a subset of 221 songs from Isophonics1(171"
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": ""
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "songs by the Beatles, 12 songs by Carole King, 20 songs by"
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "Queen and 18 songs by Zweieck), Robbie Williams [14]\n(65"
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "songs by Robbie Williams), and a subset of 185 songs from"
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "UsPop20022. The chord recognition model\nis evaluated using"
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": ""
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "mir_eval metrics [42], achieving Weighted Chord Symbol"
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": ""
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "Recall\n(WCSR) scores of 83.5% for Root, 80.8% for Thirds,"
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": ""
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "75.9% for Triads,\n71.8% for Sevenths,\n65.5% for Tetrads,"
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": ""
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "82.3% for Maj-min,\nand 80.8% for\nthe MIREX categories."
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": ""
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "These scores are considered acceptable for this study, as most"
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": ""
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "errors are minor, such as misclassifying A minor as C major."
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": ""
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "In our model, each chord is encoded by its root (e.g., D) and"
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": ""
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "quality (e.g., sus4),\nrepresenting the chord type, and mapped"
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": ""
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "into an embedding space to capture harmonic patterns. These"
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": ""
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "chord progressions are then converted into MIDI\nrepresenta-"
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": ""
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "tions based on music theory. For\ninstance, a C major chord"
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": ""
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "7th\ncomprises\nthe\nnotes C, E, G, while\na C minor\nchord"
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": ""
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": "comprises\nthe notes C, E, G, B. The start and end times of"
        },
        {
          "pipeline as illustrated in Figure 2. The data augmentation step": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "each chord are mapped to define its duration in the MIDI file.": "This MIDI\nrepresentation serves as\ninput\nto a key detection",
          "The combined feature vector is projected into a latent space": "using a feedforward layer with 512 units and ReLU activation"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "model.",
          "The combined feature vector is projected into a latent space": "before being passed to task-specific branches for mood classi-"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "Key detection is performed using the music21 library [12].",
          "The combined feature vector is projected into a latent space": "fication and valence-arousal regression. The classification and"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "To\nensure\nconsistency\nin\nthe\nrepresentation\nof\nchords, we",
          "The combined feature vector is projected into a latent space": "regression branches\neach consist of\ntwo feedforward layers"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "‘normalize’\nthem based on the key. Extracted chords\nfrom",
          "The combined feature vector is projected into a latent space": "with 256 hidden units."
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "songs\nin major keys are transposed to C major, while those",
          "The combined feature vector is projected into a latent space": ""
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "",
          "The combined feature vector is projected into a latent space": "B. Classification with Multitask Learning"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "from minor keys\nare\ntransposed to A minor. The\nextracted",
          "The combined feature vector is projected into a latent space": ""
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "chord sequences have 13 different chord types\nsuch as\n‘ma-",
          "The combined feature vector is projected into a latent space": "In our proposed multitask learning framework, we have two"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "jor’,\n‘minor’,\n‘diminished’,\n‘augmented’,\n‘suspended’,\nand",
          "The combined feature vector is projected into a latent space": "different branches to handle both categorical and dimensional"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "‘seventh’\nchords. The\nkey\nis\nencoded\nbased\non\nits mode.",
          "The combined feature vector is projected into a latent space": "music\nemotion\nprediction\ntasks. We\nuse\na Binary Cross-"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "For\ninstance,\n‘major’\ncorresponds\nto C major\nand\n‘minor’",
          "The combined feature vector is projected into a latent space": "Entropy (BCE) loss function when training the network on the"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "corresponds\nto A minor. These harmonic\nfeatures—key (as",
          "The combined feature vector is projected into a latent space": "dataset with categorical\nlabels, such as “happy” or “sad”. To"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "a scalar) and chords (as sequences of embedded tokens)—are",
          "The combined feature vector is projected into a latent space": "address\nthe class\nimbalance issue,\nthe BCE loss\nis weighted"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "integrated with MERT features and used as input to the model.",
          "The combined feature vector is projected into a latent space": "based on the frequency of\nthe positive class\nfor each label,"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "3) Temporal Modeling\nof Chord Progressions:\nThe\nout-",
          "The combined feature vector is projected into a latent space": "which is defined as follows:"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "put of\nthe\nchord detection model gives us\na\nlong sequence",
          "The combined feature vector is projected into a latent space": ""
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "of\nchords. To\ncapture\nthe\ntemporal\ndependencies\nand\nrela-",
          "The combined feature vector is projected into a latent space": ""
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "tionships\nin\nthese\nharmonic\nprogressions, we model\nthese",
          "The combined feature vector is projected into a latent space": "1 c\nc(cid:88) i\n(wiyi log(xi) + ¯wi(1 − yi) log(1 − xi)) ,\nLCat(x, y) = −"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "sequences\nusing\na Transformer-based\nencoder\narchitecture,",
          "The combined feature vector is projected into a latent space": "=1"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "",
          "The combined feature vector is projected into a latent space": "(6)"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "(marked\nas\n‘Chord Transformer’\nin Figure\n1). Each\nchord",
          "The combined feature vector is projected into a latent space": ""
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "",
          "The combined feature vector is projected into a latent space": "where x is the predicted probabilities for each emotion cate-"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "in\nthe\nt-th\nposition\nof\nthe\nsequence\nis\nrepresented\nas\na",
          "The combined feature vector is projected into a latent space": ""
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "",
          "The combined feature vector is projected into a latent space": "gory, and y is the corresponding ground-truth binary label\n(1"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "concatenation of its root embedding, C(t)\nroot ∈ Rdroot, and quality",
          "The combined feature vector is projected into a latent space": ""
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "",
          "The combined feature vector is projected into a latent space": "for presence\nand 0 for\nabsence of\nthe\ncategory),\nc denotes"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "embedding, C(t)",
          "The combined feature vector is projected into a latent space": ""
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "quality ∈ Rdquality. The combined embedding for",
          "The combined feature vector is projected into a latent space": ""
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "",
          "The combined feature vector is projected into a latent space": "the\ntotal\nnumber\nof\nemotion\nlabels,\nensuring\nthat\nthe\nloss"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "the t-th chord is expressed as follows:",
          "The combined feature vector is projected into a latent space": ""
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "",
          "The combined feature vector is projected into a latent space": "is\naveraged\nacross\nall\ncategories,\nand\nthe\nadjusts\nterm wi"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "",
          "The combined feature vector is projected into a latent space": "the\ncontribution\nof\nthe\npositive\nclass, while\nscales\nthe\nwi"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "(1)\nroot ⊕ C(t)",
          "The combined feature vector is projected into a latent space": ""
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "quality,",
          "The combined feature vector is projected into a latent space": ""
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "",
          "The combined feature vector is projected into a latent space": "contribution of\nthe negative class, which can be defined as:"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "where ⊕ denotes vector\nconcatenation,\nresulting in C(t) ∈",
          "The combined feature vector is projected into a latent space": ""
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "",
          "The combined feature vector is projected into a latent space": "2\n2pi"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "Rdroot+dquality.",
          "The combined feature vector is projected into a latent space": ",\n,\n(7)\nwi =\nwi ="
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "",
          "The combined feature vector is projected into a latent space": "1 + pi\n1 + pi"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "To incorporate the sequential structure of the chord progres-",
          "The combined feature vector is projected into a latent space": ""
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "sion, a positional encoding P(t) ∈ Rdroot+dquality\nis added to the",
          "The combined feature vector is projected into a latent space": "is the frequency of\nthe positive class for\nlabel\ni.\nwhere pi"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "embeddings:",
          "The combined feature vector is projected into a latent space": "For dimensional\nlabels,\nthe model predicts continuous va-"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "C(t)",
          "The combined feature vector is projected into a latent space": ""
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "(2)\nenc = C(t) + P(t).",
          "The combined feature vector is projected into a latent space": "lence and arousal\n(VA) values using a Mean Squared Error"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "",
          "The combined feature vector is projected into a latent space": "(MSE)\nloss. The MSE loss is given by:"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "=\nThe\nencoded\nchord\nembeddings,\nCenc",
          "The combined feature vector is projected into a latent space": ""
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "[C(1)\nrepresent\nthe\nsequence\nof\nchords\nenc , C(2)\nenc , . . . , C(T )\nenc ],",
          "The combined feature vector is projected into a latent space": ""
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "",
          "The combined feature vector is projected into a latent space": "(8)\nLVA(x, y) = (yv − xv)2 + (ya − xa)2,"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "T\nfor\nthe\nsong, where\nis\nthe\ntotal\nnumber\nof\nchords\nin",
          "The combined feature vector is projected into a latent space": ""
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "the\nsequence. A special CLS token, CLS ∈ Rdroot+dquality,\nis",
          "The combined feature vector is projected into a latent space": "where\nand\nare\nthe\nground-truth\nvalence\nand\narousal\nyv\nya"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "prepended to this sequence to aggregate global\ninformation:",
          "The combined feature vector is projected into a latent space": "respective predictions.\nvalues, and xv and xa are their"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "",
          "The combined feature vector is projected into a latent space": "We use\na\nselective update\nstrategy to mitigate\ntask inter-"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "(3)\nCin = [CLS, C(1)\nenc , C(2)\nenc , . . . , C(T )\nenc ].",
          "The combined feature vector is projected into a latent space": ""
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "",
          "The combined feature vector is projected into a latent space": "ference in our multitask learning framework. For\nthe dataset"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "",
          "The combined feature vector is projected into a latent space": "with\ncategorical\nlabels\nsuch\nas MTG-Jamendo, we\nupdate"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "The input sequence is processed by a Transformer encoder",
          "The combined feature vector is projected into a latent space": ""
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "",
          "The combined feature vector is projected into a latent space": "only\nthe\ncategorical\nbranch’s\nparameter. Likewise,\nfor\nthe"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "with two layers and eight attention heads, generating output",
          "The combined feature vector is projected into a latent space": ""
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "",
          "The combined feature vector is projected into a latent space": "datasets with dimensional\nlabels\n(e.g., DEAM or PMEmo),"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "embeddings Cout, where the first token corresponds to the CLS",
          "The combined feature vector is projected into a latent space": ""
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "",
          "The combined feature vector is projected into a latent space": "we update only the parameters of the dimensional branch. By"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "representation:",
          "The combined feature vector is projected into a latent space": ""
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "",
          "The combined feature vector is projected into a latent space": "taking a selective update strategy,\nthe model can effectively"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "(4)\nCCLS-out = Transformer(Cin)[0].",
          "The combined feature vector is projected into a latent space": ""
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "",
          "The combined feature vector is projected into a latent space": "learn from heterogeneous datasets with preserved task-specific"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "",
          "The combined feature vector is projected into a latent space": "performance."
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "The\ntaken\nas\nthe\nfirst\nelement\nof\nthe\noutput CCLS-out,",
          "The combined feature vector is projected into a latent space": ""
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "Transformer output,\nserves as a global\nrepresentation of\nthe",
          "The combined feature vector is projected into a latent space": ""
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "",
          "The combined feature vector is projected into a latent space": "C. Knowledge Distillation"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "chord progression. Rather than merely reflecting its positional",
          "The combined feature vector is projected into a latent space": ""
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "",
          "The combined feature vector is projected into a latent space": "We use the knowledge distillation technique to efficiently"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "placement,\nthe CLS token\naggregates\ninformation\nfrom all",
          "The combined feature vector is projected into a latent space": ""
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "",
          "The combined feature vector is projected into a latent space": "train our multitask models by transferring knowledge\nfrom"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "chord embeddings via the self-attention mechanism.",
          "The combined feature vector is projected into a latent space": ""
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "",
          "The combined feature vector is projected into a latent space": "the pre-trained teacher models to the multitask student models"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "It\nis concatenated with the MERT embeddings,\nfMERT, and",
          "The combined feature vector is projected into a latent space": ""
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "",
          "The combined feature vector is projected into a latent space": "[21]. Separate teacher models are trained on categorical and"
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "key embeddings, K, to form the final combined feature vector:",
          "The combined feature vector is projected into a latent space": ""
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "",
          "The combined feature vector is projected into a latent space": "dimensional datasets, each optimized for\nits\nrespective task."
        },
        {
          "each chord are mapped to define its duration in the MIDI file.": "(5)\nffinal = CCLS-out ⊕ fMERT ⊕ K.",
          "The combined feature vector is projected into a latent space": "For\ninstance, a teacher model\ntrained on the MTG-Jamendo"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "teacher models\ntrained on DEAM, PMEmo, and EmoMusic",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "spanning diverse genres.\nIt has both dynamic and static"
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "generate soft\nlabels for dimensional predictions.",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "VA annotations. We use the static VA annotations."
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "The\nstudent model\nlearns\nfrom both hard labels\n(ground",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": ""
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "To\nensure\nconsistency, we\nfollow the\nofficial\ntrain,\nval-"
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "truth) and soft\nlogits\nthrough the Kullback-Leibler\n(KL) di-",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": ""
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "idation,\nand\ntest\nsplits\nfor\nthe MTG-Jamendo\ndataset. For"
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "vergence loss, which is defined as:",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": ""
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "the\nother\ndatasets which\ndo\nnot\nprovide\nofficial\ntraining,"
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "validation, and test splits, we randomly split\nthe data into 70%"
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "(cid:18) ti",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": ""
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "for\nthe training set, 15% for\nthe validation set, and 15% for"
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "c(cid:88) i\n,\n(9)\nti log\nLKD(s, t) =",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": ""
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "si",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": ""
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "=1",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "the test set. The details of the datasets used in our experiments"
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "are shown in Table I."
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "where s and t denote the predicted and teacher-generated soft",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": ""
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "labels, respectively, c is the number of output dimensions (i.e.,",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": ""
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "B.\nImplementation details"
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "emotion\ncategories\nfor\nclassification,\nor\n2\nfor\nvalence\nand",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": ""
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "arousal\nrepresent\nthe i-th elements of\nin regression), and si, ti",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": ""
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "To extract MERT features,\nthe audio tracks are segmented"
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "the student and teacher distributions,\nrespectively.",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": ""
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "into\n30-second\nclips. We\nleverage\nthe\npopular MERT-v1-"
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "95M model,\naccessible\nvia Hugging\nFace3,\nto\nobtain\nthe"
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "D. Total Loss Function",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": ""
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "embeddings. Knowledge distillation (KD) is employed, where"
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "The total\nloss function combines the task-specific loss with",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "teacher models pre-trained on each dataset guide the student"
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "the KL divergence\nloss\nderived\nfrom the\nteacher models.",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "model\nduring multitask\nlearning.\nThe\nteacher models\nare"
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "Specifically, for categorical and dimensional datasets,\nthe total",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "trained separately on each dataset using the same architecture"
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "loss functions are defined as:",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "as the student. For\nfeature extraction, we concatenate the 5th"
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "and 6th layer\nembeddings of MERT and process\nchord/key"
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "(10)\nLTotal,Cat = α · LCat + (1 − α) · LKD,Cat,",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "features with the Chord Transformer with positional encoding."
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "The hyperparameters for\nthe total\nloss function (as defined in"
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "Equations 10 and 11) are set\nto α = 0.2 and β = 0.2. During"
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "(11)\nLTotal,VA = β · LVA + (1 − β) · LKD,VA,",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": ""
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "training, only the relevant\nloss components are updated based"
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "on the dataset\ntype (categorical or dimensional). The student"
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "where α and β are hyperparameters that control\nthe balance",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": ""
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "and teacher models are trained for 200 epochs with a batch"
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "between the task-specific loss and the KL divergence loss for",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": ""
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "size of 8, a learning rate of 0.0001, and the Adam optimizer."
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "classification and regression,\nrespectively. LKD,Cat and LKD,VA",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": ""
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "All models,\nincluding both the\nstudent\nand teacher models,"
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "refer\nto the KL divergence\nlosses\ncomputed using the\nsoft",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": ""
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "are trained on a cluster of\nfour NVIDIA Tesla V100 DGXS"
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "targets generated by the classification and regression teacher",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": ""
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "GPUs, each with 32 GB of memory. The code is implemented"
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "models,\nthe final\nrespectively. LTotal,Cat and LTotal,VA represent",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": ""
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "in PyTorch and is available online4."
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "loss values used to update\nthe\nclassification and regression",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": ""
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "branches of\nthe multitask model,\nrespectively.",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": ""
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "C. Performance Metrics"
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "IV. EXPERIMENTAL SETUP",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": ""
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "Depending\non\nthe\nlabeling\nschemes\nof\ndatasets, we\nem-"
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "In this\nsection, we discuss\nthe\nexperimental\nsetup of\nthe",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "ploy\ndifferent metrics\nfor\nevaluating\nour\nproposed models."
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "proposed unified multitask learning framework.",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "For\ninstance, we use both Precision-Recall AUC (PR-AUC)"
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "and Receiver Operating Characteristic AUC (ROC-AUC)\nfor"
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "A. Dataset",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": ""
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "MTG-Jamendo dataset with categorical\nlabels. On the other"
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "In our\nexperiment, we use\nfour different datasets\nfor\nei-",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "the R2\n(R2"
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "V )"
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "ther categorical emotion recognition or dimensional emotion",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "(R2"
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "A)"
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "prediction tasks. For categorical emotion recognition, we use",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "1) PR-AUC\nand\nROC-AUC:\nPR-AUC\n[13]\nmeasures"
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "dataset\nthe MTG-Jamendo\n[4], which\nconsists\nof\n18,486",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "precision-recall trade-offs to assess classification performance."
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "full-length tracks\nlabeled with 56 mood/theme\ntags\nsuch as",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "This is especially useful for unbalanced datasets. On the other"
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "“happy” and “sad”.",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "hand, ROC-AUC [13] assesses how well a model performs by"
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "For dimensional emotion prediction, we use three different",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "analyzing true positive and false positive rates across various"
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "datasets with valence-arousal\n(VA)\nlabels which are normal-",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "decision thresholds."
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "ized within the range of 1 to 9:",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": ""
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "2) R2 Scores for Valence and Arousal: The R2 score [15]"
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "• DEAM dataset\n[3]: This dataset\ncontains 1,802 songs",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "quantifies how well the model explains variance in valence and"
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "from Western pop and rock.\nIt has both dynamic\nand",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "arousal predictions. Higher values\nindicate better predictive"
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "static VA annotations. We use the static VA annotations.",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "accuracy, with R2\nused for valence and R2"
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": "V\nA for arousal."
        },
        {
          "dataset\ngenerates\nsoft\nlabels\nfor\ncategorical\nlabels, while": "• PMEmo dataset [55]: This dataset consists of 794 songs",
          "• EmoMusic dataset [46]: This dataset contains 744 songs": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "# of validation data"
        },
        {
          "TABLE I": "3,802"
        },
        {
          "TABLE I": "271"
        },
        {
          "TABLE I": "116"
        },
        {
          "TABLE I": "124"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "NEURAL NETWORK."
        },
        {
          "TABLE II": "MTG-Jamendo"
        },
        {
          "TABLE II": "ROC-AUC"
        },
        {
          "TABLE II": "0.7590"
        },
        {
          "TABLE II": "0.6158"
        },
        {
          "TABLE II": "0.7806"
        },
        {
          "TABLE II": "TABLE III"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Training datasets": "",
          "MTG-Jamendo (J.)": "ROC-AUC",
          "DEAM (D.)": "R2\nA",
          "EmoMusic (E.)": "R2\nA",
          "PMEmo (P.)": "R2\nA"
        },
        {
          "Training datasets": "Single dataset\ntraining (X)",
          "MTG-Jamendo (J.)": "0.7806",
          "DEAM (D.)": "0.6025",
          "EmoMusic (E.)": "0.7489",
          "PMEmo (P.)": "0.7772"
        },
        {
          "Training datasets": "J + D",
          "MTG-Jamendo (J.)": "0.7806",
          "DEAM (D.)": "0.6046",
          "EmoMusic (E.)": "-",
          "PMEmo (P.)": "-"
        },
        {
          "Training datasets": "J + E",
          "MTG-Jamendo (J.)": "0.7809",
          "DEAM (D.)": "-",
          "EmoMusic (E.)": "0.7525",
          "PMEmo (P.)": "-"
        },
        {
          "Training datasets": "J + P",
          "MTG-Jamendo (J.)": "0.7806",
          "DEAM (D.)": "-",
          "EmoMusic (E.)": "-",
          "PMEmo (P.)": "0.7780"
        },
        {
          "Training datasets": "J + D + E + P",
          "MTG-Jamendo (J.)": "0.7810",
          "DEAM (D.)": "0.6228",
          "EmoMusic (E.)": "0.7616",
          "PMEmo (P.)": "0.7940"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "J + P": "J + D + E + P",
          "0.1522\n0.7806": "0.1543\n0.7810",
          "-\n-\n-\n-\n0.5401\n0.7780": "0.5184\n0.6228\n0.6512\n0.7616\n0.5473\n0.7940"
        },
        {
          "J + P": "TABLE IV",
          "0.1522\n0.7806": "",
          "-\n-\n-\n-\n0.5401\n0.7780": "[48], which explores\nsemi-supervised learning through noisy"
        },
        {
          "J + P": "COMPARISON OF OUR PROPOSED MODEL WITH EXISTING MODELS ON",
          "0.1522\n0.7806": "",
          "-\n-\n-\n-\n0.5401\n0.7780": ""
        },
        {
          "J + P": "",
          "0.1522\n0.7806": "",
          "-\n-\n-\n-\n0.5401\n0.7780": "student\ntraining\nand\nharmonic\npitch\nclass\nprofiles\n(HPCP);"
        },
        {
          "J + P": "MTG-JAMENDO DATASET.",
          "0.1522\n0.7806": "",
          "-\n-\n-\n-\n0.5401\n0.7780": ""
        },
        {
          "J + P": "",
          "0.1522\n0.7806": "",
          "-\n-\n-\n-\n0.5401\n0.7780": "4) UIBK-DBIS [34], an ensemble-based model\nthat clusters"
        },
        {
          "J + P": "",
          "0.1522\n0.7806": "",
          "-\n-\n-\n-\n0.5401\n0.7780": "et\nemotion labels\nto train specialized classifiers; 5) Hasumi"
        },
        {
          "J + P": "Model\nPR-AUC",
          "0.1522\n0.7806": "ROC-AUC",
          "-\n-\n-\n-\n0.5401\n0.7780": ""
        },
        {
          "J + P": "",
          "0.1522\n0.7806": "",
          "-\n-\n-\n-\n0.5401\n0.7780": "al. [19], which proposes classifier group chains to capture tag"
        },
        {
          "J + P": "lileonardo [5]\n0.1508",
          "0.1522\n0.7806": "0.7747",
          "-\n-\n-\n-\n0.5401\n0.7780": ""
        },
        {
          "J + P": "",
          "0.1522\n0.7806": "",
          "-\n-\n-\n-\n0.5401\n0.7780": "et\nal.\ndependencies\nfor\nimproved music\ntagging; 6) Greer"
        },
        {
          "J + P": "SELAB-HCMUS [40]\n0.1435",
          "0.1522\n0.7806": "0.7599",
          "-\n-\n-\n-\n0.5401\n0.7780": ""
        },
        {
          "J + P": "",
          "0.1522\n0.7806": "",
          "-\n-\n-\n-\n0.5401\n0.7780": "[18], who introduce M3BERT,\na multi-task,\nself-supervised"
        },
        {
          "J + P": "Mirable [48]\n0.1356",
          "0.1522\n0.7806": "0.7687",
          "-\n-\n-\n-\n0.5401\n0.7780": ""
        },
        {
          "J + P": "UIBK-DBIS [34]\n0.1087",
          "0.1522\n0.7806": "0.7046",
          "-\n-\n-\n-\n0.5401\n0.7780": "transformer\ntrained via masked reconstruction and fine-tuned"
        },
        {
          "J + P": "Hasumi et al.\n[19]\n0.0730",
          "0.1522\n0.7806": "0.7750",
          "-\n-\n-\n-\n0.5401\n0.7780": "for\nemotion\nrecognition;\nand\n7) MERT [29],\na\nlarge-scale"
        },
        {
          "J + P": "Greer et al.\n[18]\n0.1082",
          "0.1522\n0.7806": "0.7354",
          "-\n-\n-\n-\n0.5401\n0.7780": ""
        },
        {
          "J + P": "",
          "0.1522\n0.7806": "",
          "-\n-\n-\n-\n0.5401\n0.7780": "acoustic music\nunderstanding model\n(with\n95M and\n330M"
        },
        {
          "J + P": "MERT-95M [54]\n0.1340",
          "0.1522\n0.7806": "0.7640",
          "-\n-\n-\n-\n0.5401\n0.7780": ""
        },
        {
          "J + P": "",
          "0.1522\n0.7806": "",
          "-\n-\n-\n-\n0.5401\n0.7780": "parameters) pre-trained using pseudo labels from acoustic and"
        },
        {
          "J + P": "MERT-330M [54]\n0.1400",
          "0.1522\n0.7806": "0.7650",
          "-\n-\n-\n-\n0.5401\n0.7780": ""
        },
        {
          "J + P": "",
          "0.1522\n0.7806": "",
          "-\n-\n-\n-\n0.5401\n0.7780": "musical\nteachers,\ndemonstrating\nstrong\nperformance\nacross"
        },
        {
          "J + P": "0.1543\nProposed (Ours)",
          "0.1522\n0.7806": "0.7810",
          "-\n-\n-\n-\n0.5401\n0.7780": ""
        },
        {
          "J + P": "",
          "0.1522\n0.7806": "",
          "-\n-\n-\n-\n0.5401\n0.7780": "several music understanding tasks."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "musical\nteachers,\ndemonstrating\nstrong\nperformance\nacross": ""
        },
        {
          "musical\nteachers,\ndemonstrating\nstrong\nperformance\nacross": "several music understanding tasks."
        },
        {
          "musical\nteachers,\ndemonstrating\nstrong\nperformance\nacross": "V. RESULTS"
        },
        {
          "musical\nteachers,\ndemonstrating\nstrong\nperformance\nacross": ""
        },
        {
          "musical\nteachers,\ndemonstrating\nstrong\nperformance\nacross": "In a first\nexperiment, we\nremoved the high-level musical"
        },
        {
          "musical\nteachers,\ndemonstrating\nstrong\nperformance\nacross": "features (i.e., key and chords)\nfrom our proposed framework"
        },
        {
          "musical\nteachers,\ndemonstrating\nstrong\nperformance\nacross": "and trained the network using the MERT features only. The"
        },
        {
          "musical\nteachers,\ndemonstrating\nstrong\nperformance\nacross": "performance of\nthe models with these different\ninput\nfeature"
        },
        {
          "musical\nteachers,\ndemonstrating\nstrong\nperformance\nacross": "configurations is shown in Table II. As can be seen from the"
        },
        {
          "musical\nteachers,\ndemonstrating\nstrong\nperformance\nacross": "table,\nincorporating both the MERT and high-level musical"
        },
        {
          "musical\nteachers,\ndemonstrating\nstrong\nperformance\nacross": "features significantly enhanced the performance of our model."
        },
        {
          "musical\nteachers,\ndemonstrating\nstrong\nperformance\nacross": "The model with high-level musical features includes the Chord"
        },
        {
          "musical\nteachers,\ndemonstrating\nstrong\nperformance\nacross": "Transformer model. This achieves the best results,\nincluding a"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": ""
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": "increases when adding a\nsecond dataset. However,\nthe best"
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": "performance is reached when training on all datasets (MTG-"
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": ""
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": "Jamendo + DEAM + EmoMusic + PMEmo), with a PR-AUC"
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": ""
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": "of 0.1543 and an ROC-AUC of 0.7810 on the MTG-Jamendo"
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": ""
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": "dataset,\nas well\nas\nthe best R2\nand R2\nacross\nthe\nV\nA scores"
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": "dimensional datasets. These\nresults\nshow the\nimportance of"
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": ""
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": "leveraging diverse datasets in a unified multitask framework."
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": ""
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": "Lastly, we\nevaluated\nour\nproposed model\non\nthe MTG-"
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": "Jamendo\ndataset\nusing\nits\nofficial\ntrain-validation-test\nsplit,"
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": ""
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": "which enables direct comparison with prior work. As shown"
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": ""
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": "in Table IV, our model achieves strong performance compared"
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": "to\nboth\nearlier\nbaselines\nsuch\nas\nlileonardo\n[5]\nfrom the"
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": ""
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": "MediaEval\n2021\ncompetition\n[50],\nas well\nas more\nrecent"
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": ""
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": "representative methods\n[18],\n[19],\n[54], which reflect current"
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": ""
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": "directions\nin MER modeling. The\nsubstantial\nimprovement"
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": ""
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": "over\nthese models\nsuggests\nthat our proposed framework is"
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": ""
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": "a promising approach for categorical emotion recognition in"
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": ""
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": "music.\nIn contrast, other MER datasets do not offer official"
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": ""
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": "splits, which makes direct performance comparison less con-"
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": ""
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": "sistent across studies."
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": ""
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": "These results confirm our proposed framework as a tool\nto"
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": ""
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": "unify categorical and dimensional emotion recognition without"
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": ""
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": "sacrificing\nstate-of-the-art\nperformance.\nThe\nintegration\nof"
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": ""
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": "MERT embeddings, musical\nfeatures, multitask learning, and"
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": ""
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": "knowledge distillation proves to be a highly effective method"
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": ""
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": "that enhances music emotion recognition on diverse datasets"
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": ""
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": "and labeling schemes."
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": ""
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": "VI. CONCLUSION"
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": ""
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": "In this study, we propose a unified multitask learning frame-"
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": "work for Music Emotion Recognition (MER)\nthat\nfacilitates"
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": ""
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": "training\non\ndatasets with\nboth\ncategorical\nand\ndimensional"
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": ""
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": "emotion labels. Our proposed architecture incorporates knowl-"
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": "edge distillation and takes both high-level musical\nfeatures"
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": ""
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": "such as chords and key signatures, as well as pre-trained em-"
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": ""
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": "beddings from MERT as input,\nthus enabling it\nto effectively"
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": "capture emotional nuances in music. The experimental results"
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": ""
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": "demonstrate\nthat our\nframework outperforms\nstate-of-the-art"
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": ""
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": "models on the MTG-Jamendo dataset,\nincluding the winners"
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": ""
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": "of\nthe MediaEval 2021 competition. Our best model achieves"
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": ""
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": "a\nPR-AUC of\n0.1543\nand\nan ROC-AUC of\n0.7810. This"
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": ""
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": "model\nis made available open-source online. Our results show"
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": ""
        },
        {
          "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly": "that\nour multitask\nlearning\napproach\nenables\ngeneralization"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "Jamendo dataset. Furthermore,\nthe framework achieves higher",
          "across diverse datasets, while knowledge distillation facilitates": "efficient knowledge transfer\nfrom teacher\nto student models."
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "R2\nand R2",
          "across diverse datasets, while knowledge distillation facilitates": ""
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "V\nA scores for valence and arousal on the dimensional",
          "across diverse datasets, while knowledge distillation facilitates": "In summary, our work provides a robust solution for training"
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "datasets (DEAM, EmoMusic, and PMEmo), demonstrating the",
          "across diverse datasets, while knowledge distillation facilitates": "on multiple datasets with different\ntypes of emotion labels. In"
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "importance of\nincluding the musical\nfeatures.",
          "across diverse datasets, while knowledge distillation facilitates": "future research, we may explore the addition of different mu-"
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "In a second experiment, we explore how training the net-",
          "across diverse datasets, while knowledge distillation facilitates": "sical\nfeatures as input\nto the model,\nrefine data augmentation"
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "work on heterogeneous datasets can improve performance on",
          "across diverse datasets, while knowledge distillation facilitates": "techniques, and expand the framework’s applicability to other"
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "single datasets. Table III shows the comparison of performance",
          "across diverse datasets, while knowledge distillation facilitates": "affective computing domains."
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "metrics when training on multiple datasets. We\ntrained the",
          "across diverse datasets, while knowledge distillation facilitates": ""
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "network\non multiple\ndatasets\nas\nindicated\nin\nthe\nleftmost",
          "across diverse datasets, while knowledge distillation facilitates": "ACKNOWLEDGMENT"
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "column. When comparing our\nresults without data fusion to",
          "across diverse datasets, while knowledge distillation facilitates": ""
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "",
          "across diverse datasets, while knowledge distillation facilitates": "This work\nhas\nreceived\nfunding\nfrom grant\nno.\nSUTD"
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "the\nresults\nfor\nour model\ntrained\non\neach\nof\nthe\ndatasets",
          "across diverse datasets, while knowledge distillation facilitates": ""
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "",
          "across diverse datasets, while knowledge distillation facilitates": "06. We acknowledge the use of ChatGPT for\nSKI 2021 04"
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "separately\n(row 1), we\nnotice\nthat\nthe\nperformance\nclearly",
          "across diverse datasets, while knowledge distillation facilitates": ""
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "",
          "across diverse datasets, while knowledge distillation facilitates": "grammar\nrefinement."
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "increases when adding a\nsecond dataset. However,\nthe best",
          "across diverse datasets, while knowledge distillation facilitates": ""
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "performance is reached when training on all datasets (MTG-",
          "across diverse datasets, while knowledge distillation facilitates": ""
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "",
          "across diverse datasets, while knowledge distillation facilitates": "REFERENCES"
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "Jamendo + DEAM + EmoMusic + PMEmo), with a PR-AUC",
          "across diverse datasets, while knowledge distillation facilitates": ""
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "",
          "across diverse datasets, while knowledge distillation facilitates": "[1] K. R. Agres, R. S. Schaefer, A. Volk, S. van Hooren, A. Holzapfel,"
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "of 0.1543 and an ROC-AUC of 0.7810 on the MTG-Jamendo",
          "across diverse datasets, while knowledge distillation facilitates": ""
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "",
          "across diverse datasets, while knowledge distillation facilitates": "S. Dalla Bella, M. M¨uller, M. De Witte, D. Herremans, R. Ramirez Me-"
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "dataset,\nas well\nas\nthe best R2\nand R2\nacross\nthe\nV\nA scores",
          "across diverse datasets, while knowledge distillation facilitates": "lendez, et al. Music, computing, and health: a roadmap for\nthe current"
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "dimensional datasets. These\nresults\nshow the\nimportance of",
          "across diverse datasets, while knowledge distillation facilitates": "and future\nroles of music\ntechnology for health care\nand well-being."
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "",
          "across diverse datasets, while knowledge distillation facilitates": "Music & Science, 4:2059204321997709, 2021."
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "leveraging diverse datasets in a unified multitask framework.",
          "across diverse datasets, while knowledge distillation facilitates": ""
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "",
          "across diverse datasets, while knowledge distillation facilitates": "[2] A. Aljanaki, F. Wiering, and R. C. Veltkamp. Studying emotion induced"
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "Lastly, we\nevaluated\nour\nproposed model\non\nthe MTG-",
          "across diverse datasets, while knowledge distillation facilitates": "Information Processing &\nby music\nthrough a\ncrowdsourcing game."
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "Jamendo\ndataset\nusing\nits\nofficial\ntrain-validation-test\nsplit,",
          "across diverse datasets, while knowledge distillation facilitates": "Management, 52(1):115–128, 2016."
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "",
          "across diverse datasets, while knowledge distillation facilitates": "[3] A. Aljanaki, Y.-H. Yang, and M. Soleymani. Developing a benchmark"
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "which enables direct comparison with prior work. As shown",
          "across diverse datasets, while knowledge distillation facilitates": ""
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "",
          "across diverse datasets, while knowledge distillation facilitates": "for emotional analysis of music. PloS one, 12(3):e0173392, 2017."
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "in Table IV, our model achieves strong performance compared",
          "across diverse datasets, while knowledge distillation facilitates": "[4] D. Bogdanov, M. Won, P. Tovstogan, A. Porter,\nand X. Serra.\nThe"
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "to\nboth\nearlier\nbaselines\nsuch\nas\nlileonardo\n[5]\nfrom the",
          "across diverse datasets, while knowledge distillation facilitates": "mtg-jamendo dataset\nfor automatic music tagging.\nICML, 2019."
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "",
          "across diverse datasets, while knowledge distillation facilitates": "[5] V. Bour.\nFrequency\ndependent\nconvolutions\nfor music\ntagging.\nIn"
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "MediaEval\n2021\ncompetition\n[50],\nas well\nas more\nrecent",
          "across diverse datasets, while knowledge distillation facilitates": ""
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "",
          "across diverse datasets, while knowledge distillation facilitates": "MediaEval, 2021."
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "representative methods\n[18],\n[19],\n[54], which reflect current",
          "across diverse datasets, while knowledge distillation facilitates": ""
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "",
          "across diverse datasets, while knowledge distillation facilitates": "[6] X. Cai, J. Yuan, R. Zheng, L. Huang, and K. Church. Speech emotion"
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "directions\nin MER modeling. The\nsubstantial\nimprovement",
          "across diverse datasets, while knowledge distillation facilitates": "recognition with multi-task learning. In Interspeech, volume 2021, pages"
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "",
          "across diverse datasets, while knowledge distillation facilitates": "4508–4512. Brno, 2021."
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "over\nthese models\nsuggests\nthat our proposed framework is",
          "across diverse datasets, while knowledge distillation facilitates": ""
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "",
          "across diverse datasets, while knowledge distillation facilitates": "[7]\nE. C¸ ano and M. Morisio. Moodylyrics: A sentiment annotated lyrics"
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "a promising approach for categorical emotion recognition in",
          "across diverse datasets, while knowledge distillation facilitates": ""
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "",
          "across diverse datasets, while knowledge distillation facilitates": "Proceedings\nof\nthe\n2017\ninternational\nconference\non\ndataset.\nIn"
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "music.\nIn contrast, other MER datasets do not offer official",
          "across diverse datasets, while knowledge distillation facilitates": "intelligent\nsystems, metaheuristics & swarm intelligence, pages 118–"
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "",
          "across diverse datasets, while knowledge distillation facilitates": "124, 2017."
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "splits, which makes direct performance comparison less con-",
          "across diverse datasets, while knowledge distillation facilitates": ""
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "",
          "across diverse datasets, while knowledge distillation facilitates": "[8] R. Caruana. Multitask learning. Machine learning, 28:41–75, 1997."
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "sistent across studies.",
          "across diverse datasets, while knowledge distillation facilitates": ""
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "",
          "across diverse datasets, while knowledge distillation facilitates": "[9]\nS. Chaki, P. Doshi, P. Patnaik, and S. Bhattacharya. Attentive rnns for"
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "These results confirm our proposed framework as a tool\nto",
          "across diverse datasets, while knowledge distillation facilitates": "continuous-time emotion prediction in music clips.\nIn AffCon@ AAAI,"
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "",
          "across diverse datasets, while knowledge distillation facilitates": "pages 36–46, 2020."
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "unify categorical and dimensional emotion recognition without",
          "across diverse datasets, while knowledge distillation facilitates": ""
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "",
          "across diverse datasets, while knowledge distillation facilitates": "[10] C. Chen and Q. Li. A multimodal music emotion classification method"
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "sacrificing\nstate-of-the-art\nperformance.\nThe\nintegration\nof",
          "across diverse datasets, while knowledge distillation facilitates": ""
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "",
          "across diverse datasets, while knowledge distillation facilitates": "Mathematical\nbased\non multifeature\ncombined\nnetwork\nclassifier."
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "MERT embeddings, musical\nfeatures, multitask learning, and",
          "across diverse datasets, while knowledge distillation facilitates": "Problems in Engineering, 2020(1):4606027, 2020."
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "",
          "across diverse datasets, while knowledge distillation facilitates": "[11] Y.-H. Cho, H.\nLim, D.-W. Kim,\nand\nI.-K.\nLee.\nMusic\nemotion"
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "knowledge distillation proves to be a highly effective method",
          "across diverse datasets, while knowledge distillation facilitates": ""
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "",
          "across diverse datasets, while knowledge distillation facilitates": "2016\nIEEE Int. Conf.\non\nrecognition\nusing\nchord\nprogressions.\nIn"
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "that enhances music emotion recognition on diverse datasets",
          "across diverse datasets, while knowledge distillation facilitates": ""
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "",
          "across diverse datasets, while knowledge distillation facilitates": "Systems, Man, and Cybernetics\n(SMC), pages 002588–002593.\nIEEE,"
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "and labeling schemes.",
          "across diverse datasets, while knowledge distillation facilitates": "2016."
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "",
          "across diverse datasets, while knowledge distillation facilitates": "[12] M. S. Cuthbert and C. Ariza. music21: A toolkit\nfor computer-aided"
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "VI. CONCLUSION",
          "across diverse datasets, while knowledge distillation facilitates": "musicology and symbolic music data. 2010."
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "",
          "across diverse datasets, while knowledge distillation facilitates": "[13]\nJ. Davis and M. Goadrich. The relationship between precision-recall and"
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "In this study, we propose a unified multitask learning frame-",
          "across diverse datasets, while knowledge distillation facilitates": "roc curves.\nIn Proc. of\nthe 23rd Int. conference on Machine learning,"
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "work for Music Emotion Recognition (MER)\nthat\nfacilitates",
          "across diverse datasets, while knowledge distillation facilitates": "pages 233–240, 2006."
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "",
          "across diverse datasets, while knowledge distillation facilitates": "[14] B. Di Giorgi, M. Zanoni, A. Sarti,\nand S. Tubaro.\nAutomatic\nchord"
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "training\non\ndatasets with\nboth\ncategorical\nand\ndimensional",
          "across diverse datasets, while knowledge distillation facilitates": ""
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "",
          "across diverse datasets, while knowledge distillation facilitates": "recognition\nbased\non\nthe\nprobabilistic modeling\nof\ndiatonic modal"
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "emotion labels. Our proposed architecture incorporates knowl-",
          "across diverse datasets, while knowledge distillation facilitates": "the 8th International Workshop\nharmony.\nIn nDS’13; Proceedings of"
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "edge distillation and takes both high-level musical\nfeatures",
          "across diverse datasets, while knowledge distillation facilitates": "on Multidimensional Systems, pages 1–6. VDE, 2013."
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "",
          "across diverse datasets, while knowledge distillation facilitates": "[15] N. Draper. Applied regression analysis. McGraw-Hill.\nInc, 1998."
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "such as chords and key signatures, as well as pre-trained em-",
          "across diverse datasets, while knowledge distillation facilitates": ""
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "",
          "across diverse datasets, while knowledge distillation facilitates": "[16] B. Elizalde, S. Deshmukh, M. Al\nIsmail, and H. Wang. Clap learning"
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "beddings from MERT as input,\nthus enabling it\nto effectively",
          "across diverse datasets, while knowledge distillation facilitates": "audio concepts from natural language supervision. In ICASSP 2023-2023"
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "capture emotional nuances in music. The experimental results",
          "across diverse datasets, while knowledge distillation facilitates": "IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP),"
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "",
          "across diverse datasets, while knowledge distillation facilitates": "pages 1–5.\nIEEE, 2023."
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "demonstrate\nthat our\nframework outperforms\nstate-of-the-art",
          "across diverse datasets, while knowledge distillation facilitates": ""
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "",
          "across diverse datasets, while knowledge distillation facilitates": "[17]\nL. N. Ferreira\nand\nJ. Whitehead.\nLearning\nto\ngenerate music with"
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "models on the MTG-Jamendo dataset,\nincluding the winners",
          "across diverse datasets, while knowledge distillation facilitates": ""
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "",
          "across diverse datasets, while knowledge distillation facilitates": "sentiment. arXiv:2103.06125, 2021."
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "of\nthe MediaEval 2021 competition. Our best model achieves",
          "across diverse datasets, while knowledge distillation facilitates": "[18]\nT. Greer, X. Shi, B. Ma, and S. Narayanan. Creating musical features us-"
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "",
          "across diverse datasets, while knowledge distillation facilitates": "ing multi-faceted, multi-task encoders based on transformers. Scientific"
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "a\nPR-AUC of\n0.1543\nand\nan ROC-AUC of\n0.7810. This",
          "across diverse datasets, while knowledge distillation facilitates": ""
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "",
          "across diverse datasets, while knowledge distillation facilitates": "Reports, 13(1):10713, 2023."
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "model\nis made available open-source online. Our results show",
          "across diverse datasets, while knowledge distillation facilitates": ""
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "",
          "across diverse datasets, while knowledge distillation facilitates": "[19]\nT. Hasumi, T. Komatsu, and Y. Fujita. Music tagging with classifier"
        },
        {
          "PR-AUC of 0.1521 and an ROC-AUC of 0.7806 on the MTG-": "that\nour multitask\nlearning\napproach\nenables\ngeneralization",
          "across diverse datasets, while knowledge distillation facilitates": "group chains. arXiv:2501.05050, 2025."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "Moore, M. Plakal, D. Platt, R. A. Saurous, B. Seybold,\net\nal.\nCnn",
          "on Computing for Sustainable Global Development (INDIACom), pages": "1188–1195.\nIEEE, 2024."
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "2017\nieee\nInt.\narchitectures\nfor\nlarge-scale\naudio\nclassification.\nIn",
          "on Computing for Sustainable Global Development (INDIACom), pages": "[46] M. Soleymani, M. N. Caro, E. M. Schmidt, C.-Y. Sha, and Y.-H. Yang."
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "conference on acoustics,\nspeech and signal processing (icassp), pages",
          "on Computing for Sustainable Global Development (INDIACom), pages": "the 2nd ACM\n1000 songs for emotional analysis of music.\nIn Proc. of"
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "131–135.\nIEEE, 2017.",
          "on Computing for Sustainable Global Development (INDIACom), pages": "Int. workshop on Crowdsourcing for multimedia, pages 1–6, 2013."
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "[21] G.\nHinton.\nDistilling\nthe\nknowledge\nin\na\nneural\nnetwork.",
          "on Computing for Sustainable Global Development (INDIACom), pages": "[47]\nS. A. Suresh Kumar and R. Rajan. Transformer-based automatic music"
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "arXiv:1503.02531, 2015.",
          "on Computing for Sustainable Global Development (INDIACom), pages": "mood classification using multi-modal framework. Journal of Computer"
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "[22]\nS. Hizlisoy, S. Yildirim,\nand Z. Tufekci. Music\nemotion recognition",
          "on Computing for Sustainable Global Development (INDIACom), pages": "Science & Technology, 23, 2023."
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "using convolutional long short term memory deep neural networks. Eng.",
          "on Computing for Sustainable Global Development (INDIACom), pages": "[48] H. H. Tan.\nSemi-supervised music\nemotion recognition using noisy"
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "Sci. Technol.\nInt J., 24(3):760–767, 2021.",
          "on Computing for Sustainable Global Development (INDIACom), pages": "student\ntraining and harmonic pitch class profiles.\narXiv:2112.00702,"
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "[23]\nZ. Huang, S.\nJi, Z. Hu, C. Cai,\nJ. Luo, and X. Yang. Adff: Attention",
          "on Computing for Sustainable Global Development (INDIACom), pages": "2021."
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "based\ndeep\nfeature\nfusion\napproach\nfor music\nemotion\nrecognition.",
          "on Computing for Sustainable Global Development (INDIACom), pages": "[49] G. Tong. Multimodal music emotion recognition method based on the"
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "arXiv:2204.05649, 2022.",
          "on Computing for Sustainable Global Development (INDIACom), pages": "Scientific\ncombination of knowledge distillation and transfer\nlearning."
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "[24] H.-T. Hung, J. Ching, S. Doh, N. Kim, J. Nam, and Y.-H. Yang. Emopia:",
          "on Computing for Sustainable Global Development (INDIACom), pages": "Programming, 2022(1):2802573, 2022."
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "A multi-modal pop piano dataset\nfor emotion recognition and emotion-",
          "on Computing for Sustainable Global Development (INDIACom), pages": "[50]\nP. Tovstogan, D. Bogdanov, and A. Porter. Mediaeval 2021: Emotion"
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "based music generation. arXiv:2108.01374, 2021.",
          "on Computing for Sustainable Global Development (INDIACom), pages": "and theme recognition in music using jamendo.\nIn MediaEval, 2021."
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "[25]\nE. Jeong, G. Oh, and S. Lim. Multitask emotion recognition model with",
          "on Computing for Sustainable Global Development (INDIACom), pages": "[51] H. Tran, T. Le, A. Do, T. Vu, S. Bogaerts, and B. Howard. Emotion-"
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "knowledge distillation and task discriminator. arXiv:2203.13072, 2022.",
          "on Computing for Sustainable Global Development (INDIACom), pages": "the AAAI Conf. on Artificial\naware music recommendation.\nIn Proc. of"
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "[26] X.\nJia. A music emotion classification model based on the improved",
          "on Computing for Sustainable Global Development (INDIACom), pages": "Intelligence, volume 37, pages 16087–16095, 2023."
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "Computational\nIntelligence and Neuro-\nconvolutional neural network.",
          "on Computing for Sustainable Global Development (INDIACom), pages": "[52] D. Turnbull, L. Barrington, D. Torres, and G. Lanckriet. Towards musical"
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "science, 2022(1):6749622, 2022.",
          "on Computing for Sustainable Global Development (INDIACom), pages": "the\nquery-by-semantic-description using the cal500 data set.\nIn Proc. of"
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "[27]\nP. Jonggwon, C. Kyoyun, J. Sungwook, K. Dokyun, and P. Jonghun. A",
          "on Computing for Sustainable Global Development (INDIACom), pages": "30th annual\nInt. ACM SIGIR Conf. on Research and development\nin"
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "bi-directional\ntransformer\nfor musical chord recognition.\nIn 20th Inter-",
          "on Computing for Sustainable Global Development (INDIACom), pages": "information retrieval, pages 439–446, 2007."
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "national Society for Music Information Retrieval Conference (ISMIR),",
          "on Computing for Sustainable Global Development (INDIACom), pages": "[53]\nS.-Y. Wang, J.-C. Wang, Y.-H. Yang, and H.-M. Wang. Towards time-"
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "Delft, The Netherlands, 2019.",
          "on Computing for Sustainable Global Development (INDIACom), pages": "varying music auto-tagging based on cal500 expansion.\nIn 2014 IEEE"
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "[28]\nJ. Kang\nand D. Herremans.\nAre we\nthere\nyet?\na\nbrief\nsurvey\nof",
          "on Computing for Sustainable Global Development (INDIACom), pages": "Int. Conf. on Multimedia and Expo (ICME), pages 1–6.\nIEEE, 2014."
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "music emotion prediction datasets, models and outstanding challenges.",
          "on Computing for Sustainable Global Development (INDIACom), pages": "[54]\nL. Yizhi, R. Yuan, G. Zhang, Y. Ma, X. Chen, H. Yin, C. Xiao, C. Lin,"
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "arXiv:2406.08809, 2024.",
          "on Computing for Sustainable Global Development (INDIACom), pages": "A. Ragni, E. Benetos, et al. Mert: Acoustic music understanding model"
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "[29] Y. Li, R. Yuan, G. Zhang, Y. Ma, X. Chen, H. Yin, C. Xiao, C. Lin,",
          "on Computing for Sustainable Global Development (INDIACom), pages": "with large-scale self-supervised training.\nIn The Twelfth International"
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "A. Ragni, E. Benetos, et al. Mert: Acoustic music understanding model",
          "on Computing for Sustainable Global Development (INDIACom), pages": "Conference on Learning Representations, 2023."
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "with large-scale self-supervised training. arXiv:2306.00107, 2023.",
          "on Computing for Sustainable Global Development (INDIACom), pages": "[55] K. Zhang, H. Zhang, S. Li, C. Yang, and L. Sun. The pmemo dataset"
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "[30] R. Liu, A. Roy,\nand D. Herremans.\nLeveraging llm embeddings\nfor",
          "on Computing for Sustainable Global Development (INDIACom), pages": "the 2018 acm on Int. Conf.\nfor music emotion recognition.\nIn Proc. of"
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "cross dataset\nlabel alignment and zero shot music emotion prediction.",
          "on Computing for Sustainable Global Development (INDIACom), pages": "on multimedia retrieval, pages 135–142, 2018."
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "arXiv:2410.11522, 2024.",
          "on Computing for Sustainable Global Development (INDIACom), pages": ""
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "[31] X. Liu, Q. Chen, X. Wu, Y. Liu, and Y. Liu. Cnn based music emotion",
          "on Computing for Sustainable Global Development (INDIACom), pages": ""
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "classification. arXiv:1704.05665, 2017.",
          "on Computing for Sustainable Global Development (INDIACom), pages": ""
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "[32] D. Makris, K. R. Agres, and D. Herremans. Generating lead sheets with",
          "on Computing for Sustainable Global Development (INDIACom), pages": ""
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "affect: A novel conditional seq2seq framework.\nIn 2021 Int. Joint Conf.",
          "on Computing for Sustainable Global Development (INDIACom), pages": ""
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "on Neural Networks (IJCNN), pages 1–8.\nIEEE, 2021.",
          "on Computing for Sustainable Global Development (INDIACom), pages": ""
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "[33] R. Malheiro, R. Panda, P. J. Gomes, and R. P. Paiva. Bi-modal music",
          "on Computing for Sustainable Global Development (INDIACom), pages": ""
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "9th\nInt.\nemotion\nrecognition: Novel\nlyrical\nfeatures\nand\ndataset.\nIn",
          "on Computing for Sustainable Global Development (INDIACom), pages": ""
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "Workshop on Music and Machine Learning–MML, 2016.",
          "on Computing for Sustainable Global Development (INDIACom), pages": ""
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "[34] M. Mayerl, M. V¨otter, A.\nPeintner, G.\nSpecht,\nand\nE.\nZangerle.",
          "on Computing for Sustainable Global Development (INDIACom), pages": ""
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "Recognizing song mood and theme: Clustering-based ensembles.\nIn",
          "on Computing for Sustainable Global Development (INDIACom), pages": ""
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "MediaEval, 2021.",
          "on Computing for Sustainable Global Development (INDIACom), pages": ""
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "[35] G. Mazzetta, A. Greco, M. Tagliasacchi,\nand A. Pescap`e.\nA multi-",
          "on Computing for Sustainable Global Development (INDIACom), pages": ""
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "source deep learning model\nfor music emotion recognition.\nIn Proc. of",
          "on Computing for Sustainable Global Development (INDIACom), pages": ""
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "the AIxHMI Workshop at CEUR-WS, volume 3903, pages 1–8. CEUR",
          "on Computing for Sustainable Global Development (INDIACom), pages": ""
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "Workshop Proc., 2024.",
          "on Computing for Sustainable Global Development (INDIACom), pages": ""
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "[36]\nL. B. Meyer. Emotion and meaning in music. PhD thesis, The University",
          "on Computing for Sustainable Global Development (INDIACom), pages": ""
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "of Chicago, 1954.",
          "on Computing for Sustainable Global Development (INDIACom), pages": ""
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "[37]\nJ. Ong and D. Herremans. Constructing time-series momentum portfo-",
          "on Computing for Sustainable Global Development (INDIACom), pages": ""
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "lios with deep multi-task learning.\nExpert Systems with Applications,",
          "on Computing for Sustainable Global Development (INDIACom), pages": ""
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "230:120587, 2023.",
          "on Computing for Sustainable Global Development (INDIACom), pages": ""
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "[38] R. Panda, R. Malheiro, and R. P. Paiva. Musical texture and expressivity",
          "on Computing for Sustainable Global Development (INDIACom), pages": ""
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "features for music emotion recognition.\nIn 19th Int. Society for Music",
          "on Computing for Sustainable Global Development (INDIACom), pages": ""
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "Information Retrieval Conf.\n(ISMIR 2018), pages 383–391, 2018.",
          "on Computing for Sustainable Global Development (INDIACom), pages": ""
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "[39] R. Panda, R. Malheiro,\nand R. P. Paiva.\nNovel\naudio\nfeatures\nfor",
          "on Computing for Sustainable Global Development (INDIACom), pages": ""
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "music emotion recognition.\nIEEE Transactions on Affective Computing,",
          "on Computing for Sustainable Global Development (INDIACom), pages": ""
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "11(4):614–626, 2018.",
          "on Computing for Sustainable Global Development (INDIACom), pages": ""
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "[40]\nP.-T. Pham, M.-H. Huynh, H.-D. Nguyen, and M.-T. Tran. Selab-hcmus",
          "on Computing for Sustainable Global Development (INDIACom), pages": ""
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "at mediaeval 2021: Music\ntheme\nand emotion classification with co-",
          "on Computing for Sustainable Global Development (INDIACom), pages": ""
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "teaching training strategy.\nIn MediaEval, 2021.",
          "on Computing for Sustainable Global Development (INDIACom), pages": ""
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "[41]\nJ. Qiu, C. Chen, and T. Zhang. A novel multi-task learning method for",
          "on Computing for Sustainable Global Development (INDIACom), pages": ""
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "symbolic music emotion recognition. arXiv:2201.05782, 2022.",
          "on Computing for Sustainable Global Development (INDIACom), pages": ""
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "[42] C. Raffel, B. McFee, E. J. Humphrey, J. Salamon, O. Nieto, D. Liang,",
          "on Computing for Sustainable Global Development (INDIACom), pages": ""
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "eval: A transparent\nimplementation\nD. P. Ellis, and C. C. Raffel. Mir",
          "on Computing for Sustainable Global Development (INDIACom), pages": ""
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "of common mir metrics.\nIn ISMIR, volume 10, page 2014, 2014.",
          "on Computing for Sustainable Global Development (INDIACom), pages": ""
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "[43]\nS. Rajesh and N. Nalini. Musical\ninstrument emotion recognition using",
          "on Computing for Sustainable Global Development (INDIACom), pages": ""
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "deep recurrent neural network. Procedia Comput. Sci., 167:16–25, 2020.",
          "on Computing for Sustainable Global Development (INDIACom), pages": ""
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "[44]\nJ. A. Russell. A circumplex model of affect. Journal of personality and",
          "on Computing for Sustainable Global Development (INDIACom), pages": ""
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "social psychology, 39(6):1161, 1980.",
          "on Computing for Sustainable Global Development (INDIACom), pages": ""
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "[45]\nS. Shelke and M. Patil. Exploring machine learning techniques for music",
          "on Computing for Sustainable Global Development (INDIACom), pages": ""
        },
        {
          "[20]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.": "emotion classification: A comprehensive review.\nIn 2024 11th Int. Conf.",
          "on Computing for Sustainable Global Development (INDIACom), pages": ""
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Music, computing, and health: a roadmap for the current and future roles of music technology for health care and well-being",
      "authors": [
        "K Agres",
        "R Schaefer",
        "A Volk",
        "S Van Hooren",
        "A Holzapfel",
        "S Dalla",
        "M Bella",
        "M Müller",
        "D Witte",
        "R Herremans",
        "Melendez"
      ],
      "year": "2021",
      "venue": "Music & Science"
    },
    {
      "citation_id": "2",
      "title": "Studying emotion induced by music through a crowdsourcing game",
      "authors": [
        "A Aljanaki",
        "F Wiering",
        "R Veltkamp"
      ],
      "year": "2016",
      "venue": "Information Processing & Management"
    },
    {
      "citation_id": "3",
      "title": "Developing a benchmark for emotional analysis of music",
      "authors": [
        "A Aljanaki",
        "Y.-H Yang",
        "M Soleymani"
      ],
      "year": "2017",
      "venue": "PloS one"
    },
    {
      "citation_id": "4",
      "title": "The mtg-jamendo dataset for automatic music tagging",
      "authors": [
        "D Bogdanov",
        "M Won",
        "P Tovstogan",
        "A Porter",
        "X Serra"
      ],
      "year": "2019",
      "venue": "ICML"
    },
    {
      "citation_id": "5",
      "title": "Frequency dependent convolutions for music tagging",
      "authors": [
        "V Bour"
      ],
      "year": "2021",
      "venue": "MediaEval"
    },
    {
      "citation_id": "6",
      "title": "Speech emotion recognition with multi-task learning",
      "authors": [
        "X Cai",
        "J Yuan",
        "R Zheng",
        "L Huang",
        "K Church"
      ],
      "year": "2021",
      "venue": "Interspeech"
    },
    {
      "citation_id": "7",
      "title": "Moodylyrics: A sentiment annotated lyrics dataset",
      "authors": [
        "M Morisio"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 international conference on intelligent systems, metaheuristics & swarm intelligence"
    },
    {
      "citation_id": "8",
      "title": "Multitask learning",
      "authors": [
        "R Caruana"
      ],
      "year": "1997",
      "venue": "Machine learning"
    },
    {
      "citation_id": "9",
      "title": "Attentive rnns for continuous-time emotion prediction in music clips",
      "authors": [
        "S Chaki",
        "P Doshi",
        "P Patnaik",
        "S Bhattacharya"
      ],
      "year": "2020",
      "venue": "AffCon@ AAAI"
    },
    {
      "citation_id": "10",
      "title": "A multimodal music emotion classification method based on multifeature combined network classifier",
      "authors": [
        "C Chen",
        "Q Li"
      ],
      "year": "2020",
      "venue": "Mathematical Problems in Engineering"
    },
    {
      "citation_id": "11",
      "title": "Music emotion recognition using chord progressions",
      "authors": [
        "Y.-H Cho",
        "H Lim",
        "D.-W Kim",
        "I.-K Lee"
      ],
      "year": "2016",
      "venue": "2016 IEEE Int. Conf. on Systems, Man, and Cybernetics (SMC)"
    },
    {
      "citation_id": "12",
      "title": "music21: A toolkit for computer-aided musicology and symbolic music data",
      "authors": [
        "M Cuthbert",
        "C Ariza"
      ],
      "year": "2010",
      "venue": "music21: A toolkit for computer-aided musicology and symbolic music data"
    },
    {
      "citation_id": "13",
      "title": "The relationship between precision-recall and roc curves",
      "authors": [
        "J Davis",
        "M Goadrich"
      ],
      "year": "2006",
      "venue": "Proc. of the 23rd Int. conference on Machine learning"
    },
    {
      "citation_id": "14",
      "title": "Automatic chord recognition based on the probabilistic modeling of diatonic modal harmony",
      "authors": [
        "B Di Giorgi",
        "M Zanoni",
        "A Sarti",
        "S Tubaro"
      ],
      "year": "2013",
      "venue": "nDS'13; Proceedings of the 8th International Workshop on Multidimensional Systems"
    },
    {
      "citation_id": "15",
      "title": "Applied regression analysis",
      "authors": [
        "N Draper"
      ],
      "year": "1998",
      "venue": "Applied regression analysis"
    },
    {
      "citation_id": "16",
      "title": "Clap learning audio concepts from natural language supervision",
      "authors": [
        "B Elizalde",
        "S Deshmukh",
        "M Ismail",
        "H Wang"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "17",
      "title": "Learning to generate music with sentiment",
      "authors": [
        "L Ferreira",
        "J Whitehead"
      ],
      "year": "2021",
      "venue": "Learning to generate music with sentiment",
      "arxiv": "arXiv:2103.06125"
    },
    {
      "citation_id": "18",
      "title": "Creating musical features using multi-faceted, multi-task encoders based on transformers",
      "authors": [
        "T Greer",
        "X Shi",
        "B Ma",
        "S Narayanan"
      ],
      "year": "2023",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "19",
      "title": "Music tagging with classifier group chains",
      "authors": [
        "T Hasumi",
        "T Komatsu",
        "Y Fujita"
      ],
      "year": "2025",
      "venue": "Music tagging with classifier group chains",
      "arxiv": "arXiv:2501.05050"
    },
    {
      "citation_id": "20",
      "title": "Cnn architectures for large-scale audio classification",
      "authors": [
        "S Hershey",
        "S Chaudhuri",
        "D Ellis",
        "J Gemmeke",
        "A Jansen",
        "R Moore",
        "M Plakal",
        "D Platt",
        "R Saurous",
        "B Seybold"
      ],
      "year": "2017",
      "venue": "2017 ieee Int. conference on acoustics, speech and signal processing (icassp)"
    },
    {
      "citation_id": "21",
      "title": "Distilling the knowledge in a neural network",
      "authors": [
        "G Hinton"
      ],
      "year": "2015",
      "venue": "Distilling the knowledge in a neural network",
      "arxiv": "arXiv:1503.02531"
    },
    {
      "citation_id": "22",
      "title": "Music emotion recognition using convolutional long short term memory deep neural networks",
      "authors": [
        "S Hizlisoy",
        "S Yildirim",
        "Z Tufekci"
      ],
      "year": "2021",
      "venue": "Eng. Sci. Technol. Int J"
    },
    {
      "citation_id": "23",
      "title": "Adff: Attention based deep feature fusion approach for music emotion recognition",
      "authors": [
        "Z Huang",
        "S Ji",
        "Z Hu",
        "C Cai",
        "J Luo",
        "X Yang"
      ],
      "year": "2022",
      "venue": "Adff: Attention based deep feature fusion approach for music emotion recognition",
      "arxiv": "arXiv:2204.05649"
    },
    {
      "citation_id": "24",
      "title": "Emopia: A multi-modal pop piano dataset for emotion recognition and emotionbased music generation",
      "authors": [
        "H.-T Hung",
        "J Ching",
        "S Doh",
        "N Kim",
        "J Nam",
        "Y.-H Yang"
      ],
      "year": "2021",
      "venue": "Emopia: A multi-modal pop piano dataset for emotion recognition and emotionbased music generation",
      "arxiv": "arXiv:2108.01374"
    },
    {
      "citation_id": "25",
      "title": "Multitask emotion recognition model with knowledge distillation and task discriminator",
      "authors": [
        "E Jeong",
        "G Oh",
        "S Lim"
      ],
      "year": "2022",
      "venue": "Multitask emotion recognition model with knowledge distillation and task discriminator",
      "arxiv": "arXiv:2203.13072"
    },
    {
      "citation_id": "26",
      "title": "A music emotion classification model based on the improved convolutional neural network",
      "authors": [
        "X Jia"
      ],
      "year": "2022",
      "venue": "Computational Intelligence and Neuroscience"
    },
    {
      "citation_id": "27",
      "title": "A bi-directional transformer for musical chord recognition",
      "authors": [
        "P Jonggwon",
        "C Kyoyun",
        "J Sungwook",
        "K Dokyun",
        "P Jonghun"
      ],
      "year": "2019",
      "venue": "20th International Society for Music Information Retrieval Conference (ISMIR)"
    },
    {
      "citation_id": "28",
      "title": "Are we there yet? a brief survey of music emotion prediction datasets, models and outstanding challenges",
      "authors": [
        "J Kang",
        "D Herremans"
      ],
      "year": "2024",
      "venue": "Are we there yet? a brief survey of music emotion prediction datasets, models and outstanding challenges",
      "arxiv": "arXiv:2406.08809"
    },
    {
      "citation_id": "29",
      "title": "Acoustic music understanding model with large-scale self-supervised training",
      "authors": [
        "Y Li",
        "R Yuan",
        "G Zhang",
        "Y Ma",
        "X Chen",
        "H Yin",
        "C Xiao",
        "C Lin",
        "A Ragni",
        "E Benetos"
      ],
      "year": "2023",
      "venue": "Acoustic music understanding model with large-scale self-supervised training",
      "arxiv": "arXiv:2306.00107"
    },
    {
      "citation_id": "30",
      "title": "Leveraging llm embeddings for cross dataset label alignment and zero shot music emotion prediction",
      "authors": [
        "R Liu",
        "A Roy",
        "D Herremans"
      ],
      "year": "2024",
      "venue": "Leveraging llm embeddings for cross dataset label alignment and zero shot music emotion prediction",
      "arxiv": "arXiv:2410.11522"
    },
    {
      "citation_id": "31",
      "title": "Cnn based music emotion classification",
      "authors": [
        "X Liu",
        "Q Chen",
        "X Wu",
        "Y Liu",
        "Y Liu"
      ],
      "year": "2017",
      "venue": "Cnn based music emotion classification",
      "arxiv": "arXiv:1704.05665"
    },
    {
      "citation_id": "32",
      "title": "Generating lead sheets with affect: A novel conditional seq2seq framework",
      "authors": [
        "D Makris",
        "K Agres",
        "D Herremans"
      ],
      "year": "2021",
      "venue": "2021 Int. Joint Conf. on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "33",
      "title": "Bi-modal music emotion recognition: Novel lyrical features and dataset",
      "authors": [
        "R Malheiro",
        "R Panda",
        "P Gomes",
        "R Paiva"
      ],
      "year": "2016",
      "venue": "9th Int. Workshop on Music and Machine Learning-MML"
    },
    {
      "citation_id": "34",
      "title": "Recognizing song mood and theme: Clustering-based ensembles",
      "authors": [
        "M Mayerl",
        "M Vötter",
        "A Peintner",
        "G Specht",
        "E Zangerle"
      ],
      "year": "2021",
      "venue": "MediaEval"
    },
    {
      "citation_id": "35",
      "title": "A multisource deep learning model for music emotion recognition",
      "authors": [
        "G Mazzetta",
        "A Greco",
        "M Tagliasacchi",
        "A Pescapè"
      ],
      "year": "2024",
      "venue": "Proc. of the AIxHMI Workshop at CEUR-WS"
    },
    {
      "citation_id": "36",
      "title": "Emotion and meaning in music",
      "authors": [
        "L Meyer"
      ],
      "year": "1954",
      "venue": "Emotion and meaning in music"
    },
    {
      "citation_id": "37",
      "title": "Constructing time-series momentum portfolios with deep multi-task learning",
      "authors": [
        "J Ong",
        "D Herremans"
      ],
      "year": "2023",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "38",
      "title": "Musical texture and expressivity features for music emotion recognition",
      "authors": [
        "R Panda",
        "R Malheiro",
        "R Paiva"
      ],
      "year": "2018",
      "venue": "19th Int. Society for Music Information Retrieval Conf. (ISMIR 2018)"
    },
    {
      "citation_id": "39",
      "title": "Novel audio features for music emotion recognition",
      "authors": [
        "R Panda",
        "R Malheiro",
        "R Paiva"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "40",
      "title": "Selab-hcmus at mediaeval 2021: Music theme and emotion classification with coteaching training strategy",
      "authors": [
        "P.-T Pham",
        "M.-H Huynh",
        "H.-D Nguyen",
        "M.-T Tran"
      ],
      "year": "2021",
      "venue": "MediaEval"
    },
    {
      "citation_id": "41",
      "title": "A novel multi-task learning method for symbolic music emotion recognition",
      "authors": [
        "J Qiu",
        "C Chen",
        "T Zhang"
      ],
      "year": "2022",
      "venue": "A novel multi-task learning method for symbolic music emotion recognition",
      "arxiv": "arXiv:2201.05782"
    },
    {
      "citation_id": "42",
      "title": "Mir eval: A transparent implementation of common mir metrics",
      "authors": [
        "C Raffel",
        "B Mcfee",
        "E Humphrey",
        "J Salamon",
        "O Nieto",
        "D Liang",
        "D Ellis",
        "C Raffel"
      ],
      "year": "2014",
      "venue": "ISMIR"
    },
    {
      "citation_id": "43",
      "title": "Musical instrument emotion recognition using deep recurrent neural network",
      "authors": [
        "S Rajesh",
        "N Nalini"
      ],
      "year": "2020",
      "venue": "Procedia Comput. Sci"
    },
    {
      "citation_id": "44",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "45",
      "title": "Exploring machine learning techniques for music emotion classification: A comprehensive review",
      "authors": [
        "S Shelke",
        "M Patil"
      ],
      "year": "2024",
      "venue": "2024 11th Int. Conf. on Computing for Sustainable Global Development (INDIACom)"
    },
    {
      "citation_id": "46",
      "title": "1000 songs for emotional analysis of music",
      "authors": [
        "M Soleymani",
        "M Caro",
        "E Schmidt",
        "C.-Y Sha",
        "Y.-H Yang"
      ],
      "year": "2013",
      "venue": "Proc. of the 2nd ACM Int. workshop on Crowdsourcing for multimedia"
    },
    {
      "citation_id": "47",
      "title": "Transformer-based automatic music mood classification using multi-modal framework",
      "authors": [
        "S Suresh Kumar",
        "R Rajan"
      ],
      "year": "2023",
      "venue": "Journal of Computer Science & Technology"
    },
    {
      "citation_id": "48",
      "title": "Semi-supervised music emotion recognition using noisy student training and harmonic pitch class profiles",
      "authors": [
        "H Tan"
      ],
      "year": "2021",
      "venue": "Semi-supervised music emotion recognition using noisy student training and harmonic pitch class profiles",
      "arxiv": "arXiv:2112.00702"
    },
    {
      "citation_id": "49",
      "title": "Multimodal music emotion recognition method based on the combination of knowledge distillation and transfer learning",
      "authors": [
        "G Tong"
      ],
      "year": "2022",
      "venue": "Scientific Programming"
    },
    {
      "citation_id": "50",
      "title": "Mediaeval 2021: Emotion and theme recognition in music using jamendo",
      "authors": [
        "P Tovstogan",
        "D Bogdanov",
        "A Porter"
      ],
      "year": "2021",
      "venue": "MediaEval"
    },
    {
      "citation_id": "51",
      "title": "Emotionaware music recommendation",
      "authors": [
        "H Tran",
        "T Le",
        "A Do",
        "T Vu",
        "S Bogaerts",
        "B Howard"
      ],
      "year": "2023",
      "venue": "Proc. of the AAAI Conf. on Artificial Intelligence"
    },
    {
      "citation_id": "52",
      "title": "Towards musical query-by-semantic-description using the cal500 data set",
      "authors": [
        "D Turnbull",
        "L Barrington",
        "D Torres",
        "G Lanckriet"
      ],
      "year": "2007",
      "venue": "Proc. of the 30th annual Int. ACM SIGIR Conf. on Research and development in information retrieval"
    },
    {
      "citation_id": "53",
      "title": "Towards timevarying music auto-tagging based on cal500 expansion",
      "authors": [
        "S.-Y Wang",
        "J.-C Wang",
        "Y.-H Yang",
        "H.-M Wang"
      ],
      "year": "2014",
      "venue": "2014 IEEE Int. Conf. on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "54",
      "title": "Acoustic music understanding model with large-scale self-supervised training",
      "authors": [
        "L Yizhi",
        "R Yuan",
        "G Zhang",
        "Y Ma",
        "X Chen",
        "H Yin",
        "C Xiao",
        "C Lin",
        "A Ragni",
        "E Benetos"
      ],
      "year": "2023",
      "venue": "The Twelfth International Conference on Learning Representations"
    },
    {
      "citation_id": "55",
      "title": "The pmemo dataset for music emotion recognition",
      "authors": [
        "K Zhang",
        "H Zhang",
        "S Li",
        "C Yang",
        "L Sun"
      ],
      "year": "2018",
      "venue": "Proc. of the 2018 acm on Int. Conf. on multimedia retrieval"
    }
  ]
}