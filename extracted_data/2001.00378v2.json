{
  "paper_id": "2001.00378v2",
  "title": "Deep Representation Learning In Speech Processing: Challenges, Recent Advances, And Future Trends",
  "published": "2020-01-02T10:12:23Z",
  "authors": [
    "Siddique Latif",
    "Rajib Rana",
    "Sara Khalifa",
    "Raja Jurdak",
    "Junaid Qadir",
    "Björn W. Schuller"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Research on speech processing has traditionally considered the task of designing hand-engineered acoustic features (feature engineering) as a separate distinct problem from the task of designing efficient machine learning (ML) models to make prediction and classification decisions. There are two main drawbacks to this approach: firstly, the feature engineering being manual is cumbersome and requires human knowledge; and secondly, the designed features might not be best for the objective at hand. This has motivated the adoption of a recent trend in speech community towards utilisation of representation learning techniques, which can learn an intermediate representation of the input signal automatically that better suits the task at hand and hence lead to improved performance. The significance of representation learning has increased with advances in deep learning (DL), where the representations are more useful and less dependent on human knowledge, making it very conducive for tasks like classification, prediction, etc. The main contribution of this paper is to present an up-to-date and comprehensive survey on different techniques of speech representation learning by bringing together the scattered research across three distinct research areas including Automatic Speech Recognition (ASR), Speaker Recognition (SR), and Speaker Emotion Recognition (SER). Recent reviews in speech have been conducted for ASR, SR, and SER, however, none of these has focused on the representation learning from speech-a gap that our survey aims to bridge. We also highlight different challenges and the key characteristics of representation learning models and discuss important recent advancements and point out future trends. Our review can be used by the speech research community as an essential resource to quickly grasp the current progress in representation learning and can also act as a guide for navigating study in this area of research. This work is the extended version of paper accepted in IEEE Transactions on Affective Computing 2021. https://ieeexplore.ieee.org/document/9543566 TABLE I: Comparison of our paper with that of the existing surveys. Focus Speech Paper Representation Learning ASR SR SER Deep Learning Details Bengio et al. [1] 2013 This paper reviewed the work in the area of unsupervised feature learning and deep learning, it also covered advancements in probabilistic models and autoencoders. It does not include recent models like VAE and GANs.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "I. Introduction",
      "text": "The performance of machine learning (ML) algorithms heavily depends on data representation or features. Traditionally most of the actual ML research has focused on feature engineering or the design of pre-processing data transformation pipelines to craft representations that support ML algorithms  [1] . Although such feature engineering techniques can help improve the performance of predictive models, the downside is that these techniques are labour-intensive and time-consuming.\n\nEmail: siddique.latif@usq.edu.au\n\nTo broaden the scope of ML algorithms, it is desirable to make learning algorithms less dependent on hand-crafted features.\n\nA key application of ML algorithms has been in analysing and processing speech. Nowadays, speech interfaces have become widely accepted and integrated into various real-life applications and devices. Services like Siri and Google Voice Search have become a part of our daily life and are used by millions of users  [2] . Research in speech processing and analysis has always been motivated by a desire to enable machines to participate in verbal human-machine interactions. The research goals of enabling machines to understand human speech, identify speakers, and detect human emotion have attracted researchers' attention for more than sixty years  [3] . Researchers are now focusing on transforming current speech-based systems into the next generation AI devices that react with humans more friendly and provide personalised responses according to their mental states. In all these successes, speech representations-in particular, deep learning (DL)-based speech representations-play an important role. Representation learning, broadly speaking, is the technique of learning representations of input data, usually through the transformation of the input data, where the key goal is yielding abstract and useful representations for tasks like classification, prediction, etc. One of the major reasons for the utilisation of representation learning techniques in speech technology is that speech data is fundamentally different from two-dimensional image data. Images can be analysed as a whole or in patches but speech has to be studied sequentially to capture temporal contexts.\n\nTraditionally, the efficiency of ML algorithms on speech has relied heavily on the quality of hand-crafted features. A good set of features often leads to better performance compared to a poor speech feature set. Therefore, feature engineering, which focuses on creating features from raw speech and has led to lots of research studies, has been an important field of research for a long time. DL models, in contrast, can learn feature representation automatically which minimises the dependency on hand-engineered features and thereby give better performance in different speech applications  [4] . These deep models can be trained on speech data in different ways ****This work is the extended version of paper accepted in IEEE Transactions on Affective Computing 2021. https://ieeexplore.ieee.org/document/9543566**** such as supervised, unsupervised, semi-supervised, transfer, and reinforcement learning. This survey covers all these feature learning techniques and popular deep learning models in the context of three popular speech applications  [5] : (1) automatic speech recognition (ASR); (2) speaker recognition (SR); and (3) speech emotion recognition (SER).\n\nDespite growing interest in representation learning from speech, existing contributions are scattered across different research areas and a comprehensive survey is missing. To highlight this, we present the summary of different popular and recently published review papers Table  I . The review article published in 2013 by Bengio et al.  [1]  is one of the most cited papers. It is very generic and focuses on appropriate objectives for learning good representations, for computing representations (i. e., inference), and the geometrical connections between representation learning, manifold learning, and density estimation. Due to an earlier publication date, this paper had a focus on principal component analysis (PCA), restricted Boltzmann machines (RBMs), autoencoders (AEs) and recently proposed generative models were out of the scope of this paper. The research on representation learning has evolved significantly since then as generative models like variational autoencoders (VAEs)  [10] , generative adversarial networks (GANs)  [11] , etc., have been found to be more suitable for representation learning compared to autoencoders and other classical methods. We cover all these new models in our review. Although, other recent surveys have focused on DL techniques for ASR  [7] ,  [9] , SR  [12] , and SER  [8] , none of these has focused on representation learning from speech. This article bridges this gap by presenting an up-to-date survey of research that focused on representation learning in three active areas: ASR, SR, and SER. Beyond reviewing the literature, we discuss the applications of deep representation learning, present popular DL models and their representation learning abilities, and different representation learning techniques used in the literature. We further highlight the challenges faced by deep representation learning in the speech and finally conclude this paper by discussing the recent advancement and pointing out future trends. The structure of this article is shown in Figure  1 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Background",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Traditional Feature Learning Algorithms",
      "text": "In the field of data representation learning, the algorithms are generally categorised into two classes: shallow learning algorithms and DL-based models  [13] . Shallow learning algorithms are also considered as traditional methods. They aim to learn transformations of data by extracting useful information. One of the oldest feature learning algorithms, Principal Components Analysis (PCA)  [14]  has been studied extensively over the last century. During this period, various other shallow learning algorithms have been proposed based on various learning techniques and criteria, until the popular deep models in recent years. Similar to PCA, Linear Discriminant Analysis (LDA)  [15]  is another shallow learning algorithm. Both PCA and LDA are linear data transformation techniques, however, LDA is a supervised method that requires class labels to maximise class separability. Other linear feature learning methods includes Canonical Correlation Analysis (CCA)  [16] , Multi-Dimensional Scaling (MDS)  [17] , and Independent Component Analysis (ICA)  [18] . The kernel version of some linear feature mapping algorithms are also proposed including kernel PCA (KPCA)  [19] , and generalised discriminant analysis (GDA)  [20] . They are non-linear versions of PCA and LDA, respectively. Another popular technique is Non-negative Matrix Factorisation (NMF)  [21]  that can generate sparse representations of data useful for ML tasks.\n\nMany methods for nonlinear feature reduction are also proposed to discover the non-linear hidden structure from the high dimensional data  [22] . They include Locally Linear Embedding (LLE)  [23] , Isometric Feature Mapping (Isomap)  [24] , T-distributed Stochastic Neighbour Embedding (t-SNE)  [25] , and Neural Networks (NNs)  [26] . In contrast to kernelbased methods, non-linear feature representation algorithms directly learn the mapping functions by preserving the local information of data in the low dimensional space. Traditional representation algorithms have been widely used by researchers of the speech community for transforming the speech representations to more informative features having low dimensional space (e.g.,  [27] ,  [28] ). However, these shallow feature learning algorithms dominate the data representation learning area until the successful training of deep models for representation learning of data by  Hinton and Salakhutdinov in 2006 [29] . This work was quickly followed up with similar ideas by others  [30] ,  [31] , which lead to a large number of deep models suitable for representation learning. We discuss the brief history of the success of DL in speech technology next.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "B. Brief History On Deep Learning (Dl) In Speech Technology",
      "text": "For decades, the Gaussian Mixture Model (GMM) and Hidden Markov Model (HMM) based models (GMM-HMM) ruled the speech technology due to their many advantages including their mathematical elegance and capability to model time-varying sequences  [32] . Around 1990, discriminative training was found to produce better results compared to the models trained using maximum likelihood  [33] . Since then researchers started working towards replacing GMM with a feature learning models including neural networks (NNs), restricted Boltzmann machines (RBMs), deep belief networks (DBNs), and deep neural networks (DNNs)  [34] . Hybrid models gained popularity while HMMs continued to be investigated.\n\nIn the meanwhile, researchers also worked towards replacing HMM with other alternatives. In 2012, DNNs were trained on thousands of hours of speech data and they successfully reduced the word error rate (WER) on ASR task  [35] . This is due to their ability to learn a hierarchy of representations from input data. However, soon after recurrent neural networks (RNNs) architectures including long-short term memory (LSTM) and gated recurrent units (GRUs) outperformed DNNs and became state-of-the-art models not only in ASR  [36]  but also in SER  [37] . The superior performance of RNN architectures was because of their ability to capture temporal contexts from speech  [38] ,  [39] . Later, a cascade of convolutional neural networks (CNNs), LSTM and fully connected (DNNs) layers were further shown to outperform LSTM-only models by capturing more discriminative attributes from speech  [40] ,  [41] . The lack of labelled data set the pace for the unsupervised representation learning research. For unsupervised representation from speech, AEs, RBMs, and DBNs were widely used  [42] .\n\nNowadays, there has been a significant interest in three classes of generative models including VAEs, GANs, and deep auto-regressive models  [43] ,  [44] . They have been widely being employed for speech processing-especially VAEs and GANs are becoming very influential models for learning speech representation in an unsupervised way  [45] ,  [46] . In speech analysis tasks, deep models for representation learning can either be applied to speech features or directly on the raw waveform. We present a brief history of speech features in the next section.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "C. Speech Features",
      "text": "In speech processing, feature engineering and designing of models for classification or prediction are often considered as separate problems. Feature engineering is a way of manually designing speech features by taking advantage of human ingenuity. For decades, Mel Frequency Cepstral Coefficients (MFCCs)  [47]  have been used as the principal set of features for speech analysis tasks. Four steps involve for MFCCs extraction: computation of the Fourier transform, projection of the powers of the spectrum onto the Mel scale, taking the logarithm of the Mel frequencies, and applying Discrete Cosine Transformation (DCT) for compressed representations. It is found that the last step removes the information and destroys spatial relations; therefore, it is usually omitted, which yields the log-Mel spectrum, a popular feature across the speech community. This has been the most popular feature to train DL networks.\n\nThe Mel-filter bank is inspired by auditory and physiological findings of how humans perceive speech signals  [48] . Sometimes, it becomes preferable to use features that capture transpositions as translations. For this, a suitable filter bank is spectrograms that captures how the frequency content of the speech signal changes with time  [49] . In speech research, researchers widely used CNNs for spectrogram inputs due to their image like configuration. Log-Mel spectrograms is another speech representation that provides a compact representation and became the current state of the art because models using these features usually need less data and training to achieve similar or better results.\n\nIn SER, feature engineering is more dominant and a minimalistic sets of features like GeMAPs and eGeMAPs  [50]  are also proposed based on affective physiological changes in voice production and their theoretical significance  [50] . They are also popular being used as benchmark feature sets. However, in speech analysis tasks, some works  [51] ,  [52]  show that the particular choice of features is less important compared to the design of the model architecture and the amount of training data. The research is continuing in designing such DL models and input features that involve minimum human knowledge.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "D. Databases",
      "text": "Although the success of deep learning is usually attributed to the models' capacity and higher computational power, the most crucial role is played by the availability of large-scale labelled datasets  [53] . In contrast to the vision domain, the speech community started using DNNs with considerably smaller datasets. Some popular conventional corpora used for ASR and SR includes TIMIT  [54] , Switchboard  [55] , WSJ  [56] , AMI  [57] . Similarly, EMO-DB  [58] , FAU-AIBO  [59] , RECOLA  [60] , and GEMEP  [61]  are some popular classical datasets. Recently, larger datasets are being created and released to the research community to engage the industry as well as the researchers. We summarise some of these recent and publicly available datasets in Table  II  that are widely used in the speech community.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "E. Evaluations",
      "text": "Evaluation measures vary across speech tasks. The performance of ASR systems is usually measured using word error rates (WER), which is the fraction of the sum of insertion, deletion, and substitution divided by the total number of words in the reference transcription. In speaker verification systems, two types of errors-namely, false rejection (fr), where a valid identity is rejected, and false acceptance (fa), where a fake identity is accepted-are used. These two errors are measured experimentally on test data. Based on these two errors, a detection error trade-offs (DETs) curve is drawn to evaluate the performance of the system. DET is plotted using the probability of false acceptance (P f a ) as a function of the probability of false rejection (P f r ). Another popular evaluation measure is the equal error rate (EER) which corresponds to the operating point where P f a = P f r . Similarly, the area under curve (AUC) of the receiver operating curve (ROC) is often found. The details on other evaluation measures for the speaker verification task can be found in  [71] . Both speaker identification and emotion recognition use classification accuracy as a metric. However, as data is often imbalanced across the classes in naturalistic emotion corpora, the accuracy is usually used as so-called unweighted accuracy (UA) or unweighted average recall (UAR), which represents the average recall across classes, unweighted by the number of instances by classes. This has been introduced by the first challenge in the field-the Interspeech 2009 Emotion Challenge  [72]  and has since been picked up by other challenges across the field. Also, SER systems use regression to predict emotional attributes such as continuous arousal and valence or dominance.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iii. Applications Of Deep Representation Learning",
      "text": "Learning representations is a fundamental problem in AI and it aims to capture useful information or attributes of data, where deep representation learning involves DL models for this task. Various applications of deep representation learning have been summarised in Figure  2 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Automatic Feature Learning",
      "text": "Automatic feature learning is the process of constructing explanatory variables or features that can be used for classification or prediction problems. Feature learning algorithms can be supervised or unsupervised  [73] . Deep learning (DL) models are composed of multiple hidden layers and each layer provides a kind of representation of the given data  [74] . It has been found that automatically learnt feature representations are -given enough training data -usually more efficient and repeatable than hand-crafted or manually designed features which allow building better faster predictive models  [6] . Most importantly, automatically learnt feature representation is in most cases more flexible and powerful and can be applied to any data science problem in the fields of vision processing  [75] , text processing  [76] , and speech processing  [77] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Dimension Reduction And Information Retrieval",
      "text": "Broadly speaking, dimensionality reduction methods are commonly used for two purposes:  (1)  to eliminate data redundancy and irrelevancy for higher efficiency and often increased performance , and (2) to make the data more understandable and interpretable by reducing the number of input variables  [6] . In some applications, it is very difficult to analyse the high dimensional data with a limited number of training samples  [78] . Therefore, dimension reduction becomes imperative to retrieve important variables or information relevant to the specified problems. It has been validated that the use of more interpretable features in a lower dimension can provide competitive performance or even better performance when used for designing predictive models  [79] .\n\nInformation retrieval is a process of finding information based on a user query by examining a collection of data  [80] . The queried material can be text, documents, images, or audio, and users can express their queries in the form of a text, voice, or image  [81] ,  [82] . Finding a suitable representation of a query to perform retrieval is a challenging task and DL-based representation learning techniques are playing an important role in this field. The major advantages of using representation learning models for information retrieval is that they can learn features automatically with little or no prior knowledge  [83] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Data Denoising",
      "text": "Despite the success of deep models in different fields, these models remain brittle to the noise  [84] . To deal with noisy conditions, one often performs data augmentation by adding artificially-noised examples to the training set  [85] . However, data augmentation may not help always, because the distribution of noise is not always known. In contrast, representation learning methods can be effectively utilised to learn noiserobust features learning and they often provide better results compared to data augmentation  [86] . In addition, the speech can be denoised such as by DL-based speech enhancement systems  [87] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. Clustering Structure",
      "text": "Clustering is one of the most traditional and frequently used data representation methods. It aims to categorise similar classes of data samples into one cluster using similarity measures (e. g., Euclidean distance). A large number of data clustering techniques have been proposed  [88] . Classical clustering methods usually have poor performance on highdimensional data, and suffer from high computational complexity on large-scale datasets  [89] . In contrast, DL-based clustering methods can process large and high dimensional data (e. g., images, text, speech) with a reasonable time complexity and they have emerged as effective tools for clustering structures  [89] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "E. Disentanglement And Manifold Learning",
      "text": "Disentangled representation is a method that disentangles or represents each feature into narrowly defined variables and encodes them as separate dimensions  [1] . Disentangled representation learning differs from other feature extraction or dimensionality reduction techniques as it explicitly aims to learn such representations that aligns axes with the generative factors of the input data  [90] ,  [91] . Practically, data is generated from independent factors of variation. Disentangled representation learning aims to capture these factors by different independent variables in the representation. In this way, latent variables are interpretable, generalisable, and robust against adversarial attacks  [92] .\n\nManifold learning aims to describe data as low-dimensional manifolds embedded in high-dimensional spaces  [93] . Manifold learning can retain a meaningful structure in very low dimensions compared to linear dimension reduction methods  [94] . Manifold learning algorithms attempt to describe the high dimensional data as a non-linear function of fewer underlying parameters by preserving the intrinsic geometry  [95] ,  [96] . Such parameters have a widespread application in pattern recognition, speech analysis, and computer vision  [97] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "F. Abstraction And Invariance",
      "text": "The architecture of DNNs is inspired by the hierarchical structure of the brain  [98] . It is anticipated that deep architectures might capture abstract representations  [99] . Learning abstractions is equivalent to discovering a universal model that can be across all tasks to facilitate generalisation and knowledge transfer. More abstract features are generally invariant to the local changes and are non-linear functions of the raw input  [100] . Abstraction representations also capture high-level continuous-valued attributes that are only sensitive to some very specific types of changes in the input signal. Learning such sorts of invariant features has more predictive power which has always been required by the artificial intelligence (AI) community  [101] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iv. Representation Learning Architectures",
      "text": "In 2006, DL-based automatic feature discovery was initiated by Hinton and his colleagues  [29]  and followed up by other researchers  [30] ,  [31] . This has led to a breakthrough in representation learning research and many novel DL models have been proposed. In this section, we will discuss these This work is the extended version of paper accepted in IEEE Transactions on Affective Computing 2021.\n\nhttps://ieeexplore.ieee.org/document/9543566 models and highlight the mechanics of representation learning using them.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Deep Neural Networks (Dnns)",
      "text": "Historically, the idea of deep neural networks (DNNs) is an extension of ideas emerging from research on artificial neural networks (ANNs)  [102] . Feed Forward Neural Networks (FNNs) or Multilayer Perceptrons (MLPs)  [103]  with multiple hidden layers are indeed a good example of deep architectures. DNNs consist of multiple layers, including an input layer, hidden layers, and an output layer, of processing units called \"neurons\". These neurons in each layer are densely connected with the neurons of the adjacent layers. The goal of DNNs is to approximate some function f . For instance, a DNN classifier maps an input x to a category label y by using a mapping function y = f (x; θ) and learns the value of the parameters θ that result in the best function approximation. Each layer of a DNN performs representation learning based on the input provided to it. For example, in case of a classifier, all hidden layers except the last layer (softmax) learn a representation of input data to make classification task easier. A well trained DNN network learns a hierarchy of distributed representations  [74] . Increasing the depth of DNNs promotes reusing of learnt representations and enables the learning of a deep hierarchy of representations at different levels of abstraction. Higher levels of abstraction are generally associated with invariance to local changes of the input  [1] . These representations proved very helpful in designing different speech-based systems.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Convolutional Neural Networks (Cnns)",
      "text": "Convolutional neural networks (CNNs)  [104]  are a specialised kind of deep architecture for processing of data having a grid-like topology. Examples include image data that have 2D grid pixels and time-series data (i. e., 1D grid) having samples at regular intervals of time to create a grid-like structure. CNNs are a variant of the standard FNNs. They introduce convolutional and pooling layers into the structure of DNNs, which take into account the spatial representations of the data and make the network more efficient by introducing sparse interactions, parameter sharing, and equivariant representations. The convolution operation in the convolution layer is the fundamental building block of CNNs. It consists of several learnable kernels that are convolved with the input to compute the output feature map. This operation is defined as:\n\nwhere (h k ) ij represents the (i, j) th element for the k th output feature map, q is the input feature maps, W k and b k represent the k th filter and bias, respectively. The symbol ⊗ denotes the 2D convolution operation. After the convolution operation, a pooling operation is applied, which facilitates nonlinear downsampling of the feature map and makes the representations invariant to small translations in the input  [73] . Finally, it is common to use DNN layers to accumulate the outputs from the previous layers to yield a stochastic likelihood representation for classification or regression.\n\nIn contrast to DNNs, the training process of CNNs is easy due to fewer parameters  [105] . CNNs are found very powerful in extracting low-level representations at the initial layers and high-level features (textures and semantics) in the higher layers  [106] . The convolution layer in CNNs acts as data-driven filterbank that is able to capture representations from speech  [107]  that are more generalised  [108] , discriminative  [106] , and contextual  [109] .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Recurrent Neural Networks (Rnns)",
      "text": "Recurrent neural networks (RNNs)  [110]  are an extension of FNNs by introducing recurrent connections within layers. They use the previous state of the model as additional input at each time step which creates a memory in its hidden state having information from all previous inputs. This makes RNNs to have stronger representational memory compared to hidden Markov models (HMMs), whose discrete hidden states bound their memory  [111] . Given an input sequence x(t) = (x 1 , ....., x T ) at the current time step t, they calculates the hidden state h t using the previous hidden state h t-1 and outputs a vector sequence y = (y 1 , ....., y T ). The standard equations for RNNs are given below:\n\nwhere W terms are the weight matrices (i. e., W xh is a weight matrix of an input-hidden layer), b is the bias vector, and H denotes the hidden layer function. Simple RNNs usually fail to model the long-term temporal contingencies due to the vanishing gradient problem. To deal with this problem, multiple specialised RNN architectures exist including long short-term memory (LSTM)  [112]  and gated recurrent units (GRUs)  [111]  with gating mechanism to add and forget the information selectively. Bidirectional RNNs  [113]  were proposed to model future context by passing the input sequence through two separate recurrent hidden layers. These separate recurrent layers are connected to the same output to access the temporal context in both directions to model both past and future. RNNs introduce recurrent connections to allow parameters to be shared across time which makes them very powerful in learning temporal dynamics from sequential data (e. g., audio, video). Due to these abilities, RNNs especially LSTMs have had an enormous impact in speech community and they are incorporated in state-of-the-art ASR systems  [114] .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "D. Autoencoders (Aes)",
      "text": "The idea of an autoencoding network  [115]  is to learn a mapping from high-dimensional data to a lower-dimensional feature space such that the input observations can be approximately reconstructed from the lower-dimensional representation. The function f θ is called the encoder that maps the input vector x into feature/representation vector h = f θ (x). The decoder network is responsible to map a feature vector h to reconstruct the input vector x = g θ (h). The decoder network parameterises This work is the extended version of paper accepted in IEEE Transactions on Affective Computing 2021.\n\nhttps://ieeexplore.ieee.org/document/9543566 the decoder function g θ . Overall, the parameters are optimised by minimising the following cost function:\n\nThe set of parameters θ of the encoder and decoder networks are simultaneously learnt by attempting to incur a minimal reconstruction error. If the input data have correlated structures; then, the autoencoders (AEs) can learn some of these correlations  [78] . To capture useful representations h, the cost function of Equation 4 is usually optimised with an additional constraint to prevent the AE from learning the useless identity function having zero reconstruction error. This is achieved through various ways in the different forms of AEs, as discussed below in more detail.\n\n1) Undercomplete Autoencoders (AEs): One way of learning useful feature representations h is to regularise the autoencoder by imposing constraints to have a low dimensional feature size. In this way, the AE is forced to learn the salient features/representations of data from high dimensional space to a low dimensional feature space. If an autoencoder uses a linear activation function with the mean squared error criterion; then, the resultant architecture will become equivalent to the PCA algorithm, and its hidden units will learn the principal components of input data  [1] . However, an autoencoder with non-linear activation functions can learn a more useful feature representation compared to PCA  [29] .\n\n2) Sparse Autoencoders (AEs): An AE network can also discover a useful feature representation of data, even when the size of the feature representations is larger than the input vector x  [78] . This is done by using the idea of sparsity regularisation  [31]  by imposing an additional constraint on the hidden units. Sparsity can be achieved either by penalising hidden unit biases  [31]  or the outputs' hidden unit, however, it hurts numerical optimisation. Therefore, imposing sparsity directly on the outputs of hidden units is very popular and has several variants. One way to realise a sparse AEs is to incorporate an additional term in the loss function to penalise the KL divergence between average activation of the hidden unit and the desired sparsity (ρ)  [116] . Let us consider a j as the activation of a hidden unit j for a given input x i ; then, the average activation ρ over the training set is given by:\n\nwhere n is the number of training samples. Then, the cost function of a sparse autoencoder will become:\n\nwhere L(x, g θ (f θ (x))) is the cost function of the standard autoencoder. Another way to penalise a hidden unit is to use l 1 as penalty by which the following objective becomes:\n\nSparseness plays a key role in learning a more meaningful representation of input data  [116] . It has been found that sparse AEs are simple to train and can learn better representation compared to denoising autoencoders (DAE) and RBMs  [117] .\n\nIn particular, sparse encoders can learn useful information and attributes from speech, which can facilitate better classification performance  [118] .\n\n3) Denoising Autoencoders (DAEs): Denoising autoencoders (DAEs) are considered as a stochastic version of the basic AE. They are trained to reconstruct a clean input from its corrupted version  [119] . The objective function of a DAE is given by:\n\nwhere x is the corrupted version of x, which is done via stochastic mapping x ∼ q D (x|x). During training, DAEs are still minimising the same reconstruction loss between a clean x and its reconstruction from h. The difference is that h is learnt by applying a deterministic mapping f θ to a corrupted input x. It thus learns higher level feature representations that are robust to the corruption process. The features learnt by a DAE are reported qualitatively better for tasks like classification and also better than RBM features  [1] . 4) Contractive Autoencoders (CAEs): Contractive autoencoders (CAEs) proposed by Rifai et al.  [120]  with the motivation to learn robust representations are similar to a DAEs. CAEs are forced to learn useful representations that are robust to infinitesimal input variations. This is achieved by adding an analytic contractive penalty to Equation  4 . The penalty term is the Frobenius norm of the Jacobian matrix of the hidden layer with respect to the input x. The loss function for a CAE is given by:\n\nwhere m is the number of hidden units, and z i is the activation of hidden unit i.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "E. Deep Generative Models",
      "text": "Generative models are powerful in learning the distribution of any kind of data (audio, images, or video) and aim to generate new data points. Here, we discuss four generative models due to their popularity in the speech community.\n\n1) Boltzmann Machines and Deep Belief Networks: Deep Belief Networks (DBNs)  [121]  are a powerful probabilistic generative model that consists of multiple layers of stochastic latent variables, where each layer is a Restricted Boltzmann Machine (RBM)  [122] . Boltzmann Machines (BM) are a bipartite graph in which visible units are connected to hidden units using undirected connections with weights. A BM is restricted in the sense that there are no hidden-hidden and visible-visible connections. A RBM is an energy-based model whose joint probability distribution between visible layer (v) and hidden layer (h) is given by:\n\nZ is the normalising constant also known as the partition function, and E(v, h) is an energy function defined by the following equation:\n\nThis work is the extended version of paper accepted in IEEE Transactions on Affective Computing 2021. https://ieeexplore.ieee.org/document/9543566 where v i and h i are the binary states of hidden and visible units. W ij are the weights between visible and hidden nodes. b i and a j represent the bias terms for visible and hidden units respectively.\n\nDuring the training phase, an RBM uses Markov Chain Monte Carlo (MCMC)-based algorithms  [121]  to maximise the log-likelihood of the training data. Training based on MCMC computes the gradient of the log-likelihood, which poses a significant learning problem  [123] , Moreover, DBNs are trained using layer-wise training that is also computationally inefficient. In recent years, generative models like GANs and VAEs have been proposed that can be trained via direct back-propagation and avoid the difficulties of MCMC based training. We discuss GANs and VAEs in more detail next.\n\n2) Generative Adversarial Networks (GANs): Generative adversarial networks (GANs)  [11]  use adversarial training to directly shape the output distribution of the network via backpropagation They include two neural networks-a generator, G, and a discriminator, D, which play a min-max adversarial game defined by the following optimisation program:\n\nThe generator, G, maps the latent vectors, z, drawn from some known prior, p z (e. g., Gaussian), to fake data points, G(z). The discriminator, D, is tasked with differentiating between generated samples (fake), G(z), and real data samples, x, (drawn from data distribution, p data ). The generator network, G(z), is trained to maximally confuse the discriminator into believing that samples it generates come from the data distribution. This makes the GANs very powerful. They have become very popular and are being exploited in various ways by speech community either for speech synhtesising or to augment the training material by generated feature observations or speech itself. Researchers proposed various other architecture on the idea of GAN. These models include conditional GAN  [126] , BiGAN  [127] , InfoGAN  [128] , etc. These days, GAN based architectures are widely being used for representation learning not only from images but also from speech and related fields.\n\n3) Variational Autoencoders: Variational Autoencoders (VAEs) are probabilistic models that use a stochastic encoder for modelling the posterior distribution q(z|x), and a generative network (decoder) that models the conditional log-likelihood logp(x|z). Both of these networks are jointly trained to maximise the following variational lower bound on the data loglikelihood:\n\nThe first term is the standard reconstruction term of an AE and the second term is the KL divergence between the prior p(z) and the posterior distribution q(z|x). The second term acts as a regularisation term and without it, the model is simply a standard autoencoder. VAEs are becoming very popular in learning representation from speech. Recently, various variants of VAEs are proposed in the literature, which include β-VAE  [129] , InfoVAE  [130] , PixelVAE  [131] , and many more  [132] . All these VAEs are very powerful in learning disentangled and hierarchical representations and are also popular in clustering multi-category structures of data  [132] .\n\n4) Autoregressive Networks (ANs): Autoregressive networks (ANs) are directed probabilistic models with no latent random variables. They model the joint distribution of high-dimensional data as a product of conditional distributions using the following probabilistic chain-rule:\n\nwhere x t is the t th variable of x and θ are the parameters of the AN model. The conditional probability distributions in ANs are usually modelled with a neural network that receives x < t as input and outputs a distribution over possible x t . Some of the popular ANs includes PixelRNN  [43] , PixelCNN  [133] , and WaveNet  [44] . ANs are powerful density estimators and they capture details over global data without learning a hierarchical latent representation unlike latent variable models such as GANs, VAEs, etc. In speech technology, WaveNet is very popular and has powerful acoustic modelling capabilities. They are used for speech synthesis  [44] , denoising  [134] , and also in unsupervised representation learning setting in conjunction with VAEs  [135] .\n\nIn this section, we have discussed DL models that use representation learning of speech. In Table  III  we highlight the key characteristics of DL models in terms of their representation learning abilities. All these models can be trained in different ways to learn useful representation from speech, which we have reviewed in the next section.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "V. Techniques For Representation Learning From Speech",
      "text": "Deep models can be used in different ways to automatically discover suitable representations for the task at hand, and this section covers these techniques for learning features from speech for ASR, SR, and SER. Figure  3  shows the different learning techniques that can be used to capture information from data. These techniques have different important attributes that we highlight in Table  IV   for feature learning from speech. RBMs and DBNs are found very capable in learning features from speech for different tasks including ASR  [52] ,  [136] , speaker recognition  [137] ,  [138] , and SER  [139] . Particularly, DBNs trained in a greedy layer-wise training  [121]  can learn usef ul features from the speech signal  [140] .\n\nConvolutional neural networks (CNNs)  [104]  are another popular supervised model that is widely used for feature extraction from speech. They have shown very promising results for speech and speaker recognition tasks by learning more generalised features from raw speech compared to ANNs and other feature-based approaches  [107] ,  [108] ,  [141] . After the success of CNNs in ASR, researchers also attempted to explore them for SER  [41] ,  [106] ,  [142] ,  [143] , where they used CNNs in combination with LSTM networks for modelling long term dependencies in an emotional speech. Overall, it has been found that LSTMs (or GRUs) can help CNNs in learning more useful features from speech  [40] ,  [144] .\n\nDespite the promising results, the success of supervised learning is limited by the requisite of transcriptions or labels for speech-related tasks. It cannot exploit the plethora of freely available unlabelled datasets. It is also important to note that the labelling of these datasets is very expensive in terms of time and resources. To tackle these issues, unsupervised learning comes into play to learn representations from unlabelled data. We are discussing the potentials of unsupervised learning in the next section.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "B. Unsupervised Learning",
      "text": "Unsupervised learning facilitates the analysis of input data without corresponding labels and aims to learn the underlying inherent structure or distribution of data. In the real world, data (speech, image, text) have extremely rich structures and algorithms trained in an unsupervised way to create understandings of data rather than learning for particular tasks. Unsupervised representation learning from large unlabelled datasets is an active area of research. In the context of speech analysis, it can exploit the practically available unlimited amount of unlabelled corpora to learn good intermediate feature representations, which can then be used to improve the performance of a variety of supervised tasks such as speech emotion recognition  [145] .\n\nRegarding unsupervised representation learning, researchers mostly utilised variants of autoencoders (AEs) to learn suitable features from speech data. AEs can learn high-level semantic content (e. g., phoneme identities) that are invariant to confounding low-level details (pitch contour or background noise) in speech  [135] .\n\nIn ASR and SR, most of the studies utilised VAEs for unsupervised representation learning from speech  [135] ,  [146] . VAEs can jointly learn a generative model and an inference model, which allows them to capture latent variables from observed data. In  [46] , the authors used FHVAE to capture interpretable and disentangled representations from speech without any supervision. They evaluated the model on two speech corpora and demonstrated that FHVAE can satisfactorily extract linguistic contents from speech and outperform an ivector baseline speaker verification task while reducing WER for ASR. Other autoencoding architectures like DAEs are also found very promising in finding speech representations in an unsupervised way. Most importantly, they can produce robust representation for noisy speech recognition  [147] -  [149] .\n\nSimilarly, classical models like RBMs have proved to be very successful for learning representation from speech. For instance, Jaitly and Hinton used RBMs for phoneme recognition in  [52] , and showed that RBMs can learn more discriminative features that achieved better performance compared to MFCCs. Interestingly, RBMs can also learn filterbanks from raw speech. In  [150]    [151]  or Mel scale filter banks  [140]  create high-level feature representations. Similar to ASR and SR, models including AEs, DAEs, and VAEs are mostly used for unsupervised representation learning. In  [152] , Ghosh et al. used stacked AEs for learning emotional representations from speech. They found that stacked AE can learn highly discriminative features from the speech that are suitable for the emotion classification task. Other studies  [153] ,  [154]  also used AEs for capturing emotional representation from speech and found they are very powerful in learning discriminative features. DAEs are exploited in  [155] ,  [156]  to show the suitability of DAEs for SER. In  [79] , the authors used VAEs for learning latent representations of speech emotions. They showed that VAEs can learn better emotional representations suitable for classification in contrast to standard AEs.\n\nAs outlined above, recently, adversarial learning (AL) is becoming very popular in learning unsupervised representation form speech. It involves more than one network and enables the learning in an adversarial way, which enables to learn more discriminative  [157]  and robust  [158]  features. Especially GANs  [159] , adversarial autoencoders (AAEs)  [160]  and other AL  [161]  based models are becoming popular in modelling speech not only in ASR but also SR and SER.\n\nDespite all these successes, the performance of a representation learnt in an unsupervised way is generally harder to compare with supervised methods. Semi-supervised representation learning techniques can solve this issue by simultaneously utilising both labelled and unlabelled data. We discuss semisupervised representation learning methods in the next section.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "C. Semi-Supervised Learning",
      "text": "The success in DL has predominately been enabled by key factors like advanced algorithms, processing hardware, open sharing of codes and papers, and most importantly the availability of large-scale labelled datasets and pre-trained networks on these , e. g., ImageNet. However, a large labelled database or pre-trained network for every problem like speech emotion recognition is not always available  [162] -  [164] . It is very difficult, expensive, and time-consuming to annotate such data as it requires expert human efforts  [165] . Semi-supervised learning solves this problem by utilising the large unlabelled data, together with the labelled data to build better classifiers. It reduces human efforts and provides higher accuracy, therefore, semi-supervised models are of great interest both in theory and practice  [166] .\n\nSemi-Supervised learning is very popular in SER and researchers tried various models to learn emotional representation from speech. Huang et al.  [167]  used CNN in semi-supervised for capturing affect-salient representations and reported superior performance compared to well-known handengineered features. Ladder network-based semi-supervised methods are very popular in SER and used in  [168] -  [170] . A ladder network is an unsupervised DAE that is trained along with a supervised classification or regression task. It can learn more generalised representations suitable for SER compared to the standard methods. Deng et al.  [171]  proposed a semisupervised model by combining an AE and a classifier. They considered unlabelled samples from unlabelled data as an extra garbage class in the classification problem. Features learnt by a semi-supervised AE performed better compared to an unsupervised AE. In  [165] , the authors trained an AAE by utilising the additional unlabelled emotional data to improve SER performance. They showed that additional data help to learn more generalised representations that perform better compared to supervised and unsupervised methods.\n\nIn ASR, semi-supervised learning is mainly used to circumvent the lack of sufficient training data by creating features fronts ends  [172] , by using multilingual acoustic representations  [173] , and by extracting an intermediate representation from large unpaired datasets  [174]  to improve the performance of the system. In SR, DNNs were used to learn representations for both the target speaker and interference for speech separation in a semi-supervised way  [175] . Recently, a GAN based model is exploited for a speaker diarisation system with superior results using semi-supervised training  [176] .",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "D. Transfer Learning",
      "text": "Transfer learning (TL) involves methods that utilise any knowledge resources (i.e., data, model, labels, etc.) to increase model learning and generalisation for the target task  [177] . The idea behind TL is \"Learning to Learn\", which specifies that learning from scratch (tabula rasa learning) is often limited, and experience should be used for deeper understanding  [178] . TL encompasses different approaches including multitask learning (MTL), model adaptation, knowledge transfer, covariance shift, etc. In the speech processing field, representation learning gained much interest in these approaches of TL. In this section, we cover three popular TL techniques that are being used in today's speech technology including domain adaptation, multi-task learning, and self-taught learning.\n\n1) Domain Adaptation: Deep domain adaptation is a subfield of TL and it has emerged to address the problem of unavailability of labelled data. It aims to eliminate the trainingtesting mismatch. Speech is a typical example of heterogeneous data and a mismatch always exists between the probability distributions of source and target domain data, which can degrade the performance of the system  [179] . To build more robust systems for speech-related applications in real-life, domain adaptation techniques are usually applied in the training pipeline of deep models to learn representations that explicitly minimise the difference between the source and target domains.\n\nResearchers have attempted different methods of domain adaptation using representation learning to achieve robustness under noisy conditions in ASR systems. In  [179]    [181]  domain invariant representations are extracted using Factorised Hierarchical Variational Autoencoder (FHVAE) for robust ASR. Also, some studies  [182] ,  [183]  explored unsupervised representation learning-based domain adaptation for distant conversational speech recognition. They found that a representations-learningbased approach outperformed unadapted models and other baselines. For unsupervised speaker adaptation, Fan et al.  [184]  used multi-speaker DNNs to takes advantage of shared hidden representation and achieved improved results. Many researchers exploited DNN models for learning transferable representations in multi-lingual ASR  [185] ,  [186] . Cross-lingual transfer learning is important for the practical application, and it has been found that learnt features can be transferred to improve the performance of both resource-limited and resource-rich languages  [172] . The representations learnt in this way are referred to as bottleneck features and these can be used to train models for languages even without any transcriptions  [187] . Recently, adversarial learning of representations for domain adaptation methods is becoming very popular. Researchers trained different adversarial models and were able to improve the robustness against noise  [188] , adaptation of acoustic models for accented speech  [189] ,  [190] , gender variabilities  [191] , and speaker and environment variabilities  [192] -  [195] . These studies showed that representation learning using the adversarially trained models can improve the ASR performance on unseen domains.\n\nIn SR, Shon et al. used DAE to minimise the mismatch between the training and testing domain by utilising out-ofdomain information  [196] . Interestingly, domain adversarial training is utilised by Wang et al.  [197]  to learn speakerdiscriminative representations. Authors empirically showed that the adversarial training help to solve dataset mismatch problem and outperform other unsupervised domain adaptation methods. Similarly, a GAN is recently utilised by Bhattacharya et al.  [198]  to learn speaker embeddings for a domain robust end-toend speaker verification system. They achieved significantly better results over the baseline.\n\nIn SER, domain adaptation methods are also very popular to enable the system to learn representations that can be used to perform emotion identification across different corpora and different languages. Deng et al.  [199]  used AE with shared hidden layers to learn common representations for different emotional datasets. These authors were able to minimise the mismatch between different datasets and able to increase the performance. In another study  [200] , the authors used a Universum AE for cross-corpus SER. They were able to learn more generalised representations using the Universum AE, which achieves promising results compared to standard AEs. Some studies exploited GANs for SER. For instance, Wang et al.  [197]  used adversarial training to capture common representations for both the source and target language data. Zhou et al.  [201]  used a class-wise domain adaptation method using adversarial training to address cross-corpus mismatch issues and showed that adversarial training is useful when the model is to be trained on target language with minimal labels. Gideon et al.  [202]  used an adversarial discriminative domain generalisation method for cross-corpus emotion recognition and achieved better results. Similarly,  [163]  utilised GANs in an unsupervised way to learn language invariant, and evaluated over four different language datasets. They were able to significantly improve the SER across different language using language invariant features.\n\n2) Multi-Task Learning: Multi-task learning (MTL) has led to successes in different applications of ML, from NLP  [203]  and speech recognition  [204]  to computer vision  [205] . MTL aims to optimise more than one loss function in contrast to single-task learning and uses auxiliary tasks to improve on the main task of interest  [206] . Representations learned in MTL scenario become more generalised, which are very important in the field of speech processing, since speech contains multidimensional information (message, speaker, gender, or emotion) that can be used as auxiliary tasks. As a result, MTL increases performance without requiring external speech data.\n\nIn ASR, researchers have used MTL with different auxiliary tasks including gender  [207] , speaker adaptation  [208] ,  [209] , speech enhancement  [210] ,  [211] , etc. Results in these studies have shown that learning shared representations for different tasks act as complementary information about the acoustic environment and gave a lower word error rate (WER). Similar to ASR, researchers also explored MTL in SER with significantly improved results  [165] ,  [212] . For SER, studies used emotional attributes (e. g., arousal and valance) as auxiliary tasks  [213] -  [216]  as a way to improve the performance of the system. Other auxiliary tasks that researchers considered in SER are speaker and gender recognition  [165] ,  [217] ,  [218]  to improve the accuracy of the system compared to single-task learning.\n\nMTL is an effective approach to learn shared representation that leads to no major increase of the computational power, while it improves the recognition accuracy of a system and also decreases the chance of overfitting [?],  [165] . However, MTL implies the preparation of labels for considered auxiliary tasks. Another problem that hinders MTL is dealing with temporality differences among tasks. For instance, the modelling of speaker recognition requires different temporal information than phenom recognition does  [219] . Therefore, it is viable to use memory-based deep neural networks like the recurrent networks-ideally with LSTM or GRU cells-to deal with this issue.\n\n3) Self-Taught Learning: Self-taught learning  [220]  is a new paradigm in ML, which combines semi-supervised and TL. It utilises both labelled and unlabelled data, however, unlabelled data do not need to belong to the same class labels or generative distribution as the labelled data. Such a loose restriction on unlabelled data in self-taught learning significantly simplifies learning from a huge volume of unlabelled data. This fact This work is the extended version of paper accepted in IEEE Transactions on Affective Computing 2021.\n\nhttps://ieeexplore.ieee.org/document/9543566 differentiates it from semi-supervised learning.\n\nWe found very few studies on audio based applications using self-taught learning. In  [221] , the authors used self-taught learning and developed an assistive vocal interface for users with a speech impairment. The designed interface is maximally adapted using self-taught learning to the end-users and can be used for any language, dialect, grammar, and vocabulary. In another study  [222] , the authors proposed an AE-based sample selection method using self-taught learning. They selected highly relevant samples from unlabelled data and combined with training data. The proposed model was evaluated on four benchmark datasets covering computer vision, NLP, and speech recognition with results showing that the proposed framework can decrease the negative transfer while improving the knowledge transfer performance in different scenarios.",
      "page_start": 10,
      "page_end": 12
    },
    {
      "section_name": "E. Reinforcement Learning",
      "text": "Reinforcement Learning (RL) follows the principle of behaviourist psychology where an agent learns to take actions in an environment and tries to maximise the accumulated reward over its lifetime. In a RL problem, the agent and its environment can be modelled being in a state s ∈ S and the agent can perform actions a ∈ A, each of which may be members of either discrete or continuous sets and can be multi-dimensional. A state s contains all related information about the current situation to predict future states. The goal of RL is to find a mapping from states to actions, called policy π, that picks actions a in given states s by maximising the cumulative expected reward. The policy π can be deterministic or probabilistic. RL approaches are typically based on the Markov decision process (MDP) consisting of the set of states S, the set of actions A, the rewards R, and transition probabilities T that capture the dynamics of a system. RL has been repeatedly successful in solving various problems  [223] . Most importantly, deep RL that combines deep learning with RL principles. Methods such as deep Q-learning have significantly advanced the field  [224] .\n\nFew studies used RL-based approaches to learn representations. For instance, in  [225] , the authors introduced DeepMDP, a parameterised latent space model that is trained by minimising two tractable latent space losses including prediction of rewards and prediction of the distribution over the next latent states. They showed that the optimisation of these two objectives guarantees the quality of the embedding function as a representation of the state space. They also show that utilising DeepMDP as an auxiliary task in the Atari 2600 domain leads to large performance improvements. Zhang et al.  [226]  used RL for learning optimised structured representation learning from text. They found that an RL model can learn task-friendly representations by identifying taskrelevant structures without any explicit structure annotations, which yields competitive performance.\n\nRecently, RL is also gaining interest in the speech community and researchers have proposed multiple approaches to model different speech problems. Some of the popular RL-based solutions include dialog modelling and optimisation  [227] ,  [228] , speech recognition  [229] , and emotion recognition  [230] .\n\nHowever, the problem of representation learning of speech signals is not explored using RL.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "F. Active Learning And Cooperative Learning",
      "text": "Active Learning aims at achieving improved accuracy with fewer training samples by selecting data from which it learns. This idea of cleverly picking training samples rather than random selection gives better predictive models with less human effort for labelling data  [231] . An active learner selects samples from a large pool of unlabelled data and subsequently asks queries to an oracle (e.g., human annotator) for labelling. In speech processing, accurate labelling of speech utterances is extremely important and time-consuming. It has larger abundantly available unlabelled data. In this situation, active learning can help by allowing the learning model to select the samples from it learns, which leads to better performance with less training. Studies (e.g.,  [232] ,  [233] ) utilised classical ML-based active learning for ASR with the aim to minimise the effort required in transcribing and labelling data. However, it has been showed in  [234] ,  [235]  that utilisation of deep models for active learning in speech processing can improve the performance and significantly reduce the requirement of labelled data.\n\nCooperative learning  [235] ,  [236]  combines active and semisupervised learning to best exploit available data. It is an efficient way of sharing the labelling work between human and machine which leads to reduce the time and cost of human annotation  [237] . In cooperative learning, predicted samples with insufficient confidence value are subjected to human annotations and other with with high confidence values are labelled by machines. The models trained via cooperative learning perform better compared to active or semi-supervised learning  [235] . In speech processing, a few studies utilised ML-based cooperative learning and showed its potential to significantly reduce data annotation efforts. For instance,  [238] , authors applied cooperative learning speed up the process of annotation of large multi-modal corpora. Similarly, the proposed model in  [235]  achieved the same performance with 75% fewer labelled instances compared to the model trained on the whole training data. These finding shows the potential of cooperative learning in speech processing, however, DL-based representation learning methods need to be investigated in this setting.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "G. Summarising The Findings",
      "text": "A summary of various representation learning techniques has been presented in Table  V . We segregated the studies based on the learning techniques used to train the representation learning models. Studies on supervised learning methods typically use models to learn discriminative and noise-robust representations. Supervised training of models like CNNs, LSTM/GRU RNNs, and CNN-LSTM/GRU-RNNs are widely exploited for learning of representations from raw speech.\n\nUnsupervised learning is to learn patterns in the data. We covered unsupervised representation learning for three speech applications. Autoencoding networks are widely used in unsupervised feature learning from speech. Most importantly, This work is the extended version of paper accepted in IEEE Transactions on Affective Computing 2021.\n\nhttps://ieeexplore.ieee.org/document/9543566\n\nDAEs are very popular due to their denoising abilities. They can learn high-level representations from speech that are robust to noise corruption. Some studies also exploited AEs and RBM based architectures for unsupervised feature learning due to their non-linear dimension reduction and long-range of features extraction abilities. Recently, VAEs are becoming very popular in learning speech representation due to their generative nature and distribution learning abilities. They can learn salient and robust features from speech that are very essential for speech applications including ASR, SR, and SER. Semi-supervised representation learning models are widely used in SER because speech corpora have smaller sizes compared to ASR and SR. These studies tried to exploit additional data to improve the performance of SER. The popular models include AE-based models  [165] ,  [171]  and other discriminative architectures  [167] ,  [239] . In ASR, semisupervised learning is mostly exploited for learning noise-robust representations  [240] ,  [241]  and for feature extraction  [172] ,  [173] ,  [242] .\n\nTransfer learning methods-especially domain adaptation and MTL-are very popular in ASR, SR, and SER. The domain adaptations methods in ASR, SER, and SR are mainly used to achieve adaptation by learning such representations that are robust against noise, speaker, and language and corpus difference. In all these speech applications, adversarially learnt representations are found to better solve the issue of domain mismatch. MTL methods are most popular in SER, where researchers tried to utilise additional information available (e. g., speaker, or gender) in speech to learn more generalised representations that help to improve the performance.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Vi. Challenges For Representation Learning",
      "text": "In this section, we discuss the challenges faced by representation learning. The summary of these challenges is presented in Figure  4 .",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "A. Challenge Of Training Deep Architectures",
      "text": "Theoretical and empirical evidence show the deep models usually have superior performance over classical machine learning techniques. It is also empirically validated that deep learning models require much more data to learn certain attributes efficiently  [339] . For instance, adding top-5 layers in the network on the 1000-class ImageNet dataset trained network has increased the accuracy from ∼84 %  [104]  to ∼95 %  [340] . However, training deep learning models is not straightforward; it becomes considerably more difficult to optimise a deeper network  [341] ,  [342] . For deeper models, network parameters become very large and tuning of different hyper-parameter is also very difficult. Due to the availability of modern graphics processing units (GPUs) and recent advancement in optimisation  [343]  and training strategies  [344] ,  [345] , the training of DNNs considerably accelerated; however, it is still an open research problem.\n\nTraining of representation learning models is a more tricky and difficult task. Learning high-level abstraction means more non-linearity and learning representations associated with input manifolds becoming even more complex if the model might need to unfold and distort complicated input manifolds. Learning such representation which involves disentangling and unfolding of complex manifolds requires more intense and difficult training  [1] . Natural speech has very complex manifolds  [346] ) and inherently contains information about the message, gender, age, health status, personality, friendliness, mood, and emotion. All of this information is entangled together  [347] , and the disentanglement of these attributes in some latent space is a very difficult task that requires extensive training. Most importantly, the training of unsupervised representation learning models is much more difficult in contrast to supervised ones. As highlighted in  [1] , in supervised learning, there is a clear objective to optimise. For instance, the classifiers are trained to learn such representations or features that minimise the misclassifications error. Representation learning models do not have such training objectives like classification or regression problems do.\n\nAs outlined, GANs are a novel approach for generative modelling, they aim to learn the distribution of real data points. In recent years, they have been widely utilised for representation learning in different fields including speech. However, they also proved difficult to train and face different failure modes, mainly vanishing gradients issues, convergence problems, and mode collapse issues. Different remedies are proposed to tackle these issues. For instance, modified minimax loss  [11]  can help to deal with vanishing gradients, the Wasserstein loss  [348]  and training of ensembles alleviate mode collapse, and noise addition to the discriminator inputs  [349]  or penalising discriminator weights  [350]  act as regularisation to improve a GAN's convergence. These are some earlier attempts to solve these issues; however, there is still room to improve the GANs training problems.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "B. Performance Issues Of Domain Invariant Features",
      "text": "To achieve generalisation in DL models, we need a large amount of data with similar training and testing examples. However, the performance of DL models drops significantly if test samples deviate from the distribution of the training data. Learning speech representations that are invariant to variabilities in speakers, language, etc., are very difficult to capture.   [106] ,  [142] ), DNN-LSTM  [143] ,  [299]  Unsupervised ASR To learn speech feature and and noise robust representation.\n\nDBNs (  [136] ), DNNs (  [300] ) CNNs (  [301] ), LSTM (  [302] ) AEs (  [303] ), VAEs (  [183] ) DAEs ( [147]-  [149] ,  [304] ,  [305] ) SR DBNs (  [136] ), DAEs (  [306] ), VAEs (  [46] ,  [146] ,  [307] ,  [308] ), AL (  [161] ), GAN (  [309] ) SER AEs ( [152]-  [154] ,  [310] ), DAEs  [155] ,  [156] , VAEs  [79] ,  [311]  DNNs  [315] ,  [316] , AEs (  [182] ), VAEs (  [46] ) SR AL (  [197] ), DAE (  [196] ), GANs (  [198] ) SER DBNs  [317] , CNNs (  [318] ), AL (  [201] ,  [319] ,  [320] ), AEs  [118] ,  [199] ,  [200] ,  [321] ,  [322]  Multi-Task Learning ASR To learn common representations using multi-objective training.\n\nDNNs  [323] ,  [324] , RNNs ( [325]-  [327] ), AL (  [190] ) SR DNNs (  [328] ,  [329] ), CNNs (  [330] ), RNNs (  [331] ) SER DBNs (  [332] ), DNNs (  [214] ,  [216] ,  [333] ,  [334] ), CNN-LSTM  [335] , LSTM (  [213] ,  [217] ,  [336] ,  [337] ), AL (  [338] ), GANs (  [157] ) not work well to another corpus having different recording conditions. This issue is common to all three applications of speech covered in this paper. In the past few years, researchers have achieved competitive performance by learning speaker invariant representations  [210] ,  [351] . However, language invariant representation is still very challenging. The main reason is that we have speech corpora covering only a few languages in contrast to the number of spoken languages in the world. There are more than 5 000 spoken languages in the world, but only 389 languages account for 94 % of the world's population 1 . We do not have speech corpora even for 389 languages to enable across language speech processing research. This variation, imbalance, diversity, and dynamics in speech and language corpora are causing hurdles to designing generalised representation learning algorithms.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "C. Adversary On Representation Learning",
      "text": "DL has undoubtedly offered tremendous improvements in the performance of state-of-the-art speech representation learning systems. However, recent works on adversarial examples pose enormous challenges for robust representation learning from speech by showing the susceptibility of DNNs to adversarial examples having imperceptible perturbations  [86] . Some popular adversarial attacks include the fast gradient sign method (FGSM)  [352] , Jacobian-based saliency map attack (JSMA) 1 https://www.ethnologue.com/statistics  [353] , and DeepFool  [354] ; they compute the perturbation noise based on the gradient of targeted output. Such attacks are also evaluated against speech-based systems. For instance, Carlini and Wagner  [355]  evaluated an iterative optimisation-based attack against DeepSpeech  [356]  (a state-of-the-art ASR model) with 100 % success rate. Some other attempts also proposed different adversarial attacks  [357] ,  [358]  and against speechbased systems. The success of adversarial attacks against DL models shows that the representations learnt by them are not good  [359] . Therefore, research is ongoing to tackle the challenge of adversarial attacks by exploring what DL models capture from the input data and how adversarial examples can be defined as a combination of previously learnt representations without any knowledge of adversaries.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "D. Quality Of Speech Data",
      "text": "Representation learning models aim to identify potentially useful and ultimately understandable patterns. This demands not just more data, but more comprehensive and diverse data. Therefore, for learning a good representation, data must be correct and properly labelled, and unbiased. The quality of speech data can be poor due to various reasons. For example, different background noises and music can corrupt the speech data. Similarly, the noise of microphones or recording devices can also pollute the speech signal. Although studies use 'noise injection' techniques to avoid overfitting, this works for moderately high signal-to-noise ratios  [360] . This has been This work is the extended version of paper accepted in IEEE Transactions on Affective Computing 2021.\n\nhttps://ieeexplore.ieee.org/document/9543566 an active research topic and in the past few years, different DL models have been proposed that can learn representations from noisy data. For instance, DAEs  [119]  can learn a representation of data with noise, imputation AE  [361]  can learn a representation from incomplete data, and non-local AE  [362]  can learn reliable features from corrupted data. Such techniques are also very popular in the speech community for noise invariant representation learning and we highlight this in Table  V . However, there is still a need for such DL models that can deal with the quality of data not only for speech but also for other domains.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Vii. Recent Advancements And Future Trends",
      "text": "",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "A. Open Source Datasets And Toolkits",
      "text": "There are a large number of speech databases available for speech analysis research. Some of the popular benchmark datasets including TIMIT  [54] , WSJ  [56] , AMI  [57] , and many other databases are not freely available. They are usually purchased from commercial organisations like LDC 2  , ELRA  3  , and Speech Ocean 4  . The licence fees of these datasets are affordable for most of the research institutes; however, their fee is expensive (e. g., the WSJ corpus license costs 2500 USD) for young researchers who want to start their research on speech, particularly for researchers in developing countries. Recently, a free data movement is started in the speech community and different good quality datasets are made free for the public to invoke more research in this field. VoxForge 5  and OpenSLR 6  are two popular platforms that contain freely available speech and speaker recognition datasets. Most of the SER corpora are developed by research institutes and they are freely available for research proposes.\n\nAnother important progress made by researchers of the speech community is the development of open-source toolkits for speech processing and analysis. These tools help the researchers not only for feature extraction, but also for the development of models. The details of such tools is presented in a tabular form in Table  VI . It can be noted that ASR -as the largest field of activity-has more open source toolkits compared to SR and SER. The development of such toolkits and speech corpus is providing great benefits to the speech research community and will continue to be needed to speed up the research progress on speech.",
      "page_start": 15,
      "page_end": 16
    },
    {
      "section_name": "B. Computational Advancements",
      "text": "In contrast to classical ML models, DL has a significantly larger number of parameters and involves huge amounts of matrix multiplications with many other operations. Traditional central processing units (CPUs) support such processing, therefore, advanced parallel computing is necessary for the development of deep networks. This is achieved by utilisation of graphics processing units (GPUs), which contain thousands English plus 10 other languages Kaldi  [367]  C++, Python English Julius  [368]  C, Python Japanese\n\nESPnet  [369]  Python English, Japanese, Mandarin HTK  [370]  C, Python None Speaker Identification ALIZE  [371]  C++ English Speech Emotion Recognition OpenEAR  [372]  C++ English, German of cores that can perform exceptionally fast matrix multiplications. In contrast to CPUs and GPUs, advanced Tensor Processing Units (TPUs) developed by Google offer 15-30 × higher processing speeds and 30-80 × higher performanceper-watt  [373] . A recent paper  [374]  on quantum supremacy using programmable superconducting processor shows amazing results by performing computation a Hilbert space of dimension (253 ≈ 9 × 1015) far beyond the reach of the fastest supercomputers available today. It was the first computation on a quantum processor. This will lead to more progress and the computational power will continue to grow at a doubleexponential rate. This will disrupt the area of representation learning from a vast amount of unlabelled data by unlocking new computational capabilities of quantum processors.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "C. Processing Raw Speech",
      "text": "In the past few years, the trend of using hand-engineered acoustic features is progressively changing and DL is gaining popularity as a viable alternative to learn from raw speech directly. This has removed the feature extraction module from the pipeline of the ASR, SR, and SER systems. Recently, important progress is also made by Donahue et al.  [159]  in audio generation. They proposed WaveGAN for the unsupervised synthesis of raw-waveform audio and showed that their model can learn to produce intelligible words when trained on a small vocabulary speech dataset, and can also synthesise music audio and bird vocalisations. Other recent works  [375] ,  [376]  also explored audio synthesis using GANs; however, such work is at the initial stage and will likely open new prospects of future research as it transpired with the use of GANs in the domain of vision (e. g., with DeepFakes  [377] ).\n\nThis work is the extended version of paper accepted in IEEE Transactions on Affective Computing 2021.\n\nhttps://ieeexplore.ieee.org/document/9543566",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "D. The Rise Of Adversarial Training",
      "text": "The idea of adversarial training was proposed in 2014  [11] . It leads to widespread research in various ML domains including speech representation learning. Speech-based systems-principally, ASR, SR, and SER systems-need to be robust under environmental acoustic variabilities arising from environmental, speaker, and recording conditions. This is very crucial for industrial applications of these systems.\n\nGANs are being used as a viable tool for robust speech representation learning  [255]  and also speech enhancement  [275]  to tackle the noise issues. A popular variant of GAN, cycle-consistent Generative Adversarial Networks (CycleGAN)  [378]  is being used for domain adaptation for low-resource scenarios (where a limited amount of target data is available for adaptation)  [379] . These results using CycleGANs on speech are very promising for domain adaptation. This will also lead to designing such systems that can learn domain invariant representation learning, especially for zero-resource languages to enable speech-based cross-culture applications.\n\nAnother interesting utilisation of GANs is learning from synthetic data. Researchers succeeded in the synthesis of speech signals also by GANs  [159] . Synthetic data can be utilised for such applications where large label data is not available. In SER, larger labelled data is not available. Learning representation from data can help to improve the performance of system and researchers have explored the use of synthetic data for SER  [160] ,  [380] . This shows the feasibility of learning from synthetic data and will lead to interesting research to solve the problems in the speech domain where data scarcity is a major problem.",
      "page_start": 16,
      "page_end": 17
    },
    {
      "section_name": "E. Representation Learning With Interaction",
      "text": "Good representation disentangles the underlying explanatory factors of variation. However, it is an open research question that what kind of training framework can potentially learn disentangled representations from input data. Most of the research work on representation learning used static settings without involving the interaction with the environment. Reinforcement learning (RL) facilitates the idea of learning while interacting with the environment. If RL is used to disentangle factors of variation by interacting with the environment, a good representation can be learnt. This will lead to faster convergence, in contrast, to blindly attempting to solve given problems. Such an idea has recently been validated by Thomas et al.  [381] , where the authors used RL to disentangle the independently controllable factors of variation by using a specific objective function. The authors empirically showed that the agent can disentangle these aspects of the environment without any extrinsic reward. This is an important finding that will act as the key to further research in this direction.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "F. Privacy Preserving Representations",
      "text": "When people use speech-based services such as voice authentication or speech recognition, they grant complete access to their recordings. These services can extract user's information such as gender, ethnicity, and emotional state and can be used for undesired purposes. Various other privacyrelated issues arise while using speech-based services  [382] . It is desirable in speech processing applications that there are suitable provisions for ensuring that there is no unauthorised and undisclosed eavesdropping and violation of privacy. Privacy preserved representation learning is a relatively unexplored research topic. Recently, researchers have started to utilise privacy-preserving representation learning models to protect speaker identity  [383] , gender identity  [384] . To preserve users' privacy, federated learning  [385]  is another alternative setting where the training of a shared global model is performed using multiple participating computing devices. This happens under the coordination of a central server, however, the training data remains decentralised.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Viii. Conclusions",
      "text": "In this article, we have focused on providing a comprehensive review of representation learning for speech signals using deep learning approaches in three principal speech processing areas: automatic speech recognition (ASR), speaker recognition (SR), and speech emotion recognition (SER). In all of these three areas, the use of representation learning is very promising, and there is an ongoing research on this topic in which different models and methods are being explored to disentangle speech attributes suitable for these tasks. The literature review performed in this work shows that LSTM/GRU-RNNs in combination with CNNs are suitable for capturing speech attributes. Most of the studies have used LSTM models in a supervised way. In unsupervised representation learning, DAEs and VAEs are widely deployed architectures in the speech community, with GAN-based models also attaining prominence for speech enhancement and feature learning. Apart from providing a detailed review, we have also highlighted the challenges faced by researchers working with representation learning techniques and avenues for future work. It is hoped that this article will become a definitive guide to researchers and practitioners interested to work either in speech signal or deep representation learning in general. We are curious whether in the longer run, representation learning will be the standard paradigm in speech processing. If so, we are currently witnessing the change of a paradigm moving away from signal processing and expert-crafted features into a highly data-driven era-with all its advantages, challenges, and risks.",
      "page_start": 16,
      "page_end": 17
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: II. BACKGROUND",
      "page": 2
    },
    {
      "caption": "Figure 1: Organisation of the paper.",
      "page": 3
    },
    {
      "caption": "Figure 2: Fig. 2: Applications of deep representation learning.",
      "page": 4
    },
    {
      "caption": "Figure 3: Representation Learning Techniques.",
      "page": 8
    },
    {
      "caption": "Figure 3: shows the different",
      "page": 8
    },
    {
      "caption": "Figure 4: Fig. 4: Challenges of representation learning.",
      "page": 13
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Paper": "",
          "Focus": "Representation\nLearning",
          "Details": ""
        },
        {
          "Paper": "",
          "Focus": "",
          "Details": ""
        },
        {
          "Paper": "Bengio et al.\n[1]\n2013",
          "Focus": "",
          "Details": "This paper\nreviewed the work in the area of unsupervised feature learning\nand deep learning,\nit also covered advancements in probabilistic models\nand autoencoders.\nIt does not\ninclude recent models like VAE and GANs."
        },
        {
          "Paper": "Zhong et al.\n2016 [6]",
          "Focus": "",
          "Details": "In this paper,\nthe history of data representation learning is reviewed from\ntraditional\nto recent DL methods. Challenges for\nrepresentation learning,\nrecent advancement, and future trends are not covered."
        },
        {
          "Paper": "Zhang et al.\n[7]\n2018",
          "Focus": "",
          "Details": "This paper provides a systematic overview of\nrepresentative DL approaches\nthat are designed for environmentally robust ASR."
        },
        {
          "Paper": "Swain et al.\n[8]\n2018",
          "Focus": "(cid:55)",
          "Details": "This paper\nreviewed the literature on various databases,\nfeatures, and\nclassiﬁers for SER system."
        },
        {
          "Paper": "Nassif et al.\n[9]\n2019",
          "Focus": "(cid:55)",
          "Details": "This paper presented a systematic review of studies from 2006 to 2018 on\nDL based speech recognition and highlighted the on the trends of\nresearch in ASR."
        },
        {
          "Paper": "Our paper",
          "Focus": "",
          "Details": "Our paper covers different\nrepresentation learning techniques from\nspeech, DL models, discusses different challenges, highlights recent\nadvancements and future trends. The main contribution of\nthis paper\nis to\nbring together scattered research on representation learning of speech\nacross three research areas: ASR, SR, and SER."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Application": "Speech and\nSpeaker\nRecognition",
          "Corpus": "LibriSpeech [62]",
          "Language": "English",
          "Mode": "Audio",
          "Size": "1 000 hours of speech\nof 2 484 speakers",
          "Details": "Designed for speech recognition and also used for speaker\nidentiﬁcation\nand veriﬁcation."
        },
        {
          "Application": "",
          "Corpus": "VoxCeleb2 [63]",
          "Language": "Multiple",
          "Mode": "Audio-Visual",
          "Size": "1 128 246 utterances\nof 6 112 celebrities",
          "Details": "This data is extracted from videos uploaded to YouTube and designed for\nspeaker\nidentiﬁcation and veriﬁcation."
        },
        {
          "Application": "",
          "Corpus": "TED-LIUM [64]",
          "Language": "English",
          "Mode": "Audio",
          "Size": "118 hours of speech\nof 698 speakers",
          "Details": "This corpus is extracted from 818 TED Talks for ASR."
        },
        {
          "Application": "",
          "Corpus": "THCHS-30 [65]",
          "Language": "Chinese",
          "Mode": "Audio",
          "Size": "30 hours of speech\nfrom 30 speakers",
          "Details": "This corpus is recorded for Chinese speech recognition."
        },
        {
          "Application": "",
          "Corpus": "AISHELL-1 [66]",
          "Language": "Mandarin",
          "Mode": "Audio",
          "Size": "170 hours of speech\nfrom 400 speakers",
          "Details": "An open source Mandarin ASR corpus."
        },
        {
          "Application": "",
          "Corpus": "Tuda-De [67]",
          "Language": "German",
          "Mode": "Audio",
          "Size": "127 hour of speech\nfrom 147 speakers",
          "Details": "A corpus of German utterances was publicly released for distant\nspeech recognition."
        },
        {
          "Application": "Speech\nEmotion\nRecogntion",
          "Corpus": "EMO-DB [58]",
          "Language": "German",
          "Mode": "Audio",
          "Size": "10 actors and\n494 utterances",
          "Details": "An acted corpus on 10 German sentences which which usually\nused in everyday communication."
        },
        {
          "Application": "",
          "Corpus": "SEMAINE [68]",
          "Language": "English",
          "Mode": "Audio-Visual",
          "Size": "150 participants and\n959 conversations",
          "Details": "An induced corpus recorded to build sensitive artiﬁcial\nlistener agents\nthat can engage a person in a sustained and emotionally coloured conversation."
        },
        {
          "Application": "",
          "Corpus": "IEMOCAP [69]",
          "Language": "English",
          "Mode": "Audio-Visual",
          "Size": "12 hours of speech\nfrom 10 speakers",
          "Details": "To collect\nthis data, an interactive setting served to elicit authentic emotions\nand create a larger emotional corpus to study multimodal\ninteractions."
        },
        {
          "Application": "",
          "Corpus": "MSP-IMPROV [70]",
          "Language": "English",
          "Mode": "Audio-Visual",
          "Size": "9 hours of audiovisual\ndata of 12 actors",
          "Details": "This corpus is recorded from dyadic interactions of actors to\nstudy emotions."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "",
          "Characteristics": "",
          "Applications in Speech Processing": "Automatic\nFeature Learning",
          "References": ""
        },
        {
          "Model": "DNNs",
          "Characteristics": "Good for\nlearning a hierarchy of\nrepresentations. They can learn\ninvariant and discriminative representations. Features learnt\nby DNNs are more generalised compared to traditional methods.",
          "Applications in Speech Processing": "",
          "References": "[124]"
        },
        {
          "Model": "CNNs",
          "Characteristics": "Originated from image recognition and were also extended for NLP\nand speech. They can learn a high-level abstraction from speech.",
          "Applications in Speech Processing": "",
          "References": "[104]\n[105]"
        },
        {
          "Model": "RNNs",
          "Characteristics": "Good for sequential modelling. They can learn temporal structures\nfrom speech and outperformed DNNs",
          "Applications in Speech Processing": "",
          "References": "[111]\n[125]"
        },
        {
          "Model": "AEs",
          "Characteristics": "Powerful unsupervised representation learning models that\nencode the data in sparse and compress representations.",
          "Applications in Speech Processing": "",
          "References": "[1]\n[119]"
        },
        {
          "Model": "VAEs",
          "Characteristics": "Stochastic variational\ninference and learning model. Popular\nin learning disentangled representations from speech.",
          "Applications in Speech Processing": "",
          "References": "[10]"
        },
        {
          "Model": "GANs",
          "Characteristics": "Game-theoretical\nframework and very powerful\nfor data generation\nand robust\nto overﬁtting. They can learn disentangled representation\nthat are very suitable for speech analysis.",
          "Applications in Speech Processing": "",
          "References": "[11]\n[126]"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Types/Aspect": "Attributes",
          "Supervised Learning": "•\nLearn explicitly\n• Data with labels\n• Direct\nfeedback is given\n•\nPredict outcome/future\n• No exploration",
          "Unsupervised Learning": "•\nLearn patterns and\nstructure\n• Data without\nlabels\n• No direct\nfeedback\n• No prediction\n• No exploration",
          "Semi-Supervised Learning": "•\nBlend on both supervised\nand unsupervised\n• Data with and without\nlabels\n• Direct\nfeedback is given\n•\nPredict outcome/future\n• No exploration",
          "Transfer Learning": "•\nTransfer knowledge from\none supervised task to other\n•\nLabelled data for\ndifferent\ntask\n• Direct\nfeedback is given\n•\nPredict outcome/future\n• No exploration",
          "Reinforcement Learning": "•\nReward based learning\n•\nPolicy making with\nfeedback\n•\nPredict outcome/future\n• Adaptable to changes\nthrough exploration"
        },
        {
          "Types/Aspect": "Uses",
          "Supervised Learning": "•\nClassiﬁcation\n•\nRegression",
          "Unsupervised Learning": "•\nClustering\n• Association",
          "Semi-Supervised Learning": "•\nClassiﬁcation\n•\nClustering",
          "Transfer Learning": "•\nClassiﬁcation\n•\nRegression",
          "Reinforcement Learning": "•\nClassiﬁcation\n•\nControl"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Learning Type": "Supervised",
          "Applications": "ASR",
          "Aim": "To learn discriminative and robust\nrepresentation",
          "Models": "DBNs (\n[140],\n[243]–[248]), DNNs (\n[124],\n[249]–[252]),\nCNNs [40],\n[253],\n[254],\nGANs (\n[255])"
        },
        {
          "Learning Type": "",
          "Applications": "SR",
          "Aim": "",
          "Models": "DBNs (\n[137],\n[246],\n[256]–[259]), DNNs [260]–[264],\nCNNs (\n[63],\n[265]–[269]), LSTM (\n[270]–[273])\nCNN-RNNs (\n[274]), GANs [275]"
        },
        {
          "Learning Type": "",
          "Applications": "SER",
          "Aim": "",
          "Models": "DNNs (\n[276]–[278]), RNNs (\n[279],\n[280]),\nCNNs (\n[281]–[284]),\nCNN-RNNs (\n[285]–[287])"
        },
        {
          "Learning Type": "",
          "Applications": "ASR",
          "Aim": "To learn representation from raw speech",
          "Models": "CNNs (\n[107],\n[107],\n[108],\n[288]–[291]),\nCNN-LSTM (\n[144],\n[292]–[294])"
        },
        {
          "Learning Type": "",
          "Applications": "SR",
          "Aim": "",
          "Models": "CNNs (\n[295]–[297]), CNN-LSTM (\n[298])"
        },
        {
          "Learning Type": "",
          "Applications": "SER",
          "Aim": "",
          "Models": "CNN-LSTM (\n[41],\n[106],\n[142]),\nDNN-LSTM [143],\n[299]"
        },
        {
          "Learning Type": "Unsupervised",
          "Applications": "ASR",
          "Aim": "To learn speech feature and and noise\nrobust\nrepresentation.",
          "Models": "DBNs (\n[136]), DNNs (\n[300]) CNNs (\n[301]),\nLSTM (\n[302]) AEs (\n[303]), VAEs (\n[183])\nDAEs (\n[147]–[149],\n[304],\n[305])"
        },
        {
          "Learning Type": "",
          "Applications": "SR",
          "Aim": "",
          "Models": "DBNs (\n[136]), DAEs (\n[306]),\nVAEs (\n[46],\n[146],\n[307],\n[308]), AL (\n[161]), GAN (\n[309])"
        },
        {
          "Learning Type": "",
          "Applications": "SER",
          "Aim": "",
          "Models": "AEs (\n[152]–[154],\n[310]), DAEs [155],\n[156],\nVAEs [79],\n[311], AAEs (\n[160]), GANs (\n[312])"
        },
        {
          "Learning Type": "",
          "Applications": "ASR",
          "Aim": "To learn feature from raw speech.",
          "Models": "RBMs (\n[150]), VAEs [135], GANs (\n[159])"
        },
        {
          "Learning Type": "Semi-Supervised\nLearning",
          "Applications": "ASR",
          "Aim": "To learn speech feature representations\nin semi-supervised way.",
          "Models": "DNNs (\n[172],\n[173]), AEs (\n[174],\n[313]), GANs [314]"
        },
        {
          "Learning Type": "",
          "Applications": "SR",
          "Aim": "",
          "Models": "DNNs (\n[175]), GANs [176]"
        },
        {
          "Learning Type": "",
          "Applications": "SER",
          "Aim": "",
          "Models": "DNNs(\n[239]), CNNs (\n[167]), AEs (\n[171]),\nAAEs (\n[165])"
        },
        {
          "Learning Type": "Domain\nAdaptation",
          "Applications": "ASR",
          "Aim": "To learn representation to minimise the\nacoustic mismatch between the training\nand testing conditions.",
          "Models": "DNNs [315],\n[316], AEs (\n[182]), VAEs (\n[46])"
        },
        {
          "Learning Type": "",
          "Applications": "SR",
          "Aim": "",
          "Models": "AL (\n[197]), DAE (\n[196]), GANs (\n[198])"
        },
        {
          "Learning Type": "",
          "Applications": "SER",
          "Aim": "",
          "Models": "DBNs [317], CNNs (\n[318]), AL (\n[201],\n[319],\n[320]),\nAEs [118],\n[199],\n[200],\n[321],\n[322]"
        },
        {
          "Learning Type": "Multi-Task\nLearning",
          "Applications": "ASR",
          "Aim": "To learn common representations\nusing multi-objective training.",
          "Models": "DNNs [323],\n[324], RNNs (\n[325]–[327]), AL (\n[190])"
        },
        {
          "Learning Type": "",
          "Applications": "SR",
          "Aim": "",
          "Models": "DNNs (\n[328],\n[329]), CNNs (\n[330]), RNNs (\n[331])"
        },
        {
          "Learning Type": "",
          "Applications": "SER",
          "Aim": "",
          "Models": "DBNs (\n[332]), DNNs (\n[214],\n[216],\n[333],\n[334]),\nCNN-LSTM [335], LSTM (\n[213],\n[217],\n[336],\n[337]),\nAL (\n[338]), GANs (\n[157])"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Tools": "Librosa [363]",
          "Description": "A Python based toolkit\nfor music\nand audio analysis"
        },
        {
          "Tools": "pyAudioAnalysis\n[364]",
          "Description": "A Python library facilities a wide range of\nfeature extraction and also classiﬁcation of\naudio signals, supervised and unsupervised\nsegmentation and content visualisation."
        },
        {
          "Tools": "openSMILE [365]",
          "Description": "It enables to extract a large number of\naudio feature in real\ntime.\nIt\nis written in C++."
        },
        {
          "Tools": "Speech Recognition Toolkits",
          "Description": ""
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Toolkit": "CMU Sphinx [366]",
          "Programming\nLanguage": "Jave, C, Python,\nand others",
          "Trained Models": "English plus 10\nother\nlanguages"
        },
        {
          "Toolkit": "Kaldi\n[367]",
          "Programming\nLanguage": "C++, Python",
          "Trained Models": "English"
        },
        {
          "Toolkit": "Julius [368]",
          "Programming\nLanguage": "C, Python",
          "Trained Models": "Japanese"
        },
        {
          "Toolkit": "ESPnet\n[369]",
          "Programming\nLanguage": "Python",
          "Trained Models": "English, Japanese,\nMandarin"
        },
        {
          "Toolkit": "HTK [370]",
          "Programming\nLanguage": "C, Python",
          "Trained Models": "None"
        },
        {
          "Toolkit": "Speaker Identiﬁcation",
          "Programming\nLanguage": "",
          "Trained Models": ""
        }
      ],
      "page": 15
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Representation learning: A review and new perspectives",
      "authors": [
        "Y Bengio",
        "A Courville",
        "P Vincent"
      ],
      "year": "2013",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "2",
      "title": "Automatic speech recognition from neural signals: a focused review",
      "authors": [
        "C Herff",
        "T Schultz"
      ],
      "year": "2016",
      "venue": "Frontiers in neuroscience"
    },
    {
      "citation_id": "3",
      "title": "Fuzzy approaches to speech and speaker recognition",
      "authors": [
        "D Tran"
      ],
      "year": "2000",
      "venue": "Fuzzy approaches to speech and speaker recognition"
    },
    {
      "citation_id": "4",
      "title": "Deep learning applications and challenges in big data analytics",
      "authors": [
        "M Najafabadi",
        "F Villanustre",
        "T Khoshgoftaar",
        "N Seliya",
        "R Wald",
        "E Muharemagic"
      ],
      "year": "2015",
      "venue": "Journal of Big Data"
    },
    {
      "citation_id": "5",
      "title": "A historical perspective of speech recognition",
      "authors": [
        "X Huang",
        "J Baker",
        "R Reddy"
      ],
      "year": "2014",
      "venue": "Commun. ACM"
    },
    {
      "citation_id": "6",
      "title": "An overview on data representation learning: From traditional feature learning to recent deep learning",
      "authors": [
        "G Zhong",
        "L.-N Wang",
        "X Ling",
        "J Dong"
      ],
      "year": "2016",
      "venue": "The Journal of Finance and Data Science"
    },
    {
      "citation_id": "7",
      "title": "This work is the extended version of paper",
      "venue": "IEEE Transactions on Affective Computing 2021"
    },
    {
      "citation_id": "8",
      "title": "Deep learning for environmentally robust speech recognition: An overview of recent developments",
      "authors": [
        "Z Zhang",
        "J Geiger",
        "J Pohjalainen",
        "-D Mousa",
        "W Jin",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "ACM Transactions on Intelligent Systems and Technology (TIST)"
    },
    {
      "citation_id": "9",
      "title": "Databases, features and classifiers for speech emotion recognition: a review",
      "authors": [
        "M Swain",
        "A Routray",
        "P Kabisatpathy"
      ],
      "year": "2018",
      "venue": "International Journal of Speech Technology"
    },
    {
      "citation_id": "10",
      "title": "Speech recognition using deep neural networks: A systematic review",
      "authors": [
        "A Nassif",
        "I Shahin",
        "I Attili",
        "M Azzeh",
        "K Shaalan"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "11",
      "title": "Auto-encoding variational bayes",
      "authors": [
        "D Kingma",
        "M Welling"
      ],
      "year": "2013",
      "venue": "Auto-encoding variational bayes"
    },
    {
      "citation_id": "12",
      "title": "Generative adversarial nets",
      "authors": [
        "I Goodfellow",
        "J Pouget-Abadie",
        "M Mirza",
        "B Xu",
        "D Warde-Farley",
        "S Ozair",
        "A Courville",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "13",
      "title": "On the design of automatic voice condition analysis systems. part ii: Review of speaker recognition techniques and study on the effects of different variability factors",
      "authors": [
        "J Gómez-García",
        "L Moro-Velázquez",
        "J Godino-Llorente"
      ],
      "year": "2019",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "14",
      "title": "From shallow feature learning to deep learning: Benefits from the width and depth of deep architectures",
      "authors": [
        "G Zhong",
        "X Ling",
        "L.-N Wang"
      ],
      "year": "2019",
      "venue": "Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery"
    },
    {
      "citation_id": "15",
      "title": "Liii. on lines and planes of closest fit to systems of points in space",
      "authors": [
        "K Pearson"
      ],
      "year": "1901",
      "venue": "The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science"
    },
    {
      "citation_id": "16",
      "title": "The use of multiple measurements in taxonomic problems",
      "authors": [
        "R Fisher"
      ],
      "year": "1936",
      "venue": "Annals of eugenics"
    },
    {
      "citation_id": "17",
      "title": "Canonical correlation analysis: An overview with application to learning methods",
      "authors": [
        "D Hardoon",
        "S Szedmak",
        "J Shawe-Taylor"
      ],
      "year": "2004",
      "venue": "Neural computation"
    },
    {
      "citation_id": "18",
      "title": "Modern multidimensional scaling: Theory and applications",
      "authors": [
        "I Borg",
        "P Groenen"
      ],
      "year": "2003",
      "venue": "Journal of Educational Measurement"
    },
    {
      "citation_id": "19",
      "title": "Independent component analysis: algorithms and applications",
      "authors": [
        "A Hyvärinen",
        "E Oja"
      ],
      "year": "2000",
      "venue": "Neural networks"
    },
    {
      "citation_id": "20",
      "title": "Nonlinear component analysis as a kernel eigenvalue problem",
      "authors": [
        "B Schölkopf",
        "A Smola",
        "K.-R Müller"
      ],
      "year": "1998",
      "venue": "Neural computation"
    },
    {
      "citation_id": "21",
      "title": "Generalized discriminant analysis using a kernel approach",
      "authors": [
        "G Baudat",
        "F Anouar"
      ],
      "year": "2000",
      "venue": "Neural computation"
    },
    {
      "citation_id": "22",
      "title": "Learning the parts of objects by nonnegative matrix factorization",
      "authors": [
        "D Lee",
        "H Seung"
      ],
      "year": "1999",
      "venue": "Nature"
    },
    {
      "citation_id": "23",
      "title": "Feature mapping using PCA, locally linear embedding and isometric feature mapping for EEG-based brain computer interface",
      "authors": [
        "F Lee",
        "R Scherer",
        "R Leeb",
        "A Schlögl",
        "H Bischof",
        "G Pfurtscheller"
      ],
      "year": "2004",
      "venue": "Feature mapping using PCA, locally linear embedding and isometric feature mapping for EEG-based brain computer interface"
    },
    {
      "citation_id": "24",
      "title": "Nonlinear dimensionality reduction by locally linear embedding",
      "authors": [
        "S Roweis",
        "L Saul"
      ],
      "year": "2000",
      "venue": "science"
    },
    {
      "citation_id": "25",
      "title": "A global geometric framework for nonlinear dimensionality reduction",
      "authors": [
        "J Tenenbaum",
        "V Silva",
        "J Langford"
      ],
      "year": "2000",
      "venue": "science"
    },
    {
      "citation_id": "26",
      "title": "Visualizing data using t-sne",
      "authors": [
        "L Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "27",
      "title": "Gradient-based learning applied to document recognition",
      "authors": [
        "Y Lecun",
        "L Bottou",
        "Y Bengio",
        "P Haffner"
      ],
      "year": "1998",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "28",
      "title": "Kernel-based feature extraction with a speech technology application",
      "authors": [
        "A Kocsor",
        "L Tóth"
      ],
      "year": "2004",
      "venue": "IEEE Transactions on Signal Processing"
    },
    {
      "citation_id": "29",
      "title": "Pca-based speech enhancement for distorted speech recognition",
      "authors": [
        "T Takiguchi",
        "Y Ariki"
      ],
      "year": "2007",
      "venue": "Journal of multimedia"
    },
    {
      "citation_id": "30",
      "title": "Reducing the dimensionality of data with neural networks",
      "authors": [
        "G Hinton",
        "R Salakhutdinov"
      ],
      "year": "2006",
      "venue": "science"
    },
    {
      "citation_id": "31",
      "title": "Greedy layerwise training of deep networks",
      "authors": [
        "Y Bengio",
        "P Lamblin",
        "D Popovici",
        "H Larochelle"
      ],
      "year": "2007",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "32",
      "title": "Efficient learning of sparse representations with an energy-based model",
      "authors": [
        "C Poultney",
        "S Chopra",
        "Y Cun"
      ],
      "year": "2007",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "33",
      "title": "The application of hidden markov models in speech recognition",
      "authors": [
        "M Gales",
        "S Young"
      ],
      "year": "2008",
      "venue": "Foundations and Trends® in Signal Processing"
    },
    {
      "citation_id": "34",
      "title": "Phoneme recognition using time-delay neural networks",
      "authors": [
        "A Waibel",
        "T Hanazawa",
        "G Hinton",
        "K Shikano",
        "K Lang"
      ],
      "year": "1989",
      "venue": "IEEE transactions on acoustics, speech, and signal processing"
    },
    {
      "citation_id": "35",
      "title": "Deep learning for audio signal processing",
      "authors": [
        "H Purwins",
        "B Li",
        "T Virtanen",
        "J Schlüter",
        "S.-Y Chang",
        "T Sainath"
      ],
      "year": "2019",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "36",
      "title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups",
      "authors": [
        "G Hinton",
        "L Deng",
        "D Yu",
        "G Dahl",
        "A -R. Mohamed",
        "N Jaitly",
        "A Senior",
        "V Vanhoucke",
        "P Nguyen",
        "T Sainath"
      ],
      "year": "2012",
      "venue": "IEEE Signal processing magazine"
    },
    {
      "citation_id": "37",
      "title": "Long short-term memory networks for noise robust speech recognition",
      "authors": [
        "M Wöllmer",
        "Y Sun",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proc. INTERSPEECH 2010, Makuhari, Japan"
    },
    {
      "citation_id": "38",
      "title": "Abandoning emotion classes-towards continuous emotion recognition with modelling of long-range dependencies",
      "authors": [
        "M Wöllmer",
        "F Eyben",
        "S Reiter",
        "B Schuller",
        "C Cox",
        "E Douglas-Cowie",
        "R Cowie"
      ],
      "year": "2008",
      "venue": "Proc. 9th Interspeech 2008 incorp. 12th Australasian Int. Conf. on Speech Science and Technology SST 2008"
    },
    {
      "citation_id": "39",
      "title": "Phonocardiographic sensing using deep learning for abnormal heartbeat detection",
      "authors": [
        "S Latif",
        "M Usman",
        "R Rana",
        "J Qadir"
      ],
      "year": "2018",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "40",
      "title": "Quran reciter identification: A deep learning approach",
      "authors": [
        "A Qayyum",
        "S Latif",
        "J Qadir"
      ],
      "year": "2018",
      "venue": "2018 7th International Conference on Computer and Communication Engineering (ICCCE)"
    },
    {
      "citation_id": "41",
      "title": "Convolutional, long short-term memory, fully connected deep neural networks",
      "authors": [
        "T Sainath",
        "O Vinyals",
        "A Senior",
        "H Sak"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "42",
      "title": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis",
        "F Ringeval",
        "R Brueckner",
        "E Marchi",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2016",
      "venue": "2016 IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "43",
      "title": "A review of unsupervised feature learning and deep learning for time-series modeling",
      "authors": [
        "M Längkvist",
        "L Karlsson",
        "A Loutfi"
      ],
      "year": "2014",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "44",
      "title": "Pixel recurrent neural networks",
      "authors": [
        "A Oord",
        "N Kalchbrenner",
        "K Kavukcuoglu"
      ],
      "year": "2016",
      "venue": "Pixel recurrent neural networks",
      "arxiv": "arXiv:1601.06759"
    },
    {
      "citation_id": "45",
      "title": "Wavenet: A generative model for raw audio",
      "authors": [
        "A Oord",
        "S Dieleman",
        "H Zen",
        "K Simonyan",
        "O Vinyals",
        "A Graves",
        "N Kalchbrenner",
        "A Senior",
        "K Kavukcuoglu"
      ],
      "year": "2016",
      "venue": "Wavenet: A generative model for raw audio",
      "arxiv": "arXiv:1609.03499"
    },
    {
      "citation_id": "46",
      "title": "Generative adversarial networkbased glottal waveform model for statistical parametric speech synthesis",
      "authors": [
        "B Bollepalli",
        "L Juvela",
        "P Alku"
      ],
      "year": "2019",
      "venue": "Generative adversarial networkbased glottal waveform model for statistical parametric speech synthesis",
      "arxiv": "arXiv:1903.05955"
    },
    {
      "citation_id": "47",
      "title": "Unsupervised learning of disentangled and interpretable representations from sequential data",
      "authors": [
        "W.-N Hsu",
        "Y Zhang",
        "J Glass"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "48",
      "title": "Speaker-independent isolated word recognition based on emphasized spectral dynamics",
      "authors": [
        "S Furui"
      ],
      "year": "1986",
      "venue": "ICASSP'86. IEEE International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "49",
      "title": "Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences",
      "authors": [
        "S Davis",
        "P Mermelstein"
      ],
      "year": "1980",
      "venue": "IEEE transactions on acoustics, speech, and signal processing"
    },
    {
      "citation_id": "50",
      "title": "A new method for tracking modulations in tonal music in audio data format",
      "authors": [
        "H Purwins",
        "B Blankertz",
        "K Obermayer"
      ],
      "year": "2000",
      "venue": "Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium"
    },
    {
      "citation_id": "51",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "52",
      "title": "Attentive convolutional neural network based speech emotion recognition: A study on the impact of input fea-This work is the extended version of paper",
      "authors": [
        "M Neumann",
        "N Vu"
      ],
      "venue": "IEEE Transactions on Affective Computing 2021"
    },
    {
      "citation_id": "53",
      "title": "tures, signal length, and acted speech",
      "year": "2017",
      "venue": "tures, signal length, and acted speech",
      "arxiv": "arXiv:1706.00612"
    },
    {
      "citation_id": "54",
      "title": "Learning a better representation of speech soundwaves using restricted boltzmann machines",
      "authors": [
        "N Jaitly",
        "G Hinton"
      ],
      "year": "2011",
      "venue": "2011 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "55",
      "title": "Revisiting unreasonable effectiveness of data in deep learning era",
      "authors": [
        "C Sun",
        "A Shrivastava",
        "S Singh",
        "A Gupta"
      ],
      "year": "2017",
      "venue": "Proceedings"
    },
    {
      "citation_id": "56",
      "title": "The darpa speech recognition research database: Specifications and status/william m. fisher, george r. doddington, kathleen m. goudie-marshall",
      "authors": [
        "M William"
      ],
      "year": "1986",
      "venue": "Proceedings of DARPA Workshop on Speech Recognition"
    },
    {
      "citation_id": "57",
      "title": "Switchboard: Telephone speech corpus for research and development",
      "authors": [
        "J Godfrey",
        "E Holliman",
        "J Mcdaniel"
      ],
      "year": "1992",
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "58",
      "title": "The design for the wall street journalbased csr corpus",
      "authors": [
        "D Paul",
        "J Baker"
      ],
      "year": "1992",
      "venue": "Proceedings of the workshop on Speech and Natural Language"
    },
    {
      "citation_id": "59",
      "title": "The ami system for the transcription of speech in meetings",
      "authors": [
        "T Hain",
        "L Burget",
        "J Dines",
        "G Garau",
        "V Wan",
        "M Karafi",
        "J Vepa",
        "M Lincoln"
      ],
      "year": "2007",
      "venue": "2007 IEEE International Conference on Acoustics, Speech and Signal Processing-ICASSP'07"
    },
    {
      "citation_id": "60",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Ninth European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "61",
      "title": "The interspeech 2009 emotion challenge",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner"
      ],
      "year": "2009",
      "venue": "Tenth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "62",
      "title": "Introducing the recola multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "F Ringeval",
        "A Sonderegger",
        "J Sauer",
        "D Lalanne"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG)"
    },
    {
      "citation_id": "63",
      "title": "Introducing the geneva multimodal expression corpus for experimental research on emotion perception",
      "authors": [
        "T Bänziger",
        "M Mortillaro",
        "K Scherer"
      ],
      "year": "2012",
      "venue": "Emotion"
    },
    {
      "citation_id": "64",
      "title": "Librispeech: an asr corpus based on public domain audio books",
      "authors": [
        "V Panayotov",
        "G Chen",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "65",
      "title": "Voxceleb2: Deep speaker recognition",
      "authors": [
        "J Chung",
        "A Nagrani",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "Voxceleb2: Deep speaker recognition",
      "arxiv": "arXiv:1806.05622"
    },
    {
      "citation_id": "66",
      "title": "Ted-lium: an automatic speech recognition dedicated corpus",
      "authors": [
        "A Rousseau",
        "P Deléglise",
        "Y Esteve"
      ],
      "year": "2012",
      "venue": "LREC"
    },
    {
      "citation_id": "67",
      "title": "Thchs-30: A free chinese speech corpus",
      "authors": [
        "D Wang",
        "X Zhang"
      ],
      "year": "2015",
      "venue": "Thchs-30: A free chinese speech corpus",
      "arxiv": "arXiv:1512.01882"
    },
    {
      "citation_id": "68",
      "title": "Aishell-1: An opensource mandarin speech corpus and a speech recognition baseline",
      "authors": [
        "H Bu",
        "J Du",
        "X Na",
        "B Wu",
        "H Zheng"
      ],
      "year": "2017",
      "venue": "the International Coordinating Committee on Speech Databases and Speech I/O Systems and Assessment"
    },
    {
      "citation_id": "69",
      "title": "Open source automatic speech recognition for german",
      "authors": [
        "B Milde",
        "A Köhn"
      ],
      "year": "2018",
      "venue": "Speech Communication; 13th ITG-Symposium. VDE"
    },
    {
      "citation_id": "70",
      "title": "The semaine database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent",
      "authors": [
        "G Mckeown",
        "M Valstar",
        "R Cowie",
        "M Pantic",
        "M Schroder"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "71",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "72",
      "title": "Msp-improv: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "C Busso",
        "S Parthasarathy",
        "A Burmania",
        "M Abdelwahab",
        "N Sadoughi",
        "E Provost"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "73",
      "title": "A tutorial on text-independent speaker verification",
      "authors": [
        "F Bimbot",
        "J.-F Bonastre",
        "C Fredouille",
        "G Gravier",
        "I Magrin-Chagnolleau",
        "S Meignier",
        "T Merlin",
        "J Ortega-García",
        "D Petrovska-Delacrétaz",
        "D Reynolds"
      ],
      "year": "2004",
      "venue": "EURASIP Journal on Advances in Signal Processing"
    },
    {
      "citation_id": "74",
      "title": "Recognising Realistic Emotions and Affect in Speech: State of the Art and Lessons Learnt from the First Challenge",
      "authors": [
        "B Schuller",
        "A Batliner",
        "S Steidl",
        "D Seppi"
      ],
      "year": "2011",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "75",
      "title": "Deep learning",
      "authors": [
        "I Goodfellow",
        "Y Bengio",
        "A Courville"
      ],
      "year": "2016",
      "venue": "Deep learning"
    },
    {
      "citation_id": "76",
      "title": "Learning deep architectures for ai",
      "authors": [
        "Y Bengio"
      ],
      "year": "2009",
      "venue": "Foundations and trends® in Machine Learning"
    },
    {
      "citation_id": "77",
      "title": "Self-supervised visual feature learning with deep neural networks: A survey",
      "authors": [
        "L Jing",
        "Y Tian"
      ],
      "year": "2019",
      "venue": "Self-supervised visual feature learning with deep neural networks: A survey",
      "arxiv": "arXiv:1902.06162"
    },
    {
      "citation_id": "78",
      "title": "Text feature extraction based on deep learning: a review",
      "authors": [
        "H Liang",
        "X Sun",
        "Y Sun",
        "Y Gao"
      ],
      "year": "2017",
      "venue": "EURASIP journal on wireless communications and networking"
    },
    {
      "citation_id": "79",
      "title": "Audiovisual speech recognition using deep learning",
      "authors": [
        "K Noda",
        "Y Yamaguchi",
        "K Nakadai",
        "H Okuno",
        "T Ogata"
      ],
      "year": "2015",
      "venue": "Applied Intelligence"
    },
    {
      "citation_id": "80",
      "title": "Using deep autoencoders for facial expression recognition",
      "authors": [
        "M Usman",
        "S Latif",
        "J Qadir"
      ],
      "year": "2017",
      "venue": "2017 13th International Conference on Emerging Technologies (ICET)"
    },
    {
      "citation_id": "81",
      "title": "Variational autoencoders for learning latent representations of speech emotion: A preliminary study",
      "authors": [
        "S Latif",
        "R Rana",
        "J Qadir",
        "J Epps"
      ],
      "year": "2017",
      "venue": "Variational autoencoders for learning latent representations of speech emotion: A preliminary study",
      "arxiv": "arXiv:1712.08708"
    },
    {
      "citation_id": "82",
      "title": "An introduction to neural information retrieval",
      "authors": [
        "B Mitra",
        "N Craswell"
      ],
      "year": "2018",
      "venue": "Foundations and Trends® in Information Retrieval"
    },
    {
      "citation_id": "83",
      "title": "Neural models for information retrieval",
      "authors": [
        "B Mitra",
        "N Craswell"
      ],
      "year": "2017",
      "venue": "Neural models for information retrieval",
      "arxiv": "arXiv:1705.01509"
    },
    {
      "citation_id": "84",
      "title": "One deep music representation to rule them all? a comparative analysis of different representation learning strategies",
      "authors": [
        "J Kim",
        "J Urbano",
        "C Liem",
        "A Hanjalic"
      ],
      "year": "2018",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "85",
      "title": "Learning representations for information retrieval",
      "authors": [
        "A Sordoni"
      ],
      "year": "2016",
      "venue": "Learning representations for information retrieval"
    },
    {
      "citation_id": "86",
      "title": "Adversarial machine learning at scale",
      "authors": [
        "A Kurakin",
        "I Goodfellow",
        "S Bengio"
      ],
      "year": "2016",
      "venue": "Adversarial machine learning at scale",
      "arxiv": "arXiv:1611.01236"
    },
    {
      "citation_id": "87",
      "title": "Towards robust neural networks via random self-ensemble",
      "authors": [
        "X Liu",
        "M Cheng",
        "H Zhang",
        "C.-J Hsieh"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "88",
      "title": "Adversarial machine learning and speech emotion recognition: Utilizing generative adversarial networks for robustness",
      "authors": [
        "S Latif",
        "R Rana",
        "J Qadir"
      ],
      "year": "2018",
      "venue": "Adversarial machine learning and speech emotion recognition: Utilizing generative adversarial networks for robustness",
      "arxiv": "arXiv:1811.11402"
    },
    {
      "citation_id": "89",
      "title": "N-HANS: Introducing the Augsburg Neuro-Holistic Audio-eNhancement System",
      "authors": [
        "S Liu",
        "G Keren",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "N-HANS: Introducing the Augsburg Neuro-Holistic Audio-eNhancement System"
    },
    {
      "citation_id": "90",
      "title": "Survey of clustering algorithms",
      "authors": [
        "R Xu",
        "D Wunsch"
      ],
      "year": "2005",
      "venue": "Survey of clustering algorithms"
    },
    {
      "citation_id": "91",
      "title": "A survey of clustering with deep learning: From the perspective of network architecture",
      "authors": [
        "E Min",
        "X Guo",
        "Q Liu",
        "G Zhang",
        "J Cui",
        "J Long"
      ],
      "year": "2018",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "92",
      "title": "Untangling invariant object recognition",
      "authors": [
        "J Dicarlo",
        "D Cox"
      ],
      "year": "2007",
      "venue": "Trends in cognitive sciences"
    },
    {
      "citation_id": "93",
      "title": "Towards a definition of disentangled representations",
      "authors": [
        "I Higgins",
        "D Amos",
        "D Pfau",
        "S Racaniere",
        "L Matthey",
        "D Rezende",
        "A Lerchner"
      ],
      "year": "2018",
      "venue": "Towards a definition of disentangled representations",
      "arxiv": "arXiv:1812.02230"
    },
    {
      "citation_id": "94",
      "title": "Deep variational information bottleneck",
      "authors": [
        "A Alemi",
        "I Fischer",
        "J Dillon",
        "K Murphy"
      ],
      "year": "2016",
      "venue": "Deep variational information bottleneck",
      "arxiv": "arXiv:1612.00410"
    },
    {
      "citation_id": "95",
      "title": "Stochastic neighbor embedding",
      "authors": [
        "G Hinton",
        "S Roweis"
      ],
      "year": "2003",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "96",
      "title": "An investigation of manifold learning for speech analysis",
      "authors": [
        "A Errity",
        "J Mckenna"
      ],
      "year": "2006",
      "venue": "Ninth International Conference on Spoken Language Processing"
    },
    {
      "citation_id": "97",
      "title": "Algorithms for manifold learning",
      "authors": [
        "L Cayton"
      ],
      "year": "2005",
      "venue": "Univ. of California at San Diego Tech. Rep"
    },
    {
      "citation_id": "98",
      "title": "Overview of manifold learning and its application in medical data set",
      "authors": [
        "E Golchin",
        "K Maghooli"
      ],
      "year": "2014",
      "venue": "International journal of biomedical engineering and science (IJBES)"
    },
    {
      "citation_id": "99",
      "title": "Manifold learning theory and applications",
      "authors": [
        "Y Ma",
        "Y Fu"
      ],
      "year": "2011",
      "venue": "Manifold learning theory and applications"
    },
    {
      "citation_id": "100",
      "title": "Theoretical neuroscience: computational and mathematical modeling of neural systems",
      "authors": [
        "P Dayan",
        "L Abbott",
        "L Abbott"
      ],
      "year": "2001",
      "venue": "Theoretical neuroscience: computational and mathematical modeling of neural systems"
    },
    {
      "citation_id": "101",
      "title": "Abstract learning via demodulation in a deep neural network",
      "authors": [
        "A Simpson"
      ],
      "year": "2015",
      "venue": "Abstract learning via demodulation in a deep neural network",
      "arxiv": "arXiv:1502.04042"
    },
    {
      "citation_id": "102",
      "title": "The abstract representations in speech processing",
      "authors": [
        "A Cutler"
      ],
      "year": "2008",
      "venue": "The Quarterly Journal of Experimental Psychology"
    },
    {
      "citation_id": "103",
      "title": "Abstraction learning",
      "authors": [
        "F Deng",
        "J Ren",
        "F Chen"
      ],
      "year": "2018",
      "venue": "Abstraction learning",
      "arxiv": "arXiv:1809.03956"
    },
    {
      "citation_id": "104",
      "title": "A tutorial survey of architectures, algorithms, and applications for deep learning",
      "authors": [
        "L Deng"
      ],
      "year": "2014",
      "venue": "APSIPA Transactions on Signal and Information Processing"
    },
    {
      "citation_id": "105",
      "title": "Introduction to multi-layer feed-forward neural networks",
      "authors": [
        "D Svozil",
        "V Kvasnicka",
        "J Pospichal"
      ],
      "year": "1997",
      "venue": "Chemometrics and intelligent laboratory systems"
    },
    {
      "citation_id": "106",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2012",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "107",
      "title": "Convolutional networks for images, speech, and time series",
      "authors": [
        "Y Lecun",
        "Y Bengio"
      ],
      "year": "1995",
      "venue": "The handbook of brain theory and neural networks"
    },
    {
      "citation_id": "108",
      "title": "Direct modelling of speech emotion from raw speech",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Epps"
      ],
      "year": "2019",
      "venue": "Direct modelling of speech emotion from raw speech",
      "arxiv": "arXiv:1904.03833"
    },
    {
      "citation_id": "109",
      "title": "Analysis of cnn-based speech recognition system using raw speech as input",
      "authors": [
        "D Palaz",
        "R Collobert"
      ],
      "year": "2015",
      "venue": "Idiap, Tech. Rep"
    },
    {
      "citation_id": "110",
      "title": "Convolutional neural networksbased continuous speech recognition using raw speech signal",
      "authors": [
        "D Palaz",
        "M Doss",
        "R Collobert"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "111",
      "title": "Using regional saliency for speech emotion recognition",
      "authors": [
        "Z Aldeneh",
        "E Provost"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "112",
      "title": "Sequence to sequence learning with neural networks",
      "authors": [
        "I Sutskever",
        "O Vinyals",
        "Q Le"
      ],
      "year": "2014",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "113",
      "title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
      "authors": [
        "K Cho",
        "B Van Merriënboer",
        "C Gulcehre",
        "D Bahdanau",
        "F Bougares",
        "H Schwenk",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
      "arxiv": "arXiv:1406.1078"
    },
    {
      "citation_id": "114",
      "title": "Long short-term memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "115",
      "title": "Bidirectional recurrent neural networks",
      "authors": [
        "M Schuster",
        "K Paliwal"
      ],
      "year": "1997",
      "venue": "IEEE Transactions on Signal Processing"
    },
    {
      "citation_id": "116",
      "title": "Two-pass end-to-end speech recognition",
      "authors": [
        "T Sainath",
        "R Pang",
        "D Rybach",
        "Y He",
        "R Prabhavalkar",
        "W Li",
        "M Visontai",
        "Q Liang",
        "T Strohman",
        "Y Wu"
      ],
      "year": "2019",
      "venue": "Two-pass end-to-end speech recognition",
      "arxiv": "arXiv:1908.10992"
    },
    {
      "citation_id": "117",
      "title": "Autoencoders, minimum description length and helmholtz free energy",
      "authors": [
        "G Hinton",
        "R Zemel"
      ],
      "year": "1994",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "118",
      "title": "Sparse autoencoder",
      "authors": [
        "A Ng"
      ],
      "year": "2011",
      "venue": "CS294A Lecture notes"
    },
    {
      "citation_id": "119",
      "title": "K-sparse autoencoders",
      "authors": [
        "A Makhzani",
        "B Frey"
      ],
      "year": "2013",
      "venue": "K-sparse autoencoders",
      "arxiv": "arXiv:1312.5663"
    },
    {
      "citation_id": "120",
      "title": "Sparse autoencoderbased feature transfer learning for speech emotion recognition",
      "authors": [
        "J Deng",
        "Z Zhang",
        "E Marchi",
        "B Schuller"
      ],
      "year": "2013",
      "venue": "2013 Humaine Association Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "121",
      "title": "Extracting and composing robust features with denoising autoencoders",
      "authors": [
        "P Vincent",
        "H Larochelle",
        "Y Bengio",
        "P.-A Manzagol"
      ],
      "year": "2008",
      "venue": "Proceedings of the 25th international conference on Machine learning"
    },
    {
      "citation_id": "122",
      "title": "Contractive auto-encoders: Explicit invariance during feature extraction",
      "authors": [
        "S Rifai",
        "P Vincent",
        "X Muller",
        "X Glorot",
        "Y Bengio"
      ],
      "year": "2011",
      "venue": "Proceedings of the 28th International Conference on International Conference on Machine Learning"
    },
    {
      "citation_id": "123",
      "title": "A fast learning algorithm for deep belief nets",
      "authors": [
        "G Hinton",
        "S Osindero",
        "Y.-W Teh"
      ],
      "year": "2006",
      "venue": "Neural computation"
    },
    {
      "citation_id": "124",
      "title": "A learning algorithm for boltzmann machines",
      "authors": [
        "D Ackley",
        "G Hinton",
        "T Sejnowski"
      ],
      "year": "1985",
      "venue": "Cognitive science"
    },
    {
      "citation_id": "125",
      "title": "Deep generative stochastic networks trainable by backprop",
      "authors": [
        "Y Bengio",
        "E Laufer",
        "G Alain",
        "J Yosinski"
      ],
      "year": "2014",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "126",
      "title": "Feature learning in deep neural networks-studies on speech recognition tasks",
      "authors": [
        "D Yu",
        "M Seltzer",
        "J Li",
        "J.-T Huang",
        "F Seide"
      ],
      "year": "2013",
      "venue": "Feature learning in deep neural networks-studies on speech recognition tasks",
      "arxiv": "arXiv:1301.3605"
    },
    {
      "citation_id": "127",
      "title": "Constructing long short-term memory based deep recurrent neural networks for large vocabulary speech recognition",
      "authors": [
        "X Li",
        "X Wu"
      ],
      "year": "2015",
      "venue": "Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "128",
      "title": "Unsupervised representation learning with deep convolutional generative adversarial networks",
      "authors": [
        "A Radford",
        "L Metz",
        "S Chintala"
      ],
      "year": "2015",
      "venue": "Unsupervised representation learning with deep convolutional generative adversarial networks",
      "arxiv": "arXiv:1511.06434"
    },
    {
      "citation_id": "129",
      "title": "Adversarial feature learning",
      "authors": [
        "J Donahue",
        "P Krähenbühl",
        "T Darrell"
      ],
      "year": "2016",
      "venue": "Adversarial feature learning",
      "arxiv": "arXiv:1605.09782"
    },
    {
      "citation_id": "130",
      "title": "Infogan: Interpretable representation learning by infor-mation maximizing generative adversarial nets",
      "authors": [
        "X Chen",
        "Y Duan",
        "R Houthooft",
        "J Schulman",
        "I Sutskever",
        "P Abbeel"
      ],
      "year": "2016",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "131",
      "title": "beta-vae: Learning basic visual concepts with a constrained variational framework",
      "authors": [
        "I Higgins",
        "L Matthey",
        "A Pal",
        "C Burgess",
        "X Glorot",
        "M Botvinick",
        "S Mohamed",
        "A Lerchner"
      ],
      "year": "2017",
      "venue": "ICLR"
    },
    {
      "citation_id": "132",
      "title": "Infovae: Information maximizing variational autoencoders",
      "authors": [
        "S Zhao",
        "J Song",
        "S Ermon"
      ],
      "year": "2017",
      "venue": "Infovae: Information maximizing variational autoencoders",
      "arxiv": "arXiv:1706.02262"
    },
    {
      "citation_id": "133",
      "title": "Pixelvae: A latent variable model for natural images",
      "authors": [
        "I Gulrajani",
        "K Kumar",
        "F Ahmed",
        "A Taiga",
        "F Visin",
        "D Vazquez",
        "A Courville"
      ],
      "year": "2016",
      "venue": "Pixelvae: A latent variable model for natural images",
      "arxiv": "arXiv:1611.05013"
    },
    {
      "citation_id": "134",
      "title": "Recent advances in autoencoder-based representation learning",
      "authors": [
        "M Tschannen",
        "O Bachem",
        "M Lucic"
      ],
      "year": "2018",
      "venue": "Recent advances in autoencoder-based representation learning",
      "arxiv": "arXiv:1812.05069"
    },
    {
      "citation_id": "135",
      "title": "Conditional image generation with pixelcnn decoders",
      "authors": [
        "A Van Den Oord",
        "N Kalchbrenner",
        "L Espeholt",
        "O Vinyals",
        "A Graves"
      ],
      "year": "2016",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "136",
      "title": "A wavenet for speech denoising",
      "authors": [
        "D Rethage",
        "J Pons",
        "X Serra"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "137",
      "title": "Unsupervised speech representation learning using wavenet autoencoders",
      "authors": [
        "J Chorowski",
        "R Weiss",
        "S Bengio",
        "A Oord"
      ],
      "year": "2019",
      "venue": "Unsupervised speech representation learning using wavenet autoencoders",
      "arxiv": "arXiv:1901.08810"
    },
    {
      "citation_id": "138",
      "title": "Unsupervised feature learning for audio classification using convolutional deep belief networks",
      "authors": [
        "H Lee",
        "P Pham",
        "Y Largman",
        "A Ng"
      ],
      "year": "2009",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "139",
      "title": "Speaker recognition with hybrid features from a deep belief network",
      "authors": [
        "H Ali",
        "S Tran",
        "E Benetos",
        "A Garcez"
      ],
      "year": "2018",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "140",
      "title": "Bottleneck features for speaker recognition",
      "authors": [
        "S Yaman",
        "J Pelecanos",
        "R Sarikaya"
      ],
      "year": "2012",
      "venue": "Odyssey 2012-The Speaker and Language Recognition Workshop"
    },
    {
      "citation_id": "141",
      "title": "A novel dbn feature fusion model for cross-corpus speech emotion recognition",
      "authors": [
        "Z Cairong",
        "Z Xinran",
        "Z Cheng",
        "Z Li"
      ],
      "year": "2016",
      "venue": "Journal of Electrical and Computer Engineering"
    },
    {
      "citation_id": "142",
      "title": "Phone recognition with the mean-covariance restricted boltzmann machine",
      "authors": [
        "G Dahl",
        "A -R. Mohamed",
        "G Hinton"
      ],
      "year": "2010",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "143",
      "title": "Towards directly modeling raw speech signal for speaker verification using cnns",
      "authors": [
        "H Muckenhirn",
        "M Doss",
        "S Marcell"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "144",
      "title": "End-to-end speech emotion recognition using deep neural networks",
      "authors": [
        "P Tzirakis",
        "J Zhang",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "145",
      "title": "Emotion identification from raw speech signals using dnns",
      "authors": [
        "M Sarma",
        "P Ghahremani",
        "D Povey",
        "N Goel",
        "K Sarma",
        "N Dehak"
      ],
      "year": "2018",
      "venue": "Interspeech"
    },
    {
      "citation_id": "146",
      "title": "Learning the speech front-end with raw waveform cldnns",
      "authors": [
        "T Sainath",
        "R Weiss",
        "A Senior",
        "K Wilson",
        "O Vinyals"
      ],
      "year": "2015",
      "venue": "Sixteenth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "147",
      "title": "Improving speech emotion recognition with unsupervised representation learning on unlabeled speech",
      "authors": [
        "M Neumann",
        "N Vu"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "148",
      "title": "Disentangling correlated speaker and noise for speech synthesis via data augmentation and adversarial factorization",
      "authors": [
        "W.-N Hsu",
        "Y Zhang",
        "R Weiss",
        "Y.-A Chung",
        "Y Wang",
        "Y Wu",
        "J Glass"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "149",
      "title": "Speech feature denoising and dereverberation via deep autoencoders for noisy reverberant speech recognition",
      "authors": [
        "X Feng",
        "Y Zhang",
        "J Glass"
      ],
      "year": "2014",
      "venue": "2014 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "150",
      "title": "Deep recurrent de-noising auto-encoder and blind de-reverberation for reverberated speech recognition",
      "authors": [
        "F Weninger",
        "S Watanabe",
        "Y Tachioka",
        "B Schuller"
      ],
      "year": "2014",
      "venue": "2014 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "151",
      "title": "Music removal by convolutional denoising autoencoder in speech recognition",
      "authors": [
        "M Zhao",
        "D Wang",
        "Z Zhang",
        "X Zhang"
      ],
      "year": "2015",
      "venue": "Music removal by convolutional denoising autoencoder in speech recognition"
    },
    {
      "citation_id": "152",
      "title": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)",
      "year": "2015",
      "venue": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)"
    },
    {
      "citation_id": "153",
      "title": "Unsupervised deep auditory model using stack of convolutional rbms for speech recognition",
      "authors": [
        "H Sailor",
        "H Patil"
      ],
      "year": "2016",
      "venue": "Unsupervised deep auditory model using stack of convolutional rbms for speech recognition"
    },
    {
      "citation_id": "154",
      "title": "Deep belief networks for phone recognition",
      "authors": [
        "G A.-R. Mohamed",
        "G Dahl",
        "Hinton"
      ],
      "year": "2009",
      "venue": "Nips workshop on deep learning for speech recognition and related applications"
    },
    {
      "citation_id": "155",
      "title": "Representation learning for speech emotion recognition",
      "authors": [
        "S Ghosh",
        "E Laksana",
        "L.-P Morency",
        "S Scherer"
      ],
      "year": "2016",
      "venue": "Interspeech"
    },
    {
      "citation_id": "156",
      "title": "Learning representations of affect from speech",
      "year": "2015",
      "venue": "Learning representations of affect from speech",
      "arxiv": "arXiv:1511.04747"
    },
    {
      "citation_id": "157",
      "title": "Speech emotion recognition with unsupervised feature learning",
      "authors": [
        "Z.-W Huang",
        "W -T. Xue",
        "Q.-R Mao"
      ],
      "year": "2015",
      "venue": "Frontiers of Information Technology & Electronic Engineering"
    },
    {
      "citation_id": "158",
      "title": "Modeling gender information for emotion recognition using denoising autoencoder",
      "authors": [
        "R Xia",
        "J Deng",
        "B Schuller",
        "Y Liu"
      ],
      "year": "2014",
      "venue": "2014 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "159",
      "title": "Using denoising autoencoder for emotion recognition",
      "authors": [
        "R Xia",
        "Y Liu"
      ],
      "year": "2013",
      "venue": "Using denoising autoencoder for emotion recognition"
    },
    {
      "citation_id": "160",
      "title": "Learning representations of emotional speech with deep convolutional generative adversarial networks",
      "authors": [
        "J Chang",
        "S Scherer"
      ],
      "year": "2017",
      "venue": "Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "161",
      "title": "Adversarial network bottleneck features for noise robust speaker verification",
      "authors": [
        "H Yu",
        "Z.-H Tan",
        "Z Ma",
        "J Guo"
      ],
      "year": "2017",
      "venue": "Adversarial network bottleneck features for noise robust speaker verification",
      "arxiv": "arXiv:1706.03397"
    },
    {
      "citation_id": "162",
      "title": "Synthesizing audio with generative adversarial networks",
      "authors": [
        "C Donahue",
        "J Mcauley",
        "M Puckette"
      ],
      "year": "2018",
      "venue": "Synthesizing audio with generative adversarial networks",
      "arxiv": "arXiv:1802.04208"
    },
    {
      "citation_id": "163",
      "title": "Adversarial auto-encoders for speech based emotion recognition",
      "authors": [
        "S Sahu",
        "R Gupta",
        "G Sivaraman",
        "W Abdalmageed",
        "C Espy-Wilson"
      ],
      "year": "2018",
      "venue": "Adversarial auto-encoders for speech based emotion recognition",
      "arxiv": "arXiv:1806.02146"
    },
    {
      "citation_id": "164",
      "title": "Learning speaker representations with mutual information",
      "authors": [
        "M Ravanelli",
        "Y Bengio"
      ],
      "year": "2018",
      "venue": "Learning speaker representations with mutual information",
      "arxiv": "arXiv:1812.00271"
    },
    {
      "citation_id": "165",
      "title": "Transfer learning for improving speech emotion classification accuracy",
      "authors": [
        "S Latif",
        "R Rana",
        "S Younis",
        "J Qadir",
        "J Epps"
      ],
      "year": "2018",
      "venue": "Transfer learning for improving speech emotion classification accuracy",
      "arxiv": "arXiv:1801.06353"
    },
    {
      "citation_id": "166",
      "title": "Unsupervised adversarial domain adaptation for cross-lingual speech emotion recognition",
      "authors": [
        "S Latif",
        "J Qadir",
        "M Bilal"
      ],
      "year": "2019",
      "venue": "Unsupervised adversarial domain adaptation for cross-lingual speech emotion recognition",
      "arxiv": "arXiv:1907.06083"
    },
    {
      "citation_id": "167",
      "title": "Cross lingual speech emotion recognition: Urdu vs. western languages",
      "authors": [
        "S Latif",
        "A Qayyum",
        "M Usman",
        "J Qadir"
      ],
      "year": "2018",
      "venue": "2018 International Conference on Frontiers of Information Technology (FIT)"
    },
    {
      "citation_id": "168",
      "title": "Multi-task semisupervised adversarial autoencoding for speech emotion",
      "authors": [
        "R Rana",
        "S Latif",
        "S Khalifa",
        "R Jurdak"
      ],
      "year": "2019",
      "venue": "Multi-task semisupervised adversarial autoencoding for speech emotion",
      "arxiv": "arXiv:1907.06078"
    },
    {
      "citation_id": "169",
      "title": "Semi-supervised learning literature survey",
      "authors": [
        "X Zhu"
      ],
      "year": "2005",
      "venue": "Semi-supervised learning literature survey"
    },
    {
      "citation_id": "170",
      "title": "Speech emotion recognition using cnn",
      "authors": [
        "Z Huang",
        "M Dong",
        "Q Mao",
        "Y Zhan"
      ],
      "year": "2014",
      "venue": "Proceedings of the 22Nd ACM International Conference on Multimedia"
    },
    {
      "citation_id": "171",
      "title": "Speech emotion recognition using semi-supervised learning with ladder networks",
      "authors": [
        "J Huang",
        "Y Li",
        "J Tao",
        "Z Lian",
        "M Niu",
        "J Yi"
      ],
      "year": "2018",
      "venue": "2018 First Asian Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "172",
      "title": "Semi-supervised speech emotion recognition with ladder networks",
      "authors": [
        "S Parthasarathy",
        "C Busso"
      ],
      "year": "2019",
      "venue": "Semi-supervised speech emotion recognition with ladder networks",
      "arxiv": "arXiv:1905.02921"
    },
    {
      "citation_id": "173",
      "title": "Ladder networks for emotion recognition: Using unsupervised auxiliary tasks to improve predictions of emotional attributes",
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "174",
      "title": "Semisupervised autoencoders for speech emotion recognition",
      "authors": [
        "J Deng",
        "X Xu",
        "Z Zhang",
        "S Frühholz",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "175",
      "title": "Deep neural network features and semi-supervised training for low resource speech recognition",
      "authors": [
        "S Thomas",
        "M Seltzer",
        "K Church",
        "H Hermansky"
      ],
      "year": "2013",
      "venue": "2013 IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "176",
      "title": "Multilingual representations for low resource speech recognition and keyword search",
      "authors": [
        "J Cui",
        "B Kingsbury",
        "B Ramabhadran",
        "A Sethy",
        "K Audhkhasi",
        "X Cui",
        "E Kislal",
        "L Mangu",
        "M Nussbaum-Thom",
        "M Picheny"
      ],
      "year": "2015",
      "venue": "2015 IEEE Workshop on Automatic Speech Recognition and Understanding"
    },
    {
      "citation_id": "177",
      "title": "Semisupervised end-to-end speech recognition",
      "authors": [
        "S Karita",
        "S Watanabe",
        "T Iwata",
        "A Ogawa",
        "M Delcroix"
      ],
      "year": "2018",
      "venue": "Interspeech"
    },
    {
      "citation_id": "178",
      "title": "Speech separation based on improved deep neural networks with dual outputs of speech features for both target and interfering speakers",
      "authors": [
        "Y Tu",
        "J Du",
        "Y Xu",
        "L Dai",
        "C.-H Lee"
      ],
      "year": "2014",
      "venue": "The 9th International Symposium on Chinese Spoken Language Processing"
    },
    {
      "citation_id": "179",
      "title": "A study of semisupervised speaker diarization system using GAN mixture model",
      "authors": [
        "M Pal",
        "M Kumar",
        "R Peri",
        "S Narayanan"
      ],
      "year": "2019",
      "venue": "A study of semisupervised speaker diarization system using GAN mixture model",
      "arxiv": "arXiv:1910.11416"
    },
    {
      "citation_id": "180",
      "title": "Deep learning of representations for unsupervised and transfer learning",
      "authors": [
        "Y Bengio"
      ],
      "year": "2012",
      "venue": "Proceedings of ICML workshop on unsupervised and transfer learning"
    },
    {
      "citation_id": "181",
      "title": "Learning to learn",
      "authors": [
        "S Thrun",
        "L Pratt"
      ],
      "year": "2012",
      "venue": "Learning to learn"
    },
    {
      "citation_id": "182",
      "title": "An unsupervised deep domain adaptation approach for robust speech recognition",
      "authors": [
        "S Sun",
        "B Zhang",
        "L Xie",
        "Y Zhang"
      ],
      "year": "2017",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "183",
      "title": "Learning hidden unit contributions for unsupervised acoustic model adaptation",
      "authors": [
        "P Swietojanski",
        "J Li",
        "S Renals"
      ],
      "year": "2016",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "184",
      "title": "Extracting domain invariant features by unsupervised learning for robust automatic speech recognition",
      "authors": [
        "W.-N Hsu",
        "J Glass"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "185",
      "title": "A study of enhancement, augmentation, and autoencoder methods for domain adaptation in distant speech recognition",
      "authors": [
        "H Tang",
        "W.-N Hsu",
        "F Grondin",
        "J Glass"
      ],
      "year": "2018",
      "venue": "A study of enhancement, augmentation, and autoencoder methods for domain adaptation in distant speech recognition",
      "arxiv": "arXiv:1806.04841"
    },
    {
      "citation_id": "186",
      "title": "Unsupervised adaptation with interpretable disentangled representations for distant conversational speech recognition",
      "authors": [
        "W.-N Hsu",
        "H Tang",
        "J Glass"
      ],
      "year": "2018",
      "venue": "Unsupervised adaptation with interpretable disentangled representations for distant conversational speech recognition",
      "arxiv": "arXiv:1806.04872"
    },
    {
      "citation_id": "187",
      "title": "Unsupervised speaker adaptation for dnn-based tts synthesis",
      "authors": [
        "Y Fan",
        "Y Qian",
        "F Soong",
        "L He"
      ],
      "year": "2016",
      "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "188",
      "title": "Towards end-to-end speech recognition with transfer learning",
      "authors": [
        "C.-X Qin",
        "D Qu",
        "L.-H Zhang"
      ],
      "year": "2018",
      "venue": "EURASIP Journal on Audio, Speech, and Music Processing"
    },
    {
      "citation_id": "189",
      "title": "Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers",
      "authors": [
        "J.-T Huang",
        "J Li",
        "D Yu",
        "L Deng",
        "Y Gong"
      ],
      "year": "2013",
      "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "190",
      "title": "Language independent and unsupervised acoustic models for speech recognition and keyword spotting",
      "authors": [
        "K Knill",
        "M Gales",
        "A Ragni",
        "S Rath"
      ],
      "year": "2014",
      "venue": "Language independent and unsupervised acoustic models for speech recognition and keyword spotting"
    },
    {
      "citation_id": "191",
      "title": "Unsupervised domain adaptation by adversarial learning for robust speech recognition",
      "authors": [
        "P Denisov",
        "N Vu",
        "M Font"
      ],
      "year": "2018",
      "venue": "Speech Communication; 13th ITG-Symposium. VDE"
    },
    {
      "citation_id": "192",
      "title": "Domain adversarial training for accented speech recognition",
      "authors": [
        "S Sun",
        "C.-F Yeh",
        "M.-Y Hwang",
        "M Ostendorf",
        "L Xie"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "193",
      "title": "Adversarial multi-task learning of deep neural networks for robust speech recognition",
      "authors": [
        "Y Shinohara"
      ],
      "year": "2016",
      "venue": "INTERSPEECH"
    },
    {
      "citation_id": "194",
      "title": "A multidiscriminator cyclegan for unsupervised non-parallel speech domain adaptation",
      "authors": [
        "E Hosseini-Asl",
        "Y Zhou",
        "C Xiong",
        "R Socher"
      ],
      "year": "2018",
      "venue": "A multidiscriminator cyclegan for unsupervised non-parallel speech domain adaptation",
      "arxiv": "arXiv:1804.00522"
    },
    {
      "citation_id": "195",
      "title": "Adversarial teacherstudent learning for unsupervised domain adaptation",
      "authors": [
        "Z Meng",
        "J Li",
        "Y Gong",
        "B.-H Juang"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "196",
      "title": "Speaker-invariant training via adversarial learning",
      "authors": [
        "Z Meng",
        "J Li",
        "Z Chen",
        "Y Zhao",
        "V Mazalov",
        "Y Gang",
        "B.-H Juang"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "197",
      "title": "Adversarial speaker adaptation",
      "authors": [
        "Z Meng",
        "J Li",
        "Y Gong"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "198",
      "title": "Adversarial learning of raw speech features for domain invariant speech recognition",
      "authors": [
        "A Tripathi",
        "A Mohan",
        "S Anand",
        "M Singh"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "199",
      "title": "Autoencoder based domain adaptation for speaker recognition under insufficient channel information",
      "authors": [
        "S Shon",
        "S Mun",
        "W Kim",
        "H Ko"
      ],
      "year": "2017",
      "venue": "Autoencoder based domain adaptation for speaker recognition under insufficient channel information",
      "arxiv": "arXiv:1708.01227"
    },
    {
      "citation_id": "200",
      "title": "Unsupervised domain adaptation via domain adversarial training for speaker recognition",
      "authors": [
        "Q Wang",
        "W Rao",
        "S Sun",
        "L Xie",
        "E Chng",
        "H Li"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "201",
      "title": "Generative adversarial speaker embedding networks for domain robust end-toend speaker verification",
      "authors": [
        "G Bhattacharya",
        "J Monteiro",
        "J Alam",
        "P Kenny"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "202",
      "title": "Autoencoder-based unsupervised domain adaptation for speech emotion recognition",
      "authors": [
        "J Deng",
        "Z Zhang",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2014",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "203",
      "title": "Universum autoencoder-based domain adaptation for speech emotion recognition",
      "authors": [
        "J Deng",
        "X Xu",
        "Z Zhang",
        "S Frühholz",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "204",
      "title": "Transferable positive/negative speech emotion recognition via class-wise adversarial domain adaptation",
      "authors": [
        "H Zhou",
        "K Chen"
      ],
      "year": "2018",
      "venue": "Transferable positive/negative speech emotion recognition via class-wise adversarial domain adaptation",
      "arxiv": "arXiv:1810.12782"
    },
    {
      "citation_id": "205",
      "title": "Barking up the right tree: Improving cross-corpus speech emotion recognition with adversarial discriminative domain generalization (addog)",
      "authors": [
        "J Gideon",
        "M Mcinnis",
        "E Provost"
      ],
      "year": "2019",
      "venue": "Barking up the right tree: Improving cross-corpus speech emotion recognition with adversarial discriminative domain generalization (addog)",
      "arxiv": "arXiv:1903.12094"
    },
    {
      "citation_id": "206",
      "title": "A unified architecture for natural language processing: Deep neural networks with multitask learning",
      "authors": [
        "R Collobert",
        "J Weston"
      ],
      "year": "2008",
      "venue": "Proceedings of the 25th international conference on Machine learning"
    },
    {
      "citation_id": "207",
      "title": "New types of deep neural network learning for speech recognition and related applications: An overview",
      "authors": [
        "L Deng",
        "G Hinton",
        "B Kingsbury"
      ],
      "year": "2013",
      "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "208",
      "title": "Fast r-cnn",
      "authors": [
        "R Girshick"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "209",
      "title": "Learning to learn, chapter multitask learning",
      "authors": [
        "R Caruana"
      ],
      "year": "1998",
      "venue": "Learning to learn, chapter multitask learning"
    },
    {
      "citation_id": "210",
      "title": "Multi-task learning strategies for a recurrent neural net in a hybrid tied-posteriors acoustic model",
      "authors": [
        "J Stadermann",
        "W Koska",
        "G Rigoll"
      ],
      "year": "2005",
      "venue": "Ninth European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "211",
      "title": "Rapid adaptation for deep neural networks through multi-task learning",
      "authors": [
        "Z Huang",
        "J Li",
        "S Siniscalchi",
        "I.-F Chen",
        "J Wu",
        "C.-H Lee"
      ],
      "year": "2015",
      "venue": "Sixteenth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "212",
      "title": "Speaker adaptation of deep neural networks using a hierarchy of output layers",
      "authors": [
        "R Price",
        "K -I. Iso",
        "K Shinoda"
      ],
      "year": "2014",
      "venue": "2014 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "213",
      "title": "Multitask learning in connectionist speech recognition",
      "authors": [
        "Y Lu",
        "F Lu",
        "S Sehgal",
        "S Gupta",
        "J Du",
        "C Tham",
        "P Green",
        "V Wan"
      ],
      "year": "2004",
      "venue": "Proceedings of the Australian International Conference on Speech Science and Technology"
    },
    {
      "citation_id": "214",
      "title": "Speech enhancement and recognition using multi-task learning of long short-term memory recurrent neural networks",
      "authors": [
        "Z Chen",
        "S Watanabe",
        "H Erdogan",
        "J Hershey"
      ],
      "year": "2015",
      "venue": "Sixteenth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "215",
      "title": "Emobed: Strengthening monomodal emotion recognition via training with crossmodal emotion embeddings",
      "authors": [
        "J Han",
        "Z Zhang",
        "Z Ren",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "216",
      "title": "Towards speech emotion recognition\" in the wild\" using aggregated corpora and deep multi-task learning",
      "authors": [
        "J Kim",
        "G Englebienne",
        "K Truong",
        "V Evers"
      ],
      "year": "2017",
      "venue": "Towards speech emotion recognition\" in the wild\" using aggregated corpora and deep multi-task learning",
      "arxiv": "arXiv:1708.03920"
    },
    {
      "citation_id": "217",
      "title": "Jointly predicting arousal, valence and dominance with multi-task learning",
      "authors": [
        "S Parthasarathy",
        "C Busso"
      ],
      "year": "2017",
      "venue": "Jointly predicting arousal, valence and dominance with multi-task learning"
    },
    {
      "citation_id": "218",
      "title": "A multi-task learning framework for emotion recognition using 2D continuous space",
      "authors": [
        "R Xia",
        "Y Liu"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "219",
      "title": "Predicting categorical emotions by jointly learning primary and secondary emotions through multitask learning",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech",
      "doi": "10.21437/Interspeech.2018-2464"
    },
    {
      "citation_id": "220",
      "title": "Advanced LSTM: A study about better time dependency modeling in emotion recognition",
      "authors": [
        "F Tao",
        "G Liu"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "221",
      "title": "Cross-corpus acoustic emotion recognition with multi-task learning: Seeking common ground while preserving differences",
      "authors": [
        "B Zhang",
        "E Provost",
        "G Essl"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "222",
      "title": "Multi-task learning for speech recognition: an overview",
      "authors": [
        "G Pironkov",
        "S Dupont",
        "T Dutoit"
      ],
      "year": "2016",
      "venue": "ESANN"
    },
    {
      "citation_id": "223",
      "title": "Self-taught learning: transfer learning from unlabeled data",
      "authors": [
        "R Raina",
        "A Battle",
        "H Lee",
        "B Packer",
        "A Ng"
      ],
      "year": "2007",
      "venue": "Proceedings of the 24th international conference on Machine learning"
    },
    {
      "citation_id": "224",
      "title": "A self learning vocal interface for speech-impaired users",
      "authors": [
        "B Ons",
        "N Tessema",
        "J Van De Loo",
        "J Gemmeke",
        "G De Pauw",
        "W Daelemans",
        "H Van Hamme"
      ],
      "year": "2013",
      "venue": "Proceedings of the Fourth Workshop on Speech and Language Processing for Assistive Technologies"
    },
    {
      "citation_id": "225",
      "title": "Autoencoder based sample selection for self-taught learning",
      "authors": [
        "S Feng",
        "M Duarte"
      ],
      "year": "2018",
      "venue": "Autoencoder based sample selection for self-taught learning",
      "arxiv": "arXiv:1808.01574"
    },
    {
      "citation_id": "226",
      "title": "Reinforcement learning in robotics: A survey",
      "authors": [
        "J Kober",
        "J Bagnell",
        "J Peters"
      ],
      "year": "2013",
      "venue": "The International Journal of Robotics Research"
    },
    {
      "citation_id": "227",
      "title": "Deep reinforcement learning: A brief survey",
      "authors": [
        "K Arulkumaran",
        "M Deisenroth",
        "M Brundage",
        "A Bharath"
      ],
      "year": "2017",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "228",
      "title": "Deepmdp: Learning continuous latent space models for representation learning",
      "authors": [
        "C Gelada",
        "S Kumar",
        "J Buckman",
        "O Nachum",
        "M Bellemare"
      ],
      "year": "2019",
      "venue": "Deepmdp: Learning continuous latent space models for representation learning",
      "arxiv": "arXiv:1906.02736"
    },
    {
      "citation_id": "229",
      "title": "Learning structured representation for text classification via reinforcement learning",
      "authors": [
        "T Zhang",
        "M Huang",
        "L Zhao"
      ],
      "year": "2018",
      "venue": "Thirty-Second AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "230",
      "title": "Reinforcement learning of dialogue strategies with hierarchical abstract machines",
      "authors": [
        "H Cuayáhuitl",
        "S Renals",
        "O Lemon",
        "H Shimodaira"
      ],
      "year": "2006",
      "venue": "2006 IEEE Spoken Language Technology Workshop"
    },
    {
      "citation_id": "231",
      "title": "Deep reinforcement learning for dialogue generation",
      "authors": [
        "J Li",
        "W Monroe",
        "A Ritter",
        "M Galley",
        "J Gao",
        "D Jurafsky"
      ],
      "year": "2016",
      "venue": "Deep reinforcement learning for dialogue generation",
      "arxiv": "arXiv:1606.01541"
    },
    {
      "citation_id": "232",
      "title": "Corrective and reinforcement learning for speaker-independent continuous speech recognition",
      "authors": [
        "K.-F Lee",
        "S Mahajan"
      ],
      "year": "1990",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "233",
      "title": "Emorl: continuous acoustic emotion classification using deep reinforcement learning",
      "authors": [
        "E Lakomkin",
        "M Zamani",
        "C Weber",
        "S Magg",
        "S Wermter"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Robotics and Automation (ICRA)"
    },
    {
      "citation_id": "234",
      "title": "Active discriminative text representation learning",
      "authors": [
        "Y Zhang",
        "M Lease",
        "B Wallace"
      ],
      "year": "2017",
      "venue": "Thirty-First AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "235",
      "title": "Unsupervised and active learning in automatic speech recognition for call classification",
      "authors": [
        "D Hakkani-Tur",
        "G Tur",
        "M Rahim",
        "G Riccardi"
      ],
      "year": "2004",
      "venue": "2004 IEEE International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "236",
      "title": "Active learning: Theory and applications to automatic speech recognition",
      "authors": [
        "G Riccardi",
        "D Hakkani-Tur"
      ],
      "year": "2005",
      "venue": "IEEE transactions on speech and audio processing"
    },
    {
      "citation_id": "237",
      "title": "Active learning for speech recognition: the power of gradients",
      "authors": [
        "J Huang",
        "R Child",
        "V Rao",
        "H Liu",
        "S Satheesh",
        "A Coates"
      ],
      "year": "2016",
      "venue": "Active learning for speech recognition: the power of gradients",
      "arxiv": "arXiv:1612.03226"
    },
    {
      "citation_id": "238",
      "title": "Cooperative learning and its application to emotion recognition from speech",
      "authors": [
        "Z Zhang",
        "E Coutinho",
        "J Deng",
        "B Schuller"
      ],
      "year": "2015",
      "venue": "IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP)"
    },
    {
      "citation_id": "239",
      "title": "On human machine cooperative learning control",
      "authors": [
        "M Dong",
        "Z Sun"
      ],
      "year": "2003",
      "venue": "Proceedings of the 2003 IEEE International Symposium on Intelligent Control"
    },
    {
      "citation_id": "240",
      "title": "Speech analysis in the big data era",
      "authors": [
        "B Schuller"
      ],
      "year": "2015",
      "venue": "International Conference on Text, Speech, and Dialogue"
    },
    {
      "citation_id": "241",
      "title": "Applying cooperative machine learning to speed up the annotation of social signals in large multi-modal corpora",
      "authors": [
        "J Wagner",
        "T Baur",
        "Y Zhang",
        "M Valstar",
        "B Schuller",
        "E André"
      ],
      "year": "2018",
      "venue": "Applying cooperative machine learning to speed up the annotation of social signals in large multi-modal corpora",
      "arxiv": "arXiv:1802.02565"
    },
    {
      "citation_id": "242",
      "title": "Leveraging unlabeled data for emotion recognition with enhanced collaborative semi-supervised learning",
      "authors": [
        "Z Zhang",
        "J Han",
        "J Deng",
        "X Xu",
        "F Ringeval",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "243",
      "title": "Speech separation based on signal-noise-dependent deep neural networks for robust speech recognition",
      "authors": [
        "Y.-H Tu",
        "J Du",
        "L.-R Dai",
        "C.-H Lee"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "244",
      "title": "This work is the extended version of paper",
      "venue": "IEEE Transactions on Affective Computing 2021"
    },
    {
      "citation_id": "245",
      "title": "Investigation of speech separation as a front-end for noise robust speech recognition",
      "authors": [
        "A Narayanan",
        "D Wang"
      ],
      "year": "2014",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "246",
      "title": "Graph-based semi-supervised acoustic modeling in dnn-based speech recognition",
      "authors": [
        "Y Liu",
        "K Kirchhoff"
      ],
      "year": "2014",
      "venue": "2014 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "247",
      "title": "Investigation of full-sequence training of deep belief networks for speech recognition",
      "authors": [
        "D A.-R. Mohamed",
        "L Yu",
        "Deng"
      ],
      "year": "2010",
      "venue": "Eleventh Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "248",
      "title": "Deep belief networks using discriminative features for phone recognition",
      "authors": [
        "T A.-R. Mohamed",
        "G Sainath",
        "B Dahl",
        "G Ramabhadran",
        "M Hinton",
        "Picheny"
      ],
      "year": "2011",
      "venue": "ICASSP"
    },
    {
      "citation_id": "249",
      "title": "Feature mapping using deep belief networks for robust speech recognition",
      "authors": [
        "M Gholamipoor",
        "B Nasersharif"
      ],
      "year": "2014",
      "venue": "The Modares Journal of Electrical Engineering"
    },
    {
      "citation_id": "250",
      "title": "Audio-visual deep learning for noise robust speech recognition",
      "authors": [
        "J Huang",
        "B Kingsbury"
      ],
      "year": "2013",
      "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "251",
      "title": "Dbn-based spectral feature representation for statistical parametric speech synthesis",
      "authors": [
        "Y.-J Hu",
        "Z.-H Ling"
      ],
      "year": "2016",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "252",
      "title": "Roles of pre-training and fine-tuning in context-dependent dbn-hmms for real-world speech recognition",
      "authors": [
        "D Yu",
        "L Deng",
        "G Dahl"
      ],
      "year": "2010",
      "venue": "Proc. NIPS Workshop on Deep Learning and Unsupervised Feature Learning"
    },
    {
      "citation_id": "253",
      "title": "Improved bottleneck features using pretrained deep neural networks",
      "authors": [
        "D Yu",
        "M Seltzer"
      ],
      "year": "2011",
      "venue": "Twelfth annual conference of the international speech communication association"
    },
    {
      "citation_id": "254",
      "title": "Deep neural networks employing multi-task learning and stacked bottleneck features for speech synthesis",
      "authors": [
        "Z Wu",
        "C Valentini-Botinhao",
        "O Watts",
        "S King"
      ],
      "year": "2015",
      "venue": "2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "255",
      "title": "Exploiting sparseness in deep neural networks for large vocabulary speech recognition",
      "authors": [
        "D Yu",
        "F Seide",
        "G Li",
        "L Deng"
      ],
      "year": "2012",
      "venue": "2012 IEEE International conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "256",
      "title": "Exploiting lowdimensional structures to enhance DNN based acoustic modeling in speech recognition",
      "authors": [
        "P Dighe",
        "G Luyet",
        "A Asaei",
        "H Bourlard"
      ],
      "year": "2016",
      "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "257",
      "title": "Convolutional neural networks for speech recognition",
      "authors": [
        "O Abdel-Hamid",
        "A -R. Mohamed",
        "H Jiang",
        "L Deng",
        "G Penn",
        "D Yu"
      ],
      "year": "2014",
      "venue": "IEEE/ACM Transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "258",
      "title": "Time-frequency convolutional networks for robust speech recognition",
      "authors": [
        "V Mitra",
        "H Franco"
      ],
      "year": "2015",
      "venue": "2015 IEEE Workshop on Automatic Speech Recognition and Understanding"
    },
    {
      "citation_id": "259",
      "title": "Invariant representations for noisy speech recognition",
      "authors": [
        "D Serdyuk",
        "K Audhkhasi",
        "P Brakel",
        "B Ramabhadran",
        "S Thomas",
        "Y Bengio"
      ],
      "year": "2016",
      "venue": "Invariant representations for noisy speech recognition",
      "arxiv": "arXiv:1612.01928"
    },
    {
      "citation_id": "260",
      "title": "Using deep belief networks for vector-based speaker recognition",
      "authors": [
        "W Campbell"
      ],
      "year": "2014",
      "venue": "Fifteenth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "261",
      "title": "I-vector modeling with deep belief networks for multi-session speaker recognition",
      "authors": [
        "O Ghahabi",
        "J Hernando"
      ],
      "year": "2014",
      "venue": "I-vector modeling with deep belief networks for multi-session speaker recognition"
    },
    {
      "citation_id": "262",
      "title": "Speaker recognition by means of deep belief networks",
      "authors": [
        "V Vasilakakis",
        "S Cumani",
        "P Laface",
        "P Torino"
      ],
      "year": "2013",
      "venue": "Proc. Biometric Technologies in Forensic Science"
    },
    {
      "citation_id": "263",
      "title": "Improvement of distant-talking speaker identification using bottleneck features of dnn",
      "authors": [
        "T Yamada",
        "L Wang",
        "A Kai"
      ],
      "year": "2013",
      "venue": "Interspeech"
    },
    {
      "citation_id": "264",
      "title": "End-to-end textdependent speaker verification",
      "authors": [
        "G Heigold",
        "I Moreno",
        "S Bengio",
        "N Shazeer"
      ],
      "year": "2016",
      "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "265",
      "title": "Joint training of expanded end-to-end DNN for text-dependent speaker verification",
      "authors": [
        "H.-S Heo",
        "J.-W Jung",
        "I -H. Yang",
        "S -H. Yoon",
        "H.-J Yu"
      ],
      "year": "2017",
      "venue": "Joint training of expanded end-to-end DNN for text-dependent speaker verification"
    },
    {
      "citation_id": "266",
      "title": "An investigation of augmenting speaker representations to improve speaker normalisation for dnn-based speech recognition",
      "authors": [
        "H Huang",
        "K Sim"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "267",
      "title": "X-vectors: Robust DNN embeddings for speaker recognition",
      "authors": [
        "D Snyder",
        "D Garcia-Romero",
        "G Sell",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "268",
      "title": "S-vector: A discriminative representation derived from i-vector for speaker verification",
      "authors": [
        "Y Is ¸ik",
        "H Erdogan",
        "R Sarikaya"
      ],
      "year": "2015",
      "venue": "2015 23rd European Signal Processing Conference"
    },
    {
      "citation_id": "269",
      "title": "End-to-end attention based text-dependent speaker verification",
      "authors": [
        "S.-X Zhang",
        "Z Chen",
        "Y Zhao",
        "J Li",
        "Y Gong"
      ],
      "year": "2016",
      "venue": "2016 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "270",
      "title": "End-to-end text-independent speaker verification with triplet loss on short utterances",
      "authors": [
        "C Zhang",
        "K Koishida"
      ],
      "year": "2017",
      "venue": "Interspeech"
    },
    {
      "citation_id": "271",
      "title": "Application of convolutional neural networks to speaker recognition in noisy conditions",
      "authors": [
        "M Mclaren",
        "Y Lei",
        "N Scheffer",
        "L Ferrer"
      ],
      "year": "2014",
      "venue": "Fifteenth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "272",
      "title": "Speaker identification and clustering using convolutional neural networks",
      "authors": [
        "Y Lukic",
        "C Vogt",
        "O Dürr",
        "T Stadelmann"
      ],
      "year": "2016",
      "venue": "2016 IEEE 26th international workshop on machine learning for signal processing"
    },
    {
      "citation_id": "273",
      "title": "Improved gender independent speaker recognition using convolutional neural network based bottleneck features",
      "authors": [
        "S Ranjan",
        "J Hansen"
      ],
      "year": "2017",
      "venue": "Improved gender independent speaker recognition using convolutional neural network based bottleneck features"
    },
    {
      "citation_id": "274",
      "title": "What does the speaker embedding encode?",
      "authors": [
        "S Wang",
        "Y Qian",
        "K Yu"
      ],
      "year": "2017",
      "venue": "What does the speaker embedding encode?"
    },
    {
      "citation_id": "275",
      "title": "Frame-level speaker embeddings for text-independent speaker recognition and analysis of end-to-end model",
      "authors": [
        "S Shon",
        "H Tang",
        "J Glass"
      ],
      "year": "2018",
      "venue": "2018 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "276",
      "title": "Generalised discriminative transform via curriculum learning for speaker recognition",
      "authors": [
        "E Marchi",
        "S Shum",
        "K Hwang",
        "S Kajarekar",
        "S Sigtia",
        "H Richards",
        "R Haynes",
        "Y Kim",
        "J Bridle"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "277",
      "title": "Advances in deep neural network approaches to speaker recognition",
      "authors": [
        "M Mclaren",
        "Y Lei",
        "L Ferrer"
      ],
      "year": "2015",
      "venue": "2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "278",
      "title": "Deep speaker: an end-to-end neural speaker embedding system",
      "authors": [
        "C Li",
        "X Ma",
        "B Jiang",
        "X Li",
        "X Zhang",
        "X Liu",
        "Y Cao",
        "A Kannan",
        "Z Zhu"
      ],
      "year": "2017",
      "venue": "Deep speaker: an end-to-end neural speaker embedding system",
      "arxiv": "arXiv:1705.02304"
    },
    {
      "citation_id": "279",
      "title": "Conditional generative adversarial networks for speech enhancement and noise-robust speaker verification",
      "authors": [
        "D Michelsanti",
        "Z.-H Tan"
      ],
      "year": "2017",
      "venue": "Conditional generative adversarial networks for speech enhancement and noise-robust speaker verification",
      "arxiv": "arXiv:1709.01703"
    },
    {
      "citation_id": "280",
      "title": "Investigating different representations for modeling and controlling multiple emotions in dnn-based speech synthesis",
      "authors": [
        "J Lorenzo-Trueba",
        "G Henter",
        "S Takaki",
        "J Yamagishi",
        "Y Morino",
        "Y Ochiai"
      ],
      "year": "2018",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "281",
      "title": "Learning utterance-level representations for speech emotion and age/gender recognition using deep neural networks",
      "authors": [
        "Z.-Q Wang",
        "I Tashev"
      ],
      "year": "2017",
      "venue": "2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "282",
      "title": "Optimized multi-channel deep neural network with 2D graphical representation of acoustic speech features for emotion recognition",
      "authors": [
        "M Stolar",
        "M Lech",
        "I Burnett"
      ],
      "year": "2014",
      "venue": "2014 8th International Conference on Signal Processing and Communication Systems (ICSPCS)"
    },
    {
      "citation_id": "283",
      "title": "High-level feature representation using recurrent neural network for speech emotion recognition",
      "authors": [
        "J Lee",
        "I Tashev"
      ],
      "year": "2015",
      "venue": "Sixteenth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "284",
      "title": "Attention assisted discovery of subutterance structure in speech emotion recognition",
      "authors": [
        "C.-W Huang",
        "S Narayanan"
      ],
      "year": "2016",
      "venue": "Attention assisted discovery of subutterance structure in speech emotion recognition"
    },
    {
      "citation_id": "285",
      "title": "Learning spectrotemporal features with 3D CNNs for speech emotion recognition",
      "authors": [
        "J Kim",
        "K Truong",
        "G Englebienne",
        "V Evers"
      ],
      "year": "2017",
      "venue": "2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "286",
      "title": "An experimental study of speech emotion recognition based on deep convolutional neural networks",
      "authors": [
        "W Zheng",
        "J Yu",
        "Y Zou"
      ],
      "year": "2015",
      "venue": "2015 international conference on affective computing and intelligent interaction (ACII)"
    },
    {
      "citation_id": "287",
      "title": "Learning salient features for speech emotion recognition using convolutional neural networks",
      "authors": [
        "Q Mao",
        "M Dong",
        "Z Huang",
        "Y Zhan"
      ],
      "year": "2014",
      "venue": "IEEE transactions on multimedia"
    },
    {
      "citation_id": "288",
      "title": "An attention pooling based representation learning method for speech emotion recognition",
      "authors": [
        "P Li",
        "Y Song",
        "I Mcloughlin",
        "W Guo",
        "L Dai"
      ],
      "year": "2018",
      "venue": "Interspeech"
    },
    {
      "citation_id": "289",
      "title": "Exploring spatio-temporal representations by integrating attention-based bidirectional-lstm-rnns and fcns for speech emotion recognition",
      "authors": [
        "Z Zhao",
        "Y Zheng",
        "Z Zhang",
        "H Wang",
        "Y Zhao",
        "C Li"
      ],
      "year": "2018",
      "venue": "Exploring spatio-temporal representations by integrating attention-based bidirectional-lstm-rnns and fcns for speech emotion recognition"
    },
    {
      "citation_id": "290",
      "title": "Investigation on joint representation learning for robust feature extraction in speech emotion recognition",
      "authors": [
        "D Luo",
        "Y Zou",
        "D Huang"
      ],
      "year": "2018",
      "venue": "Interspeech"
    },
    {
      "citation_id": "291",
      "title": "Speech emotion recognition using convolutional and recurrent neural networks",
      "authors": [
        "W Lim",
        "D Jang",
        "T Lee"
      ],
      "year": "2016",
      "venue": "2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "292",
      "title": "Estimating phoneme class conditional probabilities from raw speech signal using convolutional neural networks",
      "authors": [
        "D Palaz",
        "R Collobert",
        "M Doss"
      ],
      "year": "2013",
      "venue": "Estimating phoneme class conditional probabilities from raw speech signal using convolutional neural networks",
      "arxiv": "arXiv:1304.1018"
    },
    {
      "citation_id": "293",
      "title": "On learning to identify genders from raw speech signal using cnns",
      "authors": [
        "S Kabil",
        "H Muckenhirn",
        "M Magimai-Doss"
      ],
      "year": "2018",
      "venue": "Interspeech"
    },
    {
      "citation_id": "294",
      "title": "Convolutional neural networks for acoustic modeling of raw time signal in lvcsr",
      "authors": [
        "P Golik",
        "Z Tüske",
        "R Schlüter",
        "H Ney"
      ],
      "year": "2015",
      "venue": "Convolutional neural networks for acoustic modeling of raw time signal in lvcsr"
    },
    {
      "citation_id": "295",
      "title": "Very deep convolutional neural networks for raw waveforms",
      "authors": [
        "W Dai",
        "C Dai",
        "S Qu",
        "J Li",
        "S Das"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "296",
      "title": "Deep convolutional and LSTM neural networks for acoustic modelling in automatic speech recognition",
      "authors": [
        "X Liu"
      ],
      "year": "2018",
      "venue": "Deep convolutional and LSTM neural networks for acoustic modelling in automatic speech recognition"
    },
    {
      "citation_id": "297",
      "title": "Learning filterbanks from raw speech for phone recognition",
      "authors": [
        "N Zeghidour",
        "N Usunier",
        "I Kokkinos",
        "T Schaiz",
        "G Synnaeve",
        "E Dupoux"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "298",
      "title": "Feature learning with raw-waveform cldnns for voice activity detection",
      "authors": [
        "R Zazo Candil",
        "T Sainath",
        "G Simko",
        "C Parada"
      ],
      "year": "2016",
      "venue": "Feature learning with raw-waveform cldnns for voice activity detection"
    },
    {
      "citation_id": "299",
      "title": "Speaker recognition from raw waveform with sincnet",
      "authors": [
        "M Ravanelli",
        "Y Bengio"
      ],
      "year": "2018",
      "venue": "2018 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "300",
      "title": "Avoiding speaker overfitting in end-to-end dnns using raw waveform for textindependent speaker verification",
      "authors": [
        "J.-W Jung",
        "H.-S Heo",
        "I.-H Yang",
        "H.-J Shim",
        "H.-J Yu"
      ],
      "year": "2018",
      "venue": "Avoiding speaker overfitting in end-to-end dnns using raw waveform for textindependent speaker verification"
    },
    {
      "citation_id": "301",
      "title": "Speaker recognition from raw waveform with SincNet",
      "authors": [
        "M Ravanelli",
        "Y Bengio"
      ],
      "year": "2018",
      "venue": "2018 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "302",
      "title": "A complete end-to-end speaker verification system using deep neural networks: From raw signals to verification result",
      "authors": [
        "J.-W Jung",
        "H.-S Heo",
        "I.-H Yang",
        "H.-J Shim",
        "H.-J Yu"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "303",
      "title": "Improving emotion identification using phone posteriors in raw speech waveform based dnn",
      "authors": [
        "M Sarma",
        "P Ghahremani",
        "D Povey",
        "N Goel",
        "K Sarma",
        "N Dehak"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "304",
      "title": "Combining a vector space representation of linguistic context with a deep neural network for text-to-speech synthesis",
      "authors": [
        "H Lu",
        "S King",
        "O Watts"
      ],
      "year": "2013",
      "venue": "Eighth ISCA Workshop on Speech Synthesis"
    },
    {
      "citation_id": "305",
      "title": "Exploring hierarchical speech representations with a deep convolutional neural network",
      "authors": [
        "D Hau",
        "K Chen"
      ],
      "year": "2011",
      "venue": "UKCI 2011 Accepted Papers"
    },
    {
      "citation_id": "306",
      "title": "An unsupervised autoregressive model for speech representation learning",
      "authors": [
        "Y.-A Chung",
        "W.-N Hsu",
        "H Tang",
        "J Glass"
      ],
      "year": "2019",
      "venue": "An unsupervised autoregressive model for speech representation learning",
      "arxiv": "arXiv:1904.03240"
    },
    {
      "citation_id": "307",
      "title": "Audio word2vec: Unsupervised learning of audio segment representations using sequence-to-sequence autoencoder",
      "authors": [
        "Y.-A Chung",
        "C.-C Wu",
        "C.-H Shen",
        "H.-Y Lee",
        "L.-S Lee"
      ],
      "year": "2016",
      "venue": "Audio word2vec: Unsupervised learning of audio segment representations using sequence-to-sequence autoencoder",
      "arxiv": "arXiv:1603.00982"
    },
    {
      "citation_id": "308",
      "title": "Reverberant speech recognition based on denoising autoencoder",
      "authors": [
        "T Ishii",
        "H Komiyama",
        "T Shinozaki",
        "Y Horiuchi",
        "S Kuroiwa"
      ],
      "year": "2013",
      "venue": "Interspeech"
    },
    {
      "citation_id": "309",
      "title": "Speech enhancement with weighted denoising auto-encoder",
      "authors": [
        "B Xia",
        "C Bao"
      ],
      "year": "2013",
      "venue": "Speech enhancement with weighted denoising auto-encoder"
    },
    {
      "citation_id": "310",
      "title": "Deep neural network-based bottleneck feature and denoising autoencoder-based dereverberation for distant-talking speaker identification",
      "authors": [
        "Z Zhang",
        "L Wang",
        "A Kai",
        "T Yamada",
        "W Li",
        "M Iwahashi"
      ],
      "year": "2015",
      "venue": "EURASIP Journal on Audio, Speech, and Music Processing"
    },
    {
      "citation_id": "311",
      "title": "Neural discrete representation learning",
      "authors": [
        "A Van Den Oord",
        "O Vinyals"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "312",
      "title": "Hierarchical generative modeling for controllable speech synthesis",
      "authors": [
        "W.-N Hsu",
        "Y Zhang",
        "R Weiss",
        "H Zen",
        "Y Wu",
        "Y Wang",
        "Y Cao",
        "Y Jia",
        "Z Chen",
        "J Shen"
      ],
      "year": "2018",
      "venue": "Hierarchical generative modeling for controllable speech synthesis",
      "arxiv": "arXiv:1810.07217"
    },
    {
      "citation_id": "313",
      "title": "Speaker diarization using latent space clustering in generative adversarial network",
      "authors": [
        "M Pal",
        "M Kumar",
        "R Peri",
        "T Park",
        "S Kim",
        "C Lord",
        "S Bishop",
        "S Narayanan"
      ],
      "year": "2019",
      "venue": "Speaker diarization using latent space clustering in generative adversarial network",
      "arxiv": "arXiv:1910.11398"
    },
    {
      "citation_id": "314",
      "title": "Speech emotion recognition using a deep autoencoder",
      "authors": [
        "N Cibau",
        "E Albornoz",
        "H Rufiner"
      ],
      "year": "2013",
      "venue": "Anales de la XV Reunion de Procesamiento de la Informacion y Control"
    },
    {
      "citation_id": "315",
      "title": "Unsupervised learning approach to feature analysis for automatic speech emotion recognition",
      "authors": [
        "S Eskimez",
        "Z Duan",
        "W Heinzelman"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "316",
      "title": "Modeling feature representations for affective speech using generative adversarial networks",
      "authors": [
        "S Sahu",
        "R Gupta",
        "C Espy-Wilson"
      ],
      "year": "2019",
      "venue": "Modeling feature representations for affective speech using generative adversarial networks",
      "arxiv": "arXiv:1911.00030"
    },
    {
      "citation_id": "317",
      "title": "Semi-supervised training for improving data efficiency in end-to-end speech synthesis",
      "authors": [
        "Y.-A Chung",
        "Y Wang",
        "W.-N Hsu",
        "Y Zhang",
        "R Skerry-Ryan"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "318",
      "title": "Wasserstein GAN and waveform loss-based acoustic model training for multi-speaker text-to-speech synthesis systems using a wavenet vocoder",
      "authors": [
        "Y Zhao",
        "S Takaki",
        "H.-T Luong",
        "J Yamagishi",
        "D Saito",
        "N Minematsu"
      ],
      "year": "2018",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "319",
      "title": "A unified approach to transfer learning of deep neural networks with applications to speaker adaptation in automatic speech recognition",
      "authors": [
        "Z Huang",
        "S Siniscalchi",
        "C.-H Lee"
      ],
      "year": "2016",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "320",
      "title": "Unsupervised cross-lingual knowledge transfer in DNN-based LVCSR",
      "authors": [
        "P Swietojanski",
        "A Ghoshal",
        "S Renals"
      ],
      "year": "2012",
      "venue": "2012 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "321",
      "title": "Cross corpus speech emotion classification-an effective transfer learning technique",
      "authors": [
        "S Latif",
        "R Rana",
        "S Younis",
        "J Qadir",
        "J Epps"
      ],
      "year": "2018",
      "venue": "Cross corpus speech emotion classification-an effective transfer learning technique",
      "arxiv": "arXiv:1801.06353"
    },
    {
      "citation_id": "322",
      "title": "Cross-lingual and multilingual speech emotion recognition on english and french",
      "authors": [
        "M Neumann"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "323",
      "title": "Domain adversarial for acoustic emotion recognition",
      "authors": [
        "M Abdelwahab",
        "C Busso"
      ],
      "year": "2018",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "324",
      "title": "Towards adversarial learning of speaker-invariant representation for speech emotion recognition",
      "authors": [
        "M Tu",
        "Y Tang",
        "J Huang",
        "X He",
        "B Zhou"
      ],
      "year": "2019",
      "venue": "Towards adversarial learning of speaker-invariant representation for speech emotion recognition",
      "arxiv": "arXiv:1903.09606"
    },
    {
      "citation_id": "325",
      "title": "Introducing sharedhidden-layer autoencoders for transfer learning and their application in acoustic emotion recognition",
      "authors": [
        "J Deng",
        "R Xia",
        "Z Zhang",
        "Y Liu",
        "B Schuller"
      ],
      "year": "2014",
      "venue": "2014 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "326",
      "title": "Recognizing emotions from whispered speech based on acoustic feature transfer learning",
      "authors": [
        "J Deng",
        "S Frühholz",
        "Z Zhang",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "327",
      "title": "Multi-objective learning and mask-based post-processing for deep neural network based speech enhancement",
      "authors": [
        "Y Xu",
        "J Du",
        "Z Huang",
        "L.-R Dai",
        "C.-H Lee"
      ],
      "year": "2017",
      "venue": "Multi-objective learning and mask-based post-processing for deep neural network based speech enhancement",
      "arxiv": "arXiv:1703.07172"
    },
    {
      "citation_id": "328",
      "title": "Multi-task learning in deep neural networks for improved phoneme recognition",
      "authors": [
        "M Seltzer",
        "J Droppo"
      ],
      "year": "2013",
      "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "329",
      "title": "Speaker-aware training of LSTM-RNNs for acoustic modelling",
      "authors": [
        "T Tan",
        "Y Qian",
        "D Yu",
        "S Kundu",
        "L Lu",
        "K Sim",
        "X Xiao",
        "Y Zhang"
      ],
      "year": "2016",
      "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "330",
      "title": "Modeling speaker variability using long shortterm memory networks for speech recognition",
      "authors": [
        "X Li",
        "X Wu"
      ],
      "year": "2015",
      "venue": "Sixteenth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "331",
      "title": "Collaborative joint training with multitask recurrent model for speech and speaker recognition",
      "authors": [
        "Z Tang",
        "L Li",
        "D Wang",
        "R Vipperla",
        "Z Tang",
        "L Li",
        "D Wang",
        "R Vipperla"
      ],
      "year": "2017",
      "venue": "IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP)"
    },
    {
      "citation_id": "332",
      "title": "Noise and metadata sensitive bottleneck features for improving speaker recognition with non-native speech input",
      "authors": [
        "Y Qian",
        "J Tao",
        "D Suendermann-Oeft",
        "K Evanini",
        "A Ivanov",
        "V Ramanarayanan"
      ],
      "year": "2016",
      "venue": "Noise and metadata sensitive bottleneck features for improving speaker recognition with non-native speech input"
    },
    {
      "citation_id": "333",
      "title": "Multi-task learning for text-dependent speaker verification",
      "authors": [
        "N Chen",
        "Y Qian",
        "K Yu"
      ],
      "year": "2015",
      "venue": "Sixteenth annual conference of the international speech communication association"
    },
    {
      "citation_id": "334",
      "title": "Learning discriminative features for speaker identification and verification",
      "authors": [
        "S Yadav",
        "A Rai"
      ],
      "year": "2018",
      "venue": "Learning discriminative features for speaker identification and verification"
    },
    {
      "citation_id": "335",
      "title": "Multi-task recurrent model for speech and speaker recognition",
      "authors": [
        "Z Tang",
        "L Li",
        "D Wang"
      ],
      "year": "2016",
      "venue": "2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "336",
      "title": "A multi-task learning framework for emotion recognition using 2D continuous space",
      "authors": [
        "R Xia",
        "Y Liu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "337",
      "title": "Multi-task deep neural network with shared hidden layers: Breaking down the wall between emotion representations",
      "authors": [
        "Y Zhang",
        "Y Liu",
        "F Weninger",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "338",
      "title": "Speech emotion recognition via attention-based DNN from multi-task learning",
      "authors": [
        "F Ma",
        "W Gu",
        "W Zhang",
        "S Ni",
        "S.-L Huang",
        "L Zhang"
      ],
      "year": "2018",
      "venue": "Proceedings of the 16th ACM Conference on Embedded Networked Sensor Systems"
    },
    {
      "citation_id": "339",
      "title": "Attention-augmented end-to-end multi-task learning for emotion prediction from speech",
      "authors": [
        "Z Zhang",
        "B Wu",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "340",
      "title": "A multitask approach to continuous five-dimensional affect sensing in natural speech",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2012",
      "venue": "ACM Transactions on Interactive Intelligent Systems (TiiS)"
    },
    {
      "citation_id": "341",
      "title": "Discretized continuous speech emotion recognition with multi-task deep recurrent neural network",
      "authors": [
        "D Le",
        "Z Aldeneh",
        "E Provost"
      ],
      "year": "2017",
      "venue": "Discretized continuous speech emotion recognition with multi-task deep recurrent neural network"
    },
    {
      "citation_id": "342",
      "title": "Speakerinvariant affective representation learning via adversarial training",
      "authors": [
        "H Li",
        "M Tu",
        "J Huang",
        "S Narayanan",
        "P Georgiou"
      ],
      "year": "2019",
      "venue": "Speakerinvariant affective representation learning via adversarial training",
      "arxiv": "arXiv:1911.01533"
    },
    {
      "citation_id": "343",
      "title": "Training very deep networks",
      "authors": [
        "R Srivastava",
        "K Greff",
        "J Schmidhuber"
      ],
      "year": "2015",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "344",
      "title": "Going deeper with convolutions",
      "authors": [
        "C Szegedy",
        "W Liu",
        "Y Jia",
        "P Sermanet",
        "S Reed",
        "D Anguelov",
        "D Erhan",
        "V Vanhoucke",
        "A Rabinovich"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "345",
      "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "authors": [
        "A Saxe",
        "J Mcclelland",
        "S Ganguli"
      ],
      "year": "2013",
      "venue": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "arxiv": "arXiv:1312.6120"
    },
    {
      "citation_id": "346",
      "title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "347",
      "title": "Improving training of deep neural networks via singular value bounding",
      "authors": [
        "K Jia",
        "D Tao",
        "S Gao",
        "X Xu"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "348",
      "title": "Weight normalization: A simple reparameterization to accelerate training of deep neural networks",
      "authors": [
        "T Salimans",
        "D Kingma"
      ],
      "year": "2016",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "349",
      "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "authors": [
        "S Ioffe",
        "C Szegedy"
      ],
      "year": "2015",
      "venue": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "arxiv": "arXiv:1502.03167"
    },
    {
      "citation_id": "350",
      "title": "Unsupervised latent behavior manifold learning from acoustic features: Audio2behavior",
      "authors": [
        "H Li",
        "B Baucom",
        "P Georgiou"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "351",
      "title": "Towards learning fine-grained disentangled representations from speech",
      "authors": [
        "Y Gong",
        "C Poellabauer"
      ],
      "year": "2018",
      "venue": "Towards learning fine-grained disentangled representations from speech",
      "arxiv": "arXiv:1808.02939"
    },
    {
      "citation_id": "352",
      "title": "Wasserstein gan",
      "authors": [
        "M Arjovsky",
        "S Chintala",
        "L Bottou"
      ],
      "year": "2017",
      "venue": "Wasserstein gan",
      "arxiv": "arXiv:1701.07875"
    },
    {
      "citation_id": "353",
      "title": "Towards principled methods for training generative adversarial networks. arxiv",
      "authors": [
        "M Arjovsky",
        "L Bottou"
      ],
      "year": "2017",
      "venue": "Towards principled methods for training generative adversarial networks. arxiv"
    },
    {
      "citation_id": "354",
      "title": "Stabilizing training of generative adversarial networks through regularization",
      "authors": [
        "K Roth",
        "A Lucchi",
        "S Nowozin",
        "T Hofmann"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "355",
      "title": "Automatic recognition of connected vowels only using speaker-invariant representation of speech dynamics",
      "authors": [
        "S Asakawa",
        "N Minematsu",
        "K Hirose"
      ],
      "year": "2007",
      "venue": "Eighth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "356",
      "title": "Explaining and harnessing adversarial examples",
      "authors": [
        "I Goodfellow",
        "J Shlens",
        "C Szegedy"
      ],
      "year": "2014",
      "venue": "Explaining and harnessing adversarial examples",
      "arxiv": "arXiv:1412.6572"
    },
    {
      "citation_id": "357",
      "title": "The limitations of deep learning in adversarial settings",
      "authors": [
        "N Papernot",
        "P Mcdaniel",
        "S Jha",
        "M Fredrikson",
        "Z Celik",
        "A Swami"
      ],
      "year": "2016",
      "venue": "2016 IEEE European Symposium on Security and Privacy"
    },
    {
      "citation_id": "358",
      "title": "Deepfool: a simple and accurate method to fool deep neural networks",
      "authors": [
        "S.-M Moosavi-Dezfooli",
        "A Fawzi",
        "P Frossard"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "359",
      "title": "Audio adversarial examples: Targeted attacks on speech-to-text",
      "authors": [
        "N Carlini",
        "D Wagner"
      ],
      "year": "2018",
      "venue": "2018 IEEE Security and Privacy Workshops (SPW)"
    },
    {
      "citation_id": "360",
      "title": "Deep speech: Scaling up end-to-end speech recognition",
      "authors": [
        "A Hannun",
        "C Case",
        "J Casper",
        "B Catanzaro",
        "G Diamos",
        "E Elsen",
        "R Prenger",
        "S Satheesh",
        "S Sengupta",
        "A Coates"
      ],
      "year": "2014",
      "venue": "Deep speech: Scaling up end-to-end speech recognition",
      "arxiv": "arXiv:1412.5567"
    },
    {
      "citation_id": "361",
      "title": "Did you hear that? adversarial examples against automatic speech recognition",
      "authors": [
        "M Alzantot",
        "B Balaji",
        "M Srivastava"
      ],
      "year": "2018",
      "venue": "Did you hear that? adversarial examples against automatic speech recognition",
      "arxiv": "arXiv:1801.00554"
    },
    {
      "citation_id": "362",
      "title": "Adversarial attacks against automatic speech recognition systems via psychoacoustic hiding",
      "authors": [
        "L Schönherr",
        "K Kohls",
        "S Zeiler",
        "T Holz",
        "D Kolossa"
      ],
      "year": "2018",
      "venue": "Adversarial attacks against automatic speech recognition systems via psychoacoustic hiding",
      "arxiv": "arXiv:1808.05665"
    },
    {
      "citation_id": "363",
      "title": "Threat of adversarial attacks on deep learning in computer vision: A survey",
      "authors": [
        "N Akhtar",
        "A Mian"
      ],
      "year": "2018",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "364",
      "title": "Noisy training for deep neural networks in speech recognition",
      "authors": [
        "S Yin",
        "C Liu",
        "Z Zhang",
        "Y Lin",
        "D Wang",
        "J Tejedor",
        "T Zheng",
        "Y Li"
      ],
      "year": "2015",
      "venue": "EURASIP Journal on Audio, Speech, and Music Processing"
    },
    {
      "citation_id": "365",
      "title": "Incomplete big data clustering algorithm using feature selection and partial distance",
      "authors": [
        "F Bu",
        "Z Chen",
        "Q Zhang",
        "X Wang"
      ],
      "year": "2014",
      "venue": "2014 5th International Conference on Digital Home"
    },
    {
      "citation_id": "366",
      "title": "Non-local auto-encoder with collaborative stabilization for image restoration",
      "authors": [
        "R Wang",
        "D Tao"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "367",
      "title": "librosa: Audio and music signal analysis in python",
      "authors": [
        "B Mcfee",
        "C Raffel",
        "D Liang",
        "D Ellis",
        "M Mcvicar",
        "E Battenberg",
        "O Nieto"
      ],
      "year": "2015",
      "venue": "Proceedings of the 14th python in science conference"
    },
    {
      "citation_id": "368",
      "title": "pyaudioanalysis: An open-source python library for audio signal analysis",
      "authors": [
        "T Giannakopoulos"
      ],
      "year": "2015",
      "venue": "PloS one"
    },
    {
      "citation_id": "369",
      "title": "OpenSMILE -the Munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "370",
      "title": "The cmu sphinx-4 speech recognition system",
      "authors": [
        "P Lamere",
        "P Kwok",
        "E Gouvea",
        "B Raj",
        "R Singh",
        "W Walker",
        "M Warmuth",
        "P Wolf"
      ],
      "year": "2003",
      "venue": "IEEE Intl. Conf. on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "371",
      "title": "The kaldi speech recognition toolkit",
      "authors": [
        "D Povey",
        "A Ghoshal",
        "G Boulianne",
        "L Burget",
        "O Glembek",
        "N Goel",
        "M Hannemann",
        "P Motlicek",
        "Y Qian",
        "P Schwarz"
      ],
      "year": "2011",
      "venue": "IEEE 2011 workshop on automatic speech recognition and understanding"
    },
    {
      "citation_id": "372",
      "title": "Julius-an open source real-time large vocabulary recognition engine",
      "authors": [
        "A Lee",
        "T Kawahara",
        "K Shikano"
      ],
      "year": "2001",
      "venue": "Julius-an open source real-time large vocabulary recognition engine"
    },
    {
      "citation_id": "373",
      "title": "Espnet: End-to-end speech processing toolkit",
      "authors": [
        "S Watanabe",
        "T Hori",
        "S Karita",
        "T Hayashi",
        "J Nishitoba",
        "Y Unno",
        "N Soplin",
        "J Heymann",
        "M Wiesner",
        "N Chen"
      ],
      "year": "2018",
      "venue": "Espnet: End-to-end speech processing toolkit",
      "arxiv": "arXiv:1804.00015"
    },
    {
      "citation_id": "374",
      "title": "Large vocabulary continuous speech recognition using htk",
      "authors": [
        "P Woodland",
        "J Odell",
        "V Valtchev",
        "S Young"
      ],
      "year": "1994",
      "venue": "ICASSP"
    },
    {
      "citation_id": "375",
      "title": "Alize 3.0-open source toolkit for state-of-the-art speaker recognition",
      "authors": [
        "A Larcher",
        "J.-F Bonastre",
        "B Fauve",
        "K Lee",
        "C Lévy",
        "H Li",
        "J Mason",
        "J.-Y Parfait"
      ],
      "year": "2013",
      "venue": "Alize 3.0-open source toolkit for state-of-the-art speaker recognition"
    },
    {
      "citation_id": "376",
      "title": "Openear-introducing the munich open-source emotion and affect recognition toolkit",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2009",
      "venue": "2009 3rd international conference on affective computing and intelligent interaction and workshops"
    },
    {
      "citation_id": "377",
      "title": "In-datacenter performance analysis of a tensor processing unit",
      "authors": [
        "N Jouppi",
        "C Young",
        "N Patil",
        "D Patterson",
        "G Agrawal",
        "R Bajwa",
        "S Bates",
        "S Bhatia",
        "N Boden",
        "A Borchers"
      ],
      "year": "2017",
      "venue": "2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA)"
    },
    {
      "citation_id": "378",
      "title": "Quantum supremacy using a programmable superconducting processor",
      "authors": [
        "F Arute",
        "K Arya",
        "R Babbush",
        "D Bacon",
        "J Bardin",
        "R Barends",
        "R Biswas",
        "S Boixo",
        "F Brandao",
        "D Buell"
      ],
      "year": "2019",
      "venue": "Nature"
    },
    {
      "citation_id": "379",
      "title": "Gansynth: Adversarial neural audio synthesis",
      "authors": [
        "J Engel",
        "K Agrawal",
        "S Chen",
        "I Gulrajani",
        "C Donahue",
        "A Roberts"
      ],
      "year": "2019",
      "venue": "Gansynth: Adversarial neural audio synthesis",
      "arxiv": "arXiv:1902.08710"
    },
    {
      "citation_id": "380",
      "title": "Melgan: Generative adversarial networks for conditional waveform synthesis",
      "authors": [
        "K Kumar",
        "R Kumar",
        "T De Boissiere",
        "L Gestin",
        "W Teoh",
        "J Sotelo",
        "A De Brebisson",
        "Y Bengio",
        "A Courville"
      ],
      "year": "2019",
      "venue": "Melgan: Generative adversarial networks for conditional waveform synthesis",
      "arxiv": "arXiv:1910.06711"
    },
    {
      "citation_id": "381",
      "title": "Deep fakes: a looming challenge for privacy, democracy, and national security",
      "authors": [
        "R Chesney",
        "D Citron"
      ],
      "year": "2018",
      "venue": "Deep fakes: a looming challenge for privacy, democracy, and national security"
    },
    {
      "citation_id": "382",
      "title": "Unpaired image-to-image translation using cycle-consistent adversarial networks",
      "authors": [
        "J.-Y Zhu",
        "T Park",
        "P Isola",
        "A Efros"
      ],
      "year": "2017",
      "venue": "Proceedings"
    },
    {
      "citation_id": "383",
      "title": "Low-resource domain adaptation for speaker recognition using cycle-gans",
      "authors": [
        "P Nidadavolu",
        "S Kataria",
        "J Villalba",
        "N Dehak"
      ],
      "year": "2019",
      "venue": "Low-resource domain adaptation for speaker recognition using cycle-gans",
      "arxiv": "arXiv:1910.11909"
    },
    {
      "citation_id": "384",
      "title": "Cyclegan-based emotion style transfer as data augmentation for speech emotion recognition",
      "authors": [
        "F Bao",
        "M Neumann",
        "N Vu"
      ],
      "year": "2019",
      "venue": "Cyclegan-based emotion style transfer as data augmentation for speech emotion recognition"
    },
    {
      "citation_id": "385",
      "title": "Disentangling the independently controllable factors of variation by interacting with the world",
      "authors": [
        "V Thomas",
        "E Bengio",
        "W Fedus",
        "J Pondard",
        "P Beaudoin",
        "H Larochelle",
        "J Pineau",
        "D Precup",
        "Y Bengio"
      ],
      "year": "2018",
      "venue": "Disentangling the independently controllable factors of variation by interacting with the world",
      "arxiv": "arXiv:1802.09484"
    },
    {
      "citation_id": "386",
      "title": "Privacy-preserving speech processing: cryptographic and string-matching frameworks show promise",
      "authors": [
        "M Pathak",
        "B Raj",
        "S Rane",
        "P Smaragdis"
      ],
      "year": "2013",
      "venue": "IEEE signal processing magazine"
    },
    {
      "citation_id": "387",
      "title": "Privacypreserving adversarial representation learning in asr: Reality or illusion?",
      "authors": [
        "B Srivastava",
        "A Bellet",
        "M Tommasi",
        "E Vincent"
      ],
      "year": "2019",
      "venue": "Proc. INTERPSPEECH"
    },
    {
      "citation_id": "388",
      "title": "Privacy enhanced multimodal neural representations for emotion recognition",
      "authors": [
        "E Provost"
      ],
      "year": "2019",
      "venue": "Privacy enhanced multimodal neural representations for emotion recognition",
      "arxiv": "arXiv:1910.13212"
    },
    {
      "citation_id": "389",
      "title": "Privacy-preserving deep learning",
      "authors": [
        "R Shokri",
        "V Shmatikov"
      ],
      "year": "2015",
      "venue": "Proceedings of the 22nd ACM SIGSAC conference on computer and communications security"
    }
  ]
}