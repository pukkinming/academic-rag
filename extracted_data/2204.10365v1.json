{
  "paper_id": "2204.10365v1",
  "title": "Towards An Enhanced Understanding Of Bias In Pre-Trained Neural Language Models: A Survey With Special Emphasis On Affective Bias",
  "published": "2022-04-21T18:51:19Z",
  "authors": [
    "Anoop K.",
    "Manjary P. Gangan",
    "Deepak P.",
    "Lajish V. L"
  ],
  "keywords": [
    "NLP Bias",
    "Fairness",
    "Large Pre-trained Language Models",
    "Affective Bias",
    "Affective Computing"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The remarkable progress in Natural Language Processing (NLP) brought about by deep learning, particularly with the recent advent of large pre-trained neural language models, is brought into scrutiny as several studies began to discuss and report potential biases in NLP applications. Bias in NLP is found to originate from latent historical biases encoded by humans into textual data which gets perpetuated or even amplified by NLP algorithm. We present a survey to comprehend bias in large pre-trained language models, analyze the stages at which they occur in these models, and various ways in which these biases could be quantified and mitigated. Considering wide applicability of textual affective computing based downstream tasks in real-world systems such as business, healthcare, education, etc., we give a special emphasis on investigating bias in the context of affect (emotion) i.e., Affective Bias, in large pre-trained language models. We present a summary of various bias evaluation corpora that help to aid future research and discuss challenges in the research on bias in pre-trained language models. We believe that our attempt to draw a comprehensive view of bias in pre-trained language models, and especially the exploration of affective bias will be highly beneficial to researchers interested in this evolving field. The examples provided in this paper may be offensive in nature and may hurt your moral beliefs.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Natural Language Processing (NLP) has recently achieved rapid progress with the aid of deep learning, especially Pre-trained Language Models (PLM)  [50] .\n\nLarge PLMs like BERT  [31] , GPT  [88] , etc., are highly efficient at capturing linguistic properties and producing representations of text with semantic and contextual information. Inclusion of contextual representations has led large PLMs to become popular towards addressing many downstream tasks such as Question Answering, Sentiment Analysis, Neural Machine Translation, etc  [87] . These data greedy Language Models (LM) are generally trained on large-scale human generated textual corpora. However, since ancient days, language has functioned as a channel to express and propagate unfairness towards marginalized social groups and assign power to oppressive institutions  [29] . It is often very hard to analyze the quality of data in large corpora in context of such oppressive nature of language  [118] . Yet, these human generated textual corpora can carry plenty of harmful linguistic biases and social stereotypes that can lead NLP algorithms to produce unfair discrimination towards socially marginalized populations when deployed in real-word  [78] . A threatening scenario that was identified with the use of large PLM GPT-3  [19]  has been experimentally demonstrated in  [2] , for example, 'Two Muslims walked into a ', is completed by GPT3 with 'synagogue with axes and a bomb' and 'gay bar in Seattle and started shooting at will, killing five people'. This is evidently discriminatory and is probably due to islamophobia manifesting in the training text.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Potential Harms Of Bias In Nlp",
      "text": "Bias in NLP could perpetuate harms towards marginalized populations of society in different ways. Allocational and representational harms are the prominent ones engendered by the existence of biased discrimination and stereotypes in language  [118] . Allocational harms deny opportunities and resources across marginalized social groups (e.g. recidivism prediction system 3  ), whereas representational harms generate falsifications of these groups (e.g. caste and religion based discrimination  [96] ). Harms are also brought by the exclusionary social norms in language  [118] . For example, the social norms of 'family' is normally conveyed by humans as a basic social unit consisting of a married woman, man and their children; language models internalizing such social norms often end up being highly discriminatory towards people who live outside the institution of these social norms. Another nuanced notion of linguistic harm is detecting certain languages of marginalized or underrepresented groups as toxic in hate speech detection, since there is no precise universally admissible definition for toxicity  4  . Biased representation of emotions in language leads to another linguistic harm, affective harm, that discriminate marginalized social groups on the basis of certain emotions e.g. the 'angry black woman' stereotype  [75] . If PLMs are learned from a corpus that have latent male chauvinist events, the NLP systems that use them may exhibit affective harms towards females. Other types of linguistic harms include performance drop for certain social groups and languages, generation of nonsensical data and misinformation, etc.  [118] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Heterogeneous View Of Bias In Plms",
      "text": "Bias in pre-trained language models can be viewed through different perspectives, domains of bias and stages in which they occur. We illustrate this heterogeneous view of PLM biases in figure  1 . Bias in PLMs may be seen as belonging to two categories, viz., descriptive and stylistic. Descriptive biases arise from discrimination or marginalization in associating identities to certain concepts or properties based on textual semantics, e.g. word embeddings associate father to doctor and mother to nurse  [16] . Stylistic biases originate due to stylistic differences in texts with same content but generated by different socio-economic groups  [100] , e.g. unfair treatment to African American English while using language identification tools and dependency parsers  [15] . Bias in PLMs are analyzed in various domains, either primary analysis of bias with respect to the domains such as gender, race, ethnicity, age, profession, etc., or analyzing intersectional bias by considering a combination of multiple domains such as re-ligion+gender (e.g. Muslim lady), race+gender (e.g. black woman), etc. Table  1  shows works in the literature that explore bias with respect to different domains where a major portion of works relate to the gender domain. When considering the stages at which bias can occur in the context of large PLMs, data or/and algorithm design are generally the two major stages. Bias in data can arise either or both, from the pre-training or fine-tuning corpus. Algorithm bias may originate from self-supervised learning algorithms that yield non-contextual or contextual representations  [16, 102]  or/and from fine-tuning learning algorithm designed for downstream tasks  [47] .\n\nIn this paper, we survey bias in NLP, especially in pre-trained neural language models. We also give special attention to the less explored area of social biases in the context of affect i.e., Affective Bias (or emotion associated bias) specific to large PLMs. Since affective computing has potential applications in many natural language understanding tools and real-word systems (healthcare  [44] , business  [54, 105] , education  [34, 105] , etc.), it is highly necessary to study the existence of affective biases, if any, in these systems that could potentially harm or do injustice towards protected social groups based on affect. We review more than 100 papers that address bias in PLMs including non-contextual and contextual models. We collect research papers from ACL anthology, Google Scholar and arXiv, using the keywords 'bias', 'fairness', 'bias in NLP', 'fairness in NLP', 'Sentiment bias', 'Affective bias', 'Emotion bias' 'bias in pre-trained language models', etc. as the inclusion criteria for our survey. The major contributions of this survey are summarised below:\n\n• We present a comprehensive survey of bias in pre-trained language models, especially an in-depth treatment of various kinds of bias that originate in transformer based contextual pre-trained language models in NLP along with their identification, quantification and mitigation strategies. • We, for the first time, to the best of our knowledge, investigate Affective Bias, a highly socially relevant and less addressed problem, specifically in the context of large pre-trained language models. • We collect and present a large number of available bias evaluation corpora along with their suitability to evaluate large pre-trained language models. • We also discuss present research challenges in large pre-trained language models and affective biases.\n\nThe rest of the paper is organized as, the background of PLMs and bias in PLMs provided in section 2, quantifying PLM bias in section 3, mitigating PLM bias in section 4, affective bias in PLMs including their identification and mitigation strategies in section 5, a list of available bias evaluation corpora in section 6, research challenges in section 7 and concluding remarks in section 8.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Socioeconomic Status",
      "text": "Poor, Rich, Homeless  [51]  Intersectional Race+Gender (Black Women)  [43, 53, 67, 110]  2 Pre-Trained Language Models",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Background",
      "text": "Advancements in deep learning have brought NLP to a new era led by neural LMs or large PLMs by producing effective representations for textual data, where the dense and automatically extracted representations by PLMs from large textual corpora override sparse and handcrafted representations (e.g. TF-IDF feature) and their associated drawbacks. Hence, such PLM representations are generally used universally as language representations for a variety of downstream tasks in NLP to achieve significant state-of-the-art results while avoiding the burden of training a new model from the scratch  [87] . A representation becomes powerful when it is capable of comprising general purpose characteristics of the language and also useful to learn a variety of tasks. Such a potent representation in the context of language should capture the latent linguistic conventions and common sense knowledge that are hidden in text such as syntax, semantics, pragmatics, etc. A step towards such linguistic representation was the development of non-contextual embedding models by mapping words into a distributed d-dimensional embedding space or vector. The shallow architectures within that stream, such as Continuous Bag-of-Word and Skip-Gram models (word2vec) developed by Mikolov et al.  [74]  from unlabelled data formed initial attempts towards generic language representations. Despite the simplicity in architecture, they are highly capable of learning effective word embeddings that can capture hidden semantic and syntactic similarities among words. Similar to the popular word2vec architecture  [74] , GloVe  [82]  that utilizes word to word co-occurrence statistics from corpora and FastText  [49]  that utilizes sub-word information also attracted significant attention to solve many downstream tasks. However, these embeddings are non-contextual in nature and hence they fail to capture disambiguation, semantic roles, polysemous, syntactic structure, and anaphora, which rely on higher-level contextual concepts. Many researchers proposed different models that are capable of learning embeddings of sentences, paragraphs, and even documents  [59]  despite critiques on not capturing contextual representation of words.\n\nRepresentations of words in documents are contextually dependent in nature since similar words have different semantics in diverse contexts. Therefore, replacing conventional non-contextualized embeddings, recent research has presented a new generation of contextualized embedding models, such as ELMo  [83] , BERT  [31] , etc., that have become increasingly common due to their capability to describe linguistic phenomena such as polysemy. Unlike non-contextualized representations, contextualized word representations are generated using neural contextual encoders and have achieved state-of-the-art performance in most NLP tasks over the conventional embeddings even though their sophisticated nature dents interpretability. There are two major types of neural encoders, sequence models that include convolutional and recurrent models  [83] , and non-sequential models that include fully connected self-attention and advanced transformer architectures  [88, 31, 121] . Contextualized representation models developed with Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN) face difficulty in modeling long term context among other issues  [50] . This brings a new kind of learning model, Transformer  [114] , a fully self-attention based architecture that has much more parallelization compared to RNN and also one that can effectively model longer dependencies and context from textual data. To build any kind of LMs, collecting labeled data and supplying it to supervised learning models is tedious. But, on the positive side, the benefits of LM are often realizable through self-Supervised learning, a new learning paradigm applied to this scenario that utilizes plenty of available unlabelled text data to learn highly generic knowledge representations by automatically generating labels from training data itself based on pseudo supervision  [87]  e.g. masked language model  [31] . The prior non-contextualized pre-trained word embeddings (e.g., word2vec and GloVe) do not give much importance to its utility in other downstream tasks and applicability of fine-tuning strategy. Besides universal contextual word representations, contextualized PLMs such as BERT  [31] , GPT  [88] , XLNet  [121] , etc., is useful to build models that perform better on many downstream NLP tasks by fine-tuning the pre-trained model, crucially avoiding the burden of training the model from the scratch for each downstream task. The auto-encoding pre-trained architecture BERT  [31]  uses masked language model approach and overcomes the limitation of unidirectional autoregressive models like GPT  [88]  by enabling bidirectional contexts, but unavailability of mask in fine-tuning introduces pretrain-finetune discrepancy. XLNet  [121] , a generalized autoregressive pre-training model at the same time achieves better results by introducing random permutations to enable bi-directional context. There also exists a large number of different domain specific (e.g., biomedical  [60] , finance  [120] , etc.), mono and multilingual PLMs that vary based on their architecture or pre-training tasks.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Bias In Pre-Trained Language Models",
      "text": "Even though word representations are powerful enough to capture semantic similarities and exhibit word relationships through word vector similarities, the explicit and implicit existence of several stereotypes and social biases in PLMs harm its usefulness in many real-world applications. Bias in large PLMs arises from different stages of their developmental process. Figure  2  illustrates the workflow of large PLMs along with possible stages where bias may originate, particularly focussing on recent Transformer based PLMs. To mitigate bias it is essential to understand and disentangle the various sources of bias. The investigation on sources of bias leads to observations that human language that form today's data deluge, big enough to train data greedy NLP algorithms, historically accumulate several severe stereotypes and social biases that pervade society i.e., Historical Bias. Language hence is one of the most potent ways through which societal biases are brought about, propagated, and echoed  [72] . Several non-neutral stereotypes live in linguistic communication, imaging asymmetries in terms of dominance, power, quality, or status among target terms such as female and male, blacks and whites belonging to various domains like gender, race, etc.,  [36] . Taking it for granted as normal, people rehearse most of these preconceptions in day-to-day discourse, consequently routinizing these linguistic discrimination and making them be felt less visible  [77] . Therefore, even though we perfectly measure and take data samples from these historical data repositories, these are ridden with biases, i.e., Data Bias, a representative of historical bias, which thereby brings about bias in PLMs  [107] .\n\nData bias stemming from innate historical biases is the most general source of bias among different sources of bias explored in literature for various tasks  [28] , where, quality issues in data, uneven distribution (occurrence or co-occurrences) of key terms in data associated with targets concerning a domain  [18] , etc., are other factors that contribute towards it. Standard datasets used for pre-training non-contextual  [16, 22, 40, 68]  and contextual models  [110]  are found to exhibit bias or imbalance in various domains like gender, race, etc. In the context of large PLMs, data bias may be Pre-trained Data Bias at the initial training process of PLM or/and Fine-tuning Data Bias which is just downstream to it. Studies report that data biases can propagate and get further amplified by underlying machine learning models leading to Model Learning Bias at selfsupervised learning strategy to learn linguistic properties and Downstream Task Learning Bias at the level of the task specific fine-tuning model.\n\nModel learning bias is reflected in word representations derived from PLMs and produce Representation Bias. Non-contextual word embeddings such as word2vec, GloVe, etc., are known to comprise representational biases across gender  [16, 22] , racial  [68]  and ethnic groups  [40] . It has been clearly unveiled that Fig.  2 . Bias in large pre-trained language models these embeddings relate word representations of professions like 'nurse', 'receptionist' and 'homemaker' to women and 'doctor', 'philosopher' and 'computer programmer' to men  [16] . Further, they place the representation of words like 'wisdom' close to representation of 'grandfather' than 'grandmother', and encode association of popular African Americans names with unpleasant phrases  [22] , etc. Examination of bias in contextualized embeddings reveal that they also exhibit bias like conventional embeddings  [18, 126] . For example, BERT is found to encode human-like biases  [55] . Most of the recent NLP information retrieval systems such as search engines, question answering, etc. highly rely on these biased representations consequently leading to highly biased retrieval behavior. Similarly, use of the large PLM GPT-3 in language generation shows religion bias analogizing 'Muslims' to 'Terrorists'  [2] . All such biases are disturbing when one observes that word embeddings, being the base constituent of most language systems, can propagate or even intensify them  [22, 61, 127]  causing unfavorable outcomes when deployed in a plethora of downstream applications such as sentiment analysis  [79] , language generation  [66] , toxic language detection  [48] , etc. Downstream task learning bias can cause their outcomes delivered to the public to finally end up in socio-economic exclusions reinforcing harmful societal stereotypes  [16, 61, 127] . Since downstream applications are generally implemented by initializing learning models with an existing source network representation pre-trained on large datasets and later fine-tuned using datasets that suit downstream target task, pre-training data bias, pre-trained representation bias and fine-tuning data bias all can be the sources to induce bias in downstream applications. Sentiment analysis  [52] , abusive language detection  [81] , text classification  [33] , machine translation  [38, 112] , personalized medicine  [90] , corefer-ence resolution  [95, 128] , crime recidivism prediction systems  [27] , automating resume screening  [56] , online advertisements delivery  [57, 109] , etc., are some of the downstream applications that reports bias in various domains. Inappropriate or unfair choice of label usages to fine-tune downstream task is another source of bias i.e., Measurement Bias  [107] .\n\nThese biases can be distinguished as Intrinsic Bias if it occurs in pre-trained learning or Extrinsic Bias if it occurs in downstream task modeling. Besides above mentioned biases, in the perspective of real-world machine learning models, the final system must consider Evaluation Bias that occurs when benchmark dataset for a task doesn't represent certain groups (e.g., images of non-white women not being identified by the model) and Deployment Bias that occurs due incompatibility of a model designed for a particular task when used differently (e.g., using risk assessment tool created to predict future crime for the purpose of determining length of sentence)  [107] . Table  2  shows a large set of research that identify and mitigate bias from large PLMs, where most of them (around 53%) focus primarily on bias identification.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Quantifying Bias In Plms",
      "text": "In order to study unfavorable consequences of different sources of biases in various domains, it is necessary to quantify bias in some manner. Based on the stages of occurrence, bias are generally quantified in the corpus, representation and downstream tasks.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Quantifying Bias In Corpora",
      "text": "Counting occurrences or co-occurrences of key terms and deriving various statistics from them helps to discover bias in corpora. Bordia and Bowman  [18]  study gender bias in three publicly available datasets, viz., Penn Treebank  [69] , WikiText-2  [73]  and CNN/Daily Mail  [46]  that are used to build language models by finding bias scores built using word-level probability profiles within the context of gendered words by defining fixed and infinite sized context windows around the gendered words. Their bias score helps to find whether words more frequently co-occur with female or male gendered words. For fixed sized context windows, they find optimal sized windows as smaller windows can focus more on target words whereas larger windows can focus on the broader topic. Infinite sized windows are much more stable by using exponentially diminishing weights as the distance between key terms and gendered words increases. Tan et al.  [110]  count occurrences of key terms (e.g. female or male pronouns) and their co-occurrence with stereotypically gendered occupation terms and perform statistical analysis to find gender bias and also racial and intersectional biases in 1 Billion Word Benchmark  [26] , BookCorpus  [131] , Wikipedia, and WebText  [89]  datasets used to pre-train contextual word models.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Quantifying Bias In Representations",
      "text": "Geometry of vector spaces Certain works quantify bias by analyzing subspaces in embeddings. Bolukbasi et al.  [16]  demonstrates the occurrence of gender bias in word embeddings by viewing the difference between word vectors of gendered words like 'sister', 'brother', 'grandmother', 'grandfather', etc., and gender neutral words. They observe a direction that essentially captures gender and projecting gender neutral words in this direction helps them to quantify gender bias. Manzini et al.  [68]  extends this approach to suit multiclass settings and demonstrate that word representations exhibit several stereotypical biases including religion based and racial bias.\n\nWord association test Word association tests to quantify representation bias are inspired from the Implicit Association Tests (IAT) in psychology  [42]  that tries to understand human subconscious bias by measuring differences in the association of target concepts with an attribute. Caliskan et al.  [22]  proposes a statistical test called the Word Embedding Association Test (WEAT) analogous to IAT to quantify bias in non-contextual word embeddings GloVe and Word2Vec. Using WEAT, authors examine the similarity of embeddings of words in complementary categories like European American and African American names with the complementary attributes like pleasant and unpleasant attributes. The dissimilarity between the association of European American names with the attributes when compared to African American names with the same attributes, helps their study to report the existence of human-like implicit bias in embeddings. To test bias in sentence encoders like ELMo and BERT, May et al.  [70]  proposes a generalization of WEAT named the Sentence Encoder Association Test (SEAT). Using SEAT, even though they could verify presence of bias in these embeddings, the results were not very generalizable. They also point out that dissimilarities in the results don't mean contextual embeddings is free of bias, rather it may be an indication of cosine similarity not suitable to measure the similarity of embeddings of recent contextual models, and hence an alternative might be required to quantify bias in such representations. Kurita et al.  [55]  also shows that conventional cosine similarity based methods don't produce consistent results to find bias in sentence embeddings generated from the contextual models, as the embeddings of the words may differ according to the context and state of the language model. As the former tests concentrate only on individual words and predefined stereotypical attributes like pleasant and unpleasant terms in artificial contexts that don't reflect the natural use of words, Nadeem et al.  [76]  proposes two Context Association Tests (CAT) to intrinsically estimate bias in a set of large PLMs. They perform CAT at the sentence level and discourse level, where each target term has natural context, and observe that the recent contextual models BERT, GPT2, RoBERTa, and XLNet exhibit strong biases with respect to their language modeling ability. Building upon WEAT, Guo and Caliskan  [43]  propose Contextualized Embedding Association Test (CEAT) to confirm and extensively quantify biases in neural language models ELMo, BERT, GPT and GPT-2 according to different contexts.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Quantifying Bias In Downstream Tasks",
      "text": "In this category, bias is quantified by checking performance scores of system over evaluation corpus that differs only in the context of target terms in which domain of bias is being studied. For example, gender swapping to change gender of gendered words, like 'She is here' to 'He is here', and then evaluating model performance of these two sentences  [52, 66, 95, 128] . The system exhibits gender bias if it produces different performance scores for both sentences that only differ in gendered words. Dixon et al.  [33]  presents two performance evaluation metrics derived from error rate equality difference to quantify bias in text classifier constructed to identify toxic comments. Park et al.  [81]  utilizes these metrics to quantify bias along with the method to generate gender unbiased dataset proposed by Dixon et al.  [33]  to find gender bias in abusive language detection. Kiritchenko and Mohammad  [52]  utilize difference in predicted intensity scores of sentences that differ in gendered words and race, over their corpus to statistically evaluate gender and racial bias in 219 sentiment analysis models that took part in SemEval 2018: Task 1 Affect In Tweets. Zhao et al.  [128]  illustrates gender bias in coreference systems using F1 score over their evaluation corpus named WinoBias that contains pair of sentences that associate gendered pronouns (her/him) to various female or male stereotypical occupations (secretary/physician). Rudinger et al.  [95]  also perform a similar gender bias study in coreference system over pairs of sentences that differ only in gendered words. Lu et al.  [66]  quantifies gender bias in coreference resolution and language modeling by calculating dissimilarity in performances of gendered words with various occupations across pairs of sentences that are gender swapped. In addition, bias can also be measured using interpretability, by investigating model interpretations on how it reaches to certain decisions or predictions  [35] .",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Mitigating Bias In Plms",
      "text": "Various efforts have been made for mitigating bias in PLMs to reduce or remove their discriminatory influences over underrepresented or non-mainstream groups. We classify debiasing approaches into three categories, based on the stages at which they are addressed.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Mitigating Bias In Training Corpora",
      "text": "Most computational algorithms designed for extrinsic applications like sentiment analysis, text classification, etc., primarily rely on labeled training corpora making them prone to societal biases present in these corpora  [111] . Despite this being the case, such corpora are still being extensively utilized in various applications of NLP as it is expensive or labor-intensive to build new large-sized training corpora. Hence, generally, common techniques such as data augmentation and bias fine-tuning are followed for debiasing training corpora.\n\nData augmentation Data augmentation techniques debias the training corpus by supplying additional data to support the target groups with comparatively fewer data in the corpora and thereby creating a balanced corpus to train on. Augmentation can thus in a way counterbalance the under/over representations of any particular target group of a domain on training corpus. Zhao et al.  [126, 128]  adopts data augmentation to reduce gender bias in the downstream task of coreference resolution while observing that the possibility of associating occupations to masculine terms is very much higher than feminine terms. They augment gender swapped versions of data, from masculine terms to feminine terms and vice-versa, to the training corpus after anonymizing the named entities. Park et al.  [81]  utilizes the idea of data augmentation proposed by Zhao et al.  [128]  to reduce gender bias in abusive language detection. Very similar to Zhao et al.  [128] , Lu et al.  [66]  proposes Counterfactual Data Augmentation to explore gender bias in Neural NLP applications including coreference resolution and language modeling by representing bias with reference to internal scores in neural models. Apart from the attempts to remove gender bias in English language, Zmigrod et al.  [132]  proposes variation to naive gender swapping based counterfactual data augmentation to manage gender bias in morphology rich or inflected languages like Spanish and Hebrew that otherwise deliver ungrammatical sentences with simpler approaches. Maudslay et al.  [45]  improves counterfactual data augmentation by proposing two variants named Counterfactual Data Substitution and Names Intervention to address indirect bias. Liu et al.  [64]  use concept of Maudslay et al.  [45]  and propose Counterpart Data Augmentation to remove biases in dialogue systems. Even though data augmentation based debiasing is a simple technique to reduce bias in the training corpora, their annotation is expensive and increases the size of dataset, in turn, increasing the time required for training. Moreover, these techniques generally only consider isolated words to perform binary swapping and mostly ignore non-binary and more sophisticated representations in a domain. Also, these techniques rely on a pre-defined limited list of key terms and associated pairs, which may be conceivably incomplete, where some terms may have different spelling (e.g. mommy vs. mummy), different morphology (e.g. his→her and his→hers), pairing variations (e.g. breastfeed ), or produce absurd sentences due to blind swaps (e.g. she is pregnant→he is pregnant).\n\nBias fine-tuning Another approach to debias training corpora is bias finetuning by transfer learning, where, instead of the expensive process of constructing balanced corpora with respect to a domain, the idea of transfer learning is employed. As data biases generally originate from data imbalance or small sized datasets, to make sure the model doesn't overfit on biased data, Park et al.  [81]  utilize transfer learning to gender debias abusive language detection model by regularizing from a source network trained on a less biased large dataset and thereafter fine-tune on a target dataset that is largely gender biased. Even though the approach significantly reduces bias, it seems to hurt the overall model accuracy too (as may be expected). Saunders et al.  [97]  proposes an approach almost opposite to Park et al.  [81]  where they execute small-domain adaptation by fine-tuning on a less biased small dataset to address gender bias in neural machine translations.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Mitigating Bias In Representation",
      "text": "An earlier approach to debias the representations is geometric debiasing that removes subspace of protected domain/target terms concerned to a domain in embedding. For a domain, this post-process approach identifies subspace or direction in embedding that holds bias and removes the association of neutral words to target words concerning that domain. This hard debiasing approach is followed by Bolukbasi et al.  [16]  to alleviate gender bias from word embeddings by levelling the distance of gender neutral words towards the set of gendered words. They also propose soft bias correction that aims to conserve the initial embedding distances using a trade-off parameter to balance with debiasing. But as it works by removing gender information from the words, it might not be a generalizable approach such as in social science and medical applications that makes use of these gender information  [8, 71] . Also, their approach make use of classifier to distinguish gender neutral words that in turn can propagate classification errors, influencing the performance of debiasing  [129] . Zhao et al.  [129]  neutralize embedding with respect to gender by locating gender neutral words along with the process of training word vectors without employing an additional classifier. However, Gonen and Goldberg's  [41]  experiments shows that the approaches of Bolukbasi et al.  [16]  and Zhao et al.  [129]  are insufficient blind debiasing techniques that just hides but doesn't actually remove bias. For word level language models, Bordia and Bowman  [18]  make use of a loss function regularizer to penalize the projection of embeddings onto gender subspace as a soft debaising version of Bolukbasi et al.  [16] . As stated by Gonen and Goldberg  [41] , they also mention chances of bias even after debiasing the representations, as a case that their bias score may be not able to detect it. Alternative to this direct geometric debiasing, Zhao et al.  [126]  proposes a different gender neutralisation procedure by averaging the representations of original and corresponding gender swapped data versions for debiasing contextualised representations. To mitigate sentiment bias in word embedding concerned to demographic domains, Sweeney et al.  [108]  removes the correlations of demographic terms with sentiments.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Mitigating Bias In Algorithm",
      "text": "Debiasing representations are harder to be applied in contextual embeddings, since the representation of a word can vary according to the plurality of different contexts it appears in  [67] . In such cases, certain works focus at modifying algorithm/model, i.e., pre-trained or fine-tuned models, in such a way that it mitigates the bias in predictions during training (i.e., in-processing), usually by modifying the loss function of the model. Qian et al.  [86]  alters the loss function of the language model in such a way that the model adapts to equalize prediction probabilities for gendered pairs of words thereby reducing output gender bias. Silva et al.  [102]  consider utilising WEAT for debasing transformer based PLMs and later use WEAT score along with cross entropy to modify loss function of RoBERTa. Huang et al.  [47]  propose another variation of altering regular cross entropy loss function with embedding and sentiment driven regularisation terms for mitigating sentiment bias in large PLMs. Adversarial training is another way to debias algorithms by altering loss functions while training a predictor along with an adversary, which is intended to produce fair prediction by minimising the adversarial function that tries to model protected domains like gender, race, etc.  [124] . A similar approach is utilised by Zhang et al.  [125]  to mitigate gender bias in clinical contextual word embeddings. A different recent approach proposed by Liu et al.  [65]  uses reinforcement learning to debias political bias in large PLMs by utilising rewards from word embedding or a classifier.",
      "page_start": 17,
      "page_end": 18
    },
    {
      "section_name": "Affective Bias",
      "text": "Textual affective computing involves development of algorithms that accurately identify what is subjectively written in text and how to make better affectinfluenced subjective decisions, i.e., decisions that are based on emotions, sentiments, or opinions, in several real-world systems. Many systems were initially proposed to detect and measure affect (emotion or sentiment) expressed by textual data such as in movie reviews, product reviews, etc.  [80, 105] . Later these systems were widely adopted into various domains such as healthcare  [44] , commercial applications  [54, 92, 105] , politics  [21] , education  [34, 105]  and many more. When industrial giants like Google 5  , IBM 6  and Microsoft 7  developed natural language understanding tools, textual affective understanding became a crucial and significant part of them. Several observations and arguments have been made by researchers to demonstrate the importance of textual affective computing tasks, especially towards sentiment analysis. According to Cambria et al.  [23]  sentiment analysis task still needs to travel much to reach human level performance which can be attained by successfully solving several other NLP problems such as POS tagging, Named Entity Recognition, etc. Poria et al.  [84]  draw attention to optimistic future directions in affecting computing like multi-modal affective computing, sarcasm analysis and even the contemporary issue of social bias in affective NLP systems, and strive to oppose the conventional belief that sentiment analysis tasks are saturated being 20 years old. Our study on affective bias in NLP is motivated from these observations on heightened relevance of affective computing and its wide applicability in diversified NLP applications that leverage affect information  [4, 5, 37] .",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Definition And Implications",
      "text": "The study on bias in NLP and machine learning was heavily accelerated through observations on the negative impact of NLP biases towards certain social or marginalized groups when applied in the real world  8  . Affective bias is another recent research category in this direction, where researchers study any unfair or imbalanced associations of affect with key terms representing particular underrepresented or protected groups in a domain and how it influences NLP systems, such as sentiment and emotion detection. For example, Google sentiment analyzer is observed to infer that being gay 9  is bad and assigns high negative sentiments towards sentences such as 'I'm a gay black woman' and 'I'm a homosexual'. By the term Affective bias in NLP, we define the existence of unfair or biased associations of affect (maybe emotions such as anger, fear, joy, etc. or sentiment such as positive, neutral, negative) towards underrepresented groups, or over-generalized beliefs (stereotypes) about particular social groups in textual documents. For example, in textual documents, words associated with women such as 'she', 'wife', etc., are highly associated with a certain category of emotions like 'sadness' and 'anger', and representations of the Muslim religion are observed as being associated with negative terms that indicate violence  [1] , etc. The existence of affective bias in textual affective computing systems harms its utility and applicability in tasks like business and commercial decision making, healthcare, etc. The concept of affective bias is valid and applicable beyond the NLP frameworks because, as in  [20] , there are chances of high classification error rates for facial emotion detection systems towards underrepresented social groups. Similarly, it is crucial to evaluate all kinds of affective computing systems in the backdrop of affective bias, since automated emotion detection systems have a huge impact on modeling human behavior in many intelligent artificial artifacts or algorithms that imitate human emotion systems for their completeness.",
      "page_start": 18,
      "page_end": 19
    },
    {
      "section_name": "Affective Bias In Plms",
      "text": "Many textual affective computing systems including lexicon based  [94] , conventional machine learning  [108] , deep learning  [14, 100] , and hybrid  [32]  approaches perpetuate affective bias, which, in general, is transmitted from emotional bias of humans through models learnt over large scale textual corpora. For example, usage of a corpus that contains textual data where humans had expressed the anger emotion towards a particular underrepresented religion for training an algorithm can later propagate or/and amplify this affect oriented bias stereotyping religion with emotion anger. Table  3  illustrates an extensive snapshot of works in this area of research along with their major characteristics. A predominant part of existing works study affective bias specific to gender and through the perspective of sentiment analysis  [14, 52, 94, 100, 108] . Whereas, other domains like religion, politics, intersectional biases, etc., and their impact in perspective of fine-grained emotion classes (anger, fear, joy sadness, surprise, disgust) have not been investigated as much, except in  [52]  and  [115] . The inadequacy of generic evaluation corpora to perform affective bias evaluation in various domains along with the evaluation sentences having corresponding emotion/sentiment ground truth or output label is a notable gap in this area of research. When most works in literature try to identify the existence of affective bias in NLP systems, only very few explore mitigation of the harms of affective bias. The body of work in affective bias can be split into two categories, conventional approaches and runtime verification approaches. Conventional way of analyzing affective bias tries to identify bias in training data, algorithm, representation, fine-tuning data, and finally, mitigates them using different strategies  [52, 108, 115] . Whereas, runtime verification approaches monitor and uncover biased predictions in each run of a specified system using mutations of input sentences (automatically generated templates from input sentences and their paired sentences that represent different views of a stereotype). Runtime verification is generally suitable to validate whether the system satisfies fairness criteria over each run.\n\nThe conventional approach by Shen et al.  [100]  investigates bias in sentiment prediction for textual write-ups comprising similar content generated by different groups of people. The analysis and identification of bias are conducted on four publicly available lexicons and deep learning based systems. A similar approach by Zhiltsova et al.  [130]  also identifies and mitigates sentiment bias against nonnative English text by using four popular lexicon based emotion prediction systems. Both these works rely on linguistic style changes across different human groups and how it leads to affective bias in NLP. Apart from analysis of affective bias in lexicon and conventional machine learning systems, researchers also explore non-contextual word embeddings such as word2vec, GloVe, and FastText in the context of affective bias  [32, 94, 108] . A significant contribution in this regard is the work by Diaz et al.  [32]  addressing age related affective bias in ten widely used word embeddings and fifteen different sentiment analysis models. The work primarily validates whether opinion polling systems falsely report any age group (old or young) more negatively or positively, for example a sentence with adjectives of 'young' more likely scores positive sentiments than the same sentence with adjectives of 'old'  [32] . Among the similar studies based on noncontextualised word embeddings, Sweeney et al.  [108]  introduce an adversarial learning strategy to mitigate demographic affective bias in word2vec and GloVe, and Rozado et  [94]  screen word embeddings to identify bias through the notion of representing words along cultural axis in the embedding space. A more generic work of Kiritchenko et al.  [52]  identify affective bias in two hundred emotion prediction systems that participated in the shared task SemEval-2018 Task 1 Affect in Tweets. They procure an evaluation corpus, Equity Evaluation Corpus (EEC), one of the few publicly available evaluation corpus that has generic evaluation sentences and ground truth emotion labels as basic emotions anger, fear, joy, and sadness for all evaluation sentences in corpus. A similar evaluation corpus has been proposed by Venkit et al.  [115]  considering sentences from the domain of persons with disabilities.\n\nA more noteworthy approach to quantify affective bias concerning occupational stereotypes in contextualized large PLM, BERT, is discussed in  [14] . Even though more works recently identify and mitigate generic bias in large PLMs due to their efficacy and utility  [63, 76] , very few of them investigate affective bias in large PLMs. Notable works to uncover bias in sentiment analysis systems that utilize popular PLMs such as Google BERT, Facebook RoBERTa, Google AL-BERT, Google ELECTRA, and Facebook Muppet rely on runtime verification approach instead of conventional paradigms to analyze bias  [6, 122] . Another interesting approach by Huang et al.,  [47]  investigates sentiment bias introduced in text generated by language models. This emerging scenario facilitates to conduct more evaluations to identify and mitigate affective bias in large PLMs such as BERT, ALBERT, RoBERTa, XLNet, GPT, etc.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Bias Evaluation Corpora",
      "text": "Ideal bias evaluation corpora are indispensable components that helps to identify and measure the existence of different types of bias in NLP systems. Since this review focuses on bias in PLMs with special emphasis on affective bias, we present popular and publicly available bias evaluation corpora along with their limitations in the backdrop of PLMs and affective biases, in table 4; some of the evaluation corpora are avoided due to their unavailability in the public domain to aid future research (e.g.,  [47, 130] ). The most common attempt to create bias evaluation corpora are by initially building template sentences and interchanging its key terms (e.g. she→he, wife→husband, mother →father ) associated to a target (e.g. Female, Male) concerning a domain (e.g. Gender) which induces bias in natural language. For example, Diaz et al.  [32]  creates bias evaluation corpora intended to identify age related sentiment bias using template sentences like 'This 〈AGE RELATED KEY TERM〉guy was 3 or 4 feet from the tide line and the tide was going out' and interchanging 〈AGE RELATED KEY TERM〉with 'old' and 'young'. Contrary to conventional interchanging of key terms approach, StereoSet  [76]  a much diverse, large-scale and natural bias evaluation corpus procured through inter and intra sentence Context Association Test using Amazon Mechanical Turk. Design and creation of such bias evaluation corpora are often defined carefully to isolate and extract the real effect of bias in various stages such as data bias, algorithm bias, etc., and also to avoid presence of any biased data within evaluation corpus or unbalanced set of key terms referring to a particular target in a domain.\n\nEvaluating large PLMs has recently become highly essential particularly due to their increasing usage in many real-world applications. Several existing bias evaluation corpora are useful enough to evaluate large PLMs when considering the total number, i.e., size, of available evaluation sentences in these corpora  [33, 52, 128] . But these corpora fail to represent diverse real-world contexts as they are generally built synthetically using very simple and short-length sentences that are far from real-world scenarios and may even deliver nonsensical samples (e.g. 'my 〈sister〉is pregnant' →'my 〈brother〉is pregnant' ). To accurately design an efficient bias evaluation corpora that suit evaluating PLMs, Liang et al.  [63]  leverage naturally occurring text corpora from WIKITEXT-2  [73] , SST  [103] , etc., to generate evaluation sentences that can represent real-world context better than the corpus in  [33, 52, 76, 101, 128] . There is also a need for more efficient evaluation corpora to evaluate affective bias; one of the available corpus by Kiritchenko et al.  [52]  contains, as mentioned earlier, very simple synthetic and far from real-world templates where emotions are specified in the sentence explicitly, e.g., 'My brother made me feel angry' with ground-truth label anger. Another affect oriented corpus by Venkit et al.  [115]  much suitably represents real-world context suitable to evaluate PLMs, however, they are specifically designed to address bias concerned to people with disabilities, e.g. 'They were aggravated because of the Mentally Handicapped neighbour' with ground-truth label anger. To effectively test and build fair machine learning software, it is highly essential to develop more generic and real-world context enabled evaluation corpora addressing various domains like religion, race, intersectional bias, non-binary representations of gender (including lesbian, gay, etc., apart from common way of only addressing gender as a binary domain). Such corpora can eventually lead to the usage of bias evaluation corpora as an integral part of any NLP system to evaluate system bias like any other test performed on the system before being deployed in real-world  [24] . Here we discuss several challenges in NLP bias, specifically in the context of large PLMs and also in the context of affective bias.\n\nHeterogeneous nature of NLP bias A large number of research approaches to identify and mitigate NLP biases generally address gender bias (as can be observed from table 1), even though there exist many other domains such as race, religion, intersectional biases, etc., that needs to be addressed significantly. Tan et al.  [110]  provides evidence in this context that racial bias is encoded strongly in contextualized large PLMs, probably even more than gender bias. They also show that the less explored domain of intersectional biases that consider a mixture of two or more domains (e.g. African American Females) is even more higher than primary biases. Also, most existing research only concentrate on addressing a subset of target terms concerned to a domain, e.g. considering gender domain with binary targets terms male and female, instead of its non-binary nature comprising of other target terms like gay and lesbian. All these observations illustrate that the heterogeneous nature of bias in different perspectives must be considered in future works of evaluating bias to make the NLP systems fully debiased.\n\nEvaluation corpus Many existing bias evaluation corpora demonstrate the scarcity of realism in sentences, such as synthetic sentences that are poor in representing real-world context, as well as short-length sentences, etc. Accurate understanding and quantification of existing bias in NLP models may not be effective with these corpora, especially in case of large PLMs, which are capable of generating complex real-world sentences to produce benchmark results in many downstream applications. Most of the large real-world evaluation corpora focus on gender domain, and real-world context based corpora that suit other domains are scarce. In case of quantification and mitigation of affective biases, many researchers use generic evaluation corpora in  [14, 32]  where evaluations do not have an affective output label. A corpus developed by Kiritchenko et al.  [52]  contains relevant emotion labels but lacks real-world contexts and has labels largely derived from explicit mentions of emotions.\n\nGeneralizable design Generalizability of any system enables it to be used directly or indirectly within other allied systems. Bias evaluation is becoming an essential part in NLP systems to produce fair decisions by avoiding serious societal harms. This necessity can be achieved largely by designing generalizable bias identification and mitigation modules. Generalizability in the context of evaluation corpora, metrics to identify and quantify the impact of bias, performance trade-off measures post debiasing, are important challenges to be addressed in the future to bring the vital bias evaluation process as a common and simple component that any NLP system can adapt and experiment.\n\nPre-train vs. Fine-tune bias Bias in downstream applications that utilize pre-trained models originate across different stages of system development. Initially, it may be from massive amount of data used to pre-train the model which may get amplified by the pre-training algorithm. The other way of introducing bias is through fine-tuning data as well as fine-tuning algorithm. An existing challenge in this background is to analyze and understand which part of the overall system really causes bias, (i.e., is it from pre-trained or task specific fine-tuning modules), how bias is perpetuated with the harmonized workflow of system components, which parts of overall system must undergo a mitigation process, etc.\n\nLinguistic diversity and NLP bias Recent NLP research, widely explores the utility of developing large PLMs and downstream applications in various different languages by including capability to handle monolingual as well as bilingual strategies. Transformer based large PLM, IndicBERT 10 which handles low resource and morphologically highly inflected Indian languages like Malayalam, is an example. Similarly, a large variety of monolingual and bilingual corpora 11 (in languages English, Afrikaans, Arabic, etc.) and machine learning models 12 (for various tasks including question answering, text classification, etc.) in different languages illustrate same trends. In this context, it is highly essential to study bias in such different linguistic corpora and the mono and multilingual PLMs learned from those corpora. Among the very few attempts in this direction is the research to mitigate gender bias in Hindi word embedding  [85] , evaluating gender bias in Hindi-English machine translation  [91] , addressing gender bias in languages like Spanish and Hebrew  [132] , etc.\n\nInterdisciplinary methods to analyze NLP bias Sun et al.  [106]  states that many techniques deployed in non NLP systems can be applied directly or with small modifications to identify and mitigate bias in NLP. Several studies indicate that bias in NLP systems opens up scope to associate with other branches such as sociology, psychology, socio-linguistics, engineering, legal studies, etc. The Implicit Association Test  [42]  in psychology and its diverse computational representations such as Word Embedding Association Test  [22]  and Sentence Encoder Association Test  [70]  to quantify representation bias are examples to showcase interdisciplinary nature of removing bias in NLP. Also, lessons from Software Engineering 13 such as unit testing, behavior testing, etc., can be adopted to evaluate and quantify bias in machine learning models at different levels  [93] . Sun et al.  [106]  treats mitigation of NLP bias as a combined problem of sociology and engineering, where sociology can identify how really humans perceive and encode social bias into language. Research towards interdisciplinary discussions can bring light into current bias quantification and mitigation strategies and inspire developing advanced and practically relevant approaches  [7, 12, 99] .",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Conclusion",
      "text": "In this survey, we conducted a comprehensive investigation towards bias in large pre-trained language models, especially the transformer based models. We discuss different types of biases that originate at various stages of pre-trained language model workflow and the methods used to quantify and mitigate these biases. Due to the widespread utility of affective computing systems in the real-world, we give a special emphasis on the less explored area of bias that associates with affect, i.e., affective bias. Our study also lists the popular and publicly available evaluation corpora that aid future research, along with their suitability in large pre-trained language models. Finally, we discuss the challenges in this area of research, addressing which would aid the community to further improve the tasks of identification and mitigation of bias in pre-trained language modes. Materials regarding this survey will be made publicly available at https://github.com/anoopkdcs/NLPBias and we will keep updating it to aid future research.",
      "page_start": 26,
      "page_end": 26
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Heterogeneous view of bias in pre-trained language models",
      "page": 4
    },
    {
      "caption": "Figure 2: illustrates the",
      "page": 7
    },
    {
      "caption": "Figure 2: Bias in large pre-trained language models",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Different domains of bias in pre-trained language models",
      "data": [
        {
          "Domain": "Gender\nRace\nReligion\nProfession\nEthnicity\nDisability\nAge\nPolitics\nContinent\nNationality\nPhysical\nappearance\nSocioeconomic\nstatus\nIntersectional Race+Gender (Black Women)",
          "Examples of Protected/Target groups": "Male, Female, Gay, Lesbian\nBlack, White\nJewish, Hindu, Muslim, Christian\nHomemaker, Nurse, Architect\nAsian, Hispanic\nSensory (blind), Neurodiverse (autis-\ntism), Psychosocial (schizophrenia)\nOld, Young\nConservative, Liberal\nAfrica, Asia, Oceania, Europe\nAmerican, Italian\nShort, Tall, Fat, Thin, Overweight\nPoor, Rich, Homeless",
          "Work": "[6,9,10,11,13,14,16,17,18,25,30]\n[39,43,48,51,52,62,66,67,76,86]\n[94,100,102,104,108,110,113,115]\n[116,119,122,123,125,126,129]\n[51,52,76,100,110,115]\n[2,30,51,76,94]\n[6,14,39,53,76]\n[3,48,58,94]\n[51,67,115]\n[32,51,94]\n[65,94,100]\n[6,30,43]\n[51,98]\n[51,94]\n[51]\n[43,53,67,110]"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 2: shows a large set of research",
      "data": [
        {
          "Work": "[3]\n[113] DistilBERT,\n[39]\n[13]",
          "PLM": "[102] BERT, XLNet,\nALBERT,\nDistilBERT,\nRoBERTa,\nGPT-2\nBERT\nRoBERTa,\nXLM-\nRoBERTa,\nALBERT,\nBERT\nBERT\nBERT",
          "Domain": "Gender\nEthnicity\nGender\nProfession,\nGender\nGender",
          "Quantiﬁcation": "WEAT,\nsequence\nlikeli-\nhood, pronoun ranking\nNormalized\nprobability,\nCategorical bias score\nSkew\nand\nstereotype\nmatrices\nModel querying to pre-\ndict pronouns at masked\nposition, given context\nMLP regressor",
          "Mitigation": "WEAT scores\nas\nan additional\nloss\nregularizer\nM-BERT,\nCon-\ntextual\nWord\nAlignment\nData\naugmenta-\ntion\nGEnder Equality\nPrompt\nRemoving\nword\nvector\ncom-\nponents\nfrom\ngender directions"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 2: – continued from previous page",
      "data": [
        {
          "Work": "[65]\n[48]\n[98]\n[2]\n[104] XLM,\n[51]\n[11]\n[123] BERT",
          "PLM": "GPT-2\nRoBERTa\nGPT-2, T5\nGPT-3\nXLM-\nRoBERTa,\nBERT\nmulti-\nlingual\nBERT,\nRoBERTa,\nALBERT\nEMLo",
          "Domain": "Politics\nEthnicity,\nGender\nOccupation,\nNationality,\nReligion,\nGender,\nAge,\nRace,\nDisability\nReligion\nGender\nGender,\nRace,\nSexual\norientation,\nReligion,\nNationality,\nDisability,\nAge, Physical\nappear-\nance,\nSocio-\neconomic\nstatus\nGender\nGender",
          "Quantiﬁcation": "Indirect and Direct bias\nmetrics\nClassiﬁcation\nPerfor-\nmance, Group identiﬁer\nbias\nmetrics,\nAfrican\nAmerican\nVernacu-\nlar\nEnglish,\nDialect\nBias\nMetrics,\nGender\nStereotype Metrics\nSelf-Diagnosing\nPrompt\ncompletion,\nanalogical\nreasoning,\nstory generation\nPoint-wise mutual\ninfor-\nmation and its extension\nwith latent sentiment &\nregularization\nAll\nUnmasked\nLikeli-\nhood (AUL), AUL with\nAttention weights\nGender direction in gen-\nder subspace\nPrediction\nprobability\nscores",
          "Mitigation": "Reinforcement\nLearning\nUpstream\nBias\nMitigation\nSelf-Debiasing\n—\n—\n—\n—\n—"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 2: – continued from previous page",
      "data": [
        {
          "Work": "[43]\n[67]\n[62]\n[53]\n[76]\n[66]\n[125] BERT\n[9]\n[116] GPT-2\n[110] GPT-2,",
          "PLM": "ELMo,\nGPT,\nBERT, GPT-2\nDistilBERT,\nGPT-2,\nGPT-\nNEO\n[119] BERT, GPT-2,\nT5, XLNet\nBERT\nGPT-2\nBERT,\nRoBERTa,\nXLNET,\nGPT2\nLSTM\npre-\ntrained\non\nmedical notes\nBERT\nCBoW-GLoVe,\nELMo,\nGPT,\nBERT",
          "Domain": "Intersectional\nbias\nLow\nfre-\nquency\nnames\nGender\nIntersectional Predictions of GPT-2\nProfession,\nGender,\nReligion,\nRace\nGender\nEthnicity,\nGender,\nInsurance\ngroups\nProfession,\nGender\nGender\nIntersectional,\nGender, Race",
          "Quantiﬁcation": "Intersectional Intersectional Bias De-\ntection,\nEmergent\nIn-\ntersectional Bias Detec-\ntion, CEAT\nDiﬀerence/similarity\nbetween\nsentiment\nscores\nSV-WEAT,\nIntra\nand\nInter layer self-similarity\nscore\nBERT Attention maps\nLanguage\nmodelling\nscore, Stereotype\nscore,\niCAT\nBias Score,\nIntervention\nmatches\nLog\nprobability\nscore,\nextrinsic\nevaluation us-\ning downstream tasks\nAssociation test\nCausal Mediation Anal-\nysis\nCounting\noccurrences,\nModiﬁed SEAT",
          "Mitigation": "—\n—\n—\n—\n—\n—\nCounterfactual\nData\nAugmen-\ntation,\nWord\nEmbedding\nDebiasing\n—\nName\nbased\nCounterfactual\nData\nSubstitu-\ntion\n—\n—"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 2: – continued from previous page",
      "data": [
        {
          "Work": "[55]\n[126] ELMo\n[86]\n[18]\n[10]\n[47]",
          "PLM": "BERT\nLSTM\nLSTM\nELMo\nTransformer-\nXL",
          "Domain": "Gender\nGender\nGender\nGender\nGender\nCountry,\nOccupation,\nName",
          "Quantiﬁcation": "Log\nProbability\nBias\nScore, WEAT\nCo-occurrence\nOccurrences,\ncausal\ntesting,\nEuclidean\ndistance\nFixed & inﬁnite context\nbias scores\nDirect\nBias,\nBiased\nwords\nclustering\n&\nclassiﬁcation\nIndividual & group fair-\nness metrics from senti-\nment scores",
          "Mitigation": "—\nTrain-time\ndata\naugmentation,\ntest-time neutral-\nization\nLoss\nfunction\nmodiﬁcation\nLoss\nfunction\nmodiﬁcation\n—\nEmbedding\n&\nSentiment\nregu-\nlarization"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table 3: Works addressing affective bias",
      "data": [
        {
          "Work\nDomain\nQuantiﬁcation\nMitigation\nModel": "Sentiment perspective"
        },
        {
          "Work\nDomain\nQuantiﬁcation\nMitigation\nModel": "[122] Gender\n[6]\n[108] Gender\n[94]"
        }
      ],
      "page": 20
    },
    {
      "caption": "Table 3: – continued from previous page",
      "data": [
        {
          "Work": "[14]\n[32]\n[100] Gender,",
          "Domain": "Occupation,\nGender\nAge\nRace,\nPolitics",
          "Quantiﬁcation": "Statistical\nsigniﬁcance\ndiﬀerence\nMultinomial\nlog-liner\nregression, paired t-test\nDiﬀerence in mean sen-\ntiment score, statistical\nsigniﬁcance\ntest,\nlinear\nregression",
          "Mitigation": "—\n—\n—",
          "Model": "BERT,\nBi-LSTM,\nLogistic\nRegres-\nsion\nLexicon\nbased,\nconventional\nma-\nchine\nlearning,\nhybrid\nRule based, Naive\nBayes,\nDynamic\nCNN, LSTM"
        },
        {
          "Work": "Emotion perspective",
          "Domain": "",
          "Quantiﬁcation": "",
          "Mitigation": "",
          "Model": ""
        },
        {
          "Work": "[115] Gender, Race\n[130] Non-native\n[52]",
          "Domain": "En-\nglish speaker\nGender, Race",
          "Quantiﬁcation": "Mean\nscore\nof\npredic-\ntion,\nlinear\nregression\non sentiment scores\nWilcoxon\nsigned\nrank\ntest\nAverage\nscore\ndiﬀer-\nence",
          "Mitigation": "—\nLexical\nscore\n—",
          "Model": "DistilBERT,\nTextBlob,\nGoogle\nAPI, VADER\nVADER,\nAﬁnn,\nSentimentR,\nTextBlob\nDeep\nLearning,\nconventional\nma-\nchine\nlearning,\nlexicon based"
        }
      ],
      "page": 21
    },
    {
      "caption": "Table 4: – continued from previous page",
      "data": [
        {
          "Corpora": "[63]\nhttps://github.com/\npliang279/LM bias\nStereoSet [76] https://github.\ncom/moinnadeem/StereoSet\nBEC-Pro\n[9]\nhttps://github.\ncom/marionbartl/gender-\nbias-BERT\nGenderCorpus\n[14]\nhttps://github.com/\njayadevbhaskaran/gendered-\nsentiment\nAgeBias\n[32]\nhttps:\n//dataverse.harvard.edu/\ndataset.xhtml?persistentId=\ndoi:10.7910/DVN/F6EMTS\nEEC\n[52]\nhttps:\n//saifmohammad.com/\nWebPages/Biases-SA.html\nWinograd\n[95]\nhttps:\n//github.com/rudinger/\nwinogender-schemas\nWinoBias\n[128]\nhttps:\n//github.com/uclanlp/\ncorefBias/tree/master/\nWinoBias/wino\nGAP\n[117]\nhttps://github.\ncom/google-research-\ndatasets/gap-coreference\nIdentity\nsynthetic\ndataset\n[33]\nhttps://github.com/\nconversationai/unintended-\nml-bias-analysis",
          "Domain": "Gender,\nReligion\nGender,\nRace,\nReligion,\nProfession\nGender,\nProfession\nGender,\nProfession\nAge\nRace,\nGender\nGender,\nProfession\nGender,\nProfession\nGender\nHuman\nidentity\n(white,\ngay, etc.)",
          "Model": "Large PLM\nLarge PLM Less diverse\nLarge PLM Less diverse\nSentiment\nanalysis\nSentiment\nanalysis\nEmotion\ndetection\nCoreference\nresolution\nCoreference\nresolution\nCoreference\nresolution\nText\nclassi-\nﬁcation",
          "Limitation": "—\nto represent\nreal-world contexts\nto represent\nreal-world contexts\nGroud-truth\nsentiment\nlabels are not available\nGroud-truth\nsentiment\nlabels are not available\nExplicit representation of\nemotions, small & simple\nsentences,\nless diverse\nto\nrepresent\nreal-world con-\ntexts\nSmall sentences\nMiss ambiguous pronouns\nin suﬃcient volume or di-\nversity to accurately rep-\nresent practical utility of\nmodels\n—\nSmall\nand\nsimple\nsen-\ntences, less diverse to rep-\nresent real-world contexts"
        }
      ],
      "page": 23
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Large language models associate muslims with violence",
      "authors": [
        "A Abid",
        "M Farooqi",
        "J Zou"
      ],
      "year": "2021",
      "venue": "Nature Machine Intelligence",
      "doi": "10.1038/s42256-021-00359-2"
    },
    {
      "citation_id": "2",
      "title": "Persistent Anti-Muslim Bias in Large Language Models",
      "authors": [
        "A Abid",
        "M Farooqi",
        "J Zou"
      ],
      "year": "2021",
      "venue": "Persistent Anti-Muslim Bias in Large Language Models",
      "doi": "10.1145/3461702.3462624"
    },
    {
      "citation_id": "3",
      "title": "Mitigating language-dependent ethnic bias in BERT",
      "authors": [
        "J Ahn",
        "A Oh"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/2021.emnlp-main.42"
    },
    {
      "citation_id": "4",
      "title": "Affect-oriented fake news detection using machine learning",
      "authors": [
        "K Anoop"
      ],
      "year": "2019",
      "venue": "AWSAR Awarded Popular Science Stories By Scientists for the People"
    },
    {
      "citation_id": "5",
      "title": "Emotion cognizance improves health fake news identification",
      "authors": [
        "K Anoop",
        "P Deepak",
        "V Lajish"
      ],
      "year": "2020",
      "venue": "Proceedings of the 24th Symposium on International Database Engineering & Applications. IDEAS '20, Association for Computing Machinery",
      "doi": "10.1145/3410566.3410595"
    },
    {
      "citation_id": "6",
      "title": "Biasfinder: Metamorphic test generation to uncover bias for sentiment analysis systems",
      "authors": [
        "M Asyrofi",
        "Z Yang",
        "I Yusuf",
        "H Kang",
        "F Thung",
        "D Lo"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Software Engineering",
      "doi": "10.1109/TSE.2021.3136169"
    },
    {
      "citation_id": "7",
      "title": "Homophily and the glass ceiling effect in social networks",
      "authors": [
        "C Avin",
        "B Keller",
        "Z Lotker",
        "C Mathieu",
        "D Peleg",
        "Y Pignolet"
      ],
      "venue": "Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science"
    },
    {
      "citation_id": "9",
      "title": "Gender and prescription opioids: Findings from the national survey on drug use and health",
      "authors": [
        "S Back",
        "R Payne",
        "A Simpson",
        "K Brady"
      ],
      "year": "2010",
      "venue": "Addictive Behaviors",
      "doi": "10.1016/j.addbeh.2010.06.018"
    },
    {
      "citation_id": "10",
      "title": "Unmasking contextual stereotypes: Measuring and mitigating BERT's gender bias",
      "authors": [
        "M Bartl",
        "M Nissim",
        "A Gatt"
      ],
      "year": "2020",
      "venue": "Proceedings of the Second Workshop on Gender Bias in Natural Language Processing"
    },
    {
      "citation_id": "11",
      "title": "Evaluating the underlying gender bias in contextualized word embeddings",
      "authors": [
        "C Basta",
        "M Costa-Jussà",
        "N Casas"
      ],
      "year": "2019",
      "venue": "Proceedings of the First Workshop on Gender Bias in Natural Language Processing",
      "doi": "10.18653/v1/W19-3805"
    },
    {
      "citation_id": "12",
      "title": "Extensive study on the underlying gender bias in contextualized word embeddings",
      "authors": [
        "C Basta",
        "M Costa-Jussà",
        "N Casas"
      ],
      "year": "2021",
      "venue": "Neural Computing and Applications",
      "doi": "10.1007/s00521-020-05211-z"
    },
    {
      "citation_id": "13",
      "title": "How stereotypes are shared through language: a review and introduction of the aocial categories and stereotypes communication (scsc) framework",
      "authors": [
        "C Beukeboom",
        "C Burgers"
      ],
      "year": "2019",
      "venue": "Review of Communication Research",
      "doi": "10.12840/issn.2255-4165.017"
    },
    {
      "citation_id": "14",
      "title": "Investigating gender bias in bert",
      "authors": [
        "R Bhardwaj",
        "N Majumder",
        "S Poria"
      ],
      "year": "2021",
      "venue": "Cognitive Computation",
      "doi": "10.1007/s12559-021-09881-2"
    },
    {
      "citation_id": "15",
      "title": "Good secretaries, bad truck drivers? occupational gender stereotypes in sentiment analysis",
      "authors": [
        "J Bhaskaran",
        "I Bhallamudi"
      ],
      "year": "2019",
      "venue": "Proceedings of the First Workshop on Gender Bias in Natural Language Processing",
      "doi": "10.18653/v1/W19-3809"
    },
    {
      "citation_id": "16",
      "title": "Demographic dialectal variation in social media: A case study of African-American English",
      "authors": [
        "S Blodgett",
        "L Green",
        "B O'connor"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/D16-1120"
    },
    {
      "citation_id": "17",
      "title": "Man is to computer programmer as woman is to homemaker? debiasing word embeddings",
      "authors": [
        "T Bolukbasi",
        "K Chang",
        "J Zou",
        "V Saligrama",
        "A Kalai"
      ],
      "year": "2016",
      "venue": "Proceedings of the 30th International Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "18",
      "title": "Quantifying and reducing stereotypes in word embeddings",
      "authors": [
        "T Bolukbasi",
        "K Chang",
        "J Zou",
        "V Saligrama",
        "A Kalai"
      ],
      "year": "2016",
      "venue": "Quantifying and reducing stereotypes in word embeddings",
      "arxiv": "arXiv:1606.06121"
    },
    {
      "citation_id": "19",
      "title": "Identifying and reducing gender bias in word-level language models",
      "authors": [
        "S Bordia",
        "S Bowman"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop",
      "doi": "10.18653/v1/N19-3002"
    },
    {
      "citation_id": "20",
      "title": "Language models are few-shot learners",
      "authors": [
        "T Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J Kaplan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell",
        "S Agarwal",
        "A Herbert-Voss",
        "G Krueger",
        "T Henighan",
        "R Child",
        "A Ramesh",
        "D Ziegler",
        "J Wu",
        "C Winter",
        "C Hesse",
        "M Chen",
        "E Sigler",
        "M Litwin",
        "S Gray",
        "B Chess",
        "J Clark",
        "C Berner",
        "S Mccandlish",
        "A Radford",
        "I Sutskever",
        "D Amodei"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "21",
      "title": "Gender shades: Intersectional accuracy disparities in commercial gender classification",
      "authors": [
        "J Buolamwini",
        "T Gebru"
      ],
      "year": "2018",
      "venue": "Proceedings of the 1st Conference on Fairness, Accountability and Transparency. Proceedings of Machine Learning Research"
    },
    {
      "citation_id": "22",
      "title": "Using sentiment analysis to define twitter political users' classes and their homophily during the 2016 american presidential election",
      "authors": [
        "J Caetano",
        "H Lima",
        "M Santos",
        "H Marques-Neto"
      ],
      "year": "2018",
      "venue": "Journal of internet services and applications",
      "doi": "10.1186/s13174-018-0089-0"
    },
    {
      "citation_id": "23",
      "title": "Semantics derived automatically from language corpora contain human-like biases",
      "authors": [
        "A Caliskan",
        "J Bryson",
        "A Narayanan"
      ],
      "year": "2017",
      "venue": "Science",
      "doi": "10.1126/science.aal4230"
    },
    {
      "citation_id": "24",
      "title": "Sentiment analysis is a big suitcase",
      "authors": [
        "E Cambria",
        "S Poria",
        "A Gelbukh",
        "M Thelwall"
      ],
      "year": "2017",
      "venue": "IEEE Intelligent Systems",
      "doi": "10.1109/MIS.2017.4531228"
    },
    {
      "citation_id": "25",
      "title": "Fairway: A way to build fair ml software",
      "authors": [
        "J Chakraborty",
        "S Majumder",
        "Z Yu",
        "T Menzies"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "doi": "10.1145/3368089.3409697"
    },
    {
      "citation_id": "26",
      "title": "Measuring gender bias in word embeddings across domains and discovering new gender bias word categories",
      "authors": [
        "K Chaloner",
        "A Maldonado"
      ],
      "year": "2019",
      "venue": "Proceedings of the First Workshop on Gender Bias in Natural Language Processing",
      "doi": "10.18653/v1/W19-3804"
    },
    {
      "citation_id": "27",
      "title": "One billion word benchmark for measuring progress in statistical language modeling",
      "authors": [
        "C Chelba",
        "T Mikolov",
        "M Schuster",
        "Q Ge",
        "T Brants",
        "P Koehn"
      ],
      "year": "2013",
      "venue": "Computing Research"
    },
    {
      "citation_id": "28",
      "title": "Fair prediction with disparate impact: A study of bias in recidivism prediction instruments",
      "authors": [
        "A Chouldechova"
      ],
      "year": "2017",
      "venue": "Big Data",
      "doi": "10.1089/big.2016.0047"
    },
    {
      "citation_id": "29",
      "title": "Algorithmic decision making and the cost of fairness",
      "authors": [
        "S Corbett-Davies",
        "E Pierson",
        "A Feller",
        "S Goel",
        "A Huq"
      ],
      "venue": "Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining"
    },
    {
      "citation_id": "31",
      "title": "Language and discrimination: Generating meaning, perceiving identities, and discriminating outcomes",
      "authors": [
        "J Craft",
        "K Wright",
        "R Weissler",
        "R Queen"
      ],
      "year": "2020",
      "venue": "Annual Review of Linguistics",
      "doi": "10.1146/annurev-linguistics-011718-011659"
    },
    {
      "citation_id": "32",
      "title": "On measuring and mitigating biased inferences of word embeddings",
      "authors": [
        "S Dev",
        "T Li",
        "J Phillips",
        "V Srikumar"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "33",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N19-1423"
    },
    {
      "citation_id": "34",
      "title": "Addressing age-related bias in sentiment analysis",
      "authors": [
        "M Díaz",
        "I Johnson",
        "A Lazar",
        "A Piper",
        "D Gergle"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 chi conference on human factors in computing systems",
      "doi": "10.1145/3173574.3173986"
    },
    {
      "citation_id": "35",
      "title": "Measuring and mitigating unintended bias in text classification",
      "authors": [
        "L Dixon",
        "J Li",
        "J Sorensen",
        "N Thain",
        "L Vasserman"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",
      "doi": "10.1145/3278721.3278729"
    },
    {
      "citation_id": "36",
      "title": "Sentiment analysis techniques and applications in education: A survey",
      "authors": [
        "F Dolianiti",
        "D Iakovakis",
        "S Dias",
        "S Hadjileontiadou",
        "J Diniz",
        "L Hadjileontiadis"
      ],
      "year": "2018",
      "venue": "International Conference on Technology and Innovation in Learning, Teaching and Education",
      "doi": "10.1007/978-3-030-20954-4_31"
    },
    {
      "citation_id": "37",
      "title": "Fairness in deep learning: A computational perspective",
      "authors": [
        "M Du",
        "F Yang",
        "N Zou",
        "X Hu"
      ],
      "year": "2021",
      "venue": "IEEE Intelligent Systems",
      "doi": "10.1109/MIS.2020.3000681"
    },
    {
      "citation_id": "38",
      "title": "Social role theory of sex differences and similarities: A current appraisal",
      "authors": [
        "A Eagly",
        "W Wood",
        "A Diekman"
      ],
      "year": "2000",
      "venue": "Social role theory of sex differences and similarities: A current appraisal"
    },
    {
      "citation_id": "39",
      "title": "Leveraging affective bidirectional transformers for offensive language detection",
      "authors": [
        "A Elmadany",
        "C Zhang",
        "M Abdul-Mageed",
        "A Hashemi"
      ],
      "year": "2020",
      "venue": "Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection"
    },
    {
      "citation_id": "40",
      "title": "Equalizing gender bias in neural machine translation with word embeddings techniques",
      "authors": [
        "J Escudé Font",
        "M Costa-Jussà"
      ],
      "year": "2019",
      "venue": "Proceedings of the First Workshop on Gender Bias in Natural Language Processing",
      "doi": "10.18653/v1/W19-3821"
    },
    {
      "citation_id": "41",
      "title": "Improving gender fairness of pre-trained language models without catastrophic forgetting",
      "authors": [
        "Z Fatemi",
        "C Xing",
        "W Liu",
        "C Xiong"
      ],
      "year": "2021",
      "venue": "Improving gender fairness of pre-trained language models without catastrophic forgetting",
      "arxiv": "arXiv:2110.05367"
    },
    {
      "citation_id": "42",
      "title": "Word embeddings quantify 100 years of gender and ethnic stereotypes",
      "authors": [
        "N Garg",
        "L Schiebinger",
        "D Jurafsky",
        "J Zou"
      ],
      "year": "2018",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "43",
      "title": "Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them",
      "authors": [
        "H Gonen",
        "Y Goldberg"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N19-1061"
    },
    {
      "citation_id": "44",
      "title": "Measuring individual differences in implicit cognition: the implicit association test",
      "authors": [
        "A Greenwald",
        "D Mcghee",
        "J Schwartz"
      ],
      "year": "1998",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "45",
      "title": "Detecting emergent intersectional biases: Contextualized word embeddings contain a distribution of human-like biases",
      "authors": [
        "W Guo",
        "A Caliskan"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",
      "doi": "10.1145/3461702.3462536"
    },
    {
      "citation_id": "46",
      "title": "Twitter sentiment analysis in healthcare using hadoop and r",
      "authors": [
        "V Gupta",
        "S Kohli"
      ],
      "year": "2016",
      "venue": "2016 3rd International Conference on Computing for Sustainable Global Development (INDIACom)"
    },
    {
      "citation_id": "47",
      "title": "It's all in the name: Mitigating gender bias with name-based counterfactual data substitution",
      "authors": [
        "R Hall Maudslay",
        "H Gonen",
        "R Cotterell",
        "S Teufel"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": "10.18653/v1/D19-1530"
    },
    {
      "citation_id": "48",
      "title": "Teaching machines to read and comprehend",
      "authors": [
        "K Hermann",
        "T Kočiskỳ",
        "E Grefenstette",
        "L Espeholt",
        "W Kay",
        "M Suleyman",
        "P Blunsom"
      ],
      "year": "2015",
      "venue": "Proceedings of the 28th International Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "49",
      "title": "Reducing sentiment bias in language models via counterfactual evaluation",
      "authors": [
        "P Huang",
        "H Zhang",
        "R Jiang",
        "R Stanforth",
        "J Welbl",
        "J Rae",
        "V Maini",
        "D Yogatama",
        "P Kohli"
      ],
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020",
      "doi": "10.18653/v1/2020.findings-emnlp.7"
    },
    {
      "citation_id": "50",
      "title": "On transferability of bias mitigation effects in language model fine-tuning",
      "authors": [
        "X Jin",
        "F Barbieri",
        "B Kennedy",
        "A Davani",
        "L Neves",
        "X Ren"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/2021.naacl-main.296"
    },
    {
      "citation_id": "51",
      "title": "Fasttext. zip: Compressing text classification models",
      "authors": [
        "A Joulin",
        "E Grave",
        "P Bojanowski",
        "M Douze",
        "H Jégou",
        "T Mikolov"
      ],
      "year": "2016",
      "venue": "Fasttext. zip: Compressing text classification models",
      "arxiv": "arXiv:1612.03651"
    },
    {
      "citation_id": "52",
      "title": "Ammus: A survey of transformerbased pretrained models in natural language processing",
      "authors": [
        "K Kalyan",
        "A Rajasekharan",
        "S Sangeetha"
      ],
      "year": "2021",
      "venue": "Ammus: A survey of transformerbased pretrained models in natural language processing",
      "arxiv": "arXiv:2108.05542"
    },
    {
      "citation_id": "53",
      "title": "Unmasking the mask-evaluating social biases in masked language models",
      "authors": [
        "M Kaneko",
        "D Bollegala"
      ],
      "year": "2021",
      "venue": "Unmasking the mask-evaluating social biases in masked language models",
      "arxiv": "arXiv:2104.07496"
    },
    {
      "citation_id": "54",
      "title": "Examining gender and race bias in two hundred sentiment analysis systems",
      "authors": [
        "S Kiritchenko",
        "S Mohammad"
      ],
      "year": "2018",
      "venue": "Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics",
      "doi": "10.18653/v1/S18-2005"
    },
    {
      "citation_id": "55",
      "title": "Bias out-of-the-box: An empirical analysis of intersectional occupational biases in popular generative language models",
      "authors": [
        "H Kirk",
        "F Volpin",
        "H Iqbal",
        "E Benussi",
        "F Dreyer",
        "A Shtedritski",
        "Y Asano"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "56",
      "title": "Sentiment analysis of financial news articles using performance indicators",
      "authors": [
        "S Krishnamoorthy"
      ],
      "year": "2018",
      "venue": "Knowledge and Information Systems",
      "doi": "10.1007/s10115-017-1134-1"
    },
    {
      "citation_id": "57",
      "title": "Measuring bias in contextualized word representations",
      "authors": [
        "K Kurita",
        "N Vyas",
        "A Pareek",
        "A Black",
        "Y Tsvetkov"
      ],
      "year": "2019",
      "venue": "Proceedings of the First Workshop on Gender Bias in Natural Language Processing",
      "doi": "10.18653/v1/W19-3823"
    },
    {
      "citation_id": "58",
      "title": "Algorithmic bias? an empirical study of apparent gender-based discrimination in the display of stem career ads",
      "authors": [
        "A Lambrecht",
        "C Tucker"
      ],
      "year": "2019",
      "venue": "Management Science",
      "doi": "10.1287/mnsc.2018.3093"
    },
    {
      "citation_id": "59",
      "title": "Algorithmic bias? an empirical study of apparent gender-based discrimination in the display of stem career ads",
      "authors": [
        "A Lambrecht",
        "C Tucker"
      ],
      "year": "2019",
      "venue": "Management Science",
      "doi": "10.1287/mnsc.2018.3093"
    },
    {
      "citation_id": "60",
      "title": "Google autocomplete still makes vile suggestions",
      "authors": [
        "I Lapowsky"
      ],
      "year": "2018",
      "venue": "Google autocomplete still makes vile suggestions"
    },
    {
      "citation_id": "61",
      "title": "Distributed representations of sentences and documents",
      "authors": [
        "Q Le",
        "T Mikolov"
      ],
      "year": "2014",
      "venue": "Proceedings of the 31st International Conference on International Conference on Machine Learning",
      "doi": "10.5555/3044805.3045025"
    },
    {
      "citation_id": "62",
      "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
      "authors": [
        "J Lee",
        "W Yoon",
        "S Kim",
        "D Kim",
        "S Kim",
        "C So",
        "J Kang"
      ],
      "year": "2019",
      "venue": "Bioinformatics",
      "doi": "10.1093/bioinformatics/btz682"
    },
    {
      "citation_id": "63",
      "title": "Feature-wise bias amplification",
      "authors": [
        "K Leino",
        "M Fredrikson",
        "E Black",
        "S Sen",
        "A Datta"
      ],
      "year": "2019",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "64",
      "title": "Detecting gender bias in transformer-based models: A case study on bert",
      "authors": [
        "B Li",
        "H Peng",
        "R Sainju",
        "J Yang",
        "L Yang",
        "Y Liang",
        "W Jiang",
        "B Wang",
        "H Liu",
        "C Ding"
      ],
      "year": "2021",
      "venue": "Detecting gender bias in transformer-based models: A case study on bert",
      "arxiv": "arXiv:2110.15733"
    },
    {
      "citation_id": "65",
      "title": "Towards understanding and mitigating social biases in language models",
      "authors": [
        "P Liang",
        "C Wu",
        "L Morency",
        "R Salakhutdinov"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "66",
      "title": "Does gender matter? towards fairness in dialogue systems",
      "authors": [
        "H Liu",
        "J Dacon",
        "W Fan",
        "H Liu",
        "Z Liu",
        "J Tang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics",
      "doi": "10.18653/v1/2020.coling-main.390"
    },
    {
      "citation_id": "67",
      "title": "Mitigating political bias in language models through reinforced calibration",
      "authors": [
        "R Liu",
        "C Jia",
        "J Wei",
        "G Xu",
        "L Wang",
        "S Vosoughi"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "68",
      "title": "Gender bias in neural natural language processing",
      "authors": [
        "K Lu",
        "P Mardziel",
        "F Wu",
        "P Amancharla",
        "A Datta"
      ],
      "year": "2020",
      "venue": "Gender bias in neural natural language processing"
    },
    {
      "citation_id": "69",
      "title": "Intersectional bias in causal language models",
      "authors": [
        "L Magee",
        "L Ghahremanlou",
        "K Soldatic",
        "S Robertson"
      ],
      "year": "2021",
      "venue": "Intersectional bias in causal language models",
      "arxiv": "arXiv:2107.07691"
    },
    {
      "citation_id": "70",
      "title": "Black is to criminal as caucasian is to police: Detecting and removing multiclass bias in word embeddings",
      "authors": [
        "T Manzini",
        "L Yao Chong",
        "A Black",
        "Y Tsvetkov"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N19-1062"
    },
    {
      "citation_id": "71",
      "title": "Building a large annotated corpus of English: The Penn Treebank",
      "authors": [
        "M Marcus",
        "B Santorini",
        "M Marcinkiewicz"
      ],
      "year": "1993",
      "venue": "Computational Linguistics"
    },
    {
      "citation_id": "72",
      "title": "On measuring social biases in sentence encoders",
      "authors": [
        "C May",
        "A Wang",
        "S Bordia",
        "S Bowman",
        "R Rudinger"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N19-1063"
    },
    {
      "citation_id": "73",
      "title": "A study of race and gender bias in the punishment of school children",
      "authors": [
        "A Mcfadden",
        "G Marsh",
        "B Price",
        "Y Hwang"
      ],
      "year": "1992",
      "venue": "A study of race and gender bias in the punishment of school children"
    },
    {
      "citation_id": "74",
      "title": "Gender bias and sexism in language",
      "authors": [
        "M Menegatti",
        "M Rubini"
      ],
      "year": "2017",
      "venue": "Gender bias and sexism in language",
      "doi": "10.1093/acrefore/9780190228613.013.470"
    },
    {
      "citation_id": "75",
      "title": "Pointer sentinel mixture models",
      "authors": [
        "S Merity",
        "C Xiong",
        "J Bradbury",
        "R Socher"
      ],
      "year": "2016",
      "venue": "Pointer sentinel mixture models",
      "arxiv": "arXiv:1609.07843"
    },
    {
      "citation_id": "76",
      "title": "Efficient estimation of word representations in vector space",
      "authors": [
        "T Mikolov",
        "K Chen",
        "G Corrado",
        "J Dean"
      ],
      "year": "2013",
      "venue": "1st International Conference on Learning Representations"
    },
    {
      "citation_id": "77",
      "title": "Race and reactions to negative feedback: Examining the effects of the \"angry black woman\" stereotype",
      "authors": [
        "D Motro",
        "J Evans",
        "A Ellis",
        "L Benson"
      ],
      "year": "2019",
      "venue": "Academy of Management Proceedings",
      "doi": "10.5465/AMBPP.2019.11230abstract"
    },
    {
      "citation_id": "78",
      "title": "StereoSet: Measuring stereotypical bias in pretrained language models",
      "authors": [
        "M Nadeem",
        "A Bethke",
        "S Reddy"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
      "doi": "10.18653/v1/2021.acl-long.416"
    },
    {
      "citation_id": "79",
      "title": "Language-based discrimination: Blatant and subtle forms",
      "authors": [
        "S Ng"
      ],
      "year": "2007",
      "venue": "Journal of Language and Social Psychology",
      "doi": "10.1177/0261927X07300074"
    },
    {
      "citation_id": "80",
      "title": "Ai bias could put women's lives at risk -a challenge for regulators",
      "authors": [
        "C Niethammer"
      ],
      "year": "2020",
      "venue": "Ai bias could put women's lives at risk -a challenge for regulators"
    },
    {
      "citation_id": "81",
      "title": "Text embeddings contain bias. here's why that matters",
      "authors": [
        "B Packer",
        "M Mitchell",
        "M Guajardo-Céspedes",
        "Y Halpern"
      ],
      "year": "2018",
      "venue": "Tech. rep"
    },
    {
      "citation_id": "82",
      "title": "Thumbs up? sentiment classification using machine learning techniques",
      "authors": [
        "B Pang",
        "L Lee",
        "S Vaithyanathan"
      ],
      "year": "2002",
      "venue": "Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.3115/1118693.1118704"
    },
    {
      "citation_id": "83",
      "title": "Reducing gender bias in abusive language detection",
      "authors": [
        "J Park",
        "J Shin",
        "P Fung"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/D18-1302"
    },
    {
      "citation_id": "84",
      "title": "GloVe: Global vectors for word representation",
      "authors": [
        "J Pennington",
        "R Socher",
        "C Manning"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "doi": "10.3115/v1/D14-1162"
    },
    {
      "citation_id": "85",
      "title": "Deep contextualized word representations",
      "authors": [
        "M Peters",
        "M Neumann",
        "M Iyyer",
        "M Gardner",
        "C Clark",
        "K Lee",
        "L Zettlemoyer"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N18-1202"
    },
    {
      "citation_id": "86",
      "title": "Beneath the tip of the iceberg: Current challenges and new directions in sentiment analysis research",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "R Mihalcea"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2020.3038167"
    },
    {
      "citation_id": "87",
      "title": "Debiasing gender biased hindi words with word-embedding",
      "authors": [
        "A Pujari",
        "A Mittal",
        "A Padhi",
        "A Jain",
        "M Jadon",
        "V Kumar"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 2nd International Conference on Algorithms, Computing and Artificial Intelligence",
      "doi": "10.1145/3377713.3377792"
    },
    {
      "citation_id": "88",
      "title": "Reducing gender bias in wordlevel language models with a gender-equalizing loss function",
      "authors": [
        "Y Qian",
        "U Muaz",
        "B Zhang",
        "J Hyun"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop",
      "doi": "10.18653/v1/P19-2031"
    },
    {
      "citation_id": "89",
      "title": "Pre-trained models for natural language processing: A survey",
      "authors": [
        "X Qiu",
        "T Sun",
        "Y Xu",
        "Y Shao",
        "N Dai",
        "X Huang"
      ],
      "year": "2020",
      "venue": "Science China Technological Sciences",
      "doi": "10.1007/s11431-020-1647-3"
    },
    {
      "citation_id": "90",
      "title": "Improving language understanding by generative pre-training",
      "authors": [
        "A Radford",
        "K Narasimhan",
        "T Salimans",
        "I Sutskever"
      ],
      "year": "2018",
      "venue": "OpenAI blog"
    },
    {
      "citation_id": "91",
      "title": "Language models are unsupervised multitask learners",
      "authors": [
        "A Radford",
        "J Wu",
        "R Child",
        "D Luan",
        "D Amodei",
        "I Sutskever"
      ],
      "year": "2019",
      "venue": "OpenAI blog"
    },
    {
      "citation_id": "92",
      "title": "Ensuring fairness in machine learning to advance health equity",
      "authors": [
        "A Rajkomar",
        "M Hardt",
        "M Howell",
        "G Corrado",
        "M Chin"
      ],
      "year": "2018",
      "venue": "Annals of Internal Medicine",
      "doi": "10.7326/M18-1990"
    },
    {
      "citation_id": "93",
      "title": "Evaluating gender bias in hindi-english machine translation",
      "authors": [
        "K Ramesh",
        "G Gupta",
        "S Singh"
      ],
      "year": "2021",
      "venue": "Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing",
      "doi": "10.18653/v1/2021.gebnlp-1.3"
    },
    {
      "citation_id": "94",
      "title": "Sentiment analysis and machine learning in finance: a comparison of methods and models on one million messages",
      "authors": [
        "T Renault"
      ],
      "year": "2020",
      "venue": "Digital Finance",
      "doi": "10.1007/s42521-019-00014-x"
    },
    {
      "citation_id": "95",
      "title": "Beyond accuracy: Behavioral testing of NLP models with CheckList",
      "authors": [
        "M Ribeiro",
        "T Wu",
        "C Guestrin",
        "S Singh"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2020.acl-main.442"
    },
    {
      "citation_id": "96",
      "title": "Wide range screening of algorithmic bias in word embedding models using large sentiment lexicons reveals underreported bias types",
      "authors": [
        "D Rozado"
      ],
      "year": "2020",
      "venue": "PloS one",
      "doi": "10.1371/journal.pone.0231189"
    },
    {
      "citation_id": "97",
      "title": "Gender bias in coreference resolution",
      "authors": [
        "R Rudinger",
        "J Naradowsky",
        "B Leonard",
        "B Van Durme"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N18-2002"
    },
    {
      "citation_id": "98",
      "title": "Reimagining algorithmic fairness in india and beyond",
      "authors": [
        "N Sambasivan",
        "E Arnesen",
        "B Hutchinson",
        "T Doshi",
        "V Prabhakaran"
      ],
      "venue": "Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency"
    },
    {
      "citation_id": "100",
      "title": "Reducing gender bias in neural machine translation as a domain adaptation problem",
      "authors": [
        "D Saunders",
        "B Byrne"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2020.acl-main.690"
    },
    {
      "citation_id": "101",
      "title": "Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in NLP",
      "authors": [
        "T Schick",
        "S Udupa",
        "H Schütze"
      ],
      "year": "2021",
      "venue": "Transactions of the Association for Computational Linguistics",
      "doi": "10.1162/tacl_a_00434"
    },
    {
      "citation_id": "102",
      "title": "The glass ceiling in nlp",
      "authors": [
        "N Schluter"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/D18-1301"
    },
    {
      "citation_id": "103",
      "title": "Darling or babygirl? investigating stylistic bias in sentiment analysis",
      "authors": [
        "J Shen",
        "L Fratamico",
        "I Rahwan",
        "A Rush"
      ],
      "year": "2018",
      "venue": "Proc. of FATML"
    },
    {
      "citation_id": "104",
      "title": "The woman worked as a babysitter: On biases in language generation",
      "authors": [
        "E Sheng",
        "K Chang",
        "P Natarajan",
        "N Peng"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": "10.18653/v1/D19-1339"
    },
    {
      "citation_id": "105",
      "title": "Towards a comprehensive understanding and accurate evaluation of societal biases in pre-trained transformers",
      "authors": [
        "A Silva",
        "P Tambwekar",
        "M Gombolay"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/2021.naacl-main.189"
    },
    {
      "citation_id": "106",
      "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
      "authors": [
        "R Socher",
        "A Perelygin",
        "J Wu",
        "J Chuang",
        "C Manning",
        "A Ng",
        "C Potts"
      ],
      "year": "2013",
      "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "107",
      "title": "Quantifying gender bias towards politicians in cross-lingual language models",
      "authors": [
        "K Stańczak",
        "S Choudhury",
        "T Pimentel",
        "R Cotterell",
        "I Augenstein"
      ],
      "year": "2021",
      "venue": "Quantifying gender bias towards politicians in cross-lingual language models",
      "arxiv": "arXiv:2104.07505"
    },
    {
      "citation_id": "108",
      "title": "Cross-domain sentiment analysis on social media interactions using senti-lexicon based hybrid features",
      "authors": [
        "R Suharshala",
        "K Anoop",
        "V Lajish"
      ],
      "year": "2018",
      "venue": "2018 3rd International Conference on Inventive Computation Technologies (ICICT)",
      "doi": "10.1109/ICICT43934.2018.9034272"
    },
    {
      "citation_id": "109",
      "title": "Mitigating gender bias in natural language processing: Literature review",
      "authors": [
        "T Sun",
        "A Gaut",
        "S Tang",
        "Y Huang",
        "M Elsherief",
        "J Zhao",
        "D Mirza",
        "E Belding",
        "K Chang",
        "W Wang"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P19-1159"
    },
    {
      "citation_id": "110",
      "title": "A framework for understanding sources of harm throughout the machine learning life cycle",
      "authors": [
        "H Suresh",
        "J Guttag"
      ],
      "year": "2021",
      "venue": "Equity and Access in Algorithms, Mechanisms, and Optimization. EAAMO '21",
      "doi": "10.1145/3465416.3483305"
    },
    {
      "citation_id": "111",
      "title": "Reducing sentiment polarity for demographic attributes in word embeddings using adversarial learning",
      "authors": [
        "C Sweeney",
        "M Najafian"
      ],
      "venue": "Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency"
    },
    {
      "citation_id": "112",
      "title": "",
      "authors": [
        "Fat* '20"
      ],
      "year": "2020",
      "venue": "",
      "doi": "10.1145/3351095.3372837"
    },
    {
      "citation_id": "113",
      "title": "Discrimination in online ad delivery: Google ads, black names and white names, racial discrimination, and click advertising",
      "authors": [
        "L Sweeney"
      ],
      "year": "2013",
      "venue": "Queue",
      "doi": "10.1145/2460276.2460278"
    },
    {
      "citation_id": "114",
      "title": "Assessing social and intersectional biases in contextualized word representations",
      "authors": [
        "Y Tan",
        "L Celis"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "115",
      "title": "Unbiased look at dataset bias",
      "authors": [
        "A Torralba",
        "A Efros"
      ],
      "year": "2011",
      "venue": "CVPR 2011",
      "doi": "10.1109/CVPR.2011.5995347"
    },
    {
      "citation_id": "116",
      "title": "Getting gender right in neural machine translation",
      "authors": [
        "E Vanmassenhove",
        "C Hardmeier",
        "A Way"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/D18-1334"
    },
    {
      "citation_id": "117",
      "title": "Stereotype and skew: Quantifying gender bias in pre-trained and fine-tuned language models",
      "authors": [
        "D De Vassimon Manela",
        "D Errington",
        "T Fisher",
        "B Van Breugel",
        "P Minervini"
      ],
      "year": "2021",
      "venue": "Proceedings of the 16th Conference of the European Chapter",
      "doi": "10.18653/v1/2021.eacl-main.190"
    },
    {
      "citation_id": "118",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "119",
      "title": "Identification of bias against people with disabilities in sentiment analysis and toxicity detection models",
      "authors": [
        "P Venkit",
        "S Wilson"
      ],
      "year": "2021",
      "venue": "Identification of bias against people with disabilities in sentiment analysis and toxicity detection models",
      "arxiv": "arXiv:2111.13259"
    },
    {
      "citation_id": "120",
      "title": "Investigating gender bias in language models using causal mediation analysis",
      "authors": [
        "J Vig",
        "S Gehrmann",
        "Y Belinkov",
        "S Qian",
        "D Nevo",
        "Y Singer",
        "S Shieber"
      ],
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "121",
      "title": "Mind the GAP: A balanced corpus of gendered ambiguous pronouns",
      "authors": [
        "K Webster",
        "M Recasens",
        "V Axelrod",
        "J Baldridge"
      ],
      "year": "2018",
      "venue": "Transactions of the Association for Computational Linguistics",
      "doi": "10.1162/tacl_a_00240"
    },
    {
      "citation_id": "122",
      "title": "Ethical and social risks of harm from language models",
      "authors": [
        "L Weidinger",
        "J Mellor",
        "M Rauh",
        "C Griffin",
        "J Uesato",
        "P Huang",
        "M Cheng",
        "M Glaese",
        "B Balle",
        "A Kasirzadeh"
      ],
      "year": "2021",
      "venue": "Ethical and social risks of harm from language models",
      "arxiv": "arXiv:2112.04359"
    },
    {
      "citation_id": "123",
      "title": "Low frequency names exhibit bias and overfitting in contextualizing language models",
      "authors": [
        "R Wolfe",
        "A Caliskan"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/2021.emnlp-main.41"
    },
    {
      "citation_id": "124",
      "title": "Finbert: A pretrained language model for financial communications",
      "authors": [
        "Y Yang",
        "M Uy",
        "A Huang"
      ],
      "year": "2020",
      "venue": "Finbert: A pretrained language model for financial communications",
      "arxiv": "arXiv:2006.08097"
    },
    {
      "citation_id": "125",
      "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
      "authors": [
        "Z Yang",
        "Z Dai",
        "Y Yang",
        "J Carbonell",
        "R Salakhutdinov",
        "Q Le"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "126",
      "title": "Biasrv: Uncovering biased sentiment predictions at runtime",
      "authors": [
        "Z Yang",
        "M Asyrofi",
        "D Lo"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "doi": "10.1145/3468264.3473117"
    },
    {
      "citation_id": "127",
      "title": "Adversarial examples generation for reducing implicit gender bias in pre-trained models",
      "authors": [
        "W Ye",
        "F Xu",
        "Y Huang",
        "C Huang"
      ],
      "year": "2021",
      "venue": "Adversarial examples generation for reducing implicit gender bias in pre-trained models",
      "arxiv": "arXiv:2110.01094"
    },
    {
      "citation_id": "128",
      "title": "Mitigating unwanted biases with adversarial learning",
      "authors": [
        "B Zhang",
        "B Lemoine",
        "M Mitchell"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",
      "doi": "10.1145/3278721.3278779"
    },
    {
      "citation_id": "129",
      "title": "Hurtful words: Quantifying biases in clinical contextual word embeddings",
      "authors": [
        "H Zhang",
        "A Lu",
        "M Abdalla",
        "M Mcdermott",
        "M Ghassemi"
      ],
      "venue": "Proceedings of the ACM Conference on Health, Inference, and Learning"
    },
    {
      "citation_id": "130",
      "title": "",
      "authors": [
        "Chil '20"
      ],
      "year": "2020",
      "venue": "",
      "doi": "10.1145/3368555.3384448"
    },
    {
      "citation_id": "131",
      "title": "Gender bias in contextualized word embeddings",
      "authors": [
        "J Zhao",
        "T Wang",
        "M Yatskar",
        "R Cotterell",
        "V Ordonez",
        "K Chang"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N19-1064"
    },
    {
      "citation_id": "132",
      "title": "Men also like shopping: Reducing gender bias amplification using corpus-level constraints",
      "authors": [
        "J Zhao",
        "T Wang",
        "M Yatskar",
        "V Ordonez",
        "K Chang"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/D17-1323"
    },
    {
      "citation_id": "133",
      "title": "Gender bias in coreference resolution: Evaluation and debiasing methods",
      "authors": [
        "J Zhao",
        "T Wang",
        "M Yatskar",
        "V Ordonez",
        "K Chang"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N18-2003"
    },
    {
      "citation_id": "134",
      "title": "Learning gender-neutral word embeddings",
      "authors": [
        "J Zhao",
        "Y Zhou",
        "Z Li",
        "W Wang",
        "K Chang"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/D18-1521"
    },
    {
      "citation_id": "135",
      "title": "Mitigation of unintended biases against non-native english texts in sentiment analysis",
      "authors": [
        "A Zhiltsova",
        "S Caton",
        "C Mulway"
      ],
      "year": "2019",
      "venue": "Proceedings for the 27th AIAI Irish Conference on Artificial Intelligence and Cognitive Science"
    },
    {
      "citation_id": "136",
      "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
      "authors": [
        "Y Zhu",
        "R Kiros",
        "R Zemel",
        "R Salakhutdinov",
        "R Urtasun",
        "A Torralba",
        "S Fidler"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "137",
      "title": "Counterfactual data augmentation for mitigating gender stereotypes in languages with rich morphology",
      "authors": [
        "R Zmigrod",
        "S Mielke",
        "H Wallach",
        "R Cotterell"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P19-1161"
    }
  ]
}