{
  "paper_id": "2406.08081v1",
  "title": "Cldta: Contrastive Learning Based On Diagonal Transformer Autoencoder For Cross-Dataset Eeg Emotion Recognition",
  "published": "2024-06-12T11:05:42Z",
  "authors": [
    "Yuan Liao",
    "Yuhong Zhang",
    "Shenghuan Wang",
    "Xiruo Zhang",
    "Yiling Zhang",
    "Wei Chen",
    "Yuzhe Gu",
    "Liya Huang"
  ],
  "keywords": [
    "EEG",
    "Emotion Recognition",
    "Contrastive Learning",
    "Transfer Learning",
    "Cross-datasets"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recent advances in non-invasive EEG technology have broadened its application in emotion recognition, yielding a multitude of related datasets. Yet, deep learning models struggle to generalize across these datasets due to variations in acquisition equipment and emotional stimulus materials. To address the pressing need for a universal model that fluidly accommodates diverse EEG dataset formats and bridges the gap between laboratory and real-world data, we introduce a novel deep learning framework: the Contrastive Learning based Diagonal Transformer Autoencoder (CLDTA), tailored for EEG-based emotion recognition. The CLDTA employs a diagonal masking strategy within its encoder to extracts full-channel EEG data's brain network knowledge, facilitating transferability to the datasets with fewer channels. And an information separation mechanism improves model interpretability by enabling straightforward visualization of brain networks. The CLDTA framework employs contrastive learning to distill subjectindependent emotional representations and uses a calibration prediction process to enable rapid adaptation of the model to new subjects with minimal samples, achieving accurate emotion recognition. Our analysis across the SEED, SEED-IV, SEED-V, and DEAP datasets highlights CLDTA's consistent performance and proficiency in detecting both task-specific and general features of EEG signals related to emotions, underscoring its potential to revolutionize emotion recognition research.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Works",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Eeg-Based Emotion Recognition",
      "text": "EEG-based emotion recognition involves feature extraction and classification, traditionally leveraging discrete wavelet transform (DWT), power spectral density (PSD), differential entropy (DE), and differential asymmetry (DASM) with SVM or LDA classifier  [29] .\n\nCompared to conventional machine learning algorithms, deep learning has introduced end-to-end approaches that autonomously extract features using CNN, LSTM, GNN. For instance, Wang et al.  [30]  proposed a self-supervised EEG emotion recognition model based on CNN to enhance resource utilization efficiency. Ma et al.  [31]  developed a multimodal residual LSTM (MM-ResLSTM) network, while Song et al.  [17]  proposed a dynamic graph convolutional neural network (DGCNN) for EEG emotion recognition. More recently, the advent of Transformer models  [32]  achieved significant success in fields such as Natural Language Processing and > TAFFC-2024-03-0186 < Computer Vision. The emergence of Transformers also represents a significant evolution in discerning emotional states. Wang et al.  [33] , used attention mechanisms to focus on key features, helping to classify emotions by combining data from different parts of the brain.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Transfer Learning In Eeg Processing",
      "text": "The high variability in individual EEG signals  [34]  limits the generalizability of deep learning methods in emotion recognition, confining many models to lab settings despite potential wider applications  [35] . Transfer learning, aimed at applying knowledge from one domain to another, has shown promise in EEG analysis, especially in cross-session, crosssubject, and cross-database scenarios  [36] . Research has primarily focused on cross-session and cross-subject scenarios to mitigate EEG signal variability over time and between individuals. Zhang et al.  [37]  introduced a similarity-guided transfer learning method using Maximum Mean Discrepancy (MMD) and TrAdaBoost for closer data distribution alignment. Domain adaptation (DA) techniques like the bi-hemispheres domain-adversarial neural network (Bi-DANN)  [23]  and regularized graph neural network (RGNN) aim to learn domain-invariant representations. Domain generalization (DG) methods, such as the two-phase prototypical contrastive domain generalization framework (PCDG)  [38]  and the Contrastive Learning method for Inter-Subject Alignment (CLISA)  [22] , reduce reliance on new subject data by identifying subject-invariant emotional representations. Li et al.  [15]  proposed a graph-based multi-task self-supervised learning model (GMSS) for more general representation learning.\n\nIn cross-database scenarios, addressing differences between databases remains challenging but crucial for model adaptability. Lin et al.  [39]  developed a personalized model using robust principal component analysis (RPCA) to reduce intra-and inter-individual differences. Wang et al.  [40]  analyzed electrode-frequency distribution maps (EFDMs) with CNNs, noting high-frequency bands' effectiveness in emotion recognition. Liu et al.  [41]  introduced CD-EmotionNet, a transfer learning-based model for enhancing emotion recognition with few-channel EEG data, marking a step towards cross-device adaptability.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Methodology",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Overall Framework",
      "text": "This section introduces our Contrastive Learning based on the Diagonal Transformer Autoencoder (CLDTA). As illustrated in Fig.  1 , the architecture encompasses both the pretraining procedure of contrastive learning and the calibrationprediction process in emotion recognition. The pre-training phase of CLDTA involves five key components: data preprocessing and feature extraction, augmentation, the DTA Encoder, the projector, and the contrastive loss function. Initially, samples are drawn from the EEG data bank and then processed and feature extracted followed by generating a broader sample range through the data augmentation module.\n\nThe DTA Encoder subsequently extracts emotion features based on brain networks from each EEG signal. Ultimately, the projector maps the properties into a high-dimensional feature space to compute the contrastive loss, optimizing the DTA Encoder and projector. During the calibration-prediction phase, the model, which integrates the pre-trained DTA Encoder and an initialized classifier, is fine-tuned using a small set of labeled samples from new subjects. This step enables accurate emotion detection in new subjects. Once calibrated, the model is then ready for emotion recognition tasks.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Data Preprocess",
      "text": "The initial step in our process is to preprocess EEG signals to yield high-quality, artifact-free data. To obtain a more relevant and lower-dimensional representation for emotion recognition, we utilize the widely-used differential entropy (DE) feature, which is defined as follows:\n\nwhere ðœŽðœŽ 2 is the variance of the signal. Differential entropy features of each segment were extracted separately in the ð›¿ð›¿ (0.1-4 Hz), ðœƒðœƒ (4-8 Hz), ð›¼ð›¼ (8-13 Hz), ð›½ð›½ (13-31 Hz), and ð›¾ð›¾ (31-50 Hz) frequency bands. In one experiment of a subject, the DE features trained from continuous samples across time are concatenated and smoothed with a linear dynamic system (LDS) model  [12] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Data Augmentation",
      "text": "Data augmentation enhances our model by diversifying data representation and acting as a regularizer to improve robustness and performance. We have adopted effective augmentation techniques for DE data, specifically MixUp method  [42]  and Masking technique  [43] , after thorough evaluation.\n\n(1) MixUp MixUp facilitates the model's ability to discern shared information among positive pairs of samples. The MixUp data augmentation process creates a new sample, by linearly combining a pair of randomly selected training samples, x i and x j as follows:\n\nwhere Î» is a value sampled from a Beta distribution.\n\n(",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "2) Masking",
      "text": "The masking technique, otherwise referred to as channels dropout, has been demonstrated to yield superior results with sizable training sets  [44] . This data augmentation method applies a mask that sets a random subset of channels to zero, introducing controlled noise and distortion. This procedure can be mathematically expressed as:\n\n) where 'mask' is a vector of zeroes and ones of length 62.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Dta Encoder",
      "text": "This section introduces the Diagonal Transformer Autoencoder (DTA), as depicted in Fig.  2 . It draws on the fundamental principles of the Transformer encoder  [32] . This phase involves preparing the EEG data from SEED, SEED-IV, SEED-V, and DEAP datasets, which undergoes preprocessing and data augmentation before being fed into the DTA Encoder. The encoder's output is then projected, and the model is updated based on contrastive loss, which aims to cluster similar emotion features closer in the feature space while pushing dissimilar ones apart, as indicated by the \"attract\" and \"repel\" arrows among subjects' representations. (b) Calibration-Prediction: This phase consists of two steps. First, a small subset of labeled samples from a new subject is collected to calibrate the pre-trained DTA Encoder and classifier. Next, the calibrated DTA Encoder and classifier are then utilized for subsequent emotion recognition in the same subject. Calibration adjusts the model to the new subject's EEG for better accuracy and the classifier links features to emotions for predicting the subject's emotional state.\n\nFollowing the approach in  [45] , we incorporate a diagonal masking strategy (highlighted in blue in Fig.  2 ) to extract brain network knowledge, effectively bridging the gap between fullchannel EEG data and fewer-channel EEG datasets. Moreover, we use an information separation mechanism (indicated by orange dashed lines in Fig.  2 ) to isolate the learned knowledge, thereby enhancing the model's interpretability.\n\n(",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "1) Diagonal Masking Strategy",
      "text": "The self-attention mechanism tends to assign excessively high attention weights to nodes themselves, as shown in Fig.  3(a) . When processing EEG data with fewer number of channels, this sparsity of information can result in diminished accuracy, depicted in Fig.  3(b ). To counteract this issue, Fig.  3(c ) reveals that, in the pre-training stage, we capitalize on the Transformer's high parallel processing capability to focus on learning the brain network knowledge provided by fullchannel EEG datasets through the Diagonal Masking strategy.\n\nThe attention mechanism of the Transformer consists of three vectors query, key and value (QKV). V is updated based on the matching degree of Q and K (i.e., attention matrix A). In this process, Diagonal Masking Operation is like ð´ð´ ð‘–ð‘–ð‘–ð‘– = 0.\n\nð·ð·ð·ð·ð‘šð‘šð‘™ð‘™ð‘šð‘šð‘šð‘šð‘šð‘šð‘šð‘š(ð´ð´) = (ð½ð½ -ð¼ð¼) * ð´ð´ (4) where ð½ð½ represents a full matrix of ones, and ð¼ð¼ represents an identity matrix. (2) Information Separation Mechanism The Transformer architecture uses residual connections, which impacts the Diagonal Masking Operation's selfunknown-attention abilities. Furthermore, the connections between EEG channels only can be understood by examining attention weights. An input separation method has been implemented to address this issue. Through this mechanism, the key value input (KV) for each encoding layer is isolated from the network flow and is fixed as a combination of the input encoding and position encoding. The query input (Q) is the only component that gets updated across layers. Fig.  2  illustrates this information isolation mechanism within the DTA, indicated by orange dashed arrows.\n\n(3) Position Embedding and Source Data Embedding\n\nThe query (Q) input employs position embedding to transform the 3D coordinates of 62 nodes, derived from the 10-20 System, into the ð‘‘ð‘‘ ð‘šð‘šð‘šð‘šð‘šð‘šð‘šð‘šð‘šð‘š dimension using nonlinear mapping, thereby integrates prior spatial knowledge (P emb ) into the model. To augment the position encoding's expressiveness, we incorporate learnable position encoding ( L emb ). Analogously, source data embedding encodes the differential entropy (DE) features to conform to the ð‘‘ð‘‘ ð‘šð‘šð‘šð‘šð‘šð‘šð‘šð‘šð‘šð‘š dimension size. The encoding formula is as follows:\n\nIn the above formula, ð‘ð‘ð‘™ð‘™ð‘šð‘š ð‘šð‘šð‘‘ð‘‘ð‘‘ð‘‘ð‘‘ð‘‘ represents the threedimensional coordinates of the channel, P emb is the a priori position encoding. R emb is the learnable position embedding and S emb is the source data embedding. ð‘“ð‘“ ð‘–ð‘– (â€¢) is the linear function, and acvtivate (â€¢)is the activation function. (4) Self-unknown Attention As shown in Fig.  2 , we have two encoding inputs, Source data Embedding and Position Embeddings, which are, respectively,\n\n, where ð‘‘ð‘‘ ð‘šð‘šð‘šð‘šð‘šð‘šð‘šð‘šð‘šð‘š is the feature dimension and ð‘›ð‘› is the number of channels. K and V are fixed in all layers, while the query ð‘„ð‘„ ð‘–ð‘– is updated with each layer.\n\nConsidering the input ð‘„ð‘„ ð‘–ð‘– ð¾ð¾ ð‘–ð‘– ð‘‰ð‘‰ ð‘–ð‘– of the i-th encoding layer and the output ð»ð» ð‘–ð‘– , the formula is as follows:\n\n, ð·ð·ð‘“ð‘“ ð‘†ð‘†ðœ‹ðœ‹ð‘šð‘šð‘†ð‘† (9) where ð‘†ð‘†ð‘†ð‘†ð´ð´(â€¢) represents the self-unknown attention layer, ð‘†ð‘†ð´ð´(â€¢) represents the self attention layer and ð»ð» = ï¿½ð»ð» 1 ð‘–ð‘– , â‹¯ , ð»ð» ð‘šð‘š ð‘–ð‘– , â‹¯ , ð»ð» ð‘›ð‘› ð‘–ð‘– ï¿½ .\n\nIn summary, while retaining the basic structure of the Transformer, the flow of information between the encoding layers in the CLDTA encoding module is as follows:\n\nHere, we note that in the inference training process of CLDTA, the i-th element represented by ð‘„ð‘„ ð‘–ð‘– will not directly see the corresponding encoding representation from ð‘„ð‘„ ð‘–ð‘– 1 = ð‘ƒð‘ƒ 1 in any layer. However, during the testing phase, the diagonal masking mechanism is shut down, restoring it to selfattention.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "E. The Projector",
      "text": "The nonlinear projector can help the basic encoder better learn the representation of downstream prediction tasks  [46] . Here, we only use the Multilayer Perceptron (MLP), the formula is as follows:\n\nAs shown in Fig.  4 , the Projector mainly includes three fully connected layers with the number of hidden units decreasing sequentially from 128, 256, to 128. The corresponding positions in the figure show Batch Normalization, ELU and Dropout.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "F. The Contrastive Loss",
      "text": "To measure the similarity of emotion-related features between two sets of samples, we can calculate the cosine similarity of the encoded representation vectors. The input batch samples ðºðº ð´ð´ = [ðºðº 1 ð´ð´ , â‹¯ , ðºðº ð‘›ð‘› ð´ð´ ] and ðºðº ðµðµ = [ðºðº 1 ðµðµ , â‹¯ , ðºðº ð‘›ð‘› ðµðµ ] are transformed into ð‘ð‘ ð´ð´ and ð‘ð‘ ðµðµ through the DTA encoder and the projector, respectively. Then, we can compute the cosine similarity of the feature sets between ð‘ð‘ ð´ð´ and ð‘ð‘ ðµðµ :\n\nThe purpose of contrastive loss is to maximize the similarity of the EEG signals within the positive pair as fully as possible. We adopt the normalized temperature-scaled binary cross-entropy with logits loss computed by\n\nwhere Ï„ is the temperature parameter for softmax. The variable y can take the values of 0 or 1 and ðœŽðœŽ(ð‘¥ð‘¥) denotes the sigmoid function. > TAFFC-2024-03-0186 <\n\nThe smaller the loss function, the more similar the samples in the same category and the more dissimilar the samples in different categories. Adopting this loss function allows a sample to be similar to multiple samples at the same time, thereby accelerating the training.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "G. Calibration-Prediction Process",
      "text": "In the calibration-prediction process, we use the pre-trained DTA encoder to extract emotional features and predict emotional labels from the representations. We optimize the parameters of the pre-trained model and classifier using the cross-entropy loss function.\n\nThe classifier is utilized to predict emotional labels from the representations extracted from the DTA encoder. As depicted in Fig.  4 , the classifier primarily comprises two fully connected layers.\n\nð¿ð¿ð‘šð‘šð¿ð¿ðœ‹ðœ‹ð‘™ð‘™ = ð¶ð¶ð‘™ð‘™ð‘šð‘šð‘šð‘šð‘šð‘šð·ð·ð‘“ð‘“ð·ð·ðœ‹ðœ‹ð‘¡ð‘¡ (ð·ð·ð·ð·ð´ð´ ð‘ð‘ð‘ð‘ð‘šð‘šð‘‘ð‘‘ð‘ð‘ð‘‘ð‘‘ð‘–ð‘–ð‘›ð‘›ð‘šð‘šð‘šð‘š (ð·ð·ð·ð·, ð‘ð‘ð‘™ð‘™ð‘šð‘š ð‘šð‘šð‘‘ð‘‘ð‘‘ð‘‘ð‘‘ð‘‘ )) (18) Finally, when the loss function converges, it can be used for subsequent emotional recognition of the subjects.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Iv. Experiments",
      "text": "In this section, we outline the datasets employed, elucidate the data preprocessing procedures, and expound upon the implementation details of the model. Subsequently, We define the evaluation procedures and introduce the advanced deep learning benchmarks used for comparison. Lastly, we discuss the methodologies deployed for analyzing the performance of our model.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Dataset",
      "text": "We first outline the datasets selected for this study and the rationale behind their selection:\n\n(1) SEED Dataset: Developed by Zheng and Lu  [12] , this dataset includes EEG data from 15 subjects who watched 15 Chinese film clips, eliciting three emotions: positive, negative, and neutral. Each subject participated in three sessions, watching one clip per session for a total of 15 trials.\n\n(2) SEED-IV Dataset: Introduced in  [27] , this dataset features EEG and eye movement data from 15 subjects (7 men and 8 women) responding to 72 film scenes depicting four emotions: joy, sorrow, neutrality, and anxiety. Subjects participated in three sessions with 24 trials each at different times.\n\n(3) SEED-V Dataset: First utilized in  [28] , it comprises EEG and eye movement signals related to five emotions (happiness, sadness, neutral, fear, and disgust) from 15 film clips, with 16 subjects (6 males, 10 females) participating in three sessions. (4) DEAP Dataset: Established by Koelstra et al.  [13] , this dataset consists of EEG signals from 32 channels and peripheral physiological signals from 8 channels, collected from 32 participants watching 40 one-minute music videos. Participants rated the videos on arousal, appeal, likes/dislikes, dominance, and familiarity.\n\nThe SEED series dataset is expected to be an excellent benchmark for pre-training the CLDTA model, as it features a significantly larger number of subjects compared to most publicly available datasets. The datasets were collected in controlled environments to induce specific emotions using video clips, with data captured via a 62-channel ESI NeuroScan system aligned with the International 10-20 system. They offer a broad range of emotional labels for a discrete emotional modeling approach, as opposed to a valence-arousal spectrum. Contrastingly, the DEAP dataset, with its different EEG equipment, data specifications, emotional stimuli, and labeling approach, presents unique challenges for cross-dataset classification tasks. This makes it an ideal candidate for assessing the model's performance across diverse datasets.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Data Preprocessing",
      "text": "To ensure data consistency, we re-processed the original EEG data from the datasets. This study primarily utilized the EEGLAB toolbox  [47]  in MATLAB for pre-processing, which includes data input, electrode positioning, filtering, baseline correction, manual identification and removal of bad segments and channels, independent component analysis(ICA), manual exclusion of irrelevant components, and re-referencing. For the SEED, SEED-IV, and SEED-V datasets, we initially applied a band-pass filter from 0.01 to 48 Hz and a 50 Hz notch filter to eliminate noise. The criteria for rejecting bad channels are as follows: channels with a flatline duration exceeding 5 seconds; channels whose variance exceeds 4 times the standard deviation of the total channel signal; and spatially adjacent channels with a correlation less than 0.6. The criteria for rejecting time segments are: if the variance in each time window exceeds 7 times the variance of the current channel, the window is discarded. EEGLAB's 'spherical' interpolation algorithm is employed to interpolate channels discarded due to volume conduction effects, assigning different interpolation weights based on the proximity of surrounding nodes. ICA is subsequently applied to remove artifacts likely caused by eye movements, muscle movements, or other environmental noise, with up to 5 ICA components being removed. The data is re-referenced using a sample mean reference. We utilize the last 30 seconds of each trial to ensure the stimulated emotions are sufficiently coherent and intense.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Classifier",
      "text": "Projector > TAFFC-2024-03-0186 < For the DEAP dataset, we employed the same data preprocessing method. The data was first adjusted to match the 62-channel format of the SEED-series datasets, and missing channel data was filled with zeros. We adhered to the partitioning strategy outlined in  [48]  and  [41] , which converts the dataset into binary emotion recognition tasks by segmenting the valence dimension into positive/negative and the arousal dimension into high/low arousal, with the threshold for both dimensions set at 5. Thus, the processed data can be summarized as shown in TABLE I. represents the number of samples per each trial. Total: signifies the total sample count for each dataset. For the SEED-V dataset, the first trail of data from subject 5 is missing.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Training Details",
      "text": "We trained our CLDTA on NVIDIA RTX 3080ti GPU, pretraining the model on SEED, IV, and V datasets. The CLDTA was configured to 4 layers, model dimension (ð‘‘ð‘‘ ð‘šð‘šð‘šð‘šð‘šð‘šð‘šð‘šð‘šð‘š ) to 32, hidden layer dimension to 64, and multi-head attention count to 4. The Projector flattens the data and maps it to 128 dimensions. The temperature hyperparameter Ï„ for contrastive learning was set to 0.5.\n\nFor optimizing the contrastive learning model, we used the Adam optimizer  [49] , with the initial learning rate set to 1e-4, and weight decay set to 0.005 based on empirical standards. A random seed of 42 was set, batch size was configured to 256, epoch was set to 30, dropout was set to 0.1, and activation function was set as Exponential Linear Units (ELU)  [50] .\n\nFor the calibrating and transfer process of emotion recognition in MLP classifier, we used two hidden layers, each with 32 units. ELU were used between every two layers. We used cross-entropy loss and Adam optimizer for parameter optimization. The learning rate was empirically set to 1e-5. Batch size was empirically set to 128. We trained for 100 epochs with early stopping (maximal tolerance of 20 epochs without validation accuracy increase).",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "D. Test And Validation",
      "text": "We applied the leave-one-subject-out cross-validation (LOSOCV) method to assess our approach. In LOSOCV, each subject's data is alternately used for transfer learning, with the rest for training. For each test, an equal number of labeled samples per category is selected from the target subject's test set, excluding all other unlabeled samples from training. This process repeats for all subjects' data.\n\nSubject-dependent experiments use a small set of labeled samples from target subjects for transfer learning, with the remaining data for accuracy testing. The training and testing set division follows protocols from  [41]  and  [12] . For SEED, training involves the first 9 trials per session, with the next 6 trials for testing. SEED-IV uses the first 16 trials for training and the last 8 for testing. SEED-V employs a triple crossvalidation (10 for training, 5 for testing) for five emotion tasks. DEAP uses an 80% training and 20% testing split per subject.\n\nIn strictly subject-independent experiments, when no target subject calibration samples are available, calibration uses source subjects' data, followed by testing on the target subjects.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "E. Performance Comparison",
      "text": "To investigate the effectiveness of our contrast learning method, we compared it with several notable emotion recognition methods, including A-LSTM  [51] , DGCNN  [17] , BiDANN  [23] , SSL-EEG  [52] , RGNN  [24] , GMSS  [15] , and PR-PL  [25] . These methods are emblematic of prior research in emotion recognition. Their results were either directly quoted or replicated from the literature to ensure a reliable comparison with our proposed method. It's important to note that our results are compared only with advanced models under the same standard experimental settings. In our performance comparison protocols, results reproduced by our team are marked with an asterisk (*).",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "F. Methods For Analyzing Model Performance (1) Model Stability And Channel Reduction",
      "text": "In the SEED series dataset, with its 62 channels from various brain locations, the excessive number of channels not only raises computational demands but also hampers the practicality of aBCI systems. Hence, it's essential to minimize channel use while analyzing EEG data. Our model calibration tests involved randomly masking EEG channels to assess the impact of channel quantity on recognition accuracy.\n\n(2) Identifying Brain Regions for Emotion Recognition EEG channels correspond to different brain cortex areas, each associated with specific physiological functions. To pinpoint crucial regions or channels for emotion recognition, we analyzed location encoding data. Calculating the cosine similarity between channels helped us identify the importance of nodes and their community groupings.\n\n(",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "3) Contrastive Learning Evaluation",
      "text": "We evaluated the impact of contrastive learning by visualizing features before and after encoding and by measuring inter-class divergence (ICD) and intra-class similarity (ICS). ICD evaluates the similarity level among samples of the same class in the embedding space, while ICS assesses the separation degree between different class samples. A smaller intra-class distance implies higher intra-class similarity, and a larger inter-class distance indicates greater separation.  (20)  In these calculations, ð‘ð‘ ð‘ð‘ð‘šð‘šð‘ ð‘  denotes scenarios where the labels of the pair are matching, while ð‘ð‘ ð‘›ð‘›ð‘šð‘šð‘›ð‘› refers to scenarios where the labels do not match. > TAFFC-2024-03-0186 < V. RESULT AND DISCUSSION Drawing from the analysis presented in Section IV, Part A, we first evaluate the CLDTA model's performance in subjectdependent and subject-independent setups on the SEED series datasets. Then, we evaluate the cross-device and cross-dataset classification tasks on the DEAP dataset.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Emotion Recognition Performance On The Seed Series Dataset",
      "text": "(1) Subject-dependent Evaluation Three configurations of DTA were tested: DTA without transfer learning (DTA w/o TF), DTA with transfer learning within the same dataset (DTA-Single-Dataset), and DTA with transfer learning across multiple datasets (DTA-Multi-Dataset). The experimental results are shown in Table  II . The results underscore the benefits of transfer learning, especially when applied across datasets, in improving the model's effectiveness. In subject-dependent evaluations, DTA shows competitive or superior performance compared to advanced models like GMSS, achieving an accuracy of 95.09% on the SEED dataset and the highest accuracy on SEED-IV and SEED-V, indicating its capability to learn stable subject features.\n\n(2) Subject-independent Evaluation\n\nIn the subject-independent experiments detailed in Table  III , it is evident that the CLDTA model outperforms the SVM baseline by achieving respective performance enhancements of 32.4%, 23.8%, and 29.9% on the SEED, SEED-IV, and SEED-V datasets. Furthermore, the CLDTA model attains state-of-the-art results on SEED-IV and SEED-V, with accuracies of 64.11% and 61.45%, respectively.\n\nFurthermore, CLDTA consistently presents a notably low accuracy standard deviation in both testing scenarios, demonstrating its strong discrimination and generalization abilities. This comprehensive performance across different testing conditions confirms the effectiveness of the proposed transfer learning strategy in optimizing network performance, highlighting CLDTA as a viable approach for practical emotion recognition applications. However, its performance on the SEED dataset did not reach the most advanced level, which may be attributed to the dataset's broad emotional categories (positive, negative, neutral) as opposed to the more granular labels found in SEED-IV and SEED-V. These findings suggest that the efficacy of the model's learning is influenced by the granularity of emotion labeling.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "B. Cross-Device Cross-Electrode Evaluation On The Deap Dataset",
      "text": "We compared two transfer learning strategies: one employing a model pre-trained on the SEED series datasets (SEED2DEAP) and another pre-trained on DEAP itself (DEAP2DEAP), against a baseline model with no transfer learning (Rand2DEAP). This comparison elucidates the impact of transfer learning on model efficacy in varying experimental setups. The results, as shown in Table  IV , indicate that our CLDTA model, leveraging transfer learning, achieved superior accuracy. Specifically, SEED2DEAP excelled in arousal classification with a 94.11% accuracy rate and a 2.1% standard deviation, while DEAP2DEAP showed higher accuracy in Valence classification at 94.58% with a 1.4% standard deviation. These results affirm the effectiveness of our proposed transfer learning strategy in capturing crossdevice and cross-electrode EEG emotion features.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "C. Calibration Test",
      "text": "To assess performance with limited labeled samples, we explored how different quantities of labeled samples affect model calibration, comparing models with and without transfer learning. Figure  5  illustrates that when employing different quantities of calibration samples for fine-tuning, the accuracy of the CLDTA model markedly surpasses that of the fully-supervised baseline across the entire range of sample sizes, with the most pronounced advantage observed in scenarios with limited labeled data. Specifically, for the SEED dataset, CLDTA's performance nearly matches full-supervised training (90.44%) with over 20 labeled samples per category. For SEED-IV, CLDTA reaches 87.88% of full-supervised training with more than 32 labeled samples per category. For SEED-V, CLDTA's performance is close to full-supervised training (77.92%) with over 13 labeled samples per category. Beyond 40 calibration labels per category, the performance of all pre-training models converges.\n\nIn addition, we also recorded the time consumed by the model during the calibration prediction phase, as shown in Table  V . The number of training iterations required for calibrating the pre-trained model is 17% of that required by the randomly initialized model. In terms of training time, this represents a time saving of 91.48%. This demonstrates that pre-trained models are both faster and more stable in calibration compared to fully-supervised models.\n\nFor the DEAP dataset, calibration tests were conducted on three models: SEED2DEAP, DEAP2DEAP, and Rand2DEAP. Results shown in Fig.  6  indicate that pre-trained models on SEED and DEAP achieve nearly similar performances with limited samples, with a mean accuracy difference of 3.7%. This indicates that CLDTA effectively captures subjectinvariant emotional traits, showing resilience to differences in channel numbers and device types. The baseline model showed a higher tendency for overfitting compared to the pretrained models, which adapted better.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "D. Stability Analysis",
      "text": "For practical applications, minimizing the number of electrodes is advantageous for both feasibility and user comfort. Challenges including disconnections due to head movements, short circuits from excessive conductive paste, and potential electrode malfunctions can impair model performance. To evaluate our model's resilience in the face of such issues, we simulated real-world conditions such as electrode failure and noise interference.\n\nWe used the model pre-trained on the SEED series dataset for our experiments. In the electrode failure test, we simulated failures by setting channel data to zero or replacing it with data from nearby channels, with the number of failed channels ranging from 1 to 40. In the noise interference test, we added Gaussian noise with intensity varying from 0.1 to 3 times the sample variance. The results, shown in Fig.  7 , indicate the pretrained model's superior anti-interference capability compared to a fully supervised model. A small number of electrode failures slightly improved performance by 1.21%, suggesting that redundant channels may introduce noise in emotion recognition tasks. Performance declines in the pre-trained model when failures exceed 26 electrodes, while the fully supervised model's performance gradually decreases with more failures. During the noise interference experiment, the pre-trained model consistently outperformed the fully supervised model at low noise levels. When the noise intensity was under 1, the transfer model's performance decreased by only 4%, a minor reduction compared to the 10% drop in the fully supervised model. However, as noise intensity increased from 1 to 3 times the variance, the accuracy of both models dropped-the pre-trained model by 14.5% and the fully-supervised model by 8.7%. The pre-trained model's initial stability may be due to its reliance on sophisticated features learned during pretraining, making it more resistant to minor disturbances. Yet, high noise levels impact the pre-trained model more as it may inaccurately associate enhanced noise with previously learned features, leading to performance drops. Conversely, the fullysupervised model adapts better to high noise levels, possibly because it continuously fine-tunes parameters to accommodate all variations, including noise.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "E. Explainability And Connectivity Analysis",
      "text": "To investigate the role of different brain regions in emotion recognition, we conducted a connection analysis after the model stabilized, focusing on the 10-20 system. By computing cosine similarity between node positions to form an adjacency matrix and retaining only connections exceeding the mean plus 1.8 standard deviations, our analysis (Fig.  8 ) highlights significant involvement of the frontal and temporal lobes in emotion processing, along with observed asymmetry in brain hemisphere activities. These findings align with previous studies  [52] ,  [54] ,  [55] , suggesting a correlation with the spatial distribution of emotions and activation of frontal-parietal networks in response to emotional stimuli. This underscores the distinct EEG signal characteristics during emotion recognition and suggests potential for future research using advanced graph theory to further elucidate the complex interactions between brain regions and emotions.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "F. Feature Space Visualization",
      "text": "This section describes the extraction of features by the CLDTA model that align between subjects, particularly when encountering new participants. We utilize the t-SNE algorithm to evaluate the model's performance on previously unseen subjects. Due to space limitations and for clarity, we randomly chose three participants and visualized their spatial characteristics for both positive and negative emotions. Figure  9  illustrates the feature distributions of these three subjects from the SEED dataset in a two-dimensional space using t-SNE.\n\nWe analyzed the ICD and ICS metrics, as detailed in equations (  19 ) and (  20 ), with results displayed in Table VI. Initial observations from Fig.  9     This paper introduces a Transfer Learning framework utilizing contrastively pre-trained CLDTA, which encode EEG signals into subject-independent emotional representations, regardless of channel count. We tested our model against four prominent emotional databases, SEED, SEED-IV, SEED-V, and DEAP, comparing it with current benchmarks. Our CLDTA model presents several advantages over existing emotion recognition methods. It dynamically leverages spatial characteristics of EEG channels based on the 10-20 system, enabling it to accommodate diverse emotion datasets with varying channel counts. Through contrastive learning, the model potentially uncovers shared temporal-spatial patterns among different emotion categories, offering insights with neurophysiological significance. Moreover, CLDTA's ability to model new subjects with fewer calibration data and its enhanced anti-interference capabilities reduce the reliance on costly label collection and manual feature extraction. This facilitates broader and quicker deployment of emotion recognition systems, improving their practical applicability.\n\nHowever, the primary training data source is the SEED series, and despite employing multiple data augmentation techniques, the limited diversity could impact the robustness and generalizability of aBCI models in real-world applications. Additionally, further discussion and research on the topic of negative transfer remain necessary. Lastly, the practical deployment of the model, particularly in environments with a lower signal-to-noise ratio than laboratory conditions, has yet to be tested.\n\nTo enhance applicability in real-world scenarios, future work will aim to collect a more diverse dataset covering a wider range of ages and scenarios, explore the feasibility of large-scale emotional BCI models, and plan to achieve high performance with fewer EEG channels. It is only when aBCI can provide stable and effective performance across sessions, subjects, and dataset tasks that it can be expected to manage the complex and varied emotional recognition scenarios in real-life.",
      "page_start": 10,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , the architecture encompasses both the pre-",
      "page": 3
    },
    {
      "caption": "Figure 2: It draws on the",
      "page": 3
    },
    {
      "caption": "Figure 1: Overview of the Transfer Learning Pipeline Using Contrastive Learning based on Diagonal Transformer Autoencoder",
      "page": 4
    },
    {
      "caption": "Figure 2: ) to extract brain",
      "page": 4
    },
    {
      "caption": "Figure 2: ) to isolate the learned knowledge,",
      "page": 4
    },
    {
      "caption": "Figure 3: (a). When processing EEG data with fewer number of",
      "page": 4
    },
    {
      "caption": "Figure 3: (b). To counteract this issue, Fig.",
      "page": 4
    },
    {
      "caption": "Figure 2: Architecture of the DTA. The blue box signifies the",
      "page": 4
    },
    {
      "caption": "Figure 3: Visualization of Self-Attention Weights and Diagonal",
      "page": 5
    },
    {
      "caption": "Figure 2: illustrates this information isolation mechanism within the",
      "page": 5
    },
    {
      "caption": "Figure 2: , we have two encoding inputs, Source",
      "page": 5
    },
    {
      "caption": "Figure 4: , the Projector mainly includes three",
      "page": 5
    },
    {
      "caption": "Figure 4: , the classifier primarily comprises two fully",
      "page": 6
    },
    {
      "caption": "Figure 4: The architecture of the Projector and Classifier. BN",
      "page": 6
    },
    {
      "caption": "Figure 5: Performance of Fully-Supervised vs. Transfer-Learning-Based CLDTA Models in Calibration Tests Across SEED",
      "page": 9
    },
    {
      "caption": "Figure 5: illustrates that when employing",
      "page": 9
    },
    {
      "caption": "Figure 6: indicate that pre-trained models on",
      "page": 9
    },
    {
      "caption": "Figure 6: Calibration Test Results on DEAP Dataset: Comparing",
      "page": 9
    },
    {
      "caption": "Figure 7: Evaluating Pre-trained and Fully-Supervised Models'",
      "page": 10
    },
    {
      "caption": "Figure 8: ) highlights",
      "page": 10
    },
    {
      "caption": "Figure 8: Degree centrality of brain connectivity learned by the",
      "page": 10
    },
    {
      "caption": "Figure 9: illustrates the feature distributions of these three subjects",
      "page": 10
    },
    {
      "caption": "Figure 9: (a) and (b) highlight subject",
      "page": 10
    },
    {
      "caption": "Figure 9: (c) shows that the CLDTA",
      "page": 11
    },
    {
      "caption": "Figure 9: (d), both metrics exhibit further enhancement,",
      "page": 11
    },
    {
      "caption": "Figure 9: Visualization of latent features using t-SNE on the",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MODEL": "",
          "Dataset": "SEED \nSEED-IV \nSEED-V"
        },
        {
          "MODEL": "",
          "Dataset": "Acc. \nStd. \nAcc. \nStd. \nAcc. \nStd."
        },
        {
          "MODEL": "SVM[53] \nA-LSTM[51]  \nDGCNN[17] \nBiDANN[23] \nSSL-EEG[52] \nRGNN[24] \nGMSS[15] \nPR-PL[25] \nDTA w/o TF",
          "Dataset": "83.99 \n9.27 \n56.61 \n20.05 \n69.5 \n10.28 \n88.61 \n10.16 \n69.50 \n15.65 \n- \n- \n90.4 \n8.49 \n65.97 \n15.03 \n- \n- \n92.38 \n7.04 \n70.29 \n12.63 \n- \n- \n83.32 \n9.20 \n63.59 \n19.82 \n- \n- \n94.24 \n5.95 \n79.37 \n10.54 \n- \n- \n4.63 \n86.37 \n11.45 \n- \n- \n96.48 \n94.84 \n9.16 \n83.33 \n10.61 \n- \n- \n90.44 \n8.49 \n81.88 \n13.29 \n77.92 \n11.17"
        },
        {
          "MODEL": "DTA-Single-Dataset \nDTA-Multi-Dataset",
          "Dataset": "93.12 \n5.02 \n82.12 \n6.52 \n78.33 \n9.61 \n95.09 \n4.48 \n88.3 \n4.62 \n80.15 \n8.33"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MODEL": "",
          "Dataset": "SEED \nSEED-IV \nSEED-V"
        },
        {
          "MODEL": "",
          "Dataset": "Acc. \nStd. \nAcc. \nStd. \nAcc. \nStd."
        },
        {
          "MODEL": "SVM[53] \nA-LSTM[51] \nDGCNN[17] \nSSL-EEG[52] \nGMSS[15]",
          "Dataset": "56.73 \n16.29 \n51.78 \n12.85 \n47.3 \n16.53 \n72.18 \n10.85 \n55.03 \n09.28 \n- \n- \n09.02 \n52.82 \n09.23 \n- \n- \n79.95 \n67.52 \n 12.73 \n53.62 \n 08.47 \n- \n- \n11.91 \n62.13 \n08.33 \n- \n- \n76.04"
        },
        {
          "MODEL": "CLDTA",
          "Dataset": "75.09 \n05.88 \n64.11 \n04.62 \n61.45 \n10.82"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Methods": "",
          "Accuracy (Mean / SD) (%)": "Valence \nArousal"
        },
        {
          "Methods": "SVM[53] \nCD-EmotionNet [41] \nDGCNN[17]",
          "Accuracy (Mean / SD) (%)": "72.59 / 9.73 \n74.44 / 9.84 \n86.29 / 9.71 \n84.16 / 10.86 \n86.32 / 6.04 \n83.68 / 5.68"
        },
        {
          "Methods": "Rand2DEAP \nSEED2DEAP \nDEAP2DEAP",
          "Accuracy (Mean / SD) (%)": "81.92 / 3.53 \n83.37 / 4.33 \n93.31 / 1.80 \n94.11 / 2.10 \n92.58 / 1.80 \n94.58 / 1.40"
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion recognition using multi-modal data and machine learning techniques: A tutorial and review",
      "authors": [
        "J Zhang",
        "Z Yin",
        "P Chen",
        "S Nichele"
      ],
      "year": "2020",
      "venue": "Information Fusion",
      "doi": "10.1016/j.inffus.2020.01.011"
    },
    {
      "citation_id": "2",
      "title": "Utilizing Deep Learning Towards Multi-Modal Bio-Sensing and Vision-Based Affective Computing",
      "authors": [
        "T.-P Siddharth",
        "T Jung",
        "Sejnowski"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Affective Comput",
      "doi": "10.1109/TAFFC.2019.2916015"
    },
    {
      "citation_id": "3",
      "title": "Ten challenges for EEG-based affective computing",
      "authors": [
        "X Hu",
        "J Chen",
        "F Wang",
        "D Zhang"
      ],
      "year": "2019",
      "venue": "Brain Science Advances",
      "doi": "10.1177/2096595819896200"
    },
    {
      "citation_id": "4",
      "title": "Affective Brain-Computer Interfaces (aBCIs): A Tutorial",
      "authors": [
        "D Wu",
        "B.-L Lu",
        "B Hu",
        "Z Zeng"
      ],
      "year": "2023",
      "venue": "Proc. IEEE",
      "doi": "10.1109/JPROC.2023.3277471"
    },
    {
      "citation_id": "5",
      "title": "A comprehensive survey on emotion recognition based on electroencephalograph (EEG) signals",
      "authors": [
        "K Kamble",
        "J Sengupta"
      ],
      "year": "2023",
      "venue": "Multimed Tools Appl",
      "doi": "10.1007/s11042-023-14489-9"
    },
    {
      "citation_id": "6",
      "title": "Application of Electroencephalography-Based Machine Learning in Emotion Recognition: A Review",
      "authors": [
        "J Cai",
        "R Xiao",
        "W Cui",
        "S Zhang",
        "G Liu"
      ],
      "year": "2021",
      "venue": "Front. Syst. Neurosci",
      "doi": "10.3389/fnsys.2021.729707"
    },
    {
      "citation_id": "7",
      "title": "Emotion recognition and artificial intelligence: A systematic review (2014-2023) and research recommendations",
      "authors": [
        "S Khare",
        "V Blanes-Vidal",
        "E Nadimi",
        "U Acharya"
      ],
      "year": "2024",
      "venue": "Information Fusion",
      "doi": "10.1016/j.inffus.2023.102019"
    },
    {
      "citation_id": "8",
      "title": "From Word Embedding to Reading Embedding Using Large Language Model, EEG and Eye-tracking",
      "authors": [
        "Y Zhang",
        "S Yang",
        "G Cauwenberghs",
        "T.-P Jung"
      ],
      "year": "2024",
      "venue": "arXiv"
    },
    {
      "citation_id": "9",
      "title": "DREAMER: A Database for Emotion Recognition Through EEG and ECG Signals From Wireless Low-cost Off-the-Shelf Devices",
      "authors": [
        "S Katsigiannis",
        "N Ramzan"
      ],
      "year": "2018",
      "venue": "IEEE J. Biomed. Health Inform",
      "doi": "10.1109/JBHI.2017.2688239"
    },
    {
      "citation_id": "10",
      "title": "Consumer Grade Brain Sensing for Emotion Recognition",
      "authors": [
        "P Lakhan"
      ],
      "year": "2019",
      "venue": "IEEE Sensors J",
      "doi": "10.1109/JSEN.2019.2928781"
    },
    {
      "citation_id": "11",
      "title": "HybridEEGNet: A Convolutional Neural Network for EEG Feature Learning and Depression Discrimination",
      "authors": [
        "Z Wan",
        "J Huang",
        "H Zhang",
        "H Zhou",
        "J Yang",
        "N Zhong"
      ],
      "year": "2020",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2020.2971656"
    },
    {
      "citation_id": "12",
      "title": "Investigating Critical Frequency Bands and Channels for EEG-Based Emotion Recognition with Deep Neural Networks",
      "authors": [
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2015",
      "venue": "IEEE Trans. Auton. Mental Dev",
      "doi": "10.1109/TAMD.2015.2431497"
    },
    {
      "citation_id": "13",
      "title": "DEAP: A Database for Emotion Analysis ;Using Physiological Signals",
      "authors": [
        "S Koelstra"
      ],
      "year": "2012",
      "venue": "IEEE Trans. Affective Comput",
      "doi": "10.1109/T-AFFC.2011.15"
    },
    {
      "citation_id": "14",
      "title": "Multi-channel EEG-based emotion recognition in the presence of noisy labels",
      "authors": [
        "C Li",
        "Y Hou",
        "R Song",
        "J Cheng",
        "Y Liu",
        "X Chen"
      ],
      "year": "2022",
      "venue": "Sci. China Inf. Sci",
      "doi": "10.1007/s11432-021-3439-2"
    },
    {
      "citation_id": "15",
      "title": "GMSS: Graph-Based Multi-Task Self-Supervised Learning for EEG Emotion Recognition",
      "authors": [
        "Y Li"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Affective Comput",
      "doi": "10.1109/TAFFC.2022.3170428"
    },
    {
      "citation_id": "16",
      "title": "Differential entropy feature for EEG-based emotion classification",
      "authors": [
        "R.-N Duan",
        "J.-Y Zhu",
        "B.-L Lu"
      ],
      "year": "2013",
      "venue": "2013 6th International IEEE/EMBS Conference on Neural Engineering (NER)",
      "doi": "10.1109/NER.2013.6695876"
    },
    {
      "citation_id": "17",
      "title": "EEG Emotion Recognition Using Dynamical Graph Convolutional Neural Networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Affective Comput",
      "doi": "10.1109/TAFFC.2018.2817622"
    },
    {
      "citation_id": "18",
      "title": "CNN and LSTM based ensemble learning for human emotion recognition using EEG recordings",
      "authors": [
        "A Iyer",
        "S Das",
        "R Teotia",
        "S Maheshwari",
        "R Sharma"
      ],
      "year": "2023",
      "venue": "Multimed Tools Appl",
      "doi": "10.1007/s11042-022-12310-7"
    },
    {
      "citation_id": "19",
      "title": "A LSTM based deep learning network for recognizing emotions using wireless brainwave driven system",
      "authors": [
        "A Sakalle",
        "P Tomar",
        "H Bhardwaj",
        "D Acharya",
        "A Bhardwaj"
      ],
      "year": "2021",
      "venue": "Expert Systems with Applications",
      "doi": "10.1016/j.eswa.2020.114516"
    },
    {
      "citation_id": "20",
      "title": "Emotion recognition using spatial-temporal EEG features through convolutional graph attention network",
      "authors": [
        "Z Li",
        "G Zhang",
        "L Wang",
        "J Wei",
        "J Dang"
      ],
      "year": "2023",
      "venue": "J. Neural Eng",
      "doi": "10.1088/1741-2552/acb79e"
    },
    {
      "citation_id": "21",
      "title": "Dry-Contact and Noncontact Biopotential Electrodes: Methodological Review",
      "authors": [
        "Y Chi",
        "T.-P Jung",
        "G Cauwenberghs"
      ],
      "year": "2010",
      "venue": "IEEE Rev. Biomed. Eng",
      "doi": "10.1109/RBME.2010.2084078"
    },
    {
      "citation_id": "22",
      "title": "Contrastive Learning of Subject-Invariant EEG Representations for Cross-Subject Emotion Recognition",
      "authors": [
        "X Shen",
        "X Liu",
        "X Hu",
        "D Zhang",
        "S Song"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Affective Comput",
      "doi": "10.1109/TAFFC.2022.3164516"
    },
    {
      "citation_id": "23",
      "title": "A Novel Neural Network Model based on Cerebral Hemispheric Asymmetry for EEG Emotion Recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "Z Cui",
        "T Zhang",
        "Y Zong"
      ],
      "year": "2018",
      "venue": "Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence",
      "doi": "10.24963/ijcai.2018/216"
    },
    {
      "citation_id": "24",
      "title": "EEG-Based Emotion Recognition Using Regularized Graph Neural Networks",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2020.2994159"
    },
    {
      "citation_id": "25",
      "title": "PR-PL: A Novel Prototypical Representation Based Pairwise Learning Framework for Emotion Recognition Using EEG Signals",
      "authors": [
        "R Zhou"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Affective Comput",
      "doi": "10.1109/TAFFC.2023.3288118"
    },
    {
      "citation_id": "26",
      "title": "Personalizing EEG-based affective models with transfer learning",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2016",
      "venue": "Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, in IJCAI'16"
    },
    {
      "citation_id": "27",
      "title": "EmotionMeter: A Multimodal Framework for Recognizing Human Emotions",
      "authors": [
        "W.-L Zheng",
        "W Liu",
        "Y Lu",
        "B.-L Lu",
        "A Cichocki"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Cybern",
      "doi": "10.1109/TCYB.2018.2797176"
    },
    {
      "citation_id": "28",
      "title": "Comparing Recognition Performance and Robustness of Multimodal Deep Learning Models for Multimodal Emotion Recognition",
      "authors": [
        "W Liu",
        "J.-L Qiu",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Cogn. Dev. Syst",
      "doi": "10.1109/TCDS.2021.3071170"
    },
    {
      "citation_id": "29",
      "title": "Multimodal emotion recognition using EEG and eye tracking data",
      "authors": [
        "Wei-Long Zheng",
        "Bo-Nan Dong",
        "Bao-Liang Lu"
      ],
      "year": "2014",
      "venue": "2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society",
      "doi": "10.1109/EMBC.2014.6944757"
    },
    {
      "citation_id": "30",
      "title": "Self-Supervised EEG Emotion Recognition Models Based on CNN",
      "authors": [
        "X Wang",
        "Y Ma",
        "J Cammon",
        "F Fang",
        "Y Gao",
        "Y Zhang"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Neural Syst. Rehabil. Eng",
      "doi": "10.1109/TNSRE.2023.3263570"
    },
    {
      "citation_id": "31",
      "title": "Emotion Recognition using Multimodal Residual LSTM Network",
      "authors": [
        "J Ma",
        "H Tang",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2019",
      "venue": "Proceedings of the 27th ACM International Conference on Multimedia",
      "doi": "10.1145/3343031.3350871"
    },
    {
      "citation_id": "32",
      "title": "Attention Is All You Need",
      "authors": [
        "A Vaswani"
      ],
      "year": "2023",
      "venue": "arXiv"
    },
    {
      "citation_id": "33",
      "title": "Transformers for EEG-Based Emotion Recognition: A Hierarchical Spatial Information Learning Model",
      "authors": [
        "Z Wang",
        "Y Wang",
        "C Hu",
        "Z Yin",
        "Y Song"
      ],
      "year": "2022",
      "venue": "IEEE Sensors J",
      "doi": "10.1109/JSEN.2022.3144317"
    },
    {
      "citation_id": "34",
      "title": "Long-term intra-individual variability of the background EEG in normals",
      "authors": [
        "A Kondacs",
        "M SzabÃ³"
      ],
      "year": "1999",
      "venue": "Clinical Neurophysiology",
      "doi": "10.1016/S1388-2457(99)00122-4"
    },
    {
      "citation_id": "35",
      "title": "Cross-Subject Tinnitus Diagnosis Based on Multi-Band EEG Contrastive Representation Learning",
      "authors": [
        "C.-D Wang"
      ],
      "year": "2023",
      "venue": "IEEE J. Biomed. Health Inform",
      "doi": "10.1109/JBHI.2023.3264521"
    },
    {
      "citation_id": "36",
      "title": "Can Emotion Be Transferred?-A Review on Transfer Learning for EEG-Based Emotion Recognition",
      "authors": [
        "W Li",
        "W Huan",
        "B Hou",
        "Y Tian",
        "Z Zhang",
        "A Song"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Cogn. Dev. Syst",
      "doi": "10.1109/TCDS.2021.3098842"
    },
    {
      "citation_id": "37",
      "title": "Individual Similarity Guided Transfer Modeling for EEG-based Emotion Recognition",
      "authors": [
        "X Zhang"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)",
      "doi": "10.1109/BIBM47256.2019.8982972"
    },
    {
      "citation_id": "38",
      "title": "Two-Phase Prototypical Contrastive Domain Generalization for Cross-Subject EEG-Based Emotion Recognition",
      "authors": [
        "H Cai",
        "J Pan"
      ],
      "year": "2023",
      "venue": "ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/ICASSP49357.2023.10096469"
    },
    {
      "citation_id": "39",
      "title": "Constructing a Personalized Cross-Day EEG-Based Emotion-Classification Model Using Transfer Learning",
      "authors": [
        "Y.-P Lin"
      ],
      "year": "2020",
      "venue": "IEEE J. Biomed. Health Inform",
      "doi": "10.1109/JBHI.2019.2934172"
    },
    {
      "citation_id": "40",
      "title": "Emotion recognition with convolutional neural network and EEG-based EFDMs",
      "authors": [
        "F Wang"
      ],
      "year": "2020",
      "venue": "Neuropsychologia",
      "doi": "10.1016/j.neuropsychologia.2020.107506"
    },
    {
      "citation_id": "41",
      "title": "Emotion Recognition from Few-Channel EEG Signals by Integrating Deep Feature Aggregation and Transfer Learning",
      "authors": [
        "F Liu"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Affective Comput",
      "doi": "10.1109/TAFFC.2023.3336531"
    },
    {
      "citation_id": "42",
      "title": "BYOL for Audio: Self-Supervised Learning for General-Purpose Audio Representation",
      "authors": [
        "D Niizumi",
        "D Takeuchi",
        "Y Ohishi",
        "N Harada",
        "K Kashino"
      ],
      "year": "2021",
      "venue": "2021 International Joint Conference on Neural Networks (IJCNN)",
      "doi": "10.1109/IJCNN52387.2021.9534474"
    },
    {
      "citation_id": "43",
      "title": "A Multi-view Spectral-Spatial-Temporal Masked Autoencoder for Decoding Emotions with Self-supervised Learning",
      "authors": [
        "R Li",
        "Y Wang",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia",
      "doi": "10.1145/3503161.3548243"
    },
    {
      "citation_id": "44",
      "title": "Data augmentation for learning predictive models on EEG: a systematic comparison",
      "authors": [
        "C Rommel",
        "J Paillard",
        "T Moreau",
        "A Gramfort"
      ],
      "year": "2022",
      "venue": "J. Neural Eng",
      "doi": "10.1088/1741-2552/aca220"
    },
    {
      "citation_id": "45",
      "title": "Fast and Accurate Deep Bidirectional Language Representations for Unsupervised Learning",
      "authors": [
        "J Shin",
        "Y Lee",
        "S Yoon",
        "K Jung"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2020.acl-main.76"
    },
    {
      "citation_id": "46",
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "authors": [
        "T Chen",
        "S Kornblith",
        "M Norouzi",
        "G Hinton"
      ],
      "year": "2020",
      "venue": "Proceedings of the 37th International Conference on Machine Learning (ICML'20)"
    },
    {
      "citation_id": "47",
      "title": "EEGLAB: an open source toolbox for analysis of single-trial EEG dynamics including independent component analysis",
      "authors": [
        "A Delorme",
        "S Makeig"
      ],
      "year": "2004",
      "venue": "Journal of Neuroscience Methods",
      "doi": "10.1016/j.jneumeth.2003.10.009"
    },
    {
      "citation_id": "48",
      "title": "An Efficient LSTM Network for Emotion Recognition From Multichannel EEG Signals",
      "authors": [
        "X Du"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Affective Comput",
      "doi": "10.1109/TAFFC.2020.3013711"
    },
    {
      "citation_id": "49",
      "title": "Adam: A Method for Stochastic Optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2017",
      "venue": "arXiv"
    },
    {
      "citation_id": "50",
      "title": "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)",
      "authors": [
        "D.-A Clevert",
        "T Unterthiner",
        "S Hochreiter"
      ],
      "year": "2016",
      "venue": "arXiv"
    },
    {
      "citation_id": "51",
      "title": "MPED: A Multi-Modal Physiological Emotion Database for Discrete Emotion Recognition",
      "authors": [
        "T Song",
        "W Zheng",
        "C Lu",
        "Y Zong",
        "X Zhang",
        "Z Cui"
      ],
      "year": "2019",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2019.2891579"
    },
    {
      "citation_id": "52",
      "title": "From Regional to Global Brain: A Novel Hierarchical Spatial-Temporal Neural Network Model for EEG Emotion Recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "L Wang",
        "Y Zong",
        "Z Cui"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Affective Comput",
      "doi": "10.1109/TAFFC.2019.2922912"
    },
    {
      "citation_id": "53",
      "title": "Improving BCI-based emotion recognition by combining EEG feature selection and kernel classifiers",
      "authors": [
        "J Atkinson",
        "D Campos"
      ],
      "year": "2016",
      "venue": "Expert Systems with Applications",
      "doi": "10.1016/j.eswa.2015.10.049"
    },
    {
      "citation_id": "54",
      "title": "Frontal asymmetry as a mediator and moderator of emotion: An updated review",
      "authors": [
        "S Reznik",
        "J Allen"
      ],
      "year": "2018",
      "venue": "Psychophysiology",
      "doi": "10.1111/psyp.12965"
    },
    {
      "citation_id": "55",
      "title": "Frontal EEG asymmetry as a moderator and mediator of emotion",
      "authors": [
        "J Coan",
        "J Allen"
      ],
      "year": "2004",
      "venue": "Biological Psychology",
      "doi": "10.1016/j.biopsycho.2004.03.002"
    }
  ]
}