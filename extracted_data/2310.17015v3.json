{
  "paper_id": "2310.17015v3",
  "title": "Data Augmentation For Emotion Detection In Small Imbalanced Text Data",
  "published": "2023-10-25T21:29:36Z",
  "authors": [
    "Anna Koufakou",
    "Diego Grisales",
    "Ragy Costa de jesus",
    "Oscar Fox"
  ],
  "keywords": [
    "emotion recognition",
    "emotion detection",
    "data augmentation",
    "affective computing",
    "paraphrasing",
    "NLP"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition in text, the task of identifying emotions such as joy or anger, is a challenging problem in NLP with many applications. One of the challenges is the shortage of available datasets that have been annotated with emotions. Certain existing datasets are small, follow different emotion taxonomies and display imbalance in their emotion distribution. In this work, we studied the impact of data augmentation techniques precisely when applied to small imbalanced datasets, for which current state-of-the-art models (such as RoBERTa) under-perform. Specifically, we utilized four data augmentation methods (Easy Data Augmentation EDA, static and contextual Embedding-based, and ProtAugment) on three datasets that come from different sources and vary in size, emotion categories and distributions. Our experimental results show that using the augmented data when training the classifier model leads to significant improvements. Finally, we conducted two case studies: a) directly using the popular chat-GPT API to paraphrase text using different prompts, and b) using external data to augment the training set. Results show the promising potential of these methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Today's society has been tremendously impacted by recent advances in machine learning and Natural Language Processing (NLP) in particular. There is great interest to apply current state-of-the-art models to a myriad of data and tasks. One of the research areas that has attracted interest is the task of detecting emotions expressed in text. There are many applications from intelligent recommender systems to social media monitoring to mental health intervention. For existing work in this area, the reader is referred to reviews such as  [1] .\n\nThere are several challenges related to NLP for emotion recognition. Researchers must handle datasets in different formats (for example, a traditional essay or dialogue exchange or social media post). Emotions may be hard to comprehend by humans in the first place, so they are complicated for human annotators as well. Different taxonomies for categorizing emotions exist, e.g., Ekman  [2]  or Plutchik  [3] . All this is further complicated by the fact that, the same paragraph or even social media post may express more than one emotion. This shows the inherent difficulty in collecting and annotating large datasets for emotion recognition. Several datasets that do exist contain a small number of records: for example, a few thousand in COVID-19 survey dataset from  [4] . Furthermore, these datasets are heavily dominated by one or two emotions, making the datasets imbalanced and therefore harder for algorithms to detect the rare emotion categories.\n\nOne avenue that has been pursued to help alleviate the scarcity of data generally in NLP is Data Augmentation. This term refers to artificially increasing the size of the training data by generating new records from the existing training ones. There are various techniques that have been proposed for NLP, from replacing random words in the text with their synonyms  [5]  to translating the record to another language and then back to the original language  [6]  to using sophisticated models for paraphrasing the text in the original records  [7] .\n\nRecently, there have been thorough surveys of data augmentation for NLP, e.g.  [8] ,  [9] . In this paper, we followed  [10] , which conducted an extensive comparison of augmentation techniques for NLP. The authors showed that augmentation works for different datasets, including an emotion-based dataset. Additionally, researchers have applied data augmentation for mental health classification on social media data  [11] . They used Logistic regression, Support Vector Machine and Random Forest classifiers on two datasets labeled for stress and for depression/suicide. Augmentation for emotion recognition in data from software repositories was successfully used in  [12] . On the other hand, negative findings have been reported regarding the effectiveness of data augmentation on downstream classification, see  [13]  and  [14] . Very recently, three teams in the WASSA-23 shared tasks, based on a more recent version of one the datasets in our paper, used data augmentation  [15] . Nevertheless, our work is the first to provide the community with a comprehensive comparative study of Text Data Augmentation, particularly tailored to emotion detection. Specifically, in this work, our goal is to explore the impact of data augmentation on the classification of emotions in small imbalanced data. We intentionally selected recent emotion-labeled datasets that have been shown to have low accuracy in emotion detection tasks (for example, see  [16] ,  [17] ), specifically because of the larger potential benefits from augmentation. Our datasets include a dataset based on a UK survey related to COVID-19  [4] ; one collected from social media posts  [17] ; and one containing essays written after reading news articles  [18]  (we describe the datasets in detail in Section II).\n\nOur contribution is that we applied augmentation specifically focused to emotion datasets with a small size and imbalanced class distribution. For example, it has been shown in  [16] ,  [19]  that using records from a larger emotion-annotated dataset, GoEmotions  [20] , alongside existing training data can improve classification performance for data we used in this paper; however, they did not perform any augmentation techniques on the original training records. Additionally, it was shown in  [16]  that classification on their GoEmotions-based set  [20]  had relaively high f1-macro's, therefore we did not try to augment and classify GoEmotions in our experimental study. Similarly for an emotion-labeled dataset used in  [10] . The organization of this paper is as follows: Section II contains a description of the datasets used in this paper. Section III gives a background on data augmentation methods for NLP, focusing on methods we used for this work. Then, it describes the overall process we used for augmenting our data and for classification. Section IV presents our experimental setup, results and observations. Finally, Section V includes concluding remarks and future research.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Datasets",
      "text": "In this paper, we experimented with the datasets described in the following. We focused on datasets that were small and imbalanced that were also presented in 2020 or later, rather than earlier datasets such as SemEval-2018 Task 1  [21] . The datasets have a variety of emotions and distributions, and come from different sources such as social media or essays written by survey participants. All datasets are in English.\n\nCOVID-19 Survey Data 1 was presented in  [4] . This data was collected via a survey in the UK under lockdown for COVID-19 (in 2020). The participants in that survey wrote a paragraph of text as well as entered demographic data (e.g. gender) and ratings for several emotions. Following  [16] , we selected to use the \"chosen emotion\": a category chosen by each participant out of several emotion options. As in  [16] , we kept the emotion categories with at least 4% of the total records, which resulted in a dataset of 2,408 records. This dataset does not follow Ekman  [2]  or other emotion taxonomy. The resulting dataset contains Anger, Anxiety (dominant), Fear, Relaxation, and Sadness. For our experiments, we chose to use stratified split (70-30) to divide the records into a train and a test set (we did this for all datasets to which this applied).\n\n1 Data available at https://github.com/ben-aaron188/\\covid19worry\n\nEmoEvent-EN Data 2  was presented in  [17] . The dataset was collected from the Twitter platform based on various events that took place in April 2019 (for example, the Notre Dame Cathedral Fire). In order to select affective tweets, the authors in  [17]  used Linguistic Inquiry and Word Count (LIWC)  [22] . The selected tweets were then annotated by Amazon MTurkers using one of seven emotions: six emotions from Ekman's taxonomy  [2]  plus \"neutral or other emotions\". In addition, each record was labeled as offensive or not, which we ignored for this paper. The total records in the resulting sets were 7,303 in English (there was also a dataset in Spanish, which we leave for future research). The dataset on the online repository had already replaced any hashtags, urls, or mentions with 'HASHTAG', 'URL' and 'USER'. We split the data into train and test using a stratified split (70-30).\n\nWASSA-21 Data 3  was part of the 11th Workshop on Computational Approaches to Subjectivity, Sentiment & Social Media Analysis (WASSA 2021) Shared Task on Empathy Detection and Emotion Classification summarized in  [18] . This dataset is an extension of  [23] 's dataset based on news articles related to harm to an individual, group, nature, etc. The dataset contains essays written to express the author's empathy and distress in reaction to the news articles. The essays are annotated for empathy and distress, as well as personality traits and demographic information (age, gender, etc.). Each essay is also tagged with one of the Ekman's emotions  [2] : Anger, Disgust, Fear, Joy, Sadness, and Surprise. We only focused on the emotion for each essay, not the empathy or distress labels, following  [16] . We did not perform train-test split for this data, as the WASSA-21 dataset already had distinct sets for training (1,860 records) and testing (525 records).\n\nA comparison is shown in Table  I : this shows number of records per each emotion, if the emotion is present in that dataset. The counts are divided into train and test sets. The last two rows of the Table display the total number of records in each set followed by the average length of the records in characters. As shown in Table  I , there is variety of emotion emotion categories as well as distributions. For example, COVID-19 is heavily skewed towards Anxiety (57%), which is not part of the Ekman taxonomy  [2] , therefore not found in the rest of the datasets. WASSA-21 is dominated by Anger and Sadness, emotions found in all datasets. In contrast, EmoEvent-EN is heavily dominated by Joy and Others (around 73% for these two emotions) so it is more positive. As far as size, two datasets (COVID-19 and WASSA-21) have around 2.5K records, while EmoEvent-EN is more than 3 times larger (around 7K records). On the other hand, the EmoEvent-EN dataset contains tweets, so the length of each record is shorter, while the other two datasets have essays (paragraphs of sentences) -see average length in characters in Table  I .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Methodology",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Background: Data Augmentation Methods",
      "text": "In this section, we provide background for all the data augmentation methods we used in this work. We chose widelyused augmentation methods for NLP based on simple word manipulations or more sophisticated based on the entire record. All these methods have been shown to perform well (e.g. see  [10] ) therefore we chose to use them for our work.\n\nEasy Data Augmentation (EDA): This simple technique proposed in  [5]  has been shown to be quite successful. EDA combines several operations for the words in the original record. For any given sentence in the training records, EDA randomly chooses and performs one of the following (stop words are not considered, and there is a parameter for choosing the percent of the words to be altered):\n\n• Synonym Replacement: random words are replaced by random synonyms from a dictionary e.g. WordNet  [24] ; • Random Insertion: a random word from the given sentence is chosen, and one of its synonyms is then inserted in the sentence at a random position; • Random Swap: randomly chosen words are swapped;\n\n• Random Deletion: random words are removed. Embeddings: Rather than using synonyms from a dictionary as in the previous technique, one could employ word embeddings instead. These methods start by representing the words by embeddings, or vectors of n-dimensions. Then, they replace or insert words that are found to be similar in the word embedding space, e.g. by using cosine distance. In earlier work, for example,  [25] , researchers used static pretrained word embeddings. In order to take advantage of the more recent advances, we can use contextual embeddings, for example generated by a pre-trained transformer model such as BERT (Bidirectional Encoder Representations from Transformers)  [26] . In summary, the word in the original text w is replaced by words predicted by a model such as BERT based on the context around w in the original text. These methods may also chose random words to replace, insert etc. as in the previous parapraph.\n\nBART Paraphraser ProtAugment: Instead of focusing on individual words, ProtAugment  [7]  paraphrases the original text. This technique employs BART, a model \"combining Bidirectional and Auto-Regressive Transformers\"  [27] , to generate paraphrases. The authors fine-tuned BART on the paraphrase generation task using various datasets. They also utilized Diverse Beam Search  [28]  and Back Translation  [6]  to help generate diverse outputs.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. Overall Process For Augmentation And Classification",
      "text": "First, the dataset was split into training and testing sets if needed (Section II has the details for each set). The training data was fed as input to each of the Data Augmentation techniques (described in the previous paragraphs). Using each specific augmentation method, we generated a new record x ′ for each original training record x. For this new record x ′ , we also made a label copy y ′ , where we copied the original training label y. In all our experiments, we generated five paraphrased records for each original record. At the end of this augmentation phase, we had a new 'augmented' training set with text and labels. We then concatenated the 'augmented' training set to the original training set and we presented this 'increased' training dataset to the classifier during the training phase. This set of records then was transformed into features for training the classification model. The test set was also transformed into features and fed into the trained classifier in order to predict the test labels.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Experimental Results",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Experimental Setup",
      "text": "We run our code using Google colab 4  (our code is at https:// github.com/A-Koufakou/AugEmotionDetection). For augmentation, we followed code provided by  [10]  online  5  , which used an NLP augmentation library called NLPAug  6  . For any choices, we used the same as  [10] : for example, for Embeddings, we used 200-dimensional Glove Embeddings  [29]  or BERT for Contextual Embeddings. For our classification, we used Simple Transformers 7  , a library built on Hugging Face 8  . Since RoBERTa (Robustly optimized BERT approach), a transformer-based model  [30] , was shown to outperform other deep learning models in  [16]  for two of our datasets (COVID-19 and WASSA-21), we chose to use the RoBERTa model, specifically, roberta-base. For every experiment, we finetuned the model on our data, so that the pre-trained model can further learn from our data. We used 2 Epochs, learning rate of 1e -5 , maxlen of 256 and batch of 8, based on early trials and  [16] . We repeated each experiment (train/test with each set) 3 times and then reported the average performance on the predicted test labels. We reported results based on: P recision (orRecall) = T P T P + F P (orF N )\n\n(1) where T P is True Positives, F P False Positives, F N False Negatives, and N total number of records. We presented macro-averaged Precision, Recall and f1: for example, f1macro averages the f1-score over all classes, and is well-suited to imbalanced class distributions  [10] ,  [16] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Results",
      "text": "Our classification results with the original and the augmented train datasets are shown in Table  I . Each of the values in the Table is an average of 3 runs. The standard deviation of the results ranges from 0.01 to 0.04 depending on the dataset.\n\nThere was no single winner augmentation technique; we observed that the EDA method did the best for data that contains essays written by authors given a specific question or topic (COVID-19 and WASSA-21), while the embeddings methods and especially contextual embeddings were the top performing methods for tweet-based data (EmoEvent). In more detail, in Table  II  for COVID-19, EDA had the best f1-macro (45.51%), improving the original f1-macro by about 14%. The rest of the methods followed around 44%. In Table  II  for WASSA-21, we see that EDA is once again the winner (54.79% f1-macro) improving on the non-augmented f1-macro by almost 17%. The other methods followed closely behind. 9  In contrast, Table  II  shows that the BERT Embeddings Augmentation did the best for EmoEvent-EN (49.65%) improving on the non-augmented f1-macro by almost 12%. The static (Glove) Embedding method followed closely, with the rest of the augmenters in 1 or 2% lower.\n\nIn Table  II , augmented-based f1-macro's were always higher than non-augmented, however the same was not true for accuracy: for COVID-19 and EmoEvent-EN, accuracy was higher for non-augmented training than augmented. We examined the predictions per class, and when the model was trained with only original training, it did very well on the dominant class (Anxiety, for example in COVID-19) but much worse 9 Note: the winning team on the related WASSA-21 task  [19]  achieved f1macros in mid/upper 50's, with ensembles of several pre-trained transformers each fine-tuned on WASSA-21, and augmentation with GoEmotions  [20]  (see later sections). Exploring ensembles is left for future research.   I  for emotion distributions. The EDA results showed better performance overall, but did mislabel more records for Anxiety, which made accuracy lower. To examine the per-class results further, see the confusion matrix for EmoEvent-EN in Fig.  1 . We selected BERT Augmentation as it does the best for EmoEvent-EN (see Table  II ). From the confusion matrix, we can see that the model did relatively well on predicting Joy and Others (dominant emotions). The model confused Anger, Disgust, Sadness, and Surprise many times with Others. These issues were observed in the original paper  [17] : for example, the annotators had trouble annotating Fear, Disgust and Surprise, or to distinguish between Anger and Disgust (complementary emotions).\n\nIn order to examine the strength of the data augmentation, we used the BLEU (Bilingual Evaluation Understudy) metric  [31]  as per previous work  [10] ,  [11] . BLEU is used for translation problems: lower BLEU score means more diversity from the original text, therefore a stronger augmentation. The BLEU scores for the augmented datasets in this work are shown in Table  III . We calculated BLEU score without brevity punishment to avoid giving shorter texts an advantage per  [10] . As shown in Table  III , the top methods (EDA and BERT) had the lowest BLEU for EmoEvent and WASSA-21 respectively, meaning high lexical diversity could have helped these methods win. The situation is not the same for COVID-19: ProtAugmenter, not EDA, had the lowest score.\n\nFinally, regarding runtime and resource expectations of the augmentation techniques, we observed that EDA had low runtime and no GPU requirement in contrast to the other methods. Depending on the data, we observed 1-3 minutes of runtime",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Case Study A: Paraphrasing With Chatgpt",
      "text": "Motivated by the improvements achieved by the data augmentation using existing techniques already explored in the literature (e.g.  [10] ,  [11] ), we were interested to directly experiment with the popular chatGPT (Chat Generative Pre-Trained Transformer). This is a Large Language Model-based chatbot developed by OpenAI, which made headlines having reached 1 million users in a few days. We were only able to conduct a case study with COVID-19, due to lack of time and space. We also saw work just presented in WASSA-23  [32] : the HIT-SCIR team competed in WASSA-23 shared tasks and also used chatGPT paraphrasing for augmenting the WASSA-23 train sets (using RoBERTa and bi-LSTM for their emotion prediction model). Further exploring their research as well as expanding our chatGPT work to more datasets is a focus of our future research.\n\nWe wrote a script to connect to the OpenAI chat API 10 and explored various prompts for paraphrasing training records. We used a single prompt-response cycle for each record. The prompts with which we experimented that showed promise were 'summarize the following in the first person' (denoted as chatGPT-summ) and 'using a sympathetic tone, paraphrase the following' (denoted as chatGPT-symp). In summary, paraphrasing using a 'summarize' prompt in chatGPT (chatGPTsumm) had the highest f1-macro (46.10%) followed closely by EDA (45.51%). The 'sympathize' prompt in chatGPT (chatGPT-symp) had a lower f1-macro (44.72%). We tried another model we saw in the literature: a T5 model trained on ChatGPT paraphrase data 11 , with lower f1-macro results.\n\nFinally, we looked at example paraphrases displayed in Table  IV . We only show results for EDA and chatGPT due to space. In Table  IV , the paraphrases from chatGPT have a better structure and vary from the original in words and phrasing. For example, it paraphrased 'fairly safe in a secluded location' to 'grateful to be in a safe location'. This is also supported by the BLEU score for chatGPT paraphrases: it is much lower (25.04) than the values shown for COVID-19 in Table  III . This shows that chatGPT paraphrased data were more lexically diverse than the original data compared to the other augmenters. EDA 10 https://platform.openai.com/docs/api-reference/chat 11 https://huggingface.co/humarin/chatgpt paraphraser on T5 base results on the other hand inserted random words, such as 'rattling', replaced 'friends' with 'champion', and swapped the order of the words in the 'future holds'. Nevertheless, EDA is a simple method non-requiring GPU or API calls with a very low runtime comparably.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "D. Case Study B: Augmentation Using External Data",
      "text": "Besides augmentation techniques that reword or paraphrase the original data, recent work has proposed augmenting with an external dataset, perhaps from a different domain or source. For example, both  [19]  and  [16]  used GoEmotions  [20]  (human annotated reddit comments): the first for augmenting WASSA-21, and the second for both COVID-19 and WASSA-21. As both papers showed improvements, we wanted to compare our results against augmenting these data with GoEmotions. First, to match our data format, we selected a subset of the GoEmotions train set based on records annotated with a single Ekman label (about 4,300 records). For each experiment, we only kept the emotions that were present in the original dataset. Finally, similar to Section III-B, we concatenated that data with the original train set, and proceeded as before. The results were favorable for WASSA-21: the model trained with the GoEmotions-augmented data resulted in 55.94% f1-macro (surpassing EDA's 54.79% in Table  II ). However, for the COVID-19 data, f1-macro was 35.74% (much lower than 45.51% for EDA in Table  II ). Our interpretation is that WASSA-21's emotions are more representative of the Ekman taxonomy  [2] , therefore better suited to the additional records from GoEmotions. On the other hand, COVID-19 is heavily influenced by Anxiety, and the extra records we added from GoEmotions for Anger, Fear and Sadness led to negligible improvement in the prediction of these emotions. Nevertheless, we feel this topic warrants more in-depth exploration in our future work.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "V. Conclusions",
      "text": "Detecting emotions such as joy or sadness in text is a significant area of research with many challenges. An important challenge is the availability of large annotated data. In this work, we experimented with several data augmentation techniques that have been successfully utilized in the literature, to explore potential improvements in emotion detection. We experimented with various emotion-labeled datasets, originating from different sources (e.g. social media posts versus essays) and varying in topics, emotions, distributions etc. Our experiments showed that using the augmented datasets for training led to large improvements: the best performing method in each dataset improved the f1-macro by 12-16%. A simple technique, EDA, performed the best on essay data, such as WASSA-21, while contextualized embeddings did the best for tweet-based data. Additionally, we performed two case studies: a) using chatGPT, a very popular pre-trained large language model, to paraphrase the training set with different prompts, and b) using external data (GoEmotions) to augment the training set. The experiments show the promising potential of these techniques. Finally, we examined the strength of the resulting augmented text (lexical diversity vs original, using BLEU score) as well as the execution runtime. Paraphrasing with chatGPT, for example, was much more lexically diverse than simpler techniques (EDA), at the cost of much higher runtime and complexity. For future work, we intend to explore the impact of changing certain (hyper)parameters as well as experiment with more data (e.g. the latest WASSA sets presented in 2023  [15] ), augmentation methods and ensembles. We also aim to methodically study 'prompt engineering' in chatGPT, as it has been shown  [33]  that a well-designed prompt may vastly improve its results.",
      "page_start": 5,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: We selected BERT Augmentation as it does the best",
      "page": 4
    },
    {
      "caption": "Figure 1: Example Confusion Matrix (EmoEvent-EN, BERT Augmentation)",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "akoufakou@fgcu.edu"
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "Abstract—Emotion recognition in text,\nthe task of\nidentifying"
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "emotions such as joy or anger,\nis a challenging problem in NLP"
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": ""
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "with many applications. One of\nthe\nchallenges\nis\nthe\nshortage"
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": ""
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "of available datasets\nthat have been annotated with emotions."
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": ""
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "Certain\nexisting\ndatasets\nare\nsmall,\nfollow different\nemotion"
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "taxonomies and display imbalance in their emotion distribution."
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "In\nthis work, we\nstudied\nthe\nimpact\nof\ndata\naugmentation"
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "techniques precisely when applied to small\nimbalanced datasets,"
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": ""
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "for which current\nstate-of-the-art models\n(such as RoBERTa)"
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": ""
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "under-perform. Specifically, we utilized four data augmentation"
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": ""
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "methods\n(Easy Data Augmentation EDA,\nstatic and contextual"
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "Embedding-based, and ProtAugment) on three datasets that come"
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "from different\nsources\nand\nvary\nin\nsize,\nemotion\ncategories"
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "and\ndistributions. Our\nexperimental\nresults\nshow that\nusing"
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": ""
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "the augmented data when training the classifier model\nleads\nto"
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": ""
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "significant improvements. Finally, we conducted two case studies:"
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": ""
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "a) directly using the popular chat-GPT API\nto paraphrase text"
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "using different prompts, and b) using external data to augment"
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "the\ntraining set. Results\nshow the promising potential of\nthese"
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "methods."
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": ""
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "Index Terms—emotion\nrecognition,\nemotion\ndetection,\ndata"
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": ""
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "augmentation, affective computing, paraphrasing, NLP"
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": ""
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": ""
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "I.\nINTRODUCTION"
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": ""
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "Today’s society has been tremendously impacted by recent"
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "advances\nin machine\nlearning\nand Natural Language\nPro-"
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "cessing (NLP)\nin particular. There is great\ninterest\nto apply"
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "current state-of-the-art models to a myriad of data and tasks."
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "One of\nthe\nresearch areas\nthat has\nattracted interest\nis\nthe"
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "task of detecting emotions expressed in text. There are many"
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "applications\nfrom intelligent\nrecommender\nsystems\nto social"
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "media monitoring to mental health intervention. For existing"
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "work in this area,\nthe reader is referred to reviews such as [1]."
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "There\nare\nseveral\nchallenges\nrelated to NLP for\nemotion"
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "recognition. Researchers must\nhandle\ndatasets\nin\ndifferent"
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "formats (for example, a traditional essay or dialogue exchange"
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "or social media post). Emotions may be hard to comprehend"
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "by humans in the first place, so they are complicated for hu-"
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "man annotators as well. Different\ntaxonomies for categorizing"
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "emotions\nexist,\ne.g., Ekman [2] or Plutchik [3]. All\nthis\nis"
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "further\ncomplicated by the\nfact\nthat,\nthe\nsame paragraph or"
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "even social media post may express more than one emotion."
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "This shows the inherent difficulty in collecting and annotating"
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "large datasets for emotion recognition. Several datasets that do"
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "exist contain a small number of\nrecords:\nfor example, a few"
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "thousand in COVID-19 survey dataset\nfrom [4]. Furthermore,"
        },
        {
          "Department of Computing and Software Engineering, Florida Gulf Coast University, Fort Myers, Florida, USA": "these datasets are heavily dominated by one or two emotions,"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE I": ""
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "Train"
        },
        {
          "TABLE I": "274"
        },
        {
          "TABLE I": "–"
        },
        {
          "TABLE I": "536"
        },
        {
          "TABLE I": "106"
        },
        {
          "TABLE I": "1,427"
        },
        {
          "TABLE I": "2,313"
        },
        {
          "TABLE I": "–"
        },
        {
          "TABLE I": "291"
        },
        {
          "TABLE I": "165"
        },
        {
          "TABLE I": "5,112"
        },
        {
          "TABLE I": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Surprise\n–\n–": "Total records\n1,685\n723",
          "165\n70\n164\n40": "5,112\n2,191\n1,860\n525"
        },
        {
          "Surprise\n–\n–": "Avg. record length\n633",
          "165\n70\n164\n40": "138\n443"
        },
        {
          "Surprise\n–\n–": "dataset, GoEmotions\n[20],\nalongside\nexisting\ntraining\ndata",
          "165\n70\n164\n40": "EmoEvent-EN Data2 was presented in [17]. The dataset"
        },
        {
          "Surprise\n–\n–": "can improve\nclassification performance\nfor data we used in",
          "165\n70\n164\n40": "was\ncollected\nfrom the Twitter\nplatform based\non\nvarious"
        },
        {
          "Surprise\n–\n–": "this paper; however,\nthey did not perform any augmentation",
          "165\n70\n164\n40": "events that\ntook place in April 2019 (for example,\nthe Notre"
        },
        {
          "Surprise\n–\n–": "techniques on the original training records. Additionally, it was",
          "165\n70\n164\n40": "Dame Cathedral\nFire).\nIn\norder\nto\nselect\naffective\ntweets,"
        },
        {
          "Surprise\n–\n–": "shown in [16]\nthat classification on their GoEmotions-based",
          "165\n70\n164\n40": "the authors\nin [17] used Linguistic Inquiry and Word Count"
        },
        {
          "Surprise\n–\n–": "set\n[20] had relaively high f1-macro’s,\ntherefore we did not",
          "165\n70\n164\n40": "(LIWC)\n[22]. The\nselected\ntweets were\nthen\nannotated\nby"
        },
        {
          "Surprise\n–\n–": "try to augment and classify GoEmotions in our experimental",
          "165\n70\n164\n40": "Amazon MTurkers using one of seven emotions: six emotions"
        },
        {
          "Surprise\n–\n–": "study. Similarly for an emotion-labeled dataset used in [10].",
          "165\n70\n164\n40": "from Ekman’s taxonomy [2] plus “neutral or other emotions”."
        },
        {
          "Surprise\n–\n–": "The\norganization\nof\nthis\npaper\nis\nas\nfollows: Section\nII",
          "165\n70\n164\n40": "In addition, each record was labeled as offensive or not, which"
        },
        {
          "Surprise\n–\n–": "contains\na\ndescription\nof\nthe\ndatasets\nused\nin\nthis\npaper.",
          "165\n70\n164\n40": "we ignored for\nthis paper. The total\nrecords\nin the resulting"
        },
        {
          "Surprise\n–\n–": "Section III gives a background on data augmentation methods",
          "165\n70\n164\n40": "sets were 7,303 in English (there was also a dataset in Spanish,"
        },
        {
          "Surprise\n–\n–": "for NLP, focusing on methods we used for this work. Then,\nit",
          "165\n70\n164\n40": "which we leave for future research). The dataset on the online"
        },
        {
          "Surprise\n–\n–": "describes the overall process we used for augmenting our data",
          "165\n70\n164\n40": "repository had already replaced any hashtags, urls, or mentions"
        },
        {
          "Surprise\n–\n–": "and for\nclassification. Section IV presents our\nexperimental",
          "165\n70\n164\n40": "with ‘HASHTAG’, ‘URL’ and ‘USER’. We split\nthe data into"
        },
        {
          "Surprise\n–\n–": "setup,\nresults\nand observations. Finally, Section V includes",
          "165\n70\n164\n40": "train and test using a stratified split\n(70-30)."
        },
        {
          "Surprise\n–\n–": "concluding remarks and future research.",
          "165\n70\n164\n40": "WASSA-21 Data3 was part of the 11th Workshop on Com-"
        },
        {
          "Surprise\n–\n–": "",
          "165\n70\n164\n40": "putational Approaches\nto Subjectivity, Sentiment & Social"
        },
        {
          "Surprise\n–\n–": "II. DATASETS",
          "165\n70\n164\n40": ""
        },
        {
          "Surprise\n–\n–": "",
          "165\n70\n164\n40": "Media Analysis\n(WASSA 2021) Shared Task\non Empathy"
        },
        {
          "Surprise\n–\n–": "In this paper, we experimented with the datasets described",
          "165\n70\n164\n40": "Detection\nand Emotion Classification\nsummarized\nin\n[18]."
        },
        {
          "Surprise\n–\n–": "in the following. We focused on datasets that were small and",
          "165\n70\n164\n40": "This dataset\nis an extension of\n[23]’s dataset based on news"
        },
        {
          "Surprise\n–\n–": "imbalanced that were also presented in 2020 or\nlater,\nrather",
          "165\n70\n164\n40": "articles related to harm to an individual, group, nature, etc. The"
        },
        {
          "Surprise\n–\n–": "than earlier datasets such as SemEval-2018 Task 1 [21]. The",
          "165\n70\n164\n40": "dataset contains essays written to express the author’s empathy"
        },
        {
          "Surprise\n–\n–": "datasets have a variety of emotions and distributions, and come",
          "165\n70\n164\n40": "and distress\nin reaction to the news articles. The essays are"
        },
        {
          "Surprise\n–\n–": "from different sources such as social media or essays written",
          "165\n70\n164\n40": "annotated for empathy and distress, as well as personality traits"
        },
        {
          "Surprise\n–\n–": "by survey participants. All datasets are in English.",
          "165\n70\n164\n40": "and demographic information (age, gender, etc.). Each essay"
        },
        {
          "Surprise\n–\n–": "",
          "165\n70\n164\n40": "is also tagged with one of\nthe Ekman’s emotions [2]: Anger,"
        },
        {
          "Surprise\n–\n–": "COVID-19 Survey Data1 was presented in [4]. This data",
          "165\n70\n164\n40": ""
        },
        {
          "Surprise\n–\n–": "",
          "165\n70\n164\n40": "Disgust, Fear,\nJoy, Sadness,\nand Surprise. We only focused"
        },
        {
          "Surprise\n–\n–": "was\ncollected via\na\nsurvey in the UK under\nlockdown for",
          "165\n70\n164\n40": ""
        },
        {
          "Surprise\n–\n–": "",
          "165\n70\n164\n40": "on the\nemotion for\neach essay, not\nthe\nempathy or distress"
        },
        {
          "Surprise\n–\n–": "COVID-19 (in 2020). The participants in that survey wrote a",
          "165\n70\n164\n40": ""
        },
        {
          "Surprise\n–\n–": "",
          "165\n70\n164\n40": "labels,\nfollowing [16]. We did not perform train-test split\nfor"
        },
        {
          "Surprise\n–\n–": "paragraph of\ntext as well as entered demographic data (e.g.",
          "165\n70\n164\n40": ""
        },
        {
          "Surprise\n–\n–": "",
          "165\n70\n164\n40": "this data, as the WASSA-21 dataset already had distinct sets"
        },
        {
          "Surprise\n–\n–": "gender) and ratings for several emotions. Following [16], we",
          "165\n70\n164\n40": ""
        },
        {
          "Surprise\n–\n–": "",
          "165\n70\n164\n40": "for\ntraining (1,860 records) and testing (525 records)."
        },
        {
          "Surprise\n–\n–": "selected to use the “chosen emotion”: a category chosen by",
          "165\n70\n164\n40": ""
        },
        {
          "Surprise\n–\n–": "each participant out of\nseveral emotion options. As\nin [16],",
          "165\n70\n164\n40": "A comparison is\nshown in Table I:\nthis\nshows number of"
        },
        {
          "Surprise\n–\n–": "we kept\nthe emotion categories with at\nleast 4% of\nthe total",
          "165\n70\n164\n40": "records per\neach emotion,\nif\nthe\nemotion is present\nin that"
        },
        {
          "Surprise\n–\n–": "records, which resulted in a dataset of 2,408 records. This",
          "165\n70\n164\n40": "dataset. The counts are divided into train and test\nsets. The"
        },
        {
          "Surprise\n–\n–": "dataset does not follow Ekman [2] or other emotion taxonomy.",
          "165\n70\n164\n40": "last\ntwo rows of the Table display the total number of records"
        },
        {
          "Surprise\n–\n–": "The\nresulting\ndataset\ncontains Anger, Anxiety\n(dominant),",
          "165\n70\n164\n40": "in each set\nfollowed by the average length of\nthe records\nin"
        },
        {
          "Surprise\n–\n–": "Fear, Relaxation, and Sadness. For our experiments, we chose",
          "165\n70\n164\n40": "characters. As\nshown in Table I,\nthere is variety of emotion"
        },
        {
          "Surprise\n–\n–": "to use stratified split (70-30) to divide the records into a train",
          "165\n70\n164\n40": "emotion\ncategories\nas well\nas\ndistributions.\nFor\nexample,"
        },
        {
          "Surprise\n–\n–": "and a test set (we did this for all datasets to which this applied).",
          "165\n70\n164\n40": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "is not part of the Ekman taxonomy [2],\ntherefore not found in",
          "paraphrases. The authors fine-tuned BART on the paraphrase": "generation\ntask\nusing\nvarious\ndatasets. They\nalso\nutilized"
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "the rest of\nthe datasets. WASSA-21 is dominated by Anger",
          "paraphrases. The authors fine-tuned BART on the paraphrase": "Diverse Beam Search [28] and Back Translation [6]\nto help"
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "and\nSadness,\nemotions\nfound\nin\nall\ndatasets.\nIn\ncontrast,",
          "paraphrases. The authors fine-tuned BART on the paraphrase": "generate diverse outputs."
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "EmoEvent-EN is heavily dominated by Joy and Others (around",
          "paraphrases. The authors fine-tuned BART on the paraphrase": ""
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "",
          "paraphrases. The authors fine-tuned BART on the paraphrase": "B. Overall Process for Augmentation and Classification"
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "73% for\nthese two emotions) so it\nis more positive. As far as",
          "paraphrases. The authors fine-tuned BART on the paraphrase": ""
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "",
          "paraphrases. The authors fine-tuned BART on the paraphrase": "First,\nthe dataset was\nsplit\ninto training and testing sets\nif"
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "size,\ntwo datasets\n(COVID-19 and WASSA-21) have around",
          "paraphrases. The authors fine-tuned BART on the paraphrase": ""
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "",
          "paraphrases. The authors fine-tuned BART on the paraphrase": "needed (Section II has the details for each set). The training"
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "2.5K records, while EmoEvent-EN is more\nthan\n3\ntimes",
          "paraphrases. The authors fine-tuned BART on the paraphrase": ""
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "",
          "paraphrases. The authors fine-tuned BART on the paraphrase": "data was\nfed\nas\ninput\nto\neach\nof\nthe Data Augmentation"
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "larger (around 7K records). On the other hand,\nthe EmoEvent-",
          "paraphrases. The authors fine-tuned BART on the paraphrase": ""
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "",
          "paraphrases. The authors fine-tuned BART on the paraphrase": "techniques (described in the previous paragraphs). Using each"
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "EN dataset\ncontains\ntweets,\nso the\nlength of\neach record is",
          "paraphrases. The authors fine-tuned BART on the paraphrase": ""
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "",
          "paraphrases. The authors fine-tuned BART on the paraphrase": "specific augmentation method, we generated a new record x′"
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "shorter, while the other\ntwo datasets have essays (paragraphs",
          "paraphrases. The authors fine-tuned BART on the paraphrase": ""
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "",
          "paraphrases. The authors fine-tuned BART on the paraphrase": "for\neach original\ntraining record x. For\nthis new record x′,"
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "of sentences)\n- see average length in characters in Table I.",
          "paraphrases. The authors fine-tuned BART on the paraphrase": ""
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "",
          "paraphrases. The authors fine-tuned BART on the paraphrase": "we also made a label copy y′, where we copied the original"
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "III. METHODOLOGY",
          "paraphrases. The authors fine-tuned BART on the paraphrase": "training\nlabel\ny.\nIn\nall\nour\nexperiments, we\ngenerated five"
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "",
          "paraphrases. The authors fine-tuned BART on the paraphrase": "paraphrased records\nfor\neach original\nrecord. At\nthe\nend of"
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "A. Background: Data Augmentation Methods",
          "paraphrases. The authors fine-tuned BART on the paraphrase": ""
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "",
          "paraphrases. The authors fine-tuned BART on the paraphrase": "this augmentation phase, we had a new ‘augmented’\ntraining"
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "In\nthis\nsection, we\nprovide\nbackground\nfor\nall\nthe\ndata",
          "paraphrases. The authors fine-tuned BART on the paraphrase": "set with text and labels. We then concatenated the ‘augmented’"
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "augmentation methods we used in this work. We chose widely-",
          "paraphrases. The authors fine-tuned BART on the paraphrase": "training set\nto the original\ntraining set and we presented this"
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "used augmentation methods\nfor NLP based on simple word",
          "paraphrases. The authors fine-tuned BART on the paraphrase": "‘increased’ training dataset\nto the classifier during the training"
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "manipulations or more sophisticated based on the entire record.",
          "paraphrases. The authors fine-tuned BART on the paraphrase": "phase. This set of\nrecords then was transformed into features"
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "All\nthese methods have been shown to perform well\n(e.g. see",
          "paraphrases. The authors fine-tuned BART on the paraphrase": "for\ntraining\nthe\nclassification model. The\ntest\nset was\nalso"
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "[10])\ntherefore we chose to use them for our work.",
          "paraphrases. The authors fine-tuned BART on the paraphrase": "transformed into features\nand fed into the\ntrained classifier"
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "Easy Data Augmentation (EDA): This\nsimple technique",
          "paraphrases. The authors fine-tuned BART on the paraphrase": "in order\nto predict\nthe test\nlabels."
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "proposed in [5] has been shown to be quite successful. EDA",
          "paraphrases. The authors fine-tuned BART on the paraphrase": ""
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "",
          "paraphrases. The authors fine-tuned BART on the paraphrase": "IV. EXPERIMENTAL RESULTS"
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "combines\nseveral\noperations\nfor\nthe words\nin\nthe\noriginal",
          "paraphrases. The authors fine-tuned BART on the paraphrase": ""
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "",
          "paraphrases. The authors fine-tuned BART on the paraphrase": "A. Experimental Setup"
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "record. For any given sentence in the training records, EDA",
          "paraphrases. The authors fine-tuned BART on the paraphrase": ""
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "randomly chooses\nand performs one of\nthe\nfollowing (stop",
          "paraphrases. The authors fine-tuned BART on the paraphrase": "We run our code using Google colab4 (our code is at https://"
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "words are not considered, and there is a parameter for choosing",
          "paraphrases. The authors fine-tuned BART on the paraphrase": "github.com/A-Koufakou/AugEmotionDetection). For augmen-"
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "the percent of\nthe words to be altered):",
          "paraphrases. The authors fine-tuned BART on the paraphrase": "tation, we\nfollowed\ncode\nprovided\nby\n[10]\nonline5, which"
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "•\nSynonym Replacement:\nrandom words\nare\nreplaced by",
          "paraphrases. The authors fine-tuned BART on the paraphrase": "used an NLP augmentation library called NLPAug6. For any"
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "random synonyms from a dictionary e.g. WordNet\n[24];",
          "paraphrases. The authors fine-tuned BART on the paraphrase": "choices, we used the same as [10]:\nfor example,\nfor Embed-"
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "• Random Insertion: a random word from the given sen-",
          "paraphrases. The authors fine-tuned BART on the paraphrase": "dings, we used 200-dimensional Glove Embeddings\n[29] or"
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "tence is chosen, and one of its synonyms is then inserted",
          "paraphrases. The authors fine-tuned BART on the paraphrase": "BERT for Contextual Embeddings. For our classification, we"
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "in the sentence at a random position;",
          "paraphrases. The authors fine-tuned BART on the paraphrase": "used Simple Transformers7, a library built on Hugging Face8."
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "• Random Swap:\nrandomly chosen words are swapped;",
          "paraphrases. The authors fine-tuned BART on the paraphrase": "Since RoBERTa\n(Robustly\noptimized BERT\napproach),\na"
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "• Random Deletion:\nrandom words are removed.",
          "paraphrases. The authors fine-tuned BART on the paraphrase": "transformer-based model [30], was shown to outperform other"
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "",
          "paraphrases. The authors fine-tuned BART on the paraphrase": "deep learning models in [16] for two of our datasets (COVID-"
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "Embeddings: Rather\nthan using synonyms\nfrom a dictio-",
          "paraphrases. The authors fine-tuned BART on the paraphrase": ""
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "",
          "paraphrases. The authors fine-tuned BART on the paraphrase": "19 and WASSA-21), we chose to use the RoBERTa model,"
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "nary as\nin the previous\ntechnique, one\ncould employ word",
          "paraphrases. The authors fine-tuned BART on the paraphrase": ""
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "",
          "paraphrases. The authors fine-tuned BART on the paraphrase": "specifically, roberta-base. For every experiment, we fine-"
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "embeddings instead. These methods start by representing the",
          "paraphrases. The authors fine-tuned BART on the paraphrase": ""
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "",
          "paraphrases. The authors fine-tuned BART on the paraphrase": "tuned the model on our data, so that\nthe pre-trained model can"
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "words\nby\nembeddings,\nor\nvectors\nof n-dimensions. Then,",
          "paraphrases. The authors fine-tuned BART on the paraphrase": ""
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "",
          "paraphrases. The authors fine-tuned BART on the paraphrase": "further\nlearn from our data. We used 2 Epochs,\nlearning rate"
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "they replace or\ninsert words\nthat are found to be similar\nin",
          "paraphrases. The authors fine-tuned BART on the paraphrase": ""
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "",
          "paraphrases. The authors fine-tuned BART on the paraphrase": "of 1e−5, maxlen of 256 and batch of 8, based on early trials"
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "the word embedding space, e.g. by using cosine distance.\nIn",
          "paraphrases. The authors fine-tuned BART on the paraphrase": ""
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "",
          "paraphrases. The authors fine-tuned BART on the paraphrase": "and [16]. We repeated each experiment\n(train/test with each"
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "earlier work,\nfor example,\n[25],\nresearchers used static pre-",
          "paraphrases. The authors fine-tuned BART on the paraphrase": ""
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "",
          "paraphrases. The authors fine-tuned BART on the paraphrase": "set) 3 times and then reported the average performance on the"
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "trained word embeddings.\nIn order\nto take advantage of\nthe",
          "paraphrases. The authors fine-tuned BART on the paraphrase": ""
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "",
          "paraphrases. The authors fine-tuned BART on the paraphrase": "predicted test\nlabels. We reported results based on:"
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "contextual\nmore\nrecent\nadvances, we\ncan\nuse\nembeddings,",
          "paraphrases. The authors fine-tuned BART on the paraphrase": ""
        },
        {
          "COVID-19 is heavily skewed towards Anxiety (57%), which": "for\nexample\ngenerated\nby\na\npre-trained\ntransformer model",
          "paraphrases. The authors fine-tuned BART on the paraphrase": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "T P + T N": ""
        },
        {
          "T P + T N": ""
        },
        {
          "T P + T N": "N"
        },
        {
          "T P + T N": ""
        },
        {
          "T P + T N": "2 × P recision × Recall"
        },
        {
          "T P + T N": ""
        },
        {
          "T P + T N": ""
        },
        {
          "T P + T N": "P recision + Recall"
        },
        {
          "T P + T N": ""
        },
        {
          "T P + T N": ""
        },
        {
          "T P + T N": ""
        },
        {
          "T P + T N": "augmentation"
        },
        {
          "T P + T N": ""
        },
        {
          "T P + T N": ""
        },
        {
          "T P + T N": ""
        },
        {
          "T P + T N": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE II": "",
          "TABLE III": ""
        },
        {
          "TABLE II": "",
          "TABLE III": ""
        },
        {
          "TABLE II": "Acc",
          "TABLE III": "COVID-19\nEmoEvent-EN"
        },
        {
          "TABLE II": "61.55",
          "TABLE III": "66.30\n31.92"
        },
        {
          "TABLE II": "58.92",
          "TABLE III": "75.41\n40.35"
        },
        {
          "TABLE II": "59.11",
          "TABLE III": "21.83\n64.56"
        },
        {
          "TABLE II": "59.20",
          "TABLE III": "57.69\n76.20"
        },
        {
          "TABLE II": "58.23",
          "TABLE III": ""
        },
        {
          "TABLE II": "62.45",
          "TABLE III": ""
        },
        {
          "TABLE II": "60.78",
          "TABLE III": ""
        },
        {
          "TABLE II": "61.69",
          "TABLE III": "(Anger and Fear,\nin COVID-19)"
        },
        {
          "TABLE II": "62.04",
          "TABLE III": ""
        },
        {
          "TABLE II": "",
          "TABLE III": "emotion distributions. The EDA results"
        },
        {
          "TABLE II": "59.84",
          "TABLE III": ""
        },
        {
          "TABLE II": "",
          "TABLE III": ""
        },
        {
          "TABLE II": "59.11",
          "TABLE III": ""
        },
        {
          "TABLE II": "62.92",
          "TABLE III": ""
        },
        {
          "TABLE II": "61.65",
          "TABLE III": ""
        },
        {
          "TABLE II": "62.54",
          "TABLE III": ""
        },
        {
          "TABLE II": "",
          "TABLE III": "it does"
        },
        {
          "TABLE II": "60.63",
          "TABLE III": ""
        },
        {
          "TABLE II": "",
          "TABLE III": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "BERT Embed\n62.54\n54.44\n56.07\n54.27",
          "results further, see the confusion matrix for EmoEvent-EN in": ""
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "",
          "results further, see the confusion matrix for EmoEvent-EN in": "Fig. 1. We selected BERT Augmentation as\nit does\nthe best"
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "ProtAug\n60.63\n53.09\n54.74\n52.95",
          "results further, see the confusion matrix for EmoEvent-EN in": ""
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "",
          "results further, see the confusion matrix for EmoEvent-EN in": "for EmoEvent-EN (see Table II). From the confusion matrix,"
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "",
          "results further, see the confusion matrix for EmoEvent-EN in": "we can see that\nthe model did relatively well on predicting"
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "",
          "results further, see the confusion matrix for EmoEvent-EN in": "Joy\nand Others\n(dominant\nemotions). The model\nconfused"
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "where T P is True Positives, F P False Positives, F N False",
          "results further, see the confusion matrix for EmoEvent-EN in": ""
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "",
          "results further, see the confusion matrix for EmoEvent-EN in": "Anger, Disgust, Sadness, and Surprise many times with Others."
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "Negatives,\nand N total\nnumber\nof\nrecords. We\npresented",
          "results further, see the confusion matrix for EmoEvent-EN in": ""
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "",
          "results further, see the confusion matrix for EmoEvent-EN in": "These\nissues were observed in the original paper\n[17]:\nfor"
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "macro-averaged Precision, Recall\nand\nf1:\nfor\nexample,\nf1-",
          "results further, see the confusion matrix for EmoEvent-EN in": ""
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "",
          "results further, see the confusion matrix for EmoEvent-EN in": "example,\nthe annotators had trouble annotating Fear, Disgust"
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "macro averages the f1-score over all classes, and is well-suited",
          "results further, see the confusion matrix for EmoEvent-EN in": ""
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "",
          "results further, see the confusion matrix for EmoEvent-EN in": "and Surprise, or\nto distinguish between Anger\nand Disgust"
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "to imbalanced class distributions [10],\n[16].",
          "results further, see the confusion matrix for EmoEvent-EN in": ""
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "",
          "results further, see the confusion matrix for EmoEvent-EN in": "(complementary emotions)."
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "B. Results",
          "results further, see the confusion matrix for EmoEvent-EN in": ""
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "",
          "results further, see the confusion matrix for EmoEvent-EN in": "In order\nto examine the strength of\nthe data augmentation,"
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "Our\nclassification\nresults with\nthe\noriginal\nand\nthe\naug-",
          "results further, see the confusion matrix for EmoEvent-EN in": "we used the BLEU (Bilingual Evaluation Understudy) metric"
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "mented train datasets are shown in Table I. Each of the values",
          "results further, see the confusion matrix for EmoEvent-EN in": "[31]\nas\nper\nprevious work\n[10],\n[11]. BLEU is\nused\nfor"
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "in the Table is an average of 3 runs. The standard deviation of",
          "results further, see the confusion matrix for EmoEvent-EN in": "translation problems: lower BLEU score means more diversity"
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "the results ranges from 0.01 to 0.04 depending on the dataset.",
          "results further, see the confusion matrix for EmoEvent-EN in": "from the original\ntext,\ntherefore a stronger augmentation. The"
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "There was no single winner\naugmentation technique; we",
          "results further, see the confusion matrix for EmoEvent-EN in": "BLEU scores\nfor\nthe\naugmented\ndatasets\nin\nthis work\nare"
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "observed\nthat\nthe EDA method\ndid\nthe\nbest\nfor\ndata\nthat",
          "results further, see the confusion matrix for EmoEvent-EN in": "shown in Table III. We calculated BLEU score without brevity"
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "contains essays written by authors given a specific question",
          "results further, see the confusion matrix for EmoEvent-EN in": "punishment\nto\navoid\ngiving\nshorter\ntexts\nan\nadvantage\nper"
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "or\ntopic (COVID-19 and WASSA-21), while the embeddings",
          "results further, see the confusion matrix for EmoEvent-EN in": "[10]. As\nshown\nin Table\nIII,\nthe\ntop methods\n(EDA and"
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "methods and especially contextual embeddings were the top",
          "results further, see the confusion matrix for EmoEvent-EN in": "BERT) had the lowest BLEU for EmoEvent and WASSA-21"
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "performing methods for tweet-based data (EmoEvent). In more",
          "results further, see the confusion matrix for EmoEvent-EN in": "respectively, meaning high lexical diversity could have helped"
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "detail,\nin Table II for COVID-19, EDA had the best f1-macro",
          "results further, see the confusion matrix for EmoEvent-EN in": "these methods win. The situation is not\nthe same for COVID-"
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "(45.51%),\nimproving\nthe\noriginal\nf1-macro\nby\nabout\n14%.",
          "results further, see the confusion matrix for EmoEvent-EN in": "19: ProtAugmenter, not EDA, had the lowest score."
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "The\nrest of\nthe methods\nfollowed around 44%.\nIn Table\nII",
          "results further, see the confusion matrix for EmoEvent-EN in": "Finally,\nregarding runtime and resource expectations of\nthe"
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "for WASSA-21, we see that EDA is once again the winner",
          "results further, see the confusion matrix for EmoEvent-EN in": "augmentation techniques, we observed that EDA had low run-"
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "(54.79% f1-macro) improving on the non-augmented f1-macro",
          "results further, see the confusion matrix for EmoEvent-EN in": "time and no GPU requirement in contrast to the other methods."
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "by almost 17%. The other methods followed closely behind.9",
          "results further, see the confusion matrix for EmoEvent-EN in": "Depending on the data, we observed 1-3 minutes of\nruntime"
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "In contrast, Table II shows that\nthe BERT Embeddings Aug-",
          "results further, see the confusion matrix for EmoEvent-EN in": ""
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "mentation did the best for EmoEvent-EN (49.65%) improving",
          "results further, see the confusion matrix for EmoEvent-EN in": ""
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "on the non-augmented f1-macro by almost 12%. The\nstatic",
          "results further, see the confusion matrix for EmoEvent-EN in": ""
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "(Glove) Embedding method followed closely, with the rest of",
          "results further, see the confusion matrix for EmoEvent-EN in": ""
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "the augmenters in 1 or 2% lower.",
          "results further, see the confusion matrix for EmoEvent-EN in": ""
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "In Table II, augmented-based f1-macro’s were always higher",
          "results further, see the confusion matrix for EmoEvent-EN in": ""
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "than non-augmented, however the same was not\ntrue for accu-",
          "results further, see the confusion matrix for EmoEvent-EN in": ""
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "racy:\nfor COVID-19 and EmoEvent-EN, accuracy was higher",
          "results further, see the confusion matrix for EmoEvent-EN in": ""
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "for\nnon-augmented\ntraining\nthan\naugmented. We\nexamined",
          "results further, see the confusion matrix for EmoEvent-EN in": ""
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "the predictions per\nclass,\nand when the model was\ntrained",
          "results further, see the confusion matrix for EmoEvent-EN in": ""
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "with only original\ntraining,\nit did very well on the dominant",
          "results further, see the confusion matrix for EmoEvent-EN in": ""
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "class\n(Anxiety,\nfor\nexample\nin COVID-19) but much worse",
          "results further, see the confusion matrix for EmoEvent-EN in": ""
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "9Note:\nthe winning team on the related WASSA-21 task [19] achieved f1-",
          "results further, see the confusion matrix for EmoEvent-EN in": ""
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "macros in mid/upper 50’s, with ensembles of several pre-trained transformers",
          "results further, see the confusion matrix for EmoEvent-EN in": ""
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "each fine-tuned on WASSA-21, and augmentation with GoEmotions [20] (see",
          "results further, see the confusion matrix for EmoEvent-EN in": ""
        },
        {
          "Embed\n61.65\n53.50\n54.95\n53.05": "later sections). Exploring ensembles is left\nfor\nfuture research.",
          "results further, see the confusion matrix for EmoEvent-EN in": "Fig. 1.\nExample Confusion Matrix (EmoEvent-EN, BERT Augmentation)"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE IV": "EXAMPLE OF ORIGINAL RECORDS FROM COVID-19 AND THEIR PARAPHRASINGS"
        },
        {
          "TABLE IV": "Text"
        },
        {
          "TABLE IV": "I am fairly safe in a secluded location, however\nI am very worried about\nthe safety of my family and friends that"
        },
        {
          "TABLE IV": "I am fairly safe in a sequestered location, however\nI be rattling worried about"
        },
        {
          "TABLE IV": "I am grateful\nto be in a safe location, but my heart goes out\nto my loved ones who are suffering in cities"
        },
        {
          "TABLE IV": "I am unsure of what\nfuture holds worried I won’t see family members again and do things I enjoyed doing prior"
        },
        {
          "TABLE IV": "I am unsure of what holds future disquieted won’t see family members over again and do things I doing prior"
        },
        {
          "TABLE IV": "I am worried about what\nthe future holds.\nI am worried I will not be able to see my family members again"
        },
        {
          "TABLE IV": "or do the things I enjoyed doing prior\nto the virus"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "chatGPT\nI am worried about what\nthe future holds.",
          "to virus": "I am worried I will not be able to see my family members again"
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "or do the things I enjoyed doing prior\nto the virus",
          "to virus": ""
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "for EDA (no GPU) versus 28-35 minutes for ProtAugmenter",
          "to virus": "results\non\nthe\nother\nhand\ninserted\nrandom words,\nsuch\nas"
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "and 2-3 hours\nfor Embedding Augmentation methods\n(static",
          "to virus": "‘rattling’, replaced ‘friends’ with ‘champion’, and swapped the"
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "embeddings were slightly faster\nthan contextual), using GPU.",
          "to virus": "order of the words in the ‘future holds’. Nevertheless, EDA is"
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "",
          "to virus": "a simple method non-requiring GPU or API calls with a very"
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "C. Case Study A: Paraphrasing with chatGPT",
          "to virus": ""
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "",
          "to virus": "low runtime comparably."
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "Motivated by the improvements achieved by the data aug-",
          "to virus": ""
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "",
          "to virus": "D. Case Study B: Augmentation using external data"
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "mentation using existing techniques\nalready explored in the",
          "to virus": ""
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "",
          "to virus": "Besides augmentation techniques that reword or paraphrase"
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "literature\n(e.g.\n[10],\n[11]), we were\ninterested\nto\ndirectly",
          "to virus": ""
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "",
          "to virus": "the\noriginal\ndata,\nrecent work\nhas\nproposed\naugmenting"
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "experiment with the popular chatGPT (Chat Generative Pre-",
          "to virus": ""
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "",
          "to virus": "with\nan\nexternal\ndataset,\nperhaps\nfrom a\ndifferent\ndomain"
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "Trained Transformer). This is a Large Language Model-based",
          "to virus": ""
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "",
          "to virus": "or\nsource. For\nexample,\nboth\n[19]\nand\n[16]\nused GoEmo-"
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "chatbot developed by OpenAI, which made headlines having",
          "to virus": ""
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "",
          "to virus": "tions\n[20]\n(human annotated reddit\ncomments):\nthe first\nfor"
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "reached 1 million users in a few days. We were only able to",
          "to virus": ""
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "",
          "to virus": "augmenting WASSA-21,\nand\nthe\nsecond\nfor\nboth COVID-"
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "conduct a case study with COVID-19, due to lack of time and",
          "to virus": ""
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "",
          "to virus": "19 and WASSA-21. As both papers\nshowed improvements,"
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "space. We also saw work just presented in WASSA-23 [32]:",
          "to virus": ""
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "",
          "to virus": "we wanted to compare our\nresults against augmenting these"
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "the HIT-SCIR team competed in WASSA-23 shared tasks and",
          "to virus": ""
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "",
          "to virus": "data with GoEmotions. First,\nto match our data\nformat, we"
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "also used chatGPT paraphrasing for augmenting the WASSA-",
          "to virus": ""
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "",
          "to virus": "selected a subset of the GoEmotions train set based on records"
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "23 train sets (using RoBERTa and bi-LSTM for their emotion",
          "to virus": ""
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "",
          "to virus": "annotated with a\nsingle Ekman label\n(about 4,300 records)."
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "prediction model). Further exploring their\nresearch as well as",
          "to virus": ""
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "",
          "to virus": "For\neach experiment, we only kept\nthe\nemotions\nthat were"
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "expanding our chatGPT work to more datasets\nis a focus of",
          "to virus": ""
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "",
          "to virus": "present in the original dataset. Finally, similar to Section III-B,"
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "our\nfuture research.",
          "to virus": ""
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "",
          "to virus": "we\nconcatenated\nthat\ndata with\nthe\noriginal\ntrain\nset,\nand"
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "We wrote a script\nto connect\nto the OpenAI chat API10 and",
          "to virus": ""
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "",
          "to virus": "proceeded as before. The results were favorable for WASSA-"
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "explored various prompts\nfor paraphrasing training records.",
          "to virus": ""
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "",
          "to virus": "21:\nthe model\ntrained with the GoEmotions-augmented data"
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "We used a single prompt-response cycle for each record. The",
          "to virus": ""
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "",
          "to virus": "resulted\nin\n55.94% f1-macro\n(surpassing EDA’s\n54.79% in"
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "prompts with which we\nexperimented that\nshowed promise",
          "to virus": ""
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "",
          "to virus": "Table\nII). However,\nfor\nthe COVID-19 data,\nf1-macro was"
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "were ‘summarize the following in the first person’\n(denoted",
          "to virus": ""
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "",
          "to virus": "35.74% (much\nlower\nthan\n45.51% for EDA in Table\nII)."
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "as chatGPT-summ) and ‘using a sympathetic tone, paraphrase",
          "to virus": ""
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "",
          "to virus": "Our\ninterpretation\nis\nthat WASSA-21’s\nemotions\nare more"
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "the following’ (denoted as chatGPT-symp). In summary, para-",
          "to virus": ""
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "",
          "to virus": "representative of\nthe Ekman taxonomy [2],\ntherefore better"
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "phrasing using a ‘summarize’ prompt\nin chatGPT (chatGPT-",
          "to virus": ""
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "",
          "to virus": "suited\nto\nthe\nadditional\nrecords\nfrom GoEmotions. On\nthe"
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "summ) had the highest\nf1-macro (46.10%)\nfollowed closely",
          "to virus": ""
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "",
          "to virus": "other hand, COVID-19 is heavily influenced by Anxiety, and"
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "by EDA (45.51%). The\n‘sympathize’\nprompt\nin\nchatGPT",
          "to virus": ""
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "",
          "to virus": "the extra records we added from GoEmotions for Anger, Fear"
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "(chatGPT-symp)\nhad\na\nlower\nf1-macro\n(44.72%). We\ntried",
          "to virus": ""
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "",
          "to virus": "and Sadness\nled to negligible improvement\nin the prediction"
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "another model we saw in the literature: a T5 model\ntrained",
          "to virus": ""
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "",
          "to virus": "of\nthese emotions. Nevertheless, we feel\nthis\ntopic warrants"
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "on ChatGPT paraphrase data11, with lower\nf1-macro results.",
          "to virus": ""
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "",
          "to virus": "more in-depth exploration in our\nfuture work."
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "Finally, we\nlooked\nat\nexample\nparaphrases\ndisplayed\nin",
          "to virus": ""
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "Table IV. We only show results for EDA and chatGPT due to",
          "to virus": ""
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "",
          "to virus": "V. CONCLUSIONS"
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "space. In Table IV, the paraphrases from chatGPT have a better",
          "to virus": ""
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "",
          "to virus": "Detecting\nemotions\nsuch\nas\njoy\nor\nsadness\nin\ntext\nis\na"
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "structure and vary from the original in words and phrasing. For",
          "to virus": ""
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "",
          "to virus": "significant\narea\nof\nresearch with many\nchallenges. An\nim-"
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "example,\nit paraphrased ‘fairly safe in a secluded location’\nto",
          "to virus": ""
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "",
          "to virus": "portant\nchallenge\nis\nthe\navailability of\nlarge\nannotated data."
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "‘grateful to be in a safe location’. This is also supported by the",
          "to virus": ""
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "",
          "to virus": "In this work, we experimented with several data augmentation"
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "BLEU score for chatGPT paraphrases: it is much lower (25.04)",
          "to virus": ""
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "",
          "to virus": "techniques that have been successfully utilized in the literature,"
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "than the values shown for COVID-19 in Table III. This shows",
          "to virus": ""
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "",
          "to virus": "to explore potential\nimprovements\nin emotion detection. We"
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "that\nchatGPT paraphrased data were more\nlexically diverse",
          "to virus": ""
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "",
          "to virus": "experimented with various emotion-labeled datasets, originat-"
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "than the original data compared to the other augmenters. EDA",
          "to virus": ""
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "",
          "to virus": "ing\nfrom different\nsources\n(e.g.\nsocial media\nposts\nversus"
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "10https://platform.openai.com/docs/api-reference/chat",
          "to virus": "essays)\nand\nvarying\nin\ntopics,\nemotions,\ndistributions\netc."
        },
        {
          "EDA\nI am unsure of what holds future disquieted won’t see family members over again and do things I doing prior": "11https://huggingface.co/humarin/chatgpt paraphraser on T5\nbase",
          "to virus": "Our\nexperiments\nshowed that using the\naugmented datasets"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "2023\nshared\ntask\non\nempathy,\nemotion\nand\npersonality\ndetection\nin"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "method in each dataset\nimproved the\nf1-macro by 12-16%.",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "the 13th\nconversation and reactions to news articles,” in Proceedings of"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "A simple technique, EDA, performed the best on essay data,",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "Workshop on Computational Approaches\nto Subjectivity, Sentiment, &"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "such as WASSA-21, while contextualized embeddings did the",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "Social Media Analysis.\nToronto, Canada: ACL, Jul. 2023, pp. 511–525."
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "[16] A. Koufakou, J. Garciga, A. Paul, J. Morelli, and C. Frank, “Automat-"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "best for tweet-based data. Additionally, we performed two case",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "ically classifying emotions based on text: A comparative\nexploration"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "studies:\na) using chatGPT,\na very popular pre-trained large",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "of different datasets,” in 34th International Conference on Tools with"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "language model,\nto paraphrase the training set with different",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "Artificial\nIntelligence (ICTAI).\nIEEE, 2022."
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "F. Plaza-del-Arco, C. Strapparava, L. A. Urena-Lopez, and M. T. Martin-"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "prompts, and b) using external data (GoEmotions) to augment",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "Valdivia, “EmoEvent: A Multilingual Emotion Corpus based on different"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "the training set. The experiments show the promising potential",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "Events,” in Proceedings of the 12th Language Resources and Evaluation"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "of\nthese techniques. Finally, we examined the strength of\nthe",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "Conference, May 2020."
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "S. Tafreshi, O. De Clercq, V. Barriere,\nS. Buechel,\nJ.\nSedoc,\nand"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "resulting augmented text\n(lexical diversity vs original, using",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "A. Balahur, “Wassa 2021 shared task: Predicting empathy and emotion"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "BLEU score) as well as the execution runtime. Paraphrasing",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "the 11th Workshop on\nin reaction to news\nstories,” in Proceedings of"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "with chatGPT,\nfor example, was much more lexically diverse",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "Computational Approaches to Subjectivity, Sentiment and Social Media"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "Analysis.\nOnline: ACL, 2021."
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "than simpler\ntechniques\n(EDA),\nat\nthe\ncost of much higher",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "J. Mundra, R. Gupta, and S. Mukherjee, “WASSA@IITK at WASSA"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "runtime and complexity. For future work, we intend to explore",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "2021: Multi-task learning and transformer finetuning for emotion classi-"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "the\nimpact\nof\nchanging\ncertain\n(hyper)parameters\nas well",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "the 11th Workshop\nfication and empathy prediction,” in Proceedings of"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "on Computational Approaches\nto\nSubjectivity,\nSentiment\nand\nSocial"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "as\nexperiment with more data\n(e.g.\nthe\nlatest WASSA sets",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "Media Analysis.\nOnline: ACL, Apr. 2021, pp. 112–116."
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "presented in 2023 [15]), augmentation methods and ensembles.",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "[20] D. Demszky, D. Movshovitz-Attias, J. Ko, A. Cowen, G. Nemade, and"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "We\nalso aim to methodically study ‘prompt\nengineering’\nin",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "S. Ravi, “Goemotions: A dataset of fine-grained emotions,” in Proceed-"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "ings of\nthe 58th Annual Meeting of\nthe Association for Computational"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "chatGPT,\nas\nit\nhas\nbeen\nshown\n[33]\nthat\na well-designed",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "Linguistics, 2020, pp. 4040–4054."
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "prompt may vastly improve its results.",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "S. Mohammad, F. Bravo-Marquez, M. Salameh,\nand S. Kiritchenko,"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "“SemEval-2018 task 1: Affect\nin tweets,” in Proceedings of The 12th"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "REFERENCES",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "International Workshop on Semantic Evaluation, Jun. 2018."
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "J. W. Pennebaker, M. E. Francis, and R.\nJ. Booth, “Linguistic inquiry"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "[1]\nP. Nandwani and R. Verma, “A review on sentiment analysis and emotion",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "and word count: Liwc 2001,” Mahway: Lawrence Erlbaum Associates,"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "detection from text,” Social Network Analysis and Mining, vol. 11, no. 1,",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "vol. 71, 2001."
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "pp. 1–19, 2021.",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "S. Buechel, A. Buffone, B. Slaff, L. Ungar, and J. Sedoc, “Modeling"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "[2]\nP. Ekman,\n“An argument\nfor basic\nemotions,” Cognition & emotion,",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "Proceedings\nempathy\nand\ndistress\nin\nreaction\nto\nnews\nstories,”\nin"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "vol. 6, no. 3-4, pp. 169–200, 1992.",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "of\nthe 2018 Conference on Empirical Methods\nin Natural Language"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "[3] R.\nPlutchik,\n“Emotions: A general\npsychoevolutionary\ntheory,” Ap-",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "Processing, Brussels, Belgium, Oct.-Nov. 2018, pp. 4758–4765."
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "proaches to emotion, vol. 1984, no. 197-219, pp. 2–4, 1984.",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "[24] G. A. Miller, “Wordnet: a lexical database for english,” Communications"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "[4] B. Kleinberg, I. van der Vegt, and M. Mozes, “Measuring emotions in the",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "of\nthe ACM, vol. 38, no. 11, pp. 39–41, 1995."
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "the 1st Workshop\ncovid-19 real world worry dataset,” in Proceedings of",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "and D. Yang,\n“That’s\nso\nannoying!!!: A lexical\nand"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "on NLP for COVID-19 at ACL 2020, 2020.",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "frame-semantic\nembedding based data\naugmentation approach to au-"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "[5]\nJ. Wei\nand K. Zou,\n“Eda: Easy\ndata\naugmentation\ntechniques\nfor",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "tomatic categorization of annoying behaviors using# petpeeve tweets,”"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "the\nboosting performance on text classification tasks,” in Proceedings of",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "the 2015 conference on empirical methods in natural\nin Proceedings of"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "2019 Conference on Empirical Methods in Natural Language Processing",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "language processing, 2015, pp. 2557–2563."
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "(EMNLP-IJCNLP), 2019.",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "J. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“BERT:\nPre-"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "[6]\nJ. Mallinson, R. Sennrich, and M. Lapata, “Paraphrasing revisited with",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "training of deep bidirectional\ntransformers for language understanding,”"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "the 15th Conference of\nneural machine translation,” in Proceedings of",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "in Conference of\nthe NACL: Human Language Technologies, Jun. 2019."
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "the European Chapter of\nthe Association for Computational Linguistics,",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "[27] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy,"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "Valencia, Spain, Apr. 2017, pp. 881–893.",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "V\n.\nStoyanov,\nand\nL.\nZettlemoyer,\n“BART: Denoising\nsequence-to-"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "[7]\nT. Dopierre, C. Gravier,\nand W. Logerais,\n“PROTAUGMENT: Un-",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "sequence pre-training for natural\nlanguage generation,\ntranslation, and"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "supervised diverse\nshort-texts paraphrasing for\nintent detection meta-",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "of\nthe\n58th Annual Meeting\nof\nthe\ncomprehension,”\nin Proceedings"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "the 59th Annual Meeting of\nthe Association\nlearning,” in Proceedings of",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "Association for Computational Linguistics, Jul. 2020, pp. 7871–7880."
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "for Computational Linguistics, Aug. 2021, pp. 2454–2466.",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "[28] A. Vijayakumar, M. Cogswell, R. Selvaraju, Q. Sun, S. Lee, D. Cran-"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "[8]\nS. Y. Feng, V. Gangal, J. Wei, S. Chandar, S. Vosoughi, T. Mitamura,",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "dall, and D. Batra, “Diverse beam search for\nimproved description of"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "and E. Hovy, “A survey of data augmentation approaches\nfor nlp,” in",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "the AAAI Conference on Artificial\ncomplex scenes,” in Proceedings of"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "Findings of\nthe Association for Computational Linguistics, 2021.",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "Intelligence, vol. 32, no. 1, 2018."
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "[9] M. Bayer, M. Kaufhold, and C. Reuter, “A survey on data augmentation",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "J. Pennington, R. Socher, and C. D. Manning, “Glove: Global vectors"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "for\ntext classification,” ACM Comput. Surv., vol. 55, no. 7, pp. 1–39,",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "of\nthe\n2014\nconference\non\nfor word\nrepresentation,”\nin Proceedings"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "2022.",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "empirical methods in natural\nlanguage processing (EMNLP), 2014."
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "[10]\nL. Pellicer, T. Ferreira, and A. Costa, “Data augmentation techniques in",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "[30] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "natural\nlanguage processing,” Applied Soft Computing, vol. 132, 2023.",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "L. Zettlemoyer, and V. Stoyanov, “Roberta: A robustly optimized bert"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "[11] G. Ansari, M. Garg,\nand C. Saxena,\n“Data\naugmentation for mental",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "pretraining approach,” arXiv preprint arXiv:1907.11692, 2019."
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "the 18th Inter-\nhealth classification on social media,” in Proceedings of",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "[31] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "national Conference on Natural Language Processing (ICON), 2021.",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "the 40th\nautomatic evaluation of machine translation,” in Proceedings of"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "[12] M. M.\nImran, Y.\nJain, P. Chatterjee,\nand K. Damevski,\n“Data\naug-",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "Annual Meeting of\nthe Association for Computational Linguistics, 2002."
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "mentation for\nimproving emotion recognition in software\nengineering",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "[32] X. Lu, Z. Li, Y. Tong, Y. Zhao, and B. Qin, “HIT-SCIR at WASSA 2023:"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "the 37th IEEE/ACM International\ncommunication,” in Proceedings of",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "Empathy and emotion analysis at the utterance-level and the essay-level,”"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "Conference on Automated Software Engineering, 2023.",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "the 13th Workshop on Computational Approaches to\nin Proceedings of"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "[13]\nS. Longpre, Y. Wang, and C. DuBois, “How effective is\ntask-agnostic",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "Subjectivity, Sentiment, & Social Media Analysis.\nToronto, Canada:"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "of\nthe\ndata\naugmentation\nfor\npretrained\ntransformers?”\nin Findings",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "ACL, Jul. 2023, pp. 574–580."
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "Association for Computational Linguistics: EMNLP 2020, 2020.",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "P. Liu, W. Yuan,\nJ. Fu, Z.\nJiang, H. Hayashi,\nand G. Neubig,\n“Pre-"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "[14]\nI. Okimura, M. Reid, M. Kawano,\nand Y. Matsuo,\n“On\nthe\nimpact",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "train, prompt, and predict: A systematic survey of prompting methods"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "of data augmentation on downstream performance in natural\nlanguage",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "2023\nshared\ntask\non\nempathy,\nemotion\nand\npersonality\ndetection\nin"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "method in each dataset\nimproved the\nf1-macro by 12-16%.",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "the 13th\nconversation and reactions to news articles,” in Proceedings of"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "A simple technique, EDA, performed the best on essay data,",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "Workshop on Computational Approaches\nto Subjectivity, Sentiment, &"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "such as WASSA-21, while contextualized embeddings did the",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "Social Media Analysis.\nToronto, Canada: ACL, Jul. 2023, pp. 511–525."
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "[16] A. Koufakou, J. Garciga, A. Paul, J. Morelli, and C. Frank, “Automat-"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "best for tweet-based data. Additionally, we performed two case",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "ically classifying emotions based on text: A comparative\nexploration"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "studies:\na) using chatGPT,\na very popular pre-trained large",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "of different datasets,” in 34th International Conference on Tools with"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "language model,\nto paraphrase the training set with different",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "Artificial\nIntelligence (ICTAI).\nIEEE, 2022."
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "F. Plaza-del-Arco, C. Strapparava, L. A. Urena-Lopez, and M. T. Martin-"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "prompts, and b) using external data (GoEmotions) to augment",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "Valdivia, “EmoEvent: A Multilingual Emotion Corpus based on different"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "the training set. The experiments show the promising potential",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "Events,” in Proceedings of the 12th Language Resources and Evaluation"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "of\nthese techniques. Finally, we examined the strength of\nthe",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "Conference, May 2020."
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "S. Tafreshi, O. De Clercq, V. Barriere,\nS. Buechel,\nJ.\nSedoc,\nand"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "resulting augmented text\n(lexical diversity vs original, using",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "A. Balahur, “Wassa 2021 shared task: Predicting empathy and emotion"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "BLEU score) as well as the execution runtime. Paraphrasing",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "the 11th Workshop on\nin reaction to news\nstories,” in Proceedings of"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "with chatGPT,\nfor example, was much more lexically diverse",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "Computational Approaches to Subjectivity, Sentiment and Social Media"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "Analysis.\nOnline: ACL, 2021."
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "than simpler\ntechniques\n(EDA),\nat\nthe\ncost of much higher",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "J. Mundra, R. Gupta, and S. Mukherjee, “WASSA@IITK at WASSA"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "runtime and complexity. For future work, we intend to explore",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "2021: Multi-task learning and transformer finetuning for emotion classi-"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "the\nimpact\nof\nchanging\ncertain\n(hyper)parameters\nas well",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "the 11th Workshop\nfication and empathy prediction,” in Proceedings of"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "on Computational Approaches\nto\nSubjectivity,\nSentiment\nand\nSocial"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "as\nexperiment with more data\n(e.g.\nthe\nlatest WASSA sets",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "Media Analysis.\nOnline: ACL, Apr. 2021, pp. 112–116."
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "presented in 2023 [15]), augmentation methods and ensembles.",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "[20] D. Demszky, D. Movshovitz-Attias, J. Ko, A. Cowen, G. Nemade, and"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "We\nalso aim to methodically study ‘prompt\nengineering’\nin",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "S. Ravi, “Goemotions: A dataset of fine-grained emotions,” in Proceed-"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "ings of\nthe 58th Annual Meeting of\nthe Association for Computational"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "chatGPT,\nas\nit\nhas\nbeen\nshown\n[33]\nthat\na well-designed",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "Linguistics, 2020, pp. 4040–4054."
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "prompt may vastly improve its results.",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "S. Mohammad, F. Bravo-Marquez, M. Salameh,\nand S. Kiritchenko,"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "“SemEval-2018 task 1: Affect\nin tweets,” in Proceedings of The 12th"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "REFERENCES",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "International Workshop on Semantic Evaluation, Jun. 2018."
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "J. W. Pennebaker, M. E. Francis, and R.\nJ. Booth, “Linguistic inquiry"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "[1]\nP. Nandwani and R. Verma, “A review on sentiment analysis and emotion",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "and word count: Liwc 2001,” Mahway: Lawrence Erlbaum Associates,"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "detection from text,” Social Network Analysis and Mining, vol. 11, no. 1,",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "vol. 71, 2001."
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "pp. 1–19, 2021.",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "S. Buechel, A. Buffone, B. Slaff, L. Ungar, and J. Sedoc, “Modeling"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "[2]\nP. Ekman,\n“An argument\nfor basic\nemotions,” Cognition & emotion,",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "Proceedings\nempathy\nand\ndistress\nin\nreaction\nto\nnews\nstories,”\nin"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "vol. 6, no. 3-4, pp. 169–200, 1992.",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "of\nthe 2018 Conference on Empirical Methods\nin Natural Language"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "[3] R.\nPlutchik,\n“Emotions: A general\npsychoevolutionary\ntheory,” Ap-",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "Processing, Brussels, Belgium, Oct.-Nov. 2018, pp. 4758–4765."
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "proaches to emotion, vol. 1984, no. 197-219, pp. 2–4, 1984.",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "[24] G. A. Miller, “Wordnet: a lexical database for english,” Communications"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "[4] B. Kleinberg, I. van der Vegt, and M. Mozes, “Measuring emotions in the",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "of\nthe ACM, vol. 38, no. 11, pp. 39–41, 1995."
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "the 1st Workshop\ncovid-19 real world worry dataset,” in Proceedings of",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "and D. Yang,\n“That’s\nso\nannoying!!!: A lexical\nand"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "on NLP for COVID-19 at ACL 2020, 2020.",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "frame-semantic\nembedding based data\naugmentation approach to au-"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "[5]\nJ. Wei\nand K. Zou,\n“Eda: Easy\ndata\naugmentation\ntechniques\nfor",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "tomatic categorization of annoying behaviors using# petpeeve tweets,”"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "the\nboosting performance on text classification tasks,” in Proceedings of",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "the 2015 conference on empirical methods in natural\nin Proceedings of"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "2019 Conference on Empirical Methods in Natural Language Processing",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "language processing, 2015, pp. 2557–2563."
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "(EMNLP-IJCNLP), 2019.",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "J. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“BERT:\nPre-"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "[6]\nJ. Mallinson, R. Sennrich, and M. Lapata, “Paraphrasing revisited with",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "training of deep bidirectional\ntransformers for language understanding,”"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "the 15th Conference of\nneural machine translation,” in Proceedings of",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "in Conference of\nthe NACL: Human Language Technologies, Jun. 2019."
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "the European Chapter of\nthe Association for Computational Linguistics,",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "[27] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy,"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "Valencia, Spain, Apr. 2017, pp. 881–893.",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "V\n.\nStoyanov,\nand\nL.\nZettlemoyer,\n“BART: Denoising\nsequence-to-"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "[7]\nT. Dopierre, C. Gravier,\nand W. Logerais,\n“PROTAUGMENT: Un-",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "sequence pre-training for natural\nlanguage generation,\ntranslation, and"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "supervised diverse\nshort-texts paraphrasing for\nintent detection meta-",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "of\nthe\n58th Annual Meeting\nof\nthe\ncomprehension,”\nin Proceedings"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "the 59th Annual Meeting of\nthe Association\nlearning,” in Proceedings of",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "Association for Computational Linguistics, Jul. 2020, pp. 7871–7880."
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "for Computational Linguistics, Aug. 2021, pp. 2454–2466.",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "[28] A. Vijayakumar, M. Cogswell, R. Selvaraju, Q. Sun, S. Lee, D. Cran-"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "[8]\nS. Y. Feng, V. Gangal, J. Wei, S. Chandar, S. Vosoughi, T. Mitamura,",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "dall, and D. Batra, “Diverse beam search for\nimproved description of"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "and E. Hovy, “A survey of data augmentation approaches\nfor nlp,” in",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "the AAAI Conference on Artificial\ncomplex scenes,” in Proceedings of"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "Findings of\nthe Association for Computational Linguistics, 2021.",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "Intelligence, vol. 32, no. 1, 2018."
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "[9] M. Bayer, M. Kaufhold, and C. Reuter, “A survey on data augmentation",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "J. Pennington, R. Socher, and C. D. Manning, “Glove: Global vectors"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "for\ntext classification,” ACM Comput. Surv., vol. 55, no. 7, pp. 1–39,",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "of\nthe\n2014\nconference\non\nfor word\nrepresentation,”\nin Proceedings"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "2022.",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "empirical methods in natural\nlanguage processing (EMNLP), 2014."
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "[10]\nL. Pellicer, T. Ferreira, and A. Costa, “Data augmentation techniques in",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "[30] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "natural\nlanguage processing,” Applied Soft Computing, vol. 132, 2023.",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "L. Zettlemoyer, and V. Stoyanov, “Roberta: A robustly optimized bert"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "[11] G. Ansari, M. Garg,\nand C. Saxena,\n“Data\naugmentation for mental",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "pretraining approach,” arXiv preprint arXiv:1907.11692, 2019."
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "the 18th Inter-\nhealth classification on social media,” in Proceedings of",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "[31] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "national Conference on Natural Language Processing (ICON), 2021.",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "the 40th\nautomatic evaluation of machine translation,” in Proceedings of"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "[12] M. M.\nImran, Y.\nJain, P. Chatterjee,\nand K. Damevski,\n“Data\naug-",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "Annual Meeting of\nthe Association for Computational Linguistics, 2002."
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "mentation for\nimproving emotion recognition in software\nengineering",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "[32] X. Lu, Z. Li, Y. Tong, Y. Zhao, and B. Qin, “HIT-SCIR at WASSA 2023:"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "the 37th IEEE/ACM International\ncommunication,” in Proceedings of",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "Empathy and emotion analysis at the utterance-level and the essay-level,”"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "Conference on Automated Software Engineering, 2023.",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "the 13th Workshop on Computational Approaches to\nin Proceedings of"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "[13]\nS. Longpre, Y. Wang, and C. DuBois, “How effective is\ntask-agnostic",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "Subjectivity, Sentiment, & Social Media Analysis.\nToronto, Canada:"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "of\nthe\ndata\naugmentation\nfor\npretrained\ntransformers?”\nin Findings",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "ACL, Jul. 2023, pp. 574–580."
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "Association for Computational Linguistics: EMNLP 2020, 2020.",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "P. Liu, W. Yuan,\nJ. Fu, Z.\nJiang, H. Hayashi,\nand G. Neubig,\n“Pre-"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "[14]\nI. Okimura, M. Reid, M. Kawano,\nand Y. Matsuo,\n“On\nthe\nimpact",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "train, prompt, and predict: A systematic survey of prompting methods"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "of data augmentation on downstream performance in natural\nlanguage",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "in natural\nlanguage processing,” ACM Comput. Surv., vol. 55, no. 9,"
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "of\nthe\n3rd Workshop\non\nInsights\nfrom\nprocessing,”\nin Proceedings",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": "2023."
        },
        {
          "for\ntraining led to large\nimprovements:\nthe best performing": "Negative Results in NLP, 2022, pp. 88–93.",
          "[15] V. Barriere, J. Sedoc, S. Tafreshi, and S. Giorgi, “Findings of WASSA": ""
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A review on sentiment analysis and emotion detection from text",
      "authors": [
        "P Nandwani",
        "R Verma"
      ],
      "year": "2021",
      "venue": "Social Network Analysis and Mining"
    },
    {
      "citation_id": "2",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "3",
      "title": "Emotions: A general psychoevolutionary theory",
      "authors": [
        "R Plutchik"
      ],
      "year": "1984",
      "venue": "Approaches to emotion"
    },
    {
      "citation_id": "4",
      "title": "Measuring emotions in the covid-19 real world worry dataset",
      "authors": [
        "B Kleinberg",
        "I Van Der Vegt",
        "M Mozes"
      ],
      "year": "2020",
      "venue": "Proceedings of the 1st Workshop on NLP for COVID-19 at ACL 2020"
    },
    {
      "citation_id": "5",
      "title": "Eda: Easy data augmentation techniques for boosting performance on text classification tasks",
      "authors": [
        "J Wei",
        "K Zou"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "6",
      "title": "Paraphrasing revisited with neural machine translation",
      "authors": [
        "J Mallinson",
        "R Sennrich",
        "M Lapata"
      ],
      "year": "2017",
      "venue": "Proceedings of the 15th Conference of the European Chapter"
    },
    {
      "citation_id": "7",
      "title": "PROTAUGMENT: Unsupervised diverse short-texts paraphrasing for intent detection metalearning",
      "authors": [
        "T Dopierre",
        "C Gravier",
        "W Logerais"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "8",
      "title": "A survey of data augmentation approaches for nlp",
      "authors": [
        "S Feng",
        "V Gangal",
        "J Wei",
        "S Chandar",
        "S Vosoughi",
        "T Mitamura",
        "E Hovy"
      ],
      "year": "2021",
      "venue": "A survey of data augmentation approaches for nlp"
    },
    {
      "citation_id": "9",
      "title": "A survey on data augmentation for text classification",
      "authors": [
        "M Bayer",
        "M Kaufhold",
        "C Reuter"
      ],
      "year": "2022",
      "venue": "ACM Comput. Surv"
    },
    {
      "citation_id": "10",
      "title": "Data augmentation techniques in natural language processing",
      "authors": [
        "L Pellicer",
        "T Ferreira",
        "A Costa"
      ],
      "year": "2023",
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "11",
      "title": "Data augmentation for mental health classification on social media",
      "authors": [
        "G Ansari",
        "M Garg",
        "C Saxena"
      ],
      "venue": "Proceedings of the 18th International Conference on Natural Language Processing"
    },
    {
      "citation_id": "12",
      "title": "Data augmentation for improving emotion recognition in software engineering communication",
      "authors": [
        "M Imran",
        "Y Jain",
        "P Chatterjee",
        "K Damevski"
      ],
      "year": "2023",
      "venue": "Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering"
    },
    {
      "citation_id": "13",
      "title": "How effective is task-agnostic data augmentation for pretrained transformers?",
      "authors": [
        "S Longpre",
        "Y Wang",
        "C Dubois"
      ],
      "year": "2020",
      "venue": "How effective is task-agnostic data augmentation for pretrained transformers?"
    },
    {
      "citation_id": "14",
      "title": "On the impact of data augmentation on downstream performance in natural language processing",
      "authors": [
        "I Okimura",
        "M Reid",
        "M Kawano",
        "Y Matsuo"
      ],
      "year": "2022",
      "venue": "Proceedings of the 3rd Workshop on Insights from Negative Results in NLP"
    },
    {
      "citation_id": "15",
      "title": "Findings of WASSA 2023 shared task on empathy, emotion and personality detection in conversation and reactions to news articles",
      "authors": [
        "V Barriere",
        "J Sedoc",
        "S Tafreshi",
        "S Giorgi"
      ],
      "year": "2023",
      "venue": "Proceedings of the 13th Workshop on Computational Approaches to Subjectivity, Sentiment, & Social Media Analysis"
    },
    {
      "citation_id": "16",
      "title": "Automatically classifying emotions based on text: A comparative exploration of different datasets",
      "authors": [
        "A Koufakou",
        "J Garciga",
        "A Paul",
        "J Morelli",
        "C Frank"
      ],
      "year": "2022",
      "venue": "34th International Conference on Tools with Artificial Intelligence (ICTAI)"
    },
    {
      "citation_id": "17",
      "title": "EmoEvent: A Multilingual Emotion Corpus based on different Events",
      "authors": [
        "F Plaza-Del-Arco",
        "C Strapparava",
        "L Urena-Lopez",
        "M Martin-Valdivia"
      ],
      "year": "2020",
      "venue": "Proceedings of the 12th Language Resources and Evaluation Conference"
    },
    {
      "citation_id": "18",
      "title": "Wassa 2021 shared task: Predicting empathy and emotion in reaction to news stories",
      "authors": [
        "S Tafreshi",
        "O De Clercq",
        "V Barriere",
        "S Buechel",
        "J Sedoc",
        "A Balahur"
      ],
      "year": "2021",
      "venue": "Proceedings of the 11th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis. Online: ACL"
    },
    {
      "citation_id": "19",
      "title": "WASSA@IITK at WASSA 2021: Multi-task learning and transformer finetuning for emotion classification and empathy prediction",
      "authors": [
        "J Mundra",
        "R Gupta",
        "S Mukherjee"
      ],
      "year": "2021",
      "venue": "Proceedings of the 11th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis. Online: ACL"
    },
    {
      "citation_id": "20",
      "title": "Goemotions: A dataset of fine-grained emotions",
      "authors": [
        "D Demszky",
        "D Movshovitz-Attias",
        "J Ko",
        "A Cowen",
        "G Nemade",
        "S Ravi"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "21",
      "title": "SemEval-2018 task 1: Affect in tweets",
      "authors": [
        "S Mohammad",
        "F Bravo-Marquez",
        "M Salameh",
        "S Kiritchenko"
      ],
      "year": "2018",
      "venue": "Proceedings of The 12th International Workshop on Semantic Evaluation"
    },
    {
      "citation_id": "22",
      "title": "Linguistic inquiry and word count: Liwc 2001",
      "authors": [
        "J Pennebaker",
        "M Francis",
        "R Booth"
      ],
      "year": "2001",
      "venue": "Linguistic inquiry and word count: Liwc 2001"
    },
    {
      "citation_id": "23",
      "title": "Modeling empathy and distress in reaction to news stories",
      "authors": [
        "S Buechel",
        "A Buffone",
        "B Slaff",
        "L Ungar",
        "J Sedoc"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "24",
      "title": "Wordnet: a lexical database for english",
      "authors": [
        "G Miller"
      ],
      "year": "1995",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "25",
      "title": "That's so annoying!!!: A lexical and frame-semantic embedding based data augmentation approach to automatic categorization of annoying behaviors using# petpeeve tweets",
      "authors": [
        "W Wang",
        "D Yang"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "26",
      "title": "BERT: Pretraining of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Conference of the NACL: Human Language Technologies"
    },
    {
      "citation_id": "27",
      "title": "BART: Denoising sequence-tosequence pre-training for natural language generation, translation, and comprehension",
      "authors": [
        "M Lewis",
        "Y Liu",
        "N Goyal",
        "M Ghazvininejad",
        "A Mohamed",
        "O Levy",
        "V Stoyanov",
        "L Zettlemoyer"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "28",
      "title": "Diverse beam search for improved description of complex scenes",
      "authors": [
        "A Vijayakumar",
        "M Cogswell",
        "R Selvaraju",
        "Q Sun",
        "S Lee",
        "D Crandall",
        "D Batra"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "29",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "J Pennington",
        "R Socher",
        "C Manning"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)"
    },
    {
      "citation_id": "30",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "31",
      "title": "Bleu: a method for automatic evaluation of machine translation",
      "authors": [
        "K Papineni",
        "S Roukos",
        "T Ward",
        "W.-J Zhu"
      ],
      "year": "2002",
      "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "32",
      "title": "HIT-SCIR at WASSA 2023: Empathy and emotion analysis at the utterance-level and the essay-level",
      "authors": [
        "X Lu",
        "Z Li",
        "Y Tong",
        "Y Zhao",
        "B Qin"
      ],
      "year": "2023",
      "venue": "Proceedings of the 13th Workshop on Computational Approaches to Subjectivity, Sentiment, & Social Media Analysis"
    },
    {
      "citation_id": "33",
      "title": "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
      "authors": [
        "P Liu",
        "W Yuan",
        "J Fu",
        "Z Jiang",
        "H Hayashi",
        "G Neubig"
      ],
      "year": "2023",
      "venue": "ACM Comput. Surv"
    }
  ]
}