{
  "paper_id": "2110.03299v4",
  "title": "End-To-End Label Uncertainty Modeling For Speech-Based Arousal Recognition Using Bayesian Neural Networks",
  "published": "2021-10-07T09:34:28Z",
  "authors": [
    "Navin Raj Prabhu",
    "Guillaume Carbajal",
    "Nale Lehmann-Willenbrock",
    "Timo Gerkmann"
  ],
  "keywords": [
    "Bayesian networks",
    "end-to-end speech emotion recognition",
    "uncertainty",
    "subjectivity",
    "label distribution learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotions are subjective constructs. Recent end-to-end speech emotion recognition systems are typically agnostic to the subjective nature of emotions, despite their state-of-the-art performance. In this work, we introduce an end-to-end Bayesian neural network architecture to capture the inherent subjectivity in the arousal dimension of emotional expressions. To the best of our knowledge, this work is the first to use Bayesian neural networks for speech emotion recognition. At training, the network learns a distribution of weights to capture the inherent uncertainty related to subjective arousal annotations. To this end, we introduce a loss term that enables the model to be explicitly trained on a distribution of annotations, rather than training them exclusively on mean or gold-standard labels. We evaluate the proposed approach on the AVEC'16 dataset. Qualitative and quantitative analysis of the results reveals that the proposed model can aptly capture the distribution of subjective arousal annotations, with state-of-the-art results in mean and standard deviation estimations for uncertainty modeling.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "While individual subjective emotional experiences may be accessed using self-report surveys  [1] , expressions of emotions are embedded in a social context, which makes them inherently dynamic and subjective interpersonal phenomena  [2, 3] . One way in which emotions become expressed in social interactions and therefore accessible for social signal processing concerns speech signals. Speech emotion recognition (SER) research spans roughly two decades  [4] , with ever improving state-ofthe-art results. As a consequence, affective sciences and SER has shown increasing prominence in high-critical and socially relevant domains, e.g. health and employee well-being  [4, 5] .\n\nCommon SER approaches rely on hand-crafted features to predict gold-standard emotion labels  [6, 7] . Recently, end-toend deep neural networks (DNNs) have been shown to deliver state-of-the-art emotion predictions  [8, 9] , by learning features rather than relying on hand-crafted features. Despite their stateof-the-art results, they are often agnostic to the subjective nature of emotions and the resulting label uncertainty  [10, 7] , thereby inducing limited reliability in SER  [4] . However, for any realworld application context, it is crucial that SER systems should not only deliver mean or gold-standard predictions but also account for subjectivity based confidence measures  [11, 4] .\n\nHan et al.,  [10, 6]  pioneered uncertainty modeling in SER using a multi-task framework to also predict the standard deviation of emotion annotations. Sridhar et al.,  [7]  introduced a dropout-based model to estimate uncertainties. However, these uncertainty models were not trained on the distribution of emotion annotations and relied on hand-crafted features. Of note, in contrast to hand-crafted features, learning representations in an end-to-end manner implies that they are also dependent on the level of subjectivity in label annotations  [12] .\n\nIn machine learning, two types of uncertainty can be distinguished. Aleatoric uncertainty captures data inherent noise (label uncertainty) whereas epistemic uncertainty accounts for the model parameters and structure (model uncertainty)  [13] . Stochastic and probabilistic models have mainly been deployed for uncertainty modeling, using auto-encoder architectures  [14] , neural processes  [15] , and Bayesian neural networks (BNN)  [16] . Bayes by Backpropagation (BBB) for BNNs  [16]  uses simple gradient updates to optimize weight distributions. Further with its capability to produce stochastic outputs, it is a promising candidate for end-to-end uncertainty SER.\n\nAs opposed to emotions as inner subjective experiences  [1] , we focus on emotional expressions as behaviors that others subjectively perceive and respond to. A common framework for analyzing the expression of emotion is pleasure-arousal theory  [17, 18] , which describes emotional experiences in two continuous, bipolar, and orthogonal dimensions: pleasure-displeasure (valence) and activation-deactivation (arousal). It is documented in SER literature that the audio modality insufficiently explains valence  [8, 9] . Noting this, in this work, we decided to specifically focus on the label uncertainty in the arousal dimension of emotional expressions.\n\nIn this paper, we propose an end-to-end BBB-based BNN architecture for SER. To the best of our knowledge, this is the first time a BNN is used for SER. In contrast to  [10, 6, 7] , the model can be explicitly trained on a distribution of emotion annotations. For this, we introduce a loss term that promotes capturing aleotoric uncertainty (label uncertainty) rather than exclusively capturing epistemic uncertainty (model uncertainty). Finally, we show that our proposed model trained on the loss term can aptly capture label uncertainty in arousal annotations.\n\nThe rest of the paper is organized as follows. In Section 2, we present related background on label uncertainty. In Section 3, we introduce the proposed end-to-end BNN SER model. In Section 4, we explain the experimental setup. In Section 5, we present the results and raise discussions on them.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "On Uncertainty In Arousal Annotations",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ground-Truth Labels",
      "text": "A crucial challenge in studying emotions within the arousal and valence framework concerns the significant degree of subjectivity surrounding them  [4, 10, 19] . To tackle this, annotations arXiv:2110.03299v4 [eess.AS] 27 Jun 2022 {y1, y2, .., ya} for emotions are collected from a annotators  [20, 21] . The ground-truth label is then obtained as the mean m over all annotations from a annotators  [22, 23] ,\n\nAlternatively, an evaluator-weighted mean has been proposed and referred to as the gold-standard m  [24, 8] . However, in order to better represent subjective annotations, in this paper we use the mean m rather than the evaluator-weighted mean.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Point Estimates In Ser",
      "text": "Given a raw audio sequence of T frames X = [x1, x2, ..., xT ], typically, the goal is to estimate the ground-truth label mt for each time frame t ∈ [1, T ], referred to as mt. The concordance correlation coefficient (CCC), which takes both linear correlations and biases into consideration, has been widely used as a loss function for this task. For Pearson correlation r, the CCC between the ground-truth label m and its estimate m, for T frames, is formulated as\n\nwhere µm = 1\n\n, and µ m , σ 2 m are obtained similarly for m. Early approaches relied on hand-crafted features as inputs to estimate CCC. End-to-end DNNs which circumvent the limitations of hand-crafted and -chosen features have been deployed to yield state-of-the-art performance  [25, 8, 9] . Notwithstanding their performance, end-to-end DNNs are trained exclusively on m and therefore cannot account for annotator subjectivity.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Uncertainty Modeling In Ser",
      "text": "Han et al. quantified label uncertainty using a statistical estimate, the perception uncertainty s defined as the unbiased standard deviation of a annotators  [10, 6] :\n\n(\n\nThey proposed a multi-task model to additionally estimate s along with m. However, the model only accounts for the standard deviation of a annotations, rather than the whole distribution in itself. Thereby, susceptible to unreliable s estimates for lower values of a and sparsely distributed annotations. Sridhar et al. introduced a Monte-Carlo dropout-based uncertainty model, to obtain stochastic predictions and uncertainty estimates  [7] . The model is trained exclusively on m rather than the distribution of annotations, thereby only capturing the model uncertainty and not the label uncertainties.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "End-To-End Label Uncertainty Model",
      "text": "In order to better represent subjectivity in emotional expressions, we propose to estimate the emotion annotation distribution Yt for each frame t. While the true distributional family of Yt is unknown, we assume, for simplicity, that it follows a Gaussian distribution:\n\nThus, the goal is to obtain an estimate Yt of Yt and infer both mt and st from realizations of Yt.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "End-To-End Dnn Architecture",
      "text": "We propose an end-to-end architecture which uses a feature extractor to learn temporal-paralinguistic features from xt, and an uncertainty layer to estimate Yt (see Fig.  1 ). The feature extractor, inspired from  [8] , consists of three Conv1D layers followed by two stacked long-short term memory (LSTM) layers. The uncertainty layer is devised using the BBB technique  [16] , comprising three BBB-based MLP.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Model Uncertainty Loss",
      "text": "Unlike a standard neuron which optimizes a deterministic weight w, the BBB-based neuron learns a probability distribution on the weight w by calculating the variational posterior P (w|D) given the training data D  [16] . Intuitively, this regularizes w to also capture the inherent uncertainty in D.\n\nFollowing  [16] , we parametrize P (w|D) using a Gaussian N (µw, σw). For non-negative σw, we re-parametrize the standard deviation σw = log(1 + exp(ρw)). Then, θ = (µw, ρw) can be optimized using simple backpropagation.\n\nFor an optimized θ, the predictive distribution Yt for an audio frame xt, is given by P ( yt|xt) = E P (w|D) [P ( yt|xt, w)], where yt are realizations of Yt. Unfortunately, the expectation under the posterior of weights is intractable. To tackle this, the authors in  [16]  proposed to learn θ of a weight distribution q(w|θ), the variational posterior, that minimizes the Kullback-Leibler (KL) divergence with the true Bayesian posterior, resulting in the negative evidence lower bound (ELBO), f (w, θ)BBB = KL q(w|θ) P (w) -E q(w|θ) log P (D|w) .\n\n(5) Stochastic outputs in BBB are achieved using multiple forward passes n with stochastically sampled weights w, thereby modeling Yt using the n stochastic estimates. To account for the stochastic outputs, (  5 ) is approximated as, LBBB ≈ n i=1 log q(w (i) |θ) -log P (w (i) ) -log P (D|w (i) ).  (6)  where w (i) denotes the i th weight sample drawn from q(w|θ). The BBB window-size b controls how often new weights are sampled for time-continuous SER. The degree of uncertainty is assumed to be constant within this time period.\n\nDuring testing, the uncertainty estimate st is the standard deviation of Yt. Similarly, mt is the realization yt obtained using the mean of the optimized weights µw. Obtaining mt using µw helps overcome the randomization effect of sampling from q(w|θ), which showed better performances in our case.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Label Uncertainty Loss",
      "text": "Inspired by  [13] , we introduce a KL divergence-based loss term as a measurement of distribution similarity to explicitly fit our model to the label distribution Yt:\n\nThe KL divergence is asymmetric, making the order of distributions crucial. In 7, the true distribution Yt is followed by its estimate Yt, promoting a mean-seeking approximation rather than a mode-seeking one and capturing the full distribution  [26] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "End-To-End Uncertainty Loss",
      "text": "The proposed end-to-end uncertainty loss is formulated as,\n\nIntuitively, LCCC(m) optimizes for mean predictions m, LBBB optimizes for BBB weight distributions, and LKL optimizes for the label distribution Yt. For α = 0, the model only captures model uncertainty (MU). For α = 1, the model also captures label uncertainty (+LU). LCCC(m) is used as part of L to achieve faster convergence and jointly optimize for mean predictions.\n\nIncluding LCCC(m) might lead to better optimization of the feature extractor, as previously illustrated by  [8, 27] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Baselines",
      "text": "As baselines, we use Han et al.'s perception uncertainty model (MTL PU) and single-task learning model (STL)  [6] . The MTL PU model uses a multi-task technique followed by a dynamic tuning layer to account for perception uncertainty s in the final mean estimations m. The STL on the other hand only performs a single task of estimating m.\n\nFor a fair comparison, we reimplemented the baselines and tested them in our test bed. Crucially, the reimplementation also enables us to compare the models in-terms of their s estimates, which were not presented in Han et al.'s work  [6] . The only difference between Han et al.'s test bed and ours is the postprocessing pipeline. While Han et al. use the post-processing pipeline suggested in the AVEC'16  [23] , here we use the median filtering  [8]  as the sole post-processing technique to make all considered approaches comparable.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Choice Of Hyperparameters",
      "text": "The hyperparameters of the feature extractor (e.g. kernel sizes, filters) are adopted from  [27] . A similar extractor with the same hyperparameters has been used in several multimodal emotion recognition tasks with state-of-the-art performance  [9, 27] .\n\nAs the prior distribution P (w),  [16]  recommend a mixture of two Gaussians, with zero means and standard deviations as σ1 > σ2 and σ2\n\n1, thereby obtaining a spike-and-slab prior with heavy tail and concentration around zero mean. But in our case, we do not need mean centered predictions as Y does not follow such a distribution, as seen in Section 4.1. In this light, we propose to use a simple Gaussian prior with unit standard deviation N (0, 1). The µw and ρw of the posterior distribution P (w|D) are initialized uniformly in the range [-0.1, 0.1] and [-3, -2] respectively. The ranges were fine-tuned using grid search for maximized LKL at initialization on the train partition.\n\nIt is computationally expensive to sample new weights at every time-step (40 ms) and also the level of uncertainties varies rather slowly. In this light, we set BBB window-size b = 2 s (50 frames). The same window-size is also used for median filtering, the sole post-processing technique used. The number of forward passes in (  6 ) is fixed to n = 30, with the timecomplexity in consideration.\n\nFor training, we use the Adam optimizer with a learning rate of 10 -4 . The batch size used was 5, with a sequence length of 300 frames, 40 ms each and 12 s in total. Dropout with probability 0.5 was used to prevent overfitting. All the models were trained for a fixed 100 epochs. The best model is selected and used for testing when best L is observed on train partition.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Validation Metrics",
      "text": "To validate our mean estimates, we use LCCC(m), a widely used metric in literature  [8, 27, 6] . To validate the label uncertainty estimates, we use LCCC(s), along with LKL. LCCC(s), previously used in  [10] , only validates the performance on s, and ignores performances on m. In this light, we additionally use LKL which jointly validates both m and s performances. However, the metric makes a Gaussian assumption on Yt and hence can be biased. In this light, we use both these metrics for the validation with their respective benefits under consideration. Statistical significance is estimated using one-tailed t-test, asserting significance for p-values ≤ 0.05, similar to  [28] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results And Discussion",
      "text": "Table  1  shows the average performance of the baselines and the proposed models, in terms of their mean m, standard deviation s, and distribution Yt estimations, LCCC(m), LCCC(s) and LKL respectively. Comparisons with respect to LCCC(s) and LKL are not presented for the STL baseline as this algorithm does not contain uncertainty modeling and does not estimate s.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Comparison With Baselines",
      "text": "From Table  1 , we observe that both the proposed uncertainty models, MU and +LU, provide improved mean m estimations over the baselines, in-terms of LCCC(m), with statistical significance. Crucially, we observe that our models provide better LCCC(m) even in comparison with the STL baseline  [6]  which is not an uncertainty model and only estimates m without accounting for the label uncertainty. While uncertainty models MU and +LU achieve 0.756 and 0.744 LCCC(m) respectively, STL and MTL PU  [6]  achieve 0.734 and 0.719 respectively. Secondly, we note that the proposed label uncertainty model +LU achieves state-of-the-art results for standard deviation s and distribution Yt estimations, with statistical significance, achieving 0.340 LCCC(s) and 0.258 LKL respectively. The +LU model is trained on a more informative distribution of annotations Yt, in contrast to training on the s estimate  [6] , thereby leading to better capturing label uncertainty in arousal annotations. This explains the capability of distribution learning, using the LKL loss term, for modeling label uncertainty in SER. Also noting here that, in contrast to the baselines, the proposed model learns uncertainty dependent representations in an end-to-end manner for improved uncertainty estimates, inline with literature  [12]  which recommends end-to-end learning for uncertainty modeling in SER. Conclusively, the results in-terms of LCCC(m), LCCC(s), and LKL reveal the ability of the proposed +LU model to best capture label uncertainty, thereby also improving mean estimations in comparison to the baselines.\n\nTo further validate the results, we plot the distribution estimations for a test subject, seen in Figure  2 . From the figure, as the quantitative results suggest, we see that the MU and +LU models are superior to the baseline MTL PU in-terms of the distribution estimations. Specifically, the +LU captures the whole distribution of arousal annotations better than the MTL PU model, by well capturing the time-varying uncertainty without noisy estimates. This further highlights the advantage of training on LKL loss term.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Comparison Between Mu And +Lu",
      "text": "Between the proposed uncertainty models, we note that +LU outperforms MU in terms of LCCC(s), without significant degradations to LCCC(m). Specifically, +LU improves drastically and significantly in terms of LCCC(s) from 0.076 to 0.340. At the same time, for LCCC(m) a slight reduction from 0.756 to 0.744 can be observed which, however, is not statistically significant. This reveals that, by also optimizing LKL, our model can better account for subjectivity in arousal. It is important to note here that the trade-off between LCCC(s) and LCCC(s) can be further adjusted using a different prior P (w) on the weight distributions, as noted by Blundell et al.  [16]  who recommend a spike-and-slab for mean centered predictions. However, in our case, as the arousal annotations do not follow a mean centered distribution (seen in Section 4.1), we chose a simple Gaussian prior with unit standard deviation N (0, 1) to better capture the annotation distribution. Moreover, the choice of such as simple prior makes our model also scalable, eliminating the requirement to tune the prior with respect to different SER datasets.\n\nMoreover, from the Figure  2 , backing the results in Table  1 , one may see that +LU, explicitly trained on LKL, best captures the subjectivity in emotions, while MU is optimized more for predictions centered on the mean and strict standard deviations.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Label Uncertainty Bnn For Ser",
      "text": "This work is the first in literature to study BNNs for SER. The BBB technique, adopted by the proposed model, use simple gradient updates and produce stochastic outputs, making them promising candidates for end-to-end uncertainty modeling. Crucially, they open up possibilities for training the model on a distribution of annotations, rather than a less informative standard deviation estimate. Moreover, unlike the MTL PU which requires dataset-dependent tuning of the loss function, using the correlation estimate between m and s  [6] , our proposed models do not require loss function tuning and are scalable across SER datasets with a simple prior initialization.\n\nWhile the proposed model has several advantages, it also leaves room for future work. Firstly, the heuristics-based initialization of P (w|D) for optimal performances could be further studied. Secondly, in the model we assumed that Yt follows a Gaussian distribution, and had only a = 6 annotations available to model the distribution. This might sometimes lead to unstable LKL during approximation of Yt, thereby affecting the training processes. Acknowledging that gaining more annotation is resource inefficient, as future work, we will investigate techniques to model stable Yt with limited annotations.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "We introduced a BNN-based end-to-end approach for SER, which can account for the subjectivity and label uncertainty in emotional expressions. To this end, we introduced a loss term based on the KL divergence to enable our approach to be trained on a distribution of annotations. Unlike previous approaches, the stochastic outputs of our approach can be employed to estimate statistical moments such as the mean and standard deviation of the emotion annotations. Analysis of the results reveals that the proposed uncertainty model trained on the KL loss term can aptly capture the distribution of arousal annotations, achieving state-of-the-art results in mean and standard deviation estimations, in-terms of both the CCC and KL divergence metrics.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The proposed architecture. Blue denotes layers with",
      "page": 2
    },
    {
      "caption": "Figure 1: ). The feature ex-",
      "page": 2
    },
    {
      "caption": "Figure 2: Results obtained for a test subject for arousal.",
      "page": 4
    },
    {
      "caption": "Figure 2: From the ﬁg-",
      "page": 4
    },
    {
      "caption": "Figure 2: , backing the results in Table 1,",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "timo.gerkmann}@uni-hamburg.de",
          "nale.lehmann-willenbrock,": ""
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "Abstract",
          "nale.lehmann-willenbrock,": "dropout-based model to estimate uncertainties. However, these"
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "",
          "nale.lehmann-willenbrock,": "uncertainty models were not trained on the distribution of emo-"
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "Emotions are subjective constructs. Recent end-to-end speech",
          "nale.lehmann-willenbrock,": "tion annotations and relied on hand-crafted features. Of note, in"
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "emotion recognition systems are typically agnostic to the sub-",
          "nale.lehmann-willenbrock,": "contrast to hand-crafted features, learning representations in an"
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "jective nature of emotions, despite their state-of-the-art perfor-",
          "nale.lehmann-willenbrock,": "end-to-end manner implies that\nthey are also dependent on the"
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "mance. In this work, we introduce an end-to-end Bayesian neu-",
          "nale.lehmann-willenbrock,": "level of subjectivity in label annotations [12]."
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "ral network architecture to capture the inherent subjectivity in",
          "nale.lehmann-willenbrock,": ""
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "",
          "nale.lehmann-willenbrock,": "In machine learning,\ntwo types of uncertainty can be dis-"
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "the arousal dimension of emotional expressions. To the best of",
          "nale.lehmann-willenbrock,": ""
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "",
          "nale.lehmann-willenbrock,": "tinguished. Aleatoric uncertainty captures data inherent noise"
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "our knowledge, this work is the ﬁrst to use Bayesian neural net-",
          "nale.lehmann-willenbrock,": ""
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "",
          "nale.lehmann-willenbrock,": "(label uncertainty) whereas epistemic uncertainty accounts for"
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "works for speech emotion recognition. At training, the network",
          "nale.lehmann-willenbrock,": ""
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "",
          "nale.lehmann-willenbrock,": "the model parameters and structure (model uncertainty)\n[13]."
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "learns a distribution of weights to capture the inherent uncer-",
          "nale.lehmann-willenbrock,": ""
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "",
          "nale.lehmann-willenbrock,": "Stochastic and probabilistic models have mainly been deployed"
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "tainty related to subjective arousal annotations.\nTo this end,",
          "nale.lehmann-willenbrock,": ""
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "",
          "nale.lehmann-willenbrock,": "for uncertainty modeling, using auto-encoder architectures [14],"
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "we introduce a loss term that enables the model\nto be explic-",
          "nale.lehmann-willenbrock,": ""
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "",
          "nale.lehmann-willenbrock,": "neural processes\n[15],\nand Bayesian neural networks\n(BNN)"
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "itly trained on a distribution of annotations, rather than training",
          "nale.lehmann-willenbrock,": ""
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "",
          "nale.lehmann-willenbrock,": "[16].\nBayes by Backpropagation (BBB)\nfor BNNs [16] uses"
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "them exclusively on mean or gold-standard labels. We evalu-",
          "nale.lehmann-willenbrock,": ""
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "",
          "nale.lehmann-willenbrock,": "simple gradient updates to optimize weight distributions. Fur-"
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "ate the proposed approach on the AVEC’16 dataset. Qualitative",
          "nale.lehmann-willenbrock,": ""
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "",
          "nale.lehmann-willenbrock,": "ther with its capability to produce stochastic outputs,\nit\nis a"
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "and quantitative analysis of the results reveals that the proposed",
          "nale.lehmann-willenbrock,": ""
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "",
          "nale.lehmann-willenbrock,": "promising candidate for end-to-end uncertainty SER."
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "model can aptly capture the distribution of subjective arousal",
          "nale.lehmann-willenbrock,": ""
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "",
          "nale.lehmann-willenbrock,": "As opposed to emotions as inner subjective experiences [1],"
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "annotations, with state-of-the-art results in mean and standard",
          "nale.lehmann-willenbrock,": ""
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "",
          "nale.lehmann-willenbrock,": "we focus on emotional expressions as behaviors that others sub-"
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "deviation estimations for uncertainty modeling.",
          "nale.lehmann-willenbrock,": ""
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "",
          "nale.lehmann-willenbrock,": "jectively perceive and respond to. A common framework for"
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "Index Terms: Bayesian networks, end-to-end speech emotion",
          "nale.lehmann-willenbrock,": ""
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "",
          "nale.lehmann-willenbrock,": "analyzing the expression of emotion is pleasure-arousal\ntheory"
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "recognition, uncertainty, subjectivity, label distribution learning",
          "nale.lehmann-willenbrock,": ""
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "",
          "nale.lehmann-willenbrock,": "[17, 18], which describes emotional experiences in two contin-"
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "",
          "nale.lehmann-willenbrock,": "uous, bipolar, and orthogonal dimensions: pleasure-displeasure"
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "1.\nIntroduction",
          "nale.lehmann-willenbrock,": "(valence)\nand\nactivation-deactivation\n(arousal).\nIt\nis\ndocu-"
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "",
          "nale.lehmann-willenbrock,": "mented in SER literature that\nthe audio modality insufﬁciently"
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "While individual subjective emotional experiences may be ac-",
          "nale.lehmann-willenbrock,": ""
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "",
          "nale.lehmann-willenbrock,": "explains valence [8, 9]. Noting this, in this work, we decided to"
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "cessed using self-report\nsurveys\n[1], expressions of emotions",
          "nale.lehmann-willenbrock,": ""
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "",
          "nale.lehmann-willenbrock,": "speciﬁcally focus on the label uncertainty in the arousal dimen-"
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "are embedded in a social context, which makes them inherently",
          "nale.lehmann-willenbrock,": ""
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "",
          "nale.lehmann-willenbrock,": "sion of emotional expressions."
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "dynamic and subjective interpersonal phenomena [2, 3]. One",
          "nale.lehmann-willenbrock,": ""
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "",
          "nale.lehmann-willenbrock,": "In this paper, we propose an end-to-end BBB-based BNN"
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "way in which emotions become expressed in social interactions",
          "nale.lehmann-willenbrock,": ""
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "",
          "nale.lehmann-willenbrock,": "architecture for SER. To the best of our knowledge,\nthis is the"
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "and therefore accessible for social signal processing concerns",
          "nale.lehmann-willenbrock,": ""
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "",
          "nale.lehmann-willenbrock,": "ﬁrst\ntime a BNN is used for SER. In contrast\nto [10, 6, 7],\nthe"
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "speech signals.\nSpeech emotion recognition (SER)\nresearch",
          "nale.lehmann-willenbrock,": ""
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "",
          "nale.lehmann-willenbrock,": "model can be explicitly trained on a distribution of emotion an-"
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "spans roughly two decades [4], with ever\nimproving state-of-",
          "nale.lehmann-willenbrock,": ""
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "",
          "nale.lehmann-willenbrock,": "notations. For this, we introduce a loss term that promotes cap-"
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "the-art results. As a consequence, affective sciences and SER",
          "nale.lehmann-willenbrock,": ""
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "",
          "nale.lehmann-willenbrock,": "turing aleotoric uncertainty (label uncertainty)\nrather\nthan ex-"
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "has shown increasing prominence in high-critical and socially",
          "nale.lehmann-willenbrock,": ""
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "",
          "nale.lehmann-willenbrock,": "clusively capturing epistemic uncertainty (model uncertainty)."
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "relevant domains, e.g. health and employee well-being [4, 5].",
          "nale.lehmann-willenbrock,": ""
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "",
          "nale.lehmann-willenbrock,": "Finally, we show that our proposed model\ntrained on the loss"
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "Common SER approaches rely on hand-crafted features to",
          "nale.lehmann-willenbrock,": ""
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "",
          "nale.lehmann-willenbrock,": "term can aptly capture label uncertainty in arousal annotations."
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "predict gold-standard emotion labels [6, 7]. Recently, end-to-",
          "nale.lehmann-willenbrock,": ""
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "",
          "nale.lehmann-willenbrock,": "The rest of the paper is organized as follows.\nIn Section 2,"
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "end deep neural networks (DNNs) have been shown to deliver",
          "nale.lehmann-willenbrock,": ""
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "",
          "nale.lehmann-willenbrock,": "we present related background on label uncertainty.\nIn Section"
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "state-of-the-art emotion predictions [8, 9], by learning features",
          "nale.lehmann-willenbrock,": ""
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "",
          "nale.lehmann-willenbrock,": "3, we introduce the proposed end-to-end BNN SER model.\nIn"
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "rather than relying on hand-crafted features. Despite their state-",
          "nale.lehmann-willenbrock,": ""
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "",
          "nale.lehmann-willenbrock,": "Section 4, we explain the experimental setup.\nIn Section 5, we"
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "of-the-art results, they are often agnostic to the subjective nature",
          "nale.lehmann-willenbrock,": ""
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "",
          "nale.lehmann-willenbrock,": "present the results and raise discussions on them."
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "of emotions and the resulting label uncertainty [10, 7],\nthereby",
          "nale.lehmann-willenbrock,": ""
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "inducing limited reliability in SER [4]. However, for any real-",
          "nale.lehmann-willenbrock,": ""
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "world application context, it is crucial that SER systems should",
          "nale.lehmann-willenbrock,": "2. On uncertainty in arousal annotations"
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "not only deliver mean or gold-standard predictions but also ac-",
          "nale.lehmann-willenbrock,": ""
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "",
          "nale.lehmann-willenbrock,": "2.1. Ground-truth labels"
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "count for subjectivity based conﬁdence measures [11, 4].",
          "nale.lehmann-willenbrock,": ""
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "Han et al., [10, 6] pioneered uncertainty modeling in SER",
          "nale.lehmann-willenbrock,": "A crucial challenge in studying emotions within the arousal and"
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "using a multi-task framework to also predict\nthe standard de-",
          "nale.lehmann-willenbrock,": "valence framework concerns the signiﬁcant degree of subjectiv-"
        },
        {
          "guillaume.carbajal,\n{navin.raj.prabhu,": "viation of emotion annotations. Sridhar et al., [7] introduced a",
          "nale.lehmann-willenbrock,": "ity surrounding them [4, 10, 19].\nTo tackle this, annotations"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": "[20, 21]. The ground-truth label\nis then obtained as the mean"
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": "m over all annotations from a annotators [22, 23],"
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": "1 a\na(cid:88) i\nm =\n(1)\nyi."
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": "=1"
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": "Alternatively, an evaluator-weighted mean has been proposed"
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": "in\nand referred to as the gold-standard (cid:101)m [24, 8]. However,"
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": "order\nto better\nrepresent subjective annotations,\nin this paper"
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": ""
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": "we use the mean m rather than the evaluator-weighted mean."
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": "2.2. Point estimates in SER"
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": ""
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": "Given a raw audio sequence of T frames X = [x1, x2, ..., xT ],"
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": "typically,\nthe goal\nfor\nis to estimate the ground-truth label mt"
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": ""
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": "each time frame t ∈ [1, T ], referred to as (cid:98)mt. The concordance"
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": "correlation coefﬁcient (CCC), which takes both linear correla-"
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": ""
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": "tions and biases into consideration, has been widely used as a"
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": ""
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": "loss function for this task. For Pearson correlation r,\nthe CCC"
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": ""
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": "between the ground-truth label m and its estimate (cid:98)m,"
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": "frames, is formulated as"
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": ""
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": "2rσmσ\nm"
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": "(cid:98)\n(2)\nLCCC(m) ="
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": "σ2\nm)2 ,\nm + σ2"
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": "m + (µm − µ\n(cid:98)"
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": "(cid:98)"
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": "(cid:80)T\n(cid:80)T"
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": "1 T\n1 T\nwhere µm =\nm =\nt=1 mt, σ2\nt=1(mt − µm)2, and"
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": ""
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": "µ\nm, σ2"
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": "(cid:98)\nm are obtained similarly for (cid:98)m."
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": ""
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": "Early approaches relied on hand-crafted features as inputs"
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": ""
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": "to estimate CCC. End-to-end DNNs which circumvent the limi-"
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": ""
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": "tations of hand-crafted and -chosen features have been deployed"
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": ""
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": "to yield state-of-the-art performance [25, 8, 9]. Notwithstand-"
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": ""
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": "ing their performance, end-to-end DNNs are trained exclusively"
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": ""
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": "on m and therefore cannot account for annotator subjectivity."
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": ""
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": "2.3. Uncertainty modeling in SER"
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": ""
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": "Han et al.\nquantiﬁed label uncertainty using a statistical esti-"
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": ""
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": "mate, the perception uncertainty s deﬁned as the unbiased stan-"
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": "dard deviation of a annotators [10, 6]:"
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": ""
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": "(cid:118)(cid:117)(cid:117)(cid:116)\n1"
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": "a(cid:88) i\ns =\n(3)\n(yi − m)2."
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": ""
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": "a − 1"
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": "=1"
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": ""
        },
        {
          "{y1, y2, .., ya} for emotions are collected from a annotators": "They proposed a multi-task model\nto additionally estimate s"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "as a measurement of distribution similarity to explicitly ﬁt our": ""
        },
        {
          "as a measurement of distribution similarity to explicitly ﬁt our": "model to the label distribution Yt:"
        },
        {
          "as a measurement of distribution similarity to explicitly ﬁt our": ""
        },
        {
          "as a measurement of distribution similarity to explicitly ﬁt our": "(cid:90)"
        },
        {
          "as a measurement of distribution similarity to explicitly ﬁt our": "Yt(x)"
        },
        {
          "as a measurement of distribution similarity to explicitly ﬁt our": "log\ndx.\n(7)\nYt(x)\nLKL = f (Yt(cid:107) (cid:98)Yt)KL ="
        },
        {
          "as a measurement of distribution similarity to explicitly ﬁt our": "Yt(x)"
        },
        {
          "as a measurement of distribution similarity to explicitly ﬁt our": ""
        },
        {
          "as a measurement of distribution similarity to explicitly ﬁt our": "The KL divergence is asymmetric, making the order of distribu-"
        },
        {
          "as a measurement of distribution similarity to explicitly ﬁt our": "tions crucial. In 7, the true distribution Yt is followed by its es-"
        },
        {
          "as a measurement of distribution similarity to explicitly ﬁt our": ""
        },
        {
          "as a measurement of distribution similarity to explicitly ﬁt our": "timate (cid:98)Yt, promoting a mean-seeking approximation rather than"
        },
        {
          "as a measurement of distribution similarity to explicitly ﬁt our": "a mode-seeking one and capturing the full distribution [26]."
        },
        {
          "as a measurement of distribution similarity to explicitly ﬁt our": ""
        },
        {
          "as a measurement of distribution similarity to explicitly ﬁt our": "3.4. End-to-end uncertainty loss"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": ""
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "bel distribution estimations Y, in terms of Lccc(m), Lccc(s), and"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": ""
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "LKL respectively. Larger CCC indicates improved performance."
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": ""
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "Lower KL indicates improved performance.\n** indicates sta-"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": ""
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "tistically signiﬁcant better results over all other approaches in"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": ""
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "comparison, and * over only some of the approaches."
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": ""
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": ""
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "Lccc(m)\nLccc(s)\nLKL"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": ""
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "STL [6]\n0.719\n-\n-"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "MTL PU [6]\n0.734\n0.286\n0.797"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": ""
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "0.756∗\nMU\n0.076\n0.690"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "0.340∗∗\n0.258∗∗\n+LU\n0.744"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": ""
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": ""
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "hyperparameters has been used in several multimodal emotion"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": ""
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "recognition tasks with state-of-the-art performance [9, 27]."
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "As the prior distribution P (w), [16] recommend a mixture"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "of two Gaussians, with zero means and standard deviations as"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "σ1 > σ2 and σ2 (cid:28) 1, thereby obtaining a spike-and-slab prior"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "with heavy tail and concentration around zero mean. But in our"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "case, we do not need mean centered predictions as Y does not"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "follow such a distribution, as seen in Section 4.1.\nIn this light,"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "we propose to use a simple Gaussian prior with unit standard"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "deviation N (0, 1). The µw and ρw of the posterior distribution"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "P (w|D) are initialized uniformly in the range [−0.1, 0.1] and"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "[−3, −2]"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "respectively.\nThe ranges were ﬁne-tuned using grid"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "search for maximized LKL at initialization on the train partition."
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": ""
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "It\nis computationally expensive to sample new weights at"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "every time-step (40 ms) and also the level of uncertainties varies"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "rather slowly.\nIn this light, we set BBB window-size b = 2 s"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "(50 frames).\nThe same window-size is also used for median"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "ﬁltering,\nthe sole post-processing technique used. The number"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "of\nforward passes\nin (6)\nis ﬁxed to n = 30, with the time-"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "complexity in consideration."
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "For\ntraining, we use the Adam optimizer with a learning"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "rate of 10−4. The batch size used was 5, with a sequence length"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": ""
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "of 300 frames, 40 ms each and 12 s\nin total.\nDropout with"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": ""
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "probability 0.5 was used to prevent overﬁtting. All\nthe models"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": ""
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "were trained for a ﬁxed 100 epochs. The best model is selected"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": ""
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "and used for testing when best L is observed on train partition."
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": ""
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "4.4. Validation metrics"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": ""
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "To validate our mean estimates, we use LCCC(m), a widely used"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": ""
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "metric in literature [8, 27, 6]. To validate the label uncertainty"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": ""
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "estimates, we use LCCC(s), along with LKL. LCCC(s), previ-"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": ""
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "ously used in [10], only validates the performance on s, and ig-"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": ""
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "nores performances on m. In this light, we additionally use LKL"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": ""
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "which jointly validates both m and s performances. However,"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": ""
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "the metric makes a Gaussian assumption on Yt and hence can"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": ""
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "be biased. In this light, we use both these metrics for the valida-"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": ""
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "tion with their respective beneﬁts under consideration. Statis-"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": ""
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "tical signiﬁcance is estimated using one-tailed t-test, asserting"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": ""
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "signiﬁcance for p-values ≤ 0.05, similar to [28]."
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": ""
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": ""
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "5. Results and Discussion"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": ""
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "Table 1 shows the average performance of the baselines and the"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": ""
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "proposed models, in terms of their mean m, standard deviation"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "s, and distribution (cid:98)Yt estimations, LCCC(m), LCCC(s) and LKL"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": ""
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "respectively. Comparisons with respect\nto LCCC(s) and LKL"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "are not presented for\nthe STL baseline as this algorithm does"
        },
        {
          "Table 1: Comparison on mean m, standard deviation s, and la-": "not contain uncertainty modeling and does not estimate s."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: , we observe that both the proposed uncertainty TheBBBtechnique,adoptedbytheproposedmodel,usesim-",
      "data": [
        {
          "MTL PU [6]": "0.5"
        },
        {
          "MTL PU [6]": ""
        },
        {
          "MTL PU [6]": "0.0"
        },
        {
          "MTL PU [6]": ""
        },
        {
          "MTL PU [6]": "0.5"
        },
        {
          "MTL PU [6]": ""
        },
        {
          "MTL PU [6]": "Model Uncertainty (MU)"
        },
        {
          "MTL PU [6]": "0.5"
        },
        {
          "MTL PU [6]": "0.0"
        },
        {
          "MTL PU [6]": ""
        },
        {
          "MTL PU [6]": "0.5"
        },
        {
          "MTL PU [6]": ""
        },
        {
          "MTL PU [6]": "+ Label Uncertainty (+ LU)"
        },
        {
          "MTL PU [6]": "0.5"
        },
        {
          "MTL PU [6]": ""
        },
        {
          "MTL PU [6]": "0.0"
        },
        {
          "MTL PU [6]": "0.5"
        },
        {
          "MTL PU [6]": ""
        },
        {
          "MTL PU [6]": "0\n50\n100\n150\n200\n250\n300"
        },
        {
          "MTL PU [6]": "Time (seconds)"
        },
        {
          "MTL PU [6]": "Predicted - m\nGroundtruth - m\nPredicted - s\nGroundtruth - s"
        },
        {
          "MTL PU [6]": ""
        },
        {
          "MTL PU [6]": "Figure 2: Results obtained for a test subject for arousal."
        },
        {
          "MTL PU [6]": ""
        },
        {
          "MTL PU [6]": ""
        },
        {
          "MTL PU [6]": "5.1. Comparison with baselines"
        },
        {
          "MTL PU [6]": ""
        },
        {
          "MTL PU [6]": "From Table 1, we observe that both the proposed uncertainty"
        },
        {
          "MTL PU [6]": ""
        },
        {
          "MTL PU [6]": "models, MU and +LU, provide improved mean m estimations"
        },
        {
          "MTL PU [6]": ""
        },
        {
          "MTL PU [6]": "over the baselines, in-terms of LCCC(m), with statistical signif-"
        },
        {
          "MTL PU [6]": "icance. Crucially, we observe that our models provide better"
        },
        {
          "MTL PU [6]": ""
        },
        {
          "MTL PU [6]": "LCCC(m) even in comparison with the STL baseline [6] which"
        },
        {
          "MTL PU [6]": "is not an uncertainty model and only estimates m without ac-"
        },
        {
          "MTL PU [6]": ""
        },
        {
          "MTL PU [6]": "counting for\nthe label uncertainty. While uncertainty models"
        },
        {
          "MTL PU [6]": ""
        },
        {
          "MTL PU [6]": "MU and +LU achieve 0.756 and 0.744 LCCC(m) respectively,"
        },
        {
          "MTL PU [6]": "STL and MTL PU [6] achieve 0.734 and 0.719 respectively."
        },
        {
          "MTL PU [6]": ""
        },
        {
          "MTL PU [6]": "Secondly, we\nnote\nthat\nthe\nproposed\nlabel\nuncertainty"
        },
        {
          "MTL PU [6]": "model +LU achieves state-of-the-art results for standard devi-"
        },
        {
          "MTL PU [6]": ""
        },
        {
          "MTL PU [6]": "ation s and distribution (cid:98)Yt estimations, with statistical signiﬁ-"
        },
        {
          "MTL PU [6]": ""
        },
        {
          "MTL PU [6]": "cance, achieving 0.340 LCCC(s) and 0.258 LKL respectively."
        },
        {
          "MTL PU [6]": ""
        },
        {
          "MTL PU [6]": "The +LU model\nis trained on a more informative distribution"
        },
        {
          "MTL PU [6]": ""
        },
        {
          "MTL PU [6]": "in contrast\nto training on the s estimate [6],\nof annotations Yt,"
        },
        {
          "MTL PU [6]": ""
        },
        {
          "MTL PU [6]": "thereby leading to better capturing label uncertainty in arousal"
        },
        {
          "MTL PU [6]": ""
        },
        {
          "MTL PU [6]": "annotations. This explains the capability of distribution learn-"
        },
        {
          "MTL PU [6]": ""
        },
        {
          "MTL PU [6]": "ing, using the LKL loss term, for modeling label uncertainty in"
        },
        {
          "MTL PU [6]": ""
        },
        {
          "MTL PU [6]": "SER. Also noting here that, in contrast to the baselines, the pro-"
        },
        {
          "MTL PU [6]": ""
        },
        {
          "MTL PU [6]": "posed model learns uncertainty dependent representations in an"
        },
        {
          "MTL PU [6]": ""
        },
        {
          "MTL PU [6]": "end-to-end manner\nfor\nimproved uncertainty estimates,\ninline"
        },
        {
          "MTL PU [6]": "with literature [12] which recommends end-to-end learning for"
        },
        {
          "MTL PU [6]": "uncertainty modeling in SER. Conclusively, the results in-terms"
        },
        {
          "MTL PU [6]": "the ability of\nthe pro-\nof LCCC(m), LCCC(s), and LKL reveal"
        },
        {
          "MTL PU [6]": ""
        },
        {
          "MTL PU [6]": "posed +LU model to best capture label uncertainty, thereby also"
        },
        {
          "MTL PU [6]": ""
        },
        {
          "MTL PU [6]": "improving mean estimations in comparison to the baselines."
        },
        {
          "MTL PU [6]": ""
        },
        {
          "MTL PU [6]": "To further validate the results, we plot\nthe distribution es-"
        },
        {
          "MTL PU [6]": ""
        },
        {
          "MTL PU [6]": "timations\nfor a test\nsubject,\nseen in Figure 2.\nFrom the ﬁg-"
        },
        {
          "MTL PU [6]": ""
        },
        {
          "MTL PU [6]": "ure, as the quantitative results suggest, we see that the MU and"
        },
        {
          "MTL PU [6]": ""
        },
        {
          "MTL PU [6]": "+LU models are superior to the baseline MTL PU in-terms of"
        },
        {
          "MTL PU [6]": ""
        },
        {
          "MTL PU [6]": "the distribution estimations. Speciﬁcally, the +LU captures the"
        },
        {
          "MTL PU [6]": ""
        },
        {
          "MTL PU [6]": "whole distribution of arousal annotations better than the MTL"
        },
        {
          "MTL PU [6]": ""
        },
        {
          "MTL PU [6]": "PU model, by well capturing the time-varying uncertainty with-"
        },
        {
          "MTL PU [6]": ""
        },
        {
          "MTL PU [6]": "out noisy estimates.\nThis further highlights the advantage of"
        },
        {
          "MTL PU [6]": ""
        },
        {
          "MTL PU [6]": "training on LKL loss term."
        },
        {
          "MTL PU [6]": ""
        },
        {
          "MTL PU [6]": "5.2. Comparison between MU and +LU"
        },
        {
          "MTL PU [6]": ""
        },
        {
          "MTL PU [6]": "Between the proposed uncertainty models, we note that +LU"
        },
        {
          "MTL PU [6]": "outperforms MU in terms of LCCC(s), without signiﬁcant degra-"
        },
        {
          "MTL PU [6]": "Speciﬁcally, +LU improves drastically\ndations to LCCC(m)."
        },
        {
          "MTL PU [6]": "and signiﬁcantly in terms of LCCC(s) from 0.076 to 0.340. At"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "8. References": "",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": "M. Valstar,\n“Stochastic Process Regression for Cross-Cultural"
        },
        {
          "8. References": "J. E. LeDoux and S. G. Hofmann, “The subjective experience of",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": "Speech Emotion Recognition,” in Proc. Interspeech 2021, 2021,"
        },
        {
          "8. References": "emotion: a fearful view,” Current Opinion in Behavioral Sciences,",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": "pp. 3390–3394."
        },
        {
          "8. References": "vol. 19, pp. 67–72, 2018.",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": "[20]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne, “Introduc-"
        },
        {
          "8. References": "Z. Lei and N. Lehmann-Willenbrock, “Affect\nin meetings: An",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": "ing the RECOLA multimodal corpus of remote collaborative and"
        },
        {
          "8. References": "interpersonal construct\nin dynamic interaction processes,” in The",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": "affective interactions,” in 10th IEEE Int. Conf. and Workshops on"
        },
        {
          "8. References": "Cambridge handbook of meeting science.\nCambridge University",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": "Automatic Face and Gesture Recognition (FG), 2013, pp. 1–8."
        },
        {
          "8. References": "Press, 2015, pp. 456–482.",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": "[21] N. Raj Prabhu, C. Raman, and H. Hung, “Deﬁning and Quantify-"
        },
        {
          "8. References": "L. Nummenmaa, R. Hari, J. K. Hietanen, and E. Glerean, “Maps",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": "ing Conversation Quality in Spontaneous Interactions,” in Comp."
        },
        {
          "8. References": "the National Academy of\nof subjective feelings,” Proceedings of",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": "Pub. of 2020 Int. Conf. on Multimodal Interaction, 2020, pp. 196–"
        },
        {
          "8. References": "Sciences, vol. 115, no. 37, pp. 9198–9203, 2018.",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": "205."
        },
        {
          "8. References": "two decades in a",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": "[22] M. Abdelwahab and C. Busso, “Active learning for speech emo-"
        },
        {
          "8. References": "nutshell, benchmarks, and ongoing trends,” Communications of",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": "tion recognition using deep neural network,” in 2019 8th Int. Conf."
        },
        {
          "8. References": "the ACM, vol. 61, no. 5, pp. 90–99, Apr. 2018.",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": "on Affective Computing and Intelligent Interaction (ACII).\nIEEE,"
        },
        {
          "8. References": "",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": "2019."
        },
        {
          "8. References": "[5] D. Dukes, K. Abrams, R. Adolphs, M. E. Ahmed, A. Beatty,",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "K. C. Berridge, S. Broomhall, T. Brosch, J. J. Campos, Z. Clay",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": "[23] M. Valstar,\nJ. Gratch, B. Schuller,\nF. Ringeval, D. Lalanne,"
        },
        {
          "8. References": "et al., “The rise of affectivism,” Nature Human Behaviour, pp.",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": "M.\nTorres\nTorres,\nS.\nScherer,\nG.\nStratou,\nR. Cowie,\nand"
        },
        {
          "8. References": "1–5, 2021.",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": "M.\nPantic,\n“AVEC 2016:\nDepression, Mood,\nand Emotion"
        },
        {
          "8. References": "",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": "the\nRecognition Workshop and Challenge,”\nin Proceedings of"
        },
        {
          "8. References": "J. Han, Z. Zhang, Z. Ren, and B. Schuller, “Exploring perception",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": "6th International Workshop on Audio/Visual Emotion Challenge,"
        },
        {
          "8. References": "uncertainty for emotion recognition in dyadic conversation and",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": "ser.\nAVEC\n’16.\nNew\nYork,\nNY,\nUSA:\nAssociation\nfor"
        },
        {
          "8. References": "music listening,” Cognitive Computation, pp. 1–10, 2020.",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": "Computing Machinery,\n2016,\npp.\n3–10.\n[Online]. Available:"
        },
        {
          "8. References": "“Modeling uncertainty in predicting",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": "https://doi.org/10.1145/2988257.2988258"
        },
        {
          "8. References": "emotional attributes from spontaneous speech,” in ICASSP - IEEE",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": "[24] M. Grimm and K. Kroschel, “Evaluation of natural emotions us-"
        },
        {
          "8. References": "Int. Conf. on Acoustics, Speech and Signal Processing, 2020, pp.",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": "ing self assessment manikins,” in IEEE Workshop on Automatic"
        },
        {
          "8. References": "8384–8388.",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": "Speech Recognition and Understanding, 2005.\nIEEE, 2005, pp."
        },
        {
          "8. References": "",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": "381–385."
        },
        {
          "8. References": "P. Tzirakis,\nJ. Zhang, and B. W. Schuller, “End-to-End Speech",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "Emotion Recognition Using Deep Neural Networks,” in ICASSP -",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": "[25] G. Trigeorgis, F. Ringeval, R. Brueckner, E. Marchi, M. A. Nico-"
        },
        {
          "8. References": "Int. Conf. on Acoustics, Speech and Signal Processing, 2018, pp.",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": "laou, B. Schuller, and S. Zafeiriou, “Adieu features?\nend-to-end"
        },
        {
          "8. References": "5089–5093.",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": "speech emotion recognition using a deep convolutional recurrent"
        },
        {
          "8. References": "",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": "network,” in 2016 Int. Conf. on acoustics, speech and signal pro-"
        },
        {
          "8. References": "P. Tzirakis, J. Chen, S. Zafeiriou, and B. Schuller, “End-to-end",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": "cessing (ICASSP).\nIEEE, 2016."
        },
        {
          "8. References": "multimodal affect recognition in real-world environments,” Infor-",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "mation Fusion, vol. 68, pp. 46–53, 2021.",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": "[26]\nI. Goodfellow, Y. Bengio,\nand A. Courville, Deep Learning,"
        },
        {
          "8. References": "",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": "3rd ed., ser. 5.\nThe address of the publisher: MIT Press, 7 2016,"
        },
        {
          "8. References": "J. Han, Z. Zhang, M. Schmitt, M. Pantic, and B. Schuller, “From",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": "vol. 4, ch. 3, pp. 51–77, http://www.deeplearningbook.org."
        },
        {
          "8. References": "hard to soft: Towards more human-like emotion recognition by",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "modelling the perception uncertainty,” in Proceedings of the 25th",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": "[27]\nP. Tzirakis, A. Nguyen, S. Zafeiriou, and B. W. Schuller, “Speech"
        },
        {
          "8. References": "ACM Int. Conf. on Multimedia, 2017, pp. 890–897.",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": "ICASSP\nemotion\nrecognition\nusing\nsemantic\ninformation,”\nin"
        },
        {
          "8. References": "",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": "2021-2021 IEEE International Conference on Acoustics, Speech"
        },
        {
          "8. References": "",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": "and Signal Processing (ICASSP).\nIEEE, 2021, pp. 6279–6283."
        },
        {
          "8. References": "analysis\nin continuous\ninput:\nCurrent\ntrends and future direc-",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "tions,” Image and Vision Computing, vol. 31, no. 2, pp. 120–136,",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": "[28] K. Sridhar, W.-C. Lin, and C. Busso, “Generative approach using"
        },
        {
          "8. References": "2013.",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": "soft-labels to learn uncertainty in predicting emotional attributes,”"
        },
        {
          "8. References": "",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": "in 2021 9th International Conference on Affective Computing and"
        },
        {
          "8. References": "S. Alisamir and F. Ringeval, “On the evolution of speech repre-",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": "Intelligent Interaction (ACII).\nIEEE, 2021, pp. 1–8."
        },
        {
          "8. References": "sentations\nfor affective computing: A brief history and critical",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "overview,” IEEE Signal Processing Magazine, vol. 38, no. 6, pp.",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "12–21, 2021.",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "bayesian deep label distribution learning,” Applied Soft Comput-",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "ing, 2021.",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "S. Kohl, B. Romera-Paredes, C. Meyer, J. De Fauw, J. R. Led-",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "sam, K. Maier-Hein, S. Eslami, D. Jimenez Rezende, and O. Ron-",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "neberger, “A probabilistic u-net\nfor segmentation of ambiguous",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "in Neural\nimages,” Advances\nInformation Processing Systems,",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "vol. 31, 2018.",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "ton, M. Shanahan, Y. W. Teh, D. Rezende,\nand S. A. Eslami,",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "“Conditional neural processes,” in Int. Conf. on Machine Learn-",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "ing.\nPMLR, 2018, pp. 1704–1713.",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "J. Cornebise, K. Kavukcuoglu,\nand D. Wierstra,",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "“Weight uncertainty in neural network,” in International Confer-",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "ence on Machine Learning.\nPMLR, 2015, pp. 1613–1622.",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "tions.”\nJournal of personality and social psychology,\nvol. 67,",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "no. 3, p. 525, 1994.",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "J. A. Russell, “A circumplex model of affect.” Journal of person-",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        },
        {
          "8. References": "ality and social psychology, vol. 39, no. 6, p. 1161, 1980.",
          "[19] M. K. T, E. Sanchez, G. Tzimiropoulos, T. Giesbrecht,\nand": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "The subjective experience of emotion: a fearful view",
      "authors": [
        "J Ledoux",
        "S Hofmann"
      ],
      "year": "2018",
      "venue": "Current Opinion in Behavioral Sciences"
    },
    {
      "citation_id": "3",
      "title": "Affect in meetings: An interpersonal construct in dynamic interaction processes",
      "authors": [
        "Z Lei",
        "N Lehmann-Willenbrock"
      ],
      "year": "2015",
      "venue": "The Cambridge handbook of meeting science"
    },
    {
      "citation_id": "4",
      "title": "Maps of subjective feelings",
      "authors": [
        "L Nummenmaa",
        "R Hari",
        "J Hietanen",
        "E Glerean"
      ],
      "year": "2018",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "5",
      "title": "Speech emotion recognition: two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "6",
      "title": "The rise of affectivism",
      "authors": [
        "D Dukes",
        "K Abrams",
        "R Adolphs",
        "M Ahmed",
        "A Beatty",
        "K Berridge",
        "S Broomhall",
        "T Brosch",
        "J Campos",
        "Z Clay"
      ],
      "year": "2021",
      "venue": "Nature Human Behaviour"
    },
    {
      "citation_id": "7",
      "title": "Exploring perception uncertainty for emotion recognition in dyadic conversation and music listening",
      "authors": [
        "J Han",
        "Z Zhang",
        "Z Ren",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "Cognitive Computation"
    },
    {
      "citation_id": "8",
      "title": "Modeling uncertainty in predicting emotional attributes from spontaneous speech",
      "authors": [
        "K Sridhar",
        "C Busso"
      ],
      "year": "2020",
      "venue": "ICASSP -IEEE Int. Conf. on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "End-to-End Speech Emotion Recognition Using Deep Neural Networks",
      "authors": [
        "P Tzirakis",
        "J Zhang",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "ICASSP -Int. Conf. on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "10",
      "title": "End-to-end multimodal affect recognition in real-world environments",
      "authors": [
        "P Tzirakis",
        "J Chen",
        "S Zafeiriou",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "11",
      "title": "From hard to soft: Towards more human-like emotion recognition by modelling the perception uncertainty",
      "authors": [
        "J Han",
        "Z Zhang",
        "M Schmitt",
        "M Pantic",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "Proceedings of the 25th ACM Int. Conf. on Multimedia"
    },
    {
      "citation_id": "12",
      "title": "Categorical and dimensional affect analysis in continuous input: Current trends and future directions",
      "authors": [
        "H Gunes",
        "B Schuller"
      ],
      "year": "2013",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "13",
      "title": "On the evolution of speech representations for affective computing: A brief history and critical overview",
      "authors": [
        "S Alisamir"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "14",
      "title": "Uncertainty in bayesian deep label distribution learning",
      "authors": [
        "R Zheng",
        "S Zhang",
        "L Liu",
        "Y Luo",
        "M Sun"
      ],
      "year": "2021",
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "15",
      "title": "A probabilistic u-net for segmentation of ambiguous images",
      "authors": [
        "S Kohl",
        "B Romera-Paredes",
        "C Meyer",
        "J De Fauw",
        "J Ledsam",
        "K Maier-Hein",
        "S Eslami",
        "D Jimenez Rezende",
        "O Ronneberger"
      ],
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "16",
      "title": "Conditional neural processes,\" in Int. Conf. on Machine Learning",
      "authors": [
        "M Garnelo",
        "D Rosenbaum",
        "C Maddison",
        "T Ramalho",
        "D Saxton",
        "M Shanahan",
        "Y Teh",
        "D Rezende",
        "S Eslami"
      ],
      "year": "2018",
      "venue": "Conditional neural processes,\" in Int. Conf. on Machine Learning"
    },
    {
      "citation_id": "17",
      "title": "Weight uncertainty in neural network",
      "authors": [
        "C Blundell",
        "J Cornebise",
        "K Kavukcuoglu",
        "D Wierstra"
      ],
      "year": "2015",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "18",
      "title": "Pleasure-arousal theory and the intensity of emotions",
      "authors": [
        "R Reisenzein"
      ],
      "year": "1994",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "19",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "20",
      "title": "Stochastic Process Regression for Cross-Cultural Speech Emotion Recognition",
      "authors": [
        "E Sanchez",
        "G Tzimiropoulos",
        "T Giesbrecht",
        "M Valstar"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "21",
      "title": "Introducing the RECOLA multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "F Ringeval",
        "A Sonderegger",
        "J Sauer",
        "D Lalanne"
      ],
      "year": "2013",
      "venue": "10th IEEE Int. Conf. and Workshops on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "22",
      "title": "Defining and Quantifying Conversation Quality in Spontaneous Interactions",
      "authors": [
        "N Prabhu",
        "C Raman",
        "H Hung"
      ],
      "year": "2020",
      "venue": "Comp. Pub. of 2020 Int. Conf. on Multimodal Interaction"
    },
    {
      "citation_id": "23",
      "title": "Active learning for speech emotion recognition using deep neural network",
      "authors": [
        "M Abdelwahab",
        "C Busso"
      ],
      "year": "2019",
      "venue": "2019 8th Int. Conf. on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "24",
      "title": "AVEC 2016: Depression, Mood, and Emotion Recognition Workshop and Challenge",
      "authors": [
        "M Valstar",
        "J Gratch",
        "B Schuller",
        "F Ringeval",
        "D Lalanne",
        "M Torres",
        "S Scherer",
        "G Stratou",
        "R Cowie",
        "M Pantic"
      ],
      "year": "2016",
      "venue": "Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge, ser. AVEC '16",
      "doi": "10.1145/2988257.2988258"
    },
    {
      "citation_id": "25",
      "title": "Evaluation of natural emotions using self assessment manikins",
      "authors": [
        "M Grimm",
        "K Kroschel"
      ],
      "year": "2005",
      "venue": "IEEE Workshop on Automatic Speech Recognition and Understanding"
    },
    {
      "citation_id": "26",
      "title": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis",
        "F Ringeval",
        "R Brueckner",
        "E Marchi",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2016",
      "venue": "2016 Int. Conf. on acoustics"
    },
    {
      "citation_id": "27",
      "title": "Deep Learning",
      "authors": [
        "I Goodfellow",
        "Y Bengio",
        "A Courville"
      ],
      "year": "2016",
      "venue": "Deep Learning"
    },
    {
      "citation_id": "28",
      "title": "Speech emotion recognition using semantic information",
      "authors": [
        "P Tzirakis",
        "A Nguyen",
        "S Zafeiriou",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "29",
      "title": "Generative approach using soft-labels to learn uncertainty in predicting emotional attributes",
      "authors": [
        "K Sridhar",
        "W.-C Lin",
        "C Busso"
      ],
      "year": "2021",
      "venue": "2021 9th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    }
  ]
}