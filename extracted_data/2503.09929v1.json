{
  "paper_id": "2503.09929v1",
  "title": "Emotion Recognition With Clip And Sequential Learning",
  "published": "2025-03-13T01:02:06Z",
  "authors": [
    "Weiwei Zhou",
    "Chenkun Ling",
    "Zefeng Cai"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Human emotion recognition plays a crucial role in facilitating seamless interactions between humans and computers. In this paper, we present our innovative methodology for tackling the Valence-Arousal (VA) Estimation Challenge, the Expression Recognition Challenge, and the Action Unit (AU) Detection Challenge, all within the framework of the 8th Workshop and Competition on Affective Behavior Analysis in-the-wild (ABAW). Our approach introduces a novel framework aimed at enhancing continuous emotion recognition. This is achieved by fine-tuning the CLIP model with the aff-wild2 dataset, which provides annotated expression labels. The result is a fine-tuned model that serves as an efficient visual feature extractor, significantly improving its robustness. To further boost the performance of continuous emotion recognition, we incorporate Temporal Convolutional Network (TCN) modules alongside Transformer Encoder modules into our system architecture. The integration of these advanced components allows our model to outperform baseline performance, demonstrating its ability to recognize human emotions with greater accuracy and efficiency.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Facial Expression Recognition (FER) holds immense potential across various domains, including emotion detection in videos, enhancing security through facial recognition, and enriching virtual reality experiences. While significant progress has been made in tasks such as face and attribute recognition, the challenge of accurately understanding emotions remains a complex problem.\n\nEmotional expressions often involve subtle nuances, making it difficult to interpret emotions with precision and leading to ambiguity or uncertainty in assessment. This complexity poses challenges in accurately evaluating an individual's emotional state. A key obstacle is the limited scope of current FER datasets, which fail to capture the full spectrum of human emotional expressions, hindering the development of effective models. To enhance the perfor-mance and reliability of FER systems, it is crucial to expand and diversify these datasets.\n\nThe Aff-Wild and Aff-Wild2 datasets, along with their corresponding challenges  [3-10, 12-16, 22] , have played a pivotal role in advancing the field of affective recognition. The Aff-Wild2 dataset includes approximately 600 videos and 3 million frames, annotated with three key affective attributes: a) dimensional affect, including valence and arousal; b) six basic categorical affects; and c) facial action units. To encourage further utilization of the Aff-Wild2 dataset, the 8th ABAW  [11]  competition was organized, focusing on affective behavior analysis in natural, real-world environments.\n\nBuilding upon the notable success of CLIP, we previously investigated its application as a fine-tuned visual feature extractor for facial expression datasets. Subsequently, we integrated Temporal Convolutional Networks (TCN) and Transformer models into our framework for continuous emotion recognition. This methodology yielded substantial improvements in evaluation accuracy across the tasks of Valence-Arousal Estimation, Action Unit Detection, and Expression Recognition.\n\nThe structure of the paper is organized as follows: Section 2 provides a review of the literature on facial emotion recognition. Section 3 outlines the proposed methodology. Section 4 details the experimental setup and presents the results. Finally, Section 5 offers the conclusions drawn from the study.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Prior research has introduced several effective network architectures for the Aff-Wild2 dataset. Kuhnke et al.  [17]  integrated visual and audio information from videos to develop a two-stream network for emotion recognition, demonstrating superior performance. Similarly, Yue Jin et al.  [2]  proposed a transformer-based model to fuse audio and visual features for enhanced recognition accuracy.\n\nNetEase  [23]  leveraged visual information extracted from a Masked Autoencoder (MAE) model, pre-trained in a self-supervised manner on a large-scale face image dataset. Subsequently, the MAE encoder was fine-tuned on the im-age frames from the Aff-Wild2 dataset for the tasks of AU, Expression Recognition, and VA Estimation, which can be characterized as static and unimodal training. Furthermore, multi-modal and temporal information from the video data was incorporated, and a transformer-based framework was employed to effectively fuse these multi-modal features. SituTech  [19]  employed multi-modal feature combinations, derived from various pre-trained models, to capture more comprehensive and effective emotional information.\n\nTemporal Convolutional Networks (TCN) were introduced by Colin Lea et al.  [18] , which effectively capture relationships across multiple time scales, including low-, intermediate-, and high-level temporal dependencies. Building on this, Jin Fan et al.  [1]  proposed a model incorporating a spatial-temporal attention mechanism, designed to capture dynamic internal correlations, with stacked TCN backbones extracting features from various window sizes.\n\nThe Transformer mechanism, first introduced by Vaswani et al.  [21] , has demonstrated exceptional performance across numerous tasks, prompting many researchers to apply it in the domain of affective behavior analysis. Zhao et al.  [24]  developed a model utilizing spatial and temporal Transformers for facial expression analysis. Additionally, Jacob et al.  [20]  proposed a network employing a transformer correlation module to learn the relationships between action units.\n\nBuilding upon these previous works, this paper proposes the use of CLIP as a feature extractor and introduces a model comprising TCN and Transformer components to enhance emotion recognition performance.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "In this section, we provide a comprehensive description of our proposed method for addressing the three challenging tasks of affective behavior analysis in the wild, as outlined in the 8th ABAW Competition: Valence-Arousal Estimation, Expression Recognition, and Action Unit Detection. We detail the design of our model architecture, the data processing techniques employed, and the training strategies implemented for each task.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Fine-Tuning Clip",
      "text": "New fully connected layers are incorporated into the CLIP encoder. During the fine-tuning process, all parameters are updated to optimize the alignment of the feature extractor with the distribution of the Aff-Wild2 dataset. This adjustment ensures that the learned features of the model are better suited to the specific characteristics of the data, thereby enhancing its performance in emotion recognition tasks.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Temporal Convolutional Network",
      "text": "The videos are initially divided into segments with a window size w and stride s. Given the segment window w and stride s, a video with n frames would be split into [n/s] + 1 segments, where the i-th segment contains frames F (i-1) * s+1 , . . . , F (i-1) * s+w .\n\nIn other words, the videos are divided into overlapping chunks, each containing a fixed number of frames. This approach serves to decompose the video into smaller, more manageable segments that are easier to process and analyze. Each chunk overlaps with the previous and subsequent ones to ensure that no information in the video is lost, thereby maintaining continuity across the segments.\n\nWe denote visual features as f i corresponding to the i-th segment extracted by pre-trained and fine-tuned ViT-Base encoder.\n\nThe visual feature is fed into a dedicated Temporal Convolutional Network (TCN) for temporal encoding, which can be formulated as follows:\n\nThis implies that we utilize a specialized type of neural network capable of capturing the temporal patterns and dependencies of the features over time. The Temporal Convolutional Network (TCN) processes the input feature vector by applying a series of convolutional layers, each with varying kernel sizes and dilation rates, to generate an output feature vector. The output vector maintains the same length as the input but encodes richer information about the temporal context. For instance, the TCN can learn how the visual features evolve over time across each segment of the video, thus capturing the dynamic temporal changes inherent in the video data.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Temporal Encoder",
      "text": "We also employ a Transformer encoder to model the temporal information within the video segment, which can be formulated as follows:\n\nThe Transformer encoder models the context within a single segment, thereby neglecting the dependencies between frames across different segments. To address this limitation and capture the inter-segment dependencies, overlapping between consecutive segments can be utilized. This ensures that the temporal relationships between frames across segments are preserved, which implies that s ≤ w, where s is the stride and w is the window size.\n\nWe employ a different type of neural network capable of learning the relationships and interactions among features within each segment. The Transformer encoder processes the input feature vector by applying a series of selfattention layers followed by feed-forward layers, resulting in an output feature vector that holds more semantic meaning and representational power than the input. For instance, the Transformer encoder can learn the spatial relationships between different parts of the image within each segment of the video. However, the Transformer encoder operates within the confines of a single segment, neglecting the interdependencies between different segments of the video. To address this limitation, we introduce overlapping between consecutive segments, such that certain frames are shared by multiple segments. This approach allows the model to capture the temporal relationships and dependencies across segments, thereby enabling a more holistic understanding of the video's content. The degree of overlap is determined by two parameters: s, the length of a segment, and w, the sliding window size. When s is smaller than or equal to w, overlap between consecutive segments occurs, allowing the model to capture temporal dependencies across segments.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Prediction",
      "text": "After the temporal encoder, the features h i are passed through a Multi-Layer Perceptron (MLP) for regression, which can be formulated as follows:\n\nwhere y i are the predictions of i-th segment. For VA challenge, y i ∈ R l×2 . For Expr challenge, y i ∈ R l×8 . For AU challenge, y i ∈ R l×12 .\n\nThe prediction vector contains the estimated values for each segment. The Multi-Layer Perceptron (MLP) consists of several layers of neurons that learn non-linear transformations of the input features. The MLP is trained to minimize the error between the prediction vector and the ground truth vector. The ground truth vector represents the actual values we aim to predict for each segment. Depending on the specific challenge, the structure of the ground truth and prediction vectors varies.\n\nFor the Valence-Arousal (VA) challenge, we predict two values: valence and arousal. Valence indicates the degree of positivity or negativity of an emotion, while arousal reflects the level of activity or passivity of an emotion. In the Expression (Expr) recognition challenge, we predict eight values, each corresponding to one of the basic facial expressions: anger, disgust, fear, happiness, sadness, surprise, neutral, and other expressions. For the Action Unit (AU) detection challenge, we predict twelve values, one for each action unit (AU1, AU2, AU4, AU6, AU7, AU10, AU12, AU15, AU23, AU24, AU25, and AU26).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Loss Functions",
      "text": "For the VA challenge, we use the Concordance Correlation Coefficient (CCC) between the predictions and the ground truth labels as the evaluation metric. CCC measures the correlation between two sequences x and y, and its value ranges between -1 and 1. A value of -1 indicates perfect anti-correlation, 0 represents no correlation, and 1 signifies perfect correlation. The loss function is then calculated as:\n\nwhere cov(x, y) = (xµ x ) * (yµ y )\n\nFor the Expr challenge, we use the cross-entropy loss as the loss function. Cross-entropy loss is commonly used for classification tasks, where it measures the difference between the predicted probability distribution and the true distribution (ground truth):\n\nwhere y ic is a binary indicator (0 or 1) if class c is the correct classification for observation i. p ic is the predicted probability of observation i being in class c, M is the number of classes. The multiclass cross-entropy loss function evaluates how effectively a model predicts the true probabilities for each class in a given observation. It penalizes incorrect predictions by computing the logarithm of the predicted probabilities. A lower loss indicates better model performance, as it signifies that the predicted probabilities are closer to the true probabilities for each class.\n\nFor the AU challenge, we employ BCEWithLogitsLoss as the loss function, which combines a sigmoid layer with binary cross-entropy. BCEWithLogitsLoss is particularly effective for binary classification tasks, where each action unit is treated as an independent binary classification problem. It calculates the binary cross-entropy loss after applying the sigmoid activation function to the logits, allowing the model to output probabilities for each action unit:\n\nwhere N is the number of samples, y i is the target label for sample i, x i is the input logits for sample i, σ is the sigmoid function.\n\nThe advantage of using BCEWithLogitsLoss over BCELoss with a separate sigmoid function is that it combines the sigmoid activation and binary cross-entropy loss into a single, more stable operation. This integration helps to avoid numerical instability, particularly when dealing with extreme values in the logits, thereby improving both the model's performance and training efficiency.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Experiments And Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments Settings",
      "text": "All models were trained on two Nvidia GeForce GTX 3090 GPUs, each equipped with 24GB of memory.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Clip Fine Turning",
      "text": "In the fine-tuning stage of CLIP, we adjusted the batch size to 256 and lowered the learning rate to 0.0001, leveraging the AdamW optimizer.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Task Training",
      "text": "We employed the AdamW optimizer along with a cosine learning rate schedule, incorporating a warmup during the first epoch. The learning rate was set to 3e -5, the weight decay to 1e-5, the dropout probability to 0.3, and the batch size to 32.\n\nFor all three challenges, videos were split using a segment window of w = 300 and a stride of s = 200. This configuration divided each video into segments of 300 frames, with an overlap of 100 frames between consecutive segments. This strategy enabled the model to capture the temporal dynamics of facial expressions and emotions more effectively. visual and audio information, leading to improved accuracy in emotion recognition on this dataset.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Overall Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Ablation Studies",
      "text": "In this section, we present a series of experiments to investigate the significance of each module in our approach, including CLIP fine-tuning, the Temporal Convolutional Network (TCN), and the Temporal Encoder. All experiments were conducted using the official training and validation sets. The results of these experiments are summarized in Table  2 .\n\nCLIP fine-tuning. To assess the effectiveness of CLIP fine-tuning, we conducted an experiment where the finetuning step was removed, and we used the pre-trained CLIP encoder to directly extract features. The results show a significant decrease in the accuracy of all tasks. Specifically, the average CCC for VA dropped from 0.5870 to 0.5049, the average F1 score for Expr decreased from 0.4651 to 0.4123, and the average F1 score for AU reduced from 0.5801 to 0.5274. These findings indicate that CLIP fine-tuning plays a crucial role in leveraging static visual features from individual images, thus providing valuable prior knowledge that aids in learning temporal visual features for improved performance.\n\nTemporal Convolutional Network. To investigate the importance of the Temporal Convolutional Network (TCN), we removed it from the model and observed a noticeable decline in the performance metrics across all tasks. The average CCC for VA decreased from 0.5870 to 0.5698, the average F1 score for Expr dropped from 0.4651 to 0.4473, and the average F1 score for AU fell from 0.5801 to 0.5621. These results demonstrate the effectiveness of the Temporal Convolutional Network in capturing temporal dependencies and highlighting its importance in sequence modeling.\n\nTemporal Encoder. We also conducted an experiment by removing the Transformer Encoder from the model. The results showed a decline in performance across all tasks: the average CCC for VA decreased from 0.5870 to 0.5712, the average F1 score for Expr dropped from 0.4651 to 0.4528, and the average F1 score for AU decreased from 0.5801 to 0.5724. These findings highlight the effectiveness of the Transformer Encoder in capturing and enhancing the model's ability to model temporal dependencies and contextual information.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In summary, our study on human emotion recognition, presented at the 8th Workshop and Competition on Affective Behavior Analysis in-the-wild (ABAW), introduces a novel approach that combines fine-tuned CLIP with the aff-wild2 dataset. By incorporating Temporal Convolutional Network (TCN) and Transformer Encoder modules, our model significantly outperforms baseline performance. These results underscore the effectiveness of our methodology in advancing continuous emotion recognition and its potential to enhance human-computer interaction.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "China Telecom Cloud": "{zhouweiwei,lingchengk,caizf}@chinatelecom.cn"
        },
        {
          "China Telecom Cloud": "Abstract"
        },
        {
          "China Telecom Cloud": ""
        },
        {
          "China Telecom Cloud": "Human emotion recognition plays a crucial role in fa-"
        },
        {
          "China Telecom Cloud": "cilitating seamless interactions between humans and com-"
        },
        {
          "China Telecom Cloud": "puters.\nIn this paper, we present our innovative methodol-"
        },
        {
          "China Telecom Cloud": "ogy for tackling the Valence-Arousal (VA) Estimation Chal-"
        },
        {
          "China Telecom Cloud": "lenge,\nthe Expression Recognition Challenge, and the Ac-"
        },
        {
          "China Telecom Cloud": "tion Unit\n(AU) Detection Challenge, all within the frame-"
        },
        {
          "China Telecom Cloud": "work of the 8th Workshop and Competition on Affective Be-"
        },
        {
          "China Telecom Cloud": "havior Analysis in-the-wild (ABAW)."
        },
        {
          "China Telecom Cloud": "Our\napproach\nintroduces\na\nnovel\nframework\naimed"
        },
        {
          "China Telecom Cloud": "at\nenhancing\ncontinuous\nemotion\nrecognition.\nThis\nis"
        },
        {
          "China Telecom Cloud": "achieved by ﬁne-tuning the CLIP model with the aff-wild2"
        },
        {
          "China Telecom Cloud": "dataset, which provides annotated expression labels.\nThe"
        },
        {
          "China Telecom Cloud": "result is a ﬁne-tuned model that serves as an efﬁcient visual"
        },
        {
          "China Telecom Cloud": "feature extractor, signiﬁcantly improving its robustness. To"
        },
        {
          "China Telecom Cloud": "further boost the performance of continuous emotion recog-"
        },
        {
          "China Telecom Cloud": "nition, we\nincorporate Temporal Convolutional Network"
        },
        {
          "China Telecom Cloud": "(TCN) modules alongside Transformer Encoder modules"
        },
        {
          "China Telecom Cloud": "into our system architecture.\nThe integration of\nthese ad-"
        },
        {
          "China Telecom Cloud": "vanced components allows our model\nto outperform base-"
        },
        {
          "China Telecom Cloud": "line performance, demonstrating its ability to recognize hu-"
        },
        {
          "China Telecom Cloud": "man emotions with greater accuracy and efﬁciency."
        },
        {
          "China Telecom Cloud": ""
        },
        {
          "China Telecom Cloud": ""
        },
        {
          "China Telecom Cloud": "1. Introduction"
        },
        {
          "China Telecom Cloud": ""
        },
        {
          "China Telecom Cloud": "Facial Expression Recognition (FER) holds immense po-"
        },
        {
          "China Telecom Cloud": ""
        },
        {
          "China Telecom Cloud": "tential across various domains, including emotion detection"
        },
        {
          "China Telecom Cloud": "in videos,\nenhancing security through facial\nrecognition,"
        },
        {
          "China Telecom Cloud": ""
        },
        {
          "China Telecom Cloud": "and enriching virtual reality experiences. While signiﬁcant"
        },
        {
          "China Telecom Cloud": "progress has been made in tasks such as face and attribute"
        },
        {
          "China Telecom Cloud": "recognition, the challenge of accurately understanding emo-"
        },
        {
          "China Telecom Cloud": "tions remains a complex problem."
        },
        {
          "China Telecom Cloud": "Emotional\nexpressions\noften\ninvolve\nsubtle\nnuances,"
        },
        {
          "China Telecom Cloud": "making it difﬁcult to interpret emotions with precision and"
        },
        {
          "China Telecom Cloud": "leading to ambiguity or uncertainty in assessment.\nThis"
        },
        {
          "China Telecom Cloud": "complexity poses challenges in accurately evaluating an in-"
        },
        {
          "China Telecom Cloud": "dividual’s emotional state.\nA key obstacle is\nthe limited"
        },
        {
          "China Telecom Cloud": "scope of current FER datasets, which fail\nto capture the"
        },
        {
          "China Telecom Cloud": "full spectrum of human emotional expressions, hindering"
        },
        {
          "China Telecom Cloud": "the development of effective models. To enhance the perfor-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "Expression Recognition, and VA Estimation, which can be",
          "3.2. Temporal Convolutional Network": ""
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "",
          "3.2. Temporal Convolutional Network": "The videos are initially divided into segments with a"
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "characterized as static and unimodal training. Furthermore,",
          "3.2. Temporal Convolutional Network": ""
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "",
          "3.2. Temporal Convolutional Network": "Given the\nsegment win-\nwindow size w and stride\ns."
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "multi-modal and temporal information from the video data",
          "3.2. Temporal Convolutional Network": ""
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "",
          "3.2. Temporal Convolutional Network": "dow w and stride s, a video with n frames would be split"
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "was incorporated, and a transformer-based framework was",
          "3.2. Temporal Convolutional Network": ""
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "",
          "3.2. Temporal Convolutional Network": "into [n/s] + 1 segments, where the i-th segment contains"
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "employed to effectively fuse\nthese multi-modal\nfeatures.",
          "3.2. Temporal Convolutional Network": ""
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "",
          "3.2. Temporal Convolutional Network": "frames(cid:8)F(i−1)∗s+1, . . . , F(i−1)∗s+w(cid:9)."
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "SituTech [19] employed multi-modal feature combinations,",
          "3.2. Temporal Convolutional Network": ""
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "",
          "3.2. Temporal Convolutional Network": "In other words,\nthe videos are divided into overlapping"
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "derived from various pre-trained models,\nto capture more",
          "3.2. Temporal Convolutional Network": ""
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "",
          "3.2. Temporal Convolutional Network": "chunks, each containing a ﬁxed number of\nframes.\nThis"
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "comprehensive and effective emotional information.",
          "3.2. Temporal Convolutional Network": ""
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "",
          "3.2. Temporal Convolutional Network": "approach serves to decompose the video into smaller, more"
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "Temporal Convolutional Networks\n(TCN) were\nintro-",
          "3.2. Temporal Convolutional Network": ""
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "",
          "3.2. Temporal Convolutional Network": "manageable segments that are easier to process and analyze."
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "duced by Colin Lea\net\nal.\n[18], which effectively cap-",
          "3.2. Temporal Convolutional Network": ""
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "",
          "3.2. Temporal Convolutional Network": "Each chunk overlaps with the previous and subsequent ones"
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "ture\nrelationships\nacross multiple\ntime\nscales,\nincluding",
          "3.2. Temporal Convolutional Network": ""
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "",
          "3.2. Temporal Convolutional Network": "to ensure that no information in the video is lost,\nthereby"
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "low-, intermediate-, and high-level temporal dependencies.",
          "3.2. Temporal Convolutional Network": ""
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "",
          "3.2. Temporal Convolutional Network": "maintaining continuity across the segments."
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "Building on this, Jin Fan et al.\n[1] proposed a model incor-",
          "3.2. Temporal Convolutional Network": ""
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "",
          "3.2. Temporal Convolutional Network": "We denote visual features as fi corresponding to the i-th"
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "porating a spatial-temporal attention mechanism, designed",
          "3.2. Temporal Convolutional Network": ""
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "",
          "3.2. Temporal Convolutional Network": "segment extracted by pre-trained and ﬁne-tuned ViT-Base"
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "to capture dynamic internal correlations, with stacked TCN",
          "3.2. Temporal Convolutional Network": ""
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "",
          "3.2. Temporal Convolutional Network": "encoder."
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "backbones extracting features from various window sizes.",
          "3.2. Temporal Convolutional Network": ""
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "",
          "3.2. Temporal Convolutional Network": "The visual feature is fed into a dedicated Temporal Con-"
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "The\nTransformer\nmechanism,\nﬁrst\nintroduced\nby",
          "3.2. Temporal Convolutional Network": "volutional Network (TCN)\nfor\ntemporal encoding, which"
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "Vaswani et al.\n[21], has demonstrated exceptional perfor-",
          "3.2. Temporal Convolutional Network": "can be formulated as follows:"
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "mance across numerous tasks, prompting many researchers",
          "3.2. Temporal Convolutional Network": ""
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "",
          "3.2. Temporal Convolutional Network": "gi = TCN (fi)"
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "to apply it\nin the domain of affective behavior analysis.",
          "3.2. Temporal Convolutional Network": ""
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "Zhao et al.\n[24] developed a model utilizing spatial and",
          "3.2. Temporal Convolutional Network": "This implies that we utilize a specialized type of neu-"
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "",
          "3.2. Temporal Convolutional Network": "ral network capable of capturing the temporal patterns and"
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "temporal Transformers for facial expression analysis. Ad-",
          "3.2. Temporal Convolutional Network": ""
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "ditionally, Jacob et al.\n[20] proposed a network employing",
          "3.2. Temporal Convolutional Network": "dependencies of the features over time. The Temporal Con-"
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "",
          "3.2. Temporal Convolutional Network": "volutional Network (TCN) processes the input feature vec-"
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "a transformer correlation module to learn the relationships",
          "3.2. Temporal Convolutional Network": ""
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "",
          "3.2. Temporal Convolutional Network": "tor by applying a series of convolutional layers, each with"
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "between action units.",
          "3.2. Temporal Convolutional Network": ""
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "",
          "3.2. Temporal Convolutional Network": "varying kernel sizes and dilation rates,\nto generate an out-"
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "Building upon these previous works, this paper proposes",
          "3.2. Temporal Convolutional Network": ""
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "",
          "3.2. Temporal Convolutional Network": "put\nfeature vector.\nThe output vector maintains the same"
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "the use of CLIP as a\nfeature extractor and introduces a",
          "3.2. Temporal Convolutional Network": ""
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "",
          "3.2. Temporal Convolutional Network": "length as the input but encodes richer information about the"
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "model comprising TCN and Transformer components to en-",
          "3.2. Temporal Convolutional Network": ""
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "",
          "3.2. Temporal Convolutional Network": "temporal context. For instance, the TCN can learn how the"
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "hance emotion recognition performance.",
          "3.2. Temporal Convolutional Network": ""
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "",
          "3.2. Temporal Convolutional Network": "visual features evolve over time across each segment of the"
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "",
          "3.2. Temporal Convolutional Network": "video, thus capturing the dynamic temporal changes inher-"
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "",
          "3.2. Temporal Convolutional Network": "ent in the video data."
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "3. Methodology",
          "3.2. Temporal Convolutional Network": ""
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "",
          "3.2. Temporal Convolutional Network": "3.3. Temporal Encoder"
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "In this section, we provide a comprehensive description",
          "3.2. Temporal Convolutional Network": ""
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "of our proposed method for addressing the three challeng-",
          "3.2. Temporal Convolutional Network": "We\nalso employ a Transformer encoder\nto model\nthe"
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "ing tasks of affective behavior analysis in the wild, as out-",
          "3.2. Temporal Convolutional Network": "temporal information within the video segment, which can"
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "lined in the 8th ABAW Competition: Valence-Arousal Es-",
          "3.2. Temporal Convolutional Network": "be formulated as follows:"
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "timation, Expression Recognition, and Action Unit Detec-",
          "3.2. Temporal Convolutional Network": ""
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "tion. We detail the design of our model architecture, the data",
          "3.2. Temporal Convolutional Network": "hi = TransformerEncoder (gi) ."
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "processing techniques employed, and the training strategies",
          "3.2. Temporal Convolutional Network": ""
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "",
          "3.2. Temporal Convolutional Network": "The Transformer\nencoder models\nthe\ncontext within"
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "implemented for each task.",
          "3.2. Temporal Convolutional Network": ""
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "",
          "3.2. Temporal Convolutional Network": "a\nsingle\nsegment,\nthereby\nneglecting\nthe\ndependencies"
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "",
          "3.2. Temporal Convolutional Network": "between\nframes\nacross\ndifferent\nsegments.\nTo\naddress"
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "",
          "3.2. Temporal Convolutional Network": "this limitation and capture the inter-segment dependencies,"
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "3.1. Fine tuning CLIP",
          "3.2. Temporal Convolutional Network": ""
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "",
          "3.2. Temporal Convolutional Network": "overlapping between consecutive segments can be utilized."
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "New fully connected layers\nare\nincorporated into the",
          "3.2. Temporal Convolutional Network": "This ensures that the temporal relationships between frames"
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "CLIP encoder. During the ﬁne-tuning process, all parame-",
          "3.2. Temporal Convolutional Network": "across segments are preserved, which implies that s ≤ w,"
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "ters are updated to optimize the alignment of the feature ex-",
          "3.2. Temporal Convolutional Network": "where s is the stride and w is the window size."
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "tractor with the distribution of the Aff-Wild2 dataset. This",
          "3.2. Temporal Convolutional Network": "We employ a different\ntype of neural network capable"
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "adjustment ensures that\nthe learned features of\nthe model",
          "3.2. Temporal Convolutional Network": "of\nlearning the relationships and interactions among fea-"
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "are better suited to the speciﬁc characteristics of the data,",
          "3.2. Temporal Convolutional Network": "tures within each segment. The Transformer encoder pro-"
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "thereby enhancing its performance in emotion recognition",
          "3.2. Temporal Convolutional Network": "cesses the input feature vector by applying a series of self-"
        },
        {
          "age frames from the Aff-Wild2 dataset for the tasks of AU,": "tasks.",
          "3.2. Temporal Convolutional Network": "attention layers followed by feed-forward layers, resulting"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "in an output feature vector that holds more semantic mean-": "ing and representational power than the input. For instance,",
          "the correlation between two sequences x and y, and its value": "ranges between -1 and 1. A value of\n-1 indicates perfect"
        },
        {
          "in an output feature vector that holds more semantic mean-": "the Transformer encoder can learn the spatial relationships",
          "the correlation between two sequences x and y, and its value": "anti-correlation, 0 represents no correlation, and 1 signiﬁes"
        },
        {
          "in an output feature vector that holds more semantic mean-": "between different parts of\nthe image within each segment",
          "the correlation between two sequences x and y, and its value": "perfect correlation. The loss function is then calculated as:"
        },
        {
          "in an output feature vector that holds more semantic mean-": "of the video. However,\nthe Transformer encoder operates",
          "the correlation between two sequences x and y, and its value": ""
        },
        {
          "in an output feature vector that holds more semantic mean-": "within the conﬁnes of a single segment, neglecting the inter-",
          "the correlation between two sequences x and y, and its value": "2 ∗ cov(x, y)"
        },
        {
          "in an output feature vector that holds more semantic mean-": "",
          "the correlation between two sequences x and y, and its value": "CCC(x, y) ="
        },
        {
          "in an output feature vector that holds more semantic mean-": "dependencies between different segments of the video. To",
          "the correlation between two sequences x and y, and its value": ""
        },
        {
          "in an output feature vector that holds more semantic mean-": "",
          "the correlation between two sequences x and y, and its value": "σ2\nx + σ2\ny + (µx − µy)2"
        },
        {
          "in an output feature vector that holds more semantic mean-": "address this limitation, we introduce overlapping between",
          "the correlation between two sequences x and y, and its value": ""
        },
        {
          "in an output feature vector that holds more semantic mean-": "consecutive segments, such that certain frames are shared",
          "the correlation between two sequences x and y, and its value": "where\ncov(x, y) = P (x − µx) ∗ (y − µy)"
        },
        {
          "in an output feature vector that holds more semantic mean-": "by multiple segments. This approach allows the model\nto",
          "the correlation between two sequences x and y, and its value": ""
        },
        {
          "in an output feature vector that holds more semantic mean-": "capture the temporal relationships and dependencies across",
          "the correlation between two sequences x and y, and its value": "LVA = 1 − CCC"
        },
        {
          "in an output feature vector that holds more semantic mean-": "segments,\nthereby enabling a more holistic understanding",
          "the correlation between two sequences x and y, and its value": ""
        },
        {
          "in an output feature vector that holds more semantic mean-": "",
          "the correlation between two sequences x and y, and its value": "For\nthe Expr challenge, we use the cross-entropy loss"
        },
        {
          "in an output feature vector that holds more semantic mean-": "of the video’s content. The degree of overlap is determined",
          "the correlation between two sequences x and y, and its value": ""
        },
        {
          "in an output feature vector that holds more semantic mean-": "",
          "the correlation between two sequences x and y, and its value": "as the loss function. Cross-entropy loss is commonly used"
        },
        {
          "in an output feature vector that holds more semantic mean-": "the\nby two parameters: s,\nthe length of a segment, and w,",
          "the correlation between two sequences x and y, and its value": ""
        },
        {
          "in an output feature vector that holds more semantic mean-": "",
          "the correlation between two sequences x and y, and its value": "for classiﬁcation tasks, where it measures the difference be-"
        },
        {
          "in an output feature vector that holds more semantic mean-": "sliding window size. When s is smaller than or equal to w,",
          "the correlation between two sequences x and y, and its value": ""
        },
        {
          "in an output feature vector that holds more semantic mean-": "",
          "the correlation between two sequences x and y, and its value": "tween the predicted probability distribution and the true dis-"
        },
        {
          "in an output feature vector that holds more semantic mean-": "overlap between consecutive segments occurs, allowing the",
          "the correlation between two sequences x and y, and its value": ""
        },
        {
          "in an output feature vector that holds more semantic mean-": "",
          "the correlation between two sequences x and y, and its value": "tribution (ground truth):"
        },
        {
          "in an output feature vector that holds more semantic mean-": "model to capture temporal dependencies across segments.",
          "the correlation between two sequences x and y, and its value": ""
        },
        {
          "in an output feature vector that holds more semantic mean-": "3.3.1\nPrediction",
          "the correlation between two sequences x and y, and its value": "1 N\nX i\nM X c\nyic log(pic)\nLEXPR = −"
        },
        {
          "in an output feature vector that holds more semantic mean-": "",
          "the correlation between two sequences x and y, and its value": "=1"
        },
        {
          "in an output feature vector that holds more semantic mean-": "After\nthe\ntemporal\nencoder,\nthe\nare\npassed\nfeatures hi",
          "the correlation between two sequences x and y, and its value": ""
        },
        {
          "in an output feature vector that holds more semantic mean-": "through a Multi-Layer Perceptron (MLP)\nfor\nregression,",
          "the correlation between two sequences x and y, and its value": "is a binary indicator (0 or 1) if class c is the\nwhere yic"
        },
        {
          "in an output feature vector that holds more semantic mean-": "which can be formulated as follows:",
          "the correlation between two sequences x and y, and its value": "correct classiﬁcation for observation i. pic is the predicted"
        },
        {
          "in an output feature vector that holds more semantic mean-": "",
          "the correlation between two sequences x and y, and its value": "probability of observation i being in class c, M is the num-"
        },
        {
          "in an output feature vector that holds more semantic mean-": "yi = MLP(hi)",
          "the correlation between two sequences x and y, and its value": "ber of classes. The multiclass cross-entropy loss function"
        },
        {
          "in an output feature vector that holds more semantic mean-": "",
          "the correlation between two sequences x and y, and its value": "evaluates how effectively a model predicts the true prob-"
        },
        {
          "in an output feature vector that holds more semantic mean-": "where yi are the predictions of i-th segment. For VA chal-",
          "the correlation between two sequences x and y, and its value": ""
        },
        {
          "in an output feature vector that holds more semantic mean-": "",
          "the correlation between two sequences x and y, and its value": "abilities for each class in a given observation.\nIt penalizes"
        },
        {
          "in an output feature vector that holds more semantic mean-": "lenge, yi ∈ Rl×2. For Expr challenge, yi ∈ Rl×8. For AU",
          "the correlation between two sequences x and y, and its value": ""
        },
        {
          "in an output feature vector that holds more semantic mean-": "",
          "the correlation between two sequences x and y, and its value": "incorrect predictions by computing the logarithm of the pre-"
        },
        {
          "in an output feature vector that holds more semantic mean-": "challenge, yi ∈ Rl×12 .",
          "the correlation between two sequences x and y, and its value": ""
        },
        {
          "in an output feature vector that holds more semantic mean-": "",
          "the correlation between two sequences x and y, and its value": "dicted probabilities.\nA lower\nloss\nindicates better model"
        },
        {
          "in an output feature vector that holds more semantic mean-": "The prediction vector contains the estimated values for",
          "the correlation between two sequences x and y, and its value": ""
        },
        {
          "in an output feature vector that holds more semantic mean-": "",
          "the correlation between two sequences x and y, and its value": "performance, as it signiﬁes that\nthe predicted probabilities"
        },
        {
          "in an output feature vector that holds more semantic mean-": "each segment. The Multi-Layer Perceptron (MLP) consists",
          "the correlation between two sequences x and y, and its value": ""
        },
        {
          "in an output feature vector that holds more semantic mean-": "",
          "the correlation between two sequences x and y, and its value": "are closer to the true probabilities for each class."
        },
        {
          "in an output feature vector that holds more semantic mean-": "of several\nlayers of neurons that\nlearn non-linear transfor-",
          "the correlation between two sequences x and y, and its value": ""
        },
        {
          "in an output feature vector that holds more semantic mean-": "",
          "the correlation between two sequences x and y, and its value": "For the AU challenge, we employ BCEWithLogitsLoss"
        },
        {
          "in an output feature vector that holds more semantic mean-": "mations of the input features. The MLP is trained to mini-",
          "the correlation between two sequences x and y, and its value": ""
        },
        {
          "in an output feature vector that holds more semantic mean-": "",
          "the correlation between two sequences x and y, and its value": "as the loss function, which combines a sigmoid layer with"
        },
        {
          "in an output feature vector that holds more semantic mean-": "mize the error between the prediction vector and the ground",
          "the correlation between two sequences x and y, and its value": ""
        },
        {
          "in an output feature vector that holds more semantic mean-": "",
          "the correlation between two sequences x and y, and its value": "binary cross-entropy.\nBCEWithLogitsLoss is particularly"
        },
        {
          "in an output feature vector that holds more semantic mean-": "truth vector. The ground truth vector represents the actual",
          "the correlation between two sequences x and y, and its value": ""
        },
        {
          "in an output feature vector that holds more semantic mean-": "",
          "the correlation between two sequences x and y, and its value": "effective for binary classiﬁcation tasks, where each action"
        },
        {
          "in an output feature vector that holds more semantic mean-": "values we aim to predict for each segment. Depending on",
          "the correlation between two sequences x and y, and its value": ""
        },
        {
          "in an output feature vector that holds more semantic mean-": "",
          "the correlation between two sequences x and y, and its value": "unit is treated as an independent binary classiﬁcation prob-"
        },
        {
          "in an output feature vector that holds more semantic mean-": "the speciﬁc challenge, the structure of the ground truth and",
          "the correlation between two sequences x and y, and its value": ""
        },
        {
          "in an output feature vector that holds more semantic mean-": "",
          "the correlation between two sequences x and y, and its value": "lem.\nIt calculates the binary cross-entropy loss after apply-"
        },
        {
          "in an output feature vector that holds more semantic mean-": "prediction vectors varies.",
          "the correlation between two sequences x and y, and its value": ""
        },
        {
          "in an output feature vector that holds more semantic mean-": "",
          "the correlation between two sequences x and y, and its value": "ing the sigmoid activation function to the logits, allowing"
        },
        {
          "in an output feature vector that holds more semantic mean-": "For the Valence-Arousal (VA) challenge, we predict two",
          "the correlation between two sequences x and y, and its value": ""
        },
        {
          "in an output feature vector that holds more semantic mean-": "",
          "the correlation between two sequences x and y, and its value": "the model to output probabilities for each action unit:"
        },
        {
          "in an output feature vector that holds more semantic mean-": "values: valence and arousal. Valence indicates the degree",
          "the correlation between two sequences x and y, and its value": ""
        },
        {
          "in an output feature vector that holds more semantic mean-": "of positivity or negativity of an emotion, while arousal re-",
          "the correlation between two sequences x and y, and its value": ""
        },
        {
          "in an output feature vector that holds more semantic mean-": "ﬂects the level of activity or passivity of an emotion. In the",
          "the correlation between two sequences x and y, and its value": ""
        },
        {
          "in an output feature vector that holds more semantic mean-": "",
          "the correlation between two sequences x and y, and its value": "1 N\n[yi · log(σ(xi)) + (1 − yi) · log(1 − σ(xi))]\nLAU = −"
        },
        {
          "in an output feature vector that holds more semantic mean-": "Expression (Expr) recognition challenge, we predict eight",
          "the correlation between two sequences x and y, and its value": "X i"
        },
        {
          "in an output feature vector that holds more semantic mean-": "values, each corresponding to one of\nthe basic facial ex-",
          "the correlation between two sequences x and y, and its value": ""
        },
        {
          "in an output feature vector that holds more semantic mean-": "pressions: anger, disgust, fear, happiness, sadness, surprise,",
          "the correlation between two sequences x and y, and its value": ""
        },
        {
          "in an output feature vector that holds more semantic mean-": "",
          "the correlation between two sequences x and y, and its value": "where N is the number of samples, yi"
        },
        {
          "in an output feature vector that holds more semantic mean-": "neutral, and other expressions.\nFor\nthe Action Unit\n(AU)",
          "the correlation between two sequences x and y, and its value": "is the input"
        },
        {
          "in an output feature vector that holds more semantic mean-": "",
          "the correlation between two sequences x and y, and its value": "for sample i, xi"
        },
        {
          "in an output feature vector that holds more semantic mean-": "detection challenge, we predict twelve values, one for each",
          "the correlation between two sequences x and y, and its value": "sigmoid function."
        },
        {
          "in an output feature vector that holds more semantic mean-": "action unit\n(AU1, AU2, AU4, AU6, AU7, AU10, AU12,",
          "the correlation between two sequences x and y, and its value": "The\nadvantage\nof\nusing\nBCEWithLogitsLoss\nover"
        },
        {
          "in an output feature vector that holds more semantic mean-": "AU15, AU23, AU24, AU25, and AU26).",
          "the correlation between two sequences x and y, and its value": ""
        },
        {
          "in an output feature vector that holds more semantic mean-": "",
          "the correlation between two sequences x and y, and its value": "BCELoss with a separate sigmoid function is that\nit com-"
        },
        {
          "in an output feature vector that holds more semantic mean-": "",
          "the correlation between two sequences x and y, and its value": "bines the sigmoid activation and binary cross-entropy loss"
        },
        {
          "in an output feature vector that holds more semantic mean-": "3.4. Loss Functions",
          "the correlation between two sequences x and y, and its value": ""
        },
        {
          "in an output feature vector that holds more semantic mean-": "",
          "the correlation between two sequences x and y, and its value": "into a single, more stable operation. This integration helps"
        },
        {
          "in an output feature vector that holds more semantic mean-": "For\nthe VA challenge, we use the Concordance Corre-",
          "the correlation between two sequences x and y, and its value": "to avoid numerical\ninstability, particularly when dealing"
        },
        {
          "in an output feature vector that holds more semantic mean-": "lation Coefﬁcient\n(CCC) between the predictions and the",
          "the correlation between two sequences x and y, and its value": "with extreme values in the logits,\nthereby improving both"
        },
        {
          "in an output feature vector that holds more semantic mean-": "ground truth labels as the evaluation metric. CCC measures",
          "the correlation between two sequences x and y, and its value": "the model’s performance and training efﬁciency."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: Ablation studies that discuss the significance of CLIP",
      "data": [
        {
          "Task": "",
          "Evaluation Metric": "",
          "Partition": "",
          "Method": "Ours",
          "Fold 0": "0.5615",
          "Fold 1": "0.6102",
          "Fold 2": "0.5025",
          "Fold 3": "0.5712",
          "Fold 4": "0.5504"
        },
        {
          "Task": "Valence",
          "Evaluation Metric": "CCC",
          "Partition": "Validation",
          "Method": "",
          "Fold 0": "",
          "Fold 1": "",
          "Fold 2": "",
          "Fold 3": "",
          "Fold 4": ""
        },
        {
          "Task": "",
          "Evaluation Metric": "",
          "Partition": "",
          "Method": "Baseline",
          "Fold 0": "0.24",
          "Fold 1": "-",
          "Fold 2": "-",
          "Fold 3": "-",
          "Fold 4": "-"
        },
        {
          "Task": "",
          "Evaluation Metric": "",
          "Partition": "",
          "Method": "Ours",
          "Fold 0": "0.6125",
          "Fold 1": "0.5782",
          "Fold 2": "0.6231",
          "Fold 3": "0.6712",
          "Fold 4": "0.6298"
        },
        {
          "Task": "Arousal",
          "Evaluation Metric": "CCC",
          "Partition": "Validation",
          "Method": "",
          "Fold 0": "",
          "Fold 1": "",
          "Fold 2": "",
          "Fold 3": "",
          "Fold 4": ""
        },
        {
          "Task": "",
          "Evaluation Metric": "",
          "Partition": "",
          "Method": "Baseline",
          "Fold 0": "0.20",
          "Fold 1": "-",
          "Fold 2": "-",
          "Fold 3": "-",
          "Fold 4": "-"
        },
        {
          "Task": "",
          "Evaluation Metric": "",
          "Partition": "",
          "Method": "Ours",
          "Fold 0": "0.4651",
          "Fold 1": "0.4274",
          "Fold 2": "0.4562",
          "Fold 3": "0.4485",
          "Fold 4": "0.4702"
        },
        {
          "Task": "Expr",
          "Evaluation Metric": "F1-score",
          "Partition": "Validation",
          "Method": "",
          "Fold 0": "",
          "Fold 1": "",
          "Fold 2": "",
          "Fold 3": "",
          "Fold 4": ""
        },
        {
          "Task": "",
          "Evaluation Metric": "",
          "Partition": "",
          "Method": "Baseline",
          "Fold 0": "0.23",
          "Fold 1": "-",
          "Fold 2": "-",
          "Fold 3": "-",
          "Fold 4": "-"
        },
        {
          "Task": "",
          "Evaluation Metric": "",
          "Partition": "",
          "Method": "Ours",
          "Fold 0": "0.5801",
          "Fold 1": "0.5461",
          "Fold 2": "0.5120",
          "Fold 3": "0.5642",
          "Fold 4": "0.5719"
        },
        {
          "Task": "AU",
          "Evaluation Metric": "F1-score",
          "Partition": "Validation",
          "Method": "",
          "Fold 0": "",
          "Fold 1": "",
          "Fold 2": "",
          "Fold 3": "",
          "Fold 4": ""
        },
        {
          "Task": "",
          "Evaluation Metric": "",
          "Partition": "",
          "Method": "Baseline",
          "Fold 0": "0.39",
          "Fold 1": "-",
          "Fold 2": "-",
          "Fold 3": "-",
          "Fold 4": "-"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: Ablation studies that discuss the significance of CLIP",
      "data": [
        {
          "All models were trained on two Nvidia GeForce GTX": ""
        },
        {
          "All models were trained on two Nvidia GeForce GTX": "3090 GPUs, each equipped with 24GB of memory."
        },
        {
          "All models were trained on two Nvidia GeForce GTX": ""
        },
        {
          "All models were trained on two Nvidia GeForce GTX": ""
        },
        {
          "All models were trained on two Nvidia GeForce GTX": "4.1.1\nCLIP Fine Turning"
        },
        {
          "All models were trained on two Nvidia GeForce GTX": ""
        },
        {
          "All models were trained on two Nvidia GeForce GTX": "In the ﬁne-tuning stage of CLIP, we adjusted the batch size"
        },
        {
          "All models were trained on two Nvidia GeForce GTX": ""
        },
        {
          "All models were trained on two Nvidia GeForce GTX": "to 256 and lowered the learning rate to 0.0001,\nleveraging"
        },
        {
          "All models were trained on two Nvidia GeForce GTX": "the AdamW optimizer."
        },
        {
          "All models were trained on two Nvidia GeForce GTX": ""
        },
        {
          "All models were trained on two Nvidia GeForce GTX": "4.1.2\nTask Training"
        },
        {
          "All models were trained on two Nvidia GeForce GTX": "We employed the AdamW optimizer along with a cosine"
        },
        {
          "All models were trained on two Nvidia GeForce GTX": "learning rate schedule,\nincorporating a warmup during the"
        },
        {
          "All models were trained on two Nvidia GeForce GTX": ""
        },
        {
          "All models were trained on two Nvidia GeForce GTX": "ﬁrst epoch. The learning rate was set to 3e − 5, the weight"
        },
        {
          "All models were trained on two Nvidia GeForce GTX": ""
        },
        {
          "All models were trained on two Nvidia GeForce GTX": "decay to 1e−5, the dropout probability to 0.3, and the batch"
        },
        {
          "All models were trained on two Nvidia GeForce GTX": ""
        },
        {
          "All models were trained on two Nvidia GeForce GTX": "size to 32."
        },
        {
          "All models were trained on two Nvidia GeForce GTX": ""
        },
        {
          "All models were trained on two Nvidia GeForce GTX": "For all\nthree challenges, videos were split using a seg-"
        },
        {
          "All models were trained on two Nvidia GeForce GTX": ""
        },
        {
          "All models were trained on two Nvidia GeForce GTX": "ment window of w = 300 and a stride of s = 200. This con-"
        },
        {
          "All models were trained on two Nvidia GeForce GTX": ""
        },
        {
          "All models were trained on two Nvidia GeForce GTX": "ﬁguration divided each video into segments of 300 frames,"
        },
        {
          "All models were trained on two Nvidia GeForce GTX": ""
        },
        {
          "All models were trained on two Nvidia GeForce GTX": "with an overlap of 100 frames between consecutive seg-"
        },
        {
          "All models were trained on two Nvidia GeForce GTX": ""
        },
        {
          "All models were trained on two Nvidia GeForce GTX": "ments. This strategy enabled the model to capture the tem-"
        },
        {
          "All models were trained on two Nvidia GeForce GTX": ""
        },
        {
          "All models were trained on two Nvidia GeForce GTX": "poral dynamics of facial expressions and emotions more ef-"
        },
        {
          "All models were trained on two Nvidia GeForce GTX": ""
        },
        {
          "All models were trained on two Nvidia GeForce GTX": "fectively."
        },
        {
          "All models were trained on two Nvidia GeForce GTX": ""
        },
        {
          "All models were trained on two Nvidia GeForce GTX": ""
        },
        {
          "All models were trained on two Nvidia GeForce GTX": "4.2. Overall Results"
        },
        {
          "All models were trained on two Nvidia GeForce GTX": ""
        },
        {
          "All models were trained on two Nvidia GeForce GTX": "Table 1 presents the experimental results of our proposed"
        },
        {
          "All models were trained on two Nvidia GeForce GTX": "method on the validation set for the VA, Expr, and AU Chal-"
        },
        {
          "All models were trained on two Nvidia GeForce GTX": "lenges. The Concordance Correlation Coefﬁcient (CCC) is"
        },
        {
          "All models were trained on two Nvidia GeForce GTX": "used as the evaluation metric for valence and arousal pre-"
        },
        {
          "All models were trained on two Nvidia GeForce GTX": "diction, while the F1-score is applied to evaluate the re-"
        },
        {
          "All models were trained on two Nvidia GeForce GTX": "sults of\nthe Expr and AU challenges.\nAs\nshown in the"
        },
        {
          "All models were trained on two Nvidia GeForce GTX": "table, our proposed method signiﬁcantly outperforms the"
        },
        {
          "All models were trained on two Nvidia GeForce GTX": "baseline.\nThese results highlight\nthe effectiveness of our"
        },
        {
          "All models were trained on two Nvidia GeForce GTX": "approach, which combines Temporal Convolutional Net-"
        },
        {
          "All models were trained on two Nvidia GeForce GTX": "works\n(TCN) and Transformer-based models\nto integrate"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: Ablation studies that discuss the significance of CLIP",
      "data": [
        {
          "Baseline": "Table 1. Results for the ﬁve folds of three tasks",
          "0.39\n-\n-\n-\n-": ""
        },
        {
          "Baseline": "4. Experiments and Results",
          "0.39\n-\n-\n-\n-": "Method\nVA\nExpr\nAU"
        },
        {
          "Baseline": "4.1. Experiments Settings",
          "0.39\n-\n-\n-\n-": "baseline\n0.220\n0.230\n0.390"
        },
        {
          "Baseline": "",
          "0.39\n-\n-\n-\n-": "w/o. Fine-tuning\n0.5049\n0.4123\n0.5274"
        },
        {
          "Baseline": "All models were trained on two Nvidia GeForce GTX",
          "0.39\n-\n-\n-\n-": ""
        },
        {
          "Baseline": "",
          "0.39\n-\n-\n-\n-": "w/o. TCN\n0.5698\n0.4473\n0.5621"
        },
        {
          "Baseline": "3090 GPUs, each equipped with 24GB of memory.",
          "0.39\n-\n-\n-\n-": ""
        },
        {
          "Baseline": "",
          "0.39\n-\n-\n-\n-": "w/o. Temporal Encoder\n0.5712\n0.4528\n0.5724"
        },
        {
          "Baseline": "",
          "0.39\n-\n-\n-\n-": "ours\n0.5870\n0.4651\n0.5801"
        },
        {
          "Baseline": "4.1.1\nCLIP Fine Turning",
          "0.39\n-\n-\n-\n-": ""
        },
        {
          "Baseline": "",
          "0.39\n-\n-\n-\n-": "Table 2. Ablation studies that discuss the signiﬁcance of CLIP"
        },
        {
          "Baseline": "In the ﬁne-tuning stage of CLIP, we adjusted the batch size",
          "0.39\n-\n-\n-\n-": "ﬁne-tuning, Temporal Convolutional Network(TCN), and Tempo-"
        },
        {
          "Baseline": "",
          "0.39\n-\n-\n-\n-": "ral Encoder."
        },
        {
          "Baseline": "to 256 and lowered the learning rate to 0.0001,\nleveraging",
          "0.39\n-\n-\n-\n-": ""
        },
        {
          "Baseline": "the AdamW optimizer.",
          "0.39\n-\n-\n-\n-": ""
        },
        {
          "Baseline": "",
          "0.39\n-\n-\n-\n-": "visual and audio information, leading to improved accuracy"
        },
        {
          "Baseline": "4.1.2\nTask Training",
          "0.39\n-\n-\n-\n-": "in emotion recognition on this dataset."
        },
        {
          "Baseline": "We employed the AdamW optimizer along with a cosine",
          "0.39\n-\n-\n-\n-": "4.3. Ablation Studies"
        },
        {
          "Baseline": "learning rate schedule,\nincorporating a warmup during the",
          "0.39\n-\n-\n-\n-": ""
        },
        {
          "Baseline": "",
          "0.39\n-\n-\n-\n-": "In this section, we present a series of experiments to in-"
        },
        {
          "Baseline": "ﬁrst epoch. The learning rate was set to 3e − 5, the weight",
          "0.39\n-\n-\n-\n-": ""
        },
        {
          "Baseline": "",
          "0.39\n-\n-\n-\n-": "vestigate the signiﬁcance of each module in our approach,"
        },
        {
          "Baseline": "decay to 1e−5, the dropout probability to 0.3, and the batch",
          "0.39\n-\n-\n-\n-": ""
        },
        {
          "Baseline": "",
          "0.39\n-\n-\n-\n-": "including CLIP ﬁne-tuning,\nthe Temporal Convolutional"
        },
        {
          "Baseline": "size to 32.",
          "0.39\n-\n-\n-\n-": ""
        },
        {
          "Baseline": "",
          "0.39\n-\n-\n-\n-": "Network (TCN), and the Temporal Encoder.\nAll experi-"
        },
        {
          "Baseline": "For all\nthree challenges, videos were split using a seg-",
          "0.39\n-\n-\n-\n-": ""
        },
        {
          "Baseline": "",
          "0.39\n-\n-\n-\n-": "ments were conducted using the ofﬁcial training and valida-"
        },
        {
          "Baseline": "ment window of w = 300 and a stride of s = 200. This con-",
          "0.39\n-\n-\n-\n-": ""
        },
        {
          "Baseline": "",
          "0.39\n-\n-\n-\n-": "tion sets. The results of these experiments are summarized"
        },
        {
          "Baseline": "ﬁguration divided each video into segments of 300 frames,",
          "0.39\n-\n-\n-\n-": ""
        },
        {
          "Baseline": "",
          "0.39\n-\n-\n-\n-": "in Table 2."
        },
        {
          "Baseline": "with an overlap of 100 frames between consecutive seg-",
          "0.39\n-\n-\n-\n-": ""
        },
        {
          "Baseline": "",
          "0.39\n-\n-\n-\n-": "CLIP ﬁne-tuning. To assess the effectiveness of CLIP"
        },
        {
          "Baseline": "ments. This strategy enabled the model to capture the tem-",
          "0.39\n-\n-\n-\n-": ""
        },
        {
          "Baseline": "",
          "0.39\n-\n-\n-\n-": "ﬁne-tuning, we conducted an experiment where the ﬁne-"
        },
        {
          "Baseline": "poral dynamics of facial expressions and emotions more ef-",
          "0.39\n-\n-\n-\n-": ""
        },
        {
          "Baseline": "",
          "0.39\n-\n-\n-\n-": "tuning step was removed, and we used the pre-trained CLIP"
        },
        {
          "Baseline": "fectively.",
          "0.39\n-\n-\n-\n-": ""
        },
        {
          "Baseline": "",
          "0.39\n-\n-\n-\n-": "encoder to directly extract features. The results show a sig-"
        },
        {
          "Baseline": "",
          "0.39\n-\n-\n-\n-": "niﬁcant decrease in the accuracy of all\ntasks. Speciﬁcally,"
        },
        {
          "Baseline": "4.2. Overall Results",
          "0.39\n-\n-\n-\n-": ""
        },
        {
          "Baseline": "",
          "0.39\n-\n-\n-\n-": "the average CCC for VA dropped from 0.5870 to 0.5049, the"
        },
        {
          "Baseline": "Table 1 presents the experimental results of our proposed",
          "0.39\n-\n-\n-\n-": "average F1 score for Expr decreased from 0.4651 to 0.4123,"
        },
        {
          "Baseline": "method on the validation set for the VA, Expr, and AU Chal-",
          "0.39\n-\n-\n-\n-": "and the average F1 score for AU reduced from 0.5801 to"
        },
        {
          "Baseline": "lenges. The Concordance Correlation Coefﬁcient (CCC) is",
          "0.39\n-\n-\n-\n-": "0.5274. These ﬁndings indicate that CLIP ﬁne-tuning plays"
        },
        {
          "Baseline": "used as the evaluation metric for valence and arousal pre-",
          "0.39\n-\n-\n-\n-": "a crucial\nrole in leveraging static visual\nfeatures from in-"
        },
        {
          "Baseline": "diction, while the F1-score is applied to evaluate the re-",
          "0.39\n-\n-\n-\n-": "dividual\nimages,\nthus providing valuable prior knowledge"
        },
        {
          "Baseline": "sults of\nthe Expr and AU challenges.\nAs\nshown in the",
          "0.39\n-\n-\n-\n-": "that aids in learning temporal visual features for improved"
        },
        {
          "Baseline": "table, our proposed method signiﬁcantly outperforms the",
          "0.39\n-\n-\n-\n-": "performance."
        },
        {
          "Baseline": "baseline.\nThese results highlight\nthe effectiveness of our",
          "0.39\n-\n-\n-\n-": "Temporal Convolutional Network. To investigate the"
        },
        {
          "Baseline": "approach, which combines Temporal Convolutional Net-",
          "0.39\n-\n-\n-\n-": "importance of the Temporal Convolutional Network (TCN),"
        },
        {
          "Baseline": "works\n(TCN) and Transformer-based models\nto integrate",
          "0.39\n-\n-\n-\n-": "we removed it\nfrom the model and observed a noticeable"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "decline in the performance metrics across all\ntasks.\nThe": "average CCC for VA decreased from 0.5870 to 0.5698, the",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "Zafeiriou.\nDistribution matching for heterogeneous multi-"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "arXiv\npreprint\ntask\nlearning:\na\nlarge-scale\nface\nstudy."
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "average F1 score for Expr dropped from 0.4651 to 0.4473,",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "arXiv:2105.03790, 2021."
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "and the average F1 score for AU fell from 0.5801 to 0.5621.",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "[8] Dimitrios Kollias, Panagiotis Tzirakis, Alice Baird, Alan"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "These results demonstrate the effectiveness of the Temporal",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "Cowen, and Stefanos Zafeiriou. Abaw: Valence-arousal esti-"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "Convolutional Network in capturing temporal dependencies",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "mation, expression recognition, action unit detection & emo-"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "and highlighting its importance in sequence modeling.",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "tional reaction intensity estimation challenges.\nIn Proceed-"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "Temporal Encoder. We also conducted an experiment",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "ings of\nthe IEEE/CVF Conference on Computer Vision and"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "by removing the Transformer Encoder from the model. The",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "Pattern Recognition, pages 5888–5897, 2023."
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "results showed a decline in performance across all tasks:\nthe",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "[9] Dimitrios Kollias, Panagiotis Tzirakis, Alice Baird, Alan"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "average CCC for VA decreased from 0.5870 to 0.5712, the",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "Cowen, and Stefanos Zafeiriou. Abaw: Valence-arousal esti-"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "mation, expression recognition, action unit detection & emo-"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "average F1 score for Expr dropped from 0.4651 to 0.4528,",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "tional reaction intensity estimation challenges, 2023."
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "and the average F1 score for AU decreased from 0.5801",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "[10] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "to 0.5724.\nThese ﬁndings highlight\nthe effectiveness of",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "fanos Zafeiriou,\nIrene Kotsia, Alice Baird, Chris Gagne,"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "the Transformer Encoder\nin capturing and enhancing the",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "Chunchang Shao, and Guanyu Hu. The 6th affective behav-"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "model’s ability to model\ntemporal dependencies and con-",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "ior analysis in-the-wild (abaw) competition.\nIn Proceedings"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "textual information.",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "of\nthe IEEE/CVF Conference on Computer Vision and Pat-"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "tern Recognition, pages 4587–4598, 2024. 1"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "5. Conclusion",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "[11] Dimitrios Kollias, Panagiotis Tzirakis, Alan S. Cowen, Ste-"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "fanos Zafeiriou,\nIrene Kotsia, Eric Granger, Marco Peder-"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "In summary, our study on human emotion recognition,",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "soli, Simon L. Bacon, Alice Baird, Chris Gagne, Chun-"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "presented at\nthe 8th Workshop and Competition on Af-",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "chang Shao, Guanyu Hu, Souﬁane Belharbi, and Muham-"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "fective Behavior Analysis in-the-wild (ABAW),\nintroduces",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "mad Haseeb Aslam. Advancements in Affective and Behav-"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "a novel approach that combines ﬁne-tuned CLIP with the",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "ior Analysis: The 8th ABAW Workshop and Competition. 3"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "aff-wild2 dataset.\nBy incorporating Temporal Convolu-",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "2025. 1"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "[12] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "tional Network (TCN) and Transformer Encoder modules,",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "Athanasios Papaioannou, Guoying Zhao, Bj¨orn Schuller,"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "our model signiﬁcantly outperforms baseline performance.",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "These results underscore the effectiveness of our method-",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "in-the-wild: Aff-wild database and challenge, deep architec-"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "ology in advancing continuous emotion recognition and its",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "tures, and beyond. International Journal of Computer Vision,"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "potential to enhance human-computer interaction.",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "pages 1–23, 2019. 1"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "References",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "action unit\nrecognition: Aff-wild2, multi-task learning and"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "arcface. arXiv preprint arXiv:1910.04855, 2019."
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "[1]\nJin Fan, Ke Zhang, Yipan Huang, Yifei Zhu, and Baiping",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "[14] Dimitrios Kollias and Stefanos Zafeiriou.\nAffect analysis"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "Chen. Parallel spatio-temporal attention-based tcn for multi-",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "in-the-wild: Valence-arousal, expressions, action units and a"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "variate time series prediction. Neural Computing and Appli-",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "uniﬁed framework. arXiv preprint arXiv:2103.15792, 2021."
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "cations, pages 1–10, 2021. 2",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "[15] Dimitrios Kollias and Stefanos Zafeiriou. Analysing affec-"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "[2] Yue\nJin, Tianqing Zheng, Chao Gao,\nand Guoqiang Xu.",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "tive behavior in the second abaw2 competition.\nIn Proceed-"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "A multi-modal\nand multi-task\nlearning method\nfor\nac-",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "ings of\nthe IEEE/CVF International Conference on Com-"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "arXiv\npreprint\ntion\nunit\nand\nexpression\nrecognition.",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "puter Vision, pages 3652–3660, 2021."
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "arXiv:2107.04187, 2021. 1",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "[16] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "[3] Dimitrios Kollias.\nAbaw:\nLearning\nfrom synthetic",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "Dhall, Shreya Ghosh, Chunchang Shao,\nand Guanyu Hu."
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "arXiv\npreprint\ndata & multi-task\nlearning\nchallenges.",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "7th abaw competition: Multi-task learning and compound"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "arXiv:2207.01138, 2022. 1",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "expression recognition.\narXiv preprint arXiv:2407.03835,"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "[4] Dimitrios Kollias. Multi-label compound expression recog-",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "2024. 1"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "of\nnition:\nC-expr\ndatabase & network.\nIn Proceedings",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "[17]\nFelix Kuhnke, Lars Rumberg, and J¨orn Ostermann.\nTwo-"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "the IEEE/CVF Conference on Computer Vision and Pattern",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "stream aural-visual affect analysis in the wild.\nIn 2020 15th"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "Recognition, pages 5589–5598, 2023.",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "IEEE International Conference on Automatic Face and Ges-"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "[5] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "ture Recognition (FG 2020), pages 600–605. IEEE, 2020. 1"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "affective behavior\nin the ﬁrst abaw 2020 competition.\nIn",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "[18] Colin Lea, Rene Vidal, Austin Reiter, and Gregory D Hager."
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "2020\n15th\nIEEE International Conference\non Automatic",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "Temporal convolutional networks: A uniﬁed approach to ac-"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "Face and Gesture Recognition (FG 2020)(FG), pages 794–",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "tion segmentation.\nIn Computer Vision–ECCV 2016 Work-"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "800, 2020.",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "shops: Amsterdam, The Netherlands, October 8-10 and 15-"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "[6] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "16, 2016, Proceedings, Part\nIII 14, pages 47–54. Springer,"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "Zafeiriou.\nFace\nbehavior\na\nla\ncarte:\nExpressions,\naf-",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "2016. 2"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "arXiv preprint\nfect and action units\nin a single network.",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": "[19] Chuanhe Liu, Xinjie Zhang, Xiaolong Liu, Tenggan Zhang,"
        },
        {
          "decline in the performance metrics across all\ntasks.\nThe": "arXiv:1910.11111, 2019.",
          "[7] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "tures, and beyond. International Journal of Computer Vision,": ""
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": "pages 1–23, 2019. 1"
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,"
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": ""
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": "action unit\nrecognition: Aff-wild2, multi-task learning and"
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": "arcface. arXiv preprint arXiv:1910.04855, 2019."
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": ""
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": "[14] Dimitrios Kollias and Stefanos Zafeiriou.\nAffect analysis"
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": ""
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": "in-the-wild: Valence-arousal, expressions, action units and a"
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": ""
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": "uniﬁed framework. arXiv preprint arXiv:2103.15792, 2021."
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": ""
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": "[15] Dimitrios Kollias and Stefanos Zafeiriou. Analysing affec-"
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": ""
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": "tive behavior in the second abaw2 competition.\nIn Proceed-"
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": ""
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": "ings of\nthe IEEE/CVF International Conference on Com-"
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": ""
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": "puter Vision, pages 3652–3660, 2021."
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": ""
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": "[16] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav"
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": ""
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": "Dhall, Shreya Ghosh, Chunchang Shao,\nand Guanyu Hu."
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": ""
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": "7th abaw competition: Multi-task learning and compound"
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": ""
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": "expression recognition.\narXiv preprint arXiv:2407.03835,"
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": ""
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": "2024. 1"
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": ""
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": "[17]\nFelix Kuhnke, Lars Rumberg, and J¨orn Ostermann.\nTwo-"
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": ""
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": "stream aural-visual affect analysis in the wild.\nIn 2020 15th"
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": ""
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": "IEEE International Conference on Automatic Face and Ges-"
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": ""
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": "ture Recognition (FG 2020), pages 600–605. IEEE, 2020. 1"
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": ""
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": "[18] Colin Lea, Rene Vidal, Austin Reiter, and Gregory D Hager."
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": ""
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": "Temporal convolutional networks: A uniﬁed approach to ac-"
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": ""
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": "tion segmentation.\nIn Computer Vision–ECCV 2016 Work-"
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": ""
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": "shops: Amsterdam, The Netherlands, October 8-10 and 15-"
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": ""
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": "16, 2016, Proceedings, Part\nIII 14, pages 47–54. Springer,"
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": ""
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": "2016. 2"
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": ""
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": "[19] Chuanhe Liu, Xinjie Zhang, Xiaolong Liu, Tenggan Zhang,"
        },
        {
          "tures, and beyond. International Journal of Computer Vision,": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Liyu Meng, Yuchen Liu, Yuanyuan Deng,\nand Wenqiang": "Jiang. Multi-modal expression recognition with ensemble"
        },
        {
          "Liyu Meng, Yuchen Liu, Yuanyuan Deng,\nand Wenqiang": "method. arXiv preprint arXiv:2303.10033, 2023. 2"
        },
        {
          "Liyu Meng, Yuchen Liu, Yuanyuan Deng,\nand Wenqiang": "[20] Geethu Miriam Jacob and Bj¨orn Stenger. Facial action unit"
        },
        {
          "Liyu Meng, Yuchen Liu, Yuanyuan Deng,\nand Wenqiang": "detection with transformers.\nIn 2021 IEEE/CVF Conference"
        },
        {
          "Liyu Meng, Yuchen Liu, Yuanyuan Deng,\nand Wenqiang": "on Computer Vision and Pattern Recognition (CVPR), pages"
        },
        {
          "Liyu Meng, Yuchen Liu, Yuanyuan Deng,\nand Wenqiang": "7676–7685, 2021. 2"
        },
        {
          "Liyu Meng, Yuchen Liu, Yuanyuan Deng,\nand Wenqiang": "[21] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-"
        },
        {
          "Liyu Meng, Yuchen Liu, Yuanyuan Deng,\nand Wenqiang": "reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia"
        },
        {
          "Liyu Meng, Yuchen Liu, Yuanyuan Deng,\nand Wenqiang": "Polosukhin. Attention is all you need. Advances in neural"
        },
        {
          "Liyu Meng, Yuchen Liu, Yuanyuan Deng,\nand Wenqiang": "information processing systems, 30, 2017. 2"
        },
        {
          "Liyu Meng, Yuchen Liu, Yuanyuan Deng,\nand Wenqiang": "[22]\nStefanos Zafeiriou, Dimitrios Kollias, Mihalis A Nicolaou,"
        },
        {
          "Liyu Meng, Yuchen Liu, Yuanyuan Deng,\nand Wenqiang": "Athanasios Papaioannou, Guoying Zhao,\nand\nIrene Kot-"
        },
        {
          "Liyu Meng, Yuchen Liu, Yuanyuan Deng,\nand Wenqiang": "sia. Aff-wild: Valence and arousal\n‘in-the-wild’challenge."
        },
        {
          "Liyu Meng, Yuchen Liu, Yuanyuan Deng,\nand Wenqiang": "and Pattern Recognition Workshops\nIn Computer Vision"
        },
        {
          "Liyu Meng, Yuchen Liu, Yuanyuan Deng,\nand Wenqiang": "(CVPRW), 2017 IEEE Conference on,\npages 1980–1987."
        },
        {
          "Liyu Meng, Yuchen Liu, Yuanyuan Deng,\nand Wenqiang": "IEEE, 2017. 1"
        },
        {
          "Liyu Meng, Yuchen Liu, Yuanyuan Deng,\nand Wenqiang": "[23] Wei Zhang, Bowen Ma, Feng Qiu,\nand Yu Ding. Multi-"
        },
        {
          "Liyu Meng, Yuchen Liu, Yuanyuan Deng,\nand Wenqiang": "modal facial affective analysis based on masked autoencoder."
        },
        {
          "Liyu Meng, Yuchen Liu, Yuanyuan Deng,\nand Wenqiang": "the IEEE/CVF Conference on Computer\nIn Proceedings of"
        },
        {
          "Liyu Meng, Yuchen Liu, Yuanyuan Deng,\nand Wenqiang": "Vision and Pattern Recognition, pages 5792–5801, 2023. 1"
        },
        {
          "Liyu Meng, Yuchen Liu, Yuanyuan Deng,\nand Wenqiang": "[24] Zengqun Zhao and Qingshan Liu.\nFormer-dfer: Dynamic"
        },
        {
          "Liyu Meng, Yuchen Liu, Yuanyuan Deng,\nand Wenqiang": "facial expression recognition transformer.\nIn Proceedings"
        },
        {
          "Liyu Meng, Yuchen Liu, Yuanyuan Deng,\nand Wenqiang": "of\nthe 29th ACM International Conference on Multimedia,"
        },
        {
          "Liyu Meng, Yuchen Liu, Yuanyuan Deng,\nand Wenqiang": "pages 1553–1561, 2021. 2"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Parallel spatio-temporal attention-based tcn for multivariate time series prediction",
      "authors": [
        "Jin Fan",
        "Ke Zhang",
        "Yipan Huang",
        "Yifei Zhu",
        "Baiping Chen"
      ],
      "year": "2021",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "2",
      "title": "A multi-modal and multi-task learning method for action unit and expression recognition",
      "authors": [
        "Jin Yue",
        "Tianqing Zheng",
        "Chao Gao",
        "Guoqiang Xu"
      ],
      "venue": "A multi-modal and multi-task learning method for action unit and expression recognition",
      "arxiv": "arXiv:2107.04187,2021.1"
    },
    {
      "citation_id": "3",
      "title": "Learning from synthetic data & multi-task learning challenges",
      "authors": [
        "Dimitrios Kollias",
        "Abaw"
      ],
      "year": "2022",
      "venue": "Learning from synthetic data & multi-task learning challenges",
      "arxiv": "arXiv:2207.01138"
    },
    {
      "citation_id": "4",
      "title": "Multi-label compound expression recognition: C-expr database & network",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "5",
      "title": "Analysing affective behavior in the first abaw 2020 competition",
      "authors": [
        "Kollias",
        "E Schulc",
        "Hajiyev",
        "Zafeiriou"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)(FG)"
    },
    {
      "citation_id": "6",
      "title": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "arxiv": "arXiv:1910.11111"
    },
    {
      "citation_id": "7",
      "title": "Distribution matching for heterogeneous multitask learning: a large-scale face study",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Distribution matching for heterogeneous multitask learning: a large-scale face study",
      "arxiv": "arXiv:2105.03790"
    },
    {
      "citation_id": "8",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & emotional reaction intensity estimation challenges",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Alice Baird",
        "Alan Cowen",
        "Stefanos Zafeiriou"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "9",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & emotional reaction intensity estimation challenges",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Alice Baird",
        "Alan Cowen",
        "Stefanos Zafeiriou"
      ],
      "year": "2023",
      "venue": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & emotional reaction intensity estimation challenges"
    },
    {
      "citation_id": "10",
      "title": "The 6th affective behavior analysis in-the-wild (abaw) competition",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Alan Cowen",
        "Stefanos Zafeiriou",
        "Irene Kotsia",
        "Alice Baird",
        "Chris Gagne",
        "Chunchang Shao",
        "Guanyu Hu"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "11",
      "title": "Soufiane Belharbi, and Muhammad Haseeb Aslam. Advancements in Affective and Behavior Analysis: The 8th ABAW Workshop and Competition",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Alan Cowen",
        "Stefanos Zafeiriou",
        "Irene Kotsia",
        "Eric Granger",
        "Marco Pedersoli",
        "Simon Bacon",
        "Alice Baird",
        "Chris Gagne",
        "Chunchang Shao",
        "Guanyu Hu"
      ],
      "year": "2001",
      "venue": "Soufiane Belharbi, and Muhammad Haseeb Aslam. Advancements in Affective and Behavior Analysis: The 8th ABAW Workshop and Competition"
    },
    {
      "citation_id": "12",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Björn Zhao",
        "Irene Schuller",
        "Stefanos Kotsia",
        "Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "13",
      "title": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "14",
      "title": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "arxiv": "arXiv:2103.15792"
    },
    {
      "citation_id": "15",
      "title": "Analysing affective behavior in the second abaw2 competition",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "16",
      "title": "7th abaw competition: Multi-task learning and compound expression recognition",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou",
        "Irene Kotsia",
        "Abhinav Dhall",
        "Shreya Ghosh",
        "Chunchang Shao",
        "Guanyu Hu"
      ],
      "year": "2024",
      "venue": "7th abaw competition: Multi-task learning and compound expression recognition",
      "arxiv": "arXiv:2407.03835"
    },
    {
      "citation_id": "17",
      "title": "Twostream aural-visual affect analysis in the wild",
      "authors": [
        "Felix Kuhnke",
        "Lars Rumberg",
        "Jörn Ostermann"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)"
    },
    {
      "citation_id": "18",
      "title": "Temporal convolutional networks: A unified approach to action segmentation",
      "authors": [
        "Colin Lea",
        "Rene Vidal",
        "Austin Reiter",
        "Gregory Hager"
      ],
      "year": "2016",
      "venue": "Computer Vision-ECCV 2016 Workshops"
    },
    {
      "citation_id": "19",
      "title": "Multi-modal expression recognition with ensemble method",
      "authors": [
        "Chuanhe Liu",
        "Xinjie Zhang",
        "Xiaolong Liu",
        "Tenggan Zhang",
        "Liyu Meng",
        "Yuchen Liu",
        "Yuanyuan Deng",
        "Wenqiang Jiang"
      ],
      "year": "2023",
      "venue": "Multi-modal expression recognition with ensemble method",
      "arxiv": "arXiv:2303.10033"
    },
    {
      "citation_id": "20",
      "title": "Facial action unit detection with transformers",
      "authors": [
        "Miriam Geethu",
        "Björn Jacob",
        "Stenger"
      ],
      "year": "2021",
      "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "21",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "22",
      "title": "Aff-wild: Valence and arousal 'in-the-wild'challenge",
      "authors": [
        "Stefanos Zafeiriou",
        "Dimitrios Kollias",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Irene Zhao",
        "Kotsia"
      ],
      "year": "2017",
      "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference on"
    },
    {
      "citation_id": "23",
      "title": "Multimodal facial affective analysis based on masked autoencoder",
      "authors": [
        "Wei Zhang",
        "Bowen Ma",
        "Feng Qiu",
        "Yu Ding"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "24",
      "title": "Former-dfer: Dynamic facial expression recognition transformer",
      "authors": [
        "Zengqun Zhao",
        "Qingshan Liu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    }
  ]
}