{
  "paper_id": "2410.18373v2",
  "title": "Ugotme: An Embodied System For Affective Human-Robot Interaction",
  "published": "2024-10-24T02:32:56Z",
  "authors": [
    "Peizhen Li",
    "Longbing Cao",
    "Xiao-Ming Wu",
    "Xiaohan Yu",
    "Runze Yang"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Equipping humanoid robots with the capability to understand emotional states of human interactants and express emotions appropriately according to situations is essential for affective human-robot interaction. However, enabling current vision-aware multimodal emotion recognition models for affective human-robot interaction in the real-world raises embodiment challenges: addressing the environmental noise issue and meeting real-time requirements. First, in multiparty conversation scenarios, the noises inherited in the visual observation of the robot, which may come from either 1) distracting objects in the scene or 2) inactive speakers appearing in the field of view of the robot, hinder the models from extracting emotional cues from vision inputs. Secondly, realtime response, a desired feature for an interactive system, is also challenging to achieve. To tackle both challenges, we introduce an affective human-robot interaction system called UGotMe designed specifically for multiparty conversations. Two denoising strategies are proposed and incorporated into the system to solve the first issue. Specifically, to filter out distracting objects in the scene, we propose extracting face images of the speakers from the raw images and introduce a customized active face extraction strategy to rule out inactive speakers. As for the second issue, we employ efficient data transmission from the robot to the local server to improve realtime response capability. We deploy UGotMe on a human robot named Ameca to validate its real-time inference capabilities in practical scenarios. Videos demonstrating real-world deployment are available at https://lipzh5.github.io/HumanoidVLE/",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Humanoid robots and AI  [1]  are increasingly engaging with people, particularly through conversational interactions, for applications like healthcare and services. To enhance the naturalness and fluidity of human-robot conversations, it is essential to endow these robots with the capability to perceive and understand human emotional states and to express emotions in a manner that resembles human behavior. However, enabling current vision-aware multimodal emotion recognition models for affective human-robot interaction raises challenges: addressing the environmental noise issue and meeting real-time requirements.\n\nFirst, in multiparty human-robot conversation scenarios, there can be environmental noise in the visual observation of the robot. As shown in Fig  1 , the noises come from 1) distracting objects in the scene, like the shelf; 2) inactive speakers and other irrelevant persons who appear in the field",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "[Joy]",
      "text": "[neutral]\n\n[joy]",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ameca: Response [Joy]",
      "text": "Jay & Ian irrelevant ✘ Fig.  1 : In a multiparty human-robot conversation scenario, the active speaker (Tim) initiates the conversation by simply stating a fact \"I am still working on my experiments\" while holding a neutral face. The humanoid robot (Ameca) is supposed to deliver a neutral face in response to the speaker. However, two inactive speakers (Tom and Amy) and other persons (Jay and Ian) irrelevant to the conversation holding different facial expressions from the active speaker appear in the field of view of the robot, which may confuse the model, leading to the wrong answer.\n\nof view of the the robot. These noises hinder the model from focusing on emotion-rich facial expressions of active speakers and extracting emotional cues from them. Secondly, realtime response, a desired feature for an interactive system, is also hard to achieve due to factors such as the inference speed of large deep models, communication between system modules or other hardware limitations.\n\nTo tackle both challenges, we introduce an affective human-robot interaction system called UGotMe, designed specifically for multiparty human-robot conversations. Two denoising strategies are proposed and incorporated into the system to address the issue of environmental noise. Specifically, to filter out distracting objects in the scene, such as moving vehicles and stationary furniture, we propose extracting the faces of the speakers from raw images captured by the robot's onboard camera. To rule out inactive speakers or persons irrelevant to the conversation, we employ a customized active face extraction strategy. In particular, we adjust the robot's head pose and left-eye camera according to the direction of sound arrival to ensure that the active speaker is centered along the x-axis of the image. This approach allows us to accurately extract the corresponding faces. Inspired by  [2] , we apply person-specific neutral normalization to the extracted face images to account for significant individual differences. As for the second challenge, to improve real-time response capability of the system, we stream the captured images in bytes from the robot to the local server continuously in a separate thread and buffer the latest T frames, so that they can be consumed by models located on the server immediately. An overview of the system is shown in Fig.  2 , where two main stages are on-robot mulitmodal perception and on-edge vision-language to emotion modeling.\n\nIn particular, emotion recognition module of the system, the proposed Vision-Language to Emotion (VL2E) model, is designed to be compatible with the aforementioned denoising strategies, i.e., the vision encoder of VL2E consumes face sequences and extracts features at the frame level. It models intra-modal interactions within visual features using selfattention transformer. In addition, to ensure the robot can understand human emotions in a manner similar to human interaction, we incorporate context modeling by considering the most recent k utterances in the dialogue when calculating textual representations. We employ multimodal transformer to fuse features from visual and textual modalities. On the MELD dataset, VL2E outperforms all baselines in weighted average F1. An illustration of VL2E is shown in Fig.  3 . We aim to enable humanoid robots to convey emotions via facial expressions, as they are recognized as a highly effective means of communicating affective information compared to verbal cues and tone  [3] . Therefore, upon emotion recognition, we directly map them to robotic facial expressions in line with parallel empathy (generating the same emotion as the peer)  [4] . In the present study, we only have access to robotic facial expressions within a predefined set, corresponding to seven basic emotions (including neutral, surprise, fear, sadness, joy, disgust, and anger). These expressions can be applied to the humanoid robot named Ameca, which was developed by Engineered Arts. We deploy UGotMe on Ameca, demonstrating its real-time inference capabilities in practical scenarios.\n\nOur main contributions are summarized as: 1) we introduce UGotMe, an affective human-robot interaction system designed to address the environmental noise issue and meet real-time requirements, thereby facilitating immediate emotional exchanges between humans and humanoid robots; 2) we propose a vision-language to emotion (VL2E) model, which can be embedded into the embodied system and also outperforms all baselines on the MELD dataset  [5] ; 3) we deploy UGotMe on a physical humanoid robot named Ameca and demonstrate its practical usage in the real world.",
      "page_start": 1,
      "page_end": 6
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Emotion Recognition for Affective Human-Robot Interaction Research studies on emotion recognition for affective human-robot interaction can be roughly divided into two categories: vision-based and multimodal methods. Visionbased methods focus on facial expressions of human interactants and utilize them to recognize emotions  [6] ,  [7] ,  [8] ,  [9] ,  [10] ,  [11] ,  [12] . However, these methods overlook the potential insights from other modalities, such as text and audio. Although some multimodal emotion recognition methods exist for affective human-robot interaction  [13] ,  [14] ,  [15] , they are not well-suited for multiparty conversation scenarios. This is because: 1) they fail to account for potential noise from the presence of multiple participants, and 2) they do not fully leverage the dialogue context, which is crucial for accurate emotion recognition in conversations  [16] . We address both limitations by introducing an affective humanrobot interaction system called UGotMe.\n\nVision-Aware Multimodal Emotion Recognition in Conversation There are many vision-aware multimodal models for emotion recognition in conversation (ERC)  [17] ,  [18] . By \"vision-aware\", we mean that the model's input modalities include visual data. For example, TelME considers different contributions of text and non-verbal modalities (vision and audio), and incorporates cross-modal knowledge distillation to improve the efficacy of weak non-verbal modalities. However, these models do not account for environmental noises, particularly distracting facial expressions of inactive speakers in multiparty conversation scenarios, which impair their performance in real-world deployments. On the contrary, some models pay attention to environmental noises and try to alleviate them by extracting faces of speakers  [19] ,  [20] ,  [21] , though some of them do not consider real-time settings. For instance, FacialMMT  [20]  takes both previous and future conversational turns as inputs to compute textual representations of the current turn, which makes it infeasible for real-world deployment where future turns are not accessible. Motivated by these approaches, we propose a Vision-Language to Emotion (VL2E) model designed to facilitate real-world affective human-robot interactions.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Iii. Method",
      "text": "We aim to enable current vision-aware multimodal emotion recognition models for affective human-robot interaction in the real world and introduce an interaction system called UGotMe to tackle the embodiment challenges, specifically addressing the environmental noise issue and meeting realtime requirements. An overview of UGotMe is provided in Fig.  2 . The working pipeline of the system can be divided into three phases: on-robot multimodal perception, on-edge vision-language to emotion modeling, and robotic facial expression execution. In the on-robot multimodal perception phase, RGB images captured by the robot's onboard camera and audio signals from microphone are collected. Dialogue texts are transcribed from audio using Google Cloud speechto-text service, which are crucial for modeling conversation context. During the on-edge vision-language to emotion modeling phase, vision and language data are transmitted to the local server, where we apply denoising strategies to filter out distracting objects and inactive speakers from raw visual perceptions. Emotional states of human interactants are then recognized based on visual and textual inputs in this phase. Robotic facial expressions are generated by directly mapping the recognized emotional states to the robotic facial expression within a predefined set. In the robotic facial expression execution phase, the humanoid robot will execute the generated expression in line with parallel empathy (generating the same emotion as the peer)  [4] . To facilitate real-time response, we stream the captured images in bytes continuously in a separate thread, so that they can be consumed by the model immediately.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "A. Denoising Strategies",
      "text": "We propose two denoising strategies-face extraction and customized active face extraction-to address the embodiment issue related to environmental noise in multiparty conversation scenarios. Specifically, to filter out distracting objects in the scene such as moving vehicles and stationary furniture, we propose extracting the faces of the speakers from the raw RGB images captured by the robot's onboard (left-eye) camera. Customized active face extraction aims to extract the faces of active speakers who are conversing with the robot by leveraging its characteristics. In particular, to exclude inactive speakers who may be present in the robot's field of view but are not speaking, we propose to adjust the robot's head pose and left-eye camera according to the direction of sound arrival so that it can focus directly on the active speaker. We use MTCNN  [22]  for face detection. Among possible faces detected from the image, the face centered along the x-axis will be considered as the active speaker's face and subsequently extracted for further processing. Note that for model training and evaluation on datasets, we extract faces using OpenFace toolkit  [23]  and all extracted faces are utilized. Inspired by  [2] , we apply person-specific neutral normalization to account for significant individual differences. Specifically, given a sequence of face images for one utterance, we subtract the corresponding neutral face and feed delta images  [24]  to the vision encoder. During model training and evaluation on the MELD dataset, we manually select one neutral face for each leading role, while treating the first frame of the face sequence as the neutral face for others. In real-world deployments, where we may not always have access to the neutral faces of the speakers, we also use the first frame as the neutral face.We will try to apply or design more reasonable techniques for neural state estimation in our future work.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Efficient Data Transmission",
      "text": "Due to hardware restrictions, multimodal robotic sensory data has to be transmitted from the robot to a local server for further processing. The inevitable round-trip latency hinders the system from achieving real-time response. As a mitigation, we stream the captured RGB images in bytes to the server continuously in a separate thread, while buffering the most recent T frames (T = 640 in our implementation, corresponding to a streaming rate of 25 FPS). Textual data is transmitted only when conversational turns are generated. In this study, we use the ZMQ  1  Python library to transmit both visual and textual data via TCP, enabling real-time responses.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Emotion Recognition In Conversation",
      "text": "The Vision-Language to Emotion (VL2E) model dedicated to emotion recognition in multiparty conversation is designed to be compatible with the aforementioned denoising strategies. As shown in Fig.  3 , the vision encoder of the VL2E model processes face sequences of active speakers (individuals who are currently conversing with the robot) and extracts visual features at the frame level. In this way, the model can ignore the presence of irrelevant environmental noise and better focus on the emotion-rich facial expressions. In the present study, the InceptionResnetv1 model  [25]  pretrained on the CASIA-WebFace dataset  [26]  is utilized as the visual encoder. Extracted visual features, denoted as F v t , for utterance t will be fed into a self-attention transformer to model intra-modal interactions. To account for conversation context, we conduct context modeling when calculating the textual representation for the current utterance u t (where s t refers to the corresponding speaker), taking into consideration the most recent k turns  [27] :\n\nTo help the model distinguish between the context and the target turn, we adopt an approach inspired by prompt learning  [28] , appending a prompt P t to the conversation context for the t-th turn. The prompt P t is defined as follows:\n\nIn the present study, we employ the SimCSE  [29]  model as our text encoder, with the full input to the encoder being C t ⊕ P t , where ⊕ refers to the concatenation operation. The last hidden state H k t = SimCSE(C t ⊕ P t ) will be used as the context embedding, which differs from previous studies  [17] ,  [27] , where only the embeddings of the special token ⟨mask⟩ from H k t are leveraged. Note that we include speaker names during model training and evaluation on datasets, but not for embodied usage since speaker identities are not always accessible in real-world deployment. Therefore, conversation context and prompt in real-world deployment will be\n\nand Pt = for u t , speaker feels ⟨mask⟩, respectively.\n\nTo better model inter-modal interactions, we leverage the crossmodal transformer  [30]  to perform multimodal fusion once the unimodal features are extracted.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Experiments",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Setup",
      "text": "In real-world deployments of UGotMe in multiparty conversation scenarios, a humanoid robot converses with several human participants (one person at a time). It is tasked with recognizing human emotions based on conversation context and visual observation and executing the appropriate robotic facial expressions. The robot only has access to a predefined set of facial expressions that correspond to seven basic emotions: neutral, surprise, fear, sadness, joy, disgust, and anger. We have designed a script for volunteers to engage in conversations with the robot. The script is used through all real-world experiments. GPT-based language models are utilized to generate textual responses for the robot. The textto-speech service is provided by Amazon Web Services.\n\nAll experiments on the dataset are conducted on a single NVIDIA H100 GPU. We use AdamW optimizer for VL2E model training and apply cosine schedule with warmup as the learning rate scheduler.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Dataset",
      "text": "To train and evaluate the proposed VL2E model, we use MELD  [5] , a multiparty dataset comprising over 13,000 utterances extracted from Friends TV series. It also provides emotion annotations for each utterance with seven emotion categories including neutral, surprise, fear, sadness, joy, disgust, and anger.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Evaluation",
      "text": "For the proposed VL2E and other comparison methods, we use the weighted average F1 score as the evaluation metric on the class-imbalanced dataset, MELD.\n\nFor the embodied system, UGotMe, the evaluation metrics include emotion response accuracy and user experience (rated on a scale from 0 to 10, with higher scores indicating better performance). Both metrics are assessed by human raters with four raters included in this study. Emotion response accuracy is first calculated within each dialogue and then averaged across dialogues for each rater. The final scores, averaged across all raters, will be reported.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Baselines",
      "text": "On the MELD dataset, we evaluate the proposed VL2E model by comparing it to the following models for ERC: DialogueRNN  [31]  models the speaker identity, conversation context and the corresponding emotion with RNN. ConGCN  [32]  is a Graph Convolutional Network (GCN)based model used to represent utterances, speakers and their relationships. MMGCN  [33]  is a model based on multimodal fused GCN that can effectively utilize both multimodal and long-distance contextual information. DAG-ERC  [34]  is a directed acyclic neural network designed to better model the intrinsic structure within a conversation. MM-DFN  [35]  introduces a graph-based dynamic fusion module to fuse multimodal contextual features. M2FNet  [21]  employs a multi-head attention-based fusion mechanism to enhance the integration of multimodal features. EmoCaps  [19]  extracts multimodal emotion vectors using a structure named Emoformer. UniMSE  [18]  unifies multimodal sentiment analysis and ERC task through a T5-based framework  [36] . GA2MIF  [36]  is a two-stage multi-source information fusion approach based on graph and attention. FacialMMT  [20]  is a two-stage multimodal multi-task frame work that focuses on extracting faces sequences of active speakers and leverage frame-level facial expression recogniton tasks to help utterance-level emotion recognition. TelME  [17]  incorporates cross-modal knowledge distillation to optimize the efficacy of weak modalities. [joy]\n\n[angry]\n\nDylon cheated me again…\n\n[angry]\n\nactive inactive\n\n[joy]\n\n[sadness]\n\n[joy] active inactive (a) Ameca responds with an angry face when the active speaker said \"Dylon cheated me again, I don't wanna talk to him anymore\" in anger.\n\n[joy]\n\n[angry]\n\nDylon cheated me again…\n\n[angry]\n\nactive inactive\n\n[joy]\n\n[sadness]\n\n[joy] active inactive (b) Ameca responds with a joyful face when the active speaker said \"My paper was accepted finally\" in joy.  In the embodied system, UGotMe, we compare VL2E against TelME * , a reproduced version without audio component to verify the effectiveness of our proposed denoising strategies.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "V. Results",
      "text": "In this section, we aim to answer the following questions: 1) How well can VL2E perform on the dataset compared with other methods proposed for emotion recognition in conversation (ERC)? 2) How useful are the denoising strategies for both ERC on datasets and real-world human-robot conversations? 3) How well will UGotMe work in real-world affective human-robot interaction?",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Benchmark Results",
      "text": "The performance of VL2E in comparison to other methods on the MELD dataset is summarized in Table  I . The baselines tagged with * denote reproduced versions, where we remove audio component to ensure consistency in the number of input modalities compared to VL2E. These baselines are also evaluated in real-world deployment scenarios. Other baselines represent the original models, and their results are obtained from referenced papers. VL2E outperforms all baselines on MELD even though most of them employ 3 modalities in the comparison experiments. As can be seen, there is an increase of +10.26% F1 compared to Dia-logueRNN; +7.89% F1 compared to ConGCN; +8.64% F1 compared to MMGCN; +8.35% F1 compared to GA2MIF; and +0.71% F1 compared to FacialMMT. We attribute the higher performance of VL2E to the effective utilization of visual emotional cues through face extraction and personspecific neutral normalization.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Ablations",
      "text": "We conduct an ablation study to inspect the impact of each modality, and verify the effectiveness of face extraction and person-specific neutral normalization. The results are",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "✘",
      "text": "The movie we saw last light is really impressive. That's awesome. What movie did you watch? You jump, I jump.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Inactive [Sadness]",
      "text": "[joy]   summarized in Table  III . As can be observed, removing either the text or vision modality will impair performance. There is a decrease of 1.34% in F1 score when face extraction is not applied, and visual features are extracted from the entire video frame instead. This highlights the significance of focusing on emotion-rich facial expressions. Additionally, the findings underscore the need for person-specific neutral normalization.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Real-World Deployment",
      "text": "We deploy UGotMe on a humanoid robot named Ameca to demonstrate its practical usage in the real world. To improve overall user experience and ensure fair comparison, we use GPT-4o as an auxiliary model for vision-only emotion recognition through all comparison experiments. Two examples of real-world execution are shown in Fig.  4 . We also substitute the emotion recognition model in UGotMe, VL2E, with TelME and compare it against the original version. We use UGotMe-VL2E and UGotMe-TelME to distinguish between the two system versions, with UGotMe referring to UGotMe-VL2E unless otherwise specified. Comparison results in terms of emotion response accuracy and user experience are summarized in Table  II . We do not apply the customized active face extraction in UGotMe-VL2E 1 , using it as one of our baselines to validate the effectiveness of the customized face extraction strategy. As can be seen, UGotMe-VL2E outperforms all baselines in both accuracy and user experience score with an increase of 26.66% in accuracy and 1.74 in user experience score compared to UGotMe-TelME, an increase of 18.21% in accuracy and 1.26 in user experience compared to UGotMe-VL2E 1 . These results validate the effectiveness of our denoising strategies and demonstrate the feasibility of deploying the VL2E model in an embodied system. Fig.  5  shows two cases where UGotMe-TelME fails while UGotME-VL2E successes. In both cases, the inactive speaker (indicated by the green bounding box) displays a different emotion compared to the active speaker (indicated by the red bounding box). Distraction from facial expressions can significantly impact emotion recognition when no denoising strategies are applied, potentially leading to incorrect conclusions.\n\nModels for emotion recognition are hosted on a local server with one NVIDIA RTX 4090 GPU. Videos can be viewed at https://lipzh5.github.io/HumanoidVLE/.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "This paper presents UGotMe, an embodied system for affective human-robot interaction. Our key novelty is addressing embodiment issues for multimodal emotion recognition models by designing denoising strategies customized for humanoid robot (Ameca, in the present study) in multiparty conversation scenarios. We also propose a multimodal emotion recognition model namded VL2E, designed to be compatible with denoising strategies. VL2E can be integrated into UGotMe for practical applications and demonstrates superiority over all baselines on the MELD dataset. Realworld deployment experiments demonstrate that UGotMe effectively provides appropriate emotional responses to human interactants while maintaining a positive user experience, even in the presence of distracting factors.\n\nUGotMe has a number of limitations that could be addressed by future work. First, robotic facial expressions are used to deliver emotions and generated in line with parallel empathy, i.e., generating the same emotion as the peer. Future work could explore new models to enable humanoid robots generating emotions in response to peer's emotion (in line with reactive empathy  [4] ). Furthermore, acoustic signals are not utilized and incorporated into the model to help understand human emotions. Future work will investigate the possibility of transmitting acoustic signals from our humanoid robot, Ameca, and further incorporating them into our future reactive emotion generation model.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , the noises come from 1)",
      "page": 1
    },
    {
      "caption": "Figure 1: In a multiparty human-robot conversation scenario,",
      "page": 1
    },
    {
      "caption": "Figure 2: , where two main stages are",
      "page": 2
    },
    {
      "caption": "Figure 2: The working pipeline of the system can be divided",
      "page": 2
    },
    {
      "caption": "Figure 2: An overview of UGotMe, the proposed affective human-robot interaction system. The working pipeline includes",
      "page": 3
    },
    {
      "caption": "Figure 1: facilitate real-time response, we stream the captured images",
      "page": 3
    },
    {
      "caption": "Figure 3: , the vision encoder of the",
      "page": 3
    },
    {
      "caption": "Figure 3: An illustration of the VL2E model.",
      "page": 4
    },
    {
      "caption": "Figure 4: Two examples of real-world execution of UGotMe. Active speakers who are conversing with Ameca are indicated",
      "page": 5
    },
    {
      "caption": "Figure 5: A comparison between UGotMe-TelME and UGotMe-VL2E. In both cases, the inactive speaker has a sad expression,",
      "page": 6
    },
    {
      "caption": "Figure 5: shows two cases where UGotMe-TelME fails while",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Ameca:": "response"
        },
        {
          "Ameca:": "[joy]"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Tom: inactive": "[joy]"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Tim: active": "[neutral]"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Amy: inactive": "[joy]"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "VL2E T+V 80.31 61.03 14.71 46.28 65.33 28.57 55.07 67.29\na a c c ti t v iv e e in in a a c c t t iv iv e e active in in a a c c t t iv iv e e\nactive\n[a [a n n g g ry ry ] ] [a [a n n g g r r y y ] ] [j [ o jo y y ] ] [j [ o jo y y ] ] [j [ o jo y y ] ] [s [s a a d d n n e e s s s s ] ]\n(a) Ameca responds with an angry face when the active speaker (b) Ameca responds with a joyful face when the active speaker\nsaid“Dyloncheatedmeagain,Idon’twannatalktohimanymore” said “My paper was accepted finally” in joy.\nin anger.\nFig. 4: Two examples of real-world execution of UGotMe. Active speakers who are conversing with Ameca are indicated\nby the red bounding boxes.\nDylon cheated me again…\nDylon cheated me again…\nTABLEII:Comparisonresultsintermsofemotionresponse TABLE III: Ablation study of VL2E based on F1 score.\naccuracy and user experience score.\nVL2E 67.29\nSystems Accuracy↑ Userexperience↑ -w/otext 35.97\nUGotMe-TelME 50.63 6.15 -w/ovision 65.56\nUGotMe-VL2E1 59.08 6.63 -w/ofaceextraction 65.95\n-w/oneutralnormalization 66.60\nUGotMe-VL2E 77.29 7.89": "",
          "Column_2": "a a c c ti t v iv e e in in a a c c t t iv iv e e active in in a a c c t t iv iv e e\nactive\n[a [a n n g g ry ry ] ] [a [a n n g g r r y y ] ] [j [ o jo y y ] ] [j [ o jo y y ] ] [j [ o jo y y ] ] [s [s a a d d n n e e s s s s ] ]\n(a) Ameca responds with an angry face when the active speaker (b) Ameca responds with a joyful face when the active speaker\nsaid“Dyloncheatedmeagain,Idon’twannatalktohimanymore” said “My paper was accepted finally” in joy.\nin anger.\nFig. 4: Two examples of real-world execution of UGotMe. Active speakers who are conversing with Ameca are indicated\nby the red bounding boxes.\nDylon cheated me again…\nDylon cheated me again…\nTABLEII:Comparisonresultsintermsofemotionresponse TABLE III: Ablation study of VL2E based on F1 score.\naccuracy and user experience score.\nVL2E 67.29\nSystems Accuracy↑ Userexperience↑ -w/otext 35.97\nUGotMe-TelME 50.63 6.15 -w/ovision 65.56\nUGotMe-VL2E1 59.08 6.63 -w/ofaceextraction 65.95\n-w/oneutralnormalization 66.60\nUGotMe-VL2E 77.29 7.89"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "active\nactive": "[angry]\n[angry]"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[joy]": "",
          "Column_2": "active\n[joy]"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "Dylon cheated me again…\nDylon cheated me again…"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "[joy]"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "inactive": "[sadness]"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "[sadness]"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "active": "[joy]"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "inactive": "[sadness]"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "[joy]"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "inactive": "[sadness]"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "[sadness]"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "active": "[joy]"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "inactive": "[sadness]"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "active": "[joy]"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "AI robots and humanoid AI: review, perspectives and directions",
      "authors": [
        "L Cao"
      ],
      "year": "2024",
      "venue": "CoRR"
    },
    {
      "citation_id": "2",
      "title": "Cross-dataset learning and person-specific normalisation for automatic action unit detection",
      "authors": [
        "T Baltrušaitis",
        "M Mahmoud",
        "P Robinson"
      ],
      "year": "2015",
      "venue": "IEEE international conference and workshops on automatic face and gesture recognition (FG)"
    },
    {
      "citation_id": "3",
      "title": "Communication without words",
      "authors": [
        "A Mehrabian"
      ],
      "year": "2017",
      "venue": "Communication theory"
    },
    {
      "citation_id": "4",
      "title": "Empathy: A social psychological approach",
      "authors": [
        "M Davis"
      ],
      "year": "2018",
      "venue": "Empathy: A social psychological approach"
    },
    {
      "citation_id": "5",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "6",
      "title": "Affective human-robot interaction with multimodal explanations",
      "authors": [
        "H Zhu",
        "C Yu",
        "A Cangelosi"
      ],
      "year": "2022",
      "venue": "International Conference on Social Robotics"
    },
    {
      "citation_id": "7",
      "title": "Deep learningbased facial emotion recognition for human-computer interaction applications",
      "authors": [
        "M Chowdary",
        "T Nguyen",
        "D Hemanth"
      ],
      "year": "2023",
      "venue": "Deep learningbased facial emotion recognition for human-computer interaction applications"
    },
    {
      "citation_id": "8",
      "title": "Human-robot interaction based on facial expression recognition using deep learning",
      "authors": [
        "Y Maeda",
        "T Sakai",
        "K Kamei",
        "E Cooper"
      ],
      "year": "2020",
      "venue": "Human-robot interaction based on facial expression recognition using deep learning"
    },
    {
      "citation_id": "9",
      "title": "Human-robot interaction using markovian emotional model based on facial recognition",
      "authors": [
        "Y Maeda",
        "S Geshi"
      ],
      "year": "2018",
      "venue": "SCIS-ISIS"
    },
    {
      "citation_id": "10",
      "title": "Human-robot interaction based on facial expression imitation",
      "authors": [
        "A Esfandbod",
        "Z Rokhi",
        "A Taheri",
        "M Alemi",
        "A Meghdari"
      ],
      "year": "2019",
      "venue": "Human-robot interaction based on facial expression imitation"
    },
    {
      "citation_id": "11",
      "title": "Human-robot facial expression reciprocal interaction platform: case studies on children with autism",
      "authors": [
        "A Ghorbandaei Pour",
        "A Taheri",
        "M Alemi",
        "A Meghdari"
      ],
      "year": "2018",
      "venue": "International Journal of Social Robotics"
    },
    {
      "citation_id": "12",
      "title": "A facial expression emotion recognition based human-robot interaction system",
      "authors": [
        "Z Liu",
        "M Wu",
        "W Cao",
        "L Chen",
        "J Xu",
        "R Zhang",
        "M Zhou",
        "J Mao"
      ],
      "year": "2017",
      "venue": "IEEE CAA J. Autom. Sinica"
    },
    {
      "citation_id": "13",
      "title": "K-means clustering-based kernel canonical correlation analysis for multimodal emotion recognition in human-robot interaction",
      "authors": [
        "L Chen",
        "K Wang",
        "M Li",
        "M Wu",
        "W Pedrycz",
        "K Hirota"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Industrial Electronics"
    },
    {
      "citation_id": "14",
      "title": "A novel multimodal emotion recognition approach for affective human robot interaction",
      "authors": [
        "F Cid",
        "L Manso",
        "P Núnez"
      ],
      "year": "2015",
      "venue": "Proceedings of fine"
    },
    {
      "citation_id": "15",
      "title": "Affective communication system with multimodality for a humanoid robot, ami",
      "authors": [
        "H.-W Jung",
        "Y.-H Seo",
        "M Ryoo",
        "H Yang"
      ],
      "year": "2004",
      "venue": "IEEE/RAS International Conference on Humanoid Robots"
    },
    {
      "citation_id": "16",
      "title": "A survey on empathetic dialogue systems",
      "authors": [
        "Y Ma",
        "K Nguyen",
        "F Xing",
        "E Cambria"
      ],
      "year": "2020",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "17",
      "title": "Telme: Teacher-leading multimodal fusion network for emotion recognition in conversation",
      "authors": [
        "T Yun",
        "H Lim",
        "J Lee",
        "M Song"
      ],
      "year": "2024",
      "venue": "Telme: Teacher-leading multimodal fusion network for emotion recognition in conversation",
      "arxiv": "arXiv:2401.12987"
    },
    {
      "citation_id": "18",
      "title": "Unimse: Towards unified multimodal sentiment analysis and emotion recognition",
      "authors": [
        "G Hu",
        "T.-E Lin",
        "Y Zhao",
        "G Lu",
        "Y Wu",
        "Y Li"
      ],
      "year": "2022",
      "venue": "Unimse: Towards unified multimodal sentiment analysis and emotion recognition",
      "arxiv": "arXiv:2211.11256"
    },
    {
      "citation_id": "19",
      "title": "Emocaps: Emotion capsule based model for conversational emotion recognition",
      "authors": [
        "Z Li",
        "F Tang",
        "M Zhao",
        "Y Zhu"
      ],
      "year": "2022",
      "venue": "Emocaps: Emotion capsule based model for conversational emotion recognition",
      "arxiv": "arXiv:2203.13504"
    },
    {
      "citation_id": "20",
      "title": "A facial expression-aware multimodal multi-task learning framework for emotion recognition in multi-party conversations",
      "authors": [
        "W Zheng",
        "J Yu",
        "R Xia",
        "S Wang"
      ],
      "year": "2023",
      "venue": "ACL"
    },
    {
      "citation_id": "21",
      "title": "M2fnet: Multi-modal fusion network for emotion recognition in conversation",
      "authors": [
        "V Chudasama",
        "P Kar",
        "A Gudmalwar",
        "N Shah",
        "P Wasnik",
        "N Onoe"
      ],
      "year": "2022",
      "venue": "CVPR"
    },
    {
      "citation_id": "22",
      "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
      "authors": [
        "K Zhang",
        "Z Zhang",
        "Z Li",
        "Y Qiao"
      ],
      "year": "2016",
      "venue": "IEEE signal processing letters"
    },
    {
      "citation_id": "23",
      "title": "Openface: an open source facial behavior analysis toolkit",
      "authors": [
        "T Baltrušaitis",
        "P Robinson",
        "L.-P Morency"
      ],
      "year": "2016",
      "venue": "IEEE"
    },
    {
      "citation_id": "24",
      "title": "Sparse representations for facial expressions recognition via l1 optimization",
      "authors": [
        "S Zafeiriou",
        "M Petrou"
      ],
      "year": "2010",
      "venue": "CVPR"
    },
    {
      "citation_id": "25",
      "title": "Inception-v4, inception-resnet and the impact of residual connections on learning",
      "authors": [
        "C Szegedy",
        "S Ioffe",
        "V Vanhoucke",
        "A Alemi"
      ],
      "year": "2017",
      "venue": "AAAI"
    },
    {
      "citation_id": "26",
      "title": "Learning face representation from scratch",
      "authors": [
        "D Yi",
        "Z Lei",
        "S Liao",
        "S Li"
      ],
      "year": "2014",
      "venue": "Learning face representation from scratch",
      "arxiv": "arXiv:1411.7923"
    },
    {
      "citation_id": "27",
      "title": "Supervised prototypical contrastive learning for emotion recognition in conversation",
      "authors": [
        "X Song",
        "L Huang",
        "H Xue",
        "S Hu"
      ],
      "year": "2022",
      "venue": "Supervised prototypical contrastive learning for emotion recognition in conversation",
      "arxiv": "arXiv:2210.08713"
    },
    {
      "citation_id": "28",
      "title": "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
      "authors": [
        "P Liu",
        "W Yuan",
        "J Fu",
        "Z Jiang",
        "H Hayashi",
        "G Neubig"
      ],
      "year": "2023",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "29",
      "title": "Simcse: Simple contrastive learning of sentence embeddings",
      "authors": [
        "T Gao",
        "X Yao",
        "D Chen"
      ],
      "year": "2021",
      "venue": "Simcse: Simple contrastive learning of sentence embeddings",
      "arxiv": "arXiv:2104.08821"
    },
    {
      "citation_id": "30",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Y.-H Tsai",
        "S Bai",
        "P Liang",
        "J Kolter",
        "L.-P Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "ACL"
    },
    {
      "citation_id": "31",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "AAAI"
    },
    {
      "citation_id": "32",
      "title": "Modeling both context-and speaker-sensitive dependence for emotion detection in multi-speaker conversations",
      "authors": [
        "D Zhang",
        "L Wu",
        "C Sun",
        "S Li",
        "Q Zhu",
        "G Zhou"
      ],
      "year": "2019",
      "venue": "IJCAI"
    },
    {
      "citation_id": "33",
      "title": "Mmgcn: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "J Hu",
        "Y Liu",
        "J Zhao",
        "Q Jin"
      ],
      "year": "2021",
      "venue": "Mmgcn: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "arxiv": "arXiv:2107.06779"
    },
    {
      "citation_id": "34",
      "title": "Directed acyclic graph network for conversational emotion recognition",
      "authors": [
        "W Shen",
        "S Wu",
        "Y Yang",
        "X Quan"
      ],
      "year": "2021",
      "venue": "Directed acyclic graph network for conversational emotion recognition",
      "arxiv": "arXiv:2105.12907"
    },
    {
      "citation_id": "35",
      "title": "Mm-dfn: Multimodal dynamic fusion network for emotion recognition in conversations",
      "authors": [
        "D Hu",
        "X Hou",
        "L Wei",
        "L Jiang",
        "Y Mo"
      ],
      "year": "2022",
      "venue": "ICASSP"
    },
    {
      "citation_id": "36",
      "title": "Ga2mif: graph and attention based two-stage multi-source information fusion for conversational emotion detection",
      "authors": [
        "J Li",
        "X Wang",
        "G Lv",
        "Z Zeng"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on affective computing"
    }
  ]
}