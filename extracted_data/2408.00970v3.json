{
  "paper_id": "2408.00970v3",
  "title": "Multimodal Fusion Via Hypergraph Autoencoder And Contrastive Learning For Emotion Recognition In Conversation",
  "published": "2024-08-02T01:30:18Z",
  "authors": [
    "Zijian Yi",
    "Ziming Zhao",
    "Zhishu Shen",
    "Tiehua Zhang"
  ],
  "keywords": [
    "Multimodal Emotion Recognition in Conversation",
    "Variational Hypergraph Autoencoder",
    "Contrastive Learning",
    "Multimodal Fusion"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal emotion recognition in conversation (MERC) seeks to identify the speakers' emotions expressed in each utterance, offering significant potential across diverse fields. The challenge of MERC lies in balancing speaker modeling and context modeling, encompassing both long-distance and short-distance contexts, as well as addressing the complexity of multimodal information fusion. Recent research adopts graph-based methods to model intricate conversational relationships effectively. Nevertheless, the majority of these methods utilize a fixed fully connected structure to link all utterances, relying on convolution to interpret complex context. This approach can inherently heighten the redundancy in contextual messages and excessive graph network smoothing, particularly in the context of longdistance conversations. To address this issue, we propose a framework that dynamically adjusts hypergraph connections by variational hypergraph autoencoder (VHGAE), and employs contrastive learning to mitigate uncertainty factors during the reconstruction process. Experimental results demonstrate the effectiveness of our proposal against the state-of-the-art methods on IEMOCAP and MELD datasets. We release the code to support the reproducibility of this work at https://github.com/yzjred/HAUCL. \n CCS CONCEPTS â€¢ Information systems â†’ Sentiment analysis; â€¢ Computing methodologies â†’ Discourse, dialogue and pragmatics.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion is one of the crucial characteristics of human behavior  [19] . Experienced psychiatrists can assess emotions by observing an individual's behavior, which serves as a key indicator for understanding their inclinations and responses. As human-computer interaction (HCI) advances, the capability to discern emotions from dialogues using multimodal information is becoming increasingly significant  [29, 41] . This process is commonly referred to as multimodal emotion recognition in conversation (MERC). The multimodality herein includes different modal information such as the speaker's language, tone, facial expression, body movement and so on  [6, 36] . From a modeling perspective, a conversation consists of a sequence of utterances. Each utterance contains one or more modalities of information and is linked to speaker information. The target of MERC is to identify the emotion category of each utterance by analyzing the available information and contextual cues.\n\nCompared with the emotion recognition in non-dialogue scenarios  [10] , MERC necessitates a specific emphasis on modelling the speakers involved in the dialogue. Also unlike the analysis of single-modal information  [9] , the processing of multimodal information demands the utilization of distinct processing techniques to extract meaningful information from various modalities. Different modalities of information need to be synthesized to facilitate the comprehensive analysis of a conversation. For example, when a speaker utters the word \"ok\" with a tone of helplessness, solely relying on textual cues may not fully convey the speaker's emotional state. By taking into account factors such as intonation and tone assist in inferring the underlying feeling of sadness expressed by the speaker. Efficient integration and utilization of multimodal information play a crucial role in enhancing the precision of emotion recognition during conversations  [31] .\n\nCurrent research methodologies regarding MERC can be classified into two main categories: non-graph-based method  [13, 24, 25]  and graph-based method  [3, 9, 12, 14, 21] . Non-graph-based method typically utilizes recurrent neural networks (RNN) or long short-term memory (LSTM) to capture contextual information, while the output utterance representations are used for label classification. However, these methods encounter challenges in modeling long-range dependencies because of issues in information propagation and gradient vanishing problems  [9] . Graph-based method typically uses a graph to depict a conversation, with each utterance represented as a node and the relationships between utterances shown through edge weights or connections between nodes. Contextual information is captured through graph convolutions, and the resulting node embeddings are fed into subsequent classification steps  [19] .\n\nGraph-based methods can be further divided into standard graph-based and hypergraph-based methods. Standard graph-based method  [9, 12, 14, 21]  is a typical graph-based method, which represents textual information in utterances as nodes and captures contextual relationships by connecting nodes with various types of edges within a specific window size. For the standard graph-based methods, the pairwise connection approach fails to depict the actual physical structure of MERC accurately. Additionally, as the number of graph convolution layers rises, the training time and storage requirements increase exponentially. It can also result in oversmoothing of the graph and redundancy of nodes, potentially leading to inaccurate assessments  [27] .\n\nHypergraph-based method changes the point-to-point connection to a hyperedge connection structure that more closely fits the model  [3] . Hypergraph is a special graph structure capable of capturing high-order correlations, enabling the exploration of more intricate relationships  [1] . By linking multiple modalities within a single utterance and connecting all nodes of the same modality using hyperedges, the hypergraph-based method can achieve outstanding performance improvement. Nevertheless, the fixed fully connected hypergraph structure still results in information redundancy, graph smoothing and slow convergence, especially when processing longdistance conversations  [38] .\n\nTo address the aforementioned issues in existing hypergraph-based methods, we propose a multimodal fusion framework via hypergraph autoencoder and contrastive learning named HAUCL for MERC, which is applicable to multimodal data and capable of adaptively adjusting hypergraph connections. The framework consists of five modules: (1) unimodal encoding, (2) hypergraph construction, (3) hypergraph convolution, (4) hypergraph contrastive learning, and (5) classifier. The unimodal encoding module is designed to generate modality-independent representations. For the hypergraph construction module, it firstly forms an initial fully connected hypergraph structure. Then, a variational hypergraph autoencoder (VHGAE)-based approach  [32]  is introduced to realize adaptive adjustment of the hypergraph. In this paper, we develop VHGAE to map the hypergraph to the latent space to obtain node and hyperedge by sampling from space, and then learn new connections via Gumbel-Softmax  [16] . The aforementioned procedures exhibit a degree of randomness. To minimize the influence of random factors, two parameter-sharing paths are established through the utilization of contrastive learning techniques: Two VHGAEs reconstruct the hypergraph, and the reconstructed hypergraphs are utilized in the subsequent hypergraph convolution module to learn the embeddings along with contextual information. Then, point-to-point hypergraph contrastive learning module is applied to the obtained two hypergraphs, where nodes corresponding to each other in different hypergraphs are considered positive sample pairs to ensure model stability. Conversely, other nodes are treated as negative sample pairs to enhance the learning of more distinctive embeddings. Finally, the learned embeddings are fed into the classifier module for emotion category prediction.\n\nThe main contributions of this paper are summarized as follows:\n\nâ€¢ We propose a joint learning framework based on hypergraphs, which achieved synergistic optimization of hypergraph reconstruction, contrastive learning, and emotion recognition, leading to globally optimal performance. Specifically, VHGAE is integrated into MERC to adaptively adjust the hypergraph, while Gumbel-Softmax is devised to mitigate data overflow. â€¢ We utilize contrastive learning to mitigate the impact of uncertainty in the sampling process and the Gumbel-softmax learning process of VHGAE, enhancing the robustness and stability of the model. â€¢ Extensive experiments conducted on two mainstream MERC datasets, IEMOCAP and MELD, validate the effectiveness of our work. The results showed that our proposal performed superiorly compared to the state-of-the-art methods in accuracy and weighted F1 score.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Multimodal Emotion Recognition",
      "text": "Regarding the non-graph learning methods, BC-LSTM captures contextual information from surrounding utterances in three different modalities by using three independent bidirectional LSTM networks, and the output utterance representations are used for label classification  [25] . However, this method lacks the usage of speaker information and thus is not applicable to multi-person conversation scenarios. DialogueRNN utilizes three gate recurrent units (GRUs) to track the global context, speaker state, and emotion state throughout the entire dialogue, which effectively integrates speaker modeling, contextual modeling, and emotion modeling  [24] . To mimic the human reasoning process, DialogueCRN introduces reasoning modules to integrate the factors that make emotions happen  [13] .\n\nThe standard graph-based methods, such as DialogueGCN  [9] , represent textual information in utterances as nodes and capture contextual relationships through different types of edges connecting nodes within a given window size. The multimodal graph convolutional network (MMGCN) develops DialogueGCN by further incorporating audio and video modalities into the model  [14] . To address the challenge of cross-modal interaction in information fusion within ERC, MIMMN introduces a multi-view network that leverages complementary information from all modalities. It dynamically balances the relationships between all modalities during the fusion process  [33] . MM-DFN  [12]  uses a dynamic fusion mechanism to fully understand the context relationship between multiple modalities and reduce the redundancy between modalities. COGMEN  [17]  utilizes graph neural network (GNN) to leverage both local and global information in a conversation. GraphMFT  [21]  not only designs a multimodal fusion method based on graphs but also utilizes multiple graph attention networks (GATs) to capture the intra-modal contextual details and inter-modal complementary information. M3NET  [3]  introduces the hypergraph into the field of MERC. Through simple fully connected structures and randomly initialized edge weights, significant improvement in prediction accuracy and time efficiency has been achieved by multiple hypergraph convolutions.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Hypergraph Learning",
      "text": "A hypergraph acts as an extended version of the standard graph learning, specifically designed to extract high-order correlations within the data  [1, 39] . The examples of hypergraphs are shown on the left side of Figure  1 , with the corresponding standard graphs shown on the right side. The circular dots represent five nodes, i.e., from ğ‘‰ 1 to ğ‘‰ 5 . Curves with the same color form a hyperedge, and there are three hyperedges ğ‘’ 1 , ğ‘’ 2 , ğ‘’ 3 in total. In a hypergraph, connections are not limited to pairwise relationships as in a standard graph. Hyperedges can link multiple nodes together, and a single node can be linked by multiple hyperedges simultaneously. Meanwhile a hypergraph can include multiple types of hyperedges, representing multiple meanings. In this paper, we create a hypergraph in which all utterances linked to the same speaker are grouped together on a hyperedge, while also connecting similar modality into another hyperedge. This structure closely resembles the physical structure of certain models, capturing higher-order correlations and minimizing information loss during the modeling process. The effectiveness of hypergraph learning in solving the association problem of multimodal data has already been verified in various applications, such as including recommendation system  [34] , video segmentation  [35] , sleep stage classification  [23, 40] , and drug-target interaction prediction  [28] .\n\nRegarding hypergraph convolutions, the learning process involves aggregating node information onto connected hyperedges with varying weights, followed by sending messages from the hyperedges back to the connected nodes. This process is not constrained by distance, thereby mitigating the limitations of message transmission during the process  [7] . These benefits are particularly pronounced in long-distance transmissions  [8] . Therefore, the hypergraph learning process is anticipated to be effective in the MERC task, as speakers frequently discuss topics that are distant from the current conversation, utilizing long-distance cues.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "In this paper, we model the MERC task as follows: a conversation contains a sequence of utterances ğ‘¢ ğ‘– (ğ‘– = 1, ..., ğ‘ ). ğ‘ is the number of utterances. Each utterance ğ‘¢ ğ‘– consists of textual, acoustic, and visual modality, represented as ğ‘¢ ğ‘– = {ğ‘¢ ğ‘¡ ğ‘– , ğ‘¢ ğ‘ ğ‘– , ğ‘¢ ğ‘£ ğ‘– }, respectively. Meanwhile, each ğ‘¢ ğ‘– is spoken by a corresponding person ğ‘  ğ‘– . By integrating the speaker information, an utterance can be denoted as ğ‘£ ğ‘– = (ğ‘¢ ğ‘– , ğ‘  ğ‘– ). The goal of a MERC task is to predict the emotion label for each utterance ğ‘£ ğ‘– based on the given multimodal information. The overall framework of our proposed HAUCL is illustrated in Figure  2 . It includes unimodal encoding, hypergraph construction, convolution, contrastive learning and classifier.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Preprocess And Unimodal Encoding",
      "text": "This module involves extracting essential information from raw visual, textual, and acoustic modalities data. Following the approach outlined in M3NET  [3] , features from visual modalities are extracted using DenseNet  [15]  or 3D-CNN  [37] , depending on the adopted dataset. Features from acoustic and textual modalities are extracted using the OpenSmile toolkit  [5]  and the RoBERTa large model  [22]  respectively.\n\nAs mentioned above, incorporating contextual information is crucial for emotion category prediction in conversations. To enhance discourse feature representation, we employ various encoding methods tailored to the characteristics of different modalities. Specifically, we utilize GRU network  [4]  to encode context information for the textual modality, while acoustic and visual information is encoded using two fully connected multilayer perceptrons (MLPs). To facilitate the information fusion across modalities, we normalize the encoded dimension to a unified ğ‘‘ dimension as below:\n\nwhere ğ‘¢ ğ‘– is the input of unimodal encoding. ğ‘ˆ ğ‘– is the output of the model with the dimension ğ‘‘. ğ‘, ğ‘£, ğ‘¡ stands for visual, acoustic, and textual modalities respectively. ğ‘Š and ğ‘ are trainable parameters. Speaker information is a critical factor that affects the performance of the MERC task. We firstly encode the speaker information into vectors ğ‘  ğ‘– in one-hot form as:\n\nNext, we integrate them into the modality information by:\n\nThe output of this module is ğ‘‰ ğ‘– , which is the feature embeddings with modality-independent context awareness and speaker information.   Similar to the standard graphs, the incidence matrix for hypergraphs can also be defined as H âˆˆ R 3ğ‘ Ã—ğ‘€ , where ğ‘ and ğ‘€ is the number of nodes for each modality and hyperedges, respectively. So in the initialization phase, the mathematical relationship between M and N is M=N+3. We define ğ» ğ‘–,ğ‘— to determine the presence of node ğ‘– in hyperedge ğ‘— as:",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Hypergraph Construction",
      "text": "3.2.2 VHGAE. The fully connected hypergraph generated in the structure initialization stage may lead to redundancy in the subsequent update process, impeding the classification of subtle differences. To mitigate this challenge, we introduce VHGAE to reconstruct the hypergraph, aiming to identify the most appropriate hypergraph structure. VHGAE comprises of three processes: encoder, sampler, and decoder. The structure of VHGAE is illustrated in Figure  3 .\n\nEncoder: It aims to project the hypergraph into a representation consisting of sets of nodes and hyperedges. This projection can facilitate the subsequent decoding process of the non-Euclidean structure, as highlighted in the original paper that proposed variational graph auto-encoders (VGAE)  [20] .\n\nIn our proposed method, we follow the VHGAE framework and utilize a hypergraph neural network (HyperGNN) to perform hypergraph convolution on the original hypergraph. This convolution operation produces embeddings for both the nodes ğ‘£ and the hyperedges ğœ– as:\n\nWe utilize the obtained embeddings to encode the mean ğœ‡ and variance ğœ vectors for each type ğ‘˜ âˆˆ (ğ‘£, ğœ– ). This encoding process involves applying linear transformations and activation functions, as described by the following equations:\n\nwhere ğ‘Š ğ‘˜ and ğ‘ ğ‘˜ are learnable parameters specific to the type ğ‘˜. The activation functions ğœ and ğœ 1 correspond to the ReLU and Softplus functions, respectively. Through encoding the mean and variance vectors with node and hyperedge embeddings, we can effectively capture and represent the crucial information regarding the hypergraph's structure within a latent space.\n\nThese encoded vectors will play a pivotal role in the subsequent stages, enabling the generation of meaningful and relevant outputs.\n\nSample: To incorporate the reparametrization trick, we utilize sampling in the latent space to obtain new nodes and hyperedge embeddings. The sampling process introduces stochasticity while ensuring differentiable computations during the training phase.\n\nTo generate the new embeddings, we use the mean ğœ‡ ğ‘˜ and variance ğœ ğ‘˜ vectors obtained from the encoder process by Equations 6a and 6b. The reparametrization trick involves sampling from a standard normal distribution ğ›¿ âˆ¼ ğ‘ (0, 1) and scaling it by the standard deviation ğœ ğ‘˜ . The obtained sample is then added element-wise to the mean vector ğœ‡ ğ‘˜ to obtain the new embedding ğ‘š ğ‘˜ by:\n\nwhere âŠ™ represents the element-wise product between ğœ ğ‘˜ and ğ›¿. By incorporating the sampled noise ğ›¿ into the latent space representation, we introduce randomness to the model, while maintaining differentiability for efficient optimization.\n\nThe obtained embeddings ğ‘š ğ‘˜ serve as the updated representations for the output nodes or hyperedges, capturing the variability and uncertainty within the hypergraph structure.\n\nDecoder: This process aims to reconstruct the hypergraph from the latent space representation. By leveraging the updated embeddings obtained from the encoder process, we can recover the connection structure of the new hypergraph through a series of operations.\n\nFirst, we calculate the matrix â„ ğ‘– by taking the dot product between the transpose of ğ‘š ğœ– and ğ‘š ğœ by:\n\nwhere ğ‘š ğ‘‡ ğœ represents the inverse of ğ‘š ğœ . Next, we apply the Gumbel-Softmax function to the matrix â„ with a temperature coefficient ğœ to introduce stochasticity:\n\nIn the above equation, Gumbel_Softmax is a function that applies the Gumbel-Softmax relaxation. To prevent data overflow, we incorporate the addition of a constant ğ‘ to the Gumbel-Softmax operation and subsequently apply the softmax function.\n\nAfter conducting the softmax operation, The obtained matrix â„ has two columns, representing a distribution over the hypergraph connections. We extract the first column of matrix â„, which corresponds to the incidence matrix of the new hypergraph G 0 = ( V, E 0 ).\n\nBy obtaining the connection structure of the new hypergraph through the decoder, we can reconstruct the relationships and connections between nodes and hyperedges. This reconstructed hypergraph can then be further utilized for various downstream tasks.\n\nLoss function: The loss function in VHGAE consists of primary components designed to produce a reconstructed hypergraph that closely resembles the original one. Specifically, the first component measures the Kullback-Leibler (KL) divergence between the distributions of the latent variables (nodes and hyperedges) and their corresponding prior distributions. The second component quantifies the difference in connection structure between the newly generated hypergraph and the original hypergraph. The VHGAE's loss function L ğ‘” is defined as follows:\n\nwhere KL(ğ‘š ğœ , ğœ ) measures the KL divergence between the distribution of the sampled latent variables ğ‘š ğœ and the prior distribution ğœ. Similarly, KL(ğ‘š ğœ– , ğœ– ) represents the KL divergence between the distribution of the sampled hyperedge embeddings ğ‘š ğœ– and the prior distribution ğœ–. The third term CE(â„ 0 , â„) denotes the cross-entropy loss function, quantifying the connection structure difference between the original hypergraph â„ 0 and the generated hypergraph â„. This component ensures that the generated hypergraph closely matches the original hypergraph regarding the distribution of connections. By minimizing this loss function, VHGAE aims to learn an effective latent space representation that captures the essential characteristics of the hypergraph while preserving its connection structure.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Hypergraph Convolution",
      "text": "With the new hypergraph G 0 = ( V, E 0 ), we first perform node convolution by aggregating node features to update the hyperedge embeddings. The aggregation stage facilitates the integration of information from neighboring nodes into the hyperedge representation. Following the update of the hyperedge embeddings, we proceed to the hyperedge convolution stage, where hyperedge messages are disseminated to the nodes. This operation enables the information propagation from hyperedges to their incident nodes. For each hyperedge ğœ– âˆˆ E 0 , we aggregate the embeddings of its incident nodes ğ‘£ according to a predefined aggregation function by:\n\nwhere ğ‘› ğœ– represents the updated embedding for the hyperedge ğœ– and ğ‘› ğ‘£ denotes the embedding of the node ğ‘£. The aggregation function Agg combines the embeddings of the incident nodes to generate the new hyperedge embedding. For each node ğ‘£ âˆˆ V, we aggregate the messages from its incident hyperedges ğœ– using a predefined aggregation function similar to Equation  11 . By performing the node and hyperedge convolutions, we can effectively propagate information and update the embeddings in the hypergraph G 1 = ( V 0 , E 0 ). This reformulated solution enables the capture of the relationships and interactions between nodes and hyperedges, facilitating a more comprehensive understanding of the hypergraph structure.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Hypergraph Contrastive Learning",
      "text": "In order to mitigate the instability inherent in the sampling and decoding processes, we devise a dual-path scheme within our model. The primary objective is to minimize the dissimilarity between corresponding points in two hypergraphs\n\n0 , E 0 ), which are obtained through the progression of VHGAE and convolution. Concurrently, we aim to maximize the distance between each point and other points within the embedding space.\n\nWithin the context of the two hypergraph views, pairs of vertices that correspond to one another are regarded as positive pairs, whereas the remaining vertex pairs are considered negative pairs. The embedding of the ğ‘–-th vertex in the two views is denoted as ğ‘£\n\nHere, V 0 denotes the set of vertices and | V 0 | signifies the cardinality of V 0 . The term ğ‘“ (ğ‘£ (1)\n\nğ‘– ) is calculated as:\n\nğ‘— )  (13)  where\n\nHere, ğœ is a temperature parameter and ğ‘” (, ) denotes the cosine similarity function. Considering that the function ğ‘” (, ) is not symmetric, we average the positive and negative aspects. Specifically, ğ‘–â‰ ğ‘— ğ‘ (ğ‘£ By minimizing the combined loss function L ğ‘ğ‘™ , the similarity between corresponding points is expected to increase while enhancing the distance between each point and other points within the embedding space. This approach promotes alignment and discrimination of the embeddings, thereby yielding more stable and meaningful representations of the hypergraph structure.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Emotion Classifier",
      "text": "After acquiring contextual knowledge, we perform a fusion process on the node embeddings of the two hypergraphs\n\n. This process aims to integrate the information from the two hypergraphs into a unified representation.\n\nFollowing that, we concatenate the node embeddings of the three modalities that belong to the same utterance, resulting in a comprehensive representation. Specifically, let {ğ‘£ ğ‘¡ ğ‘– , ğ‘£ ğ‘ ğ‘– , ğ‘£ ğ‘£ ğ‘– } âˆˆ V 2 denote the node embeddings of the hypergraphs corresponding to the textual, acoustic, and visual modalities, respectively. We concatenate these embeddings to obtain a fused representation by:\n\nThe Concatenate function combines the embeddings of the three modalities into a unified vector, allowing for the integration of multiple sources of information. The fused representation ğ‘£ ğ‘– encompasses a broader range of information, enhancing subsequent analysis and prediction tasks by providing a more comprehensive input.\n\nGiven the fused representation ğ‘£ ğ‘– for an utterance, the formulas for predicting the emotion label are as follows:\n\nIn these formulas, ğ‘Š 2 and ğ‘Š 3 are weight matrices, ğ‘ 2 and ğ‘ 3 are bias vectors, ğ‘£ ğ‘– is the processed output of ğ‘£ ğ‘– using the ReLU activation function, ğ‘ƒ ğ‘– is the probability distribution over the emotion labels, and ğ‘¦ ğ‘– is the predicted emotion label. ğœ represents the dimension corresponding to the emotion labels. By applying these formulas, we can predict the emotion label for each utterance based on the fused representation ğ‘£ ğ‘– and the learned parameters ğ‘Š 2 , ğ‘Š 3 , ğ‘ 2 , and ğ‘ 3 .",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Training Objectives",
      "text": "We use categorical cross-entropy loss with ğ¿2 regularization term to define the error loss between the predicted emotion category and the true label during the training process as below:\n\nwhere ğ‘ represents the number of dialogues in a dataset. ğ‘ (ğ‘  ) represents the number of utterances in dialogue ğ‘ . It is worth noting that each dialogue can have a different number of utterances. ğ‘ƒ ğ‘–,ğ‘— denotes the predicted probability distribution of emotion labels for utterance ğ‘— in dialogue ğ‘–, while ğ‘¦ ğ‘–,ğ‘— represents the expected class label. The regularization weight ğœ† controls the importance of the regularization term relative to the cross-entropy loss. By combining Equations 17, 10 and 12, we define the final loss function as:\n\nwhere the hyperparameter weights ğœ† ğ‘” and ğœ† ğ‘ğ‘™ control the importance of the generalized adversarial loss and the contrastive loss, respectively.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiment 4.1 Datasets",
      "text": "In this paper, we conduct experiments on two popular multimodal datasets in the field of MERC: the interactive emotional dyadic motion capture database (IEMOCAP)  [2]  and multimodal emotionlines dataset (MELD)  [26] .\n\nâ€¢ IEMOCAP: It contains videos of two-way conversations with 10 actors (5 male and 5 female). IEMOCAP records the tone and power of speech, facial expressions, torso posture, head position, gestures, transcripts, and gaze in a duo session. In this paper, we use facial expressions, the tone and power of speech and transcripts. The emotions in this dataset are artificially classified into six categories: happy, sad, neutral, angry, excited, and frustrated. We use 120 dialogues containing 5,810 utterances for training and validation, while the remaining 31 dialogues with 1623 utterances for testing. â€¢ MELD: It is a multimodal dataset for emotion recognition in multiparty conversations, containing textual, acoustic and visual modalities for ERC, selected from Friends TV series. This dataset includes seven emotions: neutral, surprise, fear, sadness, happiness, disgust, and anger. We use 1,153 dialogues with 11,098 utterances for training and validation, while the rest 280 dialogues with 2610 utterances for testing.\n\nIt is worth noting that IEMOCAP dataset features a fixed set of two speakers engaging in multiple rounds of conversation, whereas MELD dataset may involve multiple speakers but with fewer utterances per conversation. Meanwhile, the emotion distribution within MELD dataset is imbalanced, with a significantly higher proportion of \"neutral\" emotions compared to other emotional categories, comprising nearly half of the dataset. These characteristics pose significant challenges to the model's stability.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Experimental Settings And Baselines",
      "text": "We perform all experiments on an NVIDIA GTX 1050Ti with Win11 operating system. The versions of Pytorch and cuda are 2.1.2 and 11.8, respectively. Adam optimizer is used for training. We set the batch size as 12 and the learning rate as 0.0001 on both datasets. The hyperparameter ğœ of Gumbelsoftmax in Equation 9 is 0.1 . The number of hypergraph convolutions is 1. More details regarding the main parameters can be found in Table  1  In order to validate the performance of the proposed method HAUCL in the MERC task, we introduce the ten state-of-the-art methods for comparison: (1) non-graph learning: LSTM  [25] , DialogueRNN  [24] , and Dialogue-CRN  [13] ; (2) standard graph learning: DialogueGCN  [9] , MMGCN  [14] , DIMMN  [33] , MMDFN  [12] , COGMEN  [17]  and GraphMFT  [21] ; and (3) hypergraph learning: M3NET  [3] . More details regarding the baseline methods can be found in Section 2.\n\nFor validation, we adopt the most mainstream evaluation metrics in this field: accuracy (Acc.) and weighted F1 score (WF1).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Performance Comparison",
      "text": "Table  2  summaizes the performance of different methods tested on IEMO-CAP and MELD datasets. The results show that our proposed HAUCL achieves superior performance in terms of the overall accuracy and weighted F1 score. In detail, compared with M3NET, which achieves the second-best performance, HAUCL enhances the accuracy and WF1 by 0.43% and 0.57% respectively on MELD dataset and by 1.29% and 1.15% on IEMOCAP dataset.\n\nOur model also has advantages in training time and model size.The average training time of each epoch of HAUCL is 38 secs and the model size is 173,945 KB. In comparison, M3NET  [3] , which obtains the second-best performance, has a training time of 56 secs with the size of 608,867 KB. This superior performance can be attributed to the fact that HAUCL utilizes only one hypergraph convolution layer, whereas M3NET incorporates three convolutions and three high-frequency information convolutions.\n\nThe ability of HAUCL to dynamically modify the connection structure of the hypergraph helps in reducing information redundancy, particularly in IEMOCAP dataset with a high average utterance per conversation. Additionally, the use of hypergraphs helps prevent excessive smoothing, reducing the risk of excessive smoothing occurring in standard graph-based methods. Compared with non-graph learning methods, our proposed HAUCL can demonstrate significant enhancement in long-distance information transmission and multimodal information fusion, resulting in satisfactory accuracy and weighted F1 score performance.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Sensitivity Analysis",
      "text": "We select the following four main parameters in HAUCL for sensitivity analysis tested on MELD dataset. Figures  4a  and 4b  show the ratio of hypergraph reconstruction loss and contrastive learning loss to the total loss, respectively. Figure  4c  represents the number of convolution layers passed by the new hypergraph obtained by reconstruction to learn the contextual information. Figure  4d  shows the effect of batch size. Similar trends are also observed on IEMOCAP data.\n\nThe weight of the hypergraph reconstruction loss ğœ† ğ‘” : It reflects the deviation from the original graph over the total loss (See Equation  18 ). Higher values of ğœ† ğ‘” indicate that the reconstructed hypergraph closely resembles the original graph. As shown in Figure  4a , when ğœ† ğ‘” is set to 0.5, our method demonstrates optimal performance in accuracy. Meanwhile, deviating from this optimal value, either towards larger or smaller values, results in a decline in the overall performance.\n\nThe weight of contrastive learning loss ğœ† ğ‘ğ‘™ : Similar with ğœ† ğ‘” , as the value of ğœ† ğ‘ğ‘™ increases, the method will increasingly focus on the differences between the two hypergraphs derived from the two paths. Conversely, when the dissimilarity between the two hypergraphs diminishes, our proposal's capability to withstand interference strengthens. However, when this value is excessively large, it will impact the loss of emotion recognition, i.e., when ğœ† ğ‘ğ‘™ exceeds 1.1, there is a degradation in accuracy performance as plotted in Figure  4b .\n\nHypergraph layer ğ¿: Figure  4c  demonstrates that increasing the number of covolutional layers in a hypergraph does not necessarily lead to enhanced accuracy performance. A large value of ğ¿ not only amplifies the model's complexity and runtime, but also risks oversmoothing, potentially complicating the differentiation of emotions with similar characteristics. When ğ¿ is 5, there is a sharp decrease in accuracy performance, indicating an over-smoothing phenomenon.\n\nBatch size: The selection of batch size is a crucial factor that impacts the performance of recognition  [11, 18] . Given the non-uniform distribution of MELD dataset, employing a batch size that is too small can render the model susceptible to the interference of small samples, leading to significant gradient fluctuations and convergence challenges. Conversely, an excessively large batch size may prompt the model to overly generalize, potentially compromising accuracy. As depicted in Figure  4d , the best performance is attained with a batch size of 12.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Ablation Study",
      "text": "For a more comprehensive analysis of the effectiveness of our proposed method HAUCL, we conduct ablation experiments from three different aspects: the impact of (1) speaker embedding, (2) VHGAE and contrastive learning, and (3) contrastive learning in terms of accuracy performance. The results are summarized in Table  3 .   Impact of Speaker Embedding: Speaker embedding can distinguish the input features from different speakers. Existing research has shown that incorporating speaker information can enhance the accuracy of emotion recognition tasks  [14] . The exclusion of speaker embedding (\"w/o SE\" in Table  3 ) results in a decrease in accuracy, with a degradation of 1.11% and 0.81% observed on IEMOCAP and MELD datasets, respectively. These findings indicate that incorporating person modeling can enhance the model's performance in the MERC domain.\n\nImpact of VHGAE and Contrastive Learning: We utilize VHGAE for dynamic hyperedge selection to minimize redundancy and employ contrastive learning to mitigate random errors. In Table  3 , \"w/o GCL\" indicates the direct fusion of two hypergraph convolutions without the inclusion of VHGAE and contrastive learning module. The results demonstrate the effectiveness of our proposed HAUCL: It can improve the accuracy performance by 0.78% and 0.43% on IEMOCAP and MELD datasets respectively.\n\nImpact of Contrastive Learning: \"w/o CL\" in Table  3  refers to the model that incorporates hypergraph and VHGAE without the integration of contrastive learning. The experimental results indicate a 0.98% enhancement in accuracy on IEMOCAP dataset and a 0.58% improvement on MELD dataset. These results verify that the contrastive learning module effectively",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Visualization",
      "text": "In order to demonstrate the discriminability of nodes, we present the node representations acquired through our proposed method HAUCL and the M3Net (the second-best method in Table  2 ) on MELD dataset. To visualize these representations in a more comprehensive manner, we employ t-SNE  [30]  method for dimensionality reduction, transforming the obtained nodes into three dimensions. Furthermore, we assign distinct colors to indicate the true labels of the nodes. By comparing the two figures in Figure  5 , it is evident that the data points depicted in Figure  5a  (our method) exhibit greater separation, resulting in a more discriminative segmentation. As aforementioned, the representations derived from the proposed HAUCL exhibit reduced redundancy and enhanced discriminability, thereby enabling the attainment of superior outcomes.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Time Complexity",
      "text": "This subsection summarizes the time complexity of hypergraph reconstruction. The hypergraph construction in HAUCL includes variantional encoding and decoding part. Starting with encoder, a hypergraph convolution layer is involved with time complexity ğ‘‚ (ğ‘ ğ‘‘ 2 +ğ‘ ğ‘‘ğ‘€ ), where ğ‘ is the number of utterances. ğ‘€ is the number of hyperedges in the initial trivial input graph, and ğ‘‘ is the feature dimension. To generate the latent embedding of mean and variance in Equations 6a and 6b, the time complexity is ğ‘‚ (ğ‘ ğ‘‘ 2 ).\n\nThe sampling process can be calculated as ğ‘‚ (ğ‘ + ğ‘€ ). In the decoding process, the complexity to compute the incidence matrix is ğ‘‚ (ğ‘ ğ‘€ğ‘‘ 2 ). In all, the total computational cost of our framework is ğ‘‚ (ğ‘ ğ‘€ğ‘‘ 2 )â†’ ğ‘‚ (ğ‘ 2 ğ‘‘ 2 ).",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose a joint learning framework based on hypergraph learning to improve the performance of MERC. This framework aims to address the issue of excessive redundancy stemming from the fully connected structure of graphs or hypergraphs. The proposed method HAUCL effectively integrates hypergraph adaptive reconstruction and contrastive learning, which reduces information redundancy and enhances accuracy.\n\nExperimental results verify the superiority of our proposed method against state-of-the-art ones. In the future, we expect to integrate external knowledge, such as large language models (LLM), into our framework. By focusing on linear labels such as valence-arousal-dominance (VAD) in dimensional emotion space, we aim to substitute classification labels with the goal of enhancing machines' comprehension of human behavior.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An illustration showcasing the differences between",
      "page": 2
    },
    {
      "caption": "Figure 2: An overview of our proposed framework HAUCL.",
      "page": 3
    },
    {
      "caption": "Figure 2: It includes unimodal encoding, hypergraph construction, con-",
      "page": 3
    },
    {
      "caption": "Figure 3: The structure of VHGAE.",
      "page": 4
    },
    {
      "caption": "Figure 3: Encoder: It aims to project the hypergraph into a representation consist-",
      "page": 4
    },
    {
      "caption": "Figure 4: c represents the number of convolution layers passed",
      "page": 6
    },
    {
      "caption": "Figure 4: d shows the effect of batch size. Similar trends are also",
      "page": 6
    },
    {
      "caption": "Figure 4: a, when ğœ†ğ‘”is set to 0.5,",
      "page": 6
    },
    {
      "caption": "Figure 4: c demonstrates that increasing the num-",
      "page": 6
    },
    {
      "caption": "Figure 4: d, the best performance is",
      "page": 6
    },
    {
      "caption": "Figure 4: Sensitive analysis of HAUCL on MELD dataset. All experiments test the results while fixing all other parameters with",
      "page": 7
    },
    {
      "caption": "Figure 5: Visualization of our proposed HAUCL and M3NET",
      "page": 7
    },
    {
      "caption": "Figure 5: a (our method) exhibit",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "",
          "IEMOCAP": "Emotion Categories (F1)",
          "MELD": "Overall"
        },
        {
          "Method": "",
          "IEMOCAP": "Happy\nSad\nNeutral\nAngry\nExcited\nFrustrated",
          "MELD": "Acc.\nWF1"
        },
        {
          "Method": "bc-LSTM [25]\nDialogueRNN [24]\nDialogueCRN [13]",
          "IEMOCAP": "32.62\n70.34\n51.14\n63.44\n67.91\n61.06\n33.18\n78.80\n59.21\n65.28\n71.86\n58.91\n51.59\n74.54\n62.38\n67.25\n73.96\n59.97",
          "MELD": "59.62\n56.80\n60.31\n57.66\n59.66\n56.76"
        },
        {
          "Method": "DialogueGCN [9]\nMMGCN [14]\nDIMMN [33]\nMM-DFN [12]\nCOGMEN [17]\nGraphMFT [21]",
          "IEMOCAP": "47.10\n80.88\n58.71\n66.08\n70.97\n61.21\n45.45\n77.53\n61.99\n66.67\n72.04\n64.12\n30.2\n74.2\n59.0\n62.7\n72.5\n66.6\n42.22\n78.98\n66.42\n69.77\n75.56\n66.33\n68.61\n51.91\n81.72\n66.02\n75.31\n58.23\n83.12\n70.30\n76.92\n45.99\n63.08\n63.84",
          "MELD": "58.62\n56.36\n59.31\n57.82\n60.6\n58.6\n62.49\n59.46\n62.53\n61.77\n61.30\n58.37"
        },
        {
          "Method": "M3NET [3]",
          "IEMOCAP": "57.96\n81.56\n68.30\n65.59\n74.91\n63.19",
          "MELD": "67.62\n66.15"
        },
        {
          "Method": "HAUCL (ours)",
          "IEMOCAP": "68.61\n68.23\n53.57\n82.04\n66.44\n75.60",
          "MELD": "68.05\n66.72"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A Survey on Hypergraph Representation Learning. Comput. Surveys",
      "authors": [
        "Alessia Antelmi",
        "Gennaro Cordasco",
        "Mirko Polato",
        "Vittorio Scarano",
        "Carmine Spagnuolo",
        "Dingqi Yang"
      ],
      "year": "2023",
      "venue": "A Survey on Hypergraph Representation Learning. Comput. Surveys"
    },
    {
      "citation_id": "2",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Emily Ebrahim (abe) Kazemzadeh",
        "Samuel Provost",
        "Jeannette Kim",
        "Sungbok Chang",
        "Shrikanth Lee",
        "Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "3",
      "title": "Multivariate, Multi-Frequency and Multimodal: Rethinking Graph Neural Networks for Emotion Recognition in Conversation",
      "authors": [
        "Feiyu Chen",
        "Jiejing Shao",
        "Shuyuan Zhu",
        "Heng Tao Shen"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "4",
      "title": "Gate-variants of gated recurrent unit (GRU) neural networks",
      "authors": [
        "Rahul Dey",
        "M Fathi",
        "Salem"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 IEEE 60th international midwest symposium on circuits and systems (MWSCAS)"
    },
    {
      "citation_id": "5",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin WÃ¶llmer",
        "BjÃ¶rn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the ACM International Conference on Multimedia (MM)"
    },
    {
      "citation_id": "6",
      "title": "Multimodal sentiment analysis: A systematic review of history, datasets, multimodal fusion methods, applications, challenges and future directions",
      "authors": [
        "Ankita Gandhi",
        "Kinjal Adhvaryu",
        "Soujanya Poria"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "7",
      "title": "HGNN+: General Hypergraph Neural Networks",
      "authors": [
        "Yue Gao",
        "Yifan Feng",
        "Ji Shuyi",
        "Rongrong Ji"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "8",
      "title": "Hypergraph Learning: Methods and Practices",
      "authors": [
        "Yue Gao",
        "Zizhao Zhang",
        "Haojie Lin",
        "Xibin Zhao",
        "S Du",
        "Changqing Zou"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "9",
      "title": "DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "10",
      "title": "Analyzing Modality Robustness in Multimodal Sentiment Analysis",
      "authors": [
        "Devamanyu Hazarika",
        "Yingting Li",
        "Bo Cheng",
        "Shuai Zhao",
        "Roger Zimmermann",
        "Soujanya Poria"
      ],
      "year": "2022",
      "venue": "Proceedings of the North American Chapter"
    },
    {
      "citation_id": "11",
      "title": "Control batch size and learning rate to generalize well: Theoretical and empirical evidence",
      "authors": [
        "Fengxiang He",
        "Tongliang Liu",
        "Dacheng Tao"
      ],
      "year": "2019",
      "venue": "Proceedings of the International Conference on Neural Information Processing Systems (NIPS)"
    },
    {
      "citation_id": "12",
      "title": "MM-DFN: Multimodal Dynamic Fusion Network for Emotion Recognition in Conversations",
      "authors": [
        "Dou Hu",
        "Xiaolong Hou",
        "Lingwei Wei",
        "Lian-Xin Jiang",
        "Yang Mo"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "DialogueCRN: Contextual Reasoning Networks for Emotion Recognition in Conversations",
      "authors": [
        "Dou Hu",
        "Lingwei Wei",
        "Xiaoyong Huai"
      ],
      "year": "2021",
      "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)"
    },
    {
      "citation_id": "14",
      "title": "MMGCN: Multimodal Fusion via Deep Graph Convolution Network for Emotion Recognition in Conversation",
      "authors": [
        "Jingwen Hu",
        "Yuchen Liu",
        "Jinming Zhao",
        "Qin Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)"
    },
    {
      "citation_id": "15",
      "title": "Densely Connected Convolutional Networks",
      "authors": [
        "Gao Huang",
        "Zhuang Liu",
        "Kilian Weinberger"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "16",
      "title": "Categorical reparameterization with gumbel-softmax",
      "authors": [
        "Eric Jang",
        "Shixiang Gu",
        "Ben Poole"
      ],
      "year": "2016",
      "venue": "Categorical reparameterization with gumbel-softmax",
      "arxiv": "arXiv:1611.01144"
    },
    {
      "citation_id": "17",
      "title": "COGMEN: COntextualized GNN based Multimodal Emotion recognitioN",
      "authors": [
        "Abhinav Joshi",
        "Ashwani Bhat",
        "Ayush Jain",
        "Atin Singh",
        "Ashutosh Modi"
      ],
      "year": "2022",
      "venue": "Proceedings of the Conference of the North American Chapter"
    },
    {
      "citation_id": "18",
      "title": "The effect of batch size on the generalizability of the convolutional neural networks on a histopathology dataset",
      "authors": [
        "Ibrahem Kandel",
        "Mauro Castelli"
      ],
      "year": "2020",
      "venue": "ICT express"
    },
    {
      "citation_id": "19",
      "title": "Emotion recognition and artificial intelligence: A systematic review (2014-2023) and research recommendations",
      "authors": [
        "K Smith",
        "Victoria Khare",
        "Esmaeil Blanes-Vidal",
        "U Nadimi",
        "Acharya"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "20",
      "title": "Variational graph auto-encoders",
      "authors": [
        "N Thomas",
        "Max Kipf",
        "Welling"
      ],
      "year": "2016",
      "venue": "Variational graph auto-encoders",
      "arxiv": "arXiv:1611.07308"
    },
    {
      "citation_id": "21",
      "title": "GraphMFT: A graph network based multimodal fusion technique for emotion recognition in conversation",
      "authors": [
        "Jiang Li",
        "Xiaoping Wang",
        "Guoqing Lv",
        "Zhigang Zeng"
      ],
      "year": "2023",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "22",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "23",
      "title": "Exploiting Spatial-Temporal Data for Sleep Stage Classification via Hypergraph Learning",
      "authors": [
        "Yuze Liu",
        "Ziming Zhao",
        "Tiehua Zhang",
        "Kang Wang",
        "Xin Chen",
        "Xiaowei Huang",
        "Jun Yin",
        "Zhishu Shen"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "24",
      "title": "DialogueRNN: an attentive RNN for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "25",
      "title": "Context-Dependent Sentiment Analysis in User-Generated Videos",
      "authors": [
        "E Soujanya Poria",
        "Devamanyu Cambria",
        "Navonil Hazarika",
        "Amir Majumder",
        "Louis-Philippe Zadeh",
        "Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)"
    },
    {
      "citation_id": "26",
      "title": "MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder"
      ],
      "year": "2019",
      "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)"
    },
    {
      "citation_id": "27",
      "title": "DropEdge: Towards Deep Graph Convolutional Networks on Node Classification",
      "authors": [
        "Yu Rong",
        "Wenbing Huang",
        "Tingyang Xu",
        "Junzhou Huang"
      ],
      "year": "2019",
      "venue": "Proceedings of the International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "28",
      "title": "Exploring complex and heterogeneous correlations on hypergraph for the prediction of drug-target interactions",
      "authors": [
        "Ding Ruan",
        "Shuyi Ji",
        "Clarence Chenggang",
        "Junjie Yan",
        "Xibin Zhu",
        "Yuedong Zhao",
        "Yue Yang",
        "Changqing Gao",
        "Qionghai Zou",
        "Dai"
      ],
      "year": "2021",
      "venue": "Patterns"
    },
    {
      "citation_id": "29",
      "title": "Multimodal big data affective analytics: A comprehensive survey using text, audio, visual and physiological signals",
      "authors": [
        "J Nusrat",
        "Li-Minn Shoumy",
        "Kah Ang",
        "D Phooi Seng",
        "Tanveer Motiur Rahaman",
        "Zia"
      ],
      "year": "2020",
      "venue": "Journal of Network and Computer Applications"
    },
    {
      "citation_id": "30",
      "title": "Visualizing data using t-SNE",
      "authors": [
        "Laurens Van Der Maaten",
        "Geoffrey Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "31",
      "title": "Unlocking the Power of Multimodal Learning for Emotion Recognition in Conversation",
      "authors": [
        "Yunxiao Wang",
        "Meng Liu",
        "Zhe Li",
        "Yupeng Hu",
        "Xin Luo",
        "Liqiang Nie"
      ],
      "year": "2023",
      "venue": "Proceedings of the ACM International Conference on Multimedia (MM)"
    },
    {
      "citation_id": "32",
      "title": "Augmentations in Hypergraph Contrastive Learning: Fabricated and Generative",
      "authors": [
        "Tianxin Wei",
        "Yuning You",
        "Tianlong Chen",
        "Yang Shen",
        "Jingrui He",
        "Zhangyang Wang"
      ],
      "year": "2022",
      "venue": "Proceedings of the Conference on Neural Information Processing Systems (NIPS)"
    },
    {
      "citation_id": "33",
      "title": "Dynamic interactive multiview memory network for emotion recognition in conversation",
      "authors": [
        "Jintao Wen",
        "Dazhi Jiang",
        "Geng Tu",
        "Cheng Liu",
        "Erik Cambria"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "34",
      "title": "Self-Supervised Hypergraph Transformer for Recommender Systems",
      "authors": [
        "Lianghao Xia",
        "Chao Huang",
        "Chuxu Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining"
    },
    {
      "citation_id": "35",
      "title": "Learning Multi-Granular Hypergraphs for Video-Based Person Re-Identification",
      "authors": [
        "Yichao Yan",
        "Jie Qin",
        "Jiaxin Chen",
        "Li Liu",
        "Fan Zhu",
        "Ying Tai",
        "Ling Shao"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "36",
      "title": "Disentangled Representation Learning for Multimodal Emotion Recognition",
      "authors": [
        "Dingkang Yang",
        "Shuai Huang",
        "Haopeng Kuang",
        "Yangtao Du",
        "Lihua Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the ACM International Conference on Multimedia (MM)"
    },
    {
      "citation_id": "37",
      "title": "Asymmetric 3D Convolutional Neural Networks for action recognition",
      "authors": [
        "Hao Yang",
        "Chunfeng Yuan",
        "Bing Li",
        "Yang Du",
        "Junliang Xing",
        "Weiming Hu",
        "Stephen Maybank"
      ],
      "year": "2019",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "38",
      "title": "ReCoSa: Detecting the Relevant Contexts with Self-Attention for Multi-turn Dialogue Generation",
      "authors": [
        "Hainan Zhang",
        "Yanyan Lan",
        "Liang Pang",
        "J Guo",
        "Xueqi Cheng"
      ],
      "year": "2019",
      "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)"
    },
    {
      "citation_id": "39",
      "title": "Learning from heterogeneity: A dynamic learning framework for hypergraphs",
      "authors": [
        "Tiehua Zhang",
        "Yuze Liu",
        "Zhishu Shen",
        "Xingjun Ma",
        "Xin Chen",
        "Xiaowei Huang",
        "Jun Yin",
        "Jiong Jin"
      ],
      "year": "2023",
      "venue": "Learning from heterogeneity: A dynamic learning framework for hypergraphs",
      "arxiv": "arXiv:2307.03411"
    },
    {
      "citation_id": "40",
      "title": "An adaptive federated relevance framework for spatial temporal graph learning",
      "authors": [
        "Tiehua Zhang",
        "Yuze Liu",
        "Zhishu Shen",
        "Rui Xu",
        "Xin Chen",
        "Xiaowei Huang",
        "Xi Zheng"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Artificial Intelligence"
    },
    {
      "citation_id": "41",
      "title": "Emotion Recognition From Multiple Modalities: Fundamentals and methodologies",
      "authors": [
        "Sicheng Zhao",
        "Guoli Jia",
        "Jufeng Yang",
        "Guiguang Ding",
        "Kurt Keutzer"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Magazine"
    }
  ]
}