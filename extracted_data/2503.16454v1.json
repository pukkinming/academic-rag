{
  "paper_id": "2503.16454v1",
  "title": "An Audio-Visual Fusion Emotion Generation Model Based On Neuroanatomical Alignment ‚ãÜ,‚ãÜ‚ãÜ",
  "published": "2025-02-21T14:26:58Z",
  "authors": [
    "Haidong Wang",
    "Qia Shan",
    "JianHua Zhang",
    "PengFei Xiao",
    "Ao Liu"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In the field of affective computing, traditional methods for generating emotions predominantly rely on deep learning techniques and large-scale emotion datasets. However, deep learning techniques are often complex and difficult to interpret, and standardizing large-scale emotional datasets are difficult and costly to establish. To tackle these challenges, we introduce a novel framework named Audio-Visual Fusion for Brain-like Emotion Learning(AVF-BEL). In contrast to conventional brain-inspired emotion learning methods, this approach improves the audio-visual emotion fusion and generation model through the integration of modular components, thereby enabling more lightweight and interpretable emotion learning and generation processes. The framework simulates the integration of the visual, auditory, and emotional pathways of the brain, optimizes the fusion of emotional features across visual and auditory modalities, and improves upon the traditional Brain Emotional Learning (BEL) model. The experimental results indicate a significant improvement in the similarity of the audio-visual fusion emotion learning generation model compared to single-modality visual and auditory emotion learning and generation model. Ultimately, this aligns with the fundamental phenomenon of heightened emotion generation facilitated by the integrated impact of visual and auditory stimuli. This contribution not only enhances the interpretability and efficiency of affective intelligence but also provides new insights and pathways for advancing affective computing technology. Our source code can be accessed here: https://github.com/OpenHUTB/emotion.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotions are fundamental to human life, serving as one of the complex brain functions crucial for communication, adaptation, and survival  [1, 2] . In human communication, emotions are crucial and represent a key aspect of human intelligence  [3, 4] . Sentiment analysis has gained significant attention in artificial intelligence and natural language processing fields  [5] . Affective computing combines research in emotion recognition and sentiment analysis and can utilize either unimodal or multimodal data. This data primarily includes physical information, such as text, audio, visual and remote photoplethysmography signals (rPPG)  [6, 7] .\n\nThe study of brain and emotion science traces back to 1884 with the introduction of the first explicit theory of emo- tions, known as the James-Lange theory  [8] . According to this theory, emotional experiences stem from responses to physiological changes in the body, where these physiological changes themselves constitute the emotion. During the 1930s, the Papez circuit theory was introduced, proposing that emotional experiences are shaped by the activity of the cingulate cortex and indirect activity from other cortical regions  [9] . MacLean's research suggests that the evolution of the limbic system allowed animals to experience and express emotions, liberating them from stereotyped behaviors dominated by the brainstem  [10] . Beau suggests that the anterior superior temporal gyrus houses both visual and auditory areas, utilizing shared neural codes for emotion perception  [11, 12] .\n\nThe thalamus, sensory cortex, anterior superior temporal gyrus, amygdala, and orbitofrontal cortex are all involved in emotional learning and expression. These structures collectively comprise an emotional circuit that effectively processes information from sensory organs across different modalities, enabling the learning and expression of emotions. In the context of rapid advancements in artificial intelligence, Mor√©n and Balkenius introduced the Brain Emotional Learning(BEL) model  [13] , which aims to elucidate the neurophysiological mechanisms underlying emotion formation and learning in the mammalian brain by simulating emotional processing between the amygdala and orbitofrontal cortex. Subsequently, researchers integrated the BEL model with artificial neural networks, leading to significant applications in pattern recognition, intelligent control, and facial expression classification  [14, 15, 16] . However, research on the mechanisms of emotional learning and gen-eration in the brain remains limited, especially in terms of improving the BEL model to better simulate human emotional learning and generation, which is crucial for advancing affective computing.\n\nIn this paper, we present a novel neuroanatomically aligned audio-visual fusion brain emotion learning and generating architecture. The traditional BEL model is an artificial intelligence framework inspired by the emotional processing mechanisms of the brain, primarily based on the interactions between the limbic system of mammals, particularly the amygdala and the orbitofrontal cortex. However, it adopts a simplistic unimodal approach to simulate sensory cortex functions, overlooking essential neurological mechanisms. Our biomimetic emotional pathway is more comprehensive than traditional BEL models, as it integrates the functional of both visual and auditory pathways to achieve advanced biomimicry and interpretability features.\n\nAs illustrated in Fig  1 , visual and auditory emotional stimuli are processed into emotional signals through the visual and auditory cortex modules respectively. These signals then undergo a gradual emotional transformation within the emotional pathway of the brain. This pathway mainly includes emotional processing between the visual cortex, primary auditory cortex, anterior superior temporal gyrus, amygdala, and orbitofrontal cortex. Together, they facilitate the emotional function of the brain, translating visual and auditory stimuli into emotional experiences. Our primary objective is to engage in biomimetic modeling of this pathway, with the aim of simulating its functionality while ensuring that the biomimetic modules closely align with the neural structures of the brain. In summary, our main contribution is the introduction of a novel audio-visual fusion brain emotion learning model. This model is designed with neuroanatomical alignment and tailored for affective computing to facilitate lighter and more interpretable artificial intelligence emotion generation.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Brain Emotional Learning",
      "text": "The emotional circuits in the brain have long been a central focus of neurophysiological research, as understanding the mechanisms behind emotions is crucial for motivations related to human-machine interaction and artificial intelligence in the context of hybrid intelligence  [17, 7] . Similarly, simulating the functions of emotional pathways in the human brain is a prominent area of study in the fields of artificial intelligence (AI) and affective computing, where researchers aim to replicate or model the emotional responses and behaviors of the brain.\n\nIn this context, Mor√©n et al. proposed a BEL model, which was inspired by the organizational structure of the limbic system in the mammalian brain. The BEL model emphasizes the interaction between key brain regions involved in emotional processing, such as the amygdala and the orbitofrontal cortex. It simulates the transmission and processing of emotional stimuli through the reflex pathways of the brain, offering insights into how emotional information is processed and reacted to by the neural circuits of the brain  [13, 18] . Building upon this foundational work, Lotfi et al. further developed a supervised version of the Brain Emotion Learning model. This enhanced model is capable of learning from target pattern examples, and has been experimentally compared with several alternative approaches, including Multi-Layer Perceptron (MLP) and Adaptive Neuro-Fuzzy Inference Systems (ANFIS). The distinctive feature of the supervised BEL model lies in its ability to achieve rapid training for predictive problems, making it highly effective for real-time applications in emotional learning and decision-making  [19] . In a further extension of the BEL framework, Wang et al. proposed a biomimetic memory circuit with both emotion learning and generation capabilities. This circuit is designed to conduct neuromorphic emotion learning and generation, enabling the processing of various input types to simulate emotional responses. Their model advances the state of emotional circuit simulation by incorporating memory elements, which are essential for emulating the ability of the brain to recall and modify emotional states based on past experiences  [20] . Sun et al. introduced a biomimetic circuit that simulates a three-dimensional emotional space model. This circuit generates brain-like emotional responses based on multimodal input, including visual, speech, and text information. This model represents a significant step forward in the development of emotionsensing and emotion-generating systems for bionic robots, providing a potential framework for achieving emotional companionship between humans and machines  [21] .\n\nWhile the BEL model has made notable progress in the domains of intelligent control, classification prediction, and the development of emotional circuits, the simulation and utilization of the emotional circuitry of the brain have encountered certain bottlenecks in recent years. With ongoing advancements in neuroscience and neuroengineering, it has become increasingly important to refine and enhance the biomimetic simulation of emotional circuits to improve their functionality, accuracy, and adaptability. The proposed AVF-BEL model represents an advancement over the original BEL framework, not only improving the biomimetic capabilities of emotional circuitry but also offering significant benefits in terms of design efficiency, lightweight operation, and interpretability. This development paves the way for more complex models capable of processing complex emotional stimuli in a manner more akin to the brain, potentially addressing some of the current limitations. It is hoped that, in the future, more effective emotion simulations can be realized in artificial intelligence systems and robotic applications.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Emotion Generation Technology",
      "text": "Emotion generation techniques utilize multimodal data, such as images, text, and music, to enable computers to simulate or generate emotional responses. This multidisciplinary approach leverages diverse sources of input to capture the complexity of emotional states and enhance the emotional intelligence of computational systems.\n\nZhou et al. introduced an innovative model called the Emotional Chat Machine (ECM), which not only responds appropriately in terms of content relevance and grammatical structure but also effectively manages the emotional aspects of dialogue, ensuring emotional consistency throughout interactions. The ECM model represents a pioneering attempt to address the integration of emotional elements within large-scale dialogue generation systems. Experimental results demonstrate that the model can generate responses that are both contextually appropriate and emotionally congruent, highlighting its potential for enhancing affective communication in automated systems  [22] . Building on this work, Wang et al. proposed a novel deep generative model designed to synthesize facial videos based on neutral facial images and specific facial expression labels, such as spontaneous smiles. The model is composed of two main components: an image generator and a frame sequence generator. The image generator is realized through a deep neural network that combines Generative Adversarial Networks (GANs) with Variational Autoencoders (VAEs), while the sequence generator is a label-conditioned recursive neural network. This approach allows for the generation of dynamic facial expressions that are temporally consistent. Experimental results validate the models effectiveness, demonstrating the advantages of integrating adversarial elements into the recurrent architecture for facial video synthesis, thus enhancing the realism and expressiveness of the generated videos  [23] . In another advancement, Chen et al. proposed a memory-based emotion generation circuit, which is capable of storing emotional memories, retrieving them as needed, and deploying them to generate consistent personality traits. This circuit is designed to address the challenges of low power consumption, area efficiency, and memory processing, making it suitable for integration into resource-constrained systems, such as mobile robots or embedded devices. The ability to store and recall emotional memories is crucial for enabling more natural and adaptive emotional responses in machines, reflecting the dynamic nature of human emotion processing  [24] . Further contributing to the field, Zhang et al. introduced a brain-inspired emotion generation system based on the Pleasure-Arousal-Dominance (PAD) model, a three-dimensional emotional space that simulates the neural circuits involved in emotion generation within the limbic system. This model draws inspiration from the biological structures of the brain and provides a highly realistic and biologically grounded approach to emotion simulation. The PAD model offers valuable insights into the neural underpinnings of emotion and sets the stage for the development of more accurate and nuanced emotional simulations in artificial systems  [25] .\n\nAdvancements in emotion generation technologies have made significant progress in areas such as text-based dialogue systems, emotion generation circuits, and facial expression synthesis. These breakthroughs rely heavily on sophisticated deep learning techniques, extensive emotional datasets, and optimized hardware components. However, despite these advancements, widespread deployment and application of emotion generation technologies remain challenging due to the substantial investments required in both software and hardware infrastructure. As such, there are significant barriers to achieving scalable and cost-effective solutions. In contrast to many of these resource-intensive approaches, our AVF-BEL model emphasizes a lightweight design and high interpretability, while also achieving excellent performance in neuroanatomical alignment and result similarity. These features are particularly valuable for largescale applications in areas such as robot emotion generation and artificial intelligence emotion synthesis. By optimizing for efficiency and ease of interpretation, the AVF-BEL model offers a promising avenue for overcoming the challenges associated with resource-intensive emotion generation systems, potentially enabling more widespread adoption and practical implementation in real-world applications.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Method",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Biological Mechanism",
      "text": "In developing the AVF-BEL framework, our primary inspiration stemmed from four neuroscientific brain structures:visual pathway, primary auditory cortex, anterior superior temporal gyrus, emotional pathway. Specifically, these four neural structures within neuroscience form an integrated pathway for emotional learning and generation. This pathway combines visual and auditory stimuli received by sensory cortices to shape emotional experiences.\n\nAs illustrated in Fig  1 , the referenced emotional pathway represents the short emotional circuit in the brain. It primarily involves emotional interactions between the amygdala and orbitofrontal cortex, both of which are key to emotion regulation and learning, with characteristics of simplification and biomimetic features  [13, 26, 27] .\n\nVisual Pathway: The discovery and analysis of cortical visual areas represent a significant achievement in visual neuroscience  [28, 29] . The visual cortex, located in the occipital lobe at the posterior region of the cerebral cortex, serves as the principal region of the brain for receiving, integrating, and processing visual information from the retina. The primary visual cortex (V1) is the initial cortical area en-gaged in this processing, where basic visual features such as edges, orientation, and spatial frequency are analyzed.\n\nFrom V1, visual information progresses along distinct pathways, each specializing in different aspects of visual processing. The ventral stream (V1 ‚Üí V2 ‚Üí V4 ‚Üí IT) primarily processes object recognition and identification, analyzing complex shapes, colors, and textures to support higher-level visual cognition in regions such as the inferior temporal cortex (IT)  [30, 31, 32] . The dorsal stream, involving regions such as V1 ‚Üí V2 ‚Üí V5/MT (middle temporal) ‚Üí MST (medial superior temporal), is more specialized for motion processing, spatial perception, and visually guided actions. Areas like V5/MT and MST contain motionsensitive neurons with larger receptive fields, enabling spatial integration and the detection of motion direction and speed  [33, 34, 35] . Each of these pathways within the visual cortex contributes to distinct aspects of visual perception, advancing from initial image processing in V1 to complex visual interpretations necessary for interaction with the environment.\n\nPrimary Auditory Cortex: The primary auditory cortex constitutes the core of human auditory capabilities. Understanding the organization of the primary auditory cortex forms the neural foundation for comprehending auditory behavior  [36, 37] . It is situated within the lateral sulcus of the brain, specifically within Heschl's gyrus, also referred to as the transverse temporal gyrus. The representational capacity is predominantly dependent on the cortical state, suggesting that the cortical state should be regarded as a prominent variable in all studies of sensory processing. It is essential for the initial processing of auditory information, particularly in distinguishing sound features like frequency, intensity, and duration  [38, 39, 40] .\n\nAnterior Superior Temporal Gyrus: The superior temporal sulcus (STS) is the primary region for audio-visual integration. It is vital for perceiving biological motion and is also considered essential for speech and facial processing  [41] . Within the superior temporal gyrus (STG), there exists a dual dissociation of auditory components, distinguishing between clear and noisy elements, in response to both auditory and visual speech. This structure is central to the multisensory integration of both auditory and visual speech, even in noisy environments  [42] . The multimodal integration of visual and auditory perception in the anterior superior temporal gyrus enhances the speed and accuracy of emotional information processing. Located in the lateral sulcus of the temporal lobe, between the lateral fissure and superior temporal sulcus, this area is crucial for the integrated perception of visual and auditory emotions in the brain  [43, 11] .\n\nEmotional Pathway: Evidence from anatomy, neurophysiology, functional neuroimaging, and neuropsychology delineates the anterior margin and associated structures. This encompasses the orbitofrontal cortex and amygdala, which are implicated in emotion, reward evaluation, and reward-related decision-making (excluding memory). Value representations are channeled to the anterior cingulate cortex for learning action outcomes. Various limbic structures, encompassing the amygdala and orbitofrontal emotional system, exhibit distinct connectivity and functionality  [44, 45] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Architecture Design",
      "text": "As demonstrated in Fig 2 , we have designed and implemented a comprehensive brain-based emotional learning and generation framework that facilitates audio-visual integration. This architecture is structured to mimic the processing of emotional stimuli in the brain, with a focus on visual and auditory modalities. Initially, visual stimuli are processed sequentially within the visual cortex module, where various visual features related to emotional content, such as color, motion, and spatial orientation, are analyzed and encoded. Simultaneously, auditory stimuli are processed within the auditory cortex module, where acoustic features, including pitch, rhythm, and tone, are extracted and mapped to emotional relevance. The next critical step involves the fusion module, where information from both the visual and auditory modalities is integrated. This module facilitates the synthesis of multi-sensory inputs, creating a cohesive representation of the emotional content derived from both visual and auditory cues. The integrated emotional features are then passed on to the BEL module, which performs further processing and learning. This module is designed to simulate the neurobiological mechanisms responsible for transforming visual and auditory stimuli into emotional parameters, specifically focusing on the generation of emotional positivity. One of the key innovations of our model is its alignment with neuroanatomical structures involved in emotional processing. The architecture is carefully designed to reflect the anatomical and functional interconnections between the visual cortex, auditory cortex, and the emotional learning structures of the brain. By incorporating these four core modules-visual cortex, auditory cortex, fusion, and BEL-our model not only replicates the process of emotional perception and generation but also achieves a high  level of neuroanatomical alignment. This design ensures that the operation of the model is grounded in a biologically plausible framework, enhancing its validity and interpretability.\n\nAs demonstrated in Fig  3 , the audiovisual emotion generation system operates through multi-module collaboration, enabling the extraction, integration, and analysis of visual and auditory features. The visual cortex module processes emotional visual features based on an enhanced architecture. The input layer receives five emotion-related visual features, with the first convolutional layer using a large 7√ó7 kernel to rapidly reduce input dimensions, followed by successive layers with 3√ó3 kernels to refine features incrementally. Common activation functions include Sigmoid, GELU and ReLU. The unidirectional activation property of ReLU resembles the \"on/off\" behavior of biological neurons, where activation occurs only when the input signal exceeds a certain threshold. This simplifies the computational process, accelerates gradient propagation, and avoids delays caused by complex computations. Additionally, the sparsity of ReLU helps reduce overfitting and improves computational efficiency, while effectively mitigating the vanishing gradient problem, ensuring stable gradient updates within the neural network  [46] . A max-pooling layer reduces spatial dimensions, and an adaptive average pooling layer reduces each feature map to a 1x1 size, extracting a single feature value per channel. The flattened layer transforms the features into a one-dimensional vector, which is then mapped to the emotional output space by a linear classifier. The au-ditory cortex module simulates auditory processing using a simplified neural model, defining dynamic equations for excitatory and inhibitory neurons-G-PYR (Generate pyramid), G-PV (Generate parvalbumin), and G-SOM (Generate somatostatin)-while mapping five acoustic feature input currents onto each neuron, which in turn generate auditory emotional features. The fusion module employs crossmodal fusion to process and integrate audio and visual features. It first uses fully connected layers (ùêπ ùê∂ m and ùêπ ùê∂ v ) to map the audio and visual inputs into their respective hidden feature spaces (ùêªùëñùëë m and ùêªùëñùëë v ). Attention mechanisms (ùê¥ùë°ùë°ùëõ m and ùê¥ùë°ùë°ùëõ v ) then calculate attention weights, dynamically adjusting the importance of each modality. These weighted features are concatenated and passed through a final fully connected layer before being fed into the subsequent BEL module. The BEL module utilizes a recurrent neural network architecture to process and integrate audiovisual emotional inputs received from the fusion module. First, temporal features are extracted using a sliding window approach to capture time dependencies. The hidden layer stores the learned contextual information, which is then passed through fully connected layers to generate excitatory signals (ùêπ ùê∂ A ) and inhibitory signals (ùêπ ùê∂ O ). The control layer mimics the prefrontal cortex, fine-tuning these inhibitory signals to ensure appropriate emotional responses. A reinforcement signal (Reward) provides feedback to optimize the learning of the system and strengthen desirable emotional behaviors. The following sections provide a detailed overview of the specific roles and functions of each of these four modules within the overall emotional learning and generation process.\n\nVisual Cortex Module: Kubilius et al. developed the CORnet series of models, a shallow artificial neural network (ANN) featuring four anatomical mapping regions and recurrent connections, guided by Brain-Score  [47] . This represents a large-scale amalgamation of novel neural and behavioral benchmarks, designed to assess the functional fidelity of ventral visual stream models in primates. Although notably shallower than most models, the CORnet series stands as a top-performing model on Brain-Score, surpassing comparable compact models on ImageNet  [48, 32] . The visual cortex module is built on an enhanced CORnet-Z model, which is central to extracting and processing visual emotion features for emotion generation tasks  [47] .\n\nIts architecture harnesses the powerful feature extraction capabilities of convolutional neural networks (CNNs), simulating the processing of visual information by the brain by progressively extracting emotion-related visual features across layers. The model consists of convolutional layers, ReLU activation functions, and max-pooling layers. The convolutional layers extract multi-level emotional feature maps from input images, while the ReLU activation introduces non-linearity, enabling the model to capture complex emotional patterns. The max-pooling layers effectively reduce the spatial dimensions of the feature maps, enhancing computational efficiency while preserving essential emotional information. The first convolutional block utilizes a large 7√ó7 kernel to quickly reduce the input dimensions, followed by subsequent blocks with smaller 3√ó3 kernels that progressively refine more detailed emotional features. After convolution and pooling, the model uses an adaptive average pooling layer to further reduce the feature maps to a 1√ó1 size, consolidating each channel into a single feature value. The Flatten layer then transforms these features into a onedimensional vector, which can be directly accessed through an Identity layer, facilitating subsequent emotional analysis and generation tasks. The weights of the model are initialized using Xavier uniform initialization to ensure stable training, with biases initialized to zero to prevent imbalance in signal propagation during the training process. If Batch-Norm layers are included, their parameters are also carefully initialized. The combination of effective dimensionality reduction strategies within the visual processing module enables the model to efficiently extract emotional features from visual data, thereby improving its performance in handling complex emotional imagery.\n\nThe primary function of this module is to input five visual features (speed, jitter, consonance, bigsmall, updown) extracted from original emotional animation videos into the modified CORnet-Z model. The model includes four computational areas that are conceptually analogous to the dorsal visual regions V1, V2, V4, and IT, as well as a linear classifier decoder that maps the neural populations of the final visual area to behavioral choices. In this module, the tensor ùëñ ùëù n,c,h,w is a four-dimensional tensor with dimensions (n, c, h, w), where n is the batch size, c is the number of chan-nels corresponding to the number of features, h is the height of ùëñ ùëù n,c,h,w , and w is the width of ùëñ ùëù n,c,h,w . Convolution operations ùê∂ùëúùëõùë£ c,k are performed on the input feature tensor ùëñ ùëù n,c,h,w using convolution kernel k for each channel, adding a bias term b, and then applying the ReLU activation function:\n\nThe ùê∂ùëúùëõùë£ c,k refers to the convolutional kernels that map ùëê input channels to ùëò output channels. The convolved ùëñ ùëù n,c,h,w is then subjected to max pooling, where the maximum value from each region is selected as the pooling result:\n\nXn,c,h,w = maxp(ùëã n,c,h,w ).\n\n(\n\nThe flattened and combined tensor ùëã n,c,h,w is transformed into a one-dimensional vector Xn,c,h,w . A linear transformation is then applied to the flattened feature vector X, resulting in the final feature vector ùëã a :\n\nThese vectors ùëã a will be used as input signals for subsequent fusion modules. These vectors, ùëã a , will serve as input signals for subsequent fusion modules, where ùëä and ùëè represent the weight matrix and bias term, respectively. Auditory Cortex Module: The auditory cortex of mammals is comprised of diverse inhibitory and excitatory neuron types, forming intricate microcircuits for the processing and transmission of sensory information. Various subtypes of inhibitory neurons serve distinct roles in auditory processing  [49] . The auditory cortex module utilizes a simplified primary auditory cortex model, col1_fs, aimed at simulating the behavior of different types of neurons in the auditory cortex  [50] . Initially, three populations of neurons are defined (Excitatory pyramidal neurons (Pyr), inhibitory neurons such as parvalbumin expressing neurons (PV) and somatostatin expressing neurons (SOM)), with each population described by differential equations that represent their neural dynamics. The code uses the 'NeuronGroup' function to create 400 pyramidal neurons G-PYR, 200 PV neurons G-PV, and 200 SOM neurons G-SOM, establishing a threshold (membrane potential greater than 0.5 volts) and reset conditions (membrane potential returns to zero) for each population. The input currents are set at 0.6 volts for both PYR and PV neurons, and 0.65 volts for SOM neurons, simulating different activation states under realistic physiological conditions. To monitor spiking activity, 'SpikeMonitor' is utilized to track the activity of each neuron population. Finally, the simulation is executed for a duration of 1000 milliseconds using 'run(1000ms)'. This module provides a foundational framework for studying auditory information processing, enabling the analysis of dynamic behaviors in auditory signal processing through spiking activity monitoring of various neuron populations, thus revealing interactions and functional characteristics among neurons. The primary function of this module is to input five acoustic features (pitch, tonnetz, volume, tempo, duration) extracted from original emotional audio into the modified col1_fs model  [40] . The model consists of an excitatory neural population and two inhibitory neural subpopulations, defined by different types of neuron models: excitatory neurons (G-PYR), inhibitory neurons (G-PV), and SOM inhibitory neurons (G-SOM). Each neuron receives different input currents to simulate neuronal activity in the primary auditory cortex. The dynamic model of G-PYR can be described as:\n\nwhere ùë£ PYR represents the membrane potential of excitatory neurons (G-PYR). ùêº PYR is the input current, and ùúè PYR is the time constant. The dynamic model of G-PV can be described as:\n\nwhere ùë£ PV represents the membrane potential of inhibitory neurons (G-PV). ùêº PV is the input current, and ùúè PV is the time constant. The dynamic model of G-SOM can be described as:\n\nwhere ùë£ SOM represents the membrane potential of inhibitory neurons (G-SOM). ùêº SOM is the input current, and ùúè SOM is the time constant. Mapping five emotional auditory features (pitch, tonnetz, volume, tempo, duration) to the input currents of three neurons.\n\nBased on the dynamical model of the neurons described above, it is possible to simulate changes in the membrane potential of the neurons and record their activity states. The functionality of the auditory cortex can be characterized and analyzed by monitoring neuronal activity for feature extraction.\n\nAs illustrated in Fig.  4 , there are 400 PYR neurons, 200 PV neurons, and 200 SOM neurons. spike activity for each type of neuron is monitored over a 1000-millisecond simulation period.The transformation process unfolds as follows:\n\nwhere ùëÅ ea refers to the simulation and monitoring process of neuronal activity. ùêº PYR , ùêº PV , and ùêº SOM are functions that map emotional auditory features to the input current of neurons. This equation illustrates the transformation process from emotional auditory features to neuronal activity, demonstrating the complex processing and response of the auditory cortex to musical characteristics. Fusion Module: The fusion module is designed based on a MLP architecture, specifically tailored to process emotional data from visual and auditory modalities. This module integrates and models features from different modalities through a unified multimodal processing framework. The model takes multimodal data (visual and auditory features) as input, with each modality processed separately through dedicated visual and auditory modules for initial feature extraction. In the architecture design, the input data of each modality first passes through its respective independent MLP branch, each consisting of three fully connected layers combined with ReLU activation functions and batch normalization, effectively capturing intra-modal feature representations. These modality-specific features are then integrated through a cross-modal interaction layer to generate a joint representation. The fused joint feature vector is subsequently passed through fully connected layers to complete the final emotion generation task. The model employs Mean Squared Error (MSE) as the loss function to optimize the prediction of continuous emotional data. The Adam optimizer is utilized with an appropriately set learning rate to balance the trade-off between convergence speed and training stability. To enhance training efficiency and model generalization, the model supports batch loading and random shuffling of multimodal data. The training process is set for 100 epochs, during which the model sequentially extracts visual and auditory features from the batch data in each epoch. These features are forwarded to generate the joint feature representation, with model parameters updated through backpropagation. Training losses can be recorded at fixed intervals (every 10 epochs) for subsequent inference and performance evaluation. With the aforementioned multimodal fusion module design, the model effectively integrates emotional information from both visual and auditory modalities, significantly enhancing the performance and reliability of multimodal emotion analysis, providing robust support for emotion data modeling and analysis.\n\nOverall, this module provides a clear and comprehensive foundation for the joint learning of audio and visual emotional data, making it well-suited for further multi-modal research and applications. This fusion module aims to emulate the function of a singular neural code shared by auditory and visual information within the anterior superior temporal gyrus. The multi-layer structure and non-linear activation functions of the MLP enable it to learn and represent complex non-linear relationships. This capability is particularly important for integrating emotional features, which often exhibit non-linear and intricate characteristics. Additionally, its simple structure allows for seamless integration with the BEL module implemented by recurrent neural network (RNNs), forming a hybrid model that further enhances the performance of emotional learning and generation models. The MLP can effectively integrate emotional features from audio and animation, enabling the simulation and recognition of overall emotional stimuli  [51, 52] . In this module, a multimodal Multilayer Perceptron model was implemented, integrating the visual and auditory feature vectors obtained from the preceding two modules. The input feature vectors ùëã a and ùëã m are processed by the fully connected layer and the ReLU activation function to obtain the hidden layer feature vectors ùêª m and ùêª v :\n\nwhere ùëä music is the weight matrix for the audio features. ùëè music is the bias term for the audio features.\n\nwhere ùëä video is the weight matrix for the video features. ùëè video is the bias term for the video features.\n\nwhere ùëä attn-music is the weight matrix for the audio features. ùëè attn-music is the bias term for the audio features.\n\nwhere ùëä attn-video is the weight matrix for the video features. ùëè attn-video is the bias term for the video features. The features are then weighted according to the attention weights and fused (concatenated):\n\nCombine the hidden layer feature vectors ùëã m1 and ùëã a1 of the audio and visual into a comprehensive feature vector ùëã am :\n\nwhere ùëã am will be used as input for the next BEL module. This enables seamless integration with the subsequent BEL module implemented using RNNs, thereby further enhancing the performance of the model.\n\nBEL Module: The network design of this module is primarily based on the brain emotion learning model, simulating the emotional interactions between the amygdala and the orbitofrontal cortex. Using the integrated feature vector sequence output from the audio-visual emotion fusion module as input, this module constructs a recurrent neural network based on the BEL model  [13, 18, 53, 54] . The model utilizes a sliding window approach to construct time series data, enabling it to capture dynamic emotional variations. Randomly generated signals are used to simulate thalamic output, facilitating the transmission of dynamic neural signals. The amygdala integrates inputs from the sensory cortex and thalamus, producing excitatory and inhibitory signals that reflect the complexity of emotional decision-making. In terms of the learning system, the orbitofrontal cortex simulates inhibitory functions by integrating inputs from various regions, achieving emotional regulation through behavioral adjustments. During the training process, RNN parameters are defined, and weights are progressively updated through an error feedback mechanism to enhance classification accuracy. Finally, the effectiveness of model evaluation is ensured by calculating accuracy on the test set.\n\nThis module successfully simulates the emotional pathways of the brain through a multi-layered neural network, encompassing critical processes such as visual and auditory information processing, emotional responses, and learning adaptations. The amygdala embodies an excitatory learning system responsible for perceiving and learning inputs. The learning input constitutes part of a negative feedback loop, halting the learning signal once the output attains the reinforcement signal level. Additionally, it receives inhibitory input from the orbitofrontal system, potentially dampening inappropriate emotional responses. The activity of the perception node ùê¥, corresponding to each stimulus ùëã am , is calculated as follows: ùê¥ = ùëã am √óùëâ . V represents the adaptable connection weight. In the amygdala, reinforcement learning rules are applied to adjust the connection weights V of perceptual nodes. This allows the model to adapt its output based on rewards, guiding it towards the desired level of reinforcement. This learning rule can be described by the following formula:\n\nwhere ‚ñ≥ùëâ represents the adjustment amount of the connection weight V. ùõº is the learning rate. ùëã am represents the comprehensive feature vector from the fusion module. ùëÖ e is a reinforcement signal (Reward).\n\n‚àë ùëó 0 ùê¥ represents the total output of all perception nodes. This formula indicates that for each perception node, the adjustment amount ‚ñ≥ùëâ of its connection weight V depends on the discrepancy between the current perceived input ùëã am , the expected reinforcement signal ùëÖ e , and the current model output. The orbitofrontal cortex receives inputs from the fusion module and information about actual and expected reinforcement from the amygdala. It compares the expected emotional learning rewards, and if they do not match, it inhibits the learning function of the orbitofrontal cortex. The inhibitory output ùëÇ in the orbitofrontal cortex is calculated as follows: ùëÇ = ùëã am √ó ùëà . ùëà is the malleable connection weight. The connection weight ùëà for inhibitory output is calculated as:\n\nwhere ‚ñ≥ùëà represents the adjustment amount of the connection weight ùëà , ùõΩ is the learning rate. ùëã am represents the comprehensive feature vector from the fusion module. ùëÖ e is a reinforcement signal (Reward).\n\nis the total difference between the current output ùëÇ ùëó and the expected reinforcement signal ùëÖ e . This formula indicates that for each output node, the adjustment of its connection weight ùëà depends on the current perceived input ùëã am and the total difference between all output nodes and the expected reinforcement signal ùëÖ e .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "ùê∏ =",
      "text": "‚àë ùëñ 0 ùê¥ ùëñ -\n\nwhere ùê¥ is the output of the perception node, and ùëÇ is the output of the inhibitory node. The aforementioned formula illustrates the interaction and learning mechanism between the amygdala and orbitofrontal cortex within the BEL module.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiment",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experimental Dataset",
      "text": "This study utilized an dataset (Visual and auditory brain areas share a representational structure that supports emotion perception) publicly available on the OpenNEURO platform, which investigates the structural representations of emotional perception shared between visual and auditory brain regions, with a particular focus on audiovisual source files generated by emotional stimuli  [11] . In this multimodal dataset, the emotional stimuli were generated using a dynamic model based on five parameters: velocity, irregularity, harmony/sharpness, the ratio of large movements, and the ratio of vertical movements. The output of the model was mapped to simple piano melodies or the motion trajectories of animated bouncing balls. During each model run, new emotional stimuli were probabilistically generated based on the current parameter settings. The animation and music components of the study each included 760 different samples, totaling 1520 samples. A total of 79 participants were involved, of which 47 were female. The output of these models is mapped through simple piano melodies or the motion trajectory of an animated bouncing ball. Participants expressed five basic emotions through manual evaluation: fear, sadness, anger, calmness, and happiness. Based on the SnowNLP sentiment analysis method, we rank the emotional categories according to their degree of positivity and calculate the final emotion score through a weighted average, followed by normalization to ensure the scores fall within the [0, 1] range, where values closer to 1 indicate more positive emotions. This research aims to explore the impact of multimodal stimuli on emotional perception and provide empirical support for the mechanisms of emotion generation.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Experimental Setup",
      "text": "Our implementation details are as follows, which we aim to incorporate into the experimental procedures section. The CORnet-Z module uses a multi-convolution block architecture. It utilizes a 7√ó7 convolution kernel with a stride of 2 and an increasing number of convolutional channels at each layer, implementing Xavier initialization to accelerate convergence. The neural simulation module models neuronal responses through dynamic equations based on voltage variations, and SpikeMonitor is used to track spike events. Model parameters include time constants ùúè, input currents ùêº, thresholds, and reset mechanisms, which influence the rate of voltage decay and spike firing. The simulation is configured by defining neuron populations and their differential equations while specifying input currents, with spike counts serving as feature inputs for subsequent models. The fusion module utilizes a multilayer perceptron architecture and the Adam optimizer (with a learning rate of 0.001) for training, which runs over 100 epochs with a batch size of 32. To prevent overfitting, L2 regularization and dropout techniques are incorporated. Experiments are conducted in a CPU environment, offering lightweight characteristics, with the potential for performance enhancement through GPU acceleration.\n\nAs illustrated in  Fig 5,   following the data preprocessing stage, we successfully extracted five key visual feature parameters-namely speed, jitter, consonance, bigsmall, and updown-from the animation model. These parameters are integral to the operation of the model, as they encapsulate critical aspects of the visual characteristics that influence the behavior of the animation. They will serve as essential inputs for evaluating the performance of the model, offering both quantitative and qualitative support to demonstrate its effectiveness in generating and manipulating animation features. By analyzing these parameters, we can gain deeper insights into the ability of the model to process and respond to dynamic visual stimuli, ensuring its alignment with the desired outcomes and enhancing its predictive capabilities. And after the data preprocessing stage, we also successfully extracted five essential acoustic feature parameters-pitch, timbre, volume, tempo, and syllable length-from the music model. These parameters are fundamental in capturing the intricate acoustic properties that define the musical elements and structure. They will serve as vital components for assessing the performance of the model, providing both quantitative metrics and qualitative insights into the ability of the model to analyze and generate musical features. By leveraging these parameters, we can effectively evaluate the capacity of the model to reflect the nuances of musical composition, ensuring that it produces outputs that are not only consistent with the input data but also align with the intended artistic or analytical objectives. These features will be crucial in validating the overall effectiveness of the model and its ability to process complex acoustic data accurately and meaningfully.\n\nBased on the functionality of the visual cortex module, five emotional visual features are extracted from the original emotional animation video and output as feature vectors ùëã a . Similarly, the auditory cortex module extracts five emotional auditory features from the original emotional music audio, producing feature vectors ùëã m . In the fusion module, ùëã a and ùëã m are combined into a comprehensive feature vector ùëã am . This vector is used as input for a recurrent neural network based on the BEL module to learn and generate emotional positivity parameter EPP. The EPP is a critical metric for assessing the efficacy of our model, and we will offer a thorough explanation of it shortly.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Emotional Positivity Parameter",
      "text": "The EPP utilized in this study is a standardized metric designed to measure levels of emotional positivity. Inspired by the sentiment analysis tool SnowNLP, which calculates sentiment probabilities based on the emotional tendencies of words within text sentences. This parameter is then normalized to generate a value typically ranging from 0 to 1, indicating the degree of emotional inclination within the text. Our EPP adopts a similar design to the sentiment analysis tool SnowNLP  [55] . Initially, it arranges manually assessed data of the experimental subjects' five emotions (fear, sadness, anger, calmness, happiness) from negative to positive. Through weighted averaging and normalization, emotional indicators are converted into values between 0 and 1, allowing for the measurement of emotional positivity in both video and audio.\n\nThe variables ùë§ f,s,a,c,h represent the weights of fear, sad-ness, anger, calm, and happiness. The values of ùê∏ f , ùê∏ s , ùê∏ a , ùê∏ c , and ùê∏ h are the corresponding emotion scores. Determine the EPP for each sample by applying the aforementioned formula, serving as the foundation for future experiments. Given the inherent bias in the emotion scores of the chosen dataset (e.g., if the animation displays positive emotions, the negative emotion score will be 0), we maintain consistent weights across all emotion categories when performing the weighted average. This approach helps to maximize the retention of emotional tendencies within the dataset, ensuring that the calculated emotion scores effectively reflect these tendencies without overly amplifying any specific emotion. The utilization of EPP allows for the effective measurement and comparison of emotional tendencies across various stimuli, thus offering an objective and intuitive standard for emotional assessment.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Interpretability Analysis",
      "text": "The experiments in this study utilize pre-trained models to learn emotion features from both video and audio. For each sample, the perception node output ùê¥ and the inhibition node output ùëÇ are calculated. The model output ùê∏ is derived from ùê¥ and ùëÇ, applying the inhibitory effect of the orbitofrontal cortex, and is further adjusted to generate the final EPP value. Our M-BEL and A-BEL models directly calculate the Euclidean distance percentages between the actual EPP values of all targets and the generated EPP values, then average these percentages. Subsequently, an exponential function is used to convert these averaged values into similarity percentages. For the AVF-BEL model, a weighted average of the EPP is calculated for each sample, considering the proportion of emotional features blended in both animation and audio. The percentage Euclidean distance between the true EPP values and the generated EPP values for all targets is calculated, averaged, and then transformed into a similarity percentage using an exponential function. The subsequent analysis involves visual comparisons between the manually evaluated normalized EPP of each sample and the normalized EPP generated by the three models.\n\nAs illustrated in Fig  6 , it provides a visual comparison between the manually evaluated normalized EPP of each sample and the normalized EPP generated by the M-BEL  As illustrated in Fig  7 , it provides a visual comparison between the manually evaluated normalized EPP of each sample and the normalized EPP generated by the A-BEL model. Due to the limitations of the animation data features in the dataset and the incompleteness of the A-BEL model, there is also a considerable difference between the EPP generated by the A-BEL model and the actual EPP obtained through manual evaluation. However, compared to the M-BEL model, it demonstrates a significant improvement in performance. This is also consistent with the observed finding that visual features contain more emotional information than auditory features, with visual features being more likely to trigger emotional changes and other physiological phe-nomena.\n\nAs illustrated in Fig  8 , it provides a visual comparison between the manually evaluated normalized EPP of each sample and the normalized EPP generated by the AVF-BEL model. Although limited by the constraints of the data features in the dataset, the AVF-BEL model is more refined than the M-BEL and A-BEL models. The difference between the EPP generated by the AVF-BEL model and the actual EPP obtained through manual evaluation has been further reduced. This is also consistent with the physiological responses of the human brain, where emotional changes triggered by the simultaneous interaction of visual and auditory features are stronger than those induced by a single modality.\n\nAs illustrated in",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Lightweight Analysis",
      "text": "Our AVF-BEL model uses a multi-convolutional block architecture that combines neural simulation and fusion modules. This structure allows the model to accurately simulate neuronal responses with dynamic equations and use convolutional layers for spatial feature extraction. Although the convolutional layers contain a large number of parameters, the model complexity is managed by using 7√ó7 convolutional kernels and gradually increasing the number of convolutional channels. Experiments conducted in a CPU environment emphasize lightweight characteristics, allowing the model to operate effectively in resource-constrained settings. In contrast, traditional deep learning models typically utilize fixed convolutional network architectures that often entail high parameter counts and computational demands, usually requiring GPU acceleration to meet real-time processing needs, especially when handling large-scale video and audio data, where computational resource consumption becomes significantly pronounced.\n\nThe table 1 provides a comprehensive comparison between the AVF-BEL model and multimodal transformers, highlighting significant differences across several key features  [56] . The AVF-BEL model has a shallower architecture, consisting of only two layers, making it suitable for rapid responses in low-resource environments. Its small parameter count (only a few dozen) results in linear computational complexity, which enhances its efficiency and memory usage, allowing it to run effectively on central processing units (CPUs) and handle datasets smaller than 2 GB. In contrast, multimodal transformers feature a deeper architecture, typically consisting of 12 to 24 layers, and use self-attention mechanisms to effectively capture complex relationships in the data. They are designed to process large datasets exceeding 10 GB. The parameter count for these models is substantial (often in the millions to billions), and their computational complexity is quadratic, necessitating the use of graphics processing units (GPUs) or tensor processing units (TPUs). Consequently, multimodal transformers excel in high-precision and complex tasks. In summary, the AVF-BEL model is more suitable for resource-constrained applications, while multimodal transformers demonstrate superior expressive capabilities and efficiency in handling largescale data, catering to diverse application requirements.\n\nThe weight matrices ùë£ i ,ùë§ i , and ùë§ e hold pivotal roles within both the neural network and the simulation model.  local minima during the training process, which is often a challenge in complex models with large parameter spaces. By promoting the activation of only the most relevant parameters, the model enhances its ability to generalize from the training data to unseen data, ensuring more robust and scalable performance. These observations, derived from the weight parameter heatmaps, strongly suggest that the AVF-BEL model possesses lightweight attributes, with a streamlined parameter structure that supports both computational efficiency and improved generalization. In summary, the AVF-BEL model demonstrates significant lightweight advantages, as evidenced by its shallow architecture, sparse parameter distribution, and low computational complexity. By using a multi-convolutional block design with 7√ó7 convolutional kernels and progressively increasing convolutional channels, the model maintains high efficiency while minimizing the number of active parameters. This sparsity not only reduces memory and computational demands, enabling effective operation in resourceconstrained environments, but also enhances generalization by preventing overfitting and avoiding local minima during training. As such, the AVF-BEL model stands out for its balance between performance and resource efficiency, making it well-suited for low-resource applications without compromising its functional capabilities.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Ablation Study",
      "text": "In addition to using similarity metrics to assess the generated Emotion Parameter Profile EPP values, we enhance the evaluation by converting both the normalized generated EPP values and true EPP values into binary labels (0 or 1). This transformation frames the task as a classification problem, enabling the use of classification metrics such as precision, recall, and F1-score. These metrics effectively evaluate the performance of the model in distinguishing between generated and true EPP. Precision measures the proportion of true positives among predicted positives, recall assesses the proportion of true positives among actual positives, and the F1score balances both metrics. This approach provides a more comprehensive validation of the effectiveness and accuracy of the model, offering a clearer assessment of its ability to generate EPP values that align with true emotional parameters.\n\nAs shown in The M-BEL is an enhanced BEL model that reinforces auditory cortex processing, consisting of an auditory cortex module and an emotional learning module.The M-BEL model attained a precision of 0.58, a recall of 0.47, and an F1-score of 0.52. The average similarity, calculated as the average Euclidean distance transformed into a percentage, between the EPP generated by the M-BEL model and the actual EPP obtained through manual assessment is 49.06%. The A-BEL is an improved BEL model that enhances visual cortex processing, comprising a visual cortex module and an emotional learning module. The A-BEL model, which is a visual emotion model consisting of visual cortex modules and emotion learning modules, achieved a precision of 0.65, a recall of 0.69, and an F1-score of 0.67. The average similarity, calculated as the average Euclidean distance transformed into a percentage, between the EPP generated by the A-BEL model and the actual EPP obtained through manual assessment is 65.08%. By comparing these data, we found that the M-BEL and A-BEL models, which incorporate the auditory cortex and visual cortex modules into the BEL model, show some improvement in the emotional generation similarity for both music and animation data.\n\nOur ultimate AVF-BEL model integrates visual cortex modules, auditory cortex modules, fusion modules, and emotion learning modules. The AVF-BEL model, which implements the BEL module using a RNN, achieved an accuracy of 0.79, a recall of 0.77, and an F1 score of 0.78. The average similarity, calculated as the average Euclidean distance transformed into a percentage, between the EPP generated by the AVF-BEL model and the actual EPP obtained through manual assessment, is 77.69%. The visual cortex module offers more comprehensive information compared to the auditory cortex module, thereby contributing to the superior performance of the A-BEL model over the M-BEL model. The fusion module integrates visual and auditory cortical inputs to generate audiovisual fused stimuli, further enhancing the performance advantage of the AVF-BEL model. The replacement and addition of bio-inspired modules demonstrate a clear enhancement in model performance. These experimental results validate the significant improvement in emotion generation when using fused visual and auditory stimuli. Furthermore, the study highlights the superior performance of the AVF-BEL model in computational metrics, particularly its advantage in neuroanatomical alignment, offering novel insights and methodologies for research in emotion generation.\n\nThe ablation study results demonstrate the significant impact of each module within the AVF-BEL model. The addition of the visual cortex and auditory cortex modules in the M-BEL and A-BEL models, respectively, enhances the processing of sensory inputs, leading to improvements in precision, recall, and F1-score. The AVF-BEL model, which integrates both visual and auditory cortex modules along with a fusion module, outperforms all other models across all metrics. The fusion module, which integrates visual and auditory inputs, is key to enhancing the ability of the model to generate more accurate emotional responses, achieving an average similarity of 77.69%, well beyond the performance of the individual M-BEL and A-BEL models. These results emphasize the importance of integrating multisensory stimuli to enhance emotion generation and further validate the advantages of the AVF-BEL model in neuroanatomical alignment and interpretability, providing valuable insights for future research in brain-inspired emotional computation.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Conclusion",
      "text": "The AVF-BEL model presented in this study is designed to replicate the neural pathways associated with emotion learning and generation, specifically focusing on how visual and auditory perceptions are integrated and transformed into emotional stimuli in the brain. By simulating the biological neural mechanisms involved in emotional processing, this model seeks to advance the development of lightweight, neuroanatomically aligned methods for emotion learning and generation. The model not only mimics the cortical processing of emotional features related to visual and auditory stimuli but also introduces an additional level of sophistication by simulating the integration of these sensory modalities in the anterior superior temporal gyrus, an area of the brain known for its role in multisensory integration. This biomimetic approach allows the AVF-BEL model to more accurately reflect how the brain processes and combines visual and auditory emotional information, ultimately enabling the generation of emotion parameters that are more robust and nu-anced. Furthermore, by incorporating these biomimetic functionalities, the AVF-BEL model achieves enhanced interpretability, providing clearer insights into the mechanisms underlying emotion generation. The lightweight design of the model ensures computational efficiency, making it suitable for applications in resource-constrained environments. In future research, we aim to further improve the neuroanatomical alignment of the AVF-BEL model and explore the integration of additional biomimetic modules, such as those representing the hypothalamus and the cingulate gyrus. These advancements will likely enhance the ability of the model to simulate emotional regulation and processing at a more comprehensive level, potentially extending its applicability to a broader range of real-world scenarios and improving its alignment with more complex brain-based emotion generation mechanisms.",
      "page_start": 14,
      "page_end": 15
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , visual and auditory emotional",
      "page": 2
    },
    {
      "caption": "Figure 1: The brain emotional learning and generation pathway",
      "page": 2
    },
    {
      "caption": "Figure 1: , the referenced emotional path-",
      "page": 3
    },
    {
      "caption": "Figure 2: , we have designed and imple-",
      "page": 4
    },
    {
      "caption": "Figure 2: AVF-BEL model framework:The visual cortex mod-",
      "page": 4
    },
    {
      "caption": "Figure 3: Neural network architecture diagram for emotion generation with integrated visual and auditory processing for dynamic",
      "page": 5
    },
    {
      "caption": "Figure 3: , the audiovisual emotion gen-",
      "page": 5
    },
    {
      "caption": "Figure 4: , there are 400 PYR neurons, 200",
      "page": 7
    },
    {
      "caption": "Figure 4: The M-BEL model encompasses three distinct types",
      "page": 7
    },
    {
      "caption": "Figure 5: , following the data preprocessing",
      "page": 9
    },
    {
      "caption": "Figure 5: In our model, auditory and visual emotional features are represented by five key acoustic parameters(Second row)‚Äîpitch,",
      "page": 10
    },
    {
      "caption": "Figure 6: , it provides a visual comparison",
      "page": 10
    },
    {
      "caption": "Figure 6: The similarity comparison between the true EPP and",
      "page": 11
    },
    {
      "caption": "Figure 7: The similarity comparison between the true EPP and",
      "page": 11
    },
    {
      "caption": "Figure 7: , it provides a visual comparison",
      "page": 11
    },
    {
      "caption": "Figure 8: , it provides a visual comparison",
      "page": 11
    },
    {
      "caption": "Figure 1: and Fig 2, by integrating the",
      "page": 11
    },
    {
      "caption": "Figure 8: The similarity comparison between the true EPP and",
      "page": 11
    },
    {
      "caption": "Figure 9: , reveals that many of the weight values are ap-",
      "page": 12
    },
    {
      "caption": "Figure 9: Heatmap of model weight matrices: visualization of",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Visual cortex module\nMax pool\nConv 3x3\nConv 7x7\nMean \npool\nReLU\nReLU\ninduced video\nFlatten": "induced music",
          "FCv\nHidv\nAttnv\nCon": ""
        },
        {
          "Visual cortex module\nMax pool\nConv 3x3\nConv 7x7\nMean \npool\nReLU\nReLU\ninduced video\nFlatten": "",
          "FCv\nHidv\nAttnv\nCon": "Attnm\nHidm\nFCm\nFusion module"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The foundations of human and animal e motions",
      "authors": [
        "J Panksepp"
      ],
      "year": "1998",
      "venue": "The foundations of human and animal e motions"
    },
    {
      "citation_id": "2",
      "title": "Multimodal sentiment analysis based on fusion methods: A survey",
      "authors": [
        "L Zhu",
        "Z Zhu",
        "C Zhang",
        "Y Xu",
        "X Kong"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "3",
      "title": "A brief history of human society: The origin and role of emotion in social life",
      "authors": [
        "D Massey"
      ],
      "year": "2002",
      "venue": "American sociological review"
    },
    {
      "citation_id": "4",
      "title": "Modulation of human auditory information processing by emotional visual stimuli",
      "authors": [
        "V Surakka",
        "M Tenhunen-Eskelinen",
        "J Hietanen",
        "M Sams"
      ],
      "year": "1998",
      "venue": "Cognitive Brain Research"
    },
    {
      "citation_id": "5",
      "title": "Multimodal sentiment analysis: A systematic review of history, datasets, multimodal fusion methods, applications, challenges and future directions",
      "authors": [
        "A Gandhi",
        "K Adhvaryu",
        "S Poria",
        "E Cambria",
        "A Hussain"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "6",
      "title": "A systematic review on affective computing: Emotion models, databases, and recent advances",
      "authors": [
        "Y Wang",
        "W Song",
        "W Tao",
        "A Liotta",
        "D Yang",
        "X Li",
        "S Gao",
        "Y Sun",
        "W Ge",
        "W Zhang"
      ],
      "year": "2022",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "7",
      "title": "Depression recognition using remote photoplethysmography from facial videos",
      "authors": [
        "C Casado",
        "M Ca√±ellas",
        "M L√≥pez"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "8",
      "title": "The james-lange theory of emotions: A critical examination and an alternative theory",
      "authors": [
        "W Cannon"
      ],
      "year": "1927",
      "venue": "The American journal of psychology"
    },
    {
      "citation_id": "9",
      "title": "A proposed mechanism of emotion, Archives of Neurology",
      "authors": [
        "J Papez"
      ],
      "year": "1937",
      "venue": "Psychiatry"
    },
    {
      "citation_id": "10",
      "title": "Some psychiatric implications of physiological studies on frontotemporal portion of limbic system",
      "authors": [
        "P Maclean"
      ],
      "year": "1952",
      "venue": "Electroencephalography and clinical neurophysiology"
    },
    {
      "citation_id": "11",
      "title": "Visual and auditory brain areas share a representational structure that supports emotion perception",
      "authors": [
        "B Sievers",
        "C Parkinson",
        "P Kohler",
        "J Hughes",
        "S Fogelson",
        "T Wheatley"
      ],
      "year": "2021",
      "venue": "Current Biology"
    },
    {
      "citation_id": "12",
      "title": "Predicting emotion from music videos: exploring the relative contribution of visual and auditory information to affective responses",
      "authors": [
        "P Chua",
        "D Makris",
        "D Herremans",
        "G Roig",
        "K Agres"
      ],
      "year": "2022",
      "venue": "Predicting emotion from music videos: exploring the relative contribution of visual and auditory information to affective responses",
      "arxiv": "arXiv:2202.10453"
    },
    {
      "citation_id": "13",
      "title": "Emotional learning: A computational model of the amygdala",
      "authors": [
        "J Mor√©n",
        "Christian Balkenius"
      ],
      "year": "2001",
      "venue": "Cybernetics & Systems"
    },
    {
      "citation_id": "14",
      "title": "Learning based brain emotional intelligence as a new aspect for development of an alarm system",
      "authors": [
        "T Babaie",
        "R Karimizandi",
        "C Lucas"
      ],
      "year": "2008",
      "venue": "Soft Computing"
    },
    {
      "citation_id": "15",
      "title": "Brain emotional learning based brain computer interface",
      "authors": [
        "A Ghanbari",
        "E Heidari",
        "S Setayeshi",
        "I Doroud"
      ],
      "year": "2012",
      "venue": "IJCSI International Journal of Computer Science Issues"
    },
    {
      "citation_id": "16",
      "title": "Online identification of nonlinear systems using neo-fuzzy supported brain emotional learning network",
      "authors": [
        "U Farooq",
        "J Gu",
        "M Asad",
        "G Abbas",
        "A Hanif",
        "M Balas"
      ],
      "year": "2020",
      "venue": "Journal of Intelligent & Fuzzy Systems"
    },
    {
      "citation_id": "17",
      "title": "Toward artificial emotional intelligence for cooperative social human-machine interaction",
      "authors": [
        "B Erol",
        "A Majumdar",
        "P Benavidez",
        "P Rad",
        "K.-K Choo",
        "M Jamshidi"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "18",
      "title": "Emotion and learning: A computational model of the amygdala",
      "authors": [
        "J Moren"
      ],
      "year": "2004",
      "venue": "Emotion and learning: A computational model of the amygdala"
    },
    {
      "citation_id": "19",
      "title": "Supervised brain emotional learning",
      "authors": [
        "E Lotfi",
        "M.-R Akbarzadeh-T"
      ],
      "year": "2012",
      "venue": "The 2012 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "20",
      "title": "Memristive circuit design of brain-like emotional learning and generation",
      "authors": [
        "Z Wang",
        "X Wang",
        "Z Zeng"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "21",
      "title": "Memristor-based circuit design of pad emotional space and its application in mood congruity",
      "authors": [
        "J Sun",
        "Y Wang",
        "P Liu",
        "S Wen",
        "Y Wang"
      ],
      "year": "2023",
      "venue": "IEEE Internet of Things Journal"
    },
    {
      "citation_id": "22",
      "title": "Emotional chatting machine: Emotional conversation generation with internal and external memory",
      "authors": [
        "H Zhou",
        "M Huang",
        "T Zhang",
        "X Zhu",
        "B Liu"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "23",
      "title": "Learning how to smile: Expression video generation with conditional adversarial recurrent nets",
      "authors": [
        "W Wang",
        "X Alameda-Pineda",
        "D Xu",
        "E Ricci",
        "N Sebe"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "24",
      "title": "Memristive circuit design for personalized emotion generation with memory and retrieval functions",
      "authors": [
        "Z Chen",
        "X Wang",
        "C Yang",
        "Z Wang",
        "Z Zeng"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "25",
      "title": "Memristive pad three-dimensional emotion generation system based on d-s evidence theory",
      "authors": [
        "M Zhang",
        "C Wang",
        "Y Sun",
        "T Li"
      ],
      "year": "2024",
      "venue": "Nonlinear Dynamics"
    },
    {
      "citation_id": "26",
      "title": "The cingulate cortex and limbic systems for action, emotion, and memory",
      "authors": [
        "E Rolls"
      ],
      "year": "2019",
      "venue": "Handbook of clinical neurology"
    },
    {
      "citation_id": "27",
      "title": "Principles of emotional brain circuit maturation",
      "authors": [
        "M Birnie",
        "T Baram"
      ],
      "year": "2022",
      "venue": "Science"
    },
    {
      "citation_id": "28",
      "title": "The human visual cortex",
      "authors": [
        "K Grill-Spector",
        "R Malach"
      ],
      "year": "2004",
      "venue": "Annu. Rev. Neurosci"
    },
    {
      "citation_id": "29",
      "title": "Modality-general representations of valences perceived from visual and auditory modalities",
      "authors": [
        "J Gu",
        "L Cao",
        "B Liu"
      ],
      "year": "2019",
      "venue": "Neuroimage"
    },
    {
      "citation_id": "30",
      "title": "",
      "authors": [
        "T Huff",
        "N Mahabadi",
        "P Tadi",
        "Neuroanatomy"
      ],
      "year": "2018",
      "venue": ""
    },
    {
      "citation_id": "31",
      "title": "The development of human visual cortex and clinical implications",
      "authors": [
        "C Siu",
        "K Murphy"
      ],
      "year": "2018",
      "venue": "Eye and brain"
    },
    {
      "citation_id": "32",
      "title": "Brain-like object recognition with high-performing shallow recurrent anns, Advances in neural information processing systems",
      "authors": [
        "J Kubilius",
        "M Schrimpf",
        "K Kar",
        "R Rajalingham",
        "H Hong",
        "N Majaj",
        "E Issa",
        "P Bashivan",
        "J Prescott-Roy",
        "K Schmidt"
      ],
      "year": "2019",
      "venue": "Brain-like object recognition with high-performing shallow recurrent anns, Advances in neural information processing systems"
    },
    {
      "citation_id": "33",
      "title": "Global motion processing in human visual cortical areas v2 and v3",
      "authors": [
        "M Furlan",
        "A Smith"
      ],
      "year": "2016",
      "venue": "Journal of Neuroscience"
    },
    {
      "citation_id": "34",
      "title": "The auditory cortex hosts network nodes influential for emotion processing: An fmri study on music-evoked fear and joy",
      "authors": [
        "S Koelsch",
        "S Skouras",
        "G Lohmann"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "35",
      "title": "Visual area v5/mt remembers \"what\" but not \"where",
      "authors": [
        "G Campana",
        "A Cowey",
        "V Walsh"
      ],
      "year": "2006",
      "venue": "Visual area v5/mt remembers \"what\" but not \"where"
    },
    {
      "citation_id": "36",
      "title": "Anatomy of the auditory cortex",
      "authors": [
        "D Pandya"
      ],
      "year": "1995",
      "venue": "Revue neurologique"
    },
    {
      "citation_id": "37",
      "title": "Primary auditory cortex activation by visual speech: an fmri study at 3 t",
      "authors": [
        "J Pekkola",
        "V Ojanen",
        "T Autti",
        "I J√§√§skel√§inen",
        "R M√∂tt√∂nen",
        "A Tarkiainen",
        "M Sams"
      ],
      "year": "2005",
      "venue": "Neuroreport"
    },
    {
      "citation_id": "38",
      "title": "Human primary auditory cortex follows the shape of heschl's gyrus",
      "authors": [
        "S Costa",
        "W Van Der Zwaag",
        "J Marques",
        "R Frackowiak",
        "S Clarke",
        "M Saenz"
      ],
      "year": "2011",
      "venue": "Journal of Neuroscience"
    },
    {
      "citation_id": "39",
      "title": "Statedependent population coding in primary auditory cortex",
      "authors": [
        "M Pachitariu",
        "D Lyamzin",
        "M Sahani",
        "N Lesica"
      ],
      "year": "2015",
      "venue": "Journal of Neuroscience"
    },
    {
      "citation_id": "40",
      "title": "A circuit model of auditory cortex",
      "authors": [
        "Y Park",
        "M Geffen"
      ],
      "year": "2020",
      "venue": "PLoS Computational Biology"
    },
    {
      "citation_id": "41",
      "title": "Superior temporal sulcus-it's my area: or is it?",
      "authors": [
        "G Hein",
        "R Knight"
      ],
      "year": "2008",
      "venue": "Journal of cognitive neuroscience"
    },
    {
      "citation_id": "42",
      "title": "A double dissociation between anterior and posterior superior temporal gyrus for processing audiovisual speech demonstrated by electrocorticography",
      "authors": [
        "M Ozker",
        "I Schepers",
        "J Magnotti",
        "D Yoshor",
        "M Beauchamp"
      ],
      "year": "2017",
      "venue": "Journal of cognitive neuroscience"
    },
    {
      "citation_id": "43",
      "title": "An fmri study of affective congruence across visual and auditory modalities",
      "authors": [
        "C Gao",
        "C Weber",
        "D Wedell",
        "S Shinkareva"
      ],
      "year": "2020",
      "venue": "Journal of cognitive neuroscience"
    },
    {
      "citation_id": "44",
      "title": "Limbic systems for emotion and for memory, but no single limbic system",
      "authors": [
        "E Rolls"
      ],
      "year": "2015",
      "venue": "cortex"
    },
    {
      "citation_id": "45",
      "title": "Decoding spontaneous emotional states in the human brain",
      "authors": [
        "P Kragel",
        "A Knodt",
        "A Hariri",
        "K Labar"
      ],
      "year": "2016",
      "venue": "PLoS biology"
    },
    {
      "citation_id": "46",
      "title": "Text sentiment analysis of douban film short comments based on bert-cnn-bilstm-att model",
      "authors": [
        "A He",
        "M Abisado"
      ],
      "year": "2024",
      "venue": "Text sentiment analysis of douban film short comments based on bert-cnn-bilstm-att model"
    },
    {
      "citation_id": "47",
      "title": "Cornet: Modeling the neural mechanisms of core object recognition",
      "authors": [
        "J Kubilius",
        "M Schrimpf",
        "A Nayebi",
        "D Bear",
        "D Yamins",
        "J Di-Carlo"
      ],
      "year": "2018",
      "venue": "Cornet: Modeling the neural mechanisms of core object recognition"
    },
    {
      "citation_id": "48",
      "title": "Understanding auditory representations of emotional expressions with neural networks",
      "authors": [
        "I Wieser",
        "P Barros",
        "S Heinrich",
        "S Wermter"
      ],
      "year": "2020",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "49",
      "title": "Quantitative models of auditory cortical processing",
      "authors": [
        "S Sadagopan",
        "M Kar",
        "S Parida"
      ],
      "year": "2023",
      "venue": "Hearing Research"
    },
    {
      "citation_id": "50",
      "title": "Development of auditory cortex circuits",
      "authors": [
        "M Chang",
        "P Kanold"
      ],
      "year": "2021",
      "venue": "Journal of the Association for Research in Otolaryngology"
    },
    {
      "citation_id": "51",
      "title": "Multi-target doa estimation with an audio-visual fusion mechanism",
      "authors": [
        "X Qian",
        "M Madhavi",
        "Z Pan",
        "J Wang",
        "H Li"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "52",
      "title": "Multimodal audio-visual information fusion using canonical-correlated graph neural network for energy-efficient speech enhancement",
      "authors": [
        "L Passos",
        "J Papa",
        "J Del",
        "A Ser",
        "A Hussain",
        "Adeel"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "53",
      "title": "Brain emotional learning-based pattern recognizer",
      "authors": [
        "E Lotfi",
        "M.-R Akbarzadeh-T"
      ],
      "year": "2013",
      "venue": "Cybernetics and Systems"
    },
    {
      "citation_id": "54",
      "title": "Competitive brain emotional learning",
      "authors": [
        "E Lotfi",
        "O Khazaei",
        "F Khazaei"
      ],
      "year": "2018",
      "venue": "Neural Processing Letters"
    },
    {
      "citation_id": "55",
      "title": "A novel multi-criteria decision making framework based on evidential reasoning dealing with missing information from online reviews",
      "authors": [
        "S.-F He",
        "X.-H Pan",
        "Y.-M Wang",
        "D Zamora",
        "L Mart√≠nez"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "56",
      "title": "Multimedia analysis of robustly optimized multimodal transformer based on vision and language co-learning",
      "authors": [
        "J Yoon",
        "G Choi",
        "C Choi"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    }
  ]
}