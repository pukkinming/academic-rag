{
  "paper_id": "2510.17855v1",
  "title": "Cmis-Net: A Cascaded Multi-Scale Individual Standardization Network For Backchannel Agreement Estimation",
  "published": "2025-10-15T03:21:51Z",
  "authors": [
    "Yuxuan Huang",
    "Kangzhong Wang",
    "Eugene Yujun Fu",
    "Grace Ngai",
    "Peter H. F. Ng"
  ],
  "keywords": [
    "backchannel agreement estimation",
    "multi-scale modeling",
    "individual standardization",
    "communication behavior",
    "visual cues"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Backchannels are subtle listener responses, such as nods, smiles, or short verbal cues like \"yes\" or \"uh-huh,\" which convey understanding and agreement in conversations. These signals provide feedback to speakers, improve the smoothness of interaction, and play a crucial role in developing human-like, responsive AI systems. However, the expression of backchannel behaviors is often significantly influenced by individual differences, operating across multiple scales: from instant dynamics such as response intensity (frame-level) to temporal patterns such as frequency and rhythm preferences (sequence-level). This presents a complex pattern recognition problem that contemporary emotion recognition methods have yet to fully address. Particularly, existing individualized methods in emotion recognition often operate at a single scale, overlooking the complementary nature of multi-scale behavioral cues. To address these challenges, we propose a novel Cascaded Multi-Scale Individual Standardization Network (CMIS-Net) that extracts individual-normalized backchannel features by removing person-specific neutral baselines from observed expressions. Operating at both frame and sequence levels, this normalization allows model to focus on relative changes",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Recognizing subtle and short-lived patterns in sequential data is a general task in pattern recognition  [1] . Among these subtle sequential patterns, backchannel cues serve as a representative example in conversational interactions. Backchannel refer to listener's non-verbal signals and brief verbal responses during a conversation, such as nodding and smiling or uttering short acknowledgments like \"yes\" or \"uh-huh\". As a common communication strategy, backchannels enhance the fluency and effectiveness of interactions, facilitating mutual understanding and reducing potential communication breakdowns  [2] . In addition to maintaining conversational flow, backchannel behaviors reveal the listener's attitude -agreeing or disagreeing with the speaker's statements. This subjective response, quantified as \"Backchannel Agreement\", provides crucial insights into participants' opinions and interpersonal dynamics within conversations or group meetings  [3, 4] . In face-to-face interactions, listeners' backchannel responses significantly influence speakers' verbal behavior, enabling speakers to sense listeners' engagement and agreement levels and adjust their communication strategies accordingly.\n\nInvestigating backchannel behaviors and agreement patterns holds significant practical value across diverse domains. For example, in clinical contexts, psychotherapists who observe and guide patients' backchannel responses gain deeper insights into their mental states, enabling more effective treatment strategies and improved therapeutic outcomes  [5] . Beyond clinical settings, backchannel analysis enhances our understanding of nonverbal communication in interpersonal interactions. This will result in benefits including more effective communication strategies, enhanced negotiation and conflict resolution  [6, 7] , improved classroom performance  [8] , and higher quality of childcare interactions  [9] .\n\nMoreover, with the rapid advancement of artificial intelligence, enabling machines to accurately understand users' conversational engagement and attitudes has become increasingly critical for developing responsive dialogue systems and robots. The integration of backchannel agreement detection into these systems and robots represents a significant step toward more natural human-machine interactions. While linguistic and social psychological research has long recognized the significance of backchannel behavior analysis, traditional detection methods remain constrained by their reliance on expert knowledge and manual annotation. In contrast, automatic backchannel detection offers compelling advantages in efficiency, real-time responsiveness, and scalability, making it as a promising direction for practical AI applications.\n\nPrevious studies  [4, 10, 11, 12]  have explored various deep learning architectures particularly with visual features (e.g., facial features) for automatic backchannel detection and aggreement estimation. However, these approaches face a fundamental challenge: individuals exhibit distinct facial features and behavioral patterns in neutral states and backchannel expressions. These person-specific characteristics can interfere with model predictions, leading to classification and estimation errors. Nevertheless, existing research has largely overlooked the modeling of individual differences in backchannel agreement detection. Furthermore, sequential pattern recognition often operates across multiple temporal scales  [13, 14] , particularly from instantaneous response intensity at the frame level to frequency and rhythmic patterns at the sequence level. Both of them convey crucial information about listener's agreement level. However, current individual standardization methods  [15]  typically operate at a single scale, failing to leverage the complementary nature of multi-scale behavioral cues. This single-scale limitation significantly constrains model performance. While such approaches have achieved success in various emotion recognition tasks  [15, 16] , they inherently suffer from restricted representational capacity: frame-level models may capture fine-grained local details but miss broader contextual patterns, while sequence-level models may emphasize global trends at the expense of subtle momentary cues. This trade-off becomes particularly problematic in backchannel agreement estimation, where both micro-expressions occured in a frame and discourse-level patterns spanning several frames and seconds contribute essential information about the listener's agreement levels.\n\nMoreover, some studies have found that backchannel responses predominantly cluster around neutral to mildly positive expressions, often manifesting as subtle politeness markers like gentle smiling, while strongly emotional backchannels are much less common  [17] . This pattern is reinforced by social convention contexts, where individuals typically express agreement through polite and understated responses rather than intense reactions  [18, 19] . This tendency leads to an imbalanced distribution of existing backchannel agreement datasets  [4] , with most samples concentrated in the neutralto-mild range. Consequently, models may overfit to the high-density samples of the distribution, reducing their ability to generalize to emotionally intense backchannel responses.\n\nTo address these challenges, we propose a cascaded multi-scale individual standardization network that normalizes backchannel expressions relative to individual neutral traits, effectively removing person-specific variations while focusing on generalizable agreement indicators. Besides, we incorporate an augmentation module to alleviate generalization challenges caused by data imbalance. This individual-normalized feature extraction ensures that the model learns agreement patterns that represent relative changes from personal baselines rather than absolute values, significantly improving generalization across diverse expressive behaviors. Our comprehensive experiments demonstrate that CMIS-Net effectively handles individual differences and data imbalance, achieving state-of-the-art performance. Our analysis also reveals an important distinction between modalities: auditive and visual backchannels. The former tend to carry more intense emotional expressions, present greater challenges for agreement estimation. This finding suggests that future research should move beyond treating backchannels as a unified phenomenon and instead develop modality-specific approaches that account for the unique characteristics of visual and auditive responses.\n\nThe main contributions of this paper are summarized as follows:\n\n• We propose CMIS-Net, the first cascaded multi-scale individual standardization framework that disentangles individual-specific and invariant features at both frame and sequence levels for backchannel agreement estimation. Beyond backchannel analysis, the proposed framework provides a new approach that can benefit a wide range of subtle pattern recognition tasks.\n\n• We design an encoder-decoder-based translator module for sequence-level backchannel features standardization.\n\n• We introduce an implicit data augmentation module that addresses distributional bias near agreement score peaks, improving model generalization.\n\n• We perform comprehensive ablation studies and visualizations demonstrating the effectiveness of the proposed method, and provide insights into the distinct challenges posed by auditive versus visual backchannels, suggesting they should be treated as separate modalities.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Backchannel Agreement",
      "text": "Backchannel was first introduced by Victor Yngve in 1970  [20] . Since then, increasing attention has been paid to backchannel behavior by researchers in linguistics and social psychology  [21, 22] . Early studies on backchannels mainly focused on predicting when a backchannel response occurs  [23, 24] . However, relatively few studies have addressed the task of identifying whether a behavior sequence contains backchannel signals. The task of Backchannel Agreement Estimation was not formally proposed until 2022, when Philipp Müller et al.  [4]  introduced it for the first time. They provided new annotations for the group conversation dataset MPIIGroupInteraction  [25] , creating the first public annotations for rating the strength of backchannel agreement.\n\nIn addition, they established baseline results for this new task. Specifically, they evaluated models using different modalities, including head and facial movements, body posture, and audio features. Their results showed that head motion features achieved the best performance in estimating agreement strength.\n\nSubsequently, Garima Sharma et al.  [10]  proposed a graph based approach. In their method, group interactions are modeled as a graph, where each participant is a node and edges represent social interactions between individuals. Graph convolution and edge convolution are used to capture both node features and local interaction structures. Two types of graphs were constructed: static graphs, in which each data sample has a single associated graph, and dynamic graphs, in which temporal continuity is modeled by connecting each node to itself across time steps. Similar to Müller et al.'s findings  [4] , head motion features outperformed other modalities in the agreement estimation task.\n\nMoreover, Ahmed Amer et al.  [11]  explored the use of different Transformer-based architectures for automatic backchannel detection and agreement estimation  [26] . In their single-stream setting, concatenated body and facial features pass through a Transformer layer. A two-layer stacked version with shared linear prediction achieved the best agreement estimation performance among existing methods. However, most current works focus primarily on feature selection and multimodal fusion strategies. Although some studies note that predictions may be affected by individual-specific behaviors (e.g., facial expressions), they do not systematically explore the impact of individual differences on backchannel agreement estimation. Therefore, this paper proposes a personalized backchannel agreement estimation framework.\n\nOur method disentangles individual-specific neutral features from original representations, allowing the model to focus on features more likely to contain meaningful and generalizable agreement signals to improve prediction robustness.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Individual Differences",
      "text": "Mitigating the influence of individual differences in pattern recognition remains a fundamental challenge. Due to variations in physiology, expressive capability, cultural background, and habitual behaviors, the manner in which different individuals express patterns can differ significantly. Such inter-individual variability often obscures the universal cues that models aim to learn, thereby reducing generalization performance and model robustness  [27, 28] . Similar challenges are frequently encountered in domains such as micro-expression recognition  [29] , stress detection  [30]  and facial expression recognition  [31] , where the target signals are transient and highly susceptible to individual-specific traits.\n\nIn this context, backchannel agreement recognition provides a representative example of sequential pattern recognition, where individual differences complicate the task. Although individual differences have not yet been explicitly explored in the task of backchannel agreement estimation, existing studies have shown that individuals vary significantly in terms of response frequency, modality preference, emotional intensity, of communication behaviors including backchanneling. These differences are influenced by factors such as personality traits, cultural communication norms, and prior conversational experiences  [32, 33] . For instance, some individuals may frequently and actively nodding even in low-engagement contexts, while others may withhold agreement cues to enhance persuasive impact. Such individual variation can affect model predictions and potentially reduce generalization.\n\nPrevious works attempted to reduce the impact of individual differences by building separate classifiers for different types of individuals  [34] . However, these methods require maintaining multiple classifiers, leading to increased model complexity, higher training costs, and limited extensibility and generalizability. Some approaches avoid building multiple models by introducing personalized parameters  [35]  , while others utilize transfer learning  [36, 37]  or domain adaptation methods  [38, 39] . Nevertheless, these approaches typically rely on access to some information from the target domain during training, which is often unrealistic in real-world scenarios. Moreover, their generalization ability may degrade when applied to unseen domains.\n\nThere has been limited discussion on methods that explicitly disentangle individualspecific features and preserve individual-invariant cues during modeling. Evidence from other domains suggests that such disentanglement strategies can enhance model robustness and performance. Previous studies have shown that facial expressions can be modeled as the superposition of a individual-specific neutral face and emotional components  [40] . Building on similar assumptions, the Individual Standardization Network (ISNet)  [15]  demonstrated how speech signals can be decomposed into neutral and emotional components. Inspired by these approaches, this paper investigates individual standardization methods for backchannel agreement estimation, focusing specifically on visual features and cascaded multi-scale models.",
      "page_start": 6,
      "page_end": 8
    },
    {
      "section_name": "Cascaded Multi-Scale Modeling",
      "text": "Cascaded multi-scale architectures refer to models that process information at multiple granularities through cascaded stages. In such designs, lower tiers extract finegrained features over short intervals, and higher tiers aggregate these into coarser representations. Cascaded multi-scale modeling captures both local and global patterns by integrating hierarchical features across different levels of abstraction. These complementary representations work synergistically to improve the performance.  [41]  propose an MSC-RNN for radar signal classification, with a two-tier RNN architecture that first separates clutter from targets using a short-timescale RNN, and then classifies the target type using a long-timescale RNN.  [42]  develop a hierarchical recurrent encoder-decoder for context-aware query suggestion, which encodes sequences of prior queries and generates follow-up queries. This model uses a low-level RNN per query and a high-level RNN for the session, making it sensitive to both immediate and long-range query context.  [43]  present a compact CNN for facial emotion recognition extended with a frame-to-sequence GRU. They first apply a deep CNN to each video frame and then feed the per-frame output distributions into a GRU to model temporal evolution of expressions. This cascaded structure leverages multi-frame context. In this work, we comprehensively explore the Cascaded Multi-Scale Individual Standardization Network for backchannel agreement estimation.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Cascaded Multi-Scale Individual Standardization Network (Cmis-Net)",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Data Pre-Processing",
      "text": "In our method, we first extract facial landmarks from the input videos using Open-Face 2.0  [44]  to minimize background-induced noise. Besides, rather than using raw landmarks directly, we compute inter-frame differences to generate motion-based features I ∈ R (M-1)×H , before feeding them into the CMIS-Net for backchannel agreement It is worth noting that the core component of CMIS-Net is the standardization procedure incorporating with translator modules. We systematically evaluated various design configurations and combinations for the components to determine the optimal framework for video-based backchannel agreement prediction.",
      "page_start": 8,
      "page_end": 10
    },
    {
      "section_name": "The General Translator Framework And Standardization Process",
      "text": "While the frame-level and sequence-level standardization frameworks operate at different scales, they follow a unified training and standardization procedure, as illustrated in Fig.  2 . Both frameworks employ parallel encoder architectures, E n and E e to process neutral (non-backchannel) and emotional (backchannel) samples respectively, enabling consistent feature extraction across temporal scales.\n\nIn particular, the neutral encoder (E n ) takes N different neutral samples of the same individual as input. Each sample is encoded into a neutral feature vector. By averaging these N vectors, we obtain a statistical benchmark representation that captures individual-specific characteristics.\n\nWe denote this as the neutral feature pipeline (marked in red in Fig.  2 ), which applies to both the Frame-Level Neutral Encoder and Sequence-Level Neutral Encoder modules, i.e., the FLNE (E f _n ) and SLNE (E s_n ) in Fig.  1 .\n\nThe emotional encoder (E e ) takes the original sample as input and encodes it into an emotional feature vector. This vector is then passed to the translator, which aims to disentangle the individual-specific neutral components from the emotional representation.Through the standardizer based on the subtraction operation, the standardized emotional feature vector is generated.\n\nWe refer to this as the emotional feature pipeline (marked in blue in Fig.  2 ). This pipeline similarly applies to both the frame-level (involving FLEE and FLT) and sequencelevel (involving SLEE and SLT) emotional feature encoding.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "The Training Process Of Encoders And Translators",
      "text": "The encoders and translators follow a two-step training process. In the first step, we train the neutral feature pipeline and the emotional encoders within the emotional feature pipeline. The neutral feature encoder (for both FLNE and SLNE) processes N neutral samples from the same individual as the emotional encoder's input. An approximation loss trains the neutral encoder to extract individual-specific basline expressions:\n\nwhere B is the batch size and d(v n j , v n j+1 ) represents the L1 distance between neutral features v n j and v n j+1 . This approach enables the neutral encoder to extract robust, stable individual-specific neutral feature representations.\n\nThe emotional feature pipeline, operating at both frame and sequence levels. In the first training step, we pre-train emotional encoders at each level (FLEE and SLEE) to directly estimate agreement levels. A temporary regression predictor R ensures the encoder learns meaningful features. After training, R is removed and the encoder parameters are frozen. For backchannel agreement prediction, we use a regression loss L R based on Mean Squared Error (MSE):\n\nwhere y represents ground truth labels and ŷ denotes predicted scores.\n\nThe initial emotional feature training step prepares the emotional encoders for subsequent fine-tuning with translators using L T . The translator loss is designed to extract individual-specific neutral features from emotional representations, preserving information relevant to more generalizable backchannel agreement:\n\nIn",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "The Detailed Multi-Scale Process",
      "text": "The frame-level processing begins with the Frame-Level Neutral Encoder (FLNE, The standardization (i.e., a subtraction operation) is then applied to v s_e and vs_n to extract relative emotional patterns, yielding generalizable sequence-level backchannel representations (v s_e ) for subsequent modeling.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Temporal Attention Pooling Module (Tap) And Regression",
      "text": "Unlike overt verbal expressions, backchannel are typically brief, occur in specific interactional moments, and are closely tied to the rhythm and timing of the speaker's utterances. These behaviors are not evenly distributed throughout the communication, but appear at specific critical moments. To capture this fleeting and context-dependent behavior, we introduce a temporal attention module. Temporal Attention allows the model to focus on these key moments by calculating weights for each frame. Fig.  3  shows the framework of this module Given a sequence of feature representations after the sequence-level standardization (v s_e = vs_e 1 , . . . , vs_e t , . . . , vs_e N ), this module applies linear projection to each frame to calculate a scalar attention score and applies a non-linear activation function tanh to improve expressiveness. A sigmoid is used to generate bounded importance weights.\n\nThese weights are used to re-weight each frame.\n\nThe final sequence-level representation is obtained by averaging the weighted features over time.\n\nwhere T is the number of frames. This design enables the network to down-weight uninformative frames while retaining those that contribute most to the target behavior, such as moments where a nod or backchannel utterance occurs.\n\nFinally, a regression model (R) takes the feature vector z after the multi-scale standardization and TAP as input to estimate the final agreement level. It is composed of multiple fully connected layers, enabling the extraction of high-level representations for accurate estimation. Similar to R, R is trained by L R . However, it is worth to note that, only the parameters of R are updated in this step, while the emotional encoder remains fixed.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "Translator Module",
      "text": "",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Frame-Level Translator (Flt)",
      "text": "Before frame-level encoding, we compute inter-frame differences to capture facial motion. These frame-level features encode individual movements, such as head nods, shakes, lip movements, reflecting the inherently dynamic nature of backchannel behaviors, which are conveyed through subtle motions rather than static expressions. To extract the individual-specific neutral features in frame-level, we introduce a FLT module, which consists of three stacked Frame-Level Translator Blocks (FLTB) (Fig.  4 ).\n\nFLTB generates query Q, key K, and value V vectors for each frame via linear projections. Rather than computing cross-frame attention, it calculates a scalar attention score per frame using the dot product between that frame's query and key vectors: Where, σ(•) is the sigmoid function, acting as a soft gate to modulate the value vector:\n\nThis vector f temp is then linearly projected back to the original input dimension f pro j and added to the original frame representation via a residual connection, followed by layer normalization: Although the frame-level standardization module captures useful intra-frame behavioral cues, it fails to account for temporal dynamics across frames. In our frame-work, the sequence-level features represent the frequency and temporal patterns of these actions over time. Sequence-level patterns-such as nodding frequency, repetition, or rhythm-can provide additional cues that are crucial for estimating backchannel agreement. To capture temporal features, we introduce an Encoder-Decoder LSTM (ED-LSTM) module as SLT, which aims to extract individual-invariant features from the input sequence. As shown in Fig.  5 , the ED-LSTM consists of an encoder LSTM, an attention layer, and a decoder LSTM.\n\nThe encoder module takes a sequence of frame-level features as input and passes them through a unidirectional LSTM to generate a hidden representation. It encodes the underlying sequential structure of the input behavior. To enhance the contextual attention of sequence-level representation, we add an attention module between the encoder and decoder. The decoder receives the attention-weighted context vector as input. This context vector is then passed through an LSTM decoder to reconstruct the sequence-level neutral features from emotional features.",
      "page_start": 15,
      "page_end": 17
    },
    {
      "section_name": "Implicit Data Augmentation (Ida)",
      "text": "During our experiments, we observed that the distribution of backchannel agreement data was imbalanced. Most samples had agreement levels concentrated near the agreement score peak (around 0.25 in the dataset), and the extreme high-and low-level samples were rare. This imbalance led the model to learn local concentrated feature patterns during training, which undermines its ability to generalize to data with different distributions. The detailed analysis is presented in Section 5.1.2.\n\nTo tackle this issue, we draw inspiration from Implicit Data Augmentation methods  [45, 46, 47] , which generate new feature representations in the hidden space instead of directly synthesizing input data. Accordingly, we propose an Implicit Data Augmentation (IDA) module to address this problem. It aims to reduce the model's reliance on the highly concentrated regions of the data distribution and improve its generalization performance. After the emotional encoder, we introduce this module, as illustrated in By adding this module, the model is exposed to more diversities during training, which improves its robustness to edge-case samples, enhancing overall generalizability.",
      "page_start": 17,
      "page_end": 18
    },
    {
      "section_name": "Experiments Details",
      "text": "",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Dataset",
      "text": "We use the MPIIGroupInteraction dataset  [25, 4]  for backchannel agreement estimation. MPIIGroupInteraction dataset is a publicly available dataset that provides annotations of backchannel occurrences and agreements expressed through backchannels in group interactions. The dataset consists of 78 German-speaking participants.\n\nEach interaction involves three or four participants engaging in a conversation in a quiet office environment. The dataset includes both audio recordings of the conversa-",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Experimental Settings",
      "text": "Empirical studies from both our work and prior research demonstrate that the relevant backchannel cues typically occur only in the final second of the video samples.\n\nParticularly, previous work  [11]  use the last three seconds as input yields the most effective results. Shorter intervals may lack sufficient contextual information, whereas longer intervals tend to introduce excessive non-task-related content. Based on this, we use OpenFace 2.0  [44]  to extract facial landmarks from each video frame as the initial features. To effectively capture facial dynamics during the critical period, we process only the last 3 seconds of each video and compute the coordinate differences between each frame and its subsequent frame, using the resulting landmark variation rates as input features for the model  [12] .\n\nOur model is implemented in PyTorch and trained on an NVIDIA GeForce RTX 2080 Ti GPU. The model was trained with the Stochastic Gradient Descent (SGD) optimizer, using its default parameter settings, where the momentum was set to 0.9 and the weight decay coefficient to 1e-4. The batch size was set to 32, and training was performed for a total of 100 epochs. A StepLR learning rate scheduler was employed to decay the learning rate at fixed intervals. The initial learning rate was set to 0.01 and multiplied by 0.1 every 20 epochs. The Transformer encoder consists of 6 stacked\n\nTransformer layers, each with 4 attention heads.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Results",
      "text": "We conduct comprehensive evaluation experiments, which comprises three parts: component contribution analysis, visualization studies on individual standardization effectiveness, and comparative experiments with state-of-the-art methods. Following the evaluation protocol from  [4] , we train on their training set, evaluate on the validation set, and use Mean Squared Error (MSE) to measure the estimation performance. Previous studies  [48, 49]  have demonstrated that encoder-decoder architectures possess strong sequence modeling capabilities. Based on this, we adopt such an architecture to construct our translator module. We experimented with various temporal models as encoders and decoders to identify the optimal temporal structure for the translator. This includes attention-based  [15] , GRU-based, and LSTM-based modules (ED-LSTM in Section 3.3.2, our proposed approach). The ablation results are presented in Tabel 1.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Ablation Study",
      "text": "",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Impact Of The Appropriately Configured Slt",
      "text": "Our LSTM-based model achieves better performance with an MSE of 0.057962, outperforming all counterparts including the translator model from previous work  [15] .\n\nThis improvement may be attributed to the ability of ED-LSTM to effectively capture contextual dependencies and generate more representative neutral features. Moreover, in sequence-related tasks, studies have shown that LSTM generally outperforms GRU in capturing temporal dependencies with higher accuracy  [50] , which aligns with the findings in our task.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Impact Of Ida Module",
      "text": "As shown in Fig.  6  (green curves), the training samples are densely distributed in the range of 0 to 0.5, and few samples show disagreement or strong agreement.\n\nIn particular, the sample density peaks around 0.25. This uneven distribution causes the model to overly focus on samples near 0.25, resulting in overfitting and reduced generalization ability. Consequently, the prediction results tend to concentrate around 0.25 during inference. As shown in Fig.  6  (b1), this issue becomes even more obvious in the distribution of prediction results on the validation set. The model's prediction range is mainly limited to -0.25 to 0.6, and the density peak around 0.25 is significantly higher than that of the ground truth distribution of the validation set.\n\nTo address this problem, we introduced the IDA module. By adding noise perturbations to the embedding in the latent space, this module helps prevent the model from after incorporating the IDA module.\n\nTo investigate the effect of IDA, we design four different noise injection strategies.\n\n• Non-Augmentation: Without both FLEE Augmentation (FLEE Aug.) and SLEE Augmentation(SLEE-Aug.).\n\n• FLEE Aug. only: With FLEE Aug. only.\n\n• SLEE Aug. only: With SLEE Aug. only.\n\n• Double Augmentation: With both FLEE Aug. and SLEE Aug..\n\nAs shown in Tabel 2, our results show that applying IDA individually after either the FLEE or the SLEE stage leads to notable performance improvements. Interestingly, the best performance is achieved when noise is injected at both stages simultaneously, indicating that multi-level noise augmentation provides complementary benefits.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Impact Of Different Neutral Feature Selection Strategies",
      "text": "Another important aspect of our work is the selection of neutral samples. We propose the following two ways of neutral sample selection.\n\n• Peak-Based Neutral Selection: We assume that samples with agreement levels concentrated around the peak of the training data distribution (e.g., near 0.25) represent neutral expressions. This assumption is based on the observation in Fig.  6  that the dataset shows a high density of samples in this range, which may indicate a commonly occurring, baseline response state.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "The Contribution Of Each Component",
      "text": "We conduct ablation studies to investigate the contribution of each module in our framework, including the impact of single-and multi-scale individual standardizations, as shown in Table  4 . In the single-scale setup, the best performance was achieved by the sequence-level module, reaching an MSE of 0.059249. It suggests that a framelevel module alone may struggle to capture sufficient backchannel-related information.\n\nGiven that backchannel behaviors are typically expressed in a continuous and temporally dependent manner, this observation aligns with intuitive expectations. Moreover, the results show that performance is also influenced by the choice of pooling method. Specifically, in the single-scale setting, adding SLT to the sequencelevel module resulted in degraded performance under TAP. However, this should not be interpreted as SLT failing to extract meaningful neutral features. Interestingly, when using global average pooling, the same SLT-enhanced module achieved better performance. This aligns with prior research suggesting that temporal pooling doesn't universally guarantee better results, as global pooling can sometimes more effectively integrate information across entire sequences.",
      "page_start": 22,
      "page_end": 22
    },
    {
      "section_name": "Visualizing The Effect Of Cmis-Net",
      "text": "To intuitively demonstrate the effectiveness of our proposed framework and the individual standardization process, we conducted a feature visualization experiment.\n\nSpecifically, we applied t-distributed Stochastic Neighbor Embedding (t-SNE) to project the high-dimensional features into a 2D space.\n\nAs shown in Fig.  7 , the top and bottom rows present the features before and after standardization respectively. For better visualization, we divide the agreement levels  the green and gray centroids from different individuals exhibit greater alignment after standardization. Similar patterns are observed in Group 2 (green), Group 3 (blue and gray) and Group 4 (green). The scatters show that before standardization, samples from different individuals and agreement categories are distributed chaotically. After standardization, same-category samples cluster together (blue and orange samples form distinct clusters in Group 1, so as do orange and gray in Group 2, and blue and orange in Group 3). This demonstrates that standardization significantly reduces emotional confusion between individuals.\n\nWhile standardization effectively reduces individual bias by eliminating personspecific neutral patterns, complete separation of emotional states remains challenging, particularly in 2D spaces as shown by partial overlap between orange circles and green squares in Group3. This aligns with prior findings  [15] , though such ambiguity in 2D visualization may be distinguishable in the original high-dimensional space.\n\nWe calculate the minimum inter-individual distance between each agreement category center of one individual and all different agreement category centers, except the target category, of another individual. This measures how well different agreement categories are separated across individuals in the feature space. In the visualization (Fig.  7 ), two shapes (circles and squares) represent two different individuals, while four colors indicate different agreement categories. Taking the gray circle center as an example, we compute distances from this center to all non-gray square centers and select the minimum value. Larger minimum distances indicate better separation between agreement categories across individuals, suggesting features can more effectively distinguish agreement levels. In general, the minimum distances increase after standardization (Table  5 ), demonstrating improved category separation.",
      "page_start": 23,
      "page_end": 24
    },
    {
      "section_name": "State-Of-The-Art Comparison",
      "text": "We evaluated CMIS-Net against recent methods on the benchmark backchannel agreement validation set. As shown in Table  6 , our method achieves the best MSE of 0.057962 -the first to drop below 0.060 for this task. This improvement are mainly attributed to addressing individual differences and data distribution issues that existing methods overlook while focusing on multi-modal fusion. Notably, our approach achieves superior performance even with a single-scale standardization. Backchannel communication includes visual and auditory modalities  [4] . Particularly, people typically express polite agreement during interactions, and often with auditory feedback (e.g., \"yes,\" \"right,\" \"no\") conveying stronger agreement responses.\n\nThis explains the label distribution peak around 0.25, with fewer samples showing disagreement or strong agreement. In our study, we piloted with Light-ASD  [51]  model to separately analyze the impact of visual and auditory backchannels to agreement estimation. It generates frame-level scores (negative for silence, positive for speech) at 25 fps. Following  [4] 's criterion that auditory backchannel events last ≥ 0.25 seconds, we classified samples as auditory if seven consecutive frames in the last three seconds had scores > 0.2; otherwise, they were classified as visual backchannel samples.\n\nAs shown in Tabel 7, we obtained 2792 visual backchannel samples and 566 auditive backchannel samples, totaling 3,358 instances in the training set. We then computed the absolute values of the agreement scores for all samples and performed statistical analysis. The results indicate that auditive samples show higher average emotion intensity than visual samples. This trend is consistently observed across the 25th, 50th, and 75th percentiles. These findings suggest that auditive backchannel are more likely to convey stronger emotional intensity compared to visual backchannel. In order to gain deeper insights, we conducted the following experiments:\n\n• all-to-all: All available samples are used for both training and validation.\n\n• all-to-visual: The training set includes all (visual + auditive) samples, while the validation set consists only of visual backchannel samples.\n\n• visual-to-all: The training set consists only of visual backchannel samples, while the validation set includes all samples.\n\n• visual-to-visual: Both the training and validation sets consist exclusively of visual backchannel samples.",
      "page_start": 26,
      "page_end": 27
    },
    {
      "section_name": "Mode Agreement (Mse ↓)",
      "text": "all-to all 0.057962 visual-to-all 0.058857 all-to-visual 0.053065 visual-to-visual 0.053844\n\nIn the backchannel agreement estimation experiments described above, the all-tovisual setting yields the lowest MSE. As shown in Tabel 8, the performance of the all-to-visual setting surpasses that of the all-to-all setting. It indicates that the model has already achieved relatively good predictive accuracy on visual backchannel samples, whereas its performance on auditory samples remains suboptimal. This may be due to the limited auditory backchannel samples, which constrains the model's ability to generalize to such cases. As previously discussed, auditive backchannels are often associated with stronger emotional expressions, making accurate prediction of these high-intensity samples especially meaningful. Moreover, the observation that the visual-to-all setting performs slightly worse than the all-to-all setting suggests that there may be a distributional gap between visual and auditory backchannel samples.\n\nTherefore, it is necessary to explicitly consider both visual and auditory backchannels in protocol modeling.",
      "page_start": 28,
      "page_end": 28
    },
    {
      "section_name": "Conclusions",
      "text": "In",
      "page_start": 29,
      "page_end": 29
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of the CMIS-Net architecture (a), utilizing two cascaded standardization modules: 1.",
      "page": 9
    },
    {
      "caption": "Figure 1: The proposed model aims to",
      "page": 10
    },
    {
      "caption": "Figure 2: Both frameworks employ parallel encoder architectures, En and Ee to",
      "page": 10
    },
    {
      "caption": "Figure 2: Training workflow for the translator module and standardization. The neutral feature pipeline (red",
      "page": 11
    },
    {
      "caption": "Figure 1: The emotional encoder (Ee) takes the original sample as input and encodes it into",
      "page": 11
    },
    {
      "caption": "Figure 3: The Temporal Attention Pooling (TAP) Module.",
      "page": 14
    },
    {
      "caption": "Figure 3: shows the framework of this module",
      "page": 14
    },
    {
      "caption": "Figure 4: The FLTB (left) and FLT (right) Module.",
      "page": 16
    },
    {
      "caption": "Figure 5: The ED-LSTM Translator Module.",
      "page": 16
    },
    {
      "caption": "Figure 5: , the ED-LSTM consists of an encoder LSTM,",
      "page": 17
    },
    {
      "caption": "Figure 1: During the training phase, this module adds element-wise Gaussian noise with",
      "page": 17
    },
    {
      "caption": "Figure 6: (green curves), the training samples are densely distributed",
      "page": 20
    },
    {
      "caption": "Figure 6: (b1), this issue becomes even more obvious",
      "page": 20
    },
    {
      "caption": "Figure 6: , after applying IDA, the",
      "page": 20
    },
    {
      "caption": "Figure 6: Overall comparison of density distributions before and after applying IDA. Green, blue, and red",
      "page": 21
    },
    {
      "caption": "Figure 6: that the dataset shows a high density of samples in this range, which may",
      "page": 21
    },
    {
      "caption": "Figure 7: , the top and bottom rows present the features before and after",
      "page": 23
    },
    {
      "caption": "Figure 6: , we discretized the continuous agree-",
      "page": 24
    },
    {
      "caption": "Figure 7: , we observe that the standardization process brings centroids of the",
      "page": 24
    },
    {
      "caption": "Figure 7: Feature distribution visualization across four groups before (top) and after (bottom) standardiza-",
      "page": 25
    },
    {
      "caption": "Figure 7: ), two shapes (circles and squares) represent two different individuals, while",
      "page": 26
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "CMIS-Net: A Cascaded Multi-Scale Individual": "Standardization Network for Backchannel Agreement"
        },
        {
          "CMIS-Net: A Cascaded Multi-Scale Individual": "Estimation"
        },
        {
          "CMIS-Net: A Cascaded Multi-Scale Individual": "Yuxuan Huanga,b, Kangzhong Wangc, Eugene Yujun Fud,∗,"
        },
        {
          "CMIS-Net: A Cascaded Multi-Scale Individual": "Grace Ngaic, Peter H.F. Ngb,c"
        },
        {
          "CMIS-Net: A Cascaded Multi-Scale Individual": "aKey Laboratory of Advanced Medical Imaging and Intelligent Computing of Guizhou Province,"
        },
        {
          "CMIS-Net: A Cascaded Multi-Scale Individual": "Engineering Research Center of Text Computing, Ministry of Education, State Key Laboratory of Public Big"
        },
        {
          "CMIS-Net: A Cascaded Multi-Scale Individual": "Data, College of Computer Science and Technology, Guizhou University, Guiyang, 550025, China"
        },
        {
          "CMIS-Net: A Cascaded Multi-Scale Individual": "bDepartment of Rehabilitation Sciences, The Hong Kong Polytechnic University, Hong Kong, China"
        },
        {
          "CMIS-Net: A Cascaded Multi-Scale Individual": "cDepartment of Computing, The Hong Kong Polytechnic University, Hong Kong, China"
        },
        {
          "CMIS-Net: A Cascaded Multi-Scale Individual": "dCentre for Learning, Teaching and Technology, The Education University of Hong Kong, Hong"
        },
        {
          "CMIS-Net: A Cascaded Multi-Scale Individual": "Kong, China"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "from each person’s baseline rather than absolute expression values. Furthermore, we": "introduce an implicit data augmentation module to address the observed training data"
        },
        {
          "from each person’s baseline rather than absolute expression values. Furthermore, we": "distributional bias,\nimproving model generalization. Comprehensive experiments and"
        },
        {
          "from each person’s baseline rather than absolute expression values. Furthermore, we": "visualizations demonstrate that CMIS-Net effectively handles\nindividual differences"
        },
        {
          "from each person’s baseline rather than absolute expression values. Furthermore, we": "and data imbalance, achieving state-of-the-art performance in backchannel agreement"
        },
        {
          "from each person’s baseline rather than absolute expression values. Furthermore, we": "detection. Nevertheless, we believe that the effectiveness of individual standardization"
        },
        {
          "from each person’s baseline rather than absolute expression values. Furthermore, we": "for subtle sequential pattern recognition holds the potential to offer a generalizable ap-"
        },
        {
          "from each person’s baseline rather than absolute expression values. Furthermore, we": "proach for various pattern recognition tasks that involve person-specific variations. Our"
        },
        {
          "from each person’s baseline rather than absolute expression values. Furthermore, we": "code can be found in: https://github.com/AffectiveComputingLab-HK/CMIS-Net."
        },
        {
          "from each person’s baseline rather than absolute expression values. Furthermore, we": "Keywords:\nbackchannel agreement estimation, multi-scale modeling, individual"
        },
        {
          "from each person’s baseline rather than absolute expression values. Furthermore, we": "standardization, communication behavior, visual cues"
        },
        {
          "from each person’s baseline rather than absolute expression values. Furthermore, we": "1.\nIntroduction"
        },
        {
          "from each person’s baseline rather than absolute expression values. Furthermore, we": "Recognizing subtle and short-lived patterns in sequential data is a general\ntask in"
        },
        {
          "from each person’s baseline rather than absolute expression values. Furthermore, we": "pattern recognition[1]. Among these subtle sequential patterns, backchannel cues serve"
        },
        {
          "from each person’s baseline rather than absolute expression values. Furthermore, we": "as a representative example in conversational\ninteractions. Backchannel\nrefer\nto lis-"
        },
        {
          "from each person’s baseline rather than absolute expression values. Furthermore, we": "tener’s non-verbal signals and brief verbal\nresponses during a conversation,\nsuch as"
        },
        {
          "from each person’s baseline rather than absolute expression values. Furthermore, we": "nodding and smiling or uttering short acknowledgments like \"yes\" or \"uh-huh\". As a"
        },
        {
          "from each person’s baseline rather than absolute expression values. Furthermore, we": "common communication strategy, backchannels enhance the fluency and effectiveness"
        },
        {
          "from each person’s baseline rather than absolute expression values. Furthermore, we": "of interactions, facilitating mutual understanding and reducing potential communica-"
        },
        {
          "from each person’s baseline rather than absolute expression values. Furthermore, we": "tion breakdowns [2].\nIn addition to maintaining conversational flow, backchannel be-"
        },
        {
          "from each person’s baseline rather than absolute expression values. Furthermore, we": "haviors reveal the listener’s attitude – agreeing or disagreeing with the speaker’s state-"
        },
        {
          "from each person’s baseline rather than absolute expression values. Furthermore, we": "ments.\nThis subjective response, quantified as ”Backchannel Agreement”, provides"
        },
        {
          "from each person’s baseline rather than absolute expression values. Furthermore, we": "crucial\ninsights into participants’ opinions and interpersonal dynamics within conver-"
        },
        {
          "from each person’s baseline rather than absolute expression values. Furthermore, we": "sations or group meetings [3, 4].\nIn face-to-face interactions,\nlisteners’ backchannel"
        },
        {
          "from each person’s baseline rather than absolute expression values. Furthermore, we": "responses significantly influence speakers’ verbal behavior, enabling speakers to sense"
        },
        {
          "from each person’s baseline rather than absolute expression values. Furthermore, we": "listeners’ engagement and agreement levels and adjust their communication strategies"
        },
        {
          "from each person’s baseline rather than absolute expression values. Furthermore, we": "accordingly."
        },
        {
          "from each person’s baseline rather than absolute expression values. Furthermore, we": "Investigating backchannel behaviors and agreement patterns holds significant prac-"
        },
        {
          "from each person’s baseline rather than absolute expression values. Furthermore, we": "tical value across diverse domains. For example, in clinical contexts, psychotherapists"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "who observe and guide patients’ backchannel responses gain deeper insights into their": "mental states, enabling more effective treatment strategies and improved therapeutic"
        },
        {
          "who observe and guide patients’ backchannel responses gain deeper insights into their": "outcomes\n[5].\nBeyond clinical\nsettings, backchannel analysis enhances our under-"
        },
        {
          "who observe and guide patients’ backchannel responses gain deeper insights into their": "standing of nonverbal communication in interpersonal\ninteractions.\nThis will\nresult"
        },
        {
          "who observe and guide patients’ backchannel responses gain deeper insights into their": "in benefits including more effective communication strategies, enhanced negotiation"
        },
        {
          "who observe and guide patients’ backchannel responses gain deeper insights into their": "and conflict resolution [6, 7], improved classroom performance [8], and higher quality"
        },
        {
          "who observe and guide patients’ backchannel responses gain deeper insights into their": "of childcare interactions [9]."
        },
        {
          "who observe and guide patients’ backchannel responses gain deeper insights into their": "Moreover, with the rapid advancement of artificial intelligence, enabling machines"
        },
        {
          "who observe and guide patients’ backchannel responses gain deeper insights into their": "to accurately understand users’ conversational engagement and attitudes has become"
        },
        {
          "who observe and guide patients’ backchannel responses gain deeper insights into their": "increasingly critical for developing responsive dialogue systems and robots. The inte-"
        },
        {
          "who observe and guide patients’ backchannel responses gain deeper insights into their": "gration of backchannel agreement detection into these systems and robots represents a"
        },
        {
          "who observe and guide patients’ backchannel responses gain deeper insights into their": "significant step toward more natural human-machine interactions. While linguistic and"
        },
        {
          "who observe and guide patients’ backchannel responses gain deeper insights into their": "social psychological research has long recognized the significance of backchannel be-"
        },
        {
          "who observe and guide patients’ backchannel responses gain deeper insights into their": "havior analysis,\ntraditional detection methods remain constrained by their reliance on"
        },
        {
          "who observe and guide patients’ backchannel responses gain deeper insights into their": "expert knowledge and manual annotation. In contrast, automatic backchannel detection"
        },
        {
          "who observe and guide patients’ backchannel responses gain deeper insights into their": "offers compelling advantages in efficiency,\nreal-time responsiveness, and scalability,"
        },
        {
          "who observe and guide patients’ backchannel responses gain deeper insights into their": "making it as a promising direction for practical AI applications."
        },
        {
          "who observe and guide patients’ backchannel responses gain deeper insights into their": "Previous studies [4, 10, 11, 12] have explored various deep learning architectures"
        },
        {
          "who observe and guide patients’ backchannel responses gain deeper insights into their": "particularly with visual\nfeatures (e.g.,\nfacial\nfeatures)\nfor automatic backchannel de-"
        },
        {
          "who observe and guide patients’ backchannel responses gain deeper insights into their": "tection and aggreement estimation. However,\nthese approaches face a fundamental"
        },
        {
          "who observe and guide patients’ backchannel responses gain deeper insights into their": "challenge:\nindividuals exhibit distinct facial features and behavioral patterns in neutral"
        },
        {
          "who observe and guide patients’ backchannel responses gain deeper insights into their": "states and backchannel expressions.\nThese person-specific characteristics can inter-"
        },
        {
          "who observe and guide patients’ backchannel responses gain deeper insights into their": "fere with model predictions,\nleading to classification and estimation errors. Neverthe-"
        },
        {
          "who observe and guide patients’ backchannel responses gain deeper insights into their": "less, existing research has largely overlooked the modeling of individual differences in"
        },
        {
          "who observe and guide patients’ backchannel responses gain deeper insights into their": "backchannel agreement detection."
        },
        {
          "who observe and guide patients’ backchannel responses gain deeper insights into their": "Furthermore, sequential pattern recognition often operates across multiple tempo-"
        },
        {
          "who observe and guide patients’ backchannel responses gain deeper insights into their": "ral scales [13, 14], particularly from instantaneous response intensity at the frame level"
        },
        {
          "who observe and guide patients’ backchannel responses gain deeper insights into their": "to frequency and rhythmic patterns at the sequence level. Both of them convey crucial"
        },
        {
          "who observe and guide patients’ backchannel responses gain deeper insights into their": "information about\nlistener’s agreement\nlevel. However, current\nindividual standard-"
        },
        {
          "who observe and guide patients’ backchannel responses gain deeper insights into their": "ization methods [15] typically operate at a single scale, failing to leverage the comple-"
        },
        {
          "who observe and guide patients’ backchannel responses gain deeper insights into their": "mentary nature of multi-scale behavioral cues. This single-scale limitation significantly"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "constrains model performance. While such approaches have achieved success in vari-": "ous emotion recognition tasks [15, 16], they inherently suffer from restricted represen-"
        },
        {
          "constrains model performance. While such approaches have achieved success in vari-": "tational capacity:\nframe-level models may capture fine-grained local details but miss"
        },
        {
          "constrains model performance. While such approaches have achieved success in vari-": "broader contextual patterns, while sequence-level models may emphasize global trends"
        },
        {
          "constrains model performance. While such approaches have achieved success in vari-": "at the expense of subtle momentary cues. This trade-off becomes particularly problem-"
        },
        {
          "constrains model performance. While such approaches have achieved success in vari-": "atic in backchannel agreement estimation, where both micro-expressions occured in"
        },
        {
          "constrains model performance. While such approaches have achieved success in vari-": "a frame and discourse-level patterns spanning several\nframes and seconds contribute"
        },
        {
          "constrains model performance. While such approaches have achieved success in vari-": "essential information about the listener’s agreement levels."
        },
        {
          "constrains model performance. While such approaches have achieved success in vari-": "Moreover, some studies have found that backchannel responses predominantly clus-"
        },
        {
          "constrains model performance. While such approaches have achieved success in vari-": "ter around neutral to mildly positive expressions, often manifesting as subtle politeness"
        },
        {
          "constrains model performance. While such approaches have achieved success in vari-": "markers like gentle smiling, while strongly emotional backchannels are much less com-"
        },
        {
          "constrains model performance. While such approaches have achieved success in vari-": "mon [17]. This pattern is reinforced by social convention contexts, where individuals"
        },
        {
          "constrains model performance. While such approaches have achieved success in vari-": "typically express agreement\nthrough polite and understated responses rather\nthan in-"
        },
        {
          "constrains model performance. While such approaches have achieved success in vari-": "tense reactions[18, 19]. This tendency leads to an imbalanced distribution of existing"
        },
        {
          "constrains model performance. While such approaches have achieved success in vari-": "backchannel agreement datasets [4], with most samples concentrated in the neutral-"
        },
        {
          "constrains model performance. While such approaches have achieved success in vari-": "to-mild range. Consequently, models may overfit\nto the high-density samples of\nthe"
        },
        {
          "constrains model performance. While such approaches have achieved success in vari-": "distribution,\nreducing their ability to generalize to emotionally intense backchannel"
        },
        {
          "constrains model performance. While such approaches have achieved success in vari-": "responses."
        },
        {
          "constrains model performance. While such approaches have achieved success in vari-": "To address these challenges, we propose a cascaded multi-scale individual stan-"
        },
        {
          "constrains model performance. While such approaches have achieved success in vari-": "dardization network that normalizes backchannel expressions\nrelative to individual"
        },
        {
          "constrains model performance. While such approaches have achieved success in vari-": "neutral traits, effectively removing person-specific variations while focusing on gener-"
        },
        {
          "constrains model performance. While such approaches have achieved success in vari-": "alizable agreement indicators. Besides, we incorporate an augmentation module to al-"
        },
        {
          "constrains model performance. While such approaches have achieved success in vari-": "leviate generalization challenges caused by data imbalance. This individual-normalized"
        },
        {
          "constrains model performance. While such approaches have achieved success in vari-": "feature extraction ensures that\nthe model\nlearns agreement patterns that represent rel-"
        },
        {
          "constrains model performance. While such approaches have achieved success in vari-": "ative changes from personal baselines rather\nthan absolute values,\nsignificantly im-"
        },
        {
          "constrains model performance. While such approaches have achieved success in vari-": "proving generalization across diverse expressive behaviors. Our comprehensive ex-"
        },
        {
          "constrains model performance. While such approaches have achieved success in vari-": "periments demonstrate that CMIS-Net effectively handles individual differences and"
        },
        {
          "constrains model performance. While such approaches have achieved success in vari-": "data imbalance, achieving state-of-the-art performance. Our analysis also reveals an"
        },
        {
          "constrains model performance. While such approaches have achieved success in vari-": "important distinction between modalities: auditive and visual backchannels. The for-"
        },
        {
          "constrains model performance. While such approaches have achieved success in vari-": "mer tend to carry more intense emotional expressions, present greater challenges for"
        },
        {
          "constrains model performance. While such approaches have achieved success in vari-": "agreement estimation. This finding suggests that future research should move beyond"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "The main contributions of this paper are summarized as follows:": "• We propose CMIS-Net,\nthe first cascaded multi-scale individual\nstandardiza-"
        },
        {
          "The main contributions of this paper are summarized as follows:": "tion framework that disentangles\nindividual-specific and invariant\nfeatures at"
        },
        {
          "The main contributions of this paper are summarized as follows:": "both frame and sequence levels for backchannel agreement estimation. Beyond"
        },
        {
          "The main contributions of this paper are summarized as follows:": "backchannel analysis, the proposed framework provides a new approach that can"
        },
        {
          "The main contributions of this paper are summarized as follows:": "benefit a wide range of subtle pattern recognition tasks."
        },
        {
          "The main contributions of this paper are summarized as follows:": "• We design an encoder-decoder-based translator module for sequence-level backchan-"
        },
        {
          "The main contributions of this paper are summarized as follows:": "nel features standardization."
        },
        {
          "The main contributions of this paper are summarized as follows:": "• We introduce an implicit data augmentation module that addresses distributional"
        },
        {
          "The main contributions of this paper are summarized as follows:": "bias near agreement score peaks, improving model generalization."
        },
        {
          "The main contributions of this paper are summarized as follows:": "• We perform comprehensive ablation studies and visualizations demonstrating"
        },
        {
          "The main contributions of this paper are summarized as follows:": "the effectiveness of the proposed method, and provide insights into the distinct"
        },
        {
          "The main contributions of this paper are summarized as follows:": "challenges posed by auditive versus visual backchannels, suggesting they should"
        },
        {
          "The main contributions of this paper are summarized as follows:": "be treated as separate modalities."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "In addition,\nthey established baseline results for this new task. Specifically,\nthey eval-": "uated models using different modalities,\nincluding head and facial movements, body"
        },
        {
          "In addition,\nthey established baseline results for this new task. Specifically,\nthey eval-": "posture, and audio features. Their results showed that head motion features achieved"
        },
        {
          "In addition,\nthey established baseline results for this new task. Specifically,\nthey eval-": "the best performance in estimating agreement strength."
        },
        {
          "In addition,\nthey established baseline results for this new task. Specifically,\nthey eval-": "Subsequently, Garima Sharma et al. [10] proposed a graph based approach. In their"
        },
        {
          "In addition,\nthey established baseline results for this new task. Specifically,\nthey eval-": "method, group interactions are modeled as a graph, where each participant is a node and"
        },
        {
          "In addition,\nthey established baseline results for this new task. Specifically,\nthey eval-": "edges represent social\ninteractions between individuals. Graph convolution and edge"
        },
        {
          "In addition,\nthey established baseline results for this new task. Specifically,\nthey eval-": "convolution are used to capture both node features and local interaction structures. Two"
        },
        {
          "In addition,\nthey established baseline results for this new task. Specifically,\nthey eval-": "types of graphs were constructed: static graphs, in which each data sample has a single"
        },
        {
          "In addition,\nthey established baseline results for this new task. Specifically,\nthey eval-": "associated graph, and dynamic graphs,\nin which temporal continuity is modeled by"
        },
        {
          "In addition,\nthey established baseline results for this new task. Specifically,\nthey eval-": "connecting each node to itself across time steps. Similar to Müller et al.’s findings[4],"
        },
        {
          "In addition,\nthey established baseline results for this new task. Specifically,\nthey eval-": "head motion features outperformed other modalities in the agreement estimation task."
        },
        {
          "In addition,\nthey established baseline results for this new task. Specifically,\nthey eval-": "Moreover, Ahmed Amer et al. [11] explored the use of different Transformer-based"
        },
        {
          "In addition,\nthey established baseline results for this new task. Specifically,\nthey eval-": "architectures for automatic backchannel detection and agreement estimation [26].\nIn"
        },
        {
          "In addition,\nthey established baseline results for this new task. Specifically,\nthey eval-": "their single-stream setting, concatenated body and facial features pass through a Trans-"
        },
        {
          "In addition,\nthey established baseline results for this new task. Specifically,\nthey eval-": "former layer. A two-layer stacked version with shared linear prediction achieved the"
        },
        {
          "In addition,\nthey established baseline results for this new task. Specifically,\nthey eval-": "best agreement estimation performance among existing methods."
        },
        {
          "In addition,\nthey established baseline results for this new task. Specifically,\nthey eval-": "However, most current works focus primarily on feature selection and multimodal"
        },
        {
          "In addition,\nthey established baseline results for this new task. Specifically,\nthey eval-": "fusion strategies.\nAlthough some studies note that predictions may be affected by"
        },
        {
          "In addition,\nthey established baseline results for this new task. Specifically,\nthey eval-": "individual-specific behaviors (e.g., facial expressions),\nthey do not systematically ex-"
        },
        {
          "In addition,\nthey established baseline results for this new task. Specifically,\nthey eval-": "plore the impact of individual differences on backchannel agreement estimation. There-"
        },
        {
          "In addition,\nthey established baseline results for this new task. Specifically,\nthey eval-": "fore, this paper proposes a personalized backchannel agreement estimation framework."
        },
        {
          "In addition,\nthey established baseline results for this new task. Specifically,\nthey eval-": "Our method disentangles individual-specific neutral features from original representa-"
        },
        {
          "In addition,\nthey established baseline results for this new task. Specifically,\nthey eval-": "tions, allowing the model\nto focus on features more likely to contain meaningful and"
        },
        {
          "In addition,\nthey established baseline results for this new task. Specifically,\nthey eval-": "generalizable agreement signals to improve prediction robustness."
        },
        {
          "In addition,\nthey established baseline results for this new task. Specifically,\nthey eval-": "2.2.\nIndividual Differences"
        },
        {
          "In addition,\nthey established baseline results for this new task. Specifically,\nthey eval-": "Mitigating the influence of individual differences in pattern recognition remains a"
        },
        {
          "In addition,\nthey established baseline results for this new task. Specifically,\nthey eval-": "fundamental challenge. Due to variations in physiology, expressive capability, cultural"
        },
        {
          "In addition,\nthey established baseline results for this new task. Specifically,\nthey eval-": "background, and habitual behaviors, the manner in which different individuals express"
        },
        {
          "In addition,\nthey established baseline results for this new task. Specifically,\nthey eval-": "patterns can differ significantly.\nSuch inter-individual variability often obscures the"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "universal cues that models aim to learn,\nthereby reducing generalization performance": "and model robustness [27, 28]. Similar challenges are frequently encountered in do-"
        },
        {
          "universal cues that models aim to learn,\nthereby reducing generalization performance": "mains such as micro-expression recognition [29], stress detection[30] and facial ex-"
        },
        {
          "universal cues that models aim to learn,\nthereby reducing generalization performance": "pression recognition [31], where the target signals are transient and highly susceptible"
        },
        {
          "universal cues that models aim to learn,\nthereby reducing generalization performance": "to individual-specific traits."
        },
        {
          "universal cues that models aim to learn,\nthereby reducing generalization performance": "In this context, backchannel agreement recognition provides a representative exam-"
        },
        {
          "universal cues that models aim to learn,\nthereby reducing generalization performance": "ple of sequential pattern recognition, where individual differences complicate the task."
        },
        {
          "universal cues that models aim to learn,\nthereby reducing generalization performance": "Although individual differences have not yet been explicitly explored in the task of"
        },
        {
          "universal cues that models aim to learn,\nthereby reducing generalization performance": "backchannel agreement estimation, existing studies have shown that\nindividuals vary"
        },
        {
          "universal cues that models aim to learn,\nthereby reducing generalization performance": "significantly in terms of response frequency, modality preference, emotional intensity,"
        },
        {
          "universal cues that models aim to learn,\nthereby reducing generalization performance": "of communication behaviors including backchanneling.\nThese differences are influ-"
        },
        {
          "universal cues that models aim to learn,\nthereby reducing generalization performance": "enced by factors such as personality traits, cultural communication norms, and prior"
        },
        {
          "universal cues that models aim to learn,\nthereby reducing generalization performance": "conversational experiences [32, 33]. For instance, some individuals may frequently and"
        },
        {
          "universal cues that models aim to learn,\nthereby reducing generalization performance": "actively nodding even in low-engagement contexts, while others may withhold agree-"
        },
        {
          "universal cues that models aim to learn,\nthereby reducing generalization performance": "ment cues to enhance persuasive impact. Such individual variation can affect model"
        },
        {
          "universal cues that models aim to learn,\nthereby reducing generalization performance": "predictions and potentially reduce generalization."
        },
        {
          "universal cues that models aim to learn,\nthereby reducing generalization performance": "Previous works attempted to reduce the impact of individual differences by building"
        },
        {
          "universal cues that models aim to learn,\nthereby reducing generalization performance": "separate classifiers for different\ntypes of\nindividuals [34]. However,\nthese methods"
        },
        {
          "universal cues that models aim to learn,\nthereby reducing generalization performance": "require maintaining multiple classifiers, leading to increased model complexity, higher"
        },
        {
          "universal cues that models aim to learn,\nthereby reducing generalization performance": "training costs, and limited extensibility and generalizability. Some approaches avoid"
        },
        {
          "universal cues that models aim to learn,\nthereby reducing generalization performance": "building multiple models by introducing personalized parameters [35] , while others"
        },
        {
          "universal cues that models aim to learn,\nthereby reducing generalization performance": "utilize transfer learning [36, 37] or domain adaptation methods [38, 39]. Nevertheless,"
        },
        {
          "universal cues that models aim to learn,\nthereby reducing generalization performance": "these approaches typically rely on access to some information from the target domain"
        },
        {
          "universal cues that models aim to learn,\nthereby reducing generalization performance": "during training, which is often unrealistic in real-world scenarios. Moreover,\ntheir"
        },
        {
          "universal cues that models aim to learn,\nthereby reducing generalization performance": "generalization ability may degrade when applied to unseen domains."
        },
        {
          "universal cues that models aim to learn,\nthereby reducing generalization performance": "There has been limited discussion on methods that explicitly disentangle individual-"
        },
        {
          "universal cues that models aim to learn,\nthereby reducing generalization performance": "specific features and preserve individual-invariant cues during modeling.\nEvidence"
        },
        {
          "universal cues that models aim to learn,\nthereby reducing generalization performance": "from other domains suggests that such disentanglement strategies can enhance model"
        },
        {
          "universal cues that models aim to learn,\nthereby reducing generalization performance": "robustness and performance. Previous studies have shown that facial expressions can"
        },
        {
          "universal cues that models aim to learn,\nthereby reducing generalization performance": "be modeled as the superposition of a individual-specific neutral\nface and emotional"
        },
        {
          "universal cues that models aim to learn,\nthereby reducing generalization performance": "components\n[40].\nBuilding on similar assumptions,\nthe Individual Standardization"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Network (ISNet) [15] demonstrated how speech signals can be decomposed into neu-": "tral and emotional components.\nInspired by these approaches,\nthis paper investigates"
        },
        {
          "Network (ISNet) [15] demonstrated how speech signals can be decomposed into neu-": "individual\nstandardization methods\nfor backchannel agreement estimation,\nfocusing"
        },
        {
          "Network (ISNet) [15] demonstrated how speech signals can be decomposed into neu-": "specifically on visual features and cascaded multi-scale models."
        },
        {
          "Network (ISNet) [15] demonstrated how speech signals can be decomposed into neu-": "2.3. Cascaded Multi-Scale Modeling"
        },
        {
          "Network (ISNet) [15] demonstrated how speech signals can be decomposed into neu-": "Cascaded multi-scale architectures refer to models that process information at mul-"
        },
        {
          "Network (ISNet) [15] demonstrated how speech signals can be decomposed into neu-": "tiple granularities through cascaded stages.\nIn such designs,\nlower tiers extract fine-"
        },
        {
          "Network (ISNet) [15] demonstrated how speech signals can be decomposed into neu-": "grained features over short intervals, and higher tiers aggregate these into coarser rep-"
        },
        {
          "Network (ISNet) [15] demonstrated how speech signals can be decomposed into neu-": "resentations. Cascaded multi-scale modeling captures both local and global patterns by"
        },
        {
          "Network (ISNet) [15] demonstrated how speech signals can be decomposed into neu-": "integrating hierarchical features across different\nlevels of abstraction. These comple-"
        },
        {
          "Network (ISNet) [15] demonstrated how speech signals can be decomposed into neu-": "mentary representations work synergistically to improve the performance."
        },
        {
          "Network (ISNet) [15] demonstrated how speech signals can be decomposed into neu-": "[41] propose an MSC-RNN for radar signal classification, with a two-tier RNN ar-"
        },
        {
          "Network (ISNet) [15] demonstrated how speech signals can be decomposed into neu-": "chitecture that first separates clutter from targets using a short-timescale RNN, and then"
        },
        {
          "Network (ISNet) [15] demonstrated how speech signals can be decomposed into neu-": "classifies the target type using a long-timescale RNN. [42] develop a hierarchical recur-"
        },
        {
          "Network (ISNet) [15] demonstrated how speech signals can be decomposed into neu-": "rent encoder–decoder for context-aware query suggestion, which encodes sequences of"
        },
        {
          "Network (ISNet) [15] demonstrated how speech signals can be decomposed into neu-": "prior queries and generates follow-up queries. This model uses a low-level RNN per"
        },
        {
          "Network (ISNet) [15] demonstrated how speech signals can be decomposed into neu-": "query and a high-level RNN for the session, making it sensitive to both immediate and"
        },
        {
          "Network (ISNet) [15] demonstrated how speech signals can be decomposed into neu-": "long-range query context.\n[43] present a compact CNN for facial emotion recognition"
        },
        {
          "Network (ISNet) [15] demonstrated how speech signals can be decomposed into neu-": "extended with a frame-to-sequence GRU. They first apply a deep CNN to each video"
        },
        {
          "Network (ISNet) [15] demonstrated how speech signals can be decomposed into neu-": "frame and then feed the per-frame output distributions into a GRU to model\ntemporal"
        },
        {
          "Network (ISNet) [15] demonstrated how speech signals can be decomposed into neu-": "evolution of expressions. This cascaded structure leverages multi-frame context.\nIn"
        },
        {
          "Network (ISNet) [15] demonstrated how speech signals can be decomposed into neu-": "this work, we comprehensively explore the Cascaded Multi-Scale Individual Standard-"
        },
        {
          "Network (ISNet) [15] demonstrated how speech signals can be decomposed into neu-": "ization Network for backchannel agreement estimation."
        },
        {
          "Network (ISNet) [15] demonstrated how speech signals can be decomposed into neu-": "3. Cascaded Multi-Scale Individual Standardization Network (CMIS-Net)"
        },
        {
          "Network (ISNet) [15] demonstrated how speech signals can be decomposed into neu-": "3.1. Data Pre-Processing"
        },
        {
          "Network (ISNet) [15] demonstrated how speech signals can be decomposed into neu-": "In our method, we first extract facial landmarks from the input videos using Open-"
        },
        {
          "Network (ISNet) [15] demonstrated how speech signals can be decomposed into neu-": "Face 2.0 [44] to minimize background-induced noise. Besides, rather than using raw"
        },
        {
          "Network (ISNet) [15] demonstrated how speech signals can be decomposed into neu-": "landmarks directly, we compute inter-frame differences to generate motion-based fea-"
        },
        {
          "Network (ISNet) [15] demonstrated how speech signals can be decomposed into neu-": "tures I ∈ R(M−1)×H, before feeding them into the CMIS-Net for backchannel agreement"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Frame-Level Translator": "FLT\nSLT\nSequence-Level Translator\nIDA\nImplicit Data Augmentation\nStandardization\nS"
        },
        {
          "Frame-Level Translator": "(ED-LSTM)"
        },
        {
          "Frame-Level Translator": "Figure 1: Overview of the CMIS-Net architecture (a), utilizing two cascaded standardization modules: 1."
        },
        {
          "Frame-Level Translator": "Frame-level standardization (b) extracts instant dynamic features while considering individual-specific varia-"
        },
        {
          "Frame-Level Translator": "tions from each frame; and 2. Sequence-level standardization captures temporal patterns across frames while"
        },
        {
          "Frame-Level Translator": "further disentangling person-specific characteristics. Video inputs are processed through both modules se-"
        },
        {
          "Frame-Level Translator": "quentially to obtain generalizable representations for backchannel agreement estimation."
        },
        {
          "Frame-Level Translator": "detection. This differential\nrepresentation emphasizes temporal dynamics over static"
        },
        {
          "Frame-Level Translator": "appearance, allowing the model\nto capture the subtle facial movements characteris-"
        },
        {
          "Frame-Level Translator": "tic of backchannel responses. This follows the established practices in state-of-the-art"
        },
        {
          "Frame-Level Translator": "backchannel detection research [11, 12]."
        },
        {
          "Frame-Level Translator": "3.2. Architecture and Workflow of CMIS-Net"
        },
        {
          "Frame-Level Translator": "Although backchannel behaviors are often regarded as socially normative responses,"
        },
        {
          "Frame-Level Translator": "individual neutral and backchannel expressions can vary across people.\nFor exam-"
        },
        {
          "Frame-Level Translator": "ple, some may maintain subtle positive expressions like gentle smiles as their neutral"
        },
        {
          "Frame-Level Translator": "state during conversation, while others naturally exhibit more stoic or impassive facial"
        },
        {
          "Frame-Level Translator": "configurations.\nThese person-specific neutral\ntraits may interfere with model\njudg-"
        },
        {
          "Frame-Level Translator": "ment.\nInspired by previous findings in emotion recognition [15], we model backchan-"
        },
        {
          "Frame-Level Translator": "nel agreement expressions as a composite signal containing both individual-specific"
        },
        {
          "Frame-Level Translator": "neutral traits and generalizable backchannel agreement patterns. Our approach extracts"
        },
        {
          "Frame-Level Translator": "individual-normalized features by removing person-specific neutral baselines from ob-"
        },
        {
          "Frame-Level Translator": "served expressions. This normalization process eliminates the impacts of\nindividual"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "differences, allowing the model to focus on relative changes from each person’s base-": "line rather than absolute expression variation values."
        },
        {
          "differences, allowing the model to focus on relative changes from each person’s base-": "The proposed multi-scale modeling approach comprises a frame-level\nindividual"
        },
        {
          "differences, allowing the model to focus on relative changes from each person’s base-": "standardization module that captures instantaneous normalized dynamics and a sequence-"
        },
        {
          "differences, allowing the model to focus on relative changes from each person’s base-": "level individual standardization module that normalizes temporal patterns. The overall"
        },
        {
          "differences, allowing the model to focus on relative changes from each person’s base-": "architecture of the proposed model\nis shown in Fig. 1. The proposed model aims to"
        },
        {
          "differences, allowing the model to focus on relative changes from each person’s base-": "extract\nindividual-invariant behavioral patterns of backchannel\nresponses from video"
        },
        {
          "differences, allowing the model to focus on relative changes from each person’s base-": "data.\nConsistent with prior\nindividual standardization work, we categorize samples"
        },
        {
          "differences, allowing the model to focus on relative changes from each person’s base-": "based on relative backchannel\nintensity:\n“neutral” samples contain no backchannel"
        },
        {
          "differences, allowing the model to focus on relative changes from each person’s base-": "responses, while “emotional” samples exhibit relatively higher levels of backchannel"
        },
        {
          "differences, allowing the model to focus on relative changes from each person’s base-": "behavior. Section 5.1.3 presents the empirical methodology for this classification."
        },
        {
          "differences, allowing the model to focus on relative changes from each person’s base-": "It is worth noting that the core component of CMIS-Net is the standardization pro-"
        },
        {
          "differences, allowing the model to focus on relative changes from each person’s base-": "cedure incorporating with translator modules. We systematically evaluated various"
        },
        {
          "differences, allowing the model to focus on relative changes from each person’s base-": "design configurations and combinations for the components to determine the optimal"
        },
        {
          "differences, allowing the model to focus on relative changes from each person’s base-": "framework for video-based backchannel agreement prediction."
        },
        {
          "differences, allowing the model to focus on relative changes from each person’s base-": "3.2.1. The General Translator Framework and Standardization Process"
        },
        {
          "differences, allowing the model to focus on relative changes from each person’s base-": "While the frame-level and sequence-level standardization frameworks operate at"
        },
        {
          "differences, allowing the model to focus on relative changes from each person’s base-": "different scales,\nthey follow a unified training and standardization procedure, as illus-"
        },
        {
          "differences, allowing the model to focus on relative changes from each person’s base-": "trated in Fig. 2. Both frameworks employ parallel encoder architectures, En and Ee\nto"
        },
        {
          "differences, allowing the model to focus on relative changes from each person’s base-": "process neutral (non-backchannel) and emotional (backchannel) samples respectively,"
        },
        {
          "differences, allowing the model to focus on relative changes from each person’s base-": "enabling consistent feature extraction across temporal scales."
        },
        {
          "differences, allowing the model to focus on relative changes from each person’s base-": "In particular, the neutral encoder (En) takes N different neutral samples of the same"
        },
        {
          "differences, allowing the model to focus on relative changes from each person’s base-": "individual as input.\nEach sample is encoded into a neutral\nfeature vector. By aver-"
        },
        {
          "differences, allowing the model to focus on relative changes from each person’s base-": "aging these N vectors, we obtain a statistical benchmark representation that captures"
        },
        {
          "differences, allowing the model to focus on relative changes from each person’s base-": "individual-specific characteristics."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Average\nAve\nImplicit Data Augmentation\nIDA": "Figure 2: Training workflow for the translator module and standardization. The neutral feature pipeline (red"
        },
        {
          "Average\nAve\nImplicit Data Augmentation\nIDA": "paths) generates averaged neutral representations from multiple neutral samples, while the emotional feature"
        },
        {
          "Average\nAve\nImplicit Data Augmentation\nIDA": "pipeline (blue paths) performs feature disentanglement through the translator module."
        },
        {
          "Average\nAve\nImplicit Data Augmentation\nIDA": "applies to both the Frame-Level Neutral Encoder and Sequence-Level Neutral Encoder"
        },
        {
          "Average\nAve\nImplicit Data Augmentation\nIDA": "modules, i.e., the FLNE (E f _n) and SLNE (E s_n) in Fig. 1."
        },
        {
          "Average\nAve\nImplicit Data Augmentation\nIDA": "The emotional encoder (Ee) takes the original sample as input and encodes it\ninto"
        },
        {
          "Average\nAve\nImplicit Data Augmentation\nIDA": "an emotional feature vector. This vector is then passed to the translator, which aims"
        },
        {
          "Average\nAve\nImplicit Data Augmentation\nIDA": "to disentangle the individual-specific neutral components from the emotional represen-"
        },
        {
          "Average\nAve\nImplicit Data Augmentation\nIDA": "tation.Through the standardizer based on the subtraction operation,\nthe standardized"
        },
        {
          "Average\nAve\nImplicit Data Augmentation\nIDA": "emotional feature vector is generated."
        },
        {
          "Average\nAve\nImplicit Data Augmentation\nIDA": "(3)\nve = Ee(Ie)"
        },
        {
          "Average\nAve\nImplicit Data Augmentation\nIDA": "ve = ve − T (ve)\n(4)"
        },
        {
          "Average\nAve\nImplicit Data Augmentation\nIDA": "We refer\nto this as the emotional\nfeature pipeline (marked in blue in Fig.\n2).\nThis"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3.2.2. The Training Process of Encoders and Translators": "The encoders and translators follow a two-step training process.\nIn the first step,"
        },
        {
          "3.2.2. The Training Process of Encoders and Translators": "we train the neutral feature pipeline and the emotional encoders within the emotional"
        },
        {
          "3.2.2. The Training Process of Encoders and Translators": "feature pipeline. The neutral feature encoder (for both FLNE and SLNE) processes N"
        },
        {
          "3.2.2. The Training Process of Encoders and Translators": "neutral samples from the same individual as the emotional encoder’s input. An approxi-"
        },
        {
          "3.2.2. The Training Process of Encoders and Translators": "mation loss trains the neutral encoder to extract individual-specific basline expressions:"
        },
        {
          "3.2.2. The Training Process of Encoders and Translators": "B\nN\n(cid:88)\nN−1(cid:88)\n(cid:88)"
        },
        {
          "3.2.2. The Training Process of Encoders and Translators": "1 B\n(5)\nd(vn\nLE = −\nj , vn"
        },
        {
          "3.2.2. The Training Process of Encoders and Translators": "j+1)"
        },
        {
          "3.2.2. The Training Process of Encoders and Translators": "i=1\nj=1\nj+1"
        },
        {
          "3.2.2. The Training Process of Encoders and Translators": "where B is the batch size and d(vn\nj , vn\nj+1) represents the L1 distance between neutral"
        },
        {
          "3.2.2. The Training Process of Encoders and Translators": "j and vn\nj+1. This approach enables the neutral encoder to extract robust, stable"
        },
        {
          "3.2.2. The Training Process of Encoders and Translators": "individual-specific neutral feature representations."
        },
        {
          "3.2.2. The Training Process of Encoders and Translators": "The emotional\nfeature pipeline, operating at both frame and sequence levels.\nIn"
        },
        {
          "3.2.2. The Training Process of Encoders and Translators": "the first training step, we pre-train emotional encoders at each level (FLEE and SLEE)"
        },
        {
          "3.2.2. The Training Process of Encoders and Translators": "to directly estimate agreement\nlevels. A temporary regression predictor\nR ensures the"
        },
        {
          "3.2.2. The Training Process of Encoders and Translators": "encoder learns meaningful features. After training,\nR is removed and the encoder pa-"
        },
        {
          "3.2.2. The Training Process of Encoders and Translators": "rameters are frozen. For backchannel agreement prediction, we use a regression loss"
        },
        {
          "3.2.2. The Training Process of Encoders and Translators": "LR based on Mean Squared Error (MSE):"
        },
        {
          "3.2.2. The Training Process of Encoders and Translators": "B"
        },
        {
          "3.2.2. The Training Process of Encoders and Translators": "(cid:88)"
        },
        {
          "3.2.2. The Training Process of Encoders and Translators": "1 B\n(6)\nLR = −\n(yi − ˆyi)2"
        },
        {
          "3.2.2. The Training Process of Encoders and Translators": "i=1"
        },
        {
          "3.2.2. The Training Process of Encoders and Translators": "where y represents ground truth labels and ˆy denotes predicted scores."
        },
        {
          "3.2.2. The Training Process of Encoders and Translators": "The initial emotional feature training step prepares the emotional encoders for sub-"
        },
        {
          "3.2.2. The Training Process of Encoders and Translators": "sequent fine-tuning with translators using LT . The translator loss is designed to extract"
        },
        {
          "3.2.2. The Training Process of Encoders and Translators": "individual-specific neutral\nfeatures from emotional\nrepresentations, preserving infor-"
        },
        {
          "3.2.2. The Training Process of Encoders and Translators": "mation relevant to more generalizable backchannel agreement:"
        },
        {
          "3.2.2. The Training Process of Encoders and Translators": "B"
        },
        {
          "3.2.2. The Training Process of Encoders and Translators": "(cid:88)"
        },
        {
          "3.2.2. The Training Process of Encoders and Translators": "1 B\n(7)\nd(vn\nLT = −\nave, ˆvn)"
        },
        {
          "3.2.2. The Training Process of Encoders and Translators": "i=1"
        },
        {
          "3.2.2. The Training Process of Encoders and Translators": "In the second training stage (particularly for\nthe emotional\nfeature pipeline),\nthe"
        },
        {
          "3.2.2. The Training Process of Encoders and Translators": "trained neutral encoder extracts N neutral\nfeatures from videos of\nthe same individ-"
        },
        {
          "3.2.2. The Training Process of Encoders and Translators": "ual as Ie"
        },
        {
          "3.2.2. The Training Process of Encoders and Translators": "ave. The\ni , which are averaged to create a benchmark neutral representation vn"
        },
        {
          "3.2.2. The Training Process of Encoders and Translators": "loss measures the distance between this benchmark and the Translator’s output neutral"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "features, encouraging the Translator to extract individual-specific neutral features from": "emotional representations."
        },
        {
          "features, encouraging the Translator to extract individual-specific neutral features from": "This approach enhances the model’s ability to improve generalizability across indi-"
        },
        {
          "features, encouraging the Translator to extract individual-specific neutral features from": "viduals.\nIt encourages the translator to extract\nindividual-specific neutral components"
        },
        {
          "features, encouraging the Translator to extract individual-specific neutral features from": "from the emotional\nfeatures. After standardization,\nit\nretains the information more"
        },
        {
          "features, encouraging the Translator to extract individual-specific neutral features from": "likely associated with the relative expression values indicative of backchannel agree-"
        },
        {
          "features, encouraging the Translator to extract individual-specific neutral features from": "ment. This disentanglement process allows the model\nto focus on generalizable com-"
        },
        {
          "features, encouraging the Translator to extract individual-specific neutral features from": "municative cues rather\nthan individual-specific traits, and effectively addressing the"
        },
        {
          "features, encouraging the Translator to extract individual-specific neutral features from": "challenge of cross-individual generalization."
        },
        {
          "features, encouraging the Translator to extract individual-specific neutral features from": "The neutral feature pipelines are active only during the training phase, where they"
        },
        {
          "features, encouraging the Translator to extract individual-specific neutral features from": "guide the translator modules (both FLT and SLT) to learn individual-specific baseline"
        },
        {
          "features, encouraging the Translator to extract individual-specific neutral features from": "features for standardization. During inference, only the emotional feature pipeline op-"
        },
        {
          "features, encouraging the Translator to extract individual-specific neutral features from": "erates, performing individualized standardization using the trained translators."
        },
        {
          "features, encouraging the Translator to extract individual-specific neutral features from": "3.2.3. The Detailed Multi-Scale Process"
        },
        {
          "features, encouraging the Translator to extract individual-specific neutral features from": "The frame-level processing begins with the Frame-Level Neutral Encoder (FLNE,"
        },
        {
          "features, encouraging the Translator to extract individual-specific neutral features from": "N from the same individual, encod-\n1 , . . . , In"
        },
        {
          "features, encouraging the Translator to extract individual-specific neutral features from": ", . . . , v f _n\ning them into representations v f _n = v f _n\n. FLNE integrates a multi-layer per-"
        },
        {
          "features, encouraging the Translator to extract individual-specific neutral features from": "N\n1"
        },
        {
          "features, encouraging the Translator to extract individual-specific neutral features from": "ceptron with frame-wise attention to capture backchannel-related features within indi-"
        },
        {
          "features, encouraging the Translator to extract individual-specific neutral features from": "vidual frames. Operating exclusively during training, this module guides the translator"
        },
        {
          "features, encouraging the Translator to extract individual-specific neutral features from": "in learning individual-specific neutral features. In parallel, the Frame-Level Emotional"
        },
        {
          "features, encouraging the Translator to extract individual-specific neutral features from": "Encoder\n(FLEE, E f _e) processes backchannel samples Ie\ninto emotional\nrepresenta-"
        },
        {
          "features, encouraging the Translator to extract individual-specific neutral features from": "tions v f _e, maintaining architectural consistency with FLNE. The Frame-Level Trans-"
        },
        {
          "features, encouraging the Translator to extract individual-specific neutral features from": "lator (FLT, T f ) then extracts neutral representations ˆv f _n from these emotional features"
        },
        {
          "features, encouraging the Translator to extract individual-specific neutral features from": "through stacked self-attention blocks that operate independently on each frame, refin-"
        },
        {
          "features, encouraging the Translator to extract individual-specific neutral features from": "ing intra-frame features via gated attention and residual connections. A subtraction"
        },
        {
          "features, encouraging the Translator to extract individual-specific neutral features from": "operation between v f _e and ˆv f _n extracts relative emotional patterns, yielding general-"
        },
        {
          "features, encouraging the Translator to extract individual-specific neutral features from": "izable frame-level backchannel representations (ˆv f _e) for subsequent modeling."
        },
        {
          "features, encouraging the Translator to extract individual-specific neutral features from": "Building upon frame-level processing, the sequence-level components capture tem-"
        },
        {
          "features, encouraging the Translator to extract individual-specific neutral features from": "poral dependencies across frames. The Sequence-Level Neutral Encoder (SLNE, E s_n)"
        },
        {
          "features, encouraging the Translator to extract individual-specific neutral features from": ", . . . , vs_n\nencodes frame-level representations into sequence-level representations vs_n = vs_n"
        },
        {
          "features, encouraging the Translator to extract individual-specific neutral features from": "N\n1"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 3: The Temporal Attention Pooling (TAP) Module.": "temporal\nrelationships.\nSimilarly,"
        },
        {
          "Figure 3: The Temporal Attention Pooling (TAP) Module.": ""
        },
        {
          "Figure 3: The Temporal Attention Pooling (TAP) Module.": ", . . . , vs_e\n) while maintaining structural"
        },
        {
          "Figure 3: The Temporal Attention Pooling (TAP) Module.": "N\n1"
        },
        {
          "Figure 3: The Temporal Attention Pooling (TAP) Module.": ""
        },
        {
          "Figure 3: The Temporal Attention Pooling (TAP) Module.": "individual-specific neutral feature sequences,\nvs_n,"
        },
        {
          "Figure 3: The Temporal Attention Pooling (TAP) Module.": ""
        },
        {
          "Figure 3: The Temporal Attention Pooling (TAP) Module.": ""
        },
        {
          "Figure 3: The Temporal Attention Pooling (TAP) Module.": "The standardization (i.e., a subtraction operation) is then applied to vs_e and ˆvs_n to"
        },
        {
          "Figure 3: The Temporal Attention Pooling (TAP) Module.": ""
        },
        {
          "Figure 3: The Temporal Attention Pooling (TAP) Module.": ""
        },
        {
          "Figure 3: The Temporal Attention Pooling (TAP) Module.": ""
        },
        {
          "Figure 3: The Temporal Attention Pooling (TAP) Module.": "Unlike overt verbal expressions, backchannel are typically brief, occur in specific"
        },
        {
          "Figure 3: The Temporal Attention Pooling (TAP) Module.": ""
        },
        {
          "Figure 3: The Temporal Attention Pooling (TAP) Module.": ""
        },
        {
          "Figure 3: The Temporal Attention Pooling (TAP) Module.": ""
        },
        {
          "Figure 3: The Temporal Attention Pooling (TAP) Module.": ""
        },
        {
          "Figure 3: The Temporal Attention Pooling (TAP) Module.": "to focus on these key moments by calculating weights for each frame. Fig. 3"
        },
        {
          "Figure 3: The Temporal Attention Pooling (TAP) Module.": ""
        },
        {
          "Figure 3: The Temporal Attention Pooling (TAP) Module.": "Given a sequence of feature representations after the sequence-level standardization"
        },
        {
          "Figure 3: The Temporal Attention Pooling (TAP) Module.": ", . . . , ˆvs_e\n, . . . , ˆvs_e\n),\nthis module applies linear projection to each frame to\nt"
        },
        {
          "Figure 3: The Temporal Attention Pooling (TAP) Module.": "N\n1"
        },
        {
          "Figure 3: The Temporal Attention Pooling (TAP) Module.": ""
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "improve expressiveness. A sigmoid is used to generate bounded importance weights.": ""
        },
        {
          "improve expressiveness. A sigmoid is used to generate bounded importance weights.": ""
        },
        {
          "improve expressiveness. A sigmoid is used to generate bounded importance weights.": ""
        },
        {
          "improve expressiveness. A sigmoid is used to generate bounded importance weights.": "The final sequence-level representation is obtained by averaging the weighted fea-"
        },
        {
          "improve expressiveness. A sigmoid is used to generate bounded importance weights.": ""
        },
        {
          "improve expressiveness. A sigmoid is used to generate bounded importance weights.": ""
        },
        {
          "improve expressiveness. A sigmoid is used to generate bounded importance weights.": ""
        },
        {
          "improve expressiveness. A sigmoid is used to generate bounded importance weights.": ""
        },
        {
          "improve expressiveness. A sigmoid is used to generate bounded importance weights.": ""
        },
        {
          "improve expressiveness. A sigmoid is used to generate bounded importance weights.": ""
        },
        {
          "improve expressiveness. A sigmoid is used to generate bounded importance weights.": "where T is the number of frames. This design enables the network to down-weight"
        },
        {
          "improve expressiveness. A sigmoid is used to generate bounded importance weights.": ""
        },
        {
          "improve expressiveness. A sigmoid is used to generate bounded importance weights.": ""
        },
        {
          "improve expressiveness. A sigmoid is used to generate bounded importance weights.": "Finally, a regression model (R) takes the feature vector z after the multi-scale stan-"
        },
        {
          "improve expressiveness. A sigmoid is used to generate bounded importance weights.": "It"
        },
        {
          "improve expressiveness. A sigmoid is used to generate bounded importance weights.": ""
        },
        {
          "improve expressiveness. A sigmoid is used to generate bounded importance weights.": "it"
        },
        {
          "improve expressiveness. A sigmoid is used to generate bounded importance weights.": ""
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Frame-Level Translator (FLT)": ""
        },
        {
          "Frame-Level Translator (FLT)": "FLTB"
        },
        {
          "Frame-Level Translator (FLT)": ""
        },
        {
          "Frame-Level Translator (FLT)": ""
        },
        {
          "Frame-Level Translator (FLT)": "LayerNorm"
        },
        {
          "Frame-Level Translator (FLT)": "FLTB"
        },
        {
          "Frame-Level Translator (FLT)": ""
        },
        {
          "Frame-Level Translator (FLT)": ""
        },
        {
          "Frame-Level Translator (FLT)": "LayerNorm"
        },
        {
          "Frame-Level Translator (FLT)": ""
        },
        {
          "Frame-Level Translator (FLT)": "FLTB"
        },
        {
          "Frame-Level Translator (FLT)": ""
        },
        {
          "Frame-Level Translator (FLT)": ""
        },
        {
          "Frame-Level Translator (FLT)": "LayerNorm"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 4: The FLTB (left) and FLT (right) Module.": ""
        },
        {
          "Figure 4: The FLTB (left) and FLT (right) Module.": "ftemp = α · V"
        },
        {
          "Figure 4: The FLTB (left) and FLT (right) Module.": "is then linearly projected back to the original"
        },
        {
          "Figure 4: The FLTB (left) and FLT (right) Module.": ""
        },
        {
          "Figure 4: The FLTB (left) and FLT (right) Module.": ""
        },
        {
          "Figure 4: The FLTB (left) and FLT (right) Module.": "output = LayerNorm( fpro j + input)"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "work,\nthe sequence-level\nfeatures\nrepresent\nthe frequency and temporal patterns of": "these actions over time. Sequence-level patterns—such as nodding frequency, repeti-"
        },
        {
          "work,\nthe sequence-level\nfeatures\nrepresent\nthe frequency and temporal patterns of": "tion, or rhythm—can provide additional cues that are crucial for estimating backchan-"
        },
        {
          "work,\nthe sequence-level\nfeatures\nrepresent\nthe frequency and temporal patterns of": "nel agreement. To capture temporal features, we introduce an Encoder-Decoder LSTM"
        },
        {
          "work,\nthe sequence-level\nfeatures\nrepresent\nthe frequency and temporal patterns of": "(ED-LSTM) module as SLT, which aims to extract\nindividual-invariant features from"
        },
        {
          "work,\nthe sequence-level\nfeatures\nrepresent\nthe frequency and temporal patterns of": "the input sequence. As shown in Fig. 5, the ED-LSTM consists of an encoder LSTM,"
        },
        {
          "work,\nthe sequence-level\nfeatures\nrepresent\nthe frequency and temporal patterns of": "an attention layer, and a decoder LSTM."
        },
        {
          "work,\nthe sequence-level\nfeatures\nrepresent\nthe frequency and temporal patterns of": "The encoder module takes a sequence of frame-level features as input and passes"
        },
        {
          "work,\nthe sequence-level\nfeatures\nrepresent\nthe frequency and temporal patterns of": "them through a unidirectional LSTM to generate a hidden representation.\nIt encodes"
        },
        {
          "work,\nthe sequence-level\nfeatures\nrepresent\nthe frequency and temporal patterns of": "the underlying sequential structure of\nthe input behavior. To enhance the contextual"
        },
        {
          "work,\nthe sequence-level\nfeatures\nrepresent\nthe frequency and temporal patterns of": "attention of sequence-level\nrepresentation, we add an attention module between the"
        },
        {
          "work,\nthe sequence-level\nfeatures\nrepresent\nthe frequency and temporal patterns of": "encoder and decoder. The decoder\nreceives the attention-weighted context vector as"
        },
        {
          "work,\nthe sequence-level\nfeatures\nrepresent\nthe frequency and temporal patterns of": "input. This context vector is then passed through an LSTM decoder to reconstruct the"
        },
        {
          "work,\nthe sequence-level\nfeatures\nrepresent\nthe frequency and temporal patterns of": "sequence-level neutral features from emotional features."
        },
        {
          "work,\nthe sequence-level\nfeatures\nrepresent\nthe frequency and temporal patterns of": "3.4.\nImplicit Data Augmentation (IDA)"
        },
        {
          "work,\nthe sequence-level\nfeatures\nrepresent\nthe frequency and temporal patterns of": "During our experiments, we observed that\nthe distribution of backchannel agree-"
        },
        {
          "work,\nthe sequence-level\nfeatures\nrepresent\nthe frequency and temporal patterns of": "ment data was imbalanced. Most samples had agreement\nlevels concentrated near the"
        },
        {
          "work,\nthe sequence-level\nfeatures\nrepresent\nthe frequency and temporal patterns of": "agreement score peak (around 0.25 in the dataset), and the extreme high- and low-level"
        },
        {
          "work,\nthe sequence-level\nfeatures\nrepresent\nthe frequency and temporal patterns of": "samples were rare. This imbalance led the model\nto learn local concentrated feature"
        },
        {
          "work,\nthe sequence-level\nfeatures\nrepresent\nthe frequency and temporal patterns of": "patterns during training, which undermines its ability to generalize to data with differ-"
        },
        {
          "work,\nthe sequence-level\nfeatures\nrepresent\nthe frequency and temporal patterns of": "ent distributions. The detailed analysis is presented in Section 5.1.2."
        },
        {
          "work,\nthe sequence-level\nfeatures\nrepresent\nthe frequency and temporal patterns of": "To tackle this issue, we draw inspiration from Implicit Data Augmentation methods"
        },
        {
          "work,\nthe sequence-level\nfeatures\nrepresent\nthe frequency and temporal patterns of": "[45, 46, 47], which generate new feature representations in the hidden space instead of"
        },
        {
          "work,\nthe sequence-level\nfeatures\nrepresent\nthe frequency and temporal patterns of": "directly synthesizing input data. Accordingly, we propose an Implicit Data Augmenta-"
        },
        {
          "work,\nthe sequence-level\nfeatures\nrepresent\nthe frequency and temporal patterns of": "tion (IDA) module to address this problem.\nIt aims to reduce the model’s reliance on"
        },
        {
          "work,\nthe sequence-level\nfeatures\nrepresent\nthe frequency and temporal patterns of": "the highly concentrated regions of the data distribution and improve its generalization"
        },
        {
          "work,\nthe sequence-level\nfeatures\nrepresent\nthe frequency and temporal patterns of": "performance. After the emotional encoder, we introduce this module, as illustrated in"
        },
        {
          "work,\nthe sequence-level\nfeatures\nrepresent\nthe frequency and temporal patterns of": "Fig. 1. During the training phase, this module adds element-wise Gaussian noise with"
        },
        {
          "work,\nthe sequence-level\nfeatures\nrepresent\nthe frequency and temporal patterns of": "zero mean and a standard deviation of 0.05 to the output tensor, followed by a random"
        },
        {
          "work,\nthe sequence-level\nfeatures\nrepresent\nthe frequency and temporal patterns of": "masking operation applied with a probability of 0.1. These operations help model\nto"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "mitigate overfitting to the training data.": "By adding this module,\nthe model\nis exposed to more diversities during training,"
        },
        {
          "mitigate overfitting to the training data.": "which improves its robustness to edge-case samples, enhancing overall generalizability."
        },
        {
          "mitigate overfitting to the training data.": "4. Experiments Details"
        },
        {
          "mitigate overfitting to the training data.": "4.1. Dataset"
        },
        {
          "mitigate overfitting to the training data.": "We use the MPIIGroupInteraction dataset\n[25, 4]\nfor backchannel agreement es-"
        },
        {
          "mitigate overfitting to the training data.": "timation. MPIIGroupInteraction dataset\nis a publicly available dataset\nthat provides"
        },
        {
          "mitigate overfitting to the training data.": "annotations of backchannel occurrences and agreements expressed through backchan-"
        },
        {
          "mitigate overfitting to the training data.": "nels in group interactions. The dataset consists of 78 German-speaking participants."
        },
        {
          "mitigate overfitting to the training data.": "Each interaction involves three or\nfour participants engaging in a conversation in a"
        },
        {
          "mitigate overfitting to the training data.": "quiet office environment. The dataset\nincludes both audio recordings of the conversa-"
        },
        {
          "mitigate overfitting to the training data.": "tions and individual video recordings of each participant.\nIn the agreement estimation"
        },
        {
          "mitigate overfitting to the training data.": "task,\nthere are 3358 and 1427 annotated video samples in the training and validation"
        },
        {
          "mitigate overfitting to the training data.": "sets respectively, each lasts about 10 seconds. Multiple annotators labeled each video,"
        },
        {
          "mitigate overfitting to the training data.": "with agreement\nlevels averaged across annotators and normalized to [−1, 1], where 1"
        },
        {
          "mitigate overfitting to the training data.": "and −1 indicate strong agreement and disagreement respectively."
        },
        {
          "mitigate overfitting to the training data.": "4.2. Experimental Settings"
        },
        {
          "mitigate overfitting to the training data.": "Empirical studies from both our work and prior research demonstrate that\nthe rel-"
        },
        {
          "mitigate overfitting to the training data.": "evant backchannel cues typically occur only in the final second of the video samples."
        },
        {
          "mitigate overfitting to the training data.": "Particularly, previous work [11] use the last three seconds as input yields the most ef-"
        },
        {
          "mitigate overfitting to the training data.": "fective results. Shorter intervals may lack sufficient contextual\ninformation, whereas"
        },
        {
          "mitigate overfitting to the training data.": "longer intervals tend to introduce excessive non-task-related content. Based on this, we"
        },
        {
          "mitigate overfitting to the training data.": "use OpenFace 2.0 [44] to extract facial landmarks from each video frame as the initial"
        },
        {
          "mitigate overfitting to the training data.": "features. To effectively capture facial dynamics during the critical period, we process"
        },
        {
          "mitigate overfitting to the training data.": "only the last 3 seconds of each video and compute the coordinate differences between"
        },
        {
          "mitigate overfitting to the training data.": "each frame and its subsequent frame, using the resulting landmark variation rates as"
        },
        {
          "mitigate overfitting to the training data.": "input features for the model [12]."
        },
        {
          "mitigate overfitting to the training data.": "Our model\nis implemented in PyTorch and trained on an NVIDIA GeForce RTX"
        },
        {
          "mitigate overfitting to the training data.": "2080 Ti GPU. The model was trained with the Stochastic Gradient Descent\n(SGD)"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "optimizer, using its default parameter settings, where the momentum was set to 0.9 and": "the weight decay coefficient\nto 1e-4. The batch size was set\nto 32, and training was"
        },
        {
          "optimizer, using its default parameter settings, where the momentum was set to 0.9 and": "performed for a total of 100 epochs. A StepLR learning rate scheduler was employed"
        },
        {
          "optimizer, using its default parameter settings, where the momentum was set to 0.9 and": "to decay the learning rate at fixed intervals. The initial\nlearning rate was set\nto 0.01"
        },
        {
          "optimizer, using its default parameter settings, where the momentum was set to 0.9 and": "and multiplied by 0.1 every 20 epochs. The Transformer encoder consists of 6 stacked"
        },
        {
          "optimizer, using its default parameter settings, where the momentum was set to 0.9 and": "Transformer layers, each with 4 attention heads."
        },
        {
          "optimizer, using its default parameter settings, where the momentum was set to 0.9 and": "5. Results"
        },
        {
          "optimizer, using its default parameter settings, where the momentum was set to 0.9 and": "We conduct comprehensive evaluation experiments, which comprises three parts:"
        },
        {
          "optimizer, using its default parameter settings, where the momentum was set to 0.9 and": "component contribution analysis, visualization studies on individual standardization"
        },
        {
          "optimizer, using its default parameter settings, where the momentum was set to 0.9 and": "effectiveness, and comparative experiments with state-of-the-art methods. Following"
        },
        {
          "optimizer, using its default parameter settings, where the momentum was set to 0.9 and": "the evaluation protocol from [4], we train on their training set, evaluate on the validation"
        },
        {
          "optimizer, using its default parameter settings, where the momentum was set to 0.9 and": "set, and use Mean Squared Error (MSE) to measure the estimation performance."
        },
        {
          "optimizer, using its default parameter settings, where the momentum was set to 0.9 and": "5.1. Ablation Study"
        },
        {
          "optimizer, using its default parameter settings, where the momentum was set to 0.9 and": "5.1.1.\nImpact of the Appropriately Configured SLT"
        },
        {
          "optimizer, using its default parameter settings, where the momentum was set to 0.9 and": "Table 1: Ablation study of different SLT implementations.\n↓ indicates that a lower value corresponds to"
        },
        {
          "optimizer, using its default parameter settings, where the momentum was set to 0.9 and": "better model performance."
        }
      ],
      "page": 19
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Previous\nstudies\n[48, 49] have demonstrated that encoder–decoder architectures": "possess strong sequence modeling capabilities. Based on this, we adopt such an ar-"
        },
        {
          "Previous\nstudies\n[48, 49] have demonstrated that encoder–decoder architectures": "chitecture to construct our translator module. We experimented with various temporal"
        },
        {
          "Previous\nstudies\n[48, 49] have demonstrated that encoder–decoder architectures": "models as encoders and decoders to identify the optimal\ntemporal structure for\nthe"
        },
        {
          "Previous\nstudies\n[48, 49] have demonstrated that encoder–decoder architectures": "translator. This includes attention-based [15], GRU-based, and LSTM-based modules"
        },
        {
          "Previous\nstudies\n[48, 49] have demonstrated that encoder–decoder architectures": "(ED-LSTM in Section 3.3.2, our proposed approach).\nThe ablation results are pre-"
        },
        {
          "Previous\nstudies\n[48, 49] have demonstrated that encoder–decoder architectures": "sented in Tabel 1."
        },
        {
          "Previous\nstudies\n[48, 49] have demonstrated that encoder–decoder architectures": "Our LSTM-based model achieves better performance with an MSE of 0.057962,"
        },
        {
          "Previous\nstudies\n[48, 49] have demonstrated that encoder–decoder architectures": "outperforming all counterparts including the translator model from previous work [15]."
        },
        {
          "Previous\nstudies\n[48, 49] have demonstrated that encoder–decoder architectures": "This improvement may be attributed to the ability of ED-LSTM to effectively capture"
        },
        {
          "Previous\nstudies\n[48, 49] have demonstrated that encoder–decoder architectures": "contextual dependencies and generate more representative neutral features. Moreover,"
        },
        {
          "Previous\nstudies\n[48, 49] have demonstrated that encoder–decoder architectures": "in sequence-related tasks, studies have shown that LSTM generally outperforms GRU"
        },
        {
          "Previous\nstudies\n[48, 49] have demonstrated that encoder–decoder architectures": "in capturing temporal dependencies with higher accuracy [50], which aligns with the"
        },
        {
          "Previous\nstudies\n[48, 49] have demonstrated that encoder–decoder architectures": "findings in our task."
        },
        {
          "Previous\nstudies\n[48, 49] have demonstrated that encoder–decoder architectures": "5.1.2.\nImpact of IDA module"
        },
        {
          "Previous\nstudies\n[48, 49] have demonstrated that encoder–decoder architectures": "As shown in Fig. 6 (green curves),\nthe training samples are densely distributed"
        },
        {
          "Previous\nstudies\n[48, 49] have demonstrated that encoder–decoder architectures": "in the range of 0 to 0.5, and few samples show disagreement or strong agreement."
        },
        {
          "Previous\nstudies\n[48, 49] have demonstrated that encoder–decoder architectures": "In particular,\nthe sample density peaks around 0.25. This uneven distribution causes"
        },
        {
          "Previous\nstudies\n[48, 49] have demonstrated that encoder–decoder architectures": "the model\nto overly focus on samples near 0.25,\nresulting in overfitting and reduced"
        },
        {
          "Previous\nstudies\n[48, 49] have demonstrated that encoder–decoder architectures": "generalization ability. Consequently,\nthe prediction results tend to concentrate around"
        },
        {
          "Previous\nstudies\n[48, 49] have demonstrated that encoder–decoder architectures": "0.25 during inference. As shown in Fig. 6 (b1), this issue becomes even more obvious"
        },
        {
          "Previous\nstudies\n[48, 49] have demonstrated that encoder–decoder architectures": "in the distribution of prediction results on the validation set. The model’s prediction"
        },
        {
          "Previous\nstudies\n[48, 49] have demonstrated that encoder–decoder architectures": "range is mainly limited to -0.25 to 0.6, and the density peak around 0.25 is significantly"
        },
        {
          "Previous\nstudies\n[48, 49] have demonstrated that encoder–decoder architectures": "higher than that of the ground truth distribution of the validation set."
        },
        {
          "Previous\nstudies\n[48, 49] have demonstrated that encoder–decoder architectures": "To address this problem, we introduced the IDA module. By adding noise pertur-"
        },
        {
          "Previous\nstudies\n[48, 49] have demonstrated that encoder–decoder architectures": "bations to the embedding in the latent space, this module helps prevent the model from"
        },
        {
          "Previous\nstudies\n[48, 49] have demonstrated that encoder–decoder architectures": "overfitting. As shown in group (a) and group (b) of Fig. 6, after applying IDA,\nthe"
        },
        {
          "Previous\nstudies\n[48, 49] have demonstrated that encoder–decoder architectures": "prediction distributions become wider and the peak values decrease. This visualization"
        },
        {
          "Previous\nstudies\n[48, 49] have demonstrated that encoder–decoder architectures": "demonstrates the module’s ability to improve the model generalization, which is further"
        },
        {
          "Previous\nstudies\n[48, 49] have demonstrated that encoder–decoder architectures": "confirmed by the results in Tabel 2, where MSE decreased from 0.059501 to 0.057962"
        }
      ],
      "page": 20
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(a1)": "Figure 6: Overall comparison of density distributions before and after applying IDA. Green, blue, and red",
          "(a2)": "",
          "(b1)": "",
          "(b2)": ""
        },
        {
          "(a1)": "curves represent the training set, validation set, and predicted results, respectively. In group (a), the red curve",
          "(a2)": "",
          "(b1)": "",
          "(b2)": ""
        },
        {
          "(a1)": "shows predictions on the training set; in group (b), on the validation set. Each group includes results before",
          "(a2)": "",
          "(b1)": "",
          "(b2)": ""
        },
        {
          "(a1)": "augmentation (a1 and b1) and after augmentation (a2 and b2).",
          "(a2)": "",
          "(b1)": "",
          "(b2)": ""
        },
        {
          "(a1)": "after incorporating the IDA module.",
          "(a2)": "",
          "(b1)": "",
          "(b2)": ""
        },
        {
          "(a1)": "",
          "(a2)": "To investigate the effect of IDA, we design four different noise injection strategies.",
          "(b1)": "",
          "(b2)": ""
        }
      ],
      "page": 21
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "augmentation (a1 and b1) and after augmentation (a2 and b2).": "after incorporating the IDA module."
        },
        {
          "augmentation (a1 and b1) and after augmentation (a2 and b2).": "To investigate the effect of IDA, we design four different noise injection strategies."
        },
        {
          "augmentation (a1 and b1) and after augmentation (a2 and b2).": "• Non-Augmentation: Without both FLEE Augmentation (FLEE Aug.) and SLEE"
        },
        {
          "augmentation (a1 and b1) and after augmentation (a2 and b2).": "Augmentation(SLEE-Aug.)."
        },
        {
          "augmentation (a1 and b1) and after augmentation (a2 and b2).": "• FLEE Aug. only: With FLEE Aug. only."
        },
        {
          "augmentation (a1 and b1) and after augmentation (a2 and b2).": "• SLEE Aug. only: With SLEE Aug. only."
        },
        {
          "augmentation (a1 and b1) and after augmentation (a2 and b2).": "• Double Augmentation: With both FLEE Aug. and SLEE Aug.."
        },
        {
          "augmentation (a1 and b1) and after augmentation (a2 and b2).": "As shown in Tabel 2, our results show that applying IDA individually after either"
        }
      ],
      "page": 21
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 2: Ablation study on different augmentation injection strategies.": ""
        },
        {
          "Table 2: Ablation study on different augmentation injection strategies.": "SLEE Aug."
        },
        {
          "Table 2: Ablation study on different augmentation injection strategies.": ""
        },
        {
          "Table 2: Ablation study on different augmentation injection strategies.": "False"
        },
        {
          "Table 2: Ablation study on different augmentation injection strategies.": "False"
        },
        {
          "Table 2: Ablation study on different augmentation injection strategies.": "True"
        },
        {
          "Table 2: Ablation study on different augmentation injection strategies.": "True"
        }
      ],
      "page": 22
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "True\nTrue": "",
          "0.057962": "• Non-Backchannel-Based Neutral Selection: According to the definition pro-"
        },
        {
          "True\nTrue": "",
          "0.057962": "vided by ([4]), positive samples from the backchannel detection task are used as"
        },
        {
          "True\nTrue": "",
          "0.057962": "inputs for the agreement estimation task and neutral samples are those without"
        },
        {
          "True\nTrue": "backchannel behavior. Since backchannel agreement",
          "0.057962": "inherently involves active"
        },
        {
          "True\nTrue": "",
          "0.057962": "feedback behaviors, non-backchannel instances—which lack such signals—can"
        },
        {
          "True\nTrue": "be regarded as neutral samples.",
          "0.057962": ""
        }
      ],
      "page": 22
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "be regarded as neutral samples.": "In the Peak-Based Neutral Selection, we conducted multiple experiments using dif-"
        },
        {
          "be regarded as neutral samples.": "ferent ranges centered around 0.25. For example, if the edge is set to 0.1, the selected"
        },
        {
          "be regarded as neutral samples.": "range for neutral samples becomes 0.15 to 0.35. As shown in Tabel 3,\nthe best per-"
        },
        {
          "be regarded as neutral samples.": "formance was achieved when edge equals 0.1. The results of the Peak-Based Neutral"
        },
        {
          "be regarded as neutral samples.": "Selection slightly outperformed those of\nthe Non-Backchannel-Based Neutral Selec-"
        },
        {
          "be regarded as neutral samples.": "tion. Since both selection methods demonstrated promising results, we consider these"
        },
        {
          "be regarded as neutral samples.": "two neutral selection strategies to be reasonable and effective."
        },
        {
          "be regarded as neutral samples.": "5.1.4. The contribution of each component"
        },
        {
          "be regarded as neutral samples.": "We conduct ablation studies to investigate the contribution of each module in our"
        },
        {
          "be regarded as neutral samples.": "framework, including the impact of single- and multi-scale individual standardizations,"
        },
        {
          "be regarded as neutral samples.": "as shown in Table 4.\nIn the single-scale setup,\nthe best performance was achieved by"
        },
        {
          "be regarded as neutral samples.": "the sequence-level module,\nreaching an MSE of 0.059249.\nIt suggests that a frame-"
        },
        {
          "be regarded as neutral samples.": "level module alone may struggle to capture sufficient backchannel-related information."
        },
        {
          "be regarded as neutral samples.": "Given that backchannel behaviors are typically expressed in a continuous and tempo-"
        },
        {
          "be regarded as neutral samples.": "rally dependent manner, this observation aligns with intuitive expectations."
        }
      ],
      "page": 22
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 3: Ablation study of different neutral feature selection strategies. The actual selection range for neutral": "samples is defined as neutral ± edge. The second-best results are indicated with an underline."
        },
        {
          "Table 3: Ablation study of different neutral feature selection strategies. The actual selection range for neutral": ""
        },
        {
          "Table 3: Ablation study of different neutral feature selection strategies. The actual selection range for neutral": "Neutral"
        },
        {
          "Table 3: Ablation study of different neutral feature selection strategies. The actual selection range for neutral": ""
        },
        {
          "Table 3: Ablation study of different neutral feature selection strategies. The actual selection range for neutral": "0.25"
        },
        {
          "Table 3: Ablation study of different neutral feature selection strategies. The actual selection range for neutral": "0.25"
        },
        {
          "Table 3: Ablation study of different neutral feature selection strategies. The actual selection range for neutral": "0.25"
        },
        {
          "Table 3: Ablation study of different neutral feature selection strategies. The actual selection range for neutral": "non-backchannel"
        }
      ],
      "page": 23
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 4: Ablation study of different module combinations.": ""
        },
        {
          "Table 4: Ablation study of different module combinations.": "Pooling"
        },
        {
          "Table 4: Ablation study of different module combinations.": ""
        },
        {
          "Table 4: Ablation study of different module combinations.": "Method"
        },
        {
          "Table 4: Ablation study of different module combinations.": "Global Pooling"
        },
        {
          "Table 4: Ablation study of different module combinations.": ""
        },
        {
          "Table 4: Ablation study of different module combinations.": "TAP"
        },
        {
          "Table 4: Ablation study of different module combinations.": "Global Pooling"
        },
        {
          "Table 4: Ablation study of different module combinations.": ""
        },
        {
          "Table 4: Ablation study of different module combinations.": "TAP"
        },
        {
          "Table 4: Ablation study of different module combinations.": ""
        },
        {
          "Table 4: Ablation study of different module combinations.": "Global Pooling"
        },
        {
          "Table 4: Ablation study of different module combinations.": ""
        },
        {
          "Table 4: Ablation study of different module combinations.": "TAP"
        },
        {
          "Table 4: Ablation study of different module combinations.": "Global Pooling"
        },
        {
          "Table 4: Ablation study of different module combinations.": ""
        },
        {
          "Table 4: Ablation study of different module combinations.": "TAP"
        },
        {
          "Table 4: Ablation study of different module combinations.": "Global Pooling"
        },
        {
          "Table 4: Ablation study of different module combinations.": ""
        },
        {
          "Table 4: Ablation study of different module combinations.": "TAP"
        },
        {
          "Table 4: Ablation study of different module combinations.": "Global Pooling"
        },
        {
          "Table 4: Ablation study of different module combinations.": ""
        },
        {
          "Table 4: Ablation study of different module combinations.": "TAP"
        },
        {
          "Table 4: Ablation study of different module combinations.": ""
        },
        {
          "Table 4: Ablation study of different module combinations.": "Global Pooling"
        },
        {
          "Table 4: Ablation study of different module combinations.": ""
        },
        {
          "Table 4: Ablation study of different module combinations.": "TAP"
        },
        {
          "Table 4: Ablation study of different module combinations.": "Global Pooling"
        },
        {
          "Table 4: Ablation study of different module combinations.": ""
        },
        {
          "Table 4: Ablation study of different module combinations.": "TAP"
        }
      ],
      "page": 24
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(Group 1)\n(Group 2)\n(Group 3)\n(Group 4)": "Figure 7: Feature distribution visualization across four groups before (top) and after (bottom) standardiza-"
        },
        {
          "(Group 1)\n(Group 2)\n(Group 3)\n(Group 4)": "tion. Circles and squares represent\ntwo individuals in each group.\nThe continuous agreement scores are"
        },
        {
          "(Group 1)\n(Group 2)\n(Group 3)\n(Group 4)": "divided into four ranges: [-1, 0) (gray), [0, 0.25) (blue), [0.25, 0.5) (orange), and [0.5, 1] (green). The high-"
        },
        {
          "(Group 1)\n(Group 2)\n(Group 3)\n(Group 4)": "lighted markers indicate the center points of each ranges."
        },
        {
          "(Group 1)\n(Group 2)\n(Group 3)\n(Group 4)": "the green and gray centroids from different\nindividuals exhibit greater alignment after"
        },
        {
          "(Group 1)\n(Group 2)\n(Group 3)\n(Group 4)": "standardization. Similar patterns are observed in Group 2 (green), Group 3 (blue and"
        },
        {
          "(Group 1)\n(Group 2)\n(Group 3)\n(Group 4)": "gray) and Group 4 (green).\nThe scatters show that before standardization,\nsamples"
        },
        {
          "(Group 1)\n(Group 2)\n(Group 3)\n(Group 4)": "from different\nindividuals and agreement categories are distributed chaotically. After"
        },
        {
          "(Group 1)\n(Group 2)\n(Group 3)\n(Group 4)": "standardization, same-category samples cluster together (blue and orange samples form"
        },
        {
          "(Group 1)\n(Group 2)\n(Group 3)\n(Group 4)": "distinct clusters in Group 1, so as do orange and gray in Group 2, and blue and orange"
        },
        {
          "(Group 1)\n(Group 2)\n(Group 3)\n(Group 4)": "in Group 3).\nThis demonstrates that standardization significantly reduces emotional"
        },
        {
          "(Group 1)\n(Group 2)\n(Group 3)\n(Group 4)": "confusion between individuals."
        },
        {
          "(Group 1)\n(Group 2)\n(Group 3)\n(Group 4)": "While standardization effectively reduces individual bias by eliminating person-"
        },
        {
          "(Group 1)\n(Group 2)\n(Group 3)\n(Group 4)": "specific neutral patterns, complete separation of emotional states remains challenging,"
        },
        {
          "(Group 1)\n(Group 2)\n(Group 3)\n(Group 4)": "particularly in 2D spaces as shown by partial overlap between orange circles and green"
        },
        {
          "(Group 1)\n(Group 2)\n(Group 3)\n(Group 4)": "squares in Group3. This aligns with prior findings [15],\nthough such ambiguity in 2D"
        },
        {
          "(Group 1)\n(Group 2)\n(Group 3)\n(Group 4)": "visualization may be distinguishable in the original high-dimensional space."
        },
        {
          "(Group 1)\n(Group 2)\n(Group 3)\n(Group 4)": "We calculate the minimum inter-individual distance between each agreement cate-"
        },
        {
          "(Group 1)\n(Group 2)\n(Group 3)\n(Group 4)": "gory center of one individual and all different agreement category centers, except\nthe"
        },
        {
          "(Group 1)\n(Group 2)\n(Group 3)\n(Group 4)": "target category, of another\nindividual.\nThis measures how well different agreement"
        }
      ],
      "page": 25
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 5: Minimum inter-individual distances between different agreement category centers.": "denote the two individuals. ’Total’ sums all eight centers. ’Diff’ shows the change after standardization.",
          "’Cir’ and ’Squ’": ""
        },
        {
          "Table 5: Minimum inter-individual distances between different agreement category centers.": "cir-blue",
          "’Cir’ and ’Squ’": "squ-green"
        },
        {
          "Table 5: Minimum inter-individual distances between different agreement category centers.": "2.1658",
          "’Cir’ and ’Squ’": "4.2834"
        },
        {
          "Table 5: Minimum inter-individual distances between different agreement category centers.": "",
          "’Cir’ and ’Squ’": ""
        },
        {
          "Table 5: Minimum inter-individual distances between different agreement category centers.": "2.8574",
          "’Cir’ and ’Squ’": "3.8945"
        },
        {
          "Table 5: Minimum inter-individual distances between different agreement category centers.": "0.8562",
          "’Cir’ and ’Squ’": "1.4975"
        },
        {
          "Table 5: Minimum inter-individual distances between different agreement category centers.": "",
          "’Cir’ and ’Squ’": ""
        },
        {
          "Table 5: Minimum inter-individual distances between different agreement category centers.": "7.7027",
          "’Cir’ and ’Squ’": "3.7777"
        },
        {
          "Table 5: Minimum inter-individual distances between different agreement category centers.": "3.9391",
          "’Cir’ and ’Squ’": "0.8974"
        },
        {
          "Table 5: Minimum inter-individual distances between different agreement category centers.": "",
          "’Cir’ and ’Squ’": ""
        },
        {
          "Table 5: Minimum inter-individual distances between different agreement category centers.": "4.1884",
          "’Cir’ and ’Squ’": "1.6558"
        },
        {
          "Table 5: Minimum inter-individual distances between different agreement category centers.": "1.9781",
          "’Cir’ and ’Squ’": "2.9527"
        },
        {
          "Table 5: Minimum inter-individual distances between different agreement category centers.": "",
          "’Cir’ and ’Squ’": ""
        },
        {
          "Table 5: Minimum inter-individual distances between different agreement category centers.": "2.6557",
          "’Cir’ and ’Squ’": "1.3655"
        }
      ],
      "page": 26
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 6: The performance of different methods.": "Agreement"
        },
        {
          "Table 6: The performance of different methods.": ""
        },
        {
          "Table 6: The performance of different methods.": "(MSE ↓)"
        },
        {
          "Table 6: The performance of different methods.": "0.066"
        },
        {
          "Table 6: The performance of different methods.": "0.0644"
        },
        {
          "Table 6: The performance of different methods.": "0.075"
        },
        {
          "Table 6: The performance of different methods.": "0.079"
        },
        {
          "Table 6: The performance of different methods.": "0.085"
        },
        {
          "Table 6: The performance of different methods.": "0.057962"
        }
      ],
      "page": 27
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Ours\n0.057962": "6. Discussion and Future Work"
        },
        {
          "Ours\n0.057962": "Backchannel communication includes visual and auditory modalities [4].\nPartic-"
        },
        {
          "Ours\n0.057962": "ularly, people typically express polite agreement during interactions, and often with"
        },
        {
          "Ours\n0.057962": "auditory feedback (e.g., \"yes,\" \"right,\" \"no\") conveying stronger agreement responses."
        },
        {
          "Ours\n0.057962": "This explains the label distribution peak around 0.25, with fewer samples showing dis-"
        },
        {
          "Ours\n0.057962": "agreement or strong agreement.\nIn our study, we piloted with Light-ASD [51] model"
        },
        {
          "Ours\n0.057962": "to separately analyze the impact of visual and auditory backchannels to agreement es-"
        },
        {
          "Ours\n0.057962": "timation.\nIt generates frame-level scores (negative for silence, positive for speech) at"
        },
        {
          "Ours\n0.057962": "25 fps. Following [4]’s criterion that auditory backchannel events last ≥ 0.25 seconds,"
        },
        {
          "Ours\n0.057962": "we classified samples as auditory if seven consecutive frames in the last three seconds"
        },
        {
          "Ours\n0.057962": "had scores > 0.2; otherwise, they were classified as visual backchannel samples."
        },
        {
          "Ours\n0.057962": "As shown in Tabel 7, we obtained 2792 visual backchannel samples and 566 au-"
        },
        {
          "Ours\n0.057962": "ditive backchannel samples, totaling 3,358 instances in the training set. We then com-"
        },
        {
          "Ours\n0.057962": "puted the absolute values of the agreement scores for all samples and performed statis-"
        },
        {
          "Ours\n0.057962": "tical analysis. The results indicate that auditive samples show higher average emotion"
        },
        {
          "Ours\n0.057962": "intensity than visual samples. This trend is consistently observed across the 25th, 50th,"
        },
        {
          "Ours\n0.057962": "and 75th percentiles. These findings suggest that auditive backchannel are more likely"
        },
        {
          "Ours\n0.057962": "to convey stronger emotional\nintensity compared to visual backchannel.\nIn order\nto"
        },
        {
          "Ours\n0.057962": "gain deeper insights, we conducted the following experiments:"
        }
      ],
      "page": 27
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "•": ""
        },
        {
          "•": ""
        },
        {
          "•": ""
        },
        {
          "•": "•"
        },
        {
          "•": ""
        },
        {
          "•": ""
        },
        {
          "•": "•"
        },
        {
          "•": ""
        },
        {
          "•": ""
        }
      ],
      "page": 28
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 7: Statistics of auditive backchannel labels in the training set.": "Visual"
        },
        {
          "Table 7: Statistics of auditive backchannel labels in the training set.": "2792"
        },
        {
          "Table 7: Statistics of auditive backchannel labels in the training set.": "0.297910"
        },
        {
          "Table 7: Statistics of auditive backchannel labels in the training set.": "0.182404"
        },
        {
          "Table 7: Statistics of auditive backchannel labels in the training set.": "0.000000"
        },
        {
          "Table 7: Statistics of auditive backchannel labels in the training set.": "0.167000"
        },
        {
          "Table 7: Statistics of auditive backchannel labels in the training set.": "0.250000"
        },
        {
          "Table 7: Statistics of auditive backchannel labels in the training set.": "0.417000"
        },
        {
          "Table 7: Statistics of auditive backchannel labels in the training set.": "0.917000"
        }
      ],
      "page": 28
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "In the backchannel agreement estimation experiments described above,\nthe all-to-": "visual setting yields the lowest MSE. As shown in Tabel 8,\nthe performance of\nthe"
        },
        {
          "In the backchannel agreement estimation experiments described above,\nthe all-to-": "all-to-visual setting surpasses that of the all-to-all setting.\nIt\nindicates that\nthe model"
        },
        {
          "In the backchannel agreement estimation experiments described above,\nthe all-to-": "has already achieved relatively good predictive accuracy on visual backchannel sam-"
        },
        {
          "In the backchannel agreement estimation experiments described above,\nthe all-to-": "ples, whereas its performance on auditory samples remains suboptimal. This may be"
        },
        {
          "In the backchannel agreement estimation experiments described above,\nthe all-to-": "due to the limited auditory backchannel samples, which constrains the model’s abil-"
        },
        {
          "In the backchannel agreement estimation experiments described above,\nthe all-to-": "ity to generalize to such cases. As previously discussed, auditive backchannels are"
        },
        {
          "In the backchannel agreement estimation experiments described above,\nthe all-to-": "often associated with stronger emotional expressions, making accurate prediction of"
        },
        {
          "In the backchannel agreement estimation experiments described above,\nthe all-to-": "these high-intensity samples especially meaningful. Moreover,\nthe observation that"
        },
        {
          "In the backchannel agreement estimation experiments described above,\nthe all-to-": "the visual-to-all setting performs slightly worse than the all-to-all setting suggests that"
        },
        {
          "In the backchannel agreement estimation experiments described above,\nthe all-to-": "there may be a distributional gap between visual and auditory backchannel samples."
        },
        {
          "In the backchannel agreement estimation experiments described above,\nthe all-to-": "Therefore, it is necessary to explicitly consider both visual and auditory backchannels"
        },
        {
          "In the backchannel agreement estimation experiments described above,\nthe all-to-": "in protocol modeling."
        },
        {
          "In the backchannel agreement estimation experiments described above,\nthe all-to-": "7. Conclusions"
        },
        {
          "In the backchannel agreement estimation experiments described above,\nthe all-to-": "In this paper, we propose a novel cascaded multi-scale individual\nstandardiza-"
        },
        {
          "In the backchannel agreement estimation experiments described above,\nthe all-to-": "tion framework for backchannel agreement estimation that\nrelieves individual differ-"
        },
        {
          "In the backchannel agreement estimation experiments described above,\nthe all-to-": "ences and data imbalance. By disentangling individual-normalized backchannel agree-"
        },
        {
          "In the backchannel agreement estimation experiments described above,\nthe all-to-": "ment\nfeatures\nfeatures\nfrom observed cues.\nOur model\nreduces\nthe interference of"
        },
        {
          "In the backchannel agreement estimation experiments described above,\nthe all-to-": "personal neutral\nresponse patterns and achieves more consistent agreement predic-"
        },
        {
          "In the backchannel agreement estimation experiments described above,\nthe all-to-": "tion across diverse participants. Additionally, we introduce an implicit data augmen-"
        },
        {
          "In the backchannel agreement estimation experiments described above,\nthe all-to-": "tation strategy in the latent\nspace to mitigate sample distribution bias and improve"
        },
        {
          "In the backchannel agreement estimation experiments described above,\nthe all-to-": "the model’s generalization, particularly in low-frequency, emotionally intense cases"
        },
        {
          "In the backchannel agreement estimation experiments described above,\nthe all-to-": "(strong dis/agreement)."
        },
        {
          "In the backchannel agreement estimation experiments described above,\nthe all-to-": "Our experiments confirm that individual standardization significantly improves backchan-"
        },
        {
          "In the backchannel agreement estimation experiments described above,\nthe all-to-": "nel agreement estimation. Ablation studies and visualizations further validate the con-"
        },
        {
          "In the backchannel agreement estimation experiments described above,\nthe all-to-": "tribution of each proposed module. Furthermore, with the comprehensive analysis we"
        },
        {
          "In the backchannel agreement estimation experiments described above,\nthe all-to-": "identify auditive backchannel samples as inherently more challenging due to their as-"
        },
        {
          "In the backchannel agreement estimation experiments described above,\nthe all-to-": "sociation with stronger affective signals.\nThis finding underscores\nthe necessity of"
        },
        {
          "In the backchannel agreement estimation experiments described above,\nthe all-to-": "considering visual and auditive backchannels separately in the future.\nBeyond this"
        }
      ],
      "page": 29
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "specific task, our work has broader implications for pattern recognition by multi-scale": "individual standardization, potentially benefiting various sequential recognition tasks."
        },
        {
          "specific task, our work has broader implications for pattern recognition by multi-scale": "8. Acknowledgments"
        },
        {
          "specific task, our work has broader implications for pattern recognition by multi-scale": "This work was supported,\nin part, by the Education University of Hong Kong un-"
        },
        {
          "specific task, our work has broader implications for pattern recognition by multi-scale": "der Grant RG 73/2024-2025R,\nthe Hong Kong Polytechnic University under Grant"
        },
        {
          "specific task, our work has broader implications for pattern recognition by multi-scale": "P0048656, and the Hong Kong Research Grant Council under Grant 15600219."
        },
        {
          "specific task, our work has broader implications for pattern recognition by multi-scale": "References"
        }
      ],
      "page": 30
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "[1] O. Koller, N. C. Camgoz, H. Ney, R. Bowden, Weakly supervised learning with"
        },
        {
          "References": "multi-stream cnn-lstm-hmms to discover sequential parallelism in sign language"
        },
        {
          "References": "videos,\nIEEE transactions on pattern analysis and machine intelligence 42 (9)"
        },
        {
          "References": "(2019) 2306–2320."
        },
        {
          "References": "[2] A. Kendon, Some functions of gaze-direction in social interaction, Acta psycho-"
        },
        {
          "References": "logica 26 (1967) 22–63."
        },
        {
          "References": "[3] E. Bevacqua, S. Pammi, S. J. Hyniewska, M. Schröder, C. Pelachaud, Multimodal"
        },
        {
          "References": "backchannels for embodied conversational agents, in:\nIntelligent Virtual Agents:"
        },
        {
          "References": "10th International Conference, IVA 2010, Philadelphia, PA, USA, September 20-"
        },
        {
          "References": "22, 2010. Proceedings 10, Springer, 2010, pp. 194–200."
        },
        {
          "References": "[4] P. Müller, M. Dietz, D. Schiller, D. Thomas, H. Lindsay, P. Gebhard, E. André,"
        },
        {
          "References": "A. Bulling, Multimediate’22: Backchannel detection and agreement estimation"
        },
        {
          "References": "in group interactions, in: Proceedings of the 30th ACM International Conference"
        },
        {
          "References": "on Multimedia, 2022, pp. 7109–7114."
        },
        {
          "References": "[5] H. Sacks, E. A. Schegloff, G. Jefferson, A simplest systematics for the organiza-"
        },
        {
          "References": "tion of turn-taking for conversation, language 50 (4) (1974) 696–735."
        },
        {
          "References": "[6] D. Demirdjian, T. Darrell, 3-d articulated pose tracking for untethered diectic"
        },
        {
          "References": "reference, in: Proceedings. Fourth IEEE International Conference on Multimodal"
        },
        {
          "References": "Interfaces, IEEE, 2002, pp. 267–272."
        }
      ],
      "page": 30
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[7] S. B. Goldberg, The secrets of successful mediators, Negotiation Journal 21 (3)": "(2005) 365–376."
        },
        {
          "[7] S. B. Goldberg, The secrets of successful mediators, Negotiation Journal 21 (3)": "[8] D. Fuchs, Examiner familiarity effects on test performance:\nimplications for train-"
        },
        {
          "[7] S. B. Goldberg, The secrets of successful mediators, Negotiation Journal 21 (3)": "ing and practice, Topics in Early Childhood Special Education 7 (3) (1987) 90–"
        },
        {
          "[7] S. B. Goldberg, The secrets of successful mediators, Negotiation Journal 21 (3)": "104."
        },
        {
          "[7] S. B. Goldberg, The secrets of successful mediators, Negotiation Journal 21 (3)": "[9] M. Burns, Rapport and relationships:\nthe basis of child care., Journal of Child"
        },
        {
          "[7] S. B. Goldberg, The secrets of successful mediators, Negotiation Journal 21 (3)": "Care (1984)."
        },
        {
          "[7] S. B. Goldberg, The secrets of successful mediators, Negotiation Journal 21 (3)": "[10] G. Sharma, K. Stefanov, A. Dhall,\nJ. Cai, Graph-based group modelling for"
        },
        {
          "[7] S. B. Goldberg, The secrets of successful mediators, Negotiation Journal 21 (3)": "backchannel detection,\nin: Proceedings of\nthe 30th ACM International Confer-"
        },
        {
          "[7] S. B. Goldberg, The secrets of successful mediators, Negotiation Journal 21 (3)": "ence on Multimedia, 2022, pp. 7190–7194."
        },
        {
          "[7] S. B. Goldberg, The secrets of successful mediators, Negotiation Journal 21 (3)": "[11] A. Amer, C. Bhuvaneshwara, G. K. Addluri, M. M. Shaik, V. Bonde, P. Müller,"
        },
        {
          "[7] S. B. Goldberg, The secrets of successful mediators, Negotiation Journal 21 (3)": "Backchannel detection and agreement estimation from video with transformer"
        },
        {
          "[7] S. B. Goldberg, The secrets of successful mediators, Negotiation Journal 21 (3)": "networks, in: 2023 International Joint Conference on Neural Networks (IJCNN),"
        },
        {
          "[7] S. B. Goldberg, The secrets of successful mediators, Negotiation Journal 21 (3)": "IEEE, 2023, pp. 1–8."
        },
        {
          "[7] S. B. Goldberg, The secrets of successful mediators, Negotiation Journal 21 (3)": "[12] K. Wang, M. M. Cheung, Y. Zhang, C. Yang, P. Q. Chen, E. Y. Fu, G. Ngai, Un-"
        },
        {
          "[7] S. B. Goldberg, The secrets of successful mediators, Negotiation Journal 21 (3)": "veiling subtle cues: Backchannel detection using temporal multimodal attention"
        },
        {
          "[7] S. B. Goldberg, The secrets of successful mediators, Negotiation Journal 21 (3)": "networks,\nin: Proceedings of the 31st ACM International Conference on Multi-"
        },
        {
          "[7] S. B. Goldberg, The secrets of successful mediators, Negotiation Journal 21 (3)": "media, 2023, pp. 9586–9590."
        },
        {
          "[7] S. B. Goldberg, The secrets of successful mediators, Negotiation Journal 21 (3)": "[13]\nJ. Liao, Y. Hao, Z. Zhou, J. Pan, Y. Liang, Sequence-level affective level esti-"
        },
        {
          "[7] S. B. Goldberg, The secrets of successful mediators, Negotiation Journal 21 (3)": "mation based on pyramidal\nfacial expression features, Pattern Recognition 145"
        },
        {
          "[7] S. B. Goldberg, The secrets of successful mediators, Negotiation Journal 21 (3)": "(2024) 109958."
        },
        {
          "[7] S. B. Goldberg, The secrets of successful mediators, Negotiation Journal 21 (3)": "[14] T. Li, K.-L. Chan, T. Tjahjadi, Multi-scale correlation module for video-based"
        },
        {
          "[7] S. B. Goldberg, The secrets of successful mediators, Negotiation Journal 21 (3)": "facial expression recognition in the wild, Pattern Recognition 142 (2023) 109691."
        },
        {
          "[7] S. B. Goldberg, The secrets of successful mediators, Negotiation Journal 21 (3)": "[15] W. Fan, X. Xu, B. Cai, X. Xing,\nIsnet:\nIndividual standardization network for"
        },
        {
          "[7] S. B. Goldberg, The secrets of successful mediators, Negotiation Journal 21 (3)": "speech emotion recognition,\nIEEE/ACM Transactions on Audio, Speech,\nand"
        },
        {
          "[7] S. B. Goldberg, The secrets of successful mediators, Negotiation Journal 21 (3)": "Language Processing 30 (2022) 1803–1814."
        }
      ],
      "page": 31
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[16] C. Chen, P. Zhang, Dsnet: Disentangled siamese network with neutral calibration": "for speech emotion recognition,\nJournal of Shanghai Jiaotong University (Sci-"
        },
        {
          "[16] C. Chen, P. Zhang, Dsnet: Disentangled siamese network with neutral calibration": "ence) (2024) 1–10."
        },
        {
          "[16] C. Chen, P. Zhang, Dsnet: Disentangled siamese network with neutral calibration": "[17] A. Reece, G. Cooney, P. Bull, C. Chung, B. Dawson, C. Fitzpatrick, T. Glazer,"
        },
        {
          "[16] C. Chen, P. Zhang, Dsnet: Disentangled siamese network with neutral calibration": "D. Knox, A. Liebscher, S. Marin, The candor corpus:\nInsights\nfrom a large"
        },
        {
          "[16] C. Chen, P. Zhang, Dsnet: Disentangled siamese network with neutral calibration": "multimodal dataset of naturalistic conversation, Science Advances 9 (13) (2023)"
        },
        {
          "[16] C. Chen, P. Zhang, Dsnet: Disentangled siamese network with neutral calibration": "eadf3197."
        },
        {
          "[16] C. Chen, P. Zhang, Dsnet: Disentangled siamese network with neutral calibration": "[18] P. Brown, S. C. Levinson, Politeness: Some universals in language usage, Vol. 4,"
        },
        {
          "[16] C. Chen, P. Zhang, Dsnet: Disentangled siamese network with neutral calibration": "Cambridge university press, 1987."
        },
        {
          "[16] C. Chen, P. Zhang, Dsnet: Disentangled siamese network with neutral calibration": "[19] P. Cutrone, A case\nstudy examining backchannels\nin conversations between"
        },
        {
          "[16] C. Chen, P. Zhang, Dsnet: Disentangled siamese network with neutral calibration": "japanese–british dyads (2005)."
        },
        {
          "[16] C. Chen, P. Zhang, Dsnet: Disentangled siamese network with neutral calibration": "[20] V. H. Yngve, On getting a word in edgewise,\nin: Papers from the sixth regional"
        },
        {
          "[16] C. Chen, P. Zhang, Dsnet: Disentangled siamese network with neutral calibration": "meeting Chicago Linguistic Society, April 16-18, 1970, Chicago Linguistic Soci-"
        },
        {
          "[16] C. Chen, P. Zhang, Dsnet: Disentangled siamese network with neutral calibration": "ety, Chicago, 1970, pp. 567–578."
        },
        {
          "[16] C. Chen, P. Zhang, Dsnet: Disentangled siamese network with neutral calibration": "[21] S. Duncan Jr, On the structure of speaker–auditor\ninteraction during speaking"
        },
        {
          "[16] C. Chen, P. Zhang, Dsnet: Disentangled siamese network with neutral calibration": "turns1, Language in society 3 (2) (1974) 161–180."
        },
        {
          "[16] C. Chen, P. Zhang, Dsnet: Disentangled siamese network with neutral calibration": "[22]\nJ. B. Bavelas, L. Coates, T. Johnson, Listeners as co-narrators., Journal of person-"
        },
        {
          "[16] C. Chen, P. Zhang, Dsnet: Disentangled siamese network with neutral calibration": "ality and social psychology 79 (6) (2000) 941."
        },
        {
          "[16] C. Chen, P. Zhang, Dsnet: Disentangled siamese network with neutral calibration": "[23] K. P. Truong, R. W. Poppe, D. K. Heylen, A rule-based backchannel prediction"
        },
        {
          "[16] C. Chen, P. Zhang, Dsnet: Disentangled siamese network with neutral calibration": "model using pitch and pause information,\nin:\n11th Annual Conference of\nthe"
        },
        {
          "[16] C. Chen, P. Zhang, Dsnet: Disentangled siamese network with neutral calibration": "International Speech Communication Association, INTERSPEECH 2010, Inter-"
        },
        {
          "[16] C. Chen, P. Zhang, Dsnet: Disentangled siamese network with neutral calibration": "national Speech Communication Association, 2010, pp. 3058–3061."
        },
        {
          "[16] C. Chen, P. Zhang, Dsnet: Disentangled siamese network with neutral calibration": "[24] R. Ruede, M. Müller, S. Stüker, A. Waibel, Yeah, right, uh-huh: a deep learning"
        },
        {
          "[16] C. Chen, P. Zhang, Dsnet: Disentangled siamese network with neutral calibration": "backchannel predictor,\nin: Advanced social\ninteraction with agents: 8th interna-"
        },
        {
          "[16] C. Chen, P. Zhang, Dsnet: Disentangled siamese network with neutral calibration": "tional workshop on spoken dialog systems, Springer, 2019, pp. 247–258."
        },
        {
          "[16] C. Chen, P. Zhang, Dsnet: Disentangled siamese network with neutral calibration": "[25] P. Müller, M. X. Huang, A. Bulling, Detecting low rapport during natural\ninter-"
        },
        {
          "[16] C. Chen, P. Zhang, Dsnet: Disentangled siamese network with neutral calibration": "actions in small groups from non-verbal behaviour,\nin: Proceedings of the 23rd"
        },
        {
          "[16] C. Chen, P. Zhang, Dsnet: Disentangled siamese network with neutral calibration": "International Conference on Intelligent User Interfaces, 2018, pp. 153–164."
        }
      ],
      "page": 32
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[26] A. Vaswani, N. Shazeer, N. Parmar,\nJ. Uszkoreit, L.\nJones, A. N. Gomez,": "Ł. Kaiser,\nI. Polosukhin, Attention is all you need, Advances\nin neural\ninfor-"
        },
        {
          "[26] A. Vaswani, N. Shazeer, N. Parmar,\nJ. Uszkoreit, L.\nJones, A. N. Gomez,": "mation processing systems 30 (2017)."
        },
        {
          "[26] A. Vaswani, N. Shazeer, N. Parmar,\nJ. Uszkoreit, L.\nJones, A. N. Gomez,": "[27] E. Sariyanidi, H. Gunes, A. Cavallaro, Automatic analysis of facial affect: A sur-"
        },
        {
          "[26] A. Vaswani, N. Shazeer, N. Parmar,\nJ. Uszkoreit, L.\nJones, A. N. Gomez,": "vey of registration, representation, and recognition, IEEE transactions on pattern"
        },
        {
          "[26] A. Vaswani, N. Shazeer, N. Parmar,\nJ. Uszkoreit, L.\nJones, A. N. Gomez,": "analysis and machine intelligence 37 (6) (2014) 1113–1133."
        },
        {
          "[26] A. Vaswani, N. Shazeer, N. Parmar,\nJ. Uszkoreit, L.\nJones, A. N. Gomez,": "[28] W.-S. Chu, F. De la Torre, J. F. Cohn, Selective transfer machine for personalized"
        },
        {
          "[26] A. Vaswani, N. Shazeer, N. Parmar,\nJ. Uszkoreit, L.\nJones, A. N. Gomez,": "facial expression analysis,\nIEEE transactions on pattern analysis and machine"
        },
        {
          "[26] A. Vaswani, N. Shazeer, N. Parmar,\nJ. Uszkoreit, L.\nJones, A. N. Gomez,": "intelligence 39 (3) (2016) 529–545."
        },
        {
          "[26] A. Vaswani, N. Shazeer, N. Parmar,\nJ. Uszkoreit, L.\nJones, A. N. Gomez,": "[29] C. M. Hurley, A. E. Anker, M. G. Frank, D. Matsumoto, H. C. Hwang, Back-"
        },
        {
          "[26] A. Vaswani, N. Shazeer, N. Parmar,\nJ. Uszkoreit, L.\nJones, A. N. Gomez,": "ground factors predicting accuracy and improvement\nin micro expression recog-"
        },
        {
          "[26] A. Vaswani, N. Shazeer, N. Parmar,\nJ. Uszkoreit, L.\nJones, A. N. Gomez,": "nition, Motivation and emotion 38 (5) (2014) 700–714."
        },
        {
          "[26] A. Vaswani, N. Shazeer, N. Parmar,\nJ. Uszkoreit, L.\nJones, A. N. Gomez,": "[30] G. Giannakakis, D. Grigoriadis, K. Giannakaki, O. Simantiraki, A. Roniotis,"
        },
        {
          "[26] A. Vaswani, N. Shazeer, N. Parmar,\nJ. Uszkoreit, L.\nJones, A. N. Gomez,": "M. Tsiknakis, Review on psychological stress detection using biosignals,\nIEEE"
        },
        {
          "[26] A. Vaswani, N. Shazeer, N. Parmar,\nJ. Uszkoreit, L.\nJones, A. N. Gomez,": "transactions on affective computing 13 (1) (2019) 440–460."
        },
        {
          "[26] A. Vaswani, N. Shazeer, N. Parmar,\nJ. Uszkoreit, L.\nJones, A. N. Gomez,": "[31] S. Wang, Y. Chang, Q. Li, C. Wang, G. Li, M. Mao, Pose-robust personalized"
        },
        {
          "[26] A. Vaswani, N. Shazeer, N. Parmar,\nJ. Uszkoreit, L.\nJones, A. N. Gomez,": "facial expression recognition through unsupervised multi-source domain adapta-"
        },
        {
          "[26] A. Vaswani, N. Shazeer, N. Parmar,\nJ. Uszkoreit, L.\nJones, A. N. Gomez,": "tion, Pattern Recognition 150 (2024) 110311."
        },
        {
          "[26] A. Vaswani, N. Shazeer, N. Parmar,\nJ. Uszkoreit, L.\nJones, A. N. Gomez,": "[32] P. Blomsma, J. Vaitonyté, G. Skantze, M. Swerts, Backchannel behavior\nis id-"
        },
        {
          "[26] A. Vaswani, N. Shazeer, N. Parmar,\nJ. Uszkoreit, L.\nJones, A. N. Gomez,": "iosyncratic, Language and Cognition 16 (4) (2024) 1158–1181."
        },
        {
          "[26] A. Vaswani, N. Shazeer, N. Parmar,\nJ. Uszkoreit, L.\nJones, A. N. Gomez,": "[33] E. De Sevin, S. J. Hyniewska, C. Pelachaud,\nInfluence of personality traits on"
        },
        {
          "[26] A. Vaswani, N. Shazeer, N. Parmar,\nJ. Uszkoreit, L.\nJones, A. N. Gomez,": "backchannel selection,\nin:\nIntelligent Virtual Agents: 10th International Confer-"
        },
        {
          "[26] A. Vaswani, N. Shazeer, N. Parmar,\nJ. Uszkoreit, L.\nJones, A. N. Gomez,": "ence, IVA 2010, Philadelphia, PA, USA, September 20-22, 2010. Proceedings 10,"
        },
        {
          "[26] A. Vaswani, N. Shazeer, N. Parmar,\nJ. Uszkoreit, L.\nJones, A. N. Gomez,": "Springer, 2010, pp. 187–193."
        },
        {
          "[26] A. Vaswani, N. Shazeer, N. Parmar,\nJ. Uszkoreit, L.\nJones, A. N. Gomez,": "[34]\nJ. Chen, B. Hu, Y. Wang,\nP. Moore, Y. Dai, L. Feng, Z. Ding,\nSubject-"
        },
        {
          "[26] A. Vaswani, N. Shazeer, N. Parmar,\nJ. Uszkoreit, L.\nJones, A. N. Gomez,": "independent emotion recognition based on physiological signals:\na three-stage"
        },
        {
          "[26] A. Vaswani, N. Shazeer, N. Parmar,\nJ. Uszkoreit, L.\nJones, A. N. Gomez,": "decision method, BMC medical\ninformatics and decision making 17 (2017) 45–"
        },
        {
          "[26] A. Vaswani, N. Shazeer, N. Parmar,\nJ. Uszkoreit, L.\nJones, A. N. Gomez,": "57."
        }
      ],
      "page": 33
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[35]": "",
          "J. Wang, L. Fei, L. Sun, Multi-level network lasso for multi-task personalized": "learning, Pattern Recognition 161 (2025) 111213."
        },
        {
          "[35]": "[36] X. Zhang, W. Liang, T. Ding, J. Pan, J. Shen, X. Huang, J. Gao, Individual simi-",
          "J. Wang, L. Fei, L. Sun, Multi-level network lasso for multi-task personalized": ""
        },
        {
          "[35]": "",
          "J. Wang, L. Fei, L. Sun, Multi-level network lasso for multi-task personalized": "larity guided transfer modeling for eeg-based emotion recognition, in: 2019 ieee"
        },
        {
          "[35]": "",
          "J. Wang, L. Fei, L. Sun, Multi-level network lasso for multi-task personalized": "international conference on bioinformatics and biomedicine (bibm), IEEE, 2019,"
        },
        {
          "[35]": "",
          "J. Wang, L. Fei, L. Sun, Multi-level network lasso for multi-task personalized": "pp. 1156–1161."
        },
        {
          "[35]": "[37] W.-L. Zheng, B.-L. Lu, Personalizing eeg-based affective models with transfer",
          "J. Wang, L. Fei, L. Sun, Multi-level network lasso for multi-task personalized": ""
        },
        {
          "[35]": "",
          "J. Wang, L. Fei, L. Sun, Multi-level network lasso for multi-task personalized": "learning,\nin:\nProceedings of\nthe twenty-fifth international\njoint conference on"
        },
        {
          "[35]": "",
          "J. Wang, L. Fei, L. Sun, Multi-level network lasso for multi-task personalized": "artificial intelligence, 2016, pp. 2732–2738."
        },
        {
          "[35]": "[38] K. Zhang, J. Chen, J. Wang, Y. Leng, C. W. de Silva, C. Fu, Gaussian-guided",
          "J. Wang, L. Fei, L. Sun, Multi-level network lasso for multi-task personalized": ""
        },
        {
          "[35]": "",
          "J. Wang, L. Fei, L. Sun, Multi-level network lasso for multi-task personalized": "feature alignment for unsupervised cross-subject adaptation, Pattern Recognition"
        },
        {
          "[35]": "",
          "J. Wang, L. Fei, L. Sun, Multi-level network lasso for multi-task personalized": "122 (2022) 108332."
        },
        {
          "[35]": "[39] Y. Wang, S. Qiu, X. Ma, H. He, A prototype-based spd matrix network for domain",
          "J. Wang, L. Fei, L. Sun, Multi-level network lasso for multi-task personalized": ""
        },
        {
          "[35]": "",
          "J. Wang, L. Fei, L. Sun, Multi-level network lasso for multi-task personalized": "adaptation eeg emotion recognition, Pattern Recognition 110 (2021) 107626."
        },
        {
          "[35]": "[40] R. Wu, S. Lu, Leed: Label-free expression editing via disentanglement, in: Com-",
          "J. Wang, L. Fei, L. Sun, Multi-level network lasso for multi-task personalized": ""
        },
        {
          "[35]": "",
          "J. Wang, L. Fei, L. Sun, Multi-level network lasso for multi-task personalized": "puter Vision–ECCV 2020:\n16th European Conference, Glasgow, UK, August"
        },
        {
          "[35]": "",
          "J. Wang, L. Fei, L. Sun, Multi-level network lasso for multi-task personalized": "23–28, 2020, Proceedings, Part XII 16, Springer, 2020, pp. 781–798."
        },
        {
          "[35]": "[41] D. Roy, S. Srivastava, A. Kusupati, P. Jain, M. Varma, A. Arora, One size does",
          "J. Wang, L. Fei, L. Sun, Multi-level network lasso for multi-task personalized": ""
        },
        {
          "[35]": "",
          "J. Wang, L. Fei, L. Sun, Multi-level network lasso for multi-task personalized": "not fit all: Multi-scale, cascaded rnns for radar classification, ACM Transactions"
        },
        {
          "[35]": "",
          "J. Wang, L. Fei, L. Sun, Multi-level network lasso for multi-task personalized": "on Sensor Networks (TOSN) 17 (2) (2021) 1–27."
        },
        {
          "[35]": "[42] A. Sordoni, Y. Bengio, H. Vahabi, C. Lioma, J. Grue Simonsen, J.-Y. Nie, A hi-",
          "J. Wang, L. Fei, L. Sun, Multi-level network lasso for multi-task personalized": ""
        },
        {
          "[35]": "",
          "J. Wang, L. Fei, L. Sun, Multi-level network lasso for multi-task personalized": "erarchical recurrent encoder-decoder for generative context-aware query sugges-"
        },
        {
          "[35]": "",
          "J. Wang, L. Fei, L. Sun, Multi-level network lasso for multi-task personalized": "tion, in: proceedings of the 24th ACM international on conference on information"
        },
        {
          "[35]": "",
          "J. Wang, L. Fei, L. Sun, Multi-level network lasso for multi-task personalized": "and knowledge management, 2015, pp. 553–562."
        },
        {
          "[35]": "[43] C.-M. Kuo, S.-H. Lai, M. Sarkis, A compact deep learning model for robust fa-",
          "J. Wang, L. Fei, L. Sun, Multi-level network lasso for multi-task personalized": ""
        },
        {
          "[35]": "",
          "J. Wang, L. Fei, L. Sun, Multi-level network lasso for multi-task personalized": "cial expression recognition, in: Proceedings of the IEEE conference on computer"
        },
        {
          "[35]": "",
          "J. Wang, L. Fei, L. Sun, Multi-level network lasso for multi-task personalized": "vision and pattern recognition workshops, 2018, pp. 2121–2129."
        }
      ],
      "page": 34
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[44] B. Tadas, Z. Amir, L. Y. Chong, M. Louis-Philippe, Openface 2.0: Facial behavior": ""
        },
        {
          "[44] B. Tadas, Z. Amir, L. Y. Chong, M. Louis-Philippe, Openface 2.0: Facial behavior": ""
        },
        {
          "[44] B. Tadas, Z. Amir, L. Y. Chong, M. Louis-Philippe, Openface 2.0: Facial behavior": "[45] Y. Wang, X. Pan, S. Song, H. Zhang, G. Huang, C. Wu, Implicit semantic data"
        },
        {
          "[44] B. Tadas, Z. Amir, L. Y. Chong, M. Louis-Philippe, Openface 2.0: Facial behavior": ""
        },
        {
          "[44] B. Tadas, Z. Amir, L. Y. Chong, M. Louis-Philippe, Openface 2.0: Facial behavior": ""
        },
        {
          "[44] B. Tadas, Z. Amir, L. Y. Chong, M. Louis-Philippe, Openface 2.0: Facial behavior": "[46] B. Li, F. Wu, S.-N. Lim, S. Belongie, K. Q. Weinberger, On feature normal-"
        },
        {
          "[44] B. Tadas, Z. Amir, L. Y. Chong, M. Louis-Philippe, Openface 2.0: Facial behavior": ""
        },
        {
          "[44] B. Tadas, Z. Amir, L. Y. Chong, M. Louis-Philippe, Openface 2.0: Facial behavior": ""
        },
        {
          "[44] B. Tadas, Z. Amir, L. Y. Chong, M. Louis-Philippe, Openface 2.0: Facial behavior": "[47] K. Seo, H. Cho, D. Choi,"
        },
        {
          "[44] B. Tadas, Z. Amir, L. Y. Chong, M. Louis-Philippe, Openface 2.0: Facial behavior": ""
        },
        {
          "[44] B. Tadas, Z. Amir, L. Y. Chong, M. Louis-Philippe, Openface 2.0: Facial behavior": "[48] D. Bahdanau, K. Cho, Y. Bengio, Neural machine translation by jointly learning"
        },
        {
          "[44] B. Tadas, Z. Amir, L. Y. Chong, M. Louis-Philippe, Openface 2.0: Facial behavior": ""
        },
        {
          "[44] B. Tadas, Z. Amir, L. Y. Chong, M. Louis-Philippe, Openface 2.0: Facial behavior": "[49]"
        },
        {
          "[44] B. Tadas, Z. Amir, L. Y. Chong, M. Louis-Philippe, Openface 2.0: Facial behavior": ""
        },
        {
          "[44] B. Tadas, Z. Amir, L. Y. Chong, M. Louis-Philippe, Openface 2.0: Facial behavior": "[50] D. Britz, A. Goldie, M.-T. Luong, Q. Le, Massive exploration of neural machine"
        },
        {
          "[44] B. Tadas, Z. Amir, L. Y. Chong, M. Louis-Philippe, Openface 2.0: Facial behavior": ""
        },
        {
          "[44] B. Tadas, Z. Amir, L. Y. Chong, M. Louis-Philippe, Openface 2.0: Facial behavior": "[51]"
        },
        {
          "[44] B. Tadas, Z. Amir, L. Y. Chong, M. Louis-Philippe, Openface 2.0: Facial behavior": ""
        },
        {
          "[44] B. Tadas, Z. Amir, L. Y. Chong, M. Louis-Philippe, Openface 2.0: Facial behavior": ""
        }
      ],
      "page": 35
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Weakly supervised learning with multi-stream cnn-lstm-hmms to discover sequential parallelism in sign language videos",
      "authors": [
        "O Koller",
        "N Camgoz",
        "H Ney",
        "R Bowden"
      ],
      "year": "2019",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "2",
      "title": "Some functions of gaze-direction in social interaction",
      "authors": [
        "A Kendon"
      ],
      "year": "1967",
      "venue": "Acta psychologica"
    },
    {
      "citation_id": "3",
      "title": "Multimodal backchannels for embodied conversational agents",
      "authors": [
        "E Bevacqua",
        "S Pammi",
        "S Hyniewska",
        "M Schröder",
        "C Pelachaud"
      ],
      "year": "2010",
      "venue": "Intelligent Virtual Agents: 10th International Conference"
    },
    {
      "citation_id": "4",
      "title": "Multimediate'22: Backchannel detection and agreement estimation in group interactions",
      "authors": [
        "P Müller",
        "M Dietz",
        "D Schiller",
        "D Thomas",
        "H Lindsay",
        "P Gebhard",
        "E André",
        "A Bulling"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "5",
      "title": "A simplest systematics for the organization of turn-taking for conversation",
      "authors": [
        "H Sacks",
        "E Schegloff",
        "G Jefferson"
      ],
      "year": "1974",
      "venue": "A simplest systematics for the organization of turn-taking for conversation"
    },
    {
      "citation_id": "6",
      "title": "3-d articulated pose tracking for untethered diectic reference",
      "authors": [
        "D Demirdjian",
        "T Darrell"
      ],
      "year": "2002",
      "venue": "Proceedings. Fourth IEEE International Conference on Multimodal Interfaces"
    },
    {
      "citation_id": "7",
      "title": "The secrets of successful mediators",
      "authors": [
        "S Goldberg"
      ],
      "year": "2005",
      "venue": "Negotiation Journal"
    },
    {
      "citation_id": "8",
      "title": "Examiner familiarity effects on test performance: implications for training and practice",
      "authors": [
        "D Fuchs"
      ],
      "year": "1987",
      "venue": "Topics in Early Childhood Special Education"
    },
    {
      "citation_id": "9",
      "title": "Rapport and relationships: the basis of child care",
      "authors": [
        "M Burns"
      ],
      "year": "1984",
      "venue": "Journal of Child Care"
    },
    {
      "citation_id": "10",
      "title": "Graph-based group modelling for backchannel detection",
      "authors": [
        "G Sharma",
        "K Stefanov",
        "A Dhall",
        "J Cai"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "11",
      "title": "Backchannel detection and agreement estimation from video with transformer networks",
      "authors": [
        "A Amer",
        "C Bhuvaneshwara",
        "G Addluri",
        "M Shaik",
        "V Bonde",
        "P Müller"
      ],
      "year": "2023",
      "venue": "2023 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "12",
      "title": "Unveiling subtle cues: Backchannel detection using temporal multimodal attention networks",
      "authors": [
        "K Wang",
        "M Cheung",
        "Y Zhang",
        "C Yang",
        "P Chen",
        "E Fu",
        "G Ngai"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "13",
      "title": "Sequence-level affective level estimation based on pyramidal facial expression features",
      "authors": [
        "J Liao",
        "Y Hao",
        "Z Zhou",
        "J Pan",
        "Y Liang"
      ],
      "year": "2024",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "14",
      "title": "Multi-scale correlation module for video-based facial expression recognition in the wild",
      "authors": [
        "T Li",
        "K.-L Chan",
        "T Tjahjadi"
      ],
      "year": "2023",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "15",
      "title": "Isnet: Individual standardization network for speech emotion recognition",
      "authors": [
        "W Fan",
        "X Xu",
        "B Cai",
        "X Xing"
      ],
      "year": "2022",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "16",
      "title": "Dsnet: Disentangled siamese network with neutral calibration for speech emotion recognition",
      "authors": [
        "C Chen",
        "P Zhang"
      ],
      "year": "2024",
      "venue": "Journal of Shanghai Jiaotong University (Science)"
    },
    {
      "citation_id": "17",
      "title": "The candor corpus: Insights from a large multimodal dataset of naturalistic conversation",
      "authors": [
        "A Reece",
        "G Cooney",
        "P Bull",
        "C Chung",
        "B Dawson",
        "C Fitzpatrick",
        "T Glazer",
        "D Knox",
        "A Liebscher",
        "S Marin"
      ],
      "year": "2023",
      "venue": "Science Advances"
    },
    {
      "citation_id": "18",
      "title": "Politeness: Some universals in language usage",
      "authors": [
        "P Brown",
        "S Levinson"
      ],
      "year": "1987",
      "venue": "Politeness: Some universals in language usage"
    },
    {
      "citation_id": "19",
      "title": "A case study examining backchannels in conversations between japanese-british dyads",
      "authors": [
        "P Cutrone"
      ],
      "year": "2005",
      "venue": "A case study examining backchannels in conversations between japanese-british dyads"
    },
    {
      "citation_id": "20",
      "title": "On getting a word in edgewise",
      "authors": [
        "V Yngve"
      ],
      "year": "1970",
      "venue": "Papers from the sixth regional meeting Chicago Linguistic Society"
    },
    {
      "citation_id": "21",
      "title": "On the structure of speaker-auditor interaction during speaking turns1",
      "authors": [
        "S Duncan"
      ],
      "year": "1974",
      "venue": "Language in society"
    },
    {
      "citation_id": "22",
      "title": "Listeners as co-narrators",
      "authors": [
        "J Bavelas",
        "L Coates",
        "T Johnson"
      ],
      "year": "2000",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "23",
      "title": "A rule-based backchannel prediction model using pitch and pause information",
      "authors": [
        "K Truong",
        "R Poppe",
        "D Heylen"
      ],
      "year": "2010",
      "venue": "11th Annual Conference of the International Speech Communication Association, INTERSPEECH 2010, International Speech Communication Association"
    },
    {
      "citation_id": "24",
      "title": "Yeah, right, uh-huh: a deep learning backchannel predictor",
      "authors": [
        "R Ruede",
        "M Müller",
        "S Stüker",
        "A Waibel"
      ],
      "year": "2019",
      "venue": "Advanced social interaction with agents: 8th international workshop on spoken dialog systems"
    },
    {
      "citation_id": "25",
      "title": "Detecting low rapport during natural interactions in small groups from non-verbal behaviour",
      "authors": [
        "P Müller",
        "M Huang",
        "A Bulling"
      ],
      "year": "2018",
      "venue": "Proceedings of the 23rd International Conference on Intelligent User Interfaces"
    },
    {
      "citation_id": "26",
      "title": "Attention is all you need, Advances in neural information processing systems",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Attention is all you need, Advances in neural information processing systems"
    },
    {
      "citation_id": "27",
      "title": "Automatic analysis of facial affect: A survey of registration, representation, and recognition",
      "authors": [
        "E Sariyanidi",
        "H Gunes",
        "A Cavallaro"
      ],
      "year": "2014",
      "venue": "Automatic analysis of facial affect: A survey of registration, representation, and recognition"
    },
    {
      "citation_id": "28",
      "title": "Selective transfer machine for personalized facial expression analysis",
      "authors": [
        "W.-S Chu",
        "F De La Torre",
        "J Cohn"
      ],
      "year": "2016",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "29",
      "title": "Background factors predicting accuracy and improvement in micro expression recognition",
      "authors": [
        "C Hurley",
        "A Anker",
        "M Frank",
        "D Matsumoto",
        "H Hwang"
      ],
      "year": "2014",
      "venue": "Motivation and emotion"
    },
    {
      "citation_id": "30",
      "title": "Review on psychological stress detection using biosignals",
      "authors": [
        "G Giannakakis",
        "D Grigoriadis",
        "K Giannakaki",
        "O Simantiraki",
        "A Roniotis",
        "M Tsiknakis"
      ],
      "year": "2019",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "31",
      "title": "Pose-robust personalized facial expression recognition through unsupervised multi-source domain adaptation",
      "authors": [
        "S Wang",
        "Y Chang",
        "Q Li",
        "C Wang",
        "G Li",
        "M Mao"
      ],
      "year": "2024",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "32",
      "title": "Backchannel behavior is idiosyncratic",
      "authors": [
        "P Blomsma",
        "J Vaitonyté",
        "G Skantze",
        "M Swerts"
      ],
      "year": "2024",
      "venue": "Language and Cognition"
    },
    {
      "citation_id": "33",
      "title": "Influence of personality traits on backchannel selection",
      "authors": [
        "E De Sevin",
        "S Hyniewska",
        "C Pelachaud"
      ],
      "year": "2010",
      "venue": "Intelligent Virtual Agents: 10th International Conference, IVA 2010"
    },
    {
      "citation_id": "34",
      "title": "Subjectindependent emotion recognition based on physiological signals: a three-stage decision method",
      "authors": [
        "J Chen",
        "B Hu",
        "Y Wang",
        "P Moore",
        "Y Dai",
        "L Feng",
        "Z Ding"
      ],
      "year": "2017",
      "venue": "BMC medical informatics and decision making"
    },
    {
      "citation_id": "35",
      "title": "Multi-level network lasso for multi-task personalized learning",
      "authors": [
        "J Wang",
        "L Fei",
        "L Sun"
      ],
      "year": "2025",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "36",
      "title": "Individual similarity guided transfer modeling for eeg-based emotion recognition",
      "authors": [
        "X Zhang",
        "W Liang",
        "T Ding",
        "J Pan",
        "J Shen",
        "X Huang",
        "J Gao"
      ],
      "year": "2019",
      "venue": "2019 ieee international conference on bioinformatics and biomedicine (bibm)"
    },
    {
      "citation_id": "37",
      "title": "Personalizing eeg-based affective models with transfer learning",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2016",
      "venue": "Proceedings of the twenty-fifth international joint conference on artificial intelligence"
    },
    {
      "citation_id": "38",
      "title": "Gaussian-guided feature alignment for unsupervised cross-subject adaptation",
      "authors": [
        "K Zhang",
        "J Chen",
        "J Wang",
        "Y Leng",
        "C De Silva",
        "C Fu"
      ],
      "year": "2022",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "39",
      "title": "A prototype-based spd matrix network for domain adaptation eeg emotion recognition",
      "authors": [
        "Y Wang",
        "S Qiu",
        "X Ma",
        "H He"
      ],
      "year": "2021",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "40",
      "title": "Leed: Label-free expression editing via disentanglement",
      "authors": [
        "R Wu",
        "S Lu"
      ],
      "year": "2020",
      "venue": "Computer Vision-ECCV 2020: 16th European Conference"
    },
    {
      "citation_id": "41",
      "title": "One size does not fit all: Multi-scale, cascaded rnns for radar classification",
      "authors": [
        "D Roy",
        "S Srivastava",
        "A Kusupati",
        "P Jain",
        "M Varma",
        "A Arora"
      ],
      "year": "2021",
      "venue": "ACM Transactions on Sensor Networks (TOSN)"
    },
    {
      "citation_id": "42",
      "title": "A hierarchical recurrent encoder-decoder for generative context-aware query suggestion",
      "authors": [
        "A Sordoni",
        "Y Bengio",
        "H Vahabi",
        "C Lioma",
        "J Simonsen",
        "J.-Y Nie"
      ],
      "year": "2015",
      "venue": "proceedings of the 24th ACM international on conference on information and knowledge management"
    },
    {
      "citation_id": "43",
      "title": "A compact deep learning model for robust facial expression recognition",
      "authors": [
        "C.-M Kuo",
        "S.-H Lai",
        "M Sarkis"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition workshops"
    },
    {
      "citation_id": "44",
      "title": "13th IEEE International Conference on Automatic Face & Gesture Recognition",
      "authors": [
        "B Tadas",
        "Z Amir",
        "L Chong",
        "M Louis-Philippe"
      ],
      "year": "2018",
      "venue": "13th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "45",
      "title": "Implicit semantic data augmentation for deep networks",
      "authors": [
        "Y Wang",
        "X Pan",
        "S Song",
        "H Zhang",
        "G Huang",
        "C Wu"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "46",
      "title": "On feature normalization and data augmentation",
      "authors": [
        "B Li",
        "F Wu",
        "S.-N Lim",
        "S Belongie",
        "K Weinberger"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "47",
      "title": "Implicit semantic data augmentation for hand pose estimation",
      "authors": [
        "K Seo",
        "H Cho",
        "D Choi",
        "J.-D Park"
      ],
      "year": "2022",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "48",
      "title": "Neural machine translation by jointly learning to align and translate",
      "authors": [
        "D Bahdanau",
        "K Cho",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Neural machine translation by jointly learning to align and translate",
      "arxiv": "arXiv:1409.0473"
    },
    {
      "citation_id": "49",
      "title": "Sequence to sequence learning with neural networks",
      "authors": [
        "I Sutskever",
        "O Vinyals",
        "Q Le"
      ],
      "year": "2014",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "50",
      "title": "Massive exploration of neural machine translation architectures",
      "authors": [
        "D Britz",
        "A Goldie",
        "M.-T Luong",
        "Q Le"
      ],
      "year": "2017",
      "venue": "Massive exploration of neural machine translation architectures",
      "arxiv": "arXiv:1703.03906"
    },
    {
      "citation_id": "51",
      "title": "A light weight model for active speaker detection",
      "authors": [
        "J Liao",
        "H Duan",
        "K Feng",
        "W Zhao",
        "Y Yang",
        "L Chen"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    }
  ]
}