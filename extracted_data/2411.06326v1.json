{
  "paper_id": "2411.06326v1",
  "title": "Emotion-Aware Interaction Design In Intelligent User Interface Using Multi-Modal Deep Learning",
  "published": "2024-11-10T01:26:39Z",
  "authors": [
    "Shiyu Duan",
    "Ziyi Wang",
    "Shixiao Wang",
    "Mengmeng Chen",
    "Runsheng Zhang"
  ],
  "keywords": [
    "Human-computer interaction",
    "Multimodal emotion recognition",
    "Intelligent user interface",
    "Transformer architecture",
    "Deep learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In an era where user interaction with technology is ubiquitous, the importance of user interface (UI) design cannot be overstated. A well-designed UI not only enhances usability but also fosters more natural, intuitive, and emotionally engaging experiences, making technology more accessible and impactful in everyday life. This research addresses this growing need by introducing an advanced emotion recognition system to significantly improve the emotional responsiveness of UI. By integrating facial expressions, speech, and textual data through a multi-branch Transformer model, the system interprets complex emotional cues in real-time, enabling UIs to interact more empathetically and effectively with users. Using the public MELD dataset for validation, our model demonstrates substantial improvements in emotion recognition accuracy and F1 scores, outperforming traditional methods. These findings underscore the critical role that sophisticated emotion recognition plays in the evolution of UIs, making technology more attuned to user needs and emotions. This study highlights how enhanced emotional intelligence in UIs is not only about technical innovation but also about fostering deeper, more meaningful connections between users and the digital world, ultimately shaping how people interact with technology in their daily lives.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "With the rapid development of technology, the design and development of intelligent user interfaces (UI) plays an increasingly important role in the field of modern technology. Traditional UI mainly relies on user input, such as button clicks or sliding operations for interaction  [1] . However, with the continuous advancement of artificial intelligence (AI) and deep learning technology  [2] [3] [4] , intelligent UI has gradually integrated emotion recognition functions to provide a more personalized and adaptive interactive experience through the perception of the user's emotional state. This kind of intelligent UI based on emotion recognition can be applied in a variety of scenarios, including risk alert  [5] [6] [7] [8] , health monitoring  [9] [10] [11] [12] , computer vision  [13] , customer service, and other fields, effectively improving the quality and efficiency of user experience.\n\nEmotion recognition technology relies on the analysis of multi-modal data such as users' facial expressions, voice characteristics, body movements, and text information to determine the user's current emotional state  [14] . In recent years, the successful application of deep learning, especially Transformer-based models, in the fields of natural language processing (NLP)  [15] and computer vision  [16]  has significantly improved the performance of emotion recognition systems. The advantage of the Transformer model is that it can capture the long-range dependencies of data and effectively focus on the most important information through the selfattention mechanism, thereby providing more accurate analysis in the emotion recognition process. By integrating Transformer emotion recognition technology, the intelligent UI can dynamically adjust the interface layout, interaction methods, content recommendations, etc. to better meet the user's emotional needs and preferences, thereby achieving a more natural interaction effect.\n\nIn the background, research on emotion recognition technology has broad practical significance. First of all, in the field of health monitoring, intelligent UI based on emotion recognition can monitor the user's emotional fluctuations in real-time and intervene when the user experiences negative emotions or stress, such as automatically recommending soothing music, relaxation training videos, etc  [17] . Secondly, in the field of education, intelligent UI can determine the emotional state of students during the learning process, such as anxiety, fatigue, or distraction, so as to automatically adjust the learning content or recommend a break. Finally, in the field of customer service, intelligent UI can determine whether the customer is satisfied by analyzing the customer's emotional response during the service process, and then adjusting the service strategy or recommending different products to improve customer satisfaction and loyalty.\n\nFor this research topic, we designed an emotion recognition algorithm based on Transformer and applied it to the interaction design of intelligent UI. Our algorithm combines self-attention mechanism and multi-head attention mechanism to handle multi-modal emotional data. Specifically, we adopt a multi-modal Transformer architecture, with one branch used to process facial image sequences and another branch used to analyze the time series characteristics of speech data. In addition, we also added a text analysis branch to process users' text input, such as chat logs or user comments. Through the multi-modal fusion mechanism, the model can synthesize information from multiple modalities, thereby achieving higher accuracy and stronger robustness in the emotion classification process.\n\nIn practical applications, this emotion recognition algorithm can be integrated into intelligent UI to achieve the ability to dynamically adjust interface content. For example, when the system detects that a user is showing boredom or disinterest while watching a video, it can automatically switch to the user's more preferred content type. In addition, in games, intelligent UI can determine the player's experience in the game based on changes in their expressions, such as happiness, excitement, or frustration, thereby dynamically adjusting the difficulty of the game or providing incentive mechanisms to enhance the user's immersion and participation. Such an intelligent emotional response mechanism not only improves the level of personalization of user experience, but also increases the frequency and effectiveness of interaction between the system and users.\n\nTo sum up, the application of emotion recognition based on intelligent UI has broad prospects. By sensing and responding to user emotions in real-time, smart UI can provide a more natural and efficient human-computer interaction experience. In the future, we will continue to optimize the emotion recognition algorithm, improve its accuracy and robustness in different scenarios, and explore more application scenarios, such as smart homes, online education, and virtual reality. Through continuous innovation and research, we believe that intelligent UI based on emotion recognition will bring more convenient and pleasant experiences to people's lives.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "In recent years, multi-modal deep learning and Transformer-based architectures have significantly influenced the development of emotion-aware interaction in intelligent user interfaces (UIs). These methodologies enable UIs to recognize and respond to users' emotions, enhancing personalized and empathetic interaction. Yan et al.  [18]  contributed to the interpretation of multi-dimensional time series data by transforming them into interpretable event sequences, providing a foundation for multi-modal data processing crucial for real-time emotion recognition. This approach is particularly relevant to emotion-aware UIs where processing diverse data streams, such as facial expressions and vocal cues, requires models capable of handling complex and synchronized time-based inputs effectively.\n\nIn The robustness of emotion recognition models in dynamic environments can benefit from Jiang et al.'s  [19]  work on cross-domain adaptability through adversarial networks, which stabilize the model across varied data inputs. Similarly, Du et al.  [20]  addressed the challenges of semantic complexity with Transformer models in language processing, informing the nuanced textual interpretation required in emotion-aware UIs for detecting emotional states accurately. Optimizing model efficiency, Chen et al.  [21]  introduced retrievalaugmented generation to enhance response relevance in realtime applications-a strategy adaptable to emotion-aware UIs needing rapid contextual responses. Furthermore, Huang et al.  [22]  demonstrated knowledge distillation for efficient model performance, a technique that could support real-time emotion recognition by streamlining the model's structure.\n\nGraph neural networks (GNNs) were leveraged by Wei et al.  [23]  for enhanced feature extraction across diverse data, relevant for synthesizing text sentiment, vocal tone, and facial expressions within a multi-modal UI framework. In alignment with adaptive UI responses, Wang et al.  [24]  explored large language models (LLMs) for automated, genre-aware feedback, underscoring the value of dynamic response generation. Lastly, Liu et al.  [25]  explored self-attention and embedding for personalized recommendations, aligning with the goal of adapting UI responses to users' emotional states in real time.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Method",
      "text": "In this study, we proposed a multimodal emotion recognition method based on the Transformer architecture. In addition to its emotion recognition capabilities, our intelligent UI dynamically adjusts its content based on the user's emotional state. For example, when boredom is detected, the system might switch to a more interactive mode by introducing engaging visual content or recommending interactive quizzes. When positive emotions like excitement are detected, the UI can intensify engagement by providing additional features or gamified challenges. These adaptive content strategies are vital to maintaining user engagement and ensuring a personalized experience. First, we input multimodal data, including facial image sequences, speech signals, and text data, into their respective Transformer branches. The Transformer module of each branch consists of multiple self-attention layers and feedforward neural networks to capture the feature representation of the input data. The overall architecture of the transformer is shown in Figure  1 . We optimize the model parameters by minimizing the loss function so that it can more accurately identify emotion categories.\n\nIn order to further improve the performance of the model, we used data augmentation technology during the training process so that the emotion recognition system can still perform well with limited data. The overall method shows high accuracy and generalization ability in multimodal emotion recognition.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iv. Experiment",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Datasets",
      "text": "In this study, we used the MELD (Multimodal EmotionLines Dataset) dataset for model training and verification. The MELD dataset is a public multimodal emotion recognition dataset designed specifically for studying emotion recognition in conversation scenarios. It comes from a conversation segment of the TV series \"Friends\". Each sample contains a facial expression image sequence, a corresponding voice signal, and a text conversation content, covering seven major emotion categories: joy, anger, sadness, fear, surprise, disgust, and neutral.\n\nThe uniqueness of the MELD dataset is that it not only contains the emotion label of each interlocutor, but also takes into account the conversation context information, allowing researchers to use the context to more accurately predict emotions when training the model. In addition, each sample in the dataset is multimodal, including a facial image sequence (showing the facial changes of the interlocutor during the conversation), a voice file (capturing the speaker's tone and acoustic characteristics), and the corresponding text (i.e., the conversation content). The combination of these multimodal data provides comprehensive information for the model, making the emotion recognition process more accurate and multi-dimensional.\n\nBy using the MELD dataset, we were able to fuse visual, speech, and text information in the emotion recognition model, fully train the Transformer-based model, and test its performance in the multimodal emotion recognition task. The richness and diversity of the dataset ensures that the model can learn and perform accurate emotion classification in a variety of scenarios and emotions, thus verifying the effectiveness of our approach.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. Experiments",
      "text": "In the experiment, we selected five comparative experimental models to evaluate the performance of the emotion recognition system. These models are all deep learning models, including:  In the end, the model we proposed performed best in the experiment, achieving an AUC of 0.817 and an F1 value of 0.795. This result shows that through the efficient Transformer architecture and the deep fusion of multimodal information, our method still shows higher robustness and accuracy in emotion recognition. Compared with other models, our method shows better feature extraction and fusion capabilities when processing multimodal data, effectively improving the accuracy and stability of emotion recognition. These results prove that in emotion recognition tasks, making full use of multimodal data and adopting a reasonable deep learning architecture design are the keys to improving the performance of emotion recognition systems. Finally, we also give the rising graph of AUC during training, the result is shown in Figure  2 . For instance, the system can recognize when a user is disengaged and adapt by recommending alternative content. This dynamic adaptability creates a more engaging experience that aligns more closely with user needs. Unlike traditional UIs, which lack emotional awareness, this system provides a unique empathy-driven approach that fosters deeper and more meaningful user engagement. The significance of UIs extends beyond mere functionality; they are essential for enabling intuitive, seamless, and empathetic interactions that align with the evolving expectations of digital users. Emotionally intelligent UIs enhance user satisfaction by providing more personalized and responsive interactions, which are crucial for fostering deeper user engagement and loyalty. This study not only contributes to the theoretical advancements in UI design but also underscores the practical implications of emotionally aware interfaces in enhancing user experience. Despite the notable achievements of this research, the acquisition and processing of multi-modal data present ongoing challenges, particularly in practical applications. To overcome these limitations and ensure the model's applicability across diverse scenarios, future initiatives will focus on integrating transfer learning and data augmentation strategies to bolster the model's robustness and generalization capabilities. In pursuit of refining human-computer interaction, this research reaffirms the necessity of continuous technological enhancements and application diversification. As UIs evolve to become more adept at interpreting and responding to human emotions, they promise to redefine the paradigms of user interaction with technology, emphasizing the critical role of UIs in the landscape of digital innovation. This commitment to developing highly sophisticated, emotionally intelligent UIs is fundamental to realizing more natural, insightful, and usercentric technological interactions.",
      "page_start": 3,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Figure 1 Transformer overall architecture",
      "page": 2
    },
    {
      "caption": "Figure 2: Figure 2 AUC result rise chart",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Experiment result Figure 2.",
      "data": [
        {
          "Model": "Transformer",
          "Auc": "0.685",
          "F1": "0.653"
        },
        {
          "Model": "CNN-LSTM",
          "Auc": "0.724",
          "F1": "0.698"
        },
        {
          "Model": "BERT-LSTM",
          "Auc": "0.756",
          "F1": "0.727"
        },
        {
          "Model": "MM-Transformer",
          "Auc": "0.789",
          "F1": "0.761"
        },
        {
          "Model": "Ours",
          "Auc": "0.817",
          "F1": "0.795"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "UI/UX Prototype Usability Analysis of E-Commerce Websites",
      "authors": [
        "M Fachrizal",
        "A Paramitha Fadillah",
        "A Budiarto"
      ],
      "year": "2023",
      "venue": "2023 International Conference on Informatics Engineering"
    },
    {
      "citation_id": "2",
      "title": "Deep Learning for Medical Text Processing: BERT Model Fine-Tuning and Comparative Study",
      "authors": [
        "J Hu",
        "Y Cang",
        "G Liu",
        "M Wang",
        "W He",
        "R Bao"
      ],
      "year": "2024",
      "venue": "Deep Learning for Medical Text Processing: BERT Model Fine-Tuning and Comparative Study",
      "arxiv": "arXiv:2410.20792"
    },
    {
      "citation_id": "3",
      "title": "Deep Learning-Based Channel Squeeze U-Structure for Lung Nodule Detection and Segmentation",
      "authors": [
        "M Sui",
        "J Hu",
        "T Zhou",
        "Z Liu",
        "L Wen",
        "J Du"
      ],
      "year": "2024",
      "venue": "Deep Learning-Based Channel Squeeze U-Structure for Lung Nodule Detection and Segmentation",
      "arxiv": "arXiv:2409.13868"
    },
    {
      "citation_id": "4",
      "title": "Predicting Liquidity Coverage Ratio with Gated Recurrent Units: A Deep Learning Model for Risk Management",
      "authors": [
        "Z Xu",
        "J Pan",
        "S Han",
        "H Ouyang",
        "Y Chen",
        "M Jiang"
      ],
      "year": "2024",
      "venue": "Predicting Liquidity Coverage Ratio with Gated Recurrent Units: A Deep Learning Model for Risk Management",
      "arxiv": "arXiv:2410.19211"
    },
    {
      "citation_id": "5",
      "title": "Graph Neural Networks in Financial Markets: Modeling Volatility and Assessing Value-at-Risk",
      "authors": [
        "K Xu",
        "Y Wu",
        "H Xia",
        "N Sang",
        "B Wang"
      ],
      "year": "2022",
      "venue": "Journal of Computer Technology and Software"
    },
    {
      "citation_id": "6",
      "title": "Unveiling the Potential of Graph Neural Networks",
      "authors": [
        "B Liu",
        "I Li",
        "J Yao",
        "Y Chen",
        "G Huang",
        "J Wang"
      ],
      "year": "2024",
      "venue": "SME Credit Risk Assessment",
      "arxiv": "arXiv:2409.17909"
    },
    {
      "citation_id": "7",
      "title": "Applying Hybrid Graph Neural Networks to Strengthen Credit Risk Analysis",
      "authors": [
        "M Sun",
        "W Sun",
        "Y Sun",
        "S Liu",
        "M Jiang",
        "Z Xu"
      ],
      "year": "2024",
      "venue": "Applying Hybrid Graph Neural Networks to Strengthen Credit Risk Analysis",
      "arxiv": "arXiv:2410.04283"
    },
    {
      "citation_id": "8",
      "title": "Dynamic Fraud Detection: Integrating Reinforcement Learning into Graph Neural Networks",
      "authors": [
        "Y Dong",
        "J Yao",
        "J Wang",
        "Y Liang",
        "S Liao",
        "M Xiao"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 6th International Conference on Data-driven Optimization of Complex Systems (DOCS)"
    },
    {
      "citation_id": "9",
      "title": "Research on Intelligent System of Medical Image Recognition and Disease Diagnosis Based on Big Data",
      "authors": [
        "Y Zi",
        "X Cheng",
        "T Mei",
        "Q Wang",
        "Z Gao",
        "H Yang"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 IEEE 2nd International Conference on Image Processing and Computer Applications (ICIPCA)"
    },
    {
      "citation_id": "10",
      "title": "Axial attention transformer networks: A new frontier in breast cancer detection",
      "authors": [
        "W He",
        "R Bao",
        "Y Cang",
        "J Wei",
        "Y Zhang",
        "J Hu"
      ],
      "year": "2024",
      "venue": "Axial attention transformer networks: A new frontier in breast cancer detection",
      "arxiv": "arXiv:2409.12347"
    },
    {
      "citation_id": "11",
      "title": "Research on adverse drug reaction prediction model combining knowledge graph embedding and deep learning",
      "authors": [
        "Y Li",
        "W Zhao",
        "B Dang",
        "X Yan",
        "M Gao",
        "W Wang",
        "M Xiao"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 4th International Conference on Machine Learning and Intelligent Systems Engineering (MLISE)"
    },
    {
      "citation_id": "12",
      "title": "Medical Image Segmentation with Bilateral Spatial Attention and Transfer Learning",
      "authors": [
        "D Sun",
        "M Sui",
        "Y Liang",
        "J Hu",
        "J Du"
      ],
      "year": "2024",
      "venue": "Journal of Computer Science and Software Applications"
    },
    {
      "citation_id": "13",
      "title": "A Lightweight GAN-Based Image Fusion Algorithm for Visible and Infrared Images",
      "authors": [
        "Z Wu",
        "H Gong",
        "J Chen",
        "Z Yuru",
        "L Tan",
        "G Shi"
      ],
      "year": "2024",
      "venue": "A Lightweight GAN-Based Image Fusion Algorithm for Visible and Infrared Images",
      "arxiv": "arXiv:2409.15332"
    },
    {
      "citation_id": "14",
      "title": "Deep Learning with HM-VGG: AI Strategies for Multi-modal Image Analysis",
      "authors": [
        "J Du",
        "Y Cang",
        "T Zhou",
        "J Hu",
        "W He"
      ],
      "year": "2024",
      "venue": "Deep Learning with HM-VGG: AI Strategies for Multi-modal Image Analysis",
      "arxiv": "arXiv:2410.24046"
    },
    {
      "citation_id": "15",
      "title": "Balancing Innovation and Privacy: Data Security Strategies in Natural Language Processing Applications",
      "authors": [
        "S Liu",
        "G Liu",
        "B Zhu",
        "Y Luo",
        "L Wu",
        "R Wang"
      ],
      "year": "2024",
      "venue": "Balancing Innovation and Privacy: Data Security Strategies in Natural Language Processing Applications",
      "arxiv": "arXiv:2410.08553"
    },
    {
      "citation_id": "16",
      "title": "Adversarial Neural Networks in Medical Imaging Advancements and Challenges in Semantic Segmentation",
      "authors": [
        "H Liu",
        "B Zhang",
        "Y Xiang",
        "Y Hu",
        "A Shen",
        "Y Lin"
      ],
      "year": "2024",
      "venue": "Adversarial Neural Networks in Medical Imaging Advancements and Challenges in Semantic Segmentation",
      "arxiv": "arXiv:2410.13099"
    },
    {
      "citation_id": "17",
      "title": "Real-Time Facial Expression Recognition Using Convolutional Neural Networks for Adaptive User Interfaces",
      "authors": [
        "J Sundi",
        "H Kumar",
        "R Bedi"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 5th International Conference for Emerging Technology"
    },
    {
      "citation_id": "18",
      "title": "Transforming Multidimensional Time Series into Interpretable Event Sequences for Advanced Data Mining",
      "authors": [
        "X Yan",
        "Y Jiang",
        "W Liu",
        "D Yi",
        "J Wei"
      ],
      "year": "2024",
      "venue": "Transforming Multidimensional Time Series into Interpretable Event Sequences for Advanced Data Mining",
      "arxiv": "arXiv:2409.14327"
    },
    {
      "citation_id": "19",
      "title": "Wasserstein Distance-Weighted Adversarial Network for Cross-Domain Credit Risk Assessment",
      "authors": [
        "M Jiang",
        "J Lin",
        "H Ouyang",
        "J Pan",
        "S Han",
        "B Liu"
      ],
      "year": "2024",
      "venue": "Wasserstein Distance-Weighted Adversarial Network for Cross-Domain Credit Risk Assessment",
      "arxiv": "arXiv:2409.18544"
    },
    {
      "citation_id": "20",
      "title": "Transformers in Opinion Mining: Addressing Semantic Complexity and Model Challenges in NLP",
      "authors": [
        "J Du",
        "Y Jiang",
        "Y Liang"
      ],
      "year": "2024",
      "venue": "Transactions on Computational and Scientific Methods"
    },
    {
      "citation_id": "21",
      "title": "Optimizing Retrieval-Augmented Generation with Elasticsearch for Enhanced Question-Answering Systems",
      "authors": [
        "J Chen",
        "R Bao",
        "H Zheng",
        "Z Qi",
        "J Wei",
        "J Hu"
      ],
      "year": "2024",
      "venue": "Optimizing Retrieval-Augmented Generation with Elasticsearch for Enhanced Question-Answering Systems",
      "arxiv": "arXiv:2410.14167"
    },
    {
      "citation_id": "22",
      "title": "Optimizing YOLOv5s Object Detection through Knowledge Distillation Algorithm",
      "authors": [
        "G Huang",
        "A Shen",
        "Y Hu",
        "J Du",
        "J Hu",
        "Y Liang"
      ],
      "year": "2024",
      "venue": "Optimizing YOLOv5s Object Detection through Knowledge Distillation Algorithm",
      "arxiv": "arXiv:2410.12259"
    },
    {
      "citation_id": "23",
      "title": "Self-Supervised Graph Neural Networks for Enhanced Feature Extraction in Heterogeneous Information Networks",
      "authors": [
        "J Wei",
        "Y Liu",
        "X Huang",
        "X Zhang",
        "W Liu",
        "X Yan"
      ],
      "year": "2024",
      "venue": "Self-Supervised Graph Neural Networks for Enhanced Feature Extraction in Heterogeneous Information Networks",
      "arxiv": "arXiv:2410.17617"
    },
    {
      "citation_id": "24",
      "title": "Automated Genre-Aware Article Scoring and Feedback Using Large Language Models",
      "authors": [
        "C Wang",
        "Y Dong",
        "Z Zhang",
        "R Wang",
        "S Wang",
        "J Chen"
      ],
      "year": "2024",
      "venue": "Automated Genre-Aware Article Scoring and Feedback Using Large Language Models",
      "arxiv": "arXiv:2410.14165"
    },
    {
      "citation_id": "25",
      "title": "A Recommendation Model Utilizing Separation Embedding and Self-Attention for Feature Mining",
      "authors": [
        "W Liu",
        "R Wang",
        "Y Luo",
        "J Wei",
        "Z Zhao",
        "J Huang"
      ],
      "year": "2024",
      "venue": "A Recommendation Model Utilizing Separation Embedding and Self-Attention for Feature Mining",
      "arxiv": "arXiv:2410.15026"
    }
  ]
}