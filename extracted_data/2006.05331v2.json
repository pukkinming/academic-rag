{
  "paper_id": "2006.05331v2",
  "title": "Data Augmentation For Enhancing Eeg-Based Emotion Recognition With Deep Generative Models",
  "published": "2020-06-04T21:23:09Z",
  "authors": [
    "Yun Luo",
    "Li-Zhen Zhu",
    "Zi-Yu Wan",
    "Bao-Liang Lu"
  ],
  "keywords": [
    "Emotion recognition",
    "EEG",
    "GAN",
    "VAE",
    "deep generative model",
    "data augmentation"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The data scarcity problem in emotion recognition from electroencephalography (EEG) leads to difficulty in building an affective model with high accuracy using machine learning algorithms or deep neural networks. Inspired by emerging deep generative models, we propose three methods for augmenting EEG training data to enhance the performance of emotion recognition models. Our proposed methods are based on two deep generative models, variational autoencoder (VAE) and generative adversarial network (GAN), and two data augmentation strategies. For the full usage strategy, all of the generated data are augmented to the training dataset without judging the quality of the generated data, while for partial usage, only highquality data are selected and appended to the training dataset. These three methods are called conditional Wasserstein GAN (cWGAN), selective VAE (sVAE), and selective WGAN (sWGAN). To evaluate the effectiveness of these methods, we perform a systematic experimental study on two public EEG datasets for emotion recognition, namely, SEED and DEAP. We first generate realistic-like EEG training data in two forms: power spectral density and differential entropy. Then, we augment the original training datasets with a different number of generated realisticlike EEG data. Finally, we train support vector machines and deep neural networks with shortcut layers to build affective models using the original and augmented training datasets. The experimental results demonstrate that the augmented training datasets produced by our methods enhance the performance of EEG-based emotion recognition models and outperform the existing data augmentation methods such as conditional VAE, Gaussian noise, and rotational data augmentation.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotion plays a significant role in how people think, behave, and communicate. Artificial emotional intelligence, which is also known as emotion AI or affective computing, focuses on developing devices and systems that can automatically recognize human emotion and has attracted considerable attention very recently  [1] ,  [2] . For example, integrating emotion assessment in human-computer interaction systems with emotion recognition can make machines more intelligent and provide more humanized interactions. Moreover, studies have shown that some mental diseases, such as depression and autism, are relevant to emotions  [3] . The introduction of emotion AI to these studies can create a high potential for treating psychiatric diseases. Because emotion AI has many potential applications, attention is being focused on the possibility of recognizing emotions from different behavioral cues, such as facial expression  [4] , posture  [5] , voice  [6] , and neurophysiological signals  [7] . Among these signals, electroencephalography (EEG) has been demonstrated as one of the most reliable signals due to its high accuracy and objective benefits. In recent years, EEG-based emotion recognition has attracted widespread attention from academics and industries  [8] ,  [9] ,  [10] ,  [11] ,  [12] . Researchers have made considerable progress in feature extraction and model construction. However, these studies are faced with a problem: the lack of training data.\n\nCompared with visual and audio signals, which can be easily accessed from standard datasets, data acquisition is still one of the bottlenecks in EEG-based emotion recognition tasks. There are mainly five reasons: a) The price of EEG acquisition devices for research is quite high. Additionally, EEG-based emotion recognition experiments are timeconsuming and require tedious preparations, such as skin cleaning and gel injection, which makes it difficult to conduct many experiments. b) These experiments cannot last for a long time because the subjects may feel uncomfortable wearing EEG acquisition devices. Therefore, it is difficult to acquire large-scale labeled EEG data in one experiment. c) The raw EEG data are usually mixed with noise and various artifacts, and researchers have to discard some bad channels and data, which aggravates the data scarcity problem. d) It is difficult to collect precisely labeled data since the subjects may not evoke emotion well in emotion recognition experiments. e) There are only a few public EEG-based emotion recognition datasets, such as SEED  1    [13] , DEAP  2    [14] , DREAMER  [15] , MAHNOB-HCI  3    [16] , and MPED  [17] . Moreover, the scales of these datasets are much smaller than those of public image datasets (e.g., ImageNet). These factors limit the quantity of labeled training data for EEG-based emotion recognition and hinder the performance of emotion recognition models trained by machine learning algorithms and deep neural networks.\n\nIt is common sense that a machine learning model will be more accurate when it can access more training data. For example, the release of the trillion-word corpus by Google improves text-based models  [18] . Machine learning models can be more robust and reliable when learning more effective features from sufficient training data, especially for deep learning models that need a vast quantity of training data. Deep learning models have recently achieved remarkable results in the fields of computer vision, speech recognition, and natural language processing due to the accessibility of large datasets  [19] .\n\nIn the field of EEG-based emotion recognition, Zheng and Lu used deep neural networks to recognize three emotions and reached a compelling accuracy  [13] . In their work, they only applied a two-layer deep belief network. The achievements in the image, speech, and natural language processing fields indicate that there is considerable room for further studying the problem of EEG-based emotion recognition by leveraging the ability of deeper neural networks. However, compared with shallow layer models, deep-layer models use more parameters and require a large number of labeled training data to explore the potentials of deep neural networks. Consequently, the primary issue that should be addressed in EEG-based emotion recognition is to acquire sufficient and high-quality training data.\n\nGenerating artificial data by applying a transformation from the original data is one of the conventional solutions to solving the data scarcity problem. This approach is called data augmentation. Recently, various data augmentation methods have been applied to generate EEG data  [20] ,  [21] ,  [22] . Some researchers have generated EEG data by applying a geometric transformation to the original data and reported the performance of classifiers improved by adding the generated data. Other researchers have focused on using deep generative models to generate artificial EEG data  [23] . Compared with signal-level transformation through geometric transformation, the deep generative model learns the representation of the real distribution at a deeper level, which leads to better classification results. However, the performance of the classifier after data augmentation were not demonstrated  [23] . In our previous study, we generated realistic-like EEG features by taking advantage of GANs  [24] . Then, we compared the performance of affective models without and with appending the generated data to the training dataset. The experimental results demonstrated that the GAN-based data augmentation method could improve the performance of affective models. In this paper, we further explore the generative methods based on the above achievement.\n\nIt is difficult to collect completely pure EEG signals due to the low signal-to-noise ratio (SNR). In addition, it is common for classifiers to handle the high-level features of EEG data in EEG-based emotion recognition tasks. Therefore, this work focuses on generating power spectral density (PSD) and differential entropy (DE) features, which are two commonly used features in emotion recognition tasks  [13] ,  [25] ,  [26] .\n\nThe work includes two emerging deep generative models: variational autoencoder (VAE)  [27]  and Wasserstein generative adversarial network (WGAN) with gradient penalty  [28] ,  [29] . We propose two data augmentation strategies: full usage of generated data and partial usage of generated data. As illustrated in Fig.  1 , the basic ideas behind the full usage strategy and partial usage strategy are to use all of the generated data and select part of the generated data. Since we cannot guarantee that all of the generated data have high qualities, it is important for us to decide how to use the generated data.\n\nFor the full usage strategy, we propose conditional Wasserstein GAN (cWGAN) to control the category of the generated data. Then, we append all of the generated data to the original training dataset without considering their quality.\n\nFor the partial usage strategy, we propose two methods called selective VAE (sVAE) and selective WGAN (sWGAN) to generate data. In these two methods, the generated data are unlabeled. By applying SVMs as classifiers, we choose the generated data with high classification confidence and append the selected data to the original training dataset. Unlike images, the generated EEG features are high-dimensional data and are intractable for humans to judge the quality of the generated data. Therefore, these two methods are based on a simple idea, and the generated data are regarded as high quality when they are classified with high classification confidence by a classifier trained by the original dataset. We use two conventional pattern classifiers, SVMs and deep neural networks with shortcut layers, to train affective models on two public EEG datasets widely used for emotion recognition: SEED and DEAP.\n\nFor a comparison study, we introduce three conventional data augmentation methods for EEG-based emotion recognition: conditional VAE (cVAE)  [27] , which adopts a similar generated strategy as cWGAN, and Gaussian noise method (Gau), which augments the datasets by adding Gussain noise to the original data  [22] , and rotational data augmentation method (RDA), which generates new data by applying a geometric rotation to the original data  [20] . We perform a systematic experimental study to compare the proposed methods with these conventional methods. We use 5-fold cross-validation to measure the classification performance of different augmented methods. The proposed framework is illustrated in Fig.  2 .\n\nThe main contributions of this paper lie in the following aspects:\n\n1) To the best of our knowledge, we adopt deep generative methods to augment EEG training data for emotion recognition for the first time. 2) We propose three methods for generating EEG data based on different generative methods and two different strategies for using the generated data. 3) We carry out a systematic comparison between different features, different generative methods and different classifiers on two EEG datasets. And the experimental results demonstrate that our proposed methods could make the affective models have better performance on EEG-based emotion recognition. The rest of the paper is organized as follows. Section 2 provides an overview of related work on generative methods, data augmentation methods for EEG-based emotion recognition, and a brief introduction to deep neural networks. In section 3, we introduce different methods in detail. Section 4 describes the two datasets, SEED and DEAP, and presents the details of our experimental settings. A systematic comparison between different methods and the efficiency of our proposed methods by conducting a series of data augmentation experiments is presented in section 5. Finally, in section 6, we present conclusions about our work.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "In this section, we briefly introduce relevant work on EEGbased emotion recognition, deep generative methods, data augmentation methods, and deep neural networks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Eeg-Based Emotion Recognition",
      "text": "EEG-based emotion recognition has received considerable attention. Mühl et al. introduced affective factors into traditional brain-computer interfaces (BCIs)  [30]  and presented the definition of affective brain-computer interfaces (aBCIs)  [31] . They summarized neurophysiology-based affect detection methods in recent years and discussed the limitations and challenges in this emerging field. Alarcao and Fonseca surveyed different EEG-based emotion recognition methods and compared the main aspects involved in the recognition process, including stimuli, feature extraction methods, and classifiers  [32] . Jenke et al. reviewed feature extraction and selection methods for 33 EEG-based emotion recognition studies  [33] . Petrantonakis and Hadjileontiadis presented a novel EEG-based feature extraction technique by employing higher-order crossings analysis  [34] .\n\nResearchers used different emotion stimuli in their studies, such as music  [35] ,  [36] , image  [37] ,  [38]  and movie clip  [13] ,  [14] . Among these stimuli, the movie clip is believed to be one of the most efficient methods for eliciting human emotion. Koelstra et al. developed a publicly available EEGbased emotion dataset called DEAP by recruiting 32 subjects to watch 40 music videos. Zheng and Lu required 15 subjects to watch 15 selected Chinese movie clips to elicit three emotions: happy, sad, and neutral (SEED dataset)  [13] . Then, they performed a systematic comparison between various feature extraction, feature selection, feature smoothing and classification methods in a three-category EEG-based emotion recognition task and showed the stable patterns of EEG in this task  [39] . In addition, they developed a multimodal framework for emotion recognition called EmotionMeter  [40] . In their work, they designed a six-electrode placement to collect EEG in an emotion recognition task including four emotions: happy, sad, neutral and fear. By combining EEG and eye movement signals, they achieved appealing recognition results  [40] . Zhao et al. also adopted this framework and extended it to fivecategory emotion recognition: happy, sad, neutral, fear, and disgust  [41] . Their results demonstrated the effectiveness of EEG signals for emotion recognition tasks.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. Deep Generative Methods",
      "text": "Generative models aim to learn the data distribution of a given dataset using unsupervised learning to generate new data with some variations and have been widely studied in the field of machine learning. Recent advances in parameterizing these models using deep neural networks have allowed them to scale to diverse data, including images, text, and speech. Two of the most promising and efficient deep generative models are the variational autoencoder (VAE)  [27]  and generative adversarial network (GAN)  [42] .\n\nAs a variation in the autoencoder, VAE aims at generating new data only based on the given data  [27] . It solves the variational inference problem that maximizes the marginalized data likelihood by using a generative network (encoder) and a recognition network (decoder). At the end of the training, the encoder can generate realistic-like data. VAEs have shown great potential in generating different data  [43] ,  [44] ,  [45] . Considering the generative ability of VAEs, we choose the vanilla VAE as one of our generative models.\n\nAs an emerging topic, GAN has attracted growing interest since it was first proposed by Goodfellow et al.  [42] . The idea of GAN is to sample noise from distributions such as Gaussian and transform them into real data distributions. GANs are based on a mini-max game theory that aims to find the Nash equilibrium between the two components, generator and discriminator. The generator learns the data distribution, while the discriminator estimates the probability that a sample comes from the real data distribution or the noise distribution. During the game, the generator tries to fool the discriminator by generating realistic-like data, while the discriminator tries not to be fooled by improving the discriminating capability. After the adversarial process, the generator can produce highquality faked data.\n\nGANs have witnessed great success in recent years. Considering that the original GAN has no control over modes of the generated data, Mirza and Osindero added the label as an additional parameter to the generator and the discriminator to control the category of the generated data  [46] . A similar idea was also adopted in InfoGAN by introducing latent codes  [47] . GANs also show promise in generating realistic-like data in specific fields.  Ledig et al.  proposed SRGAN for image superresolution  [48] . By leveraging the ability of GANs, they could create high-resolution images from a single lowresolution image. Wu et al. proposed 3D-GAN to generate 3D objects from a probabilistic space using volumetric convolutional networks and generative adversarial networks  [49] . They focused on video generation by taking advantage of GANs and achieved considerable results. In addition, GANs have also been applied to the generation of dialogue  [50] , electronic health records (EHRs)  [51] , and polyphonic music  [52] .\n\nAlthough GANs have demonstrated great generation abilities, they have some problems, such as nonconvergence, mode collapse, and diminished gradient. Chief among them is training stability (nonconvergence), which is mainly caused by the adversarial game. Some pioneering works focus on fixing this problem. Radford et al. reported some network architecture recommendations about GANs and designed a sophisticated network called DCGAN  [53] . Their work made a great contribution to solving the instability problem of GANs' training process. However, DCGAN was designed for image generation and requires specific design techniques, which limited it to scale to other fields. For this reason, other researchers have focused on altering the structure  [54]  or the loss function  [55]  of the original GAN to ensure training stability. Wasserstein GAN (WGAN) is one of the most dramatic attempts to handle this problem  [28] . Arjovsky et al. regarded the mini-max game as minimizing the Wasserstein distance between the two distributions and replaced the original loss function of GAN by the Wasserstein distance. Their work significantly improved the stability of GAN training while maintaining the generation ability of GANs. In addition, WGAN requires no extra sophisticated network designation and can be easily applied to the generation of different signals, such as EEG. Based on WGAN, Gulrajani et al. proposed using a gradient penalty in the training, which improved the performance of WGAN  [29] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Data Augmentation",
      "text": "Data augmentation aims at generating new data of the given dataset by applying transformations to the original data while preserving the label  [56] . This method is commonly applied to reduce overfitting and improve classification performance  [57]  since the generated data have a similar data distribution to the original data and can be used to increase the quantity of training data. In the field of image classification with small data size, this technique has been successfully adopted  [58] . It is common to generate additional images by applying different distortions, scaling, or moving window/pixel shifts to the real images  [59] . A similar technique has also been adopted to generate EEG signals. Krell and Su proposed rotational distortions that were similar to affine/rotational distortions of images to generate artificial EEG signals  [20] . Lotte generated artificial EEG trials by the relevant combinations and distortions of the original trials  [21] . Wang et al. generated EEG features by directly adding different Gaussian noises to the original feature and applied deep neural networks to verify the effect  [22] . All of the abovementioned methods reported that the performance of the classifiers was improved by data augmentation.\n\nSome pioneering works have focused on augmenting data by GANs, which demonstrated great generative ability. Zheng et al. adopted DCGAN to generate images and used artificial images for person reidentification tasks  [60] . Their results presented the feasibility of GAN-based data augmentation. They also reported that the classifier was less prone to overfitting by adding generated training samples. By applying a CycleGAN to augment the training dataset, Zhu et al. improved the classification accuracy of the emotion recognition task based on images  [61] . For EEG signal generation, Hartmann et al. proposed EEG-GAN to generate raw EEG signals  [23] . In their work, they presented a series of evaluation metrics to demonstrate the potential for GANs to generate EEG data. However, they did not report the performance of the classifier when adding the generated EEG data to the training dataset. In our previous work, we extended the GAN-based augmentation method to EEG-based emotion recognition  [24] . The experimental results demonstrated the efficiency of our data augmentation method for EEG-based emotion recognition.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Deep Neural Networks",
      "text": "Deep neural networks have shown great success in recent decades. In 2006, Hinton et al. proposed an effective method that enabled deep autoencoder networks to learn low-dimensional codes and solved the problem of gradient disappearance  [62] . In the field of computer vision, ImageNet, published in 2009, enables the development and application of large-scale data-driven machine learning methods. Krizhevsky et al. trained a large, deep convolutional neural network called AlexNet using ReLU and a regularization method called 'dropout', and acquired inspiring results. Based on the work of Krizhevsky, VGG applied smaller filters and explored the impact of the depth of convolutional neural networks (CNNs) on image recognition accuracy  [63] . Except for the field of computer vision, deep neural networks have also been widely applied to the fields of natural language processing  [64]  and speech recognition  [65] .\n\nAlthough deep neural networks obtain exciting results in many tasks, they still suffer from problems such as the curse of depth. It is difficult to train a neural network effectively with too many layers. To solve this problem, He et al. proposed a residual learning framework called Resnet  [66] , which had shortcuts between layers to transform the information. Inspired by this, we apply the deep neural network (DNN) with shortcut layers as one of our classifiers.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iii. Method",
      "text": "In this section, we first give a brief introduction to VAE and WGAN. Then, we present our three deep generative models, cWGAN, sVAE, and sWGAN. Next, we describe three conventional data augmentation methods, cVAE, Gau, and RDA. Finally, we briefly describe DNN with shortcut layers.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Vae",
      "text": "The VAE is a latent variable generative model that consists of an encoder and a decoder. This model combines variational inference with the conventional autoencoder framework. The encoder encodes x into a latent representation space z, where x represents a real datapoint and has weights and biases λ. We denote the encoder q λ (z|x). The decoder outputs the probability distribution of real data given the latent representation z. It has weights and biases φ, which is denoted by p φ (x|z).\n\nThe generative model aims to maximize the probability of each x in the training set according to\n\nHowever, this integral requires exponential time to compute. In practice, p(x|z) will be nearly zero for most z, which contributes almost nothing to estimate p(x). The VAE attempts to sample z, which is likely to produce x, by approximating the posterior p(z|x) with q λ (z|x). It uses the Kullback-Leibler (KL) divergence, which measures the distance between two distributions:\n\nThe goal of KL divergence is to find the parameter λ to minimize this divergence. However, it is still impossible to compute the KL divergence directly since p(x) appears in the formula, which is intractable as mentioned above. We can define the following function:\n\nwhere ELBO represents the evidence lower bound. Combining Eq. (  3 ) with the KL divergence and rewrite, p(x) can be rewritten as\n\nNote that the KL divergence is always greater than or equal to zero according to Jensen's inequality. Therefore, minimizing the KL divergence is equivalent to maximizing ELBO. Now, we can decompose the ELBO into a sum where each term depends on a single datapoint since no datapoint shares its latent z with another datapoint in VAE. We can write the ELBO i for a single datapoint i (the i th datapoint) as\n\nwhere the first term is the expected log-likelihood and the second term is the negative KL divergence between the encoder distributions q λ (z|x i ) and p(z). The first term forces the decoder to learn to reconstruct the data from latent representation, and poor reconstruction results in a large cost in this loss function. The second term can also be viewed as a regularizer, which measures how much information is lost when using q λ (z|x i ) to represent p(z). The encoder receives a penalty if it outputs latent representations z that are different from those from p(z). This term maintains the diversity of the latent representation.\n\nIn VAE, the choice of p(x|z) is often a Gaussian distribution. Then, the first term of ELBO i can also be viewed as the reconstruction loss. The VAE assumes p(z) = N (0, I) and q λ (z|x i ) = N (µ(x i ), Σ(x i )), where N represents a Gaussian distribution. Therefore, the second term of ELBO i can be formalized as:\n\nwhere k is the dimension of the Gaussian distribution and tr(x i ) is the trace function. We define Σ(x i ) as a diagonal matrix, so the formula can be rewritten as\n\nμ Σ",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Enc Dec",
      "text": "Fig.  4 : The network of cVAE. Here, x r , y r , x g , µ, Σ, z, Enc, and Dec represent one real sample, real label, generated sample, mean value, standard deviation, resampled noise, encoder, and decoder, respectively.\n\nIn practice, we use logΣ(x i ) instead of Σ(x i ) since it is more numerically stable to take the exponent. Hence, the final goal of VAE is\n\nwhere xi is the reconstructed data, and µ(x i ) and logΣ(x i ) are both calculated by the neural network.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Wgan",
      "text": "A typical GAN consists of two competing parts, which are both parameterized as deep neural networks. A generator G produces synthetic data given a noise variable input, while a discriminator D attempts to identify whether a sample comes from the real data distribution X r or the generated data distribution X g . In other words, the discriminator is trained to estimate the probability of a given sample from the real data distribution. The generator is optimized to trick the discriminator to offer a high probability for the generated data. The two parts are optimized simultaneously to find a Nash equilibrium. More formally, the procedure can be expressed as a mini-max function:\n\nwhere θ g and θ d represent the parameters of the generator and discriminator, respectively, and Z can be a uniform noise distribution or a Gaussian noise distribution. The function is optimized in two steps: (i) Maximize it by fixing G and X g , and obtain the optimum of D; (ii) Minimize the function by the optional D, and then minimize the Jensen-Shannon divergence between X r and X g . The game achieves equilibrium if and only if X r = X g .\n\nAlthough GAN has shown great success in realistic data generation, it suffers from some major problems, such as nonconvergence, mode collapse and diminished gradient. Researchers believed that the Jensen-Shannon divergence could lead to vanishing gradients, which was the main reason for the GAN's instability. In real-world tasks such as image generation, the distribution of real images always lies in low-dimensional manifolds, and the distribution of generated images also rests in low-dimensional manifolds. The two distributions are almost certainly disjoint and have no overlaps. In this situation, the Jensen-Shannon divergence between the two distributions is a fixed number, which cannot provide useful gradients for GAN training.\n\nArjovsky et al.  [28]  adopted the Wasserstein distance, which is also called the earth mover's distance (EM distance), in GAN training to solve the instability problem. The distance formula for the continuous probability domain is\n\nwhere Π(X r , X g ) is the set of all possible joint probability distributions between X r and X g . For the Wasserstein distance, even if the two distributions have no overlaps, it can still provide useful and smooth gradients for GAN training. However, it is difficult to implement the infimum of Eq. (  2 ). An alternative method for calculating the Wasserstein distance in reality is to apply its Kantorovich-Rubinstein duality:\n\nwhere f denotes the set of 1-Lipschitz functions and K is a constant number. In realistic implementations, f is replaced by discriminator D and ||f || L ≤ K is replaced by ||D|| L ≤ 1.\n\nThere are many methods for realizing the 1-Lipschitz constraint in WGAN. One possible method is to restrict the parameters of the discriminator in a limited range, such as -0.1 to 0.1. However, this weight-clipping method will introduce some problems. The model may produce poor quality data and does not converge since clipping reduces the capacity of the model. Another method is to use gradient penalty  [29] . In this method, an extra penalty term is added to the loss function:\n\nwhere λ is a hyperparameter controlling the trade-off between the original objective and gradient penalty, and x denotes the data points sampled from the straight line between the real distribution X r and the generated distribution X g :",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "C. Cwgan",
      "text": "In this paper, we propose the cWGAN and apply it to EEG-based emotion recognition. As shown in Fig.  3 , we can generate data with specified categories by using cWGAN. This method is based on the gradient penalty version of WGAN. The cWGAN is formulated as max\n\nwhere Y r represents the category distribution of the real data, and x is defined in Eq.  (13) . In this work, λ is set to 10 because we find that this value could make the training procedure more stable in our preliminary experiment. The last term in Eq. (  14 ) penalizes the model if the gradient norm moves away from its target norm. The Lipschitz constraint is realized, and the model almost loses no capacity. Thus, cWGAN can generate data with high quality and converge quickly. Since the discriminator loss (D-loss) is the The Wasserstein distance between the two conditional distributions can represent the training procedure for cWGAN.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "D. Svae And Swgan",
      "text": "In cWGAN, we append all of the generated data to the training dataset. Here, we consider another strategy in which partially generated data are adopted to enlarge the training dataset in sVAE and sWGAN. These two methods are based on the observation that the generated data have different qualities, and only generated data with high quality are selected as new training data. This procedure has two steps: a) we generate some data by VAE or WGAN; b) we choose the generated data with high quality to enlarge the dataset. We repeat the above two steps until we obtain enough training data.\n\nIn this work, we use the classification confidence to examine data quality. We first train a classifier with the original training dataset and then use the trained classifier to classify the generated data, and only data with high classification confidence (higher than the thre hold) are appended to the training dataset. We train a new classifier with the appended dataset and repeat the two steps mentioned above until we have enough generated data. We present the algorithm in Algorithm III-C.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "E. Cvae",
      "text": "In this method, we aim to generate data with the specified category. As shown in Fig.  4 , to control the generated category, an extra label is added to the encoder and decoder. We first feed the training data point and the corresponding label to the encoder, then we concatenate the hidden representation with the corresponding label and feed it to the decoder to train the network. Then, we can generate data with the specified label by feeding the decoder with the noise sampled from the Gaussian distribution and the assigned label. Therefore, the cVAE  [27]  can be formulated as",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "F. Gaussian Noise",
      "text": "One of the straightforward augmentation methods is adding Gaussian noise (Gau) to the original training data, whose probability density function obeys a Gaussian distribution:\n\nwhere z is a random variable, µ means expectation and σ is the standard deviation. In our experiment, µ is set to 0 and σ is set to 0.001. Intuitively, more training data can be generated while preserving the characteristics of the original data by adding Gaussian noise.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "G. Rotational Data Augmentation",
      "text": "Rotational data augmentation (RDA) was proposed by Krell and Su  [20] , which aims to create data with strong spatial robustness, since there might be spatial shifts of EEG caps within sessions and between sessions during the experiments. To address this problem, RDA generates artificial data associated with the electrodes' new positions by adding rotations on three coordinates. According to their result  [20] , augmentation around the y-axis and z-axis increased the performance, especially around the z-axis. Therefore, we choose to perform the rotations around the z-axis. Specifically, we set an angle between 12 • and 24 • over all subjects and calculate the new data by interpolation based on radial basis functions.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "H. Classifier",
      "text": "In this paper, we implement two kinds of classifiers: SVMs and deep neural networks.\n\nThe deep neural network is a neural network with multiple hidden layers. Here, we randomly add some residual functions between two layers. The idea of the residual function is borrowed from Resnet  [66] . The residual function is a way to avoid the problem of vanishing gradients, and it does this by using shortcuts to jump over some layers. Because the numbers of different nodes are different, the dimensions of input and output are different. Therefore, we can use a linear projection to match the dimensions. This function can be expressed as follows:\n\nwhere x and y are the input and output vectors, respectively, F (x) = W 2 σ(W 1 x), which means two fully connected layers and a ReLU function, σ, between them, and W s is the linear projection to change the dimension. The output should go through another ReLU function before it is passed to the next layer.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Iv. Experimental Settings",
      "text": "In this section, we describe the details about the two datasets, data preprocessing, performance evaluation, and hyperparameters.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Algorithm 1 The Work Flow Of Svae And Swgan",
      "text": "Input: Real dataset X r = {x i r } m i=1 and corresponding labels Y r = {y i r } m i=1 and thre hold Output: Generated data X g = {x i g } n i=1 and corresponding labels Y g = {y i g } n i=1\n\n1: X g , Y g = N ull, N ull 2: repeat 3:\n\nX all g = sVAE(X r , noise) or X all g = sWGAN(X r , noise) Y all g , class conf = classifier test(model, X all g ) 7:\n\nfor i in X all g do 8:\n\nend if 10:\n\nend for 11: until len(X g ) == n 12: return X g , Y g",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A. Dataset Description",
      "text": "The SEED dataset  [13]  contains the EEG signals of 15 participants. They were required to watch 15 well-prepared video clips that can elicit exactly one of the three kinds of emotion: positive, neutral, and negative. The criteria of film clip selection ensure that Each clip is well-edited to create coherent emotion eliciting and maximizing emotional meanings. In addition, each clip can explicitly elicit one exact kind of emotion, and the time of the clips is enough but not too long to elicit the participants' corresponding emotion sufficiently. The order of presentation is arranged so that two film clips targeting the same emotion are not shown consecutively. Each participant took part in the experiment three times with an interval of at least 7 days. The signals were sampled at a rate of 1,000 Hz with an ESI NeuroScan System from a 62-electrode headset.\n\nThe DEAP dataset  [14]  contains the EEG and peripheral physiological signals of 32 participants as they watched 40 one-minute-long excerpts of music videos. The music videos were selected from 120 one-minute extracts of music videos rated from an online self-assessment by 14-16 volunteers based on valence, dominance, arousal, like, and familiarity. Valence, arousal, dominance and like were rated directly after each trial on a continuous 9-point scale using a standard mouse self-assessment. The signals were sampled at 512 Hz with 48 channels. (32 EEG channels, 12 peripheral physiological channels including galvanic skin response and temperature, 3 unused channels and 1 status channel). The signals from EEG channels are sampled according to an international 10-20 system.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "B. Data Preprocessing",
      "text": "Previous works have shown that the DE feature of EEG signals is efficient for EEG-based emotion recognition  [13] ,  [39] ,  [26] . Therefore, we generate DE features to augment the datasets. We also generate the PSD feature, which is a conventional feature for EEG-based emotion recognition, to verify our method. Since both the SEED and DEAP datasets have been preprocessed, we use the short-time Fourier transform (STFT) with a 1-s-long nonoverlapping overleaping Hanning window to extract the PSD feature of the EEG signal from the two datasets directly. For the Gaussian distribution, the DE feature is defined as\n\nwhere X represents the Gaussian distribution N (µ, σ 2 ), and π and e are constants. Shi et al.  [67]  demonstrated that the value of DE is equal to the logarithmic spectral energy for a fixed-length EEG sequence in a certain band. According to their result, we extracted the DE feature from the preprocessed EEG signal of the two datasets.\n\nConsidering the dynamic characteristics of EEG-based emotion recognition tasks, we employ the linear dynamic system approach to filter the PSD and DE features, which has also been adopted in previous works  [13] ,  [39] .\n\nPSD and DE features are extracted from five frequency bands: δ: 1-3 Hz, θ: 4-7 Hz, α: 8-13 Hz, β: 14-30 Hz, and γ: 31-50 Hz for the SEED dataset  [13] . Therefore, both of these features have 310 dimensions (62 channels × 5 frequency bands). For each experiment, there were 3,394 labeled samples. In this work, we viewed the SEED dataset as a three-category classification problem.\n\nWe also extracted PSD and DE features for the DEAP dataset. Since the δ band was filtered in this dataset, we only computed the two features of four frequency bands: θ, α, β, and γ. In this time, both features had 128 dimensions (32 channels × 4 frequency bands). Each experiment had 2,400 labeled samples. Here, we adopted a four-category emotion model using valence and arousal values: high valence (level > 5) and high arousal (level > 5), high valence (level > 5) and low arousal (level ≤ 5), low valence (level ≤ 5) and high arousal (level > 5), and low valence (level > 5) and high arousal (level > 5).",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "C. Evaluation Details",
      "text": "We adopted 5-fold cross-validation for each experiment on the two datasets. For each experiment, we trained 5 recognition models and computed the average recognition accuracy of the five models as the recognition accuracy of the experiment. Each model had the same hyperparameters. We regarded the average accuracy of all experiments as the final accuracy. For the SEED dataset, there were 45 experiments and nearly 678 samples for each fold. For the DEAP dataset, there were 32 experiments and 480 samples for each fold.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "D. Hyperparameter Details",
      "text": "In the SVM classifier, we used the linear kernel. The parameter c was searched from 2 -10 to 2 10 to find the optimal value.\n\nWe performed a random search on the learning rate, number of network layers and size of batches of classifier of deep neural network. The learning rate was randomly selected from 0.0005, 0.0001, 0.00005 and 0.00001 with the Adam optimizer. The number of layers was searched from 4 to 8. The size of batches was randomly selected from 128, 256 and 512. Both networks with residual functions and without residual functions were searched. For the network with residual functions, the residual functions were applied every two layers. The input dimension was determined by the corresponding input feature, and the dimension of the output label was 3 for the SEED dataset and 4 for the DEAP dataset. The number of hidden nodes for each layer was randomly searched. The ReLU activation function was used for all hidden layers. We normalize PSD and DE features before feeding them to the networks.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "V. Experiments And Results",
      "text": "In this section, we first perform a systematic experimental study to evaluate the effectiveness and generalization ability of our methods. We augmented different EEG-based emotion datasets by different features generated by our methods. We apply different classifiers to evaluate the performances of these generative methods. We also compare our proposed methods with conventional generative methods. Then, we visualized the generated data to show why our proposed methods work. Finally, we discuss the proposed methods.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "A. Different Number Of Appended Training Data",
      "text": "We first conducted data augmentation experiments on the SEED dataset and use the SVM as the classifier. Each experiment had 3,394 samples. We generated 0, 200, 500, 1000, 3,000, 5,000, 10,000, 15,000 and 20,000 artificial samples of the two features and added them to the original training datasets. Here, '0' indicates that we only use the original training dataset without data augmentation. We did not generate more samples because we found that most of the experiments reach their peaks before 20,000 samples were appended. The remaining experiments reached their peaks when 20,000 samples were appended. And the p-values between sWGAN method and the conventional methods are all less than 0.01.\n\nWe compared the performances of different data augmentation methods when applying the PSD feature, as shown in table  I . The average accuracy was 60.3% when we only used the original training set. For conventional methods, cVAE reached its best mean accuracy of 63.4% when 3,000 samples were appended. Gau reached its optimal performance of 63.1% when adding 10,000 samples into the original training set. RDA had the best performance of 63.2% when 500 samples were appended. For our methods, cWGAN achieved its best mean accuracy of 65.2% when 15,000 samples were appended. When 15,000 samples were appended, sVAE reached its best mean accuracy of 63.5%. sWGAN achieved its best mean accuracy of 67.7% when 20,000 samples were appended. According to table I, our methods achieved better performance than conventional methods. sWGAN had the best performance among all the methods.\n\nTable  II  illustrates the results of the data augmentation methods for the DE feature. For SVM, the baseline was 84.3%. For conventional data augmentation methods, cVAE reached its best result of 85.2% when 1,000 sampled data points were appended. The best accuracy for Gau was 85.1% with 3,000 augmented data points. The best mean accuracy of RDA was 85.6% when 5,000 samples were appended. For our methods, cWGAN reached its best mean accuracy of 87.4% when appending 15,000 samples. For the two selective augmentation methods, sVAE achieved the best mean accuracy of 87.8% when 1,000 samples were appended, and sWGAN achieved the best mean accuracy of 90.8% when 10,000 samples were appended.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "B. Classification With Deep Neural Networks",
      "text": "To increase the reliability of the performance comparison of different data augmentation approaches, we also implemented deep neural networks with shortcut layers to build the affective models. Considering that the DE feature is better for the PSD feature in emotion recognition tasks and that the PSD feature had similar improvements in terms of the mean accuracy with the DE feature, we only augmented the training data with the DE features when using the DNN as the classifier. The baseline was 83.3%. For conventional methods, cVAE, Gau, and RDA reached the best mean accuracy of 86.5% (3000 samples), 86.2% (10000 samples), and 85.7% (200 samples), respectively. For our proposed methods, the best mean accuracy of cWGAN is 91.6% when we added 3,000 samples. The two selective methods obtained the best mean accuracy of 87.5% and 93.5% when we added 1,000 samples, respectively. The results in table II demonstrate that our methods had better performance than conventional methods. The sWGAN achieved the best performance for both classifiers.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "C. Generated Data With Two Different Features",
      "text": "For the DEAP dataset, we also used different data augmentation methods to augment PSD and DE features. Each experiment had 2,400 samples. We generated the same number of samples as mentioned above.\n\nTable  III  shows the mean accuracies and standard deviations of PSD data augmentation. The mean accuracy of the 4category emotion recognition model was 42.7% when we only used the original training data. For conventional methods, cVAE reached the best mean accuracy of 44.9% when 3,000   samples were appended. The best performance for Gau was 44.5% when 5,000 samples were appended. RDA reached the best mean accuracy of 45.2% when 20,000 samples were appended. For our methods, cWGAN obtained the best mean accuracy of 45.0% when we added 5,000 generated samples to the original training dataset. sVAE had the best mean accuracy of 46.1% when 15,000 samples were generated. sWGAN achieved its best mean accuracy of 47.6% when 20,000 samples were appended. Our methods also showed better performance, and sWGAN had the best performance in terms of accuracy.\n\nTable  IV  presents the results of DE data augmentation. For SVM, the baseline was 45.4%. For conventional methods, cVAE had the best mean accuracy of 48.1% when 10,000 samples were appended. The best accuracy for Gau was 46.1% when we appended 1,000 samples. RDA obtained the best mean accuracy of 46.3% when the number of appended samples was 3,000. For our methods, cWGAN obtained the best mean accuracy of 48.9% when 5,000 samples were appended. sVAE reached the best mean accuracy of 48.4% when 5,000 samples were appended, and sWGAN obtained the best mean accuracy of 50.8% when 15,000 samples were appended.\n\nFor DNN, the classification accuracy was 44.9% when no data augmentation method was applied. For conventional methods, cVAE, Gau, and RDA reached the best mean ac- We also observed that our methods showed better performance than conventional methods. sWGAN had the best mean accuracy when applying SVM as the classifier, while sVAE had the best performance when applying DNN as the classifier.\n\nAs we can see from the above results, the DE feature had better mean accuracies than the PSD feature on both datasets, and the standard deviations were smaller. These results were consistent with previous studies  [13] ,  [39] . In addition, compared with conventional methods, our methods were more efficient for improving the performance of emotion recognition models. For the SEED dataset, the mean accuracy improved 10.2% with DE features when we used sWGAN as the data augmentation method and adopted DNN as the classifier. The DEAP dataset had the highest improvement of 5.4% in terms of mean accuracy when sWGAN was adopted as the data augmentation method and SVM was used as the classifier. Moreover, the data augmentation methods were more efficient for DNN in most cases.\n\nIn addition, we observe that all the data augmentation methods (both ours and conventional methods) reached their peaks, and then their performance decayed when we gradually increased the number of appended samples. However, for our methods, most of the experiments still showed better performance compared with their baselines when appending fewer than 20,000 generated samples. Although experiments with different datasets, features, classifiers, and methods reached different peaks, our results show that the peaks appeared before the training datasets were enlarged 10 times.\n\nAs shown in Fig.  5 , we plotted the mean accuracies and standard deviations of different methods on different classifiers and datasets. We only shown the results of DE feature because DE feature had better performance than PSD feature and the two features had the similar tendency in terms of mean accuracy. And the results of Gau and RDA were averaged.\n\nCompared with the conventional methods, our methods had better mean accuracies in the most experiments. Besides, GAN-based methods (cWGAN and sWGAN) shown better performance than VAE-based (cVAE and sVAE) methods in most cases. Moreover, the selective methods (sVAE and sWGAN) were better than conditional methods (cVAE and cWGAN) in most of the experiments. Specially, sWGAN always had better mean accuracies than cWGAN.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "D. Visualization Of The Generated Data",
      "text": "We visualize the generated data with two methods, twodimensional circular view of the scalp and two-dimensional visualization using t-SNE, to show why our proposed methods work. We selected cWGAN as the generated method and the SEED dataset (DE feature) to represent our results since sVAE and sWGAN have similar vision performance. Fig.  6  depicts the two-dimensional circular view of the scalp. The generated data have a similar data distribution as the real data. For positive emotion, the lateral areas of both real and generated data are more activated in beta and gamma bands than the other two emotions. For neutral emotion, both the real and generated data had high alpha responses. For negative emotion, high gamma responses at prefrontal sites appeared in real and generated data. These phenomena indicated that our methods can capture the information of the real data distribution. Therefore, the generated samples can be appended to the training set to enhance the performance of the affective models.\n\nAs shown in Fig.  7 , we plotted the distributions of real and generated DE features (generated by cWGAN) by t-SNE  [68] . Data from each emotion was clustered in the latent space, and the generated data were close to the corresponding real data, which implies that the generated data carry enough realistic information. This phenomenon also indicates that the data generated by our methods can be used to augment the training dataset.\n\nIn addition, the distribution of real data was sparse, and the boundaries of different categories in the data manifold were not obvious. The generated data supplemented the training data manifold, which led to better margins for the classifier. Therefore, we can improve the classification performance by training the classifier with the generated data. We can also explain this phenomenon from another point of view. The generated data have a similar data distribution to the real data, but they are not the real data. Therefore, the generated data not only carry realistic information but also have diverse information. The classifier trained by the augmented data was more robust. This phenomenon is also consistent with the aforementioned classification results. However, the possibility of generating bad quality samples increased when we added the generating number. This phenomenon occurs no matter what generative methods we apply. For example, we wanted to generate a sample of positive emotions in the SEED dataset, but we might obtain a sample that is more similar to a negative sample by the generative model. We called this sample a bad quality sample. In Fig.  7 , we can find some bad quality samples. For example, some generated neutral samples (red points) were more close to the real positive samples (blue lines). In this case, the bad quality sample misled the classifier, and the classification accuracy decreased. We can also find a similar phenomenon in the above tables: the accuracies decayed when too many generated data were appended.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "E. Discussions On Different Affective Models",
      "text": "The abovementioned results show that the performance of the emotion recognition models can be improved by using our proposed data augmentation methods. We achieved performance improvements in different datasets, features, and classifiers, which demonstrates the generalization ability and effectiveness of our methods. Although all three proposed methods improved the performance of EEG-based emotion recognition tasks, they had some differences in terms of stability, accuracy and time usage.  For stability, sVAE had better performance than cWGAN and sWGAN. Although the WGAN had good convergence performance and was more stable than the original GAN, it may collapse because of adversarial training. However, VAE is more stable.\n\nFor accuracy, sWGAN had better classification performances than sVAE most of the time. This phenomenon indicates that GAN can capture more latent information than VAE. Therefore, the data generated by GAN are more useful for building the recognition model than those generated by VAE. In addition, sWGAN always performed better than cWGAN on both datasets, which indicates that the selective methods are more efficient at improving emotion recognition models.\n\nFor time usage, cWGAN had a quicker convergence speed than sWGAN and sVAE. cWGAN uses all of the generated data without considering their quality, while sWGAN and sVAE need to select the generated data and use the highquality data to augment the training set. Therefore, the two methods require more computation time to determine the quality of the generated data.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Vi. Conclusions",
      "text": "In this paper, we proposed three deep generative methods for enhancing EEG-based emotion recognition by generating training data. We generated realistic-like PSD and DE features of EEG data with our proposed methods: cWAGN, sVAE, and sWGAN. We augmented the original training dataset using the generated data to improve the accuracy of EEGbased emotion recognition models. The experimental results on two emotion datasets demonstrate the effectiveness of our methods. The emotion recognition models trained on the augmented training datasets achieved 10.2% and 5.4% improvements on the SEED dataset and the DEAP dataset, respectively. By visualizing the generated data, we explained the reason for the accuracy improvements. We also studied the performance of the classifiers when adding different numbers of generated data to the original training set. We observed that the classification accuracy decayed when too many generated data were appended. Our experimental results indicate that the number of generated data should be less than 10 times of the original training dataset, and then the affective models achieved the best performance. In addition, we carried out a systematic comparison between the proposed methods. We find that sWGAN had the best performance in terms of accuracy, while it cost more time than cWGAN.",
      "page_start": 13,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Illustration of two strategies of data augmentation used",
      "page": 2
    },
    {
      "caption": "Figure 2: The main contributions of this paper lie in the following",
      "page": 2
    },
    {
      "caption": "Figure 2: Illustration of our proposed data augmentation framework. First, we extract the DE feature and PSD feature from",
      "page": 3
    },
    {
      "caption": "Figure 3: The network of cWGAN. Here, xr, yr, xg, z, G, and D",
      "page": 5
    },
    {
      "caption": "Figure 4: The network of cVAE. Here, xr, yr, xg, µ, Σ, z, Enc,",
      "page": 6
    },
    {
      "caption": "Figure 4: , to control the generated category,",
      "page": 7
    },
    {
      "caption": "Figure 5: , we plotted the mean accuracies and",
      "page": 11
    },
    {
      "caption": "Figure 6: depicts the two-dimensional circular view of the",
      "page": 11
    },
    {
      "caption": "Figure 7: , we plotted the distributions of real and",
      "page": 11
    },
    {
      "caption": "Figure 5: Mean accuracies (Acc) and standard deviations (Std) of different methods on different classiﬁers and datasets: (a) Acc",
      "page": 12
    },
    {
      "caption": "Figure 7: , we can ﬁnd some bad quality samples. For example, some",
      "page": 12
    },
    {
      "caption": "Figure 6: Topographic map of the scalp for real and generated DE features (cWGAN) in the SEED dataset.",
      "page": 13
    },
    {
      "caption": "Figure 7: Two-dimensional visualizations of the real and gener-",
      "page": 13
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion AI, explained",
      "authors": [
        "M Somers"
      ],
      "year": "2019",
      "venue": "Emotion AI, explained"
    },
    {
      "citation_id": "2",
      "title": "Hype cycle for emerging technologies",
      "authors": [
        "D Smith",
        "B Burke"
      ],
      "year": "2019",
      "venue": "Hype cycle for emerging technologies"
    },
    {
      "citation_id": "3",
      "title": "Depression and implicit emotion processing: An EEG study",
      "authors": [
        "A Bocharov",
        "G Knyazev",
        "A Savostyanov"
      ],
      "year": "2017",
      "venue": "Neurophysiologie Clinique"
    },
    {
      "citation_id": "4",
      "title": "A facial expression emotion recognition based human-robot interaction system",
      "authors": [
        "Z Liu",
        "W Min",
        "W Cao",
        "L Chen",
        "J Xu",
        "R Zhang",
        "M Zhou",
        "J Mao"
      ],
      "year": "2017",
      "venue": "IEEE/CAA Journal of Automatica Sinica"
    },
    {
      "citation_id": "5",
      "title": "Using body movement and posture for emotion detection in non-acted scenarios",
      "authors": [
        "M Garber-Barron",
        "M Si"
      ],
      "year": "2012",
      "venue": "IEEE International Conference on Fuzzy Systems"
    },
    {
      "citation_id": "6",
      "title": "Emotion recognition from expressions in face, voice, and body: the multimodal emotion recognition test (mert)",
      "authors": [
        "B Tanja",
        "G Didier",
        "K Scherer"
      ],
      "year": "2009",
      "venue": "Emotion"
    },
    {
      "citation_id": "7",
      "title": "Feature extraction for emotion recognition and modelling using neurophysiological data",
      "authors": [
        "A Samara",
        "M Menezes",
        "L Galway"
      ],
      "year": "2017",
      "venue": "International Conference on Ubiquitous Computing & Communications & International Symposium on Cyberspace & Security"
    },
    {
      "citation_id": "8",
      "title": "Emotional state classification from eeg data using machine learning approach",
      "authors": [
        "X.-W Wang",
        "D Nie",
        "B.-L Lu"
      ],
      "year": "2014",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "9",
      "title": "Prediction of subjective ratings of emotional pictures by EEG features",
      "authors": [
        "D Mcfarland",
        "M Parvaz",
        "W Sarnacki",
        "R Goldstein",
        "J Wolpaw"
      ],
      "year": "2016",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "10",
      "title": "Emotions recognition using EEG signals: A survey",
      "authors": [
        "S Alarco",
        "M Fonseca"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "Deep learning for electroencephalogram (EEG) classification tasks: a review",
      "authors": [
        "A Craik",
        "Y He",
        "J Contreras-Vidal"
      ],
      "year": "2019",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "12",
      "title": "Unsupervised learning in reservoir computing for eeg-based emotion recognition",
      "authors": [
        "R Fourati",
        "B Ammar",
        "J Sanchez-Medina",
        "A Alimi"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    },
    {
      "citation_id": "14",
      "title": "DEAP: A database for emotion analysis; using physiological signals",
      "authors": [
        "S Koelstra",
        "C Mühl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "15",
      "title": "Dreamer: A database for emotion recognition through eeg and ecg signals from wireless low-cost offthe-shelf devices",
      "authors": [
        "S Katsigiannis",
        "N Ramzan"
      ],
      "year": "2018",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "16",
      "title": "A multimodal database for affect recognition and implicit tagging",
      "authors": [
        "M Soleymani",
        "J Lichtenauer",
        "T Pun",
        "M Pantic"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "17",
      "title": "Mped: A multi-modal physiological emotion database for discrete emotion recognition",
      "authors": [
        "T Song",
        "W Zheng",
        "C Lu",
        "Y Zong",
        "X Zhang",
        "Z Cui"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "18",
      "title": "The unreasonable effectiveness of data",
      "authors": [
        "A Halevy",
        "P Norvig",
        "F Pereira"
      ],
      "year": "2009",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "19",
      "title": "A survey on deep learning for big data",
      "authors": [
        "Q Zhang",
        "L Yang",
        "Z Chen",
        "L Peng"
      ],
      "year": "2018",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "20",
      "title": "Rotational data augmentation for electroencephalographic data",
      "authors": [
        "M Krell",
        "S Kim"
      ],
      "year": "2017",
      "venue": "2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)"
    },
    {
      "citation_id": "21",
      "title": "Signal processing approaches to minimize or suppress calibration time in oscillatory activity-based brain-computer interfaces",
      "authors": [
        "F Lotte"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "22",
      "title": "Data augmentation for EEG-based emotion recognition with deep convolutional neural networks",
      "authors": [
        "F Wang",
        "S.-H Zhong",
        "J.-F Peng",
        "J.-M Jiang",
        "Y Liu"
      ],
      "year": "2018",
      "venue": "International Conference on Multimedia Modeling"
    },
    {
      "citation_id": "23",
      "title": "EEG-GAN: Generative adversarial networks for electroencephalograhic (EEG) brain signals",
      "authors": [
        "K Hartmann",
        "R Schirrmeister",
        "T Ball"
      ],
      "year": "2018",
      "venue": "EEG-GAN: Generative adversarial networks for electroencephalograhic (EEG) brain signals",
      "arxiv": "arXiv:1806.01875"
    },
    {
      "citation_id": "24",
      "title": "EEG data augmentation for emotion recognition using a conditional Wasserstein GAN",
      "authors": [
        "Y Luo",
        "B.-L Lu"
      ],
      "year": "2018",
      "venue": "2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)"
    },
    {
      "citation_id": "25",
      "title": "Differential entropy feature for EEG-based emotion classification",
      "authors": [
        "R.-N Duan",
        "J.-Y Zhu",
        "B.-L Lu"
      ],
      "year": "2013",
      "venue": "International IEEE/EMBS Conference on Neural Engineering (NER)"
    },
    {
      "citation_id": "26",
      "title": "EEG-based emotion recognition using hierarchical network with subnetwork nodes",
      "authors": [
        "Y Yang",
        "Q Wu",
        "W -L. Zheng",
        "B.-L Lu"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "27",
      "title": "Auto-encoding variational bayes",
      "authors": [
        "D Kingma",
        "M Welling"
      ],
      "year": "2014",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "28",
      "title": "Wasserstein GAN",
      "authors": [
        "M Arjovsky",
        "S Chintala",
        "L Bottou"
      ],
      "year": "2017",
      "venue": "Wasserstein GAN",
      "arxiv": "arXiv:1701.07875"
    },
    {
      "citation_id": "29",
      "title": "Improved training of Wasserstein GANs",
      "authors": [
        "I Gulrajani",
        "F Ahmed",
        "M Arjovsky",
        "V Dumoulin",
        "A Courville"
      ],
      "year": "2017",
      "venue": "Neural Information Processing Systems"
    },
    {
      "citation_id": "30",
      "title": "Context-aware brain-computer interfaces: exploring the information space of user, technical system and environment",
      "authors": [
        "T Zander",
        "S Jatzev"
      ],
      "year": "2011",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "31",
      "title": "A survey of affective brain computer interfaces: principles, state-of-the-art, and challenges",
      "authors": [
        "C Mühl",
        "B Allison",
        "A Nijholt",
        "G Chanel"
      ],
      "year": "2014",
      "venue": "Brain-Computer Interfaces"
    },
    {
      "citation_id": "32",
      "title": "Emotions recognition using EEG signals: A survey",
      "authors": [
        "S Alarcao",
        "M Fonseca"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "33",
      "title": "Feature extraction and selection for emotion recognition from EEG",
      "authors": [
        "R Jenke",
        "A Peer",
        "M Buss"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "34",
      "title": "Emotion recognition from EEG using higher order crossings",
      "authors": [
        "P Petrantonakis",
        "L Hadjileontiadis"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Information Technology in Biomedicine"
    },
    {
      "citation_id": "35",
      "title": "EEGbased emotion recognition in music listening",
      "authors": [
        "Y Lin",
        "C Wang",
        "T Jung",
        "T Wu",
        "S Jeng",
        "J Duann",
        "J Chen"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "36",
      "title": "Real-time EEG-based emotion recognition for music therapy",
      "authors": [
        "O Sourina",
        "Y Liu",
        "M Nguyen"
      ],
      "year": "2012",
      "venue": "Journal on Multimodal User Interfaces"
    },
    {
      "citation_id": "37",
      "title": "Evaluating classifiers for emotion recognition using EEG",
      "authors": [
        "A Sohaib",
        "S Qureshi",
        "J Hagelback",
        "O Hilborn",
        "P Jercic"
      ],
      "year": "2013",
      "venue": "International Conference on Augmented Cognition"
    },
    {
      "citation_id": "38",
      "title": "A novel emotion elicitation index using frontal brain asymmetry for enhanced EEG-based emotion recognition",
      "authors": [
        "P Petrantonakis",
        "L Hadjileontiadis"
      ],
      "year": "2011",
      "venue": "Engineering in Medicine and Biology Society"
    },
    {
      "citation_id": "39",
      "title": "Identifying stable patterns over time for emotion recognition from EEG",
      "authors": [
        "W.-L Zheng",
        "J.-Y Zhu",
        "B.-L Lu"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "40",
      "title": "Emotionmeter: A multimodal framework for recognizing human emotions",
      "authors": [
        "W.-L Zheng",
        "W Liu",
        "Y.-F Lu",
        "B.-L Lu",
        "A Cichocki"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "41",
      "title": "Classification of five emotions from EEG and eye movement signals: complementary representation properties",
      "authors": [
        "L.-M Zhao",
        "R Li",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2019",
      "venue": "International IEEE/EMBS Conference on Neural Engineering (NER)"
    },
    {
      "citation_id": "42",
      "title": "Generative adversarial nets",
      "authors": [
        "I Goodfellow",
        "J Pouget-Abadie",
        "M Mirza",
        "B Xu",
        "D Warde-Farley",
        "S Ozair",
        "A Courville",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Neural Information Processing Systems"
    },
    {
      "citation_id": "43",
      "title": "Markov chain monte carlo and variational inference: Bridging the gap",
      "authors": [
        "T Salimans",
        "D Kingma",
        "M Welling"
      ],
      "year": "2015",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "44",
      "title": "Deep convolutional inverse graphics network",
      "authors": [
        "T Kulkarni",
        "W Whitney",
        "P Kohli",
        "J Tenenbaum"
      ],
      "year": "2015",
      "venue": "Neural Information Processing Systems"
    },
    {
      "citation_id": "45",
      "title": "Draw: A recurrent neural network for image generation",
      "authors": [
        "K Gregor",
        "I Danihelka",
        "A Graves",
        "D Rezende",
        "D Wierstra"
      ],
      "year": "2015",
      "venue": "Draw: A recurrent neural network for image generation",
      "arxiv": "arXiv:1502.04623"
    },
    {
      "citation_id": "46",
      "title": "Conditional generative adversarial nets",
      "authors": [
        "M Mirza",
        "S Osindero"
      ],
      "year": "2014",
      "venue": "Conditional generative adversarial nets",
      "arxiv": "arXiv:1411.1784"
    },
    {
      "citation_id": "47",
      "title": "InfoGAN: Interpretable representation learning by information maximizing generative adversarial nets",
      "authors": [
        "X Chen",
        "Y Duan",
        "R Houthooft",
        "J Schulman",
        "I Sutskever",
        "P Abbeel"
      ],
      "year": "2016",
      "venue": "InfoGAN: Interpretable representation learning by information maximizing generative adversarial nets",
      "arxiv": "arXiv:1606.03657v1"
    },
    {
      "citation_id": "48",
      "title": "Photo-Realistic single image super-resolution using a generative adversarial network",
      "authors": [
        "C Ledig",
        "L Theis",
        "F Huszar",
        "J Caballero",
        "A Cunningham",
        "A Acosta",
        "A Aitken",
        "A Tejani",
        "J Totz",
        "Z Wang"
      ],
      "year": "2017",
      "venue": "Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "49",
      "title": "Learning a probabilistic latent space of object shapes via 3d generativeadversarial modeling",
      "authors": [
        "J Wu",
        "C Zhang",
        "T Xue",
        "W Freeman",
        "J Tenenbaum"
      ],
      "year": "2016",
      "venue": "Neural Information Processing Systems"
    },
    {
      "citation_id": "50",
      "title": "Adversarial learning for neural dialogue generation",
      "authors": [
        "J Li",
        "W Monroe",
        "T Shi",
        "S Jean",
        "A Ritter",
        "J Dan"
      ],
      "year": "2017",
      "venue": "Adversarial learning for neural dialogue generation",
      "arxiv": "arXiv:1701.06547"
    },
    {
      "citation_id": "51",
      "title": "Generating multi-label discrete electronic health records using generative adversarial networks",
      "authors": [
        "E Choi",
        "S Biswal",
        "B Malin",
        "J Duke",
        "W Stewart",
        "J Sun"
      ],
      "year": "2017",
      "venue": "Generating multi-label discrete electronic health records using generative adversarial networks",
      "arxiv": "arXiv:1703.06490"
    },
    {
      "citation_id": "52",
      "title": "C-RNN-GAN: Continuous recurrent neural networks with adversarial training",
      "authors": [
        "O Mogren"
      ],
      "year": "2016",
      "venue": "C-RNN-GAN: Continuous recurrent neural networks with adversarial training",
      "arxiv": "arXiv:1611.09904"
    },
    {
      "citation_id": "53",
      "title": "Unsupervised representation learning with deep convolutional generative adversarial networks",
      "authors": [
        "A Radford",
        "L Metz",
        "S Chintala"
      ],
      "year": "2016",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "54",
      "title": "BEGAN: Boundary equilibrium generative adversarial networks",
      "authors": [
        "D Berthelot",
        "T Schumm",
        "L Metz"
      ],
      "year": "2017",
      "venue": "BEGAN: Boundary equilibrium generative adversarial networks",
      "arxiv": "arXiv:1703.10717"
    },
    {
      "citation_id": "55",
      "title": "Loss-sensitive generative adversarial networks on lipschitz densities",
      "authors": [
        "G Qi"
      ],
      "year": "2017",
      "venue": "Loss-sensitive generative adversarial networks on lipschitz densities",
      "arxiv": "arXiv:1701.06264v6"
    },
    {
      "citation_id": "56",
      "title": "The art of data augmentation",
      "authors": [
        "D Van Dyk",
        "X.-L Meng"
      ],
      "year": "2001",
      "venue": "Journal of Computational and Graphical Statistics"
    },
    {
      "citation_id": "57",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2012",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "58",
      "title": "The effectiveness of data augmentation in image classification using deep learning",
      "authors": [
        "L Perez",
        "J Wang"
      ],
      "year": "2017",
      "venue": "The effectiveness of data augmentation in image classification using deep learning",
      "arxiv": "arXiv:1712.04621"
    },
    {
      "citation_id": "59",
      "title": "Best practices for convolutional neural networks applied to visual document analysis",
      "authors": [
        "P Simard",
        "D Steinkraus",
        "J Platt"
      ],
      "year": "2003",
      "venue": "International Conference on Document Analysis and Recognition"
    },
    {
      "citation_id": "60",
      "title": "Unlabeled samples generated by gan improve the person re-identification baseline in vitro",
      "authors": [
        "Z Zheng",
        "L Zheng",
        "Y Yang"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "61",
      "title": "Emotion classification with data augmentation using generative adversarial networks",
      "authors": [
        "X Zhu",
        "Y Liu",
        "J Li",
        "T Wan",
        "Z Qin"
      ],
      "year": "2018",
      "venue": "Pacific-Asia Conference on Knowledge Discovery and Data Mining"
    },
    {
      "citation_id": "62",
      "title": "Reducing the dimensionality of data with neural networks",
      "authors": [
        "G Hinton",
        "R Salakhutdinov"
      ],
      "year": "2006",
      "venue": "Science"
    },
    {
      "citation_id": "63",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "64",
      "title": "Efficient estimation of word representations in vector space",
      "authors": [
        "T Mikolov",
        "K Chen",
        "G Corrado",
        "J Dean"
      ],
      "year": "2013",
      "venue": "Efficient estimation of word representations in vector space",
      "arxiv": "arXiv:1301.3781"
    },
    {
      "citation_id": "65",
      "title": "Context-dependent pretrained deep neural networks for large-vocabulary speech recognition",
      "authors": [
        "G Dahl",
        "D Yu",
        "L Deng",
        "A Acero"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Audio, Speech and Language Processing"
    },
    {
      "citation_id": "66",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "67",
      "title": "Differential entropy feature for EEGbased vigilance estimation",
      "authors": [
        "L Shi",
        "Y Jiao",
        "B Lu"
      ],
      "year": "2013",
      "venue": "International IEEE/EMBS Conference on Neural Engineering (NER)"
    },
    {
      "citation_id": "68",
      "title": "Visualizing data using t-SNE",
      "authors": [
        "L Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of Machine Learning Research"
    }
  ]
}