{
  "paper_id": "2311.10170v1",
  "title": "Improving Unimodal Inference With Multimodal Transformers",
  "published": "2023-11-16T19:53:35Z",
  "authors": [
    "Kateryna Chumachenko",
    "Alexandros Iosifidis",
    "Moncef Gabbouj"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper proposes an approach for improving performance of unimodal models with multimodal training. Our approach involves a multi-branch architecture that incorporates unimodal models with a multimodal transformer-based branch. By co-training these branches, the stronger multimodal branch can transfer its knowledge to the weaker unimodal branches through a multi-task objective, thereby improving the performance of the resulting unimodal models. We evaluate our approach on tasks of dynamic hand gesture recognition based on RGB and Depth, audiovisual emotion recognition based on speech and facial video, and audio-video-text based sentiment analysis. Our approach outperforms the conventionally trained unimodal counterparts. Interestingly, we also observe that optimization of the unimodal branches improves the multimodal branch, compared to a similar multimodal model trained from scratch.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The availability of an abundance of data in the modern world has driven the development of machine learning methods exploiting such data to their fullest. Recently, there has been an increase in emergence of novel approaches utilizing multiple data modalities simultaneously, such as video, audio, text, or other sensor data, for solving a variety of tasks  [1, 2] . Such methods are referred to as multimodal methods and they have been proven successful in a plethora of application fields, including emotion recognition  [3] , hand gesture recognition  [4] , human activity recognition  [5] , and others. Leveraging multiple data sources concurrently can lead to improved performance of the learning model as data of different modalities can complement and enrich each other.\n\nResearch within the field of multimodal methods has been largely focused on tasks where all modalities of interest are assumed to be present both during training and test stages, and has involved development of novel feature fusion methods  [5] , solving multimodal alignment problems  [6] , etc. Nevertheless, it is not always desirable to rely on the assumption of all modalities of interest being present at inference time. In real-world applications, data of one or multiple modalities might be unavailable at arbitrary inference steps due to, e.g., transmission delays and media failures, or simply the application at hand might not be suitable for utilizing certain modalities, while they might be available during training. Utilization of unimodal models therefore remains widely adopted due to their simplicity and easier applicability to real-world tasks. Nevertheless, models relying only on unimodal data at inference time can benefit from multimodal training. Such approach can aid in learning richer This project has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 871449 (OpenDR). feature representations from single modality by relating it with other modalities, and help highlight unimodal information that is most relevant for the task. At the same time, the computational costs associated with the model are not increased.\n\nIn this work, we propose an approach for improving performance of unimodal models with multimodal training, and employ a multi-branch architecture with both unimodal, and multimodal Transformer-based branches. Unimodal and multimodal branches are co-trained and knowledge from the stronger multimodal branch is continuously transferred to the unimodal branches via a multi-task objective, hence improving the performance of resulting unimodal models. We perform experiments on three multimodal tasks and observe consistent improvements in the performance of the models. At the same time, we also observe that our approach not only improves the performance of unimodal models, but also that of the multimodal teacher model, compared to the similar model trained from scratch. Our contributions can be summarized as follows:\n\n• We propose an approach for improving the performance of arbitrary unimodal models with multimodal training, with no additional computational cost incurred by unimodal model at inference time; • The proposed framework is agnostic of the underlying modalities or unimodal architecture types, while in the experiments we showcase various architectures, including 3D-CNNs, 2D+1D-CNNs, and transformer-based ones; • We validate our approach on three multimodal tasks and ob-serve consistent improvements, with different modalities, architectures, and loss functions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Modern research directions in the field of multimodal learning have largely focused on advanced modality fusion methods  [2, 1, 7]  and include a variety of approaches, ranging from CNN-based cross-modal Squeeze-and-Excitation blocks  [5] , to translation based approaches  [8] . Within the field of multimodal fusion, perhaps the most notable recent development is the adoption of multimodal Transformers that allow to capture global correspondences between modalities, hence making them an especially favorable choice for temporal sequence modelling tasks where alignment between modalities is an important challenge  [3, 9, 10] . The idea behind cross-modal Transformers lies in adoption of self-attention mechanism  [11]  with queries and key-value pairs originating from different modalities, and one of the most notable instantiations of such approach is the Multimodal Transformer (MULT)  [6] .\n\nNevertheless, the above-mentioned approaches have their limitations. Primarily, they all rely on the assumption that the same set of sensors/modalities are available at both training and inference, while such expectation is idealistic and is an especially relevant limitation for real-world applications where flexibility is required. A set of methods aim to solve this issue by introducing the multimodal training unimodal testing paradigm, aiming at improving unimodal models by utilizing multimodal data during training. Such methods can be broadly categorized into a few types, with the first type being the methods aiming to reconstruct or otherwise hallucinate a missing modality  [12, 13, 14, 15] . Other methods optimize certain alignment objectives between multiple modalities, e.g., by contrastive learning  [16] , or by spatiotemporal semantic alignment  [17] . Nevertheless, such methods are mostly suited for well-paired modality types, such as RGB and Depth, or RGB and Point Clouds, while having limited suitability for modalities where data types are drastically different and their correspondence is not immediately obvious, e.g., audio and RGB frames, or text and RGB frames. In our work, we take aim to overcome this issue, and propose a generalized framework suitable for various data modalities and unimodal architectures.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Proposed Approach",
      "text": "This section describes the proposed approach for improving the performance of an arbitrary unimodal model with multimodal training. We consider the following problem setup: given a set of data representations of arbitrary modalities and corresponding unimodal model architectures, we seek to improve performance of said unimodal models by exploiting multimodal information during training. Concretely, our approach relies on a general framework in which unimodal models are united in a joint architecture by a multimodal Transformer-based branch attached to intermediate features of unimodal models of each modality, hence each unimodal model becomes a separate branch. The multimodal branch is jointly cotrained with resulting unimodal branches, and shares early feature extraction layers with the unimodal branches. Additionally, knowledge transfer between the multimodal Transformer and the unimodal branches is achieved by optimizing a multi-task objective. During inference, the multimodal branch as well as branches corresponding to modalities that are not of interest are dropped, restoring the original architecture of the unimodal model, but with parameters optimized during multimodal training. Overall, a schematic represen- tation of the proposed approach, with two example modalities A and B, is outlined in Figure  1 .\n\nAs can be seen, data of each modality i, Xi, is input to a sequence of layers serving as backbone for both unimodal and multimodal branches, resulting in feature representation Φi for modality i. Further, Φi is processed with the remaining part of the unimodal branch, as well as the multimodal Transformer branch (as described further) independently, where each branch has its own task-specific head that optimizes the task-specific objective L task (e.g., crossentropy for classification tasks). Additionally, a knowledge transfer objective from stronger multimodal branch to weaker unimodal branches L kt is optimized, where L kt can be represented by a variety of different objective functions, as will be discussed further.\n\nUnimodal and multimodal branches as well as task-specific and knowledge transfer objectives are optimized jointly. Shared feature layers receive gradient updates from task-specific objectives of both uni-and multimodal branches, hence forcing them to remain informative for both inference paths and avoiding the loss of modalityspecific information, while retaining information useful for modality fusion. In turn, knowledge transfer objective encourages the remaining segment of unimodal branch to learn in accordance with the multimodal transformer, hence improving its performance.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Multimodal Transformer",
      "text": "Here, we describe the multimodal Transformer branch. Given feature representations of two modalities ΦA and ΦB, cross-modal attention that fuses modality B into modality A is defined as:\n\nfollowed by another linear projection layer, where Wq, Wv, and W k are learnable projection matrices, d is the feature dimensionality, and ΦA and ΦB are features of modalities A and B. This is generally referred to as cross-attention and it is a generalization of the self-attention mechanism  [11]  where queries originate from modality A and key-value pairs originate from modality B. Similarly, fusion of modality B into modality A is achieved by learning queries from modality B and key-value pairs from modality A.\n\nThe overall multimodal Transformer branch is similar to the one proposed in  [6]  and consists of the previously defined crossattention blocks, optionally followed by unimodal self-attention blocks in each modality, as shown in Figure  2 . That is, for fusion of two modalities A and B, two cross-attention blocks A-> B and B-> A are employed and their resulting features concatenated, and in the case where the number of modalities is greater than two, pair-wise cross-attention blocks are calculated within each pair. The prediction head is unimodal model-specific.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Unimodal Branches",
      "text": "The proposed approach is agnostic of underlying unimodal models and can be combined with an arbitrary architecture. For the sake of completeness, we describe several examples of architectures used in our experimental evaluation further. For the task of dynamic gesture recognition based on RGB and Depth modalities, each unimodal branch is either an I3D  [18]  or MobileNetv2  [19]  architecture, primarily based on 3D convolutional layers. The multimodal branch in I3D variant is attached after \"M ixed 4f \" layer, and in the case of MobileNetv2, prior to the last two convolutional blocks. Hence, the majority of the layers is shared between the multimodal and unimodal branches. The extracted 3D convolutional features Φ have the shape of B × C × T × H × W , on which we perform spatial mean pooling, resulting in B ×C ×T input tokens input to the multimodal Transformer. For the task of audiovisual emotion recognition, we adopt an architecture similar to  [3] , with vision branch being the EfficientNet backbone followed by blocks of 1D-Convolutional layers, and audio branch is also a set of 1D-Convolutional layers. Here, we add multimodal Transformer branch on the output of \"Stage 1\" convolutional block in both branches. This can be compared to 'intermediate transformer' fusion described in  [3] , where outputs of multimodal Transformers are not fused back to their corresponding branches, but instead connect to their own output layer.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multi-Task Training Objective",
      "text": "The overall training objective is given by\n\nwhere i is the modality indicator, L i task is task-specific objective for branch of modality i, L mm task is the task-specific objective of the multimodal branch, and L i kt is the knowledge transfer loss from multimodal branch to unimodal branch i, and α, β, γ are scaling coefficients. A multitude of objective functions can serve the purpose of knowledge transfer. Here, we consider three cases, which we refer to as decision-level alignment, feature-level alignment, and attentionlevel alignment.\n\nIn decision-level alignment objective, the goal is to transfer high-level information about predictions and class probability distributions from stronger multimodal branch to weaker unimodal branch. To achieve this, for standard classification tasks, we formulate knowledge transfer as knowledge distillation task  [20]  and optimize KL-divergence L KL kt between soft pseudo-labels generated by multimodal branch and softmax outputs of unimodal branches. Soft probability distribution between classes is achieved by applying temperature T > 1 to predicted class probabilities. Such knowledge transfer allows the unimodal model to capture fine-grained class boundaries from the stronger multimodal model.\n\nIn feature-level alignment objective, the goal is to transfer broader semantic feature-level information from multi-to unimodal branch. Such formulation can be more general and suitable for a wider variety of tasks. For this goal, we adopt cosine similarity\n\nbetween the final hidden layer output features of the multimodal and unimodal branches, hence promoting the transfer of feature-level semantic information, aimed at improving the performance of task at hand. Lastly, when unimodal branch architectures are also Transformerbased, a mechanism that we refer to as attention-level alignment can be employed. Here, knowledge transfer can be achieved by aligning self-attention probability distributions over temporal tokens in unimodal and multimodal branches. Intuitively, tokens in multimodal Transformer attend to tokens of other modalities globally via self-attention in cross-modal Transformer blocks. Subsequently, unimodal Transformer blocks in multimodal Transformer operate over tokens that have already 'seen' corresponding tokens of other modalities. The softmax probabilties of unimodal self-attention in final stages of multimodal Transformer can then be distilled to the corresponding unimodal branches similarly to the first case, by calculating KL-divergence over soft pseudo-labels. We further refer to this approach and objective function as L att kt .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Evaluation",
      "text": "As described earlier, to the best of our knowledge the few existing methods aimed at unimodal inference with multimodal training are primarily suitable for well-paired modalities as they rely on fine-grained spatial information transfer or modality reconstruction/hallucination. This makes their application in more general scenarios and more heterogeneous modalities largely non-trivial if not impossible. On the other hand, our proposed approach is generalized and makes no assumption on the underlying data. Therefore, to show the effectiveness of our method, we compare the models trained within our framework to unimodal counterparts proposed in recent literature  [3, 6, 18, 19]  on a variety of tasks and modalities of different types, and show that our proposed approach improves their performance. We perform experiments on three tasks / datasets: egocentric dynamic gesture recognition using EgoGesture dataset  [4] , audiovisual emotion recognition using RAVDESS dataset  [21] , and multimodal sentiment analysis on CMU-MOSEI dataset  [22] . We train independently unimodal models with available modalities; multimodal model comprised of shared layers and multimodal Transformer; and the proposed multimodal architecture with knowledge transfer trained jointly, where we evaluate each of the resulting unimodal and multimodal branches independently. In each dataset, we report the performance on the test set, with the model selected based on best performance on the validation set. Each modality model is selected independently from other modalities and knowledge transfer loss weight is a hyperparameter. Best result is highlighted in bold, and results outperforming the baseline are underlined. Hand gesture recognition. For egocentric dynamic hand gesture recognition, we use EgoGesture dataset  [4, 23] , which is a hand gesture recognition dataset comprised of RGB and Depth modalities and including 83 hand gesture classes depicted in 24,161 short hand gesture clips, performed by 50 subjects. Unimodal branches are as described in Sec. 3.2, and multimodal branch is comprised of a multimodal Transformer attached to intermediate layers of Depth and RGB branches. As this task is formulated as a video classification problem, we adopt decision-level alignment for knowledge transfer, and minimize KL-divergence with T = 5 between soft output probability distributions of multimodal and unimodal branches.\n\nTable  1  shows the results of the proposed approach. As can be seen, the proposed training framework outperforms the unimodal counterparts on both modalities and both architectures, leading to up to 2.5% improvement in accuracy. Interestingly, we observe that the proposed approach also improves the performance of the multimodal branch when it is trained in conjunction with unimodal branches, compared to the multimodal branch trained independently. This shows that providing unimodal feedback during training forces the shared feature layers to retain more information specific to each independent modality, hence improving the multimodal performance.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Method",
      "text": "Acc-Audio Acc-Video Acc-MM Unimodal models  [3]  60 Audiovisual emotion recognition. For audiovisual emotion recognition we employ the RAVDESS dataset  [21]  which consists of face and speech recordings of 24 actors acting out 8 emotions and posing a classification task, with 60 video sequences recorded for each actor. The architecture follows the description in Section 3.2, with unimodal models trained from scratch. Knowledge transfer loss L kt is the KL-divergence between soft outputs with T = 5 and the task-specific loss is standard cross-entropy. Table  2  shows the results obtained in audiovisual emotion recognition tasks. As can be seen, the findings are consistent with those obtained in previous task, and the proposed approach improves both unimodal counterparts by up to 3%. Similarly, the multimodal branch is improved as well.  Multimodal sentiment analysis Next, for the task of multimodal sentiment analysis, we perform experiments on the unaligned version of CMU-MOSEI dataset  [22] , which contains 23,454 utterances extracted from movie review video clips taken from YouTube. The dataset consists of audio, vision, and text modalities, where each utterance is labeled with a sentiment score in the range [-3, . . . , 3] by human annotators. Since the dataset poses the regression task, the model is optimized with L1 loss as task-specific objective, and we evaluate both feature-level and attention-level alignment knowledge transfer objectives L cos kt and L att kt . We follow the standard protocol of the dataset and report mean average error, correlation with human annotations (annotations are obtained from multiple annotators), and 7-class accuracy. We report average results over 3 random    [6] , and the multimodal branch is identical to Figure  2 .\n\nTable  3  shows the results on the CMU-MOSEI dataset. Firstly, we observe that in our baseline experiments, text-only model outperforms the multimodal one (which is rather consistent with previous works, where text modality performance often lies close to the multimodal one  [6] ), while the text model trained under our proposed framework outperforms both of them. In fact, the proposed approach outperforms the baselines on all the modalities compared to unimodal models, with especially big increase observed in correlation metric, and the multimodal branch also outperforms the multimodal model trained independently. We observe that feature-level loss is more beneficial for improving the stronger text modality, and subsequently the multimodal branch. In turn, attention-level alignment shows to be more beneficial for audio and vision modalities. This shows that multimodal branch is mainly driven by the text modality (judging by their performance), hence features of the final hidden layer are likely to be more easily transferable to unimodal text branch than audio or vision branches. Instead, audio and vision branches can benefit from softer attention-level alignment, which does not enforce strong similarity to other modality, but instead, to tokens of the same modality enriched with multimodal information.\n\nAblation studies We perform a few ablations on the EgoGesture dataset. First, as our primary goal is to improve the unimodal branches, we train an architecture identical to the one described earlier, but the shared weights are only updated from the uni-modal branch, and are frozen in the multimodal path. The results can be seen in Table  1 , the freezing of the layers does not have a significant effect on the model, with unimodal models being marginally below the standard variant. Next, we investigate the effect of the knowledge transfer loss and train the identical model but without optimization of the knowledge transfer objective from the multi-modal to the unimodal branch. As can be seen in Table  1 , multi-modal branch still outperforms the one trained from scratch (showcasing again the benefits of unimodal gradient updates to the shared layers), but unimodal branches retain the unimodal performance, hence showing the effect of the knowledge transfer loss. We are additionally providing ablations on the α (coefficient of the knowledge transfer loss), with β and γ (task-specific losses) fixed to 1, which can be seen in Table  4  using EgoGesture dataset and MobileNetV2. As can be seen, any α outperforms the baseline, while the best result is achieved at α=5.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "We have presented a general framework for improving performance of an arbitrary unimodal model with multimodal training that involves co-training of the unimodal models with multimodal Transformer and multi-task objective aimed at knowledge transfer from multimodal to unimodal branches. The proposed approach shows improved performance on 3 tasks of different modalities and structures. We also found that providing unimodal feedback to early layers of multimodal model aids its performance in a multimodal setting. Future work may include research on higher adaptiveness of the co-training, such that not all unimodal models are co-trained in the same manner, but instead relatively to their capacity.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Description of the proposed framework. For a two modality case A",
      "page": 1
    },
    {
      "caption": "Figure 2: Example of a multimodal Transformer with three modalities A, B,",
      "page": 2
    },
    {
      "caption": "Figure 1: As can be seen, data of each modality i, Xi, is input to a se-",
      "page": 2
    },
    {
      "caption": "Figure 2: That is, for fusion of",
      "page": 2
    },
    {
      "caption": "Figure 2: Table 3 shows the results on the CMU-MOSEI dataset. Firstly,",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 1: shows the results of the proposed approach. As can Table4:ResultswithdifferentαonEgoGesture",
      "data": [
        {
          "Audio [6]\n(ours)\nA-Lcos\nkt\nA-Latt\n(ours)\nkt": "Vision [6]\n(ours)\nV-Lcos\nkt\nV-Latt\n(ours)\nkt",
          "0.8146\n0.2395\n41.05\n0.2812\n0.8125\n40.76\n0.8111\n41.17\n0.2493": "0.8079\n0.2313\n42.18\n0.2774\n0.8028\n42.18\n0.7978\n42.73\n0.2680"
        },
        {
          "Audio [6]\n(ours)\nA-Lcos\nkt\nA-Latt\n(ours)\nkt": "Text [6]\nT-Lcos\n(ours)\nkt\nT-Latt\n(ours)\nkt",
          "0.8146\n0.2395\n41.05\n0.2812\n0.8125\n40.76\n0.8111\n41.17\n0.2493": "0.6290\n0.6481\n48.72\n0.6199\n0.6570\n49.62\n0.6203\n0.6537\n49.02"
        },
        {
          "Audio [6]\n(ours)\nA-Lcos\nkt\nA-Latt\n(ours)\nkt": "Multimodal [6]\nMM-Lcos\n(ours)\nkt\nMM-Latt\n(ours)\nkt",
          "0.8146\n0.2395\n41.05\n0.2812\n0.8125\n40.76\n0.8111\n41.17\n0.2493": "0.6407\n0.6748\n48.72\n0.6273\n0.6793\n49.32\n0.6331\n0.6625\n49.09"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Selfsupervised multimodal versatile networks",
      "authors": [
        "Jean-Baptiste Alayrac",
        "Adria Recasens",
        "Rosalia Schneider",
        "Relja Arandjelović",
        "Jason Ramapuram",
        "Jeffrey De Fauw",
        "Lucas Smaira",
        "Sander Dieleman",
        "Andrew Zisserman"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "3",
      "title": "Unit: Multimodal multitask learning with a unified transformer",
      "authors": [
        "Ronghang Hu",
        "Amanpreet Singh"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "4",
      "title": "Self-attention fusion for audiovisual emotion recognition with incomplete data",
      "authors": [
        "Kateryna Chumachenko",
        "Alexandros Iosifidis",
        "Moncef Gabbouj"
      ],
      "year": "2022",
      "venue": "26th International Conference on Pattern Recognition"
    },
    {
      "citation_id": "5",
      "title": "Egogesture: a new dataset and benchmark for egocentric hand gesture recognition",
      "authors": [
        "Yifan Zhang",
        "Congqi Cao",
        "Jian Cheng",
        "Hanqing Lu"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "6",
      "title": "Mmtm: Multimodal transfer module for cnn fusion",
      "authors": [
        "Hamid Reza",
        "Vaezi Joze",
        "Amirreza Shaban",
        "Kazuhito Michael L Iuzzolino",
        "Koishida"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "7",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Liang",
        "J Zico Kolter",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "8",
      "title": "Multimodal machine learning: A survey and taxonomy",
      "authors": [
        "Tadas Baltrušaitis",
        "Chaitanya Ahuja",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "9",
      "title": "Face parsing from rgb and depth using cross-domain mutual learning",
      "authors": [
        "Jihyun Lee",
        "Binod Bhattarai",
        "Tae-Kyun Kim"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "10",
      "title": "Multimodal transformer fusion for continuous emotion recognition",
      "authors": [
        "Jian Huang",
        "Jianhua Tao",
        "Bin Liu",
        "Zheng Lian",
        "Mingyue Niu"
      ],
      "year": "2020",
      "venue": "International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "Multimodal emotion recognition using cross-modal attention and 1d convolutional neural networks",
      "authors": [
        "Krishna Dn",
        "Ankita Patil"
      ],
      "year": "2020",
      "venue": "Multimodal emotion recognition using cross-modal attention and 1d convolutional neural networks"
    },
    {
      "citation_id": "12",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "13",
      "title": "Learning with privileged information via adversarial discriminative modality distillation",
      "authors": [
        "C Nuno",
        "Pietro Garcia",
        "Vittorio Morerio",
        "Murino"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "14",
      "title": "Modality distillation with multiple stream networks for action recognition",
      "authors": [
        "C Nuno",
        "Pietro Garcia",
        "Vittorio Morerio",
        "Murino"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision"
    },
    {
      "citation_id": "15",
      "title": "Unimodal face classification with multimodal training",
      "authors": [
        "Wenbin Teng",
        "Chongyang Bai"
      ],
      "year": "2021",
      "venue": "Proceedings of the 16th International Conference on Automatic Face and Gesture Recognition (FG 2021)"
    },
    {
      "citation_id": "16",
      "title": "Learning common representation from rgb and depth images",
      "authors": [
        "Giorgio Giannone",
        "Boris Chidlovskii"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "17",
      "title": "Improving unimodal object recognition with multimodal contrastive learning",
      "authors": [
        "Johannes Meyer",
        "Andreas Eitel",
        "Thomas Brox",
        "Wolfram Burgard"
      ],
      "year": "2020",
      "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems"
    },
    {
      "citation_id": "18",
      "title": "Improving the performance of unimodal dynamic handgesture recognition with multimodal training",
      "authors": [
        "Mahdi Abavisani",
        "Reza Hamid",
        "Joze Vaezi",
        "M Vishal",
        "Patel"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "19",
      "title": "Quo vadis, action recognition? a new model and the kinetics dataset",
      "authors": [
        "Joao Carreira",
        "Andrew Zisserman"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "20",
      "title": "Mobilenetv2: Inverted residuals and linear bottlenecks",
      "authors": [
        "Mark Sandler",
        "Andrew Howard",
        "Menglong Zhu",
        "Andrey Zhmoginov",
        "Liang-Chieh Chen"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "21",
      "title": "Distilling the knowledge in a neural network",
      "authors": [
        "Geoffrey Hinton",
        "Oriol Vinyals",
        "Jeff Dean"
      ],
      "year": "2015",
      "venue": "Distilling the knowledge in a neural network",
      "arxiv": "arXiv:1503.02531"
    },
    {
      "citation_id": "22",
      "title": "The ryerson audiovisual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "R Steven",
        "Frank Livingstone",
        "Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "23",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amir Zadeh",
        "Ali Bagher",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "24",
      "title": "Egocentric gesture recognition using recurrent 3d convolutional neural networks with spatiotemporal transformer modules",
      "authors": [
        "Congqi Cao",
        "Yifan Zhang",
        "Yi Wu",
        "Hanqing Lu",
        "Jian Cheng"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE"
    }
  ]
}