{
  "paper_id": "2411.05361v2",
  "title": "Dynamic-Superb Phase-2: A Collaboratively Expanding Benchmark For Measuring The Ca-Pabilities Of Spoken Language Models With 180 Tasks",
  "published": "2024-11-08T06:33:22Z",
  "authors": [
    "Chien-yu Huang",
    "Wei-Chih Chen",
    "Shu-wen Yang",
    "Andy T. Liu",
    "Chen-An Li",
    "Yu-Xiang Lin",
    "Wei-Cheng Tseng",
    "Anuj Diwan",
    "Yi-Jen Shih",
    "Jiatong Shi",
    "William Chen",
    "Chih-Kai Yang",
    "Wenze Ren",
    "Xuanjun Chen",
    "Chi-Yuan Hsiao",
    "Puyuan Peng",
    "Shih-Heng Wang",
    "Chun-Yi Kuan",
    "Ke-Han Lu",
    "Kai-Wei Chang",
    "Fabian Ritter-Gutierrez",
    "Kuan-Po Huang",
    "Siddhant Arora",
    "You-Kuan Lin",
    "Ming To Chuang",
    "Eunjung Yeo",
    "Kalvin Chang",
    "Chung-Ming Chien",
    "Kwanghee Choi",
    "Jun-You Wang",
    "Cheng-Hsiu Hsieh",
    "Yi-Cheng Lin",
    "Chee-En Yu",
    "I-Hsiang Chiu",
    "Heitor R. Guimar√£es",
    "Jionghao Han",
    "Tzu-Quan Lin",
    "Tzu-Yuan Lin",
    "Homu Chang",
    "Ting-Wu Chang",
    "Chun Wei Chen",
    "Shou-Jen Chen",
    "Yu-Hua Chen",
    "Hsi-Chun Cheng",
    "Kunal Dhawan",
    "Jia-Lin Fang",
    "Shi-Xin Fang",
    "Kuan-Yu Fang Chiang",
    "Chi An Fu",
    "Hsien-Fu Hsiao",
    "Ching Yu Hsu",
    "Shao-Syuan Huang",
    "Lee Chen Wei",
    "Hsi-Che Lin",
    "Hsuan-Hao Lin",
    "Hsuan-Ting Lin",
    "Jian-Ren Lin",
    "Ting-Chun Liu",
    "Li-Chun Lu",
    "Tsung-Min Pai",
    "Ankita Pasad",
    "Shih-Yun Shan Kuan",
    "Suwon Shon",
    "Yuxun Tang",
    "Yun-Shao Tsai",
    "Jui-Chiang Wei",
    "Tzu-Chieh Wei",
    "Chengxi Wu",
    "Dien-Ruei Wu",
    "Chao-Han Huck Yang",
    "Chieh-Chi Yang",
    "Jia Qi Yip",
    "Shao-Xiang Yuan",
    "Vahid Noroozi",
    "Zhehuai Chen",
    "Haibin Wu",
    "Karen Livescu",
    "David Harwath",
    "Shinji Watanabe",
    "Hung-yi Lee"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal foundation models, such as Gemini and ChatGPT, have revolutionized human-machine interactions by seamlessly integrating various forms of data. Developing a universal spoken language model that comprehends a wide range of natural language instructions is critical for bridging communication gaps and facilitating more intuitive interactions. However, the absence of a comprehensive evaluation benchmark poses a significant challenge. We present Dynamic-SUPERB Phase-2, an open and evolving benchmark for the comprehensive evaluation of instruction-based universal speech models. Building upon the first generation, this second version incorporates 125 new tasks contributed collaboratively by the global research community, expanding the benchmark to a total of 180 tasks, making it the largest benchmark for speech and audio evaluation. While the first generation of Dynamic-SUPERB was limited to classification tasks, Dynamic-SUPERB Phase-2 broadens its evaluation capabilities by introducing a wide array of novel and diverse tasks, including regression and sequence generation, across speech, music, and environmental audio. Evaluation results show that no model performed well universally. SALMONN-13B excelled in English ASR and Qwen2-Audio-7B-Instruct showed high accuracy in emotion recognition, but current models still require further innovations to handle a broader range of tasks. We open-source all task data and the evaluation pipeline at https://github.com/dynamic-superb/dynamic-superb.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Recent advancements in large language models (LLMs) have accelerated the development of natural language processing (NLP)  (Touvron et al., 2023a; Achiam et al., 2023; Li et al., 2023b; Anthropic, 2023; Bai et al., 2023) . These models can follow natural language instructions, making users quickly adopt them for a variety of applications. They have been integrated into commercial products, such as ChatGPT  (Achiam et al., 2023)  and Claude  (Anthropic, 2023) , as well as in the open-source research community, including the LLaMA series  (Touvron et al., 2023a; b; Dubey et al., 2024 ). Yet, they are primarily text-based models, meaning they cannot process speech or audio, which are essential for more natural ways of communication and interaction with the real world.\n\nCompared to written text, spoken language has always been a more natural and convenient way for humans to communicate. Spoken language conveys a wealth of information, including semantics, prosody, emotion, and speaker characteristics, while text is limited to representing semantic information, which can sometimes even depend on the prosodic cues present in spoken language  (Lin et al., 2024) . This highlights the need for universal speech models and explains why automatic speech recognition (ASR) systems using text-based language models are not optimal. Several attempts have been made to develop instruction-based universal speech or audio models capable of performing various tasks, such as LTU-AS  (Gong et al., 2023) , SALMONN  (Tang et al., 2024a) , Qwen-Audio  (Chu et al., 2023; 2024) , and WavLLM  (Hu et al., 2024) . Despite significant research in universal speech models, evaluating them effectively and comprehensively remains a major challenge. In NLP, benchmarks for text LLMs include a large number of tasks. CrossFit  (Ye et al., 2021)  includes 160 tasks, BIG-bench  (Srivastava et al., 2022)  comprises 204 tasks, and Natural-Instructions  (Mishra et al., 2022; Wang et al., 2022)  provides 1,616 tasks. For universal speech models, several benchmarks have been proposed but with a fixed and limited set of tasks (typically around a dozen) to evaluate specific capabilities of models  (Yang et al., 2021; Dunbar et al., 2022; Maimon et al., 2024) . This is insufficient for evaluating a universal model, as we aim to investigate whether models can handle a broader range of tasks beyond fixed benchmarks. There is a strong demand for a benchmark that can evaluate these models across various aspects with a wide range of speech tasks, which led to the creation of Dynamic-SUPERB  (Huang et al., 2024a) .\n\nDynamic-SUPERB is the first benchmark for evaluating universal instruction-based speech and audio models. As models advance, we dynamically expand the benchmark by adding new tasks to provide better guidance for researchers in model development. We have designed a well-structured pipeline that harnesses the collective efforts of the community to gather diverse challenging, novel, and creative tasks. The first generation of Dynamic-SUPERB consists of 55 tasks, covering various aspects of speech (e.g., semantics, speakers, etc.), making it the speech benchmark with the most tasks. However, this is not sufficient to pave the way for developing universal speech models, especially given the complexity and richness of the information conveyed by spoken language. This paper presents the Dynamic-SUPERB Phase-2. We expanded it to 180 tasks, more than double the size of its first iteration, with contributions from the global research community. They cover a broad range of types, and some tasks present novel challenges that have not been explored in any previous research. Besides, previous speech research has typically treated music and environmental audio as background noise to be ignored. However, these sounds contain rich information and share overlapping elements that complement each other. Thus, in Dynamic-SUPERB, we also introduced preliminary music and audio tasks from established music and audio benchmarks  (Yuan et al., 2023; Turian et al., 2022) . We define the core tasks by reformulating the SUPERB  (Yang et al., 2021)  (speech), MARBLE  (Yuan et al., 2023) (music) , and HEAR  (Turian et al., 2022)  (audio) benchmarks for quick-round research experiments. One challenge of evaluating with such a large-scale benchmark is deriving concrete and useful insights from hundreds of evaluation results. We provide a taxonomy for every task in Dynamic-SUPERB, where tasks are clustered by the specific model capabilities they probe. Researchers can follow this taxonomy to develop or reinforce specific capabilities of the models they build. To our knowledge, Dynamic-SUPERB is the largest benchmark and the first to provide such a detailed task taxonomy in speech processing.\n\nWe conducted a comprehensive evaluation of several models using Dynamic-SUPERB Phase-2. To handle the diverse output formats of these models, we propose an automated LLM-based pipeline for general evaluation across tasks. The evaluation results show that these models perform well only on a limited range of tasks. For example, SALMONN-13B performed well on English ASR, while Qwen2-Audio-7B-Instruct achieved high accuracy in emotion recognition. However, they do not generalize well to a much broader range of tasks in speech recognition and paralinguistics. Notably, training on diverse data, even with significant differences in signal-level characteristics, can enhance performance across domains. We observed that spoken language models outperformed music models in certain music tasks. This highlights the potential for developing unified models for speech, music, and general audio. We have open-sourced all materials to support reproducibility and further research. We invite researchers to help expand Dynamic-SUPERB, making it more diverse and comprehensive to advance universal spoken language models.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Related Works",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Instruction-Following Universal Speech Models",
      "text": "LLMs have shown strong natural language processing abilities and are used in speech, audio, and music applications. Recent frameworks integrate a pre-trained speech encoder with an LLM through fine-tuning techniques such as LoRA  (Hu et al., 2021) . LTU-AS  (Gong et al., 2023)  combines Whisper  (Radford et al., 2023)  with LLaMA  (Touvron et al., 2023a) , and it is fine-tuned on openended speech and audio question-answering. SALMONN adopts a window-level Q-former  (Li et al., 2023a)  to generate soft prompts fusing representations from the speech and audio encoders. Qwen-Audio  (Chu et al., 2023)  introduces task-specific tags into Qwen to encourage knowledge sharing and prevent interference across diverse tasks, and it supports multi-turn dialogues for both audio and text inputs. WavLLM  (Hu et al., 2024)  uses curriculum learning to prevent overfitting on specific tasks while maintaining the LLM's original capabilities. All these models accept speech, audio, and text as input but output only text, focusing on understanding rather than generation in speech and audio. Several attempts have also used prompts for speech, music, and audio generation  (Guo et al., 2023; Agostinelli et al., 2023; Liu et al., 2023a) . However, to our knowledge, there is no universal model capable of handling both generation and understanding tasks. Moreover, these models have not been comprehensively evaluated on a benchmark, which hinders fair comparisons between them.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Evaluation Benchmarks",
      "text": "Several benchmarks have been developed to evaluate speech models. SUPERB  (Yang et al., 2021)  is the most widely used benchmark for assessing the performance of speech foundation models across various tasks that cover different aspects of speech. On the other hand, SLUE  (Shon et al., 2022; 2023)  focuses more specifically on spoken language understanding. Yet, they are primarily limited to English. LeBenchmark  (Evain et al., 2021; Parcollet et al., 2023)  evaluates speech foundation models specifically for French, while IndicSUPERB  (Javed et al., 2023)  is dedicated to Indian languages. ML-SUPERB  (Shi et al., 2023; 2024b)  offers ASR and language identification tasks in 100+ languages. Aside from speech, MARBLE  (Yuan et al., 2023)  and HEAR  (Turian et al., 2022)  offer platforms for evaluating various music and a wide range of audio tasks. Using these benchmarks, researchers build a specialized model for each task. Thus, the number of tasks is limited due to the growing costs associated with adding more tasks. Conversely, universal models are expected to do various tasks without fine-tuning for each one, allowing users to engage in far more diverse applications and enabling benchmark developers to include more tasks for comprehensive evaluation. In NLP and CV, benchmarks have been developed to evaluate models across a much broader range of tasks  (Bitton et al., 2023; Srivastava et al., 2022; Mishra et al., 2022) . However, in speech, we lack a benchmark of comparable scale. Dynamic-SUPERB  (Huang et al., 2024a)  is the first benchmark for instruction-based universal speech models. While its first version includes far more tasks than all other speech benchmarks, they are all classification tasks. AIR-bench  (Yang et al., 2024b) , on the other hand, includes tasks from speech, music, and audio, and expands beyond classification, although the number of tasks available for evaluating universal models remains limited (19 tasks in foundation track). Thus, there remains a critical need for a comprehensive benchmark that evaluates universal models across a broader range of tasks to fully assess their capabilities. Consequently, we developed Dynamic-SUPERB Phase-2, substantially upgrading the first generation with a detailed taxonomy and establishing community contribution protocols to facilitate continual task integration. (3) The model follows natural language instructions to execute the corresponding tasks. To comprehensively evaluate these universal models, we have collected hundreds of tasks and developed a task taxonomy to better guide benchmark users (Figure  1a ). All tasks in Dynamic-SUPERB Phase-2 are intended solely for testing purposes; we do not provide training data for two main reasons. First, current models are trained on large-scale, open-source, or proprietary datasets, making it challenging to ensure that all models are trained under consistent conditions. Second, we aim to evaluate universal models that do not require fine-tuning for downstream tasks.\n\nThe Dynamic-SUPERB project is designed to evolve dynamically with research advancements by incorporating novel tasks from the research community (Figure  1b ). With the first call for tasks, Dynamic-SUPERB Phase-2 has grown from its first generation, expanding from 55 to 180 tasks. Table  1  compares several benchmarks used in speech, music, and audio research, demonstrating that Dynamic-SUPERB Phase-2 is the largest benchmark covering all three areas. It provides a more fine-grained evaluation than any other benchmark.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Task Formulation",
      "text": "As Figure  1c  depicts, in Dynamic-SUPERB Phase-2, each task is structured to include: (1) Text instruction: A natural language instruction that guides the model on the task to perform. Each task has multiple different instructions to evaluate the model's ability to understand instructions.\n\n(2) Audio component: At least one audio element, which can be in the input, the output, or both.\n\n(3) (Optional) Text component: Text elements (other than instruction) that may serve as inputs or outputs. The number of audio elements in the inputs or outputs may vary depending on the task. For example, in speaker verification, two utterances are used to determine whether they were produced by the same speaker (green blocks in Figure  1c ). Besides, text inputs are not always required; for instance, ASR does not involve any text inputs. We use text format for instructions instead of spoken format. Spoken instructions involve varying content, speaker characteristics, and prosody, making them more complex. Text instructions act as an intermediary, bridging text and spoken universal models. These designs ensure consistency and simplify the model's understanding and processing of diverse tasks while maintaining the benchmark's extendibility with minimal constraints.\n\nAnother problem in evaluating universal models is the format of classification and regression tasks. Generally, a task-specialized model generates predicted labels (soft distributions or hard labels) within a pre-defined set of labels for classification or numeric values for regression within a predefined range. However, a universal model does not necessarily have access to this information, and thus it is hard to evaluate them in the same way, especially using it to perform several different tasks.\n\nTo address this, we believe that a universal speech model should produce outputs in natural language for all tasks (model outputs in Figure  1c ). Therefore, in Dynamic-SUPERB, all classification labels are represented as text. For regression tasks with specific formats (e.g., scalars, JSON, or Pythonstyle lists), we can parse the natural language outputs using a post-processing pipeline (Section 4.2).",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Call For Tasks",
      "text": "Dynamic-SUPERB fosters community collaboration, encouraging the addition of innovative tasks to remain relevant in this rapidly changing field. In Phase-2, we initiated a call for tasks in March 2024 to invite contributions from the research community. We established an organized and transparent submission process on our GitHub portal, where we organize members who serve as editors to guide contributors. Contributors propose tasks by providing relevant information on GitHub. Each proposal is assigned to an editor who checks for major issues and prevents duplicated efforts. Then, task proposers upload their data to our Huggingface space in specified formats, using only datasets with proper licenses. They complete submissions by opening a pull request on GitHub with the necessary files. Afterward, editors review submissions on a rolling basis, offering suggestions for refinement rather than immediate acceptance or rejection. After iterative improvements, accepted tasks are merged into the repository. Between March and July 2024, we received more than 140 task proposals and accepted over 120 new tasks. Tasks still under review are not included here but will be featured in the next phase. For details, please refer to Appendix F.  One primary challenge in building a large-scale benchmark is offering valuable insights to its users.\n\nTo address this, we developed a task taxonomy 1  that helps researchers interpret performance results across various tasks. Researchers can leverage this taxonomy to select specific tasks for model development instead of evaluating every task in the benchmark.\n\nFigures 2 shows the high-level task taxonomy in Dynamic-SUPERB Phase-2. Due to the space limitation, we only show some representative tasks. Each leaf node includes at least one task and may be further categorized into more fine-grained sub-domains, which are not shown here due to space constraints. For example, within 'Speaker & Language/Speaker', we have two categories: 'Speaker Characteristics' and 'Speaker Identification', each containing several tasks. Please refer to Appendix B for the complete task list. We first categorize tasks into two primary fields: (I) speech and (II) music & audio, which are generally distinguished by the source of the sound. For instance, speech is produced by human vocal cords, music is created using instruments, and audio includes sounds generated by other creatures, materials, or natural phenomena. We then split each field into several domains based on the key attributes and challenges that the tasks within them present.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Speech",
      "text": "As Figure  2a  shows, there are 8 domains within the speech field. Speech Recognition focuses on converting spoken language into text. This includes tackling various challenges like multilingual and code-switching ASR, as well as spontaneous ASR which is very different from audiobookstyle ones. It also contains specialized tasks such as command recognition and keyword spotting. Speaker and Language addresses the analysis of speaker characteristics and languages, covering tasks such as speaker verification, diarization, and language identification. Spoken Language Understanding deals with understanding and analysis of the content and semantics of spoken language. It covers tasks like sentiment analysis and speech translation. Phonetics, Phonology, and Prosody focuses on the sound structure of speech, including phoneme recognition, pronunciation evaluation, and prosodic features like stress and accent classification. Paralinguistics explores non-verbal aspects of speech, such as emotion recognition and vocal event detection, which can capture nuances like screaming or coughing. Speech Enhancement aims to improve speech quality by detecting and mitigating noise, reverberation, and other degradations, but currently, we only have understanding tasks. Speech, Voice, and Hearing Disorders is dedicated to identifying and classifying disorders such as stuttering and so on. Safety and Security focuses on detecting synthetic or manipulated speech, addressing tasks like spoof detection and recognizing deepfake voices.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Audio & Music",
      "text": "The audio and music field includes a wide range of tasks that focus on various attributes of sound beyond speech, such as musical elements, environmental sounds, and advanced sound analysis. This field is divided into 9 domains, each addressing a specific aspect of audio or music processing. Music Classification tasks focus on categorizing musical elements such as instruments, genres, and emotions, providing a foundation for recognizing and analyzing different types of musical content. Pitch Analysis delves into identifying the pitch and harmony within music, including tasks like pitch extraction, chord classification, and key detection. Rhythm Analysis involves tasks such as beat tracking, which is critical for understanding the temporal structure of music. In Singing Analysis, tasks address both lyric recognition and translation, as well as the classification of vocal techniques used in singing. Quality Assessment evaluates the perceived quality of singing, including automated predictions of Mean Opinion Scores (MOS). The Sound Event domain is broader, covering various sound sources such as animals, the environment, and human activities. Tasks range from animal sound classification to emergency traffic detection and even advanced tasks like multichannel sound event understanding. Safety domain includes detecting deepfakes in singing voices and identifying manipulated audio files, ensuring sound authenticity and integrity. Spatial Audio covers all tasks related to understanding spatial information, such as estimating the distance or position of sounds in the real world. Finally, Signal Characteristics Analysis domain addresses general signal-level characteristics of audio, including sound effect detection, and duration prediction.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Core Tasks",
      "text": "While our task taxonomy provides a comprehensive, hierarchical, and systematic approach for benchmark users to easily get started with Dynamic-SUPERB Phase-2, evaluating all tasks with limited resources remains a challenge. To address this, alongside the task taxonomy, we have selected core tasks for speech, music, and audio. These core tasks are reduced subsets of essential tasks that have been widely used or studied within the research community. We include tasks from three popular benchmarks: SUPERB (speech), MARBLE (music), and HEAR (audio). The three benchmarks were originally designed for evaluating encoders, not for an instruction-following universal model, so modifications are required. We crafted instructions for each task, reduced the data size, and reformulated them into the Dynamic-SUPERB format. Besides, we replaced the datasets in some tasks to address licensing issues. Since the core task set is much smaller than the full benchmark, researchers can more efficiently evaluate their models across a reasonable range of domains.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experimental Settings",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Models",
      "text": "We evaluated several publicly-available models on Dynamic-SUPERB Phase-2: SALMONN, LTU-AS, Qwen-Audio-Chat, Qwen2-Audio-7B-Instruct, WavLLM, MU-LLaMA  (Liu et al., 2024) , GAMA  (Ghosh et al., 2024) . SALMONN is further categorized into 7B and 13B versions based on the size of its LLM component. All of these models are publicly available, and we utilized their official implementations for inference without any modifications. The first five models are instruction-based speech models, each also trained with audio understanding capabilities. However, as they were not trained on music data, we do not expect them to perform well on music-related tasks. Hence, we further included MU-LLaMA and GAMA. MU-LLaMA is specifically designed for various music-understanding tasks, and GAMA is trained with both general audio data and a music corpus. Similar to the speech models, MU-LLaMA and GAMA adopt an LLM-based framework. They use a music or audio encoder, such as MERT  (LI et al., 2024)  or Q-former, to convert music or audio into features, which the LLM then uses as prompts for subsequent reasoning. The sampling strategies used for generating outputs from the LLMs were retained as defined in their official implementations. Finally, we implemented a cascaded system baseline called Whisper-LLaMA.\n\nThe system first transcribes audio using Whisper-v3-large, and then LLaMA3.1-8B processes the transcriptions to perform various tasks based on the provided instructions.\n\nWhen evaluating models on these diverse tasks, several challenges inevitably arise. One challenge was testing the baseline models on tasks involving multiple audio inputs. For example, in speaker verification, a model determines whether two utterances are produced by the same speaker. Among the evaluated models, only Qwen-Audio and Qwen2-Audio provide interfaces that allow inputting multiple audio files. For the remaining models, we concatenated all audio files, separated by 0.5 seconds of silence, and used the concatenated file as input.\n\nLast, different models have their own maximum supported audio durations. Models using Whisper to encode speech face an inherent limitation: Whisper can process audio files of up to 30 seconds, and some models are restricted to handling even shorter durations. Consequently, we retained the original model settings and did not modify the model architecture to accommodate longer audio clips. A preliminary analysis of all audio (Appendix G) in Dynamic-SUPERB Phase-2 shows that only a small proportion (around 6.57%) exceeds 30 seconds in length. Hence, we believe our settings do not largely impact the evaluation results and ensure reasonable inference efficiency.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "The outputs of universal speech models are natural language sentences, making it difficult to assess their correctness using conventional metrics. For classification tasks, such as emotion recognition, a task-specific model outputs a label from predefined emotions in the dataset, while a universal model generates sentences like \"The speaker sounds happy.\" In this case, we can easily assess the correctness of the former by comparing labels, but this approach does not apply to the latter. Similarly, in regression tasks, a natural language response like \"Using MOS scoring criteria, I give the audio a score of 3 out of 5\" makes it difficult to directly use metrics such as mean square error.\n\nIn evaluation, we categorize tasks into three types: (1) classification, (2) regression, and (3) sequence generation. For each type, we use different pipelines to evaluate the models' outputs. For classification tasks, we utilize external LLMs (GPT-4o) as referees, with the temperature set to be zero for evaluation consistency, to evaluate whether the outputs from speech or music models 2  match the ground truth. This approach has been widely adopted in the NLP community  (Wang et al., 2023; Liu et al., 2023b; Chiang & Lee, 2023) , and we extend its application to speech research. We design a prompt that includes the task instructions, the model's output to be evaluated, and the corresponding ground truth. The LLM judge gets this prompt and determines if the output aligns with the ground truth using a chain-of-thought reasoning process. By leveraging the strong instruction-following capabilities of these LLMs, we constrain them to provide the final decision in a fixed format, allowing us to extract the answer using simple methods such as regular expressions 3  . We then define accuracy as the percentage of outputs that the LLM judge considers aligned with the ground truth. Please refer to Appendix E for details on the prompts and the alignment between LLMs and humans. Importantly, although we used GPT-4o for evaluation in the main text, to support reproducibility we have also included comprehensive evaluation results using the open-source LLM (LLaMA3.1-8b-Instruct) in Appendix C.\n\nFor regression tasks, the above method cannot be applied because performance is not assessed using hard labels. Instead, we use LLMs (GPT-4o) as a post-processor to transform natural language responses into a format compatible with the original metrics used in these tasks. Specifically, we developed a prompt that directs the LLMs to extract essential information from the output and convert it into the same format as the ground truth. If the output lacks correct or relevant information that can be converted to match the ground truth format, the LLM returns \"N/A\" to mark it as invalid. Since conventional metrics cannot accommodate \"N/A\" and setting a default value is unreasonable due to the variability of metrics, we also calculate the N/A rate for each regression task, defined as the percentage of invalid outputs within a task. A higher N/A rate indicates that the model struggles with following instructions, which may indirectly affect its performance on the task.\n\nFor sequence generation tasks such as ASR and captioning, we apply their original metrics directly to the raw outputs from baseline models. This is because identifying redundant prefixes or sections unrelated to the task objectives is highly challenging in sequence generation, even with human involvement. Besides, our review of the original evaluation results reported by these baseline models revealed no explicit mention of post-processing procedures. Therefore, we base our evaluations solely on the unprocessed outputs, ensuring consistency and objectivity across all models.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Domain-Level Performance Comparison In Taxonomy",
      "text": "Here, we analyze the results based on the taxonomy. Due to space limitations, we can only report and compare the results of different models at the domain level in this subsection. Since different metrics are used across tasks, we adopted a relative-score-based method to summarize task performance.\n\nFor each task, we calculated the relative improvement of each model compared to the cascaded system baseline (Whisper + LLaMA) and then obtained the domain-level scores by averaging all its improvements across the tasks within each domain. For regression tasks, we introduced the N/A rate to measure how well a model meets the task requirements. The original task metric was only computed on instances that followed the task format (after post-processing by the LLM). To account for whether a model can follow instructions, we incorporated the N/A rate into the reported scores. Accordingly, we calculated the improvement based on scaled values. For metrics where a higher value indicates better performance (such as the F1 score), we multiplied the metric value by (1 -N/A rate). Conversely, for metrics where a lower value indicates better performance (such as word error rate), we divided the metric value by (1 -N/A rate).\n\nFigure  3  presents a domain-level comparison of relative scores across different models. Whisper-LLaMA consistently has zero scores throughout all domains, serving as the reference baseline for evaluating other models. In speech domains, no model outperforms Whisper-LLaMA in speech recognition and spoken language understanding. This shows that using ASR remains a strong baseline for language understanding, as text more explicitly represents semantic information than speech. However, Whisper-LLaMA lags behind SALMONN in phoneme recognition, a task for which SALMONN is explicitly trained, indicating that task-specific training still provides superior performance in specialized domains.\n\nConversely, in the speaker and paralinguistics domains, all models surpass the baseline. This improvement occurs because the ASR process in the cascaded system tends to discard critical information from the speech signal, such as speaker characteristics, pitch, and emotion. In contrast, other models utilize soft representations that retain more speech information, giving them the potential to learn beyond ASR, and some of these models were also trained to perform specific tasks within certain domains 4  . Additionally, music-specific large language models like GAMA-IT and Mu-LLaMA perform poorly on speech recognition and understanding tasks, which is expected since they were primarily designed for music understanding. Nevertheless, they surprisingly achieved scores comparable to the speech models in speaker and paralinguistics.\n\nTurning to audio and music domains, where ASR models cannot transcribe non-speech information into text, most models outperform the baseline in several areas, such as music classification and sound event detection. Surprisingly, spoken language models primarily trained on speech data (Qwen models, SALMONN, WavLLM) 5  outperform the two music models (GAMA-IT and Mu-LLaMA) in various music-related domains such as harmony and pitch, music classification, and rhythm analysis. We speculate that training on diverse data, even with significant differences in signal-level characteristics, enhances performance across domains, emphasizing the importance of developing unified models for speech, music, and general audio processing.\n\nLastly, we observe some outliers with significantly higher or lower values in the figure, typically resulting from unbounded evaluation metrics. For instance, in the phonetics and prosody domain, WavLLM and LTU-AS exhibit poor scores due to their erroneous outputs in phone/phoneme seg- ment counting tasks, predicting thousands of segments in utterances lasting only seconds and resulting in an excessively high mean square error. While the relative score-based approach effectively summarizes model performance, domain-level scores can be distorted by specific tasks within a domain. Thus, we encourage researchers aiming to develop model performance in specific domains to comprehensively report task-level scores to more accurately reflect their models' capabilities.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Core Tasks Results",
      "text": "We tested the performance of the models on core tasks that are widely studied and commonly used. Table  2  presents the detailed results of the models on SUPERB tasks, displaying each task's performance in its own metric rather than relative scores. Please refer to Appendix D for results on HEAR and MARBLE. It is worth noting that these tasks have been adapted to the Dynamic-SUPERB framework. Therefore, the results are not directly comparable to previous studies that tested on the original SUPERB. However, they indicate current performance levels on these tasks.\n\nNo single model excels across all tasks. In phoneme recognition (PR), the SALMONN models were the only ones to achieve a relatively lower phoneme error rate (PER) compared to the others. For keyword spotting (KS), Qwen-Audio-Chat was the only model to perform slightly better than a random guess (50%). Whisper-LLaMA surpassed all other models in intent classification (IC).\n\nRegarding emotion recognition (ER), Qwen-Audio-Chat and Qwen2-Audio-7B-Instruct stood out, with the latter even achieving about 75% accuracy. In ASR, SALMONN-13B and WavLLM are the only two models that achieved a word error rate (WER) lower than 10%. Query-by-example (QbE) appears to be very challenging for all models, as none performed better than a random guess (50%). Slot-filling (SF) is evaluated using two metrics: F1 for slot type and CER for slot value. While Whisper-LLaMA and SALMONN-7B are the top two in this task, their results are still far from ideal. In speaker verification (SV), all models performed badly, as even the best ones achieved results close to random guessing (50%). In speaker diarization (SD), results were similarly poor, with some even reaching a 100% N/A rate. Comparing Figure  3  and Table  2 , we observed that performance on core tasks sometimes deviates from trends observed at the domain level. For example, although SALMONN-13B outperforms other models in ASR, it lags behind Whisper-LLaMA in the speech recognition domain, revealing its limited capabilities and highlighting the need to enhance its generalizability. In summary, among tested models, some excel in specific tasks, while others perform poorly across the board, and none dominate across all tasks. We believe these findings provide valuable insights for researchers aiming to improve models, starting from core tasks and progressively tackling each domain in the benchmark for broader applicability.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusions",
      "text": "This paper presents Dynamic-SUPERB Phase-2, the largest benchmark for evaluating instructionbased universal spoken language models, building upon collaborative efforts across the research community. Dynamic-SUPERB covers a wide range of diverse tasks and offers a fine-grained task taxonomy. The recent models show good performance on specific tasks but poor generalization across common tasks like those in SUPERB, highlighting the need for further research on universal models. All materials are made openly available to facilitate reproduction and benchmarking. We sincerely invite researchers to join our vibrant community and collaborate to advance the field.\n\nLimitations: Although Dynamic-SUPERB Phase-2 is the largest and most comprehensive benchmark, we acknowledge its limitations. It lacks comprehensive speech-generation tasks, as Phase-2 focused on understanding tasks due to the few universal generation models. Despite our efforts to develop the task taxonomy scientifically, new domains may emerge as the benchmark grows, and tasks can be categorized in various ways. While our automatic evaluation pipeline using LLMs correlates well with human evaluations (Appendix E) for current tasks, it may not generalize to all future tasks. Upholding the core spirit of the Dynamic-SUPERB project, we are striving to address these issues and enhance the benchmark for the next phase.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Ethics Statement",
      "text": "Dynamic-SUPERB Phase-2 includes several datasets, and we asked contributors to describe how they used them. Most contributors derived task data by uniformly sampling from the original datasets without applying any special processing. Therefore, Dynamic-SUPERB Phase-2 inevitably inherits the biases present in these datasets across tasks. Additionally, we asked all contributors to provide the necessary license information to ensure that we can include them in the benchmark properly. More specifically, all task data in Dynamic-SUPERB are derived from datasets with licenses that permit remixing and redistribution. As we expect the benchmark to grow dynamically in the future, we maintain a complete list of dataset licenses on our official GitHub page (https://github.com/dynamic-superb/dynamic-superb/blob/ main/docs/dataset_license.md), which will be updated as new tasks are introduced. If there are any changes to the dataset license in the future, we may adjust the benchmark task accordingly.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Reproducibility Statement",
      "text": "We open-source all task data and the evaluation pipeline at https://github.com/ dynamic-superb/dynamic-superb. For LLM-based evaluation, we set GPT-4o's temperature to 0 to ensure a stable evaluation process. However, we acknowledge that GPT-4o's behavior may change with future updates. Therefore, we provide a comprehensive set of evaluation results using LLaMA-3.1-8b-Instruct in Appendix C (Table  6 ). Additionally, in Appendix E, we present a preliminary study on using open-source LLMs for evaluation, demonstrating promising correlations with human annotations. We encourage researchers to use these open-source LLMs when evaluating their own models to support reproducibility.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "D Hear & Marble Evaluation Results",
      "text": "Tables  7  and 8  show the evaluation results for the HEAR and MARBLE tasks, respectively, as collected in Dynamic-SUPERB Phase-2. Importantly, we replaced the datasets for some tasks to address licensing issues and reduced their size to enable more efficient inference. Furthermore, we removed certain tasks because they overlap with existing SUPERB tasks. Consequently, these results are not directly comparable to those evaluated on the original HEAR and MARBLE.  Classification Post-processing Prompt:\n\nYou will be given a question, a corresponding correct answer(s), and a response from a model. The model's response is a reply to the question. Your task is to judge if the \"Model's Response\" aligns with the \"Ground Truth Answer\" based on the \"Question.\"\n\nPlease strictly follow the guidelines below:\n\n-Briefly explain the reasons for your judgment.\n\n-Answer with the format \"Result: <YES or NO>\" at the end.\n\n-Output \"YES\" if the response aligns with the ground truth answer; output \"NO\" if the response does not match the ground truth answer, selects incorrect or irrelevant options, or provides more answers than required.\n\n-The questions would be single-choice or multi-choice: For single-choice questions, the model's response should contain one and only one answer.  Regression Post-processing Prompt: You will be provided with an \"instruction,\" a \"ground-truth label,\" and a \"model output.\" Your job is to extract the value from the given \"model output\" and return the post-processed \"model output\". Provide your output with just the post-processed output without an explanation.\n\n[Task Description] -Analyze the \"instruction\" and \"ground-truth label\" to understand the desired output format.\n\n-Post-process the \"model output\" so that it matches the format of the \"ground-truth label\" and is numerically comparable to it.\n\n[Guidelines] -If the \"model output\" can be adjusted to match the format of the \"ground-truth label,\" return only the post-processed result.\n\n-If the \"model output\" is already in an appropriate format, return it as is without changing it.\n\n-If it can not be adjusted into the correct format, does not clearly indicate any of the candidates, is ambiguous, or implies more than one answer, return \"N/A.\" -Do not confuse the \"ground-truth label\" with \"model output\"; you are only supposed to use the \"ground-truth label\" to understand the desired output format.\n\n-Do not tamper with the original content of \"model output\"; you are only supposed to post-process the format of \"model output\" so it matches the format of \"ground-truth label\" and can be compared or evaluated against each other directly.\n\n-The post-processed output should only be a number in numeric form.\n\nBelow you have three examples. Please consider their patterns to understand your task better:\n\n[Example 1 -Inputs] -\"instruction\": \"Please identify the total times of code-switching in this wavefile.\" -\"ground-truth label\": 8 -\"model output\": \"There are no instances of code-switching in this wavefile; it is purely Mandarin speech.\" -Your response: 0 [Example 2 -Inputs] -\"instruction\": \"\"Listen to the audio and assess the clarity and accuracy of pronunciation by considering precise phonology, pronunciation errors, and overall comprehensibility. Use an Arabic numeral from 0 to 10 for your answer.\"\" -\"ground-truth label\": 3 -\"model output\": \"The audio is of a man speaking, in a Chinese, saying, \"I has to find a different back door\". The accuracy of the pronunciation is 90%.\" -Your response: 9\n\n[Example 3 -Inputs] -\"instruction\": \"Please predict the spatial distance(0.5m-4.5m) based on the given audio.\" -\"ground-truth label\": 2.1196093634329904 -\"model output\": \"Based on the given audio, the spatial distance is likely between 0.5m and 4.5m.\" -Your response: N/A\n\n[Inputs] -\"instruction\": {instruction} -\"ground-truth label\": {label} -\"model output\": {model output} -Your response:",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "F Call For Tasks",
      "text": "Here we provide material from the call for tasks. Figure  8  shows a task proposal that we ask each contributor to initiate. Task proposers need to provide a high-level description of the task, explain its importance and challenges, and list the datasets they plan to use along with the dataset licenses. Figure  9  presents the standard README format for each task. In the README, task proposers include the task introduction, its challenges, the dataset, and the evaluation metric. Importantly, we",
      "page_start": 64,
      "page_end": 64
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An overview of Dynamic-SUPERB.",
      "page": 4
    },
    {
      "caption": "Figure 1: depicts the framework of Dynamic-SUPERB Phase-2. Our goal is to evaluate universal",
      "page": 4
    },
    {
      "caption": "Figure 1: a). All tasks in Dynamic-SUPERB Phase-2 are intended solely for",
      "page": 4
    },
    {
      "caption": "Figure 1: b). With the first call for tasks,",
      "page": 4
    },
    {
      "caption": "Figure 1: c depicts, in Dynamic-SUPERB Phase-2, each task is structured to include: (1) Text",
      "page": 4
    },
    {
      "caption": "Figure 1: c). Besides, text inputs are not always required; for",
      "page": 5
    },
    {
      "caption": "Figure 1: c). Therefore, in Dynamic-SUPERB, all classification labels",
      "page": 5
    },
    {
      "caption": "Figure 2: Task taxonomy in Dynamic-SUPERB.",
      "page": 5
    },
    {
      "caption": "Figure 2: a shows, there are 8 domains within the speech field. Speech Recognition focuses on",
      "page": 6
    },
    {
      "caption": "Figure 3: presents a domain-level comparison of relative scores across different models. Whisper-",
      "page": 8
    },
    {
      "caption": "Figure 3: Domain-level relative-score-based performance comparison across different models. We",
      "page": 9
    },
    {
      "caption": "Figure 3: and Table 2, we observed that per-",
      "page": 10
    },
    {
      "caption": "Figure 4: Task taxonomy of speech.",
      "page": 22
    },
    {
      "caption": "Figure 5: Task taxonomy of audio and music.",
      "page": 23
    },
    {
      "caption": "Figure 6: illustrates the prompt for classi-",
      "page": 62
    },
    {
      "caption": "Figure 7: shows the prompt for regression task evaluation. In addition",
      "page": 62
    },
    {
      "caption": "Figure 6: The prompt used for post-processing the classification tasks inference outputs in the study.",
      "page": 63
    },
    {
      "caption": "Figure 7: The prompt used for post-processing the regression tasks inference outputs in the study.",
      "page": 64
    },
    {
      "caption": "Figure 8: shows a task proposal that we ask each",
      "page": 64
    },
    {
      "caption": "Figure 9: presents the standard README format for each task. In the README, task proposers",
      "page": 64
    },
    {
      "caption": "Figure 10: shows the JSON",
      "page": 65
    },
    {
      "caption": "Figure 8: An example of a task proposal from a task contributor.",
      "page": 65
    },
    {
      "caption": "Figure 9: An example of the README file required in the task contribution.",
      "page": 66
    },
    {
      "caption": "Figure 10: An example of the JSON file required in the task contribution.",
      "page": 66
    },
    {
      "caption": "Figure 11: shows the distribution of all audio files in Dynamic-SUPERB Phase-2. Only about 6.5%",
      "page": 67
    },
    {
      "caption": "Figure 11: Duration distribution of all audios in Dynamic-SUPERB Phase-2.",
      "page": 67
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "PR": "PER‚Üì",
          "KS": "Acc‚Üë",
          "IC": "Acc‚Üë",
          "ER": "Acc‚Üë",
          "ASR": "WER‚Üì",
          "QbE": "Acc‚Üë",
          "SF": "F1‚àó‚Üë",
          "Column_9": "CER‚àó‚Üì",
          "SV": "Acc‚Üë",
          "SD": "DER‚àó‚Üì"
        },
        {
          "Column_1": "Whisper-LLaMA\nSALMONN-7B\nSALMONN-13B\nQwen-Audio-Chat\nQwen2-Audio-7B-Inst.\nWavLLM\nLTU-AS\nGAMA-IT\nMu-LLaMA",
          "PR": "100.1\n25.4\n24.6\n100.6\n101.0\n100.0\n102.8\n100.4\n110.3",
          "KS": "36.5\n30.5\n2.0\n60.5\n47.0\n43.0\n1.0\n2.0\n4.0",
          "IC": "36.5\n21.0\n5.5\n26.5\n26.0\n28.0\n0.1\n1.0\n3.5",
          "ER": "12.5\n12.1\n12.5\n70.4\n75.8\n12.1\n25.8\n0.0\n12.9",
          "ASR": "34.0\n15.1\n2.8\n69.4\n36.7\n6.9\n96.3\n116.7\n103.7",
          "QbE": "46.5\n49.0\n51.5\n48.0\n53.5\n49.5\n45.0\n2.5\n51.0",
          "SF": "53.6\n61.6\n29.6\n42.2\n42.4\n50.8\n10.2\n2.7\n1.0",
          "Column_9": "51.3\n61.5\n106.3\n90.1\n55.7\n84.9\n559.3\n4750.0\n15869.6",
          "SV": "45.0\n45.0\n51.0\n49.0\n51.0\n49.0\n44.0\n45.0\n44.0",
          "SD": "1068.7\n-\n-\n8852.2\n4664.4\n3621.2\n14923.3\n187.6\n-"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 8: MARBLE (music) Results. LLM-C stands for LLM Classification, NAR stands for Not",
      "data": [
        {
          "Metric": "LLM-C‚Üë"
        },
        {
          "Metric": "LLM-C‚Üë"
        },
        {
          "Metric": "LLM-C‚Üë"
        },
        {
          "Metric": "LLM-C‚Üë"
        },
        {
          "Metric": "LLM-C‚Üë"
        },
        {
          "Metric": "LLM-C‚Üë"
        },
        {
          "Metric": "NAR‚Üì\nTER‚Üì"
        },
        {
          "Metric": "LLM-C‚Üë"
        },
        {
          "Metric": "LLM-C‚Üë"
        },
        {
          "Metric": "LLM-C‚Üë"
        },
        {
          "Metric": "LLM-C‚Üë"
        },
        {
          "Metric": "NAR‚Üì\nACC‚Üë"
        },
        {
          "Metric": "LLM-C‚Üë"
        }
      ],
      "page": 61
    },
    {
      "caption": "Table 8: MARBLE (music) Results. LLM-C stands for LLM Classification, NAR stands for Not",
      "data": [
        {
          "Metric": "LLM-C‚Üë"
        },
        {
          "Metric": "LLM-C‚Üë"
        },
        {
          "Metric": "LLM-C‚Üë"
        },
        {
          "Metric": "LLM-C‚Üë"
        },
        {
          "Metric": "LLM-C‚Üë"
        },
        {
          "Metric": "NAR‚Üì\nMSE‚Üë"
        },
        {
          "Metric": "LLM-C‚Üë"
        },
        {
          "Metric": "LLM-C‚Üë"
        }
      ],
      "page": 61
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Shyamal Anadkat, et al. Gpt-4 technical report",
      "authors": [
        "Josh Achiam",
        "Steven Adler",
        "Sandhini Agarwal",
        "Lama Ahmad",
        "Ilge Akkaya",
        "Florencia Leoni Aleman",
        "Diogo Almeida",
        "Janko Altenschmidt",
        "Sam Altman"
      ],
      "year": "2023",
      "venue": "Shyamal Anadkat, et al. Gpt-4 technical report",
      "arxiv": "arXiv:2303.08774"
    },
    {
      "citation_id": "2",
      "title": "SemEval-2012 task 6: A pilot on semantic textual similarity",
      "authors": [
        "Eneko Agirre",
        "Daniel Cer",
        "Mona Diab",
        "Aitor Gonzalez-Agirre"
      ],
      "year": "2012",
      "venue": "*SEM 2012: The First Joint Conference on Lexical and Computational Semantics"
    },
    {
      "citation_id": "3",
      "title": "Generating music from text",
      "authors": [
        "Andrea Agostinelli",
        "I Timo",
        "Zal√°n Denk",
        "Jesse Borsos",
        "Mauro Engel",
        "Antoine Verzetti",
        "Qingqing Caillon",
        "Aren Huang",
        "Adam Jansen",
        "Marco Roberts",
        "Tagliasacchi"
      ],
      "year": "2023",
      "venue": "Generating music from text",
      "arxiv": "arXiv:2301.11325"
    },
    {
      "citation_id": "4",
      "title": "Accentdb: A database of non-native english accents to assist neural speech recognition",
      "authors": [
        "Afroz Ahamad",
        "Ankit Anand",
        "Pranesh Bhargava"
      ],
      "year": "2020",
      "venue": "Proceedings of The 12th Language Resources and Evaluation Conference"
    },
    {
      "citation_id": "5",
      "title": "Studying emotion induced by music through a crowdsourcing game",
      "authors": [
        "Anna Aljanaki",
        "Frans Wiering",
        "Remco Veltkamp"
      ],
      "year": "2016",
      "venue": "Emotion and Sentiment in Social and Expressive Media",
      "doi": "10.1016/j.ipm.2015.03.004"
    },
    {
      "citation_id": "6",
      "title": "Modal analysis and transcription of strokes of the mridangam using non-negative matrix factorization",
      "authors": [
        "Akshay Anantapadmanabhan",
        "Ashwin Bellur",
        "Hema Murthy"
      ],
      "year": "2013",
      "venue": "2013 IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "7",
      "title": "Evaluating query-by-example speech search in a zero-resource setting with real-life queries",
      "authors": [
        "Xavier Anguera",
        "Luis-J Rodriguez-Fuentes",
        "Andi Buzo",
        "Florian Metze",
        "Igor Sz√∂ke",
        "Mikel Penagarikano",
        "Quesst"
      ],
      "year": "2014",
      "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/ICASSP.2015.7179090"
    },
    {
      "citation_id": "8",
      "title": "Large language model",
      "authors": [
        "Anthropic",
        "Claude"
      ],
      "year": "2023",
      "venue": "Large language model"
    },
    {
      "citation_id": "9",
      "title": "Common voice: A massively-multilingual speech corpus",
      "authors": [
        "Rosana Ardila",
        "Megan Branson",
        "Kelly Davis",
        "Michael Kohler",
        "Josh Meyer",
        "Michael Henretty",
        "Reuben Morais",
        "Lindsay Saunders",
        "Francis Tyers",
        "Gregor Weber"
      ],
      "year": "2020",
      "venue": "Proceedings of the Twelfth Language Resources and Evaluation Conference"
    },
    {
      "citation_id": "10",
      "title": "Large-scale audio dataset for emergency vehicle sirens and road noises",
      "authors": [
        "Muhammad Asif",
        "Muhammad Usaid",
        "Munaf Rashid",
        "Tabarka Rajab",
        "Samreen Hussain",
        "Sarwar Wasi"
      ],
      "year": "2022",
      "venue": "Scientific data"
    },
    {
      "citation_id": "11",
      "title": "Qwen technical report",
      "authors": [
        "Jinze Bai",
        "Shuai Bai",
        "Yunfei Chu",
        "Zeyu Cui",
        "Kai Dang",
        "Xiaodong Deng",
        "Yang Fan",
        "Wenbin Ge",
        "Yu Han",
        "Fei Huang"
      ],
      "year": "2023",
      "venue": "Qwen technical report",
      "arxiv": "arXiv:2309.16609"
    },
    {
      "citation_id": "12",
      "title": "The Fifth 'CHiME' Speech Separation and Recognition Challenge: Dataset, Task and Baselines",
      "authors": [
        "Jon Barker",
        "Shinji Watanabe",
        "Emmanuel Vincent",
        "Jan Trmal"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech",
      "doi": "10.21437/Interspeech.2018-1768"
    },
    {
      "citation_id": "13",
      "title": "SLURP: A Spoken Language Understanding Resource Package",
      "authors": [
        "Emanuele Bastianelli",
        "Andrea Vanzo",
        "Pawel Swietojanski",
        "Verena Rieser"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "14",
      "title": "Audiomnist: Exploring explainable artificial intelligence for audio analysis on a simple benchmark",
      "authors": [
        "S√∂ren Becker",
        "Johanna Vielhaben",
        "Marcel Ackermann",
        "Klaus-Robert M√ºller",
        "Sebastian Lapuschkin",
        "Wojciech Samek"
      ],
      "year": "2024",
      "venue": "Journal of the Franklin Institute"
    },
    {
      "citation_id": "15",
      "title": "Real-time detection of ai-generated speech for deepfake voice conversion",
      "authors": [
        "J Jordan",
        "Ahmad Bird",
        "Lotfi"
      ],
      "year": "2023",
      "venue": "Real-time detection of ai-generated speech for deepfake voice conversion",
      "arxiv": "arXiv:2308.12734"
    },
    {
      "citation_id": "16",
      "title": "Visit-bench: a benchmark for vision-language instruction following inspired by real-world use",
      "authors": [
        "Yonatan Bitton",
        "Hritik Bansal",
        "Jack Hessel",
        "Rulin Shao",
        "Wanrong Zhu",
        "Anas Awadalla",
        "Josh Gardner",
        "Rohan Taori",
        "Ludwig Schimdt"
      ],
      "year": "2023",
      "venue": "Proceedings of the 37th International Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "17",
      "title": "The mtg-jamendo dataset for automatic music tagging",
      "authors": [
        "Dmitry Bogdanov",
        "Minz Won",
        "Philip Tovstogan",
        "Alastair Porter",
        "Xavier Serra"
      ],
      "year": "2019",
      "venue": "ICML"
    },
    {
      "citation_id": "18",
      "title": "ISMIR04 Genre Identification task dataset (1.0)",
      "authors": [
        "P Cano",
        "N Wack",
        "P Herrera"
      ],
      "year": "2018",
      "venue": "ISMIR04 Genre Identification task dataset (1.0)",
      "doi": "10.5281/zenodo.1302992"
    },
    {
      "citation_id": "19",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "Houwei Cao",
        "David Cooper",
        "Michael Keutmann",
        "Ruben Gur",
        "Ani Nenkova",
        "Ragini Verma"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2014.2336244"
    },
    {
      "citation_id": "20",
      "title": "The ami meeting corpus: A pre-announcement",
      "authors": [
        "Jean Carletta",
        "Simone Ashby",
        "Sebastien Bourban",
        "Mike Flynn",
        "Mael Guillemot",
        "Thomas Hain",
        "Jaroslav Kadlec",
        "Vasilis Karaiskos",
        "Wessel Kraaij",
        "Melissa Kronenthal"
      ],
      "year": "2005",
      "venue": "International workshop on machine learning for multimodal interaction"
    },
    {
      "citation_id": "21",
      "title": "Towards multimodal sarcasm detection (an Obviously perfect paper)",
      "authors": [
        "Santiago Castro",
        "Devamanyu Hazarika",
        "Ver√≥nica P√©rez-Rosas",
        "Roger Zimmermann"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P19-1455"
    },
    {
      "citation_id": "22",
      "title": "High school english listening exam",
      "authors": [
        "Ceec"
      ],
      "venue": "High school english listening exam"
    },
    {
      "citation_id": "23",
      "title": "Stress detection of english words for a capt system using word-length dependent gmm-based bayesian classifiers",
      "authors": [
        "Liang-Yu Chen",
        "Jyh-Shing Roger"
      ],
      "year": "2012",
      "venue": "Interdisciplinary Information Sciences",
      "doi": "10.4036/iis.2012.65"
    },
    {
      "citation_id": "24",
      "title": "Can large language models be an alternative to human evaluations?",
      "authors": [
        "Cheng-Han Chiang",
        "Hung-Yi Lee"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "25",
      "title": "Phonetic segmentation of the UCLA phonetics lab archive",
      "authors": [
        "Eleanor Chodroff",
        "Bla≈æ Pa≈æon",
        "Annie Baker",
        "Steven Moran"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)"
    },
    {
      "citation_id": "26",
      "title": "Children's song dataset for singing voice research",
      "authors": [
        "Soonbeom Choi",
        "Won Il Kim",
        "Sae Byul Park",
        "Sangeon Yong",
        "Juhan Nam"
      ],
      "year": "2020",
      "venue": "The 21th International Society for Music Information Retrieval Conference"
    },
    {
      "citation_id": "27",
      "title": "Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models",
      "authors": [
        "Yunfei Chu",
        "Jin Xu",
        "Xiaohuan Zhou",
        "Qian Yang",
        "Shiliang Zhang",
        "Zhijie Yan",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "year": "2023",
      "venue": "Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models",
      "arxiv": "arXiv:2311.07919"
    },
    {
      "citation_id": "28",
      "title": "Qwen2-audio technical report",
      "authors": [
        "Yunfei Chu",
        "Jin Xu",
        "Qian Yang",
        "Haojie Wei",
        "Xipin Wei",
        "Zhifang Guo",
        "Yichong Leng",
        "Yuanjun Lv",
        "Jinzheng He",
        "Junyang Lin"
      ],
      "year": "2024",
      "venue": "Qwen2-audio technical report",
      "arxiv": "arXiv:2407.10759"
    },
    {
      "citation_id": "29",
      "title": "Librimix: An open-source dataset for generalizable speech separation",
      "authors": [
        "Joris Cosentino",
        "Manuel Pariente",
        "Samuele Cornell",
        "Antoine Deleforge",
        "Emmanuel Vincent"
      ],
      "year": "2020",
      "venue": "Librimix: An open-source dataset for generalizable speech separation"
    },
    {
      "citation_id": "30",
      "title": "Prosaudit, a prosodic benchmark for self-supervised speech models",
      "authors": [
        "Maureen De Seyssel",
        "Marvin Lavechin",
        "Hadrien Titeux",
        "Arthur Thomas",
        "Gwendal Virlet",
        "Andrea Revilla",
        "Guillaume Wisniewski",
        "Bogdan Ludusan",
        "Emmanuel Dupoux"
      ],
      "year": "2023",
      "venue": "INTERSPEECH 2023",
      "doi": "10.21437/Interspeech.2023-438"
    },
    {
      "citation_id": "31",
      "title": "Musical instrument chord classification",
      "authors": [
        "Deepcontractor"
      ],
      "year": "2024",
      "venue": "Musical instrument chord classification"
    },
    {
      "citation_id": "32",
      "title": "Fma: A dataset for music analysis",
      "authors": [
        "Micha√´l Defferrard",
        "Kirell Benzi",
        "Pierre Vandergheynst",
        "Xavier Bresson"
      ],
      "year": "2017",
      "venue": "18th International Society for Music Information Retrieval Conference"
    },
    {
      "citation_id": "33",
      "title": "Asvspoof 2017 version 2.0: meta-data analysis and baseline enhancements",
      "authors": [
        "H√©ctor Delgado",
        "Massimiliano Todisco",
        "Md Sahidullah",
        "Nicholas Evans",
        "Tomi Kinnunen",
        "Kong Aik Lee",
        "Junichi Yamagishi"
      ],
      "year": "2018",
      "venue": "The Speaker and Language Recognition Workshop",
      "doi": "10.21437/Odyssey.2018-42"
    },
    {
      "citation_id": "34",
      "title": "Clotho: an audio captioning dataset",
      "authors": [
        "Konstantinos Drossos",
        "Samuel Lipping",
        "Tuomas Virtanen"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "doi": "10.1109/ICASSP40776.2020.9052990"
    },
    {
      "citation_id": "35",
      "title": "The llama 3 herd of models",
      "authors": [
        "Abhimanyu Dubey",
        "Abhinav Jauhri",
        "Abhinav Pandey",
        "Abhishek Kadian",
        "Ahmad Al-Dahle",
        "Aiesha Letman",
        "Akhil Mathur",
        "Alan Schelten",
        "Amy Yang",
        "Angela Fan"
      ],
      "year": "2024",
      "venue": "The llama 3 herd of models",
      "arxiv": "arXiv:2407.21783"
    },
    {
      "citation_id": "36",
      "title": "The Zero Resource Speech Challenge 2021: Spoken Language Modelling",
      "authors": [
        "Ewan Dunbar",
        "Mathieu Bernard",
        "Nicolas Hamilakis",
        "Anh Tu",
        "Maureen Nguyen",
        "Patricia De Seyssel",
        "Morgane Roz√©",
        "Eugene Rivi√®re",
        "Emmanuel Kharitonov",
        "Dupoux"
      ],
      "venue": "INTERSPEECH 2021",
      "doi": "10.21437/Interspeech.2021-1755"
    },
    {
      "citation_id": "37",
      "title": "Self-supervised language learning from raw audio: Lessons from the zero resource speech challenge",
      "authors": [
        "Ewan Dunbar",
        "Nicolas Hamilakis",
        "Emmanuel Dupoux"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "38",
      "title": "Neural audio synthesis of musical notes with wavenet autoencoders",
      "authors": [
        "Jesse Engel",
        "Cinjon Resnick",
        "Adam Roberts",
        "Sander Dieleman",
        "Mohammad Norouzi",
        "Douglas Eck",
        "Karen Simonyan"
      ],
      "year": "2017",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "39",
      "title": "Franc ¬∏ois Portet, Solange Rossato, Fabien Ringeval, Didier Schwab, and Laurent Besacier. Lebenchmark: A reproducible framework for assessing self-supervised representation learning from speech",
      "authors": [
        "Sol√®ne Evain",
        "Ha Nguyen",
        "Hang Le",
        "Marcely Zanon Boito",
        "Salima Mdhaffar",
        "Sina Alisamir",
        "Ziyi Tong",
        "Natalia Tomashenko",
        "Marco Dinarelli",
        "Titouan Parcollet",
        "Alexandre Allauzen",
        "Yannick Est√®ve",
        "Benjamin Lecouteux"
      ],
      "year": "2021",
      "venue": "Interspeech 2021",
      "doi": "10.21437/Interspeech.2021-556"
    },
    {
      "citation_id": "40",
      "title": "ASAP: a dataset of aligned scores and performances for piano transcription",
      "authors": [
        "Francesco Foscarin",
        "Andrew Mcleod",
        "Philippe Rigaux",
        "Florent Jacquemard",
        "Masahiko Sakai"
      ],
      "year": "2020",
      "venue": "International Society for Music Information Retrieval Conference (ISMIR)"
    },
    {
      "citation_id": "41",
      "title": "Wavefake: A data set to facilitate audio deepfake detection",
      "authors": [
        "Joel Frank",
        "Lea Sch√∂nherr"
      ],
      "venue": "Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track"
    },
    {
      "citation_id": "42",
      "title": "The people's speech: A large-scale diverse english speech recognition dataset for commercial usage",
      "authors": [
        "Daniel Galvez",
        "Greg Diamos",
        "Juan Torres",
        "Keith Achorn",
        "Juan Cer√≥n",
        "Anjali Gopi",
        "David Kanter",
        "Max Lam",
        "Mark Mazumder",
        "Vijay Reddi"
      ],
      "year": "2021",
      "venue": "Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks"
    },
    {
      "citation_id": "43",
      "title": "Gama: A large audio-language model with advanced audio understanding and complex reasoning abilities",
      "authors": [
        "Sreyan Ghosh",
        "Sonal Kumar",
        "Ashish Seth",
        "Chandra Kiran Reddy",
        "Utkarsh Evuru",
        "S Tyagi",
        "Oriol Sakshi",
        "Ramani Nieto",
        "Dinesh Duraiswami",
        "Manocha"
      ],
      "year": "2024",
      "venue": "Gama: A large audio-language model with advanced audio understanding and complex reasoning abilities",
      "arxiv": "arXiv:2406.11768"
    },
    {
      "citation_id": "44",
      "title": "Cochleanet: A robust languageindependent audio-visual model for real-time speech enhancement",
      "authors": [
        "Mandar Gogate",
        "Kia Dashtipour",
        "Ahsan Adeel",
        "Amir Hussain"
      ],
      "year": "2020",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "45",
      "title": "Physiobank, physiotoolkit, and physionet: components of a new research resource for complex physiologic signals",
      "authors": [
        "Ary L Goldberger",
        "A Luis",
        "Leon Amaral",
        "Jeffrey Glass",
        "Plamen Hausdorff",
        "Roger Ch Ivanov",
        "Joseph Mark",
        "George Mietus",
        "Chung-Kang Moody",
        "H Eugene Peng",
        "Stanley"
      ],
      "year": "2000",
      "venue": "Physiobank, physiotoolkit, and physionet: components of a new research resource for complex physiologic signals"
    },
    {
      "citation_id": "46",
      "title": "Vocalsound: A dataset for improving human vocal sounds recognition",
      "authors": [
        "Yuan Gong",
        "Jin Yu",
        "James Glass"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "doi": "10.1109/ICASSP43922.2022.9746828"
    },
    {
      "citation_id": "47",
      "title": "Joint audio and speech understanding",
      "authors": [
        "Yuan Gong",
        "Alexander Liu",
        "Hongyin Luo",
        "Leonid Karlinsky",
        "James Glass"
      ],
      "year": "2023",
      "venue": "2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)"
    },
    {
      "citation_id": "48",
      "title": "Prompttts: Controllable text-tospeech with text descriptions",
      "authors": [
        "Zhifang Guo",
        "Yichong Leng",
        "Yihan Wu",
        "Sheng Zhao",
        "Xu Tan"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "49",
      "title": "Enabling factorized piano music modeling and generation with the MAESTRO dataset",
      "authors": [
        "Curtis Hawthorne",
        "Andriy Stasyuk",
        "Adam Roberts",
        "Ian Simon",
        "Cheng-Zhi Anna Huang",
        "Sander Dieleman",
        "Erich Elsen",
        "Jesse Engel",
        "Douglas Eck"
      ],
      "year": "2019",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "50",
      "title": "Cornell birdcall identification",
      "authors": [
        "Addison Howard"
      ],
      "year": "2020",
      "venue": "Cornell birdcall identification"
    },
    {
      "citation_id": "51",
      "title": "Lora: Low-rank adaptation of large language models",
      "authors": [
        "J Edward",
        "Phillip Hu",
        "Zeyuan Wallis",
        "Yuanzhi Allen-Zhu",
        "Shean Li",
        "Lu Wang",
        "Weizhu Wang",
        "Chen"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "52",
      "title": "Towards robust and adaptive speech large language model",
      "authors": [
        "Shujie Hu",
        "Long Zhou",
        "Shujie Liu",
        "Sanyuan Chen",
        "Hongkun Hao",
        "Jing Pan",
        "Xunying Liu",
        "Jinyu Li",
        "Sunit Sivasankaran",
        "Linquan Liu"
      ],
      "year": "2024",
      "venue": "Towards robust and adaptive speech large language model",
      "arxiv": "arXiv:2404.00656"
    },
    {
      "citation_id": "53",
      "title": "Dynamic-superb: Towards a dynamic, collaborative, and comprehensive instruction-tuning benchmark for speech",
      "authors": [
        "Chien-Yu Huang",
        "Ke-Han Lu",
        "Shih-Heng Wang",
        "Chi-Yuan Hsiao",
        "Chun-Yi Kuan",
        "Haibin Wu",
        "Siddhant Arora",
        "Kai-Wei Chang",
        "Jiatong Shi",
        "Yifan Peng"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "54",
      "title": "Zero resource code-switched speech benchmark using speech utterance pairs for multiple spoken languages",
      "authors": [
        "Kuan-Po Huang",
        "Chih-Kai Yang",
        "Yu-Kuan Fu",
        "Ewan Dunbar",
        "Hung-Yi Lee"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "55",
      "title": "Openmic-2018: An open data-set for multiple instrument recognition",
      "authors": [
        "Eric Humphrey",
        "Simon Durand",
        "Brian Mcfee"
      ],
      "year": "2018",
      "venue": "ISMIR"
    },
    {
      "citation_id": "56",
      "title": "IEEE DCASE 2016 Challenge",
      "authors": [
        "Ieee Dcase"
      ],
      "year": "2016",
      "venue": "IEEE DCASE 2016 Challenge"
    },
    {
      "citation_id": "57",
      "title": "The lj speech dataset",
      "authors": [
        "Keith Ito",
        "Linda Johnson"
      ],
      "year": "2017",
      "venue": "The lj speech dataset"
    },
    {
      "citation_id": "58",
      "title": "Indicsuperb: A speech processing universal performance benchmark for indian languages",
      "authors": [
        "Tahir Javed",
        "Kaushal Bhogale",
        "Abhigyan Raman",
        "Pratyush Kumar",
        "Anoop Kunchukuttan",
        "Mitesh Khapra"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "59",
      "title": "Interpersonal relationship labels for the CALLHOME corpus",
      "authors": [
        "Denys Katerenchuk",
        "David Guy Brizan",
        "Andrew Rosenberg ; Nicoletta Calzolari",
        "Khalid Choukri",
        "Christopher Cieri",
        "Thierry Declerck",
        "Sara Goggi",
        "Koiti Hasida",
        "Hitoshi Isahara",
        "Bente Maegaard",
        "Joseph Mariani",
        "H√©l√®ne Mazo"
      ],
      "year": "2018",
      "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)"
    },
    {
      "citation_id": "60",
      "title": "The corpus of regional african american language",
      "authors": [
        "Tyler Kendall",
        "Charlie Farrington"
      ],
      "year": "2023",
      "venue": "The corpus of regional african american language",
      "doi": "10.7264/1ad5-6t35"
    },
    {
      "citation_id": "61",
      "title": "Vocal imitation set: a dataset of vocally imitated sound events using the audioset ontology",
      "authors": [
        "Bongjun Kim",
        "Madhav Ghei",
        "Bryan Pardo",
        "Zhiyao Duan"
      ],
      "year": "2018",
      "venue": "DCASE"
    },
    {
      "citation_id": "62",
      "title": "Two data sets for tempo estimation and key detection in electronic dance music annotated from user corrections",
      "authors": [
        "Peter Knees",
        "Faraldo √Ångel",
        "Herrera P√©rez",
        "Richard Boyer",
        "Sebastian Vogl",
        "Florian B√∂ck",
        "H√∂rschl√§ger",
        "Le Mickael",
        "Goff"
      ],
      "year": "2015",
      "venue": "Proceedings of the 16th International Society for Music Information Retrieval Conference (ISMIR)"
    },
    {
      "citation_id": "63",
      "title": "A study on data augmentation of reverberant speech for robust speech recognition",
      "authors": [
        "Tom Ko",
        "Vijayaditya Peddinti",
        "Daniel Povey",
        "Michael Seltzer",
        "Sanjeev Khudanpur"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/ICASSP.2017.7953152"
    },
    {
      "citation_id": "64",
      "title": "Evaluation of algorithms using games: The case of music tagging",
      "authors": [
        "Edith Law",
        "Kris West",
        "I Michael",
        "Mert Mandel",
        "J Bay",
        "Stephen Downie"
      ],
      "year": "2009",
      "venue": "ISMIR"
    },
    {
      "citation_id": "65",
      "title": "Sep-28k: A dataset for stuttering event detection from podcasts with people who stutter",
      "authors": [
        "Colin Lea",
        "Vikramjit Mitra",
        "Aparna Joshi",
        "Sachin Kajarekar",
        "Jeffrey Bigham"
      ],
      "year": "2021",
      "venue": "ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/ICASSP39728.2021.9413520"
    },
    {
      "citation_id": "66",
      "title": "Dailytalk: Spoken dialogue dataset for conversational text-to-speech",
      "authors": [
        "Keon Lee",
        "Kyumin Park",
        "Daeyoung Kim"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "67",
      "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
      "authors": [
        "Junnan Li",
        "Dongxu Li",
        "Silvio Savarese",
        "Steven Hoi"
      ],
      "year": "2023",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "68",
      "title": "MERT: Acoustic music understanding model with large-scale self-supervised training",
      "authors": [
        "L Yizhi",
        "Ruibin Yuan",
        "Ge Zhang",
        "Yinghao Ma",
        "Xingran Chen",
        "Hanzhi Yin",
        "Chenghao Xiao",
        "Chenghua Lin",
        "Anton Ragni",
        "Emmanouil Benetos",
        "Norbert Gyenge",
        "Roger Dannenberg",
        "Ruibo Liu",
        "Wenhu Chen",
        "Gus Xia",
        "Yemin Shi",
        "Wenhao Huang",
        "Zili Wang",
        "Yike Guo",
        "Jie Fu"
      ],
      "year": "2024",
      "venue": "The Twelfth International Conference on Learning Representations"
    },
    {
      "citation_id": "69",
      "title": "Textbooks are all you need ii: phi-1.5 technical report",
      "authors": [
        "Yuanzhi Li",
        "S√©bastien Bubeck",
        "Ronen Eldan",
        "Allie Del Giorno",
        "Suriya Gunasekar",
        "Yin Tat"
      ],
      "year": "2023",
      "venue": "Textbooks are all you need ii: phi-1.5 technical report",
      "arxiv": "arXiv:2309.05463"
    },
    {
      "citation_id": "70",
      "title": "Paralinguistics-enhanced large language modeling of spoken dialogue",
      "authors": [
        "Guan-Ting Lin",
        "Prashanth Gurunath Shivakumar",
        "Ankur Gandhe",
        "Chao-Han Huck",
        "Yile Yang",
        "Shalini Gu",
        "Andreas Ghosh",
        "Hung-Yi Stolcke",
        "Ivan Lee",
        "Bulyko"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "71",
      "title": "Audioldm: text-to-audio generation with latent diffusion models",
      "authors": [
        "Haohe Liu",
        "Zehua Chen",
        "Yi Yuan",
        "Xinhao Mei",
        "Xubo Liu",
        "Danilo Mandic",
        "Wenwu Wang",
        "Mark Plumbley"
      ],
      "year": "2023",
      "venue": "Proceedings of the 40th International Conference on Machine Learning"
    },
    {
      "citation_id": "72",
      "title": "Music understanding llama: Advancing text-to-music generation with question answering and captioning",
      "authors": [
        "Shansong Liu",
        "Atin Sakkeer Hussain",
        "Chenshuo Sun",
        "Ying Shan"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "73",
      "title": "G-eval: Nlg evaluation using gpt-4 with better human alignment",
      "authors": [
        "Yang Liu",
        "Dan Iter",
        "Yichong Xu",
        "Shuohang Wang",
        "Ruochen Xu",
        "Chenguang Zhu"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "74",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "R Steven",
        "Frank Livingstone",
        "Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "75",
      "title": "Ascend: A spontaneous chinese-english dataset for code-switching in multi-turn conversation",
      "authors": [
        "Samuel Holy Lovenia",
        "Genta Cahyawijaya",
        "Peng Indra Winata",
        "Xu Xu",
        "Zihan Yan",
        "Rita Liu",
        "Tiezheng Frieske",
        "Wenliang Yu",
        "Elham Dai",
        "Barezi"
      ],
      "venue": "Proceedings of the 13th Language Resources and Evaluation Conference (LREC)"
    },
    {
      "citation_id": "76",
      "title": "",
      "authors": [
        "Ken Maclean",
        "Ken Voxforge",
        "Maclean"
      ],
      "year": "2018",
      "venue": ""
    },
    {
      "citation_id": "77",
      "title": "A suite for acoustic language model evaluation",
      "authors": [
        "Amit Gallil Maimon",
        "Yossi Roth",
        "Adi"
      ],
      "year": "2024",
      "venue": "A suite for acoustic language model evaluation",
      "arxiv": "arXiv:2409.07437"
    },
    {
      "citation_id": "78",
      "title": "Cross-task generalization via natural language crowdsourcing instructions",
      "authors": [
        "Swaroop Mishra",
        "Daniel Khashabi",
        "Chitta Baral",
        "Hannaneh Hajishirzi"
      ],
      "year": "2022",
      "venue": "ACL"
    },
    {
      "citation_id": "79",
      "title": "The nccu (national chengchi university) corpus of spoken taiwan mandarin",
      "venue": "The nccu (national chengchi university) corpus of spoken taiwan mandarin"
    },
    {
      "citation_id": "80",
      "title": "Audio-based identification of beehive states",
      "authors": [
        "In√™s Nolasco",
        "Alessandro Terenzi",
        "Stefania Cecchi",
        "Simone Orcioni",
        "Helen Bear",
        "Emmanouil Benetos"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "81",
      "title": "The coughvid crowdsourcing dataset, a corpus for the study of large-scale cough analysis algorithms",
      "authors": [
        "Lara Orlandic",
        "Tomas Teijeiro",
        "David Atienza"
      ],
      "year": "2021",
      "venue": "Scientific Data"
    },
    {
      "citation_id": "82",
      "title": "Librispeech: an asr corpus based on public domain audio books",
      "authors": [
        "Vassil Panayotov",
        "Guoguo Chen",
        "Daniel Povey",
        "Sanjeev Khudanpur"
      ],
      "year": "2015",
      "venue": "2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "83",
      "title": "Domestic cat sound classification using transfer learning",
      "authors": [
        "Raj Yagya",
        "Joonwhoan Pandeya",
        "Lee"
      ],
      "year": "2018",
      "venue": "International Journal of Fuzzy Logic and Intelligent Systems"
    },
    {
      "citation_id": "84",
      "title": "Domestic cat sound classification using learned features from deep neural nets",
      "authors": [
        "Raj Yagya",
        "Dongwhoon Pandeya",
        "Joonwhoan Kim",
        "Lee"
      ],
      "year": "2018",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "85",
      "title": "Lebenchmark 2.0: a standardized, replicable and enhanced framework for self-supervised representations of french speech",
      "authors": [
        "Titouan Parcollet",
        "Ha Nguyen",
        "Solene Evain",
        "Zanon Marcely",
        "Adrien Boito",
        "Salima Pupier",
        "Hang Mdhaffar",
        "Sina Le",
        "Natalia Alisamir",
        "Marco Tomashenko",
        "Dinarelli"
      ],
      "year": "2023",
      "venue": "Lebenchmark 2.0: a standardized, replicable and enhanced framework for self-supervised representations of french speech",
      "arxiv": "arXiv:2309.05472"
    },
    {
      "citation_id": "86",
      "title": "Esc: Dataset for environmental sound classification",
      "authors": [
        "J Karol",
        "Piczak"
      ],
      "year": "2015",
      "venue": "Proceedings of the 23rd ACM international conference on Multimedia"
    },
    {
      "citation_id": "87",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Gautam Naik",
        "Erik Cambria",
        "Rada Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P19-1050"
    },
    {
      "citation_id": "88",
      "title": "Robocall audio from the ftc's project point of no entry",
      "authors": [
        "Sathvik Prasad",
        "Bradley Reaves"
      ],
      "venue": "Robocall audio from the ftc's project point of no entry"
    },
    {
      "citation_id": "89",
      "title": "Mls: A large-scale multilingual dataset for speech research",
      "authors": [
        "Qiantong Vineel Pratap",
        "Anuroop Xu",
        "Gabriel Sriram",
        "Ronan Synnaeve",
        "Collobert"
      ],
      "year": "2020",
      "venue": "Interspeech 2020",
      "doi": "10.21437/Interspeech.2020-2826"
    },
    {
      "citation_id": "90",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "year": "2023",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "91",
      "title": "MUSDB18-HQ -an uncompressed version of musdb18",
      "authors": [
        "Zafar Rafii",
        "Antoine Liutkus",
        "Fabian-Robert St√∂ter"
      ],
      "year": "2019",
      "venue": "Stylianos Ioannis Mimilakis, and Rachel Bittner",
      "doi": "10.5281/zenodo.3338373"
    },
    {
      "citation_id": "92",
      "title": "Nonspeech7k dataset: Classification and analysis of human non-speech sound",
      "authors": [
        "Guiqing Muhammad Mamunur Rashid",
        "Chengrui Li",
        "Du"
      ],
      "year": "2023",
      "venue": "IET Signal Processing"
    },
    {
      "citation_id": "93",
      "title": "General purpose audio effect removal",
      "authors": [
        "Matthew Rice",
        "Christian Steinmetz",
        "George Fazekas",
        "Joshua Reiss"
      ],
      "year": "2023",
      "venue": "IEEE Workshop on Applications of Signal Processing to Audio and Acoustics"
    },
    {
      "citation_id": "94",
      "title": "A dataset and taxonomy for urban sound research",
      "authors": [
        "J Salamon",
        "C Jacoby",
        "J Bello"
      ],
      "year": "2014",
      "venue": "22nd ACM International Conference on Multimedia (ACM-MM'14)"
    },
    {
      "citation_id": "95",
      "title": "Spatial librispeech: An augmented dataset for spatial audio learning",
      "authors": [
        "Miguel Sarabia",
        "Elena Menyaylenko",
        "Alessandro Toso",
        "Skyler Seto",
        "Zakaria Aldeneh",
        "Shadi Pirhosseinloo",
        "Luca Zappella",
        "Barry-John Theobald",
        "Nicholas Apostoloff",
        "Jonathan Sheaffer"
      ],
      "year": "2023",
      "venue": "INTERSPEECH 2023",
      "doi": "10.21437/Interspeech.2023-2117"
    },
    {
      "citation_id": "96",
      "title": "Ml-superb: Multilingual speech universal performance benchmark",
      "authors": [
        "Jiatong Shi",
        "Dan Berrebbi",
        "William Chen",
        "En-Pei Hu",
        "Wei-Ping Huang",
        "Ho-Lam Chung",
        "Xuankai Chang",
        "Shang-Wen Li",
        "Abdelrahman Mohamed",
        "Hung Yi Lee",
        "Shinji Watanabe"
      ],
      "year": "2023",
      "venue": "INTERSPEECH 2023",
      "doi": "10.21437/Interspeech.2023-1316"
    },
    {
      "citation_id": "97",
      "title": "Singing voice data scaling-up: An introduction to ace-opencpop and ace-kising",
      "authors": [
        "Jiatong Shi",
        "Yueqian Lin",
        "Xinyi Bai",
        "Keyi Zhang",
        "Yuning Wu",
        "Yuxun Tang",
        "Yifeng Yu",
        "Qin Jin",
        "Shinji Watanabe"
      ],
      "year": "2024",
      "venue": "Singing voice data scaling-up: An introduction to ace-opencpop and ace-kising",
      "arxiv": "arXiv:2401.17619"
    },
    {
      "citation_id": "98",
      "title": "Mlsuperb 2.0: Benchmarking multilingual speech models across modeling constraints, languages, and datasets",
      "authors": [
        "Jiatong Shi",
        "Shih-Heng Wang",
        "William Chen",
        "Martijn Bartelds",
        "Vanya Kumar",
        "Jinchuan Tian",
        "Xuankai Chang",
        "Dan Jurafsky",
        "Karen Livescu",
        "Hung Yi Lee",
        "Shinji Watanabe"
      ],
      "year": "2024",
      "venue": "Interspeech 2024",
      "doi": "10.21437/Interspeech.2024-2248"
    },
    {
      "citation_id": "99",
      "title": "Starss23: An audio-visual dataset of spatial recordings of real scenes with spatiotemporal annotations of sound events",
      "authors": [
        "Kazuki Shimada",
        "Archontis Politis",
        "Parthasaarathy Sudarsanam",
        "Daniel Krause",
        "Kengo Uchida",
        "Sharath Adavanne",
        "Aapo Hakala",
        "Yuichiro Koyama",
        "Naoya Takahashi",
        "Shusuke Takahashi"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "100",
      "title": "Slue: New benchmark tasks for spoken language understanding evaluation on natural speech",
      "authors": [
        "Suwon Shon",
        "Ankita Pasad",
        "Felix Wu",
        "Pablo Brusco",
        "Yoav Artzi",
        "Karen Livescu",
        "Kyu J Han"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "101",
      "title": "Slue phase-2: A benchmark suite of diverse spoken language understanding tasks",
      "authors": [
        "Suwon Shon",
        "Siddhant Arora",
        "Chyi-Jiunn Lin",
        "Ankita Pasad",
        "Felix Wu",
        "Roshan Sharma",
        "Wei-Lun Wu",
        "Hung-Yi Lee",
        "Karen Livescu",
        "Shinji Watanabe"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "102",
      "title": "A music, speech, and noise corpus",
      "authors": [
        "David Snyder",
        "Guoguo Chen",
        "Daniel Povey",
        "Musan"
      ],
      "year": "2015",
      "venue": "A music, speech, and noise corpus",
      "arxiv": "arXiv:1510.08484"
    },
    {
      "citation_id": "103",
      "title": "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
      "authors": [
        "Aarohi Srivastava",
        "Abhinav Rastogi",
        "Abhishek Rao",
        "Abu Awal",
        "Md Shoeb",
        "Abubakar Abid",
        "Adam Fisch",
        "Adam Adam R Brown",
        "Aditya Santoro",
        "Adri√† Gupta",
        "Garriga-Alonso"
      ],
      "year": "2022",
      "venue": "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
      "arxiv": "arXiv:2206.04615"
    },
    {
      "citation_id": "104",
      "title": "LibriCount, a dataset for speaker count estimation",
      "authors": [
        "Fabian-Robert St√∂ter",
        "Soumitro Chakrabarty",
        "Emanu√´l Habets",
        "Bernd Edler"
      ],
      "year": "2018",
      "venue": "LibriCount, a dataset for speaker count estimation",
      "doi": "10.5281/zenodo.1216072"
    },
    {
      "citation_id": "105",
      "title": "Automatic acoustic detection of birds through deep learning: the first bird audio detection challenge",
      "authors": [
        "Dan Stowell",
        "D Michael",
        "Hanna Wood",
        "Yannis Pamu≈Ça",
        "Herv√© Stylianou",
        "Glotin"
      ],
      "year": "2019",
      "venue": "Methods in Ecology and Evolution"
    },
    {
      "citation_id": "106",
      "title": "Salmonn: Towards generic hearing abilities for large language models",
      "authors": [
        "Changli Tang",
        "Wenyi Yu",
        "Guangzhi Sun",
        "Xianzhao Chen",
        "Tian Tan",
        "Wei Li",
        "Lu Lu",
        "Chao Ma Zejun",
        "Zhang"
      ],
      "year": "2024",
      "venue": "The Twelfth International Conference on Learning Representations"
    },
    {
      "citation_id": "107",
      "title": "Singmos: An extensive open-source singing voice dataset for mos prediction",
      "authors": [
        "Yuxun Tang",
        "Jiatong Shi",
        "Yuning Wu",
        "Qin Jin"
      ],
      "year": "2024",
      "venue": "Singmos: An extensive open-source singing voice dataset for mos prediction",
      "arxiv": "arXiv:2406.10911"
    },
    {
      "citation_id": "108",
      "title": "A study of instrument-wise onset detection in beijing opera percussion ensembles",
      "authors": [
        "Mi Tian",
        "Ajay Srinivasamurthy",
        "Mark Sandler",
        "Xavier Serra"
      ],
      "year": "2014",
      "venue": "2014 ieee international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "109",
      "title": "Open and efficient foundation language models",
      "authors": [
        "Hugo Touvron",
        "Thibaut Lavril",
        "Gautier Izacard",
        "Xavier Martinet",
        "Marie-Anne Lachaux",
        "Timoth√©e Lacroix",
        "Baptiste Rozi√®re",
        "Naman Goyal",
        "Eric Hambro",
        "Faisal Azhar"
      ],
      "year": "2023",
      "venue": "Open and efficient foundation language models",
      "arxiv": "arXiv:2302.13971"
    },
    {
      "citation_id": "110",
      "title": "Llama 2: Open foundation and fine-tuned chat models",
      "authors": [
        "Hugo Touvron",
        "Louis Martin",
        "Kevin Stone",
        "Peter Albert",
        "Amjad Almahairi",
        "Yasmine Babaei",
        "Nikolay Bashlykov",
        "Soumya Batra",
        "Prajjwal Bhargava",
        "Shruti Bhosale"
      ],
      "year": "2023",
      "venue": "Llama 2: Open foundation and fine-tuned chat models",
      "arxiv": "arXiv:2307.09288"
    },
    {
      "citation_id": "111",
      "title": "English lexical stress detection and sentence-based intonation assessment based on contour shape description. Master's thesis",
      "authors": [
        "Sheng-Chi Tsai"
      ],
      "year": "2015",
      "venue": "English lexical stress detection and sentence-based intonation assessment based on contour shape description. Master's thesis"
    },
    {
      "citation_id": "112",
      "title": "An initial study on stress detection for spoken english",
      "authors": [
        "Ching-Yu Tseng"
      ],
      "year": "2008",
      "venue": "An initial study on stress detection for spoken english"
    },
    {
      "citation_id": "113",
      "title": "Hear: Holistic evaluation of audio representations",
      "authors": [
        "Joseph Turian",
        "Jordie Shier",
        "Raj Humair",
        "Bhiksha Khan",
        "Raj",
        "W Bj√∂rn",
        "Christian Schuller",
        "Colin Steinmetz",
        "George Malloy",
        "Gissel Tzanetakis",
        "Kirk Velarde",
        "Mcnally"
      ],
      "year": "2022",
      "venue": "NeurIPS 2021 Competitions and Demonstrations Track"
    },
    {
      "citation_id": "114",
      "title": "Sound event detection in domestic environments with weakly labeled data and soundscape synthesis",
      "authors": [
        "Nicolas Turpault",
        "Romain Serizel",
        "Ankit Parag Shah",
        "Justin Salamon"
      ],
      "year": "2019",
      "venue": "Workshop on Detection and Classification of Acoustic Scenes and Events"
    },
    {
      "citation_id": "115",
      "title": "Voxlingua107: a dataset for spoken language recognition",
      "authors": [
        "J√∂rgen Valk",
        "Tanel Alum√§e"
      ],
      "year": "2021",
      "venue": "2021 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "116",
      "title": "Semantic object prediction and spatial sound super-resolution with binaural sounds",
      "authors": [
        "Arun Balajee Vasudevan",
        "Dengxin Dai",
        "Luc Van Gool"
      ],
      "year": "2020",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "117",
      "title": "Covost 2 and massively multilingual speech translation",
      "authors": [
        "Changhan Wang",
        "Anne Wu",
        "Jiatao Gu",
        "Juan Pino"
      ],
      "year": "2021",
      "venue": "Interspeech 2021",
      "doi": "10.21437/Interspeech.2021-2027"
    },
    {
      "citation_id": "118",
      "title": "Is chatgpt a good nlg evaluator? a preliminary study",
      "authors": [
        "Jiaan Wang",
        "Yunlong Liang",
        "Fandong Meng",
        "Zengkui Sun",
        "Haoxiang Shi",
        "Zhixu Li",
        "Jinan Xu",
        "Jianfeng Qu",
        "Jie Zhou"
      ],
      "year": "2023",
      "venue": "Proceedings of EMNLP Workshop"
    },
    {
      "citation_id": "119",
      "title": "Super-naturalinstructions:generalization via declarative instructions on 1600+ tasks",
      "authors": [
        "Yizhong Wang",
        "Swaroop Mishra",
        "Pegah Alipoormolabashi",
        "Yeganeh Kordi",
        "Amirreza Mirzaei",
        "Anjana Arunkumar",
        "Arjun Ashok",
        "Arut Selvan Dhanasekaran",
        "Atharva Naik",
        "David Stap"
      ],
      "year": "2022",
      "venue": "EMNLP"
    },
    {
      "citation_id": "120",
      "title": "Speech commands: A dataset for limited-vocabulary speech recognition",
      "authors": [
        "Pete Warden"
      ],
      "year": "2018",
      "venue": "Speech commands: A dataset for limited-vocabulary speech recognition",
      "arxiv": "arXiv:1804.03209"
    },
    {
      "citation_id": "121",
      "title": "Chime-6 challenge: Tackling multispeaker speech recognition for unsegmented recordings",
      "authors": [
        "Shinji Watanabe",
        "Michael Mandel",
        "Jon Barker",
        "Emmanuel Vincent",
        "Ashish Arora",
        "Xuankai Chang",
        "Sanjeev Khudanpur",
        "Vimal Manohar",
        "Daniel Povey",
        "Desh Raj"
      ],
      "year": "2020",
      "venue": "CHiME 2020-6th International Workshop on Speech Processing in Everyday Environments"
    },
    {
      "citation_id": "122",
      "title": "Human screaming detection dataset",
      "authors": [
        "Wavsource"
      ],
      "year": "2000",
      "venue": "Human screaming detection dataset"
    },
    {
      "citation_id": "123",
      "title": "Wham!: Extending speech separation to noisy environments",
      "authors": [
        "Gordon Wichern",
        "Joe Antognini",
        "Michael Flynn",
        "Licheng Richard Zhu",
        "Emmett Mcquinn",
        "Dwight Crow",
        "Ethan Manilow",
        "Jonathan Roux"
      ],
      "year": "2019",
      "venue": "Wham!: Extending speech separation to noisy environments",
      "doi": "10.21437/Interspeech.2019-2821"
    },
    {
      "citation_id": "124",
      "title": "Vocalset: A singing voice dataset",
      "authors": [
        "Julia Wilkins",
        "Prem Seetharaman",
        "Alison Wahl",
        "Bryan Pardo"
      ],
      "year": "2018",
      "venue": "ISMIR"
    },
    {
      "citation_id": "125",
      "title": "Asvspoof 2015: the first automatic speaker verification spoofing and countermeasures challenge",
      "authors": [
        "Zhizheng Wu",
        "Tomi Kinnunen",
        "Nicholas Evans",
        "Junichi Yamagishi"
      ],
      "year": "2015",
      "venue": "Asvspoof 2015: the first automatic speaker verification spoofing and countermeasures challenge",
      "doi": "10.21437/Interspeech.2015-462"
    },
    {
      "citation_id": "126",
      "title": "CSTR VCTK Corpus: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit (version 0.92). sound",
      "authors": [
        "Junichi Yamagishi",
        "Christophe Veaux",
        "Kirsten Macdonald"
      ],
      "year": "2019",
      "venue": "CSTR VCTK Corpus: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit (version 0.92). sound",
      "doi": "10.7488/ds/2645"
    },
    {
      "citation_id": "127",
      "title": "Investigating zero-shot generalizability on mandarin-english code-switched asr and speech-to-text translation of recent foundation models with self-supervision and weak supervision",
      "authors": [
        "Chih-Kai Yang",
        "Kuan-Po Huang",
        "Ke-Han Lu",
        "Chun-Yi Kuan",
        "Chi-Yuan Hsiao",
        "Hung-Yi Lee"
      ],
      "year": "2024",
      "venue": "2024 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW)"
    },
    {
      "citation_id": "128",
      "title": "Air-bench: Benchmarking large audio-language models via generative comprehension",
      "authors": [
        "Qian Yang",
        "Jin Xu",
        "Wenrui Liu",
        "Yunfei Chu",
        "Ziyue Jiang",
        "Xiaohuan Zhou",
        "Yichong Leng",
        "Yuanjun Lv",
        "Zhou Zhao",
        "Chang Zhou"
      ],
      "year": "2024",
      "venue": "Air-bench: Benchmarking large audio-language models via generative comprehension",
      "arxiv": "arXiv:2402.07729"
    },
    {
      "citation_id": "129",
      "title": "Abdelrahman Mohamed, and Hung-yi Lee. Superb: Speech processing universal performance benchmark",
      "authors": [
        "Shu-Wen Yang",
        "Po-Han Chi",
        "Yung-Sung Chuang",
        "Cheng-I Jeff Lai",
        "Kushal Lakhotia",
        "Yist Lin",
        "Andy Liu",
        "Jiatong Shi",
        "Xuankai Chang",
        "Guan-Ting Lin",
        "Tzu-Hsien Huang",
        "Wei-Cheng Tseng",
        "Da-Rong Ko Tik Lee",
        "Zili Liu",
        "Shuyan Huang",
        "Shang-Wen Dong",
        "Shinji Li",
        "Watanabe"
      ],
      "year": "2021",
      "venue": "Interspeech 2021",
      "doi": "10.21437/Interspeech.2021-1775"
    },
    {
      "citation_id": "130",
      "title": "Crossfit: A few-shot learning challenge for cross-task generalization in nlp",
      "authors": [
        "Qinyuan Ye",
        "Bill Yuchen Lin",
        "Xiang Ren"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "131",
      "title": "Music audio representation benchmark for universal evaluation",
      "authors": [
        "Ruibin Yuan",
        "Yinghao Ma",
        "Yizhi Li",
        "Ge Zhang",
        "Xingran Chen",
        "Hanzhi Yin",
        "Yiqi Liu",
        "Jiawen Huang",
        "Zeyue Tian",
        "Binyue Deng"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "132",
      "title": "Ctrsvdd: A benchmark dataset and baseline analysis for controlled singing voice deepfake detection",
      "authors": [
        "Yongyi Zang",
        "Jiatong Shi",
        "You Zhang",
        "Ryuichi Yamamoto",
        "Jionghao Han",
        "Yuxun Tang",
        "Shengyuan Xu",
        "Wenxiao Zhao",
        "Jing Guo",
        "Tomoki Toda",
        "Zhiyao Duan"
      ],
      "year": "2024",
      "venue": "Interspeech 2024",
      "doi": "10.21437/Interspeech.2024-2242"
    },
    {
      "citation_id": "133",
      "title": "Libritts: A corpus derived from librispeech for text-to-speech",
      "authors": [
        "Heiga Zen",
        "Viet Dang",
        "Rob Clark",
        "Yu Zhang",
        "Ron Weiss",
        "Ye Jia",
        "Zhifeng Chen",
        "Yonghui Wu"
      ],
      "year": "2019",
      "venue": "Libritts: A corpus derived from librispeech for text-to-speech",
      "doi": "10.21437/Interspeech.2019-2441"
    },
    {
      "citation_id": "134",
      "title": "speechocean762: An open-source non-native english speech corpus for pronunciation assessment",
      "authors": [
        "Junbo Zhang",
        "Zhiwen Zhang",
        "Yongqing Wang",
        "Zhiyong Yan",
        "Qiong Song",
        "Yukai Huang",
        "Ke Li",
        "Daniel Povey",
        "Yujun Wang"
      ],
      "year": "2021",
      "venue": "Interspeech 2021",
      "doi": "10.21437/Interspeech.2021-1259"
    },
    {
      "citation_id": "135",
      "title": "M4singer: A multi-style, multi-singer and musical score provided mandarin singing corpus",
      "authors": [
        "Lichao Zhang",
        "Ruiqi Li",
        "Shoutong Wang",
        "Liqun Deng",
        "Jinglin Liu",
        "Yi Ren",
        "Jinzheng He",
        "Rongjie Huang",
        "Jieming Zhu",
        "Xiao Chen",
        "Zhou Zhao"
      ],
      "year": "2022",
      "venue": "Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track"
    },
    {
      "citation_id": "136",
      "title": "3d-speaker: A large-scale multi-device, multi-distance, and multi-dialect corpus for speech representation disentanglement",
      "authors": [
        "Siqi Zheng",
        "Luyao Cheng",
        "Yafeng Chen",
        "Hui Wang",
        "Qian Chen"
      ],
      "year": "2023",
      "venue": "3d-speaker: A large-scale multi-device, multi-distance, and multi-dialect corpus for speech representation disentanglement",
      "arxiv": "arXiv:2306.15354"
    }
  ]
}