{
  "paper_id": "2508.14548v1",
  "title": "Emotale: An Enacted Speech-Emotion Dataset In Danish",
  "published": "2025-08-20T09:01:54Z",
  "authors": [
    "Maja J. Hjuler",
    "Harald V. Skat-Rørdam",
    "Line H. Clemmensen",
    "Sneha Das"
  ],
  "keywords": [
    "speech emotion recognition",
    "speech processing",
    "paralinguistic speech",
    "transferability",
    "evaluation"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "While multiple emotional speech corpora exist for commonly spoken languages, there is a lack of functional datasets for smaller (spoken) languages, such as Danish. To our knowledge, Danish Emotional Speech (DES), published in 1997, is the only other database of Danish emotional speech. We present EmoTale 1 ; a corpus comprising Danish and English speech recordings with their associated enacted emotion annotations. We demonstrate the validity of the dataset by investigating and presenting its predictive power using speech emotion recognition (SER) models. We develop SER models for EmoTale and the reference datasets using self-supervised speech model (SSLM) embeddings and the openSMILE feature extractor. We find the embeddings superior to the hand-crafted features. The best model achieves an unweighted average recall (UAR) of 64.1% on the EmoTale corpus using leave-one-speaker-out cross-validation, comparable to the performance on DES.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction & Background",
      "text": "Speech signals are rich in information, both linguistic (in the form of sentences and words) and paralinguistic (denoting mood and affective state). Speech also carries information about multiple, potentially personal traits of the speaker, such as age, gender, and nationality. Multiple psychological and neuroscientific models of the mind hypothesize that language and emotion are certainly linked  [1] . For example, some cultures express anger more vocally, while others might be more restrained. Investigating voice and speech to judge emotional states dates back more than half a century  [2] ,  [3] , and the earliest speech emotion recognizers (SERs) were proposed over two decades ago  [4] ,  [5] .\n\nEmotions are inherently subjective; different people perceive emotions differently, and this can lead to differences in annotating emotional data  [6] ,  [7] . Overall, two different labeling schemes are adopted in the literature: categorical class labels, which are nominal and discrete, and dimensional labels, which are continuous. The former often follows the basic emotion theory developed by Paul Ekman  [8] , which assumes the existence of six basic and universal emotions that transcend language, cultural, and ethnic differences. The emotions, also known as The Big 6, are anger, disgust, fear, happiness, neutral, and sadness. Following the dimensional scheme, emotions can be described numerically in the two dimensions activation/arousal and valence, or in three dimensions by including dominance. For example, happiness is characterized by positive valence, high activation, and neutral dominance, i.e., neither dominant nor submissive.\n\nIn speech emotion recognition (SER), frequently used emotions include happiness, anger, sadness, disgust, fear, frustration, surprise, and boredom. For a baseline comparison, it is common practice to include neutral as one of the emotions expressed. In many SER databases, utterances are spoken with enacted emotions, but emotional responses can also be induced through specific tasks, scenarios, or stimuli to capture genuine emotional expressions. Alternatively, natural/spontaneous speech can be collected from existing digital resources, such as TV shows or podcasts, and annotated retrospectively. For English SER, IEMOCAP  [9]  and MSP-Podcast  [10] ,  [11]  are two of the most frequently used corpora due to their relatively large size, and the inclusion of both categorical and dimensional labels. The Danish DES database  [12]  was published in 1997 and contains four speakers (two male and two female) expressing five emotions: neutral, surprise, happiness, sadness, and anger. All utterances are equally balanced for each gender and actor. In listening tests for the DES corpus, emotions were correctly classified 67.3% of the time on average  [12] . However, DES includes single words and questions, and it was not developed specifically for speech emotion recognition No.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Danish Sentence",
      "text": "English sentence 1.\n\nDugen ligger på køleskabet.\n\nThe tablecloth is lying on the fridge.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "2.",
      "text": "Det sorte ark papir er placeret deroppe ved siden af tømmerstykket.\n\nThe black sheet of paper is located up there beside the piece of timber.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "3.",
      "text": "De bar det bare ovenpå og nu skal de ned igen.\n\nThey just carried it upstairs and now they are going down again.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "4.",
      "text": "Det vil vaere på det sted, hvor vi altid opbevarer det.\n\nIt will be in the place where we always store it.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "5.",
      "text": "Om syv timer er det morgen.\n\nIn seven hours it will be morning. Five emotions: Neutral, Anger, Sadness, Happiness, Boredom",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Purposes.",
      "text": "Contemporary state-of-the-art SER research is most often based on deep learning models  [13] -  [16]  like the SUPERB benchmark  [17] . Rapid development of scale-based deep learning was enabled by the availability of large and exhaustive speech emotion datasets  [9] ,  [10] . The most comprehensive SER datasets are in English or other large (spoken) languages. Developing SER models that transfer well to unseen languages, addresses the lack of resources in smaller languages while enabling the accessibility of these models. However, at minimum, a test dataset is necessary to validate the suitability and safety of a SER model before deployment. In this work, we take the first step towards presenting a Danish-SER dataset to address the gap in functional datasets. Our contributions are: 1) the EmoTale dataset: a corpus comprising 450 Danish and 350 English speech recordings with associated categorical and dimensional emotion annotations. 2) we also present a thorough validation of the quality of EmoTale by analyzing its predictive capacity using reference datasets. Through this process, we revisit transferability of SER and present insights with respect to other multilingual datasets.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Ii. Design Of Emotale",
      "text": "To enable cross-corpus comparability and transferability, the design choices in EmoTale are similar to existing small-scale SER datasets. The data collection procedure was inspired by the Berlin Database of Emotional Speech (Emo-DB)  [18] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Dataset Curation",
      "text": "Recruitment: Participants with acting experience and Danish and English language skills were recruited through physical flyers and posts on social media, and theater schools in the Greater Copenhagen area were contacted by email and phone. An online registration form was available in Google Forms, where participants signed up by providing their contact information and choosing their desired experiment date from a list of options. The exclusion criteria were age < 7 years or no Danish-speaking skills. In compliant with GDPR requirements, we obtained written consent from the participant or the guardian of participants under 18, and information about gender and age was recorded. Data collection procedure: The data recordings were performed in multiple sessions and locations with no ambient noise. At the start of a session, the participant was fitted with RØDE Wireless Go microphones and was walked through the experiment, and allowed to ask questions. Five sentences were enacted with five different emotions (Tab. I), and the participant enacted all sentences for a specific emotion before moving on to the next. The sentences are translations of selected sentences from Emo-DB  [18] ; to minimize subjective associations and differences, the sentences were selected such that they are emotionally neutral and comprise minimal contextual information. We relied on the participant's ability to self-induce an emotion by recalling a situation where it had been felt strongly. The participants were allowed to repeat the sentences as many times as they liked, but only the last recording was retained. Since Danish speakers are fluent in English, the participants could choose to contribute with enacted English speech in addition to Danish. The utterances were recorded at a 48 kHz sampling frequency and saved in .WAV format. The audio filenames comprise the meta information on the language, speaker ID, emotion, and sentence. For example, the file DK_004_A_5.wav is the fifth sentence spoken by speaker 004 in Danish, with angry affect. Data protection and ethical considerations: Ethical approval was obtained from the institutional review board prior to the study  [19] . The samples are pseudo-anonymized by generating a random identifier for each participant. Since the emotions are enacted and the selected sentences do not contain personal contextual information, potential misuse of the data to cause harm to the participants is reduced. The dataset is supported by a datasheet  [20] , in the later part of the paper. Annotation procedure: In addition to emotion categories, many existing datasets annotate speech-emotion samples using dimensional labels  [9] ,  [21] . We adopt a similar approach in the EmoTale corpus, where utterances are manually annotated for arousal, valence, and dominance on a scale from 1 to 5, with increments of 0.5. Arousal indicates the level of excitement or activation associated with the emotion, ranging from calm (1) to excited  (5) . Valence reflects the emotional tone, with values ranging from negative (1) to positive  (5) . Dominance measures the level of dominance associated with the emotion, with a scale from submissive (1) to dominant  (5) . The first, second, and last authors independently assigned labels to all utterances in EmoTale, each providing one categorical label for the intended emotion and three numerical labels for arousal, valence, and dominance. The categorical annotations serve to validate the enacted emotions.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Description Of Emotale",
      "text": "EmoTale comprises emotional speech from 18 participants, of whom 12 are female and six are male. The total number of Danish and English utterances are 450 and 350, respectively. The average age of the participants was 22.8 years, ranging from 9 to 39 years old. Age and gender distributions of participants are illustrated in Fig.  2 . The goal of this dataset is to develop infrastructure to enable the evaluation and safe deployment  [22]  of existing speech processing and SER on the Danish-speaking population, including children. Therefore, speakers under the age of 18 are also included in the dataset. Some files were cropped to exclude a 'click' sound (from experimenters' keyboard) at the start or end of the recording.\n\nInter-rater reliability (IRR): In addition to the enacted emotion, three independent annotators provided four labels per instance: one categorical label for the intended emotion and three numerical labels for arousal, valence, and dominance, each ranging from 1 to 5 with increments of 0. To evaluate the IRR between dimensional emotion annotations (valence, arousal, dominance), we employ Concordance Correlation Coefficient (CCC)  [25] , which is suitable for ratings on a fine-grained, continuous, or interval scale. As seen in Table  III , the results indicate moderate to strong agreement for arousal and valence, and moderate agreement for dominance.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Validating The Emotion-Signal In Emotale",
      "text": "We validate the signals in EmoTale by a) comparing human annotations to the predictions from a pre-trained SSL, and b) analyzing the predictive power of the data samples by training and evaluating SER models in Danish. We employ the following datasets as references on the validity and quality of EmoTale: Emo-DB (German), Urdu (Urdu)  [26] , DES (Danish), and AESDD (Greek)  [27] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Labels: Human-Annotation Vs. Pre-Trained Model",
      "text": "A pre-trained model (PTM), w2v2-FT-dim, fine-tuned on the MSP-Podcast with dimensional labels 2  , outputs activation, valence, and dominance scores ranging between 0 to 1. These  were rescaled to a range between 1 and 5 to compare with manual labels as follows:\n\nwhere A i denotes the activation score, and A min and A max are the overall minimum and maximum activation scores, respectively. Valence and dominance scores were rescaled in the same way. Fig.  3  compares the scores predicted by the PTM for EmoTale to the labels by Annotator 1. Activation and valence labels for Emo-DB, Urdu, and AESDD were employed using  [28] , while DES and EmoTale were annotated as part of this work. Predictions by w2v2-FT-dim were compared against the human-annotated labels using CCC in Fig.  4 ; A high CCC is observed for activation/arousal and dominance, implying a high agreement between the outcome of PTM and human-annotated labels, but the scores are consistently lower for valence over all datasets.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Validation With Handcrafted Features & Ptm Embeddings",
      "text": "We explore the predictive power of the samples in EmoTale with respect to the reference datasets by evaluating the performance of SER models on all the datasets, in the process revisiting cross-lingual transferability of speech-emotions. Method: As for the SER models, we employ a support vector classifier (SVC), a) with hand-crafted features, and b) PTM embeddings, also known as deep features. The PTM feature embeddings are extracted as the last hidden states of the pretrained model, i.e., the last layer before any task-specific head is applied, and it is assumed that model embeddings provide a compact representation of the emotional content in a speech signal. In transformer models, this is the output from the final transformer block. The experimental procedure is adapted from the one outlined by Wagner et al 3 . The speech samples in DES, EmoTale, Emo-DB, Urdu, and AESDD datasets are downsampled to 16 kHz as the PTM input requirement, and stereo audio files were converted to mono by averaging to a single channel. The pipelines are illustrated in Fig.  1 .\n\nThe eGeMAPS (extended Geneva Minimalistic Acoustic Parameter Set)  [29]  and the ComParE (Computational Paralinguistics Challenge)  [30]  feature sets were extracted using the openSMILE toolkit  [31]  and serve as two separate baselines. These were tested against embeddings from the wav2vec2 base model 4  [32]  as well as a wav2vec2 model fine-tuned for SER on the RAVDESS corpus  [33]  (w2v2-FT-cat) 5 and one fine-tuned on MSP-Podcast (w2v2-FT-dim)  [34] . The latter is fine-tuned on dimensional scores and not categorical labels, therefore, the output of hidden states is necessary to access the latent space of the model. Model embeddings are extracted by applying average pooling over the hidden states of the last transformer layer. Subsequently, the features are input to a SVC with a linear kernel, and Leave-One-Speaker-Out (LOSO) cross-validation is applied. In each fold, features were standardized using the mean and standard deviation of the respective training set. We used a linear kernel to resemble the method in  [35] . Evaluation: Applying LOSO cross-validation introduces variability in the performance metric. The aggregated unweighted average recall (UAR) across cross-validation folds is used for evaluation. However, it may overlook performance differences across individual speakers. Each iteration of LOSO involves training a model on a different subset of data, hence, for S speakers it is more accurate to consider the S different models separately. For the same dataset, each model is tested under the same conditions, whereby we can apply paired ttests to statistically model performances. The UAR scores are computed as the sum of class-wise recall divided by the number of emotion classes, and the overall score is the average UAR across all datasets. To provide a comprehensive view of model performance, we report both the aggregated results (highlighted rows in Tab. IV) and the mean results across speakers (Speaker UAR). The former combines the predictions of all folds into a single confusion matrix and calculates the UAR. Once the SVC parameters are fixed, changing the random seed does not affect results, hence, the standard deviation is zero. The latter calculates the UAR for each LOSO cross-validation fold individually and takes the mean to consider how well the model generalizes across speakers. Similarly, sentence UARs are found by first grouping prediction sentences, calculating the UAR per group, and then taking a simple average across the groups. In this way, all the speakers and sentences are given equal weight. Standard deviations are reported to provide insights into the variability of the UAR scores across speakers, sentences, and datasets. Sentence UARs are not included for the Urdu corpus since it 3 https://github.com/audeering/w2v2-how-to/blob/main/notebook.ipynb 4 https://huggingface.co/facebook/wav2vec2-base 5 https://huggingface.co/ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition  contains natural utterances, hence, no sentences are repeated.\n\nResults: For Emo-DB, the results reported in Table  IV  using ComParE and w2v2-FT-dim embeddings are reproduced from  [13] . The performance trends observed on EmoTale align with those seen in Emo-DB and DES, reinforcing the consistency and reliability of the dataset. Specifically, the UAR scores for the three datasets follow the same trend with model performance in descending order using the features: w2v2-FT-dim, w2v2-FT-cat, ComParE, eGeMAPS, and w2v2-b. Interestingly, Urdu deviates from the trend with eGeMAPS features outperforming both ComParE and the embeddings from the PTM fine-tuned on categorical labels, w2v2-FT-cat. In all cases, deep features from the finetuned models yield the highest UARs, while embeddings from the wav2vec2 model without fine-tuning perform the worst. Furthermore, the PTM fine-tuned on dimensional labels leads to the highest mean UAR across datasets, highlighting the benefit of fine-tuning.\n\nTo further validate model performance on EmoTale, we applied pairwise t-tests  [36]  across LOSO folds to assess the statistical significance of differences between feature sets. While fine-tuning of dimensional labels (w2v2-FT-dim) yields a statistically significant improvement over categorical labels (w2v2-FT-cat) for Emo-DB, this distinction does not hold for EmoTale nor the other datasets, which negates the argument against categorical labels  [37] . Similarly, for several datasets, there is no statistically significant difference in model performance when training on eGeMAPS features compared to wav2vec2 base model embeddings. The single best and two best models with statistical significance are marked in Table  IV  when such a conclusion could be drawn based on pairwise t-tests. These findings further strengthen EmoTale's role as a reliable benchmark for emotional speech, with results that reflect those of established corpora.\n\nWe also observe from Tab. IV that the scores for DES are relatively low, and model performance is sentence-dependent, in contrast to the EMO-DB and EmoTale. This could be explained by DES being designed differently from the other datasets. For example, the sentence ID NO refers to a single word Nej (No), which may not be sufficient for the model to recognize the emotion. Similarly, the sentences with ID: SE4, SE5, SE6, and SE8 are all questions, and might be spoken with a different intonation. Embeddings from the PTMs generally produce more stable results (low variation), however, a relatively high standard deviation is observed for w2v2-FT-dim features across EmoTale speakers (12.4) and DES sentences (12.7). This could be explained by differences inherent in the two datasets: EmoTale has a larger age range of speakers compared to the other datasets, and DES contains sentences that vary in linguistic and paralinguistic content. The UAR scores for Urdu are very speaker-dependent compared to the other datasets. This can be explained by a high number of speakers and a low number of sentences per speaker.\n\nTo assess cross-corpus transferability, the SVC models were retrained to recognize a subset of four emotion classes (happy, angry, sad, and neutral) on ComParE and w2v2-FT-dim features for the Emo-DB, EmoTale, DES, and Urdu corpora. These datasets were selected specifically because they include all four emotion labels. The UAR scores for all train-test combinations are shown in the heatmaps in Fig.  5 . The incorpus UAR scores in the diagonal of the matrices are found by LOSO cross-validation following the same methodology as earlier, but only including the four emotions. We wish to develop SER models that generalize well on new, unseen data, especially in real-world applications. Furthermore, a model that transfers well is less likely to be overfitted on the training data. Although performance generally drops in the cross-corpus domain, the deep features seem to be more transferable. Importantly, EmoTale proves to be a strong evaluation benchmark. While models trained EmoTale perform comparably to those trained on other corpora, EmoTale consistently supports meaningful generalization. For example, inferring on Emo-DB yields higher cross-domain scores than in-corpus UAR scores when trained on EmoTale and DES. This continues the pattern from the previous analysis, where Emo-DB achieved significantly higher model performances than the other corpora. This could be explained by the perception tests carried out during the creation of Emo-DB, where utterances recognized by more than 80% of the listeners were kept in the database. Hence, the database is expected to contain utterances with highly pronounced affect.\n\nIV. CONCLUSIONS Unavailability of Danish affect datasets not only impedes the development of the technology, but also impacts the validation of existing methods on Danish speakers. We present EmoTale, a bilingual enacted speech-emotion dataset in Danish and English, intended to enable the evaluation of SER models in the Danish language. In addition to categorical emotion labels, EmoTale includes dimensional annotations for arousal, valence, and dominance. Annotation reliability is high: Concordance Correlation Coefficient (CCC) scores indicate moderate to strong agreement for arousal and valence, and moderate agreement for dominance, while Cohen's Kappa values indicate substantial consistency in categorical labeling. To demonstrate the validity of the dataset, we evaluate its labels and predictive capacity using both pre-trained model embeddings and hand-crafted, acoustic features. Our experiments demonstrate that (a) model performance on EmoTale is comparable to that on established reference datasets, and (b) feature embeddings from PTMs consistently outperform handcrafted features, particularly in cross-corpus transfer scenarios. While models trained on EmoTale perform comparably to those trained on other corpora, EmoTale consistently supports meaningful generalization. These findings further strengthen the validity of EmoTale as a reliable benchmark for Danish emotional speech.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "V. Acknowledgment",
      "text": "Co-funded by the French National Research Agency under the Pantagruel project (ANR-23-IAS1-0001) and the European Union under the Marie Skłodowska-Curie Grant Agreement No 101081465 (AUFRANDE). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the Research Executive Agency, which cannot be held responsible for them.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Processing pipelines for hand-crafted (top) and deep features (bottom).",
      "page": 1
    },
    {
      "caption": "Figure 2: Age and gender distribution of EmoTale participants.",
      "page": 2
    },
    {
      "caption": "Figure 2: The goal of this dataset",
      "page": 3
    },
    {
      "caption": "Figure 3: EmoTale Annotator 1 labels for the utterances in Danish compared",
      "page": 3
    },
    {
      "caption": "Figure 4: Concordance Correlation Coefficients between w2v2-FT-dim and",
      "page": 3
    },
    {
      "caption": "Figure 3: compares the scores predicted by the",
      "page": 3
    },
    {
      "caption": "Figure 1: The eGeMAPS (extended Geneva Minimalistic Acoustic",
      "page": 4
    },
    {
      "caption": "Figure 5: Unweighted Average Recall (UAR) scores for SVC trained on",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "No.": "1.\n2.\n3.\n4.\n5.",
          "Danish sentence": "Dugen ligger p˚a køleskabet.\nDet sorte ark papir er placeret deroppe\nved siden af\ntømmerstykket.\nDe bar det bare ovenp˚a og nu skal de\nned igen.\nDet vil være p˚a det\nsted, hvor vi altid\nopbevarer det.\nOm syv timer er det morgen.",
          "English sentence": "The tablecloth is lying on the fridge.\nThe black sheet of paper\nis located up\nthere beside the piece of\ntimber.\nThey just\ncarried it upstairs\nand now\nthey are going down again.\nIt will be in the place where we always\nstore it.\nIn seven hours it will be morning."
        },
        {
          "No.": "Five emotions: Neutral, Anger, Sadness, Happiness, Boredom",
          "Danish sentence": "",
          "English sentence": ""
        }
      ],
      "page": 2
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The role of language in emotion: existing evidence and future directions",
      "authors": [
        "K Lindquist"
      ],
      "year": "2017",
      "venue": "Current Opinion in Psychology"
    },
    {
      "citation_id": "2",
      "title": "Vocal pitch during simulated emotion",
      "authors": [
        "G Fairbanks",
        "W Pronovost"
      ],
      "year": "1938",
      "venue": "Science"
    },
    {
      "citation_id": "3",
      "title": "Judgment of emotion in word-free voice samples",
      "authors": [
        "W Soskin",
        "P Kauffman"
      ],
      "year": "1961",
      "venue": "Journal of Communication"
    },
    {
      "citation_id": "4",
      "title": "Recognizing emotion in speech",
      "authors": [
        "F Dellaert",
        "T Polzin",
        "A Waibel"
      ],
      "year": "1996",
      "venue": "Proceeding of Fourth International Conference on Spoken Language Processing. ICSLP'96"
    },
    {
      "citation_id": "5",
      "title": "Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "6",
      "title": "On the praxes and politics of ai speech emotion recognition",
      "authors": [
        "E Kang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency"
    },
    {
      "citation_id": "7",
      "title": "The ordinal nature of emotions",
      "authors": [
        "G Yannakakis",
        "R Cowie",
        "C Busso"
      ],
      "year": "2017",
      "venue": "2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "8",
      "title": "Are there basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Are there basic emotions"
    },
    {
      "citation_id": "9",
      "title": "Iemocap: interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "10",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2019",
      "venue": "Ieee Transactions on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "Msp-improv: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "C Busso",
        "S Parthasarathy",
        "A Burmania",
        "M Abdelwahab",
        "N Sadoughi",
        "E Provost"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "12",
      "title": "Design recording and verification of a danish emotional speech database",
      "authors": [
        "I Engberg",
        "A Hansen",
        "O Andersen",
        "P Dalsgaard"
      ],
      "year": "1997",
      "venue": "EU-ROSPEECH'97 : 5th European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "13",
      "title": "Dawn of the transformer era in speech emotion recognition: Closing the valence gap",
      "authors": [
        "J Wagner",
        "A Triantafyllopoulos",
        "H Wierstorf",
        "M Schmitt",
        "F Burkhardt",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "Ieee Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "14",
      "title": "A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding",
      "authors": [
        "Y Wang",
        "A Boumadane",
        "A Heba"
      ],
      "year": "2022",
      "venue": "A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding"
    },
    {
      "citation_id": "15",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "Emotion recognition from speech using wav2vec 2.0 embeddings"
    },
    {
      "citation_id": "16",
      "title": "A multilingual framework based on pre-training model for speech emotion recognition",
      "authors": [
        "Z Zhang",
        "X Zhang",
        "M Guo",
        "W Zhang",
        "K Li",
        "Y Huang"
      ],
      "year": "2021",
      "venue": "2021 Asia-pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "17",
      "title": "Superb: Speech processing universal performance benchmark",
      "authors": [
        "S.-W Yang",
        "P.-H Chi",
        "Y.-S Chuang",
        "C.-I Lai",
        "K Lakhotia",
        "Y Lin",
        "A Liu",
        "J Shi",
        "X Chang",
        "G.-T Lin",
        "T.-H Huang",
        "W.-C Tseng",
        "K -T. Lee",
        "D.-R Liu",
        "Z Huang",
        "S Dong",
        "S.-W Li",
        "S Watanabe",
        "A Mohamed",
        "H.-Y Lee"
      ],
      "year": "2021",
      "venue": "Superb: Speech processing universal performance benchmark"
    },
    {
      "citation_id": "18",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "A database of german emotional speech"
    },
    {
      "citation_id": "19",
      "title": "Ethical approval application to the IRB for DanskEmoTale: a pilot study",
      "year": "2022",
      "venue": "Ethical approval application to the IRB for DanskEmoTale: a pilot study"
    },
    {
      "citation_id": "20",
      "title": "Datasheets for datasets",
      "authors": [
        "T Gebru",
        "J Morgenstern",
        "B Vecchione",
        "J Vaughan",
        "H Wallach",
        "H Iii",
        "K Crawford"
      ],
      "year": "2021",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "21",
      "title": "Zero-shot cross-lingual speech emotion recognition: A study of loss functions and feature importance",
      "authors": [
        "S Das",
        "N Lonfeldt",
        "N Lund",
        "A Pagsberg",
        "L Clemmensen"
      ],
      "year": "2022",
      "venue": "Proceedings of 2nd Symposium on Security and Privacy in Speech Communication"
    },
    {
      "citation_id": "22",
      "title": "Speech detection for child-clinician conversations in danish for low-resource in-the-wild conditions: a case study",
      "authors": [
        "S Das",
        "N Lønfeldt",
        "A Pagsberg",
        "L Clemmensen"
      ],
      "year": "2022",
      "venue": "Speech detection for child-clinician conversations in danish for low-resource in-the-wild conditions: a case study",
      "arxiv": "arXiv:2204.11550"
    },
    {
      "citation_id": "23",
      "title": "A coefficient of agreement for nominal scales",
      "authors": [
        "J Cohen"
      ],
      "year": "1960",
      "venue": "Educational and Psychological Measurement",
      "doi": "10.1177/001316446002000104"
    },
    {
      "citation_id": "24",
      "title": "Inter-rater reliability for emotion annotation in human-computer interaction -comparison and methodological improvements",
      "authors": [
        "I Siegert",
        "R Böck",
        "A Wendemuth"
      ],
      "venue": "Journal of Multimodal User Interfaces"
    },
    {
      "citation_id": "25",
      "title": "A concordance correlation coefficient to evaluate reproducibility",
      "authors": [
        "I Lawrence",
        "K Lin"
      ],
      "year": "1989",
      "venue": "Biometrics"
    },
    {
      "citation_id": "26",
      "title": "Cross lingual speech emotion recognition: Urdu vs. western languages",
      "authors": [
        "S Latif",
        "A Qayyum",
        "M Usman",
        "J Qadir"
      ],
      "year": "2018",
      "venue": "2018 International conference on frontiers of information technology (FIT)"
    },
    {
      "citation_id": "27",
      "title": "Speech emotion recognition for performance interaction",
      "authors": [
        "N Vryzas",
        "R Kotsakis",
        "A Liatsou",
        "C Dimoulas",
        "G Kalliris"
      ],
      "year": "2018",
      "venue": "Journal of the Audio Engineering Society"
    },
    {
      "citation_id": "28",
      "title": "Continuous metric learning for transferable speech emotion recognition and embedding across low-resource languages",
      "authors": [
        "S Das",
        "N Lund",
        "N Lønfeldt",
        "A Pagsberg",
        "L Clemmensen"
      ],
      "year": "2022",
      "venue": "Proceedings of the Northern Lights Deep Learning Workshop"
    },
    {
      "citation_id": "29",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "30",
      "title": "The interspeech 2010 paralinguistic challenge",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner",
        "F Burkhardt",
        "L Devillers",
        "C Müller",
        "S Narayanan"
      ],
      "year": "2010",
      "venue": "The interspeech 2010 paralinguistic challenge"
    },
    {
      "citation_id": "31",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "32",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "H Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "wav2vec 2.0: A framework for self-supervised learning of speech representations"
    },
    {
      "citation_id": "33",
      "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)",
      "doi": "10.5281/zenodo.1188976"
    },
    {
      "citation_id": "34",
      "title": "Model for dimensional speech emotion recognition based on wav2vec 2.0",
      "authors": [
        "J Wagner",
        "A Triantafyllopoulos",
        "H Wierstorf",
        "M Schmitt",
        "F Burkhardt",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "Model for dimensional speech emotion recognition based on wav2vec 2.0",
      "doi": "10.5281/zenodo.6221127"
    },
    {
      "citation_id": "35",
      "title": "The INTERSPEECH 2016 Computational Paralinguistics Challenge: Deception, Sincerity & Native Language",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner",
        "J Hirschberg",
        "J Burgoon",
        "A Baird",
        "A Elkins",
        "Y Zhang",
        "E Coutinho",
        "K Evanini"
      ],
      "year": "2016",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "36",
      "title": "",
      "authors": [
        "P Brockhoff",
        "J Møller",
        "E Andersen",
        "P Bacher",
        "L Christiansen"
      ],
      "year": "2018",
      "venue": ""
    },
    {
      "citation_id": "37",
      "title": "Towards transferable speech emotion representation: on loss functions for cross-lingual latent representations",
      "authors": [
        "S Das",
        "N Lønfeldt",
        "A Pagsberg",
        "L Clemmensen"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "38",
      "title": "Datasheets for datasets",
      "authors": [
        "T Gebru",
        "J Morgenstern",
        "B Vecchione",
        "J Vaughan",
        "H Wallach",
        "H Iii",
        "K Crawford"
      ],
      "year": "2021",
      "venue": "Datasheets for datasets"
    }
  ]
}