{
  "paper_id": "2303.10741v2",
  "title": "Computer Vision Es/Ma/On Of Emo/On Reac/On Intensity In The Wild",
  "published": "2023-03-19T19:09:41Z",
  "authors": [
    "Yang Qian",
    "Ali Kargarandehkordi",
    "Onur Cezmi Mutlu",
    "Saimourya Surabhi",
    "Mohammadmahdi Honarmand",
    "Dennis Paul Wall",
    "Peter Washington"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Introducjon",
      "text": "Natural facial expressions are the most potent, universally recognized signals for conveying emo:onal states and inten:ons  [1, 2] . Mental disease diagnosis, human social/physiological interac:on detec:on, social robo:cs, and many other sociotechnical applica:ons have been target domains for research on automa:c emo:on recogni:on  [3] [4] [5] [6] [7] [8] .\n\nAn example of affec:ve compu:ng which is central to our research groups' collec:ve work is digital therapeu:cs for developmental delays. Emo:onal expressions play a crucial role in certain types of pediatric developmental disorders. Au:sm spectrum disorder (ASD), for example, affects almost 1 in 44 people in the United States  [9] , and it is one of the fastest-growing developmental disorders in terms of prevalence  [10, 11] . Children with au:sm tend to evoke emo:ons differently than neurotypical peers, and it is more difficult for them to produce the correct facial expressions  [12] [13] [14] . Digital therapeu:cs have been developed to assist children who struggle with emo:on by providing real:me emo:on cues corresponding to the evoca:ons of a conversa:onal partner using real-:me computer vision recogni:on of emo:on expression  [15] [16] [17] [18] [19] [20] [21] [22] [23] . Such digital and wearable devices enable families to provide therapy in the comfort of their homes and ability to customize the interven:on structure to suit their child's needs  [24] [25] [26] [27] [28] [29] [30] . However, these systems use models which only predict basic emo:on categories and could benefit from the development of more sophis:cated emo:on recogni:on models.\n\nHume-React is a large-scale mul:modal database containing user-generated video content and corresponding annota:ons of emo:on reac:on intensity.\n\nBy releasing the Hume-Reac:on dataset, Hume contributed to the 5th Workshop and Compe::on on Affec:ve Behavior Analysis InThe-Wild (ABAW) and to affec:ve compu:ng research as a whole. The released dataset includes more than 75 hours of video recordings consis:ng of spontaneous reac:ons of 2,222 individuals to 1841 evoca:ve video elicitors. Each video is annotated by individuals with seven self-reported emo:ons at a scale of intensity 1-100  [31] . Predic:on of con:nuous intensity rather than category alone can expand the possibili:es of digital therapeu:cs for ASD.\n\nWe propose an affect recogni:on and level es:ma:on model for the Emo:onal Reac:on Intensity (ERI) Es:ma:on task in the 5th ABAW Compe::on  [32] . In contrast to the most recent ABAW compe::ons, where mul:-task learning was the central theme or among one of the main challenges  [33, 34] , this year the focus is on only uni-task solu:ons to four challenges: Valence-Arousal (VA) Es:ma:on  [35, 36] , Expression (Expr) Classifica:on  [37, 38] , Ac:on Unit (AU) Detec:on  [37, 39] , and Emo:onal Reac:on Intensity (ERI) Es:ma:on. We designed our algorithms to surpass the baseline network performance ResNet50 (pre-trained VGGFACE2) with fixed convolu:onal weights and employed mul:ple modifica:ons to enhance the proposed models and achieve beeer efficiency in detec:ng emo:on labels and es:ma:ng their intensity levels. Our code is publicly available here: heps:// github.com/YangQiantwx/EERI_CVPR_2023.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "A mul:tude of features, including visual, audio, text, and physiological signals, have been introduced in prior mul:modal deep learning models. We focus on reviewing visual features, as it was the modality used in this project.\n\nWe first describe common visual feature representa:ons. The Facial Ac:on Coding System (FACS) is a widely used affect recogni:on network that recognizes specific emo:ons based on facial Ac:on Units (AU)  [40] . Gabor wavelet is another emo:on recogni:on tool successfully applied to facial representa:on  [41] . Benefi:ng from the growing applica:on of deep learning, researchers have discovered that extrac:ng features based on deep learning techniques can achieve higher accuracy. For instance, to extract visual features,  [42]  uses CNN and RNN stack based on a convolu:onal recurrent neural network. To prove the efficiency of audio/visual networks,  [43]  proposes the usage of 2D+1D convolu:onal neural networks. In the AVECs, researchers use other deep learning methods that all perform beeer than tradi:onal feature extractors  [44] [45] [46] .\n\nThere are several prior works for facial emo:on recogni:on  [4, 7, 8, 47, 48] . A major boeleneck in affec:ve compu:ng is that emo:on expression models are limited by the datasets they are trained on. Exis:ng emo:on datasets use only one of the three common types of emo:onal representa:ons: Categorical Emo:ons (CE), Ac:on Units (AU), and Valence Arousal (VA). The similari:es between some expressions (i.e., the ambiguity of the labels in the dataset) is another challenge that increases the difficulty of dis:nguishing some facial expressions. This ambiguity might originate from inconsistent labeling. For example, \"Sadness\" can be similar to \"Disgust\" and it might be difficult to dis:nguish these two facial expressions.\n\nTo tackle some of the men:oned limita:ons above, the use of video-based datasets with alterna:ve labeling representa:ons has emerged. Dealing with the complexi:es of high-dimensional video data becomes a central challenge. Due to rapid expression changes, many frames might not contain reliable informa:on for predic:ng facial expressions let alone es:ma:on of the emo:on's intensity. Labeling the video frame by frame  [49]  adds further complexity.\n\nThe Aff-Wild  [50] [51] [52] [53]  and Aff-Wild2 [32, 54-58] Audio/Visual (A/V) datasets are current examples used in both academic and industrial communi:es that contain all three representa:on labels men:oned above. Aff-Wild2 is comprised of 548 videos of around 2.7M frames annotated in terms of the seven primary expressions (i.e., anger, disgust, fear, happiness, sadness, surprise, and neutral).\n\nThe 5th Workshop and Compe::on on Affec:ve Behavior Analysis in-the-wild (ABAW) introduces four primary challenges. These include: 1) Valence-Arousal (VA) Es:ma:on i.e., how posi:ve/nega:ve and ac:ve/passive an emo:onal state is, 2) Expression (Expr) Classifica:on, and 3) Ac:on Unit (AU) Detec:on (specific movements of facial muscles from Facial Ac:on Coding System), and 4) a new 4th challenge called Emo:onal Reac:on Intensity (ERI) Es:ma:on using a new dataset of emo:on reac:on intensi:es (Hume Reac:on).",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "We train deep neural networks which use convolu:onal feature extractors  [59]  pretrained on a small sample dataset from AffectNet  [60]  to represent visual features. We op:mize all models using Mean Squared Error (MSE) loss:\n\nTo evaluate model performance, we measure the Pearson correla:on coefficient between the predicted emo:on intensity and the ground truth intensity label for each emo:on and calculate their average scores:",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Data Preprocessing And Normaliza8On",
      "text": "Re:naFace  [61]  was used to detect faces in each frame. The detected face coordinates were then used to crop the input frames to only show the detected faces, helping to prevent overfiong irrelevant por:ons of the image.\n\n32 evenly separated frames were sampled for each video. The resul:ng shape of our training data is a tensor with shape [x,  32, 112, 112, 3] , where x is the total number of data points used for training. We applied three data augmenta:on techniques for face analysis and emo:on recogni:on  [62] : brightness increases of up to 150% of the maximum value, horizontal flipping and rota:on (20% range).\n\nThe data were normalized by subtrac:ng the mean (µ) of each feature and dividing by the standard devia:on (σ), ensuring that each feature has a mean of θ and a standard devia:on of 1: We designed two types of frameworks for our network architecture. Both of them u:lize convolu:onal neural networks for feature extrac:on. To interpret the features across :mestamps, the first framework relies on long shortterm memory (LSTM) network  [63, 64]  and the second one is based on Transformer  [46] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Model Architecture",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Cnn-Lstm",
      "text": "Our first proposed network architecture, termed the CNN-LSTM framework (Figure  1 ), is primarily characterized by its two key components -a Convolu:onal Neural Network (CNN) and a Long Short-Term Memory (LSTM) network. This pairing of feature extrac:on and temporal analysis forms an effec:ve solu:on for processing spa:ally complex sequen:al data.\n\nIni:ally, the input data passes through the CNN component of our model. This stage transforms the high dimensional input into a reduced feature space, genera:ng a feature map that encapsulates the salient aspects of the input data. This feature map is then fed into the LSTM layer. The LSTM processes these features across :me steps, capturing the temporal dependencies within the sequen:al data.\n\nFollowing the LSTM stage, the extracted temporal features are further processed through a dense layer. This layer acts as a feature processor that dis:lls the temporal features into a refined format. Lastly, another dense layer takes the refined features and transforms them into the final output. This layer employs a sigmoid ac:va:on func:on to predict the final emo:on reac:on intensi:es in the range of [0, 1].",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Cnn-Transformer",
      "text": "In the second framework, we incorporate a Convolu:onal Neural Network (CNN) with a Transformer model (Figure  2 ) to perform complex spa:al and temporal analysis.\n\nThe framework begins with applying a CNN to the input tensor, which has the shape of (T,H,W,3) where T denotes the number of frames, H and W are the height and width of each frame. The CNN extracts the spa:al features from the input, resul:ng in a feature tensor of shape (T,h,w,d), where h and w are the reduced height and width, and d is the depth of the feature map.\n\nTo effec:vely capture spa:al rela:onships in these features, we pass them through a spa:al embedding layer, which reduces the dimensionality of the tensor to (T,h,w,d ′ ). Following this, we perform global average pooling to remove the spa:al dimensions, thus resul:ng in a feature vector of size (T,d ′ ).\n\nNext, we u:lize posi:on encoding to the feature vector and feed it into a TransformerEncoder layer characterized by parameters: dff (size of the feed-forward layer in the Transformer block), dmodel (embedding dimension) = d', and num heads (number of aeen:on heads). The output from this step maintains the shape (T, d') and can be expressed as:\n\nwhere Xpos is the posi:on-encoded feature vector.\n\nThe output from the TransformerEncoder is then fed into a 1D Temporal Convolu:onal Network (TCN) for another layer of encoding, this :me considering the temporal dependencies within the feature sequence. Following this, we again apply posi:on encoding to the TCN output and pass it through another TransformerEncoder layer, using the same parameters as before.\n\nFinally, we perform global average pooling on the output from the second TransformerEncoder layer. This pooled output is then passed through two consecu:ve dense layers. The final layer applies a sigmoid ac:va:on func:on to yield the final output.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments",
      "text": "In our experiments, we trained and tested four dis:nct models -two using a pre-trained ResNet18 and two using a pre-trained ResNet50 for the CNN backbones.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Hardware And Soeware Setup",
      "text": "All computa:ons were executed on an Nvidia A100 GPU, with models built using Keras. Our ini:al learning rate was set at 0.0002 and the total epoch is 50.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Model Configura8On",
      "text": "The LSTM layer comprised 512 units. For the Transformer, we set the number of layers to 3, the embedding dimension (dmodel) to 256, the size of the feedforward layer (dff) to 128, and the number of aeen:on heads to 8.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Training Process",
      "text": "The Adam op:mizer was employed for training, with a batch size of 128. Training was monitored using three callback func:ons:\n\n• EarlyStopping, which terminated training and restored the best model weights when the valida:on Pearson correla:on failed to improve by a minimum delta of 0.0001 over 12 epochs.\n\n• ReduceLROnPlateau, which reduced the learning rate by a factor of 0.5 when the valida:on Pearson correla:on did not improve by at least 0.0001 over 6 epochs.\n\n• ModelCheckpoint, which saved the model weights in a file whenever an improvement in the valida:on Pearson correla:on was detected.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "The performance of each model on the Hume-Reac:on dataset is provided in Table  1 . It was evaluated based on the Pearson Correla:on Coefficient. The baseline model achieved a Pearson Correla:on Coefficient of 0.249. The ResNet50 + Transformer model outperformed all the other models and the baseline, achieving the highest Pearson Correla:on Coefficient of 0.312. Meanwhile, the ResNet18 + LSTM model slightly underperformed when compared to the baseline. The results demonstrate the effec:veness of using Transformer models over LSTM when combined with CNNs, especially with ResNet50 as the base.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Model Performance",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "We evaluated a series of neural network architectures for emo:onal reac:on intensity es:ma:on using the Hume Reac:on dataset. Among the architectures tested, our model leveraging the combina:on of a ResNet50 and Transformer (referred as R50 + Trans.) achieved the highest performance when opera:ng on visual data solely. In contrast, the model coupling ResNet18 with LSTM (R18 + LSTM) displayed slightly inferior results compared to our baseline, implying the dis:nct advantage of Transformer over LSTM in this task.\n\nThe superior performance of the R50 + Trans. model can be aeributed to the comprehensive integra:on of CNN and Transformer. This robust architecture is capable of capturing both spa:al aeributes from CNN and temporal dependencies from Transformer, thereby allowing effec:ve es:ma:on of emo:onal intensity.\n\nIn our future work, we plan to explore addi:onal enhancements to our model. We will inves:gate alterna:ve feature representa:ons, refine model architectures, op:mize hyperparameters, and experiment with different data fusion strategies. We expect that these efforts will further boost the model's effec:veness in predic:ng emo:onal reac:on intensity. Despite the significant improvement brought by our best model, this is just the beginning of our journey towards a more sophis:cated and accurate es:ma:on of emo:onal intensity.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: CNN-LSTM architecture.",
      "page": 3
    },
    {
      "caption": "Figure 1: ), is primarily characterized",
      "page": 3
    },
    {
      "caption": "Figure 2: ) to perform complex spa:al and temporal",
      "page": 3
    },
    {
      "caption": "Figure 2: CNN-Transformer architecture.",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 1: It was evaluated based on",
      "data": [
        {
          "Model": "Baseline",
          "PCC": "0.249"
        },
        {
          "Model": "ResNet18 + LSTM",
          "PCC": "0.248"
        },
        {
          "Model": "ResNet50 + LSTM",
          "PCC": "0.259"
        },
        {
          "Model": "ResNet18 + Transformer",
          "PCC": "0.286"
        },
        {
          "Model": "ResNet50 + Transformer",
          "PCC": "0.312"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The expression of the emo/ons in man and animals",
      "authors": [
        "C Darwin",
        "P Prodger"
      ],
      "year": "1998",
      "venue": "The expression of the emo/ons in man and animals"
    },
    {
      "citation_id": "2",
      "title": "Recognizing ac?on units for facial expression analysis",
      "authors": [
        "Y.-I Tian",
        "T Kanade",
        "J Cohn"
      ],
      "year": "2001",
      "venue": "IEEE Transac/ons on pa6ern analysis and machine intelligence"
    },
    {
      "citation_id": "3",
      "title": "Training and profiling a pediatric emo?on recogni?on classifier on mobile devices",
      "authors": [
        "A Banerjee",
        "P Washington",
        "C Mutlu",
        "A Kline",
        "D Wall"
      ],
      "year": "2021",
      "venue": "Training and profiling a pediatric emo?on recogni?on classifier on mobile devices",
      "arxiv": "arXiv:2108.11754"
    },
    {
      "citation_id": "4",
      "title": "Leveraging video data from a digital smartphone au?sm therapy to train an emo?on detec?on classifier",
      "authors": [
        "C Hou",
        "H Kalantarian",
        "P Washington",
        "K Dunlap",
        "D Wall"
      ],
      "year": "2021",
      "venue": "medRxiv"
    },
    {
      "citation_id": "5",
      "title": "A mobile game for automa?c emo?on-labeling of images",
      "authors": [
        "H Kalantarian",
        "K Jedoui",
        "P Washington",
        "D Wall"
      ],
      "year": "2018",
      "venue": "IEEE transac/ons on games"
    },
    {
      "citation_id": "6",
      "title": "Labeling images with facial emo?on and the poten?al for pediatric healthcare",
      "authors": [
        "H Kalantarian",
        "K Jedoui",
        "P Washington",
        "Q Tariq",
        "K Dunlap",
        "J Schwartz",
        "D Wall"
      ],
      "year": "2019",
      "venue": "Ar/ficial intelligence in medicine"
    },
    {
      "citation_id": "7",
      "title": "Training an emo?on detec?on classifier using frames from a mobile therapeu?c game for children with developmental disorders",
      "authors": [
        "P Washington",
        "H Kalantarian",
        "J Kent",
        "A Husic",
        "A Kline",
        "E Leblanc",
        "C Hou",
        "C Mutlu",
        "K Dunlap",
        "Y Penev"
      ],
      "year": "2020",
      "venue": "Training an emo?on detec?on classifier using frames from a mobile therapeu?c game for children with developmental disorders",
      "arxiv": "arXiv:2012.08678"
    },
    {
      "citation_id": "8",
      "title": "Training affec?ve computer vision models by crowdsourcing soi-target labels",
      "authors": [
        "P Washington",
        "H Kalantarian",
        "J Kent",
        "A Husic",
        "A Kline",
        "E Leblanc",
        "C Hou",
        "C Mutlu",
        "K Dunlap",
        "Y Penev"
      ],
      "year": "2021",
      "venue": "Cogni/ve computa/on"
    },
    {
      "citation_id": "9",
      "title": "Prevalence and characteris?cs of au?sm spectrum disorder among children aged 8 yearsau?sm and developmental disabili?es monitoring network, 11 sites, united states",
      "authors": [
        "D Christensen",
        "K Braun",
        "J Baio",
        "D Bilder",
        "J Charles",
        "J Constan?no",
        "J Daniels",
        "M Durkin",
        "R Fitzgerald",
        "M Kurzius-Spencer"
      ],
      "year": "2012",
      "venue": "MMWR Surveillance Summaries"
    },
    {
      "citation_id": "10",
      "title": "Introduc?on. focus: au?sm spectrum disorders",
      "authors": [
        "K Ardhanareeswaran",
        "F Volkmar"
      ],
      "year": "2015",
      "venue": "The Yale Journal of Biology and Medicine"
    },
    {
      "citation_id": "11",
      "title": "Whioling down the wait ?me: exploring models to minimize the delay from ini?al concern to diagnosis and treatment of au?sm spectrum disorder",
      "authors": [
        "E Gordon-Lipkin",
        "J Foster",
        "G Peacock"
      ],
      "year": "2016",
      "venue": "Pediatric Clinics"
    },
    {
      "citation_id": "12",
      "title": "The classifica?on of abnormal hand movement to aid in au?sm detec?on: Machine learning study",
      "authors": [
        "A Lakkapragada",
        "A Kline",
        "O Mutlu",
        "K Paskov",
        "B Chrisman",
        "N Stockham",
        "P Washington",
        "D Wall"
      ],
      "year": "2022",
      "venue": "The classifica?on of abnormal hand movement to aid in au?sm detec?on: Machine learning study"
    },
    {
      "citation_id": "13",
      "title": "Toward con?nuous social phenotyping: analyzing gaze paoerns in an emo?on recogni?on task for children with au?sm through wearable smart glasses",
      "authors": [
        "A Nag",
        "N Haber",
        "C Voss",
        "S Tamura",
        "J Daniels",
        "J Ma",
        "B Chiang",
        "S Ramachandran",
        "J Schwartz",
        "T Winograd"
      ],
      "year": "2020",
      "venue": "Journal of medical Internet research"
    },
    {
      "citation_id": "14",
      "title": "Automa?c recogni?on of posed facial expression of emo?on in individuals with au?sm spectrum disorder",
      "authors": [
        "J Manfredonia",
        "A Bangerter",
        "N Manyakov",
        "S Ness",
        "D Lewin",
        "A Skalkin",
        "M Boice",
        "M Goodwin",
        "G Dawson",
        "R Hendren"
      ],
      "year": "2019",
      "venue": "Journal of au/sm and developmental disorders"
    },
    {
      "citation_id": "15",
      "title": "Ac?vity recogni?on with moving cameras and few training examples: applica?ons for detec?on of au?sm-related headbanging",
      "authors": [
        "P Washington",
        "A Kline",
        "O Mutlu",
        "E Leblanc",
        "C Hou",
        "N Stockham",
        "K Paskov",
        "B Chrisman",
        "D Wall"
      ],
      "venue": "Extended Abstracts of the 2021 CHI Conference on Human Factors in Compu/ng Systems"
    },
    {
      "citation_id": "16",
      "title": "A wearable social interac?on aid for children with au?sm",
      "authors": [
        "P Washington",
        "C Voss",
        "N Haber",
        "S Tanaka",
        "J Daniels",
        "C Feinstein",
        "T Winograd",
        "D Wall"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Compu/ng Systems"
    },
    {
      "citation_id": "17",
      "title": "A wearable social interac?on aid for children with au?sm",
      "authors": [
        "N Haber",
        "C Voss",
        "J Daniels",
        "P Washington",
        "A Fazel",
        "A Kline",
        "T De",
        "T Winograd",
        "C Feinstein",
        "D Wall"
      ],
      "year": "2020",
      "venue": "A wearable social interac?on aid for children with au?sm",
      "arxiv": "arXiv:2004.14281"
    },
    {
      "citation_id": "18",
      "title": "A review of and roadmap for data science and machine learning for the neuropsychiatric phenotype of au?sm",
      "authors": [
        "P Washington",
        "D Wall"
      ],
      "year": "2023",
      "venue": "A review of and roadmap for data science and machine learning for the neuropsychiatric phenotype of au?sm",
      "arxiv": "arXiv:2303.03577"
    },
    {
      "citation_id": "19",
      "title": "The poten?al for machine learning-based wearables to improve socializa?on in teenagers and adults with au?sm spectrum disorder-reply",
      "authors": [
        "C Voss",
        "N Haber",
        "D Wall"
      ],
      "year": "2019",
      "venue": "JAMA pediatrics"
    },
    {
      "citation_id": "20",
      "title": "Exploratory study examining the at-home feasibility of a wearable tool for social-affec?ve learning in children with au?sm",
      "authors": [
        "J Daniels",
        "J Schwartz",
        "C Voss",
        "N Haber",
        "A Fazel",
        "A Kline",
        "P Washington",
        "C Feinstein",
        "T Winograd",
        "D Wall"
      ],
      "year": "2018",
      "venue": "NPJ digital medicine"
    },
    {
      "citation_id": "21",
      "title": "Effect of wearable digital interven?on for improving socializa?on in children with au?sm spectrum disorder: a randomized clinical trial",
      "authors": [
        "C Voss",
        "J Schwartz",
        "J Daniels",
        "A Kline",
        "N Haber",
        "P Washington",
        "Q Tariq",
        "T Robinson",
        "M Desai",
        "J Phillips"
      ],
      "year": "2019",
      "venue": "Effect of wearable digital interven?on for improving socializa?on in children with au?sm spectrum disorder: a randomized clinical trial"
    },
    {
      "citation_id": "22",
      "title": "The performance of emo?on classifiers for children with parent-reported au?sm: quan?ta?ve feasibility study",
      "authors": [
        "H Kalantarian",
        "K Jedoui",
        "K Dunlap",
        "J Schwartz",
        "P Washington",
        "A Husic",
        "Q Tariq",
        "M Ning",
        "A Kline",
        "D Wall"
      ],
      "year": "2020",
      "venue": "JMIR mental health"
    },
    {
      "citation_id": "23",
      "title": "Guess what? towards understanding au?sm from structured video using facial affect",
      "authors": [
        "H Kalantarian",
        "P Washington",
        "J Schwartz",
        "J Daniels",
        "N Haber",
        "D Wall"
      ],
      "year": "2019",
      "venue": "Journal of healthcare informa/cs research"
    },
    {
      "citation_id": "24",
      "title": "Superpower glass",
      "authors": [
        "A Kline",
        "C Voss",
        "P Washington",
        "N Haber",
        "H Schwartz",
        "Q Tariq",
        "T Winograd",
        "C Feinstein",
        "D Wall"
      ],
      "year": "2019",
      "venue": "GetMobile: Mobile Compu/ng and Communica/ons"
    },
    {
      "citation_id": "25",
      "title": "Superpower glass: delivering unobtrusive real-?me social cues in wearable systems",
      "authors": [
        "C Voss",
        "P Washington",
        "N Haber",
        "A Kline",
        "J Daniels",
        "A Fazel",
        "T De",
        "B Mccarthy",
        "C Feinstein",
        "T Winograd"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 ACM Interna/onal Joint Conference on Pervasive and Ubiquitous Compu/ng: Adjunct"
    },
    {
      "citation_id": "26",
      "title": "Data-driven diagnos?cs and the poten?al of mobile ar?ficial intelligence for digital therapeu?c phenotyping in computa?onal psychiatry",
      "authors": [
        "P Washington",
        "N Park",
        "P Srivastava",
        "C Voss",
        "A Kline",
        "M Varma",
        "Q Tariq",
        "H Kalantarian",
        "J Schwartz",
        "R Patnaik"
      ],
      "year": "2020",
      "venue": "Data-driven diagnos?cs and the poten?al of mobile ar?ficial intelligence for digital therapeu?c phenotyping in computa?onal psychiatry"
    },
    {
      "citation_id": "27",
      "title": "Making emo?ons transparent: Google glass helps au?s?c kids understand facial expressions through augmented-reaiity therapy",
      "authors": [
        "N Haber",
        "C Voss",
        "D Wall"
      ],
      "year": "2020",
      "venue": "IEEE Spectrum"
    },
    {
      "citation_id": "28",
      "title": "Superpowerglass: a wearable aid for the at-home therapy of children with au?sm",
      "authors": [
        "P Washington",
        "C Voss",
        "A Kline",
        "N Haber",
        "J Daniels",
        "A Fazel",
        "T De",
        "C Feinstein",
        "T Winograd",
        "D Wall"
      ],
      "year": "2017",
      "venue": "Proceedings of the ACM on interac/ve, mobile, wearable and ubiquitous technologies"
    },
    {
      "citation_id": "29",
      "title": "Feasibility tes?ng of a wearable behavioral aid for social learning in children with au?sm",
      "authors": [
        "J Daniels",
        "N Haber",
        "C Voss",
        "J Schwartz",
        "S Tamura",
        "A Fazel",
        "A Kline",
        "P Washington",
        "J Phillips",
        "T Winograd"
      ],
      "year": "2018",
      "venue": "Applied clinical informa/cs"
    },
    {
      "citation_id": "30",
      "title": "Designing a holis?c at-home learning aid for au?sm",
      "authors": [
        "C Voss",
        "N Haber",
        "P Washington",
        "A Kline",
        "B Mccarthy",
        "J Daniels",
        "A Fazel",
        "T De",
        "C Feinstein",
        "T Winograd"
      ],
      "year": "2020",
      "venue": "Designing a holis?c at-home learning aid for au?sm",
      "arxiv": "arXiv:2002.04263"
    },
    {
      "citation_id": "31",
      "title": "The muse 2022 mul?modal sen?ment analysis challenge: humor, emo?onal reac?ons, and stress",
      "authors": [
        "L Christ",
        "S Amiriparian",
        "A Baird",
        "P Tzirakis",
        "A Kathan",
        "N Muller",
        "L Stappen",
        "E.-M Meßner",
        "A K¨ Onig",
        "A Cowen"
      ],
      "year": "2022",
      "venue": "Proceedings of the 3rd Interna/onal on Mul/modal Sen/ment Analysis Workshop and Challenge"
    },
    {
      "citation_id": "32",
      "title": "Abaw: Valence-arousal es?ma?on, expression recogni?on, ac?on unit detec?on & emo?onal reac?on intensity es?ma?on challenges",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "A Baird",
        "A Cowen",
        "S Zafeiriou"
      ],
      "year": "2023",
      "venue": "Abaw: Valence-arousal es?ma?on, expression recogni?on, ac?on unit detec?on & emo?onal reac?on intensity es?ma?on challenges",
      "arxiv": "arXiv:2303.01498"
    },
    {
      "citation_id": "33",
      "title": "Frame-level predic?on of facial expressions, valence, arousal and ac?on units for mobile devices",
      "authors": [
        "A Savchenko"
      ],
      "year": "2022",
      "venue": "Frame-level predic?on of facial expressions, valence, arousal and ac?on units for mobile devices",
      "arxiv": "arXiv:2203.13436"
    },
    {
      "citation_id": "34",
      "title": "Prior aided streaming network for mul?-task affec?ve recogni?onat the 2nd abaw2 compe??on",
      "authors": [
        "W Zhang",
        "Z Guo",
        "K Chen",
        "L Li",
        "Z Zhang",
        "Y Ding"
      ],
      "year": "2021",
      "venue": "Prior aided streaming network for mul?-task affec?ve recogni?onat the 2nd abaw2 compe??on",
      "arxiv": "arXiv:2107.03708"
    },
    {
      "citation_id": "35",
      "title": "Con?nuous emo?on recogni?on using visual-audio-linguis?c informa?on: A technical report for abaw3",
      "authors": [
        "S Zhang",
        "R An",
        "Y Ding",
        "C Guan"
      ],
      "year": "2022",
      "venue": "Con?nuous emo?on recogni?on using visual-audio-linguis?c informa?on: A technical report for abaw3"
    },
    {
      "citation_id": "36",
      "title": "Mul?-modal emo?on es?ma?on for in-the-wild videos",
      "authors": [
        "L Meng",
        "Y Liu",
        "X Liu",
        "Z Huang",
        "W Jiang",
        "T Zhang",
        "Y Deng",
        "R Li",
        "Y Wu",
        "J Zhao"
      ],
      "year": "2022",
      "venue": "Mul?-modal emo?on es?ma?on for in-the-wild videos",
      "arxiv": "arXiv:2203.13032"
    },
    {
      "citation_id": "37",
      "title": "Transformer-based mul?modal informa?on fusion for facial expression analysis",
      "authors": [
        "W Zhang",
        "F Qiu",
        "S Wang",
        "H Zeng",
        "Z Zhang",
        "R An",
        "B Ma",
        "Y Ding"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pa6ern Recogni/on"
    },
    {
      "citation_id": "38",
      "title": "Coarse-to-fine cascaded networks with smooth predic?ng for video facial expression recogni?on",
      "authors": [
        "F Xue",
        "Z Tan",
        "Y Zhu",
        "Z Ma",
        "G Guo"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pa6ern Recogni/on"
    },
    {
      "citation_id": "39",
      "title": "Facial ac?on unit recogni?on with mul?-models ensembling",
      "authors": [
        "W Jiang",
        "Y Wu",
        "F Qiao",
        "L Meng",
        "Y Deng",
        "C Liu"
      ],
      "year": "2022",
      "venue": "Facial ac?on unit recogni?on with mul?-models ensembling",
      "arxiv": "arXiv:2203.13046"
    },
    {
      "citation_id": "40",
      "title": "Facial ac?on coding system",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1978",
      "venue": "Environmental Psychology & Nonverbal Behavior"
    },
    {
      "citation_id": "41",
      "title": "Feature-based facial expression recogni?on: Sensi?vity analysis and experiments with a mul?layer perceptron",
      "authors": [
        "Z Zhang"
      ],
      "year": "1999",
      "venue": "Interna/onal journal of pa6ern recogni/on and Ar/ficial Intelligence"
    },
    {
      "citation_id": "42",
      "title": "Convolu?onal mkl based mul?modal emo?on recogni?on and sen?ment analysis",
      "authors": [
        "S Poria",
        "I Chaturvedi",
        "E Cambria",
        "A Hussain"
      ],
      "year": "2016",
      "venue": "Convolu?onal mkl based mul?modal emo?on recogni?on and sen?ment analysis"
    },
    {
      "citation_id": "43",
      "title": "Openface:ˇ an open source facial behavior analysis toolkit",
      "authors": [
        "T Baltrusai?s",
        "P Robinson",
        "L.-P Morency"
      ],
      "year": "2016",
      "venue": "2016 IEEE winter conference on applica/ons of computer vision (WACV)"
    },
    {
      "citation_id": "44",
      "title": "Mul?modal mul?task learning for dimensional and con?nuous emo?on recogni?on",
      "authors": [
        "S Chen",
        "Q Jin",
        "J Zhao",
        "S Wang"
      ],
      "year": "2017",
      "venue": "Proceedings of the 7th Annual Workshop on Audio/Visual Emo/on Challenge"
    },
    {
      "citation_id": "45",
      "title": "Con?nuous mul?modal emo?on predic?on based on long short term memory recurrent neural network",
      "authors": [
        "J Huang",
        "Y Li",
        "J Tao",
        "Z Lian",
        "Z Wen",
        "M Yang",
        "J Yi"
      ],
      "year": "2017",
      "venue": "Proceedings of the 7th Annual Workshop on Audio/Visual Emo/on Challenge"
    },
    {
      "citation_id": "46",
      "title": "Aoen?on is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural informa/on processing systems"
    },
    {
      "citation_id": "47",
      "title": "Facial expression recogni?on with convolu?onal neural networks",
      "authors": [
        "A Raghuvanshi",
        "V Choksi"
      ],
      "year": "2016",
      "venue": "CS231n Course Projects"
    },
    {
      "citation_id": "48",
      "title": "Automa?c facial expression recogni?on using features of salient facial patches",
      "authors": [
        "S Happy",
        "A Routray"
      ],
      "year": "2014",
      "venue": "Automa?c facial expression recogni?on using features of salient facial patches"
    },
    {
      "citation_id": "49",
      "title": "Framebased facial expression recogni?on using geometrical features",
      "authors": [
        "A Saeed",
        "A Al-Hamadi",
        "R Niese",
        "M Elzobi"
      ],
      "year": "2014",
      "venue": "Advances in human-computer interac/on"
    },
    {
      "citation_id": "50",
      "title": "Analysing affec?",
      "authors": [
        "D Kollias",
        "A Schulc",
        "E Hajiyev",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "Face and Gesture Recogni/on (FG 2020)"
    },
    {
      "citation_id": "51",
      "title": "Face behavior a la carte: Expressions, affect and ac?on units in a single network",
      "authors": [
        "D Kollias",
        "V Sharmanska",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Face behavior a la carte: Expressions, affect and ac?on units in a single network",
      "arxiv": "arXiv:1910.11111"
    },
    {
      "citation_id": "52",
      "title": "Deep affect predic?on in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "B Schuller",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Deep affect predic?on in-the-wild: Aff-wild database and challenge, deep architectures, and beyond"
    },
    {
      "citation_id": "53",
      "title": "Aff-wild: valence and arousal'inthewild'challenge",
      "authors": [
        "S Zafeiriou",
        "D Kollias",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "I Kotsia"
      ],
      "year": "2017",
      "venue": "Proceedings"
    },
    {
      "citation_id": "54",
      "title": "Abaw: Valence-arousal es?ma?on, expression recogni?on, ac?on unit detec?on & mul?-task learning challenges",
      "authors": [
        "D Kollias"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pa6ern Recogni/on"
    },
    {
      "citation_id": "55",
      "title": "Distribu?on matching for heterogeneous mul?-task learning: a largescale face study",
      "authors": [
        "D Kollias",
        "V Sharmanska",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "Distribu?on matching for heterogeneous mul?-task learning: a largescale face study",
      "arxiv": "arXiv:2105.03790"
    },
    {
      "citation_id": "56",
      "title": "Expression, affect, ac?on unit recogni?on: Aff-wild2, mul?-task learning and arcface",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, ac?on unit recogni?on: Aff-wild2, mul?-task learning and arcface",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "57",
      "title": "Proceedings of the IEEE/CVF Interna/onal Conference on Computer Vision",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Interna/onal Conference on Computer Vision"
    },
    {
      "citation_id": "58",
      "title": "Affect analysis in-the-wild: Valence-arousal, expressions, ac?on units and a unified framework",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "Affect analysis in-the-wild: Valence-arousal, expressions, ac?on units and a unified framework",
      "arxiv": "arXiv:2103.15792"
    },
    {
      "citation_id": "59",
      "title": "Densely connected convolu?onal networks",
      "authors": [
        "G Huang",
        "Z Liu",
        "L Van Der Maaten",
        "K Weinberger"
      ],
      "year": "2017",
      "venue": "Proceedings"
    },
    {
      "citation_id": "60",
      "title": "Affectnet: A database for facial expression, valence, and arousal compu?ng in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2019",
      "venue": "IEEE Transac/ons on Affec/ve Compu/ng"
    },
    {
      "citation_id": "61",
      "title": "Re?naface: Single-shot mul?-level face localisa?on in the wild",
      "authors": [
        "J Deng",
        "J Guo",
        "E Ververas",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "Proceedings"
    },
    {
      "citation_id": "62",
      "title": "Image based sta?c facial expression recogni?on with mul?ple deep network learning",
      "authors": [
        "Z Yu",
        "C Zhang"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 ACM on interna/onal conference on mul/modal interac/on"
    },
    {
      "citation_id": "63",
      "title": "Long short-term memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computa/on"
    },
    {
      "citation_id": "64",
      "title": "Lstm for dynamic emo?on and group emo?on recogni?on in the wild",
      "authors": [
        "B Sun",
        "Q Wei",
        "L Li",
        "Q Xu",
        "J He",
        "L Yu"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM interna/onal conference on mul/modal interac/on"
    }
  ]
}