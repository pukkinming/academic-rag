{
  "paper_id": "2401.16878v2",
  "title": "Enhancing Eeg Signal-Based Emotion Recognition With Synthetic Data: Diffusion Model Approach",
  "published": "2024-01-30T10:31:15Z",
  "authors": [
    "Gourav Siddhad",
    "Masakazu Iwamura",
    "Partha Pratim Roy"
  ],
  "keywords": [
    "Brain-Computer Interface (BCI)",
    "Deep Learning",
    "Diffusion Probabilistic Model",
    "Electroencephalography (EEG)",
    "Emotion Recognition",
    "Synthetic Data"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotions are crucial in human life, influencing perceptions, relationships, behaviour, and choices. Emotion recognition using Electroencephalography (EEG) in the Brain-Computer Interface (BCI) domain presents significant challenges, particularly the need for extensive datasets. This study aims to generate synthetic EEG samples that are similar to real samples but are distinct by augmenting noise to a conditional denoising diffusion probabilistic model, thus addressing the prevalent issue of data scarcity in EEG research. The proposed method is tested on the DEAP dataset, showcasing upto 4.21% improvement in classification performance when using synthetic data. This is higher compared to the traditional GAN-based and DDPM-based approaches. The proposed diffusion-based approach for EEG data generation appears promising in refining the accuracy of emotion recognition systems and marks a notable contribution to EEG-based emotion recognition. Our research further evaluates the effectiveness of state-of-the-art classifiers on EEG data, employing both real and synthetic data with varying noise levels.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "1: Introduction",
      "text": "Emotions are fundamental to human experience, influencing perceptions, relationships, behaviors, and decision-making. They significantly impact physical and mental health, mood, energy levels, and overall well-being. Understanding and responding effectively to emotions is crucial for both interpersonal interactions and personal success. Electroencephalography (EEG), a non-invasive and cost-effective neuroimaging technique, has emerged as a promising tool for capturing subtle changes in emotional states. It has been employed in Brain-Computer Interfaces (BCIs) to provide real-time responses to affective states  (1) . EEG has shown potential applications in human-computer interaction, emotion recognition  (2) , and medical diagnosis  (3) . However, despite its high temporal resolution, EEG-based emotion classification remains challenging due to inherent data complexities, including low spatial resolution, limited data availability, and the need for robust feature extraction and classification methods  (4) . Deep learning (DL) has emerged as a promising approach for EEG signal processing, offering the ability to automatically extract features from minimally preprocessed data  (5) . This contrasts with traditional methods that rely on manual feature engineering. However, the effectiveness of DL in BCI is heavily contingent on the availability of large, high-quality training datasets. Acquiring sufficient EEG data is challenging due to the time-consuming and resource-intensive nature of data collection, as well as the susceptibility of EEG signals to noise and artifacts  (6) . To address these limitations, generative models have gained prominence. While Generative Adversarial Networks (GANs) have shown promise in generating realistic data, they suffer from instabilities such as mode collapse. Diffusion Probabilistic Models (DDPMs), on the other hand, have demonstrated remarkable performance in image and audio generation, offering a more stable training process and high-quality outputs  (7) . While EEG emotion recognition has advanced through the application of DL techniques, the field is hindered by the scarcity and variability of available data. Acquiring large, balanced EEG datasets is challenging due to factors such as data privacy, ethical considerations, and the time-consuming nature of data collection  (8) . These limitations impede the development of robust DL models. Synthetic EEG data generation offers a potential solution to augment limited real-world data. However, generating synthetic data that accurately reflects the complexities of real EEG signals remains a significant challenge. While various methods have been proposed to generate synthetic EEG features or image-based representations, generating raw EEG data presents unique obstacles such as the need for meaningful evaluation metrics  (9) . Despite these challenges, neural-based generative models generate synthetic EEG data  (10) . Additionally, synthetic data, while being similar to the real data and sharing its properties, should not be same as the real data. This creates a contradiction and presents a challenge in generating and validating synthetic data. Diffusion models have demonstrated superior performance compared to traditional EEG data augmentation techniques such as noise addition, Fourier transform surrogates, and frequency shifting, particularly in the context of motor imagery classification  (11) . This study aims to extend these findings by generating synthetic EEG signals with following contributions:\n\n• This study employs a Conditional Denoising Diffusion Model to generate raw synthetic EEG data, providing a viable alternative to real datasets.\n\n• The diffusion model is trained with noise augmentation from a standard normal distribution, producing synthetic samples that capture meaningful variations of real EEG data rather than exact replicas.\n\n• By integrating synthetic data into the training process, classification accuracy of real EEG data is enhanced, with performance improving as the proportion of synthetic data increases.\n\n• The efficacy of the proposed method is thoroughly evaluated on the DEAP dataset, with both raw and synthetic EEG data, establishing its applicability and effectiveness in EEG data classification.\n\nThe rest of the paper is organized as follows. Section 2 provides a comprehensive overview of existing research on emotion classification and synthetic EEG data generation. Section 3 describes the methodology used for the whole experiment, including the dataset, diffusion model, and classifiers. Section 4 reports the results, and the work is concluded in Section 5.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "2: Related Work",
      "text": "This section provides a comprehensive overview of recent advancements in EEG-based emotion recognition and synthetic data generation, with a particular emphasis on the contributions of machine and deep learning techniques. The section is divided into two subsections: an analysis of contemporary EEG studies focused on emotion recognition and an exploration of how synthetic data generation can enhance these studies. The objective is to summarize the current state of the field and identify potential avenues for future research.\n\nA. Emotion Recognition. EEG-based emotion recognition remains a challenging task due to factors such as temporal asymmetry, signal instability, and inter-individual brain variability  (12) . However, recent advancements in DL have significantly improved emotion recognition techniques. The integration of Long Short-Term Memory (LSTM) models has been instrumental in enhancing EEG signal analysis  (13) .\n\nLSTMs effectively capture temporal dynamics and extract relevant features for emotion recognition  (14) . One notable approach involves augmenting LSTM with a multi-view dynamic emotion graph, which considers both electrode channel relationships and temporal information  (15) . The effectiveness of EEG-based emotion recognition is also contingent on the selection of discriminative features. For instance, one study demonstrated the extraction of narrowband rhythmic components from multichannel EEG, followed by the computation of short-time entropy and energy features for each component. This process was complemented by spatial filtering, with subsequent emotion recognition achieved using Support Vector Machine (SVM)  (16) . Another study employed Principal Component Analysis (PCA) for dimensionality reduction prior to SVM classification, further highlight-ing the effectiveness of SVM in EEG-based emotion recognition  (17) .\n\nAttention-based methods have emerged as a powerful tool for enhancing neural network performance in EEG analysis by focusing on relevant input segments, drawing inspiration from fields such as psychology, neuroscience, and machine learning  (18, 19) . One approach involves a multi-scale feature fusion network (AM-MSFFN) that incorporates highlevel features at different scales  (20) . Another study utilizes a pre-trained convolution capsule network combined with an attention mechanism for emotion recognition  (21)     model architecture and noise scheduling, and the augmentation module developed specifically for this study.   p(x, y ). The DDPM (  44 ) is adapted for conditional signal generation. A target sample y 0 is generated by the conditional DDPM through T refinement stages, starting from a pure noise sample y T ∼ N (0, I). The model iteratively refines the output sample based on learned conditional distributions p θ y t-1 | y t , x , resulting in a sequence y T -1 , y T -2 , . . . , y 0 where the final output y 0 is distributed according to p(y | x), as illustrated in Fig 1 . The iterative refinement process involves a forward diffusion step that adds Gaussian noise to the output via a fixed Markov chain q y t | y t-1 , followed by a reverse process that recovers the signal from the noise, conditioned on x. The denoising model f θ learns this reverse chain by utilizing both the source and noisy target samples to estimate the noise.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "3: Methodology",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Conditional Denoising Diffusion",
      "text": "A.1. Gaussian Diffusion Process. Following  (44) , the forward diffusion process q gradually adds Gaussian noise to target sample y 0 over T iterations, as defined by\n\n(1)\n\nwhere α 1:t are hyper-parameters such that 0 < α t < 1, determining the noise variance. The variance of the random variables is attenuated by √ α t to ensure boundedness as t → ∞.\n\nThe distribution of y t given y 0 is expressed as\n\nwhere γ t = t i=1 α i . The posterior distribution of y t-1 given (y 0 , y t ) is given by\n\nA.2. Optimizing the Denoising Model. The denoising network, crucial for inference in diffusion models presented in Section A.3, is conditioned on source sample x. The neural denoising model f θ is trained to reconstruct the noiseless target sample y 0 using x and a noisy target y. y is defined as\n\nThis definition aligns the noisy target sample y with the marginal distribution in the forward diffusion process given in eq. (  3 ). The model f θ (x, y, γ) predicts the noise vector ϵ, conditioned on γ, similar to generative models  (46, 47) . The training objective for f θ is given by\n\nwhere ϵ ∼ N (0, I), (x, y) is sampled from training data, p ∈ {1, 2}, and γ ∼ p(γ). The distribution of γ significantly impacts model quality and outputs, as detailed in Section A.4.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A.3. Inference Via Iterative",
      "text": "Refinement. The inference process is defined as a reverse Markovian process (similar to (42)), starting from Gaussian noise y T p θ (y 0:T\n\nThe inference process is defined in terms of isotropic Gaussian conditional distributions, p θ y t-1 | y t , x . When the forward process noise variance α 1:T ≈ 1, the optimal reverse process p y t-1 | y t , x is approximately Gaussian  (43) . Accordingly, the choice of Gaussian conditionals in the inference process  (11)  provides a reasonable fit to the true reverse process. Moreover, for the process to start from pure Gaussian noise, 1 -γ T needs to be sufficiently large (i.e., close to one) from eq. (  8 ), aligning with the prior in eq. (  10 ), allowing the sampling process to start at pure Gaussian noise.\n\nThe denoising model f θ estimates ϵ from noisy samples y including y t . Thus, approximating y 0 by reconfiguring eq. (  7 )\n\nFollowing (  44 ), ŷ0 is substituted into the posterior distribution of q y t-1 | y 0 , y t in eq. (  4 ), to parameterize the mean of p θ y t-1 | y t , x as\n\nand the variance of p θ y t-1 | y t , x is set to (1 -α t ), in the forward process  (44) . Thus, each iterative refinement step in the model is structured as\n\nwhere ϵ t ∼ N (0, I). This resembles the Langevin dynamics step with f θ providing an estimate of the gradient of the data log density.\n\nA.4. Model Architecture and Noise Schedulers. The model adopts a U-Net architecture similar to the one in DDPM  (44) , integrating self-attention and modifications from  (48) . Specifically, it replaces DDPM's original residual blocks with those from BigGAN and rescales skip connections by\n\n2 . The number of residual blocks and channel multipliers at various resolutions is increased to enhance the model. The model is conditioned on input x by introducing noise to the source sample and concatenating it with y t along the channel dimension. The training noise schedule follows  (47) , employing a piece-wise distribution for γ, p(γ) = T t=1 1 T U (γ t-1 , γ t ). Training involves uniformly sampling a time step t ∼ {0, . . . , T } and subsequently sampling γ ∼ U (γ t-1 , γ t ). This study adopts a technique from  (47) , which efficiently generates samples by directly conditioning on γ rather than on t as in  (44) , allowing flexibility in choosing diffusion steps and noise schedules. Assuming a linear noise schedule, the maximum diffusion steps is set to 100. An inexpensive hyper-parameter search over start and end noise levels B Classifiers was performed, avoiding the need for model retraining  (47) . The diffusion model employs a denoising U-Net (Fig 2 ) to process noisy target and source conditioning samples.\n\nA.5. Augmentation Module. The diffusion process involves two samples: a source sample x and a target sample y. To modify the source sample, an augmentation module is employed that adds noise from a standard normal distribution Z, creating a condition sample as shown in Fig  1 . This is formalized by:\n\nwhere ∆ exhibits variability. These variations are crucial for the diffusion model as they lead to the generation of samples influenced by noise. While the model relies on real samples as a foundation for learning, it adapts to produce new samples with subtle differences.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B.",
      "text": "Classifiers. This study employs a comparative analysis of three state-of-the-art classifiers for EEG-based emotion recognition: SVM (49), EEGNet  (28) , and TSception  (29) . SVM, a traditional machine learning algorithm, excels in classification tasks by maximizing the margin between different classes. EEGNet, a CNN-based architecture, leverages depth-wise and separable convolutions to efficiently extract spatio-temporal features from EEG signals. TSception, a more recent approach, incorporates dynamic temporal and asymmetric spatial layers to capture complex EEG patterns associated with emotions. While TSception serves as the primary classifier in this research, SVM and EEGNet are included to provide a comprehensive evaluation of the generated synthetic data's impact on classification performance.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "4: Results And Discussion",
      "text": "The efficacy of this study is evaluated using real EEG data and synthetic data generated with varying noise levels (∆).\n\nEvaluation is conducted on SVM, EEGNet, and TSception classifiers, with the following key experimental aspects:\n\n• Training classifiers on purely real data, followed by a combination of real and synthetic data.\n\n• Investigating accuracy changes relative to the number of synthetic EEG samples included in training.\n\n• Assessing classification accuracy variations with synthetic data generated with different noise levels (∆) in the diffusion model.\n\n• Comparing performance outcomes when using synthetic data generated by vanilla diffusion and GAN.\n\nAll classifiers undergo testing on a dedicated real data test set.\n\nA CPU, and an NVIDIA RTX A2000 12GB GPU. This hardware facilitated the implementation of DL models using Python 3.10 and the PyTorch library. The Adam optimizer, known for its computational efficiency, was used with default parameters (η = 0.001, β 1 = 0.9, β 2 = 0.999), and a linear warmup was applied over 10,000 training steps. EEG-Net and TSception were trained for 100 epochs, with batches of 16 and a learning rate of 1e -4. For SVM, the Radial Basis Function (RBF) kernel from scikit-learn (52) was used with default settings. Classification accuracy was determined through stratified five-fold cross-validation, averaging the results for comprehensive assessment.\n\nFor diffusion experiments, T = 500 was maintained and the γ t values were uniformly spaced. While larger T values can potentially improve model performance, using T = 2000 as in  (42)  results in noiseless samples, contrary to the goals of this study. Additionally, this setting also accelerates sample generation (or inference) compared to earlier models  (44, 48) , which required between 1000 and 2000 diffusion steps. The diffusion model training involved batches of 32 and a 0.2 dropout rate for 1 million steps, utilizing the latest checkpoint.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "C. Effect Of Synthetic Data In Classification",
      "text": "Performance. Synthetic samples from the diffusion model are created by introducing slight variations to real samples through augmented noise (∆), aiming to closely resemble the original data without being identical. Synthetic samples generated through diffusion correspond one-to-one with real samples, ensuring equal quantities of both. The classifier was trained on a mix of real and synthetic data and then tested on real data alone. This approach was compared to training exclusively with real data, under the hypothesis that if the synthetic data offered no additional information, the performances would be identical. Tab 1 presents a comprehensive comparison of classifier performance (SVM, EEGNet, and TSception) on the DEAP dataset within a 95% confidence interval, adding robustness to the findings, under various data augmentation conditions (∆ = 0.01, 0.05, 0.1). The 'Real'\n\nTable  1 . Comparison of SVM, EEGNet, and TSception classifiers' accuracy and performance on the DEAP dataset. This analysis includes scenarios with real data alone, and combined real and synthetic data generated by our method at noise levels ∆ = 0.01, 0.05, 0.1, vanilla conditional diffusion (∆ = 0), and GAN. The accuracies are presented with 95% confidence intervals, with real and synthetic samples used in equal proportions (i.e., 100%) in the \"Real + Synthetic\" scenarios.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Baseline Vanilla Diffusion",
      "text": "Proposed Method Proposed Method Proposed Method GAN (50) column shows the baseline accuracy of each classifier when trained and tested on real data only. This serves as a reference point for evaluating the impact of synthetic data. The columns under 'Vanilla Diffusion,' 'Proposed Method,' and 'GAN' show the accuracy and gain (or loss) when classifiers are trained on a combination of real and synthetic data. The gain values indicate the improvement or decline in performance compared to the baseline. They refer to synthetic data generated via vanilla diffusion model, our approach, and GAN, respectively. All classifiers were tested exclusively on the real dataset.\n\nSVM shows modest improvements in accuracy with synthetic data. The highest gain is observed at ∆ = 0.05 for Arousal and ∆ = 0.01 for Dominance, Liking, and Valence. Generally, EEGNet benefits more from including synthetic data, particularly at lower noise levels (∆ = 0.01 and 0.05). However, the performance drops when trained with GANgenerated data. TSception shows significant improvements in accuracy with synthetic data, especially at ∆ = 0.01 across all affective states. Like EEGNet, its performance decreases with GAN-generated data. When trained with ∆ = 0, implying no noise addition, the generated synthetic data is identical to real data, and using it for training is the same as using real data twice, which did not alter classifier accuracy.\n\nTraining with synthetic data alongside real data enhances classifier performance on real data tests. During training, the combination of real and synthetic data (∆ = 0.01) yields the highest accuracy improvements. Lower noise (∆) levels consistently result in more significant accuracy gains. All classifiers show a decrease in performance when trained with Tab 3 presents the maximum accuracy achieved by TSception when trained on both real and synthetic EEG data, compared to using only real data. The results demonstrate a consistent improvement in classification accuracy when incorporating synthetic samples. Notably, arousal (blue) exhibits the highest accuracy, followed by valence (yellow), dominance (red), and liking (green). The results highlight a direct relationship between increased synthetic data inclusion and improved classifier performance, with the most significant accuracy observed at a high mix of synthetic data (500% synthetic). Overall, these results suggest that adding synthetic data generated with a slight noise to the classifier's training improves its ability for emotion recognition from EEG data.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "5: Conclusion",
      "text": "",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The method-",
      "page": 3
    },
    {
      "caption": "Figure 1: Forward and reverse diffusion, denoising, and augmentation steps",
      "page": 3
    },
    {
      "caption": "Figure 2: U-Net architecture of diffusion model. The noisy source sample",
      "page": 4
    },
    {
      "caption": "Figure 3: , the model’s",
      "page": 7
    },
    {
      "caption": "Figure 3: Line chart depicting the variation in classifier accuracy across emotional states with increasing proportions of diffusion-generated data (∆= 0.01)",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Comparison of SVM, EEGNet, and TSception classifiers’ accuracy and performance on the DEAP dataset. This analysis includes scenarios",
      "page": 6
    },
    {
      "caption": "Table 2: Impact of training TSception by combining real data and Gaus-",
      "page": 6
    },
    {
      "caption": "Table 3: Maximum accuracy achieved in ablation experiments by training",
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "EEGbased cognitive state classification and analysis of brain dynamics using deep ensemble model and graphical brain network",
      "authors": [
        "Debashis Das Chakladar",
        "Partha Roy",
        "Masakazu Iwamura"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "2",
      "title": "Human emotion recognition from EEG-based brain-computer interface using machine learning: a comprehensive review",
      "authors": [
        "Asmaa Essam H Houssein",
        "Abdelmgeid A Hammad",
        "Ali"
      ],
      "year": "2022",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "3",
      "title": "Review on emotion recognition based on electroencephalography",
      "authors": [
        "Haoran Liu",
        "Ying Zhang",
        "Yujun Li",
        "Xiangyi Kong"
      ],
      "year": "2021",
      "venue": "Frontiers in Computational Neuroscience"
    },
    {
      "citation_id": "4",
      "title": "An intelligent EEG classification methodology based on sparse representation enhanced deep learning networks",
      "authors": [
        "Jing-Shan Huang",
        "Yang Li",
        "Bin-Qiang Chen",
        "Chuang Lin",
        "Bin Yao"
      ],
      "year": "2020",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "5",
      "title": "Linking attention-based multiscale CNN with dynamical GCN for driving fatigue detection",
      "authors": [
        "Hongtao Wang",
        "Linfeng Xu",
        "Anastasios Bezerianos",
        "Chuangquan Chen",
        "Zhiguo Zhang"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "6",
      "title": "Optimizing the ICA-based removal of ocular EEG artifacts from free viewing experiments",
      "authors": [
        "Olaf Dimigen"
      ],
      "year": "2020",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "7",
      "title": "Diffusion probabilistic models beat GANs on medical images",
      "authors": [
        "Gustav Müller-Franzes",
        "Jan Niehues",
        "Firas Khader",
        "Soroosh Tayebi Arasteh",
        "Christoph Haarburger",
        "Christiane Kuhl",
        "Tianci Wang",
        "Tianyu Han",
        "Sven Nebelung",
        "Jakob Kather"
      ],
      "year": "2022",
      "venue": "Diffusion probabilistic models beat GANs on medical images",
      "arxiv": "arXiv:2212.07501"
    },
    {
      "citation_id": "8",
      "title": "Wavelet ELM-AE based data augmentation and deep learning for efficient emotion recognition using EEG recordings",
      "authors": [
        "Berna Ari",
        "Kamran Siddique",
        "Ömer Faruk Alçin",
        "Muzaffer Aslan",
        "Abdulkadir Şengür",
        "Raja Majid"
      ],
      "year": "2022",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "9",
      "title": "Survey on the research direction of EEG-based signal processing",
      "authors": [
        "Congzhong Sun",
        "Chaozhou Mou"
      ],
      "year": "2023",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "10",
      "title": "Simulating brain signals: Creating synthetic EEG data via neural-based generative models for improved ssvep classification",
      "authors": [
        "Nik Khadijah",
        "Nik Aznan",
        "Amir Atapour-Abarghouei",
        "Stephen Bonner",
        "Jason Connolly",
        "Noura Al Moubayed",
        "Toby Breckon"
      ],
      "year": "2019",
      "venue": "2019 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "11",
      "title": "Data augmentation for EEG motor imagery classification using diffusion model",
      "authors": [
        "Nutapol Soingern",
        "Akraradet Sinsamersuk",
        "Itthi Chatnuntawech",
        "Chaklam Silpasuwanchai"
      ],
      "year": "2023",
      "venue": "International Conference on Data Science and Artificial Intelligence"
    },
    {
      "citation_id": "12",
      "title": "Emotion recognition by discriminating EEG segments with high affective content from automatically selected relevant channels",
      "authors": [
        "Parthana Sarma",
        "Shovan Barma"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "13",
      "title": "A convolutional long short-term memory-based neural network for epilepsy detection from EEG",
      "authors": [
        "Md Nurul",
        "Ahad Tawhid",
        "Siuly Siuly",
        "Tianning Li"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "14",
      "title": "Spatial-temporal feature fusion neural network for EEG-based emotion recognition",
      "authors": [
        "Zhe Wang",
        "Yongxiong Wang",
        "Jiapeng Zhang",
        "Chuanfei Hu",
        "Zhong Yin",
        "Yu Song"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "15",
      "title": "LSTM-enhanced multi-view dynamical emotion graph representation for EEG signal recognition",
      "authors": [
        "Guixun Xu",
        "Wenhui Guo",
        "Yanjiang Wang"
      ],
      "year": "2023",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "16",
      "title": "Md Rabiul Islam, and Md Khademul Islam Molla. Emotion recognition using narrowband spatial features of electroencephalography",
      "authors": [
        "Iffat Farhana",
        "Jungpil Shin",
        "Shabbir Mahmood"
      ],
      "year": "2023",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "17",
      "title": "A comparative analysis of machine learning methods for emotion recognition using EEG and peripheral physiological signals",
      "authors": [
        "Vikrant Doma",
        "Matin Pirouz"
      ],
      "year": "2020",
      "venue": "Journal of Big Data"
    },
    {
      "citation_id": "18",
      "title": "EEG-based emotion recognition via transformer neural architecture search",
      "authors": [
        "Chang Li",
        "Zhongzhen Zhang",
        "Xiaodong Zhang",
        "Guoning Huang",
        "Yu Liu",
        "Xun Chen"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Industrial Informatics"
    },
    {
      "citation_id": "19",
      "title": "Debi Prosad Dogra, and Partha Pratim Roy. Efficacy of transformer networks for classification of EEG data",
      "authors": [
        "Gourav Siddhad",
        "Anmol Gupta"
      ],
      "year": "2024",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "20",
      "title": "Emotion recognition via multi-scale feature fusion network and attention mechanism",
      "authors": [
        "Yiye Jiang",
        "Songyun Xie",
        "Xinzhou Xie",
        "Yujie Cui",
        "Hao Tang"
      ],
      "year": "2023",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "21",
      "title": "EEG emotion recognition based on the attention mechanism and pre-trained convolution capsule network",
      "authors": [
        "Shuaiqi Liu",
        "Zeyao Wang",
        "Yanling An",
        "Jie Zhao",
        "Yingying Zhao",
        "Yu-Dong Zhang"
      ],
      "year": "2023",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "22",
      "title": "attention-based neural network for EEG emotion recognition",
      "authors": [
        "Guowen Xiao",
        "Meng Shi",
        "Mengwen Ye",
        "Bowen Xu",
        "Zhendi Chen",
        "Quansheng Ren"
      ],
      "year": "2022",
      "venue": "Cognitive Neurodynamics"
    },
    {
      "citation_id": "23",
      "title": "AMDET: Attention based multiple dimensions EEG transformer for emotion recognition",
      "authors": [
        "Yongling Xu",
        "Yang Du",
        "Ling Li",
        "Honghao Lai",
        "Jing Zou",
        "Tianying Zhou",
        "Lushan Xiao",
        "Li Liu",
        "Pengcheng Ma"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "24",
      "title": "Cascaded convolutional neural network architecture for speech emotion recognition in noisy conditions",
      "authors": [
        "Youngja Nam",
        "Chankyu Lee"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "25",
      "title": "Emotion recognition using three-dimensional feature and convolutional neural network from multichannel EEG signals",
      "authors": [
        "Hao Chao",
        "Liang Dong"
      ],
      "year": "2020",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "26",
      "title": "A novel bi-hemispheric discrepancy model for EEG emotion recognition",
      "authors": [
        "Yang Li",
        "Lei Wang",
        "Wenming Zheng",
        "Yuan Zong",
        "Lei Qi",
        "Zhen Cui",
        "Tong Zhang",
        "Tengfei Song"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "27",
      "title": "EEG-based emotion recognition using 4D convolutional recurrent neural network",
      "authors": [
        "Fangyao Shen",
        "Guojun Dai",
        "Guang Lin",
        "Jianhai Zhang",
        "Wanzeng Kong",
        "Hong Zeng"
      ],
      "year": "2020",
      "venue": "Cognitive Neurodynamics"
    },
    {
      "citation_id": "28",
      "title": "EEGNet: a compact convolutional neural network for EEG-based brain-computer interfaces",
      "authors": [
        "Amelia Vernon J Lawhern",
        "Nicholas Solon",
        "Waytowich",
        "Stephen M Gordon",
        "P Chou",
        "Brent Hung",
        "Lance"
      ],
      "year": "2018",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "29",
      "title": "TSception: Capturing temporal dynamics and spatial asymmetry from EEG for emotion recognition",
      "authors": [
        "Yi Ding",
        "Neethu Robinson",
        "Su Zhang",
        "Qiuhao Zeng",
        "Cuntai Guan"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "30",
      "title": "A multi-dimensional graph convolution network for EEG emotion recognition",
      "authors": [
        "Guanglong Du",
        "Jinshao Su",
        "Linlin Zhang",
        "Kang Su",
        "Xueqian Wang",
        "Shaohua Teng",
        "Peter Liu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "31",
      "title": "Siam-GCAN: a siamese graph convolutional attention network for EEG emotion recognition",
      "authors": [
        "Hong Zeng",
        "Qi Wu",
        "Yanping Jin",
        "Haohao Zheng",
        "Mingming Li",
        "Yue Zhao",
        "Hua Hu",
        "Wanzeng Kong"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "32",
      "title": "Emotion recognition using spatial-temporal EEG features through convolutional graph attention network",
      "authors": [
        "Zhongjie Li",
        "Gaoyan Zhang",
        "Longbiao Wang",
        "Jianguo Wei",
        "Jianwu Dang"
      ],
      "year": "2023",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "33",
      "title": "Semi-supervised regression with adaptive graph learning for EEG-based emotion recognition",
      "authors": [
        "Tianhui Sha",
        "Yikai Zhang",
        "Yong Peng",
        "Wanzeng Kong"
      ],
      "year": "2023",
      "venue": "Mathematical Biosciences and Engineering"
    },
    {
      "citation_id": "34",
      "title": "Graph convolutional neural network based on channel graph fusion for EEG emotion recognition",
      "authors": [
        "Wen Qian",
        "Yuxin Ding",
        "Weiyi Li"
      ],
      "year": "2022",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "35",
      "title": "EEG-based emotion recognition using spectral graph convolutional neural network based on partial directed coherence",
      "authors": [
        "Dae-Hyeon Kim",
        "Young-Seok Choi"
      ],
      "year": "2022",
      "venue": "2022 IEEE International Conference on Consumer Electronics-Asia (ICCE-Asia)"
    },
    {
      "citation_id": "36",
      "title": "Deep learning approach to generate a synthetic cognitive psychology behavioral dataset",
      "authors": [
        "Jung-Gu Choi",
        "Yoonjin Nah",
        "Inhwan Ko",
        "Sanghoon Han"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "37",
      "title": "Synthetic data generation using combinatorial testing and variational autoencoder",
      "authors": [
        "Krishna Khadka",
        "Jaganmohan Chandrasekaran",
        "Yu Lei",
        "Raghu Kacker",
        "Richard Kuhn"
      ],
      "year": "2023",
      "venue": "2023 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)"
    },
    {
      "citation_id": "38",
      "title": "A novel approach to create synthetic biomedical signals using BiRNN",
      "authors": [
        "Andres Hernandez-Matamoros",
        "Hamido Fujita",
        "Hector Perez-Meana"
      ],
      "year": "2020",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "39",
      "title": "Synthetic biological signals machine-generated by gpt-2 improve the classification of EEG and EMG through data augmentation",
      "authors": [
        "Jordan J Bird",
        "Michael Pritchard",
        "Antonio Fratini",
        "Anikó Ekárt",
        "Diego Faria"
      ],
      "year": "2021",
      "venue": "IEEE Robotics and Automation Letters"
    },
    {
      "citation_id": "40",
      "title": "A generative model to synthesize EEG data for epileptic seizure prediction",
      "authors": [
        "Khansa Rasheed",
        "Junaid Qadir",
        "Terence O'brien",
        "Levin Kuhlmann",
        "Adeel Razi"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "41",
      "title": "Generalized generative deep learning models for biosignal synthesis and modality transfer",
      "authors": [
        "Theekshana Dissanayake",
        "Tharindu Fernando",
        "Simon Denman"
      ],
      "year": "2022",
      "venue": "Sridha Sridharan, and Clinton Fookes"
    },
    {
      "citation_id": "42",
      "title": "Image super-resolution via iterative refinement",
      "authors": [
        "Chitwan Saharia",
        "Jonathan Ho",
        "William Chan",
        "Tim Salimans",
        "David Fleet",
        "Mohammad Norouzi"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "43",
      "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
      "authors": [
        "Jascha Sohl-Dickstein",
        "Eric Weiss",
        "Niru Maheswaranathan",
        "Surya Ganguli"
      ],
      "year": "2015",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "44",
      "title": "Denoising diffusion probabilistic models",
      "authors": [
        "Jonathan Ho",
        "Ajay Jain",
        "Pieter Abbeel"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "45",
      "title": "U-net: Convolutional networks for biomedical image segmentation",
      "authors": [
        "Olaf Ronneberger",
        "Philipp Fischer",
        "Thomas Brox"
      ],
      "year": "2015",
      "venue": "Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference"
    },
    {
      "citation_id": "46",
      "title": "Generative modeling by estimating gradients of the data distribution",
      "authors": [
        "Yang Song",
        "Stefano Ermon"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "47",
      "title": "Estimating gradients for waveform generation",
      "authors": [
        "Nanxin Chen",
        "Yu Zhang",
        "Heiga Zen",
        "Ron Weiss",
        "Mohammad Norouzi",
        "William Chan",
        "Wavegrad"
      ],
      "year": "2020",
      "venue": "Estimating gradients for waveform generation",
      "arxiv": "arXiv:2009.00713"
    },
    {
      "citation_id": "48",
      "title": "Score-based generative modeling through stochastic differential equations",
      "authors": [
        "Yang Song",
        "Jascha Sohl-Dickstein",
        "P Diederik",
        "Abhishek Kingma",
        "Stefano Kumar",
        "Ben Ermon",
        "Poole"
      ],
      "year": "2020",
      "venue": "Score-based generative modeling through stochastic differential equations",
      "arxiv": "arXiv:2011.13456"
    },
    {
      "citation_id": "49",
      "title": "Support-vector networks",
      "authors": [
        "Corinna Cortes",
        "Vladimir Vapnik"
      ],
      "year": "1995",
      "venue": "Machine Learning"
    },
    {
      "citation_id": "50",
      "title": "Generative adversarial networks",
      "authors": [
        "Ian Goodfellow",
        "Jean Pouget-Abadie",
        "Mehdi Mirza",
        "Bing Xu",
        "David Warde-Farley",
        "Sherjil Ozair",
        "Aaron Courville",
        "Yoshua Bengio"
      ],
      "year": "2020",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "51",
      "title": "DEAP: A database for emotion analysis; using physiological signals",
      "authors": [
        "Sander Koelstra",
        "Christian Muhl",
        "Mohammad Soleymani",
        "Jong-Seok Lee",
        "Ashkan Yazdani",
        "Touradj Ebrahimi",
        "Anton Thierry Pun",
        "Ioannis Nijholt",
        "Patras"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "52",
      "title": "Scikit-learn: Machine learning in Python",
      "authors": [
        "F Pedregosa",
        "G Varoquaux",
        "A Gramfort",
        "V Michel",
        "B Thirion",
        "O Grisel",
        "M Blondel",
        "P Prettenhofer",
        "R Weiss",
        "V Dubourg",
        "J Vanderplas",
        "A Passos",
        "D Cournapeau",
        "M Brucher",
        "M Perrot",
        "E Duchesnay"
      ],
      "year": "2011",
      "venue": "Journal of Machine Learning Research"
    }
  ]
}