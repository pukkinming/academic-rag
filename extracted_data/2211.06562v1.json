{
  "paper_id": "2211.06562v1",
  "title": "Improving The Robustness Of Distilhubert To Unseen Noisy Conditions Via Data Augmentation, Curriculum Learning, And Multi-Task Enhancement",
  "published": "2022-11-12T03:50:22Z",
  "authors": [
    "Heitor R. Guimarães",
    "Arthur Pimentel",
    "Anderson R. Avila",
    "Mehdi Rezagholizadeh",
    "Tiago H. Falk"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Self-supervised speech representation learning aims to extract meaningful factors from the speech signal that can later be used across different downstream tasks, such as speech and/or emotion recognition. Existing models, such as HuBERT, however, can be fairly large thus may not be suitable for edge speech applications. Moreover, realistic applications typically involve speech corrupted by noise and room reverberation, hence models need to provide representations that are robust to such environmental factors. In this study, we build on the so-called DistilHu-BERT model, which distils HuBERT to a fraction of its original size, with three modifications, namely: (i) augment the training data with noise and reverberation, while the student model needs to distill the clean representations from the teacher model; (ii) introduce a curriculum learning approach where increasing levels of noise are introduced as the model trains, thus helping with convergence and with the creation of more robust representations; and (iii) introduce a multi-task learning approach where the model also reconstructs the clean waveform jointly with the distillation task, thus also acting as an enhancement step to ensure additional environment robustness to the representation. Experiments on three SUPERB tasks show the advantages of the proposed method not only relative to the original DistilHuBERT, but also to the original HuBERT, thus showing the advantages of the proposed method for \"in the wild\" edge speech applications.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Self-supervised learning (SSL) enables the learning of meaningful and disentangled features from unlabeled speech data that can be used across different tasks. These representations are becoming essential and are now part of several state-of-the-art speech applications  [2, 6] . Widely-used representations include wav2vec 2.0  [2]  and HuBERT  [6] . The latter is an SSL model that extracts features directly from the raw speech signal. Via an offline k-means clustering step, the model learns combined acoustic and language representations from masked inputs using a BERT-like reconstruction loss.\n\nExisting models, however, have two limitations when edge applications are considered: (1) their large model sizes, and (2) their robustness to environmental conditions not seen during training. For example, pre-trained models can range from 95 million parameters to 2 billion  [1] , which makes training and deployment on edge devices difficult. To overcome this issue, different model compression schemes have been explored. DistilHuBERT, for example, applied knowledge distillation on the HuBERT model and was able to reduce the number of parameters to a fraction of the original  [4] . LightHuBERT, in turn, proposed a 2-stage distillation approach to also reduce the size of the original HuBERT  [14] .\n\nRegarding environmental robustness, commonly with in-the-wild speech applications, there is a shift in the distribution of the test data relative to the distribution of the data used to train the models. This domain shift can be detrimental to downstream task performance. The work in  [7] , for example, proposes the use of domain adversarial training and data augmentation to improve HuBERT robustness to unseen conditions; such system is termed Robust HuBERT.\n\nIn this study, we propose to combine several innovations to make HuBERT not only smaller, but also more robust to unseen environmental conditions. In particular, we propose to combine data augmentation, curriculum learning, and multi-task training to ensure that the extracted representation is robust. As a proof of concept, we implement these modifications on top of the DistilHuBERT model to benefit from the compression gains already achieved.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Proposed Methods",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Building On Distilhubert",
      "text": "DistilHuBERT uses knowledge distillation on top of the HuBERT model to achieve model compression. In our case, the teacher model is the HuBERT Base model comprised of 12 Transformer layers and 95 million parameters, and the student model has only two Transformer layers and roughly four times fewer parameters. All the parameters are initialized with the values from the teacher model. Distillation occurs with the student model learning a representation which is the input to three prediction heads responsible for predicting the representations from the {4 th , 8 th , and 12 th } layers of the teacher model. The prediction heads are discarded after training, making the model efficient and compact. In addition, the distillation mechanism is optimized for a loss function that combines reconstruction elements with a cosine similarity metric, as follows:\n\nThe variable l corresponds to the layers from the teacher model to distill, T is the number of time steps, h h h\n\nt is the D-dimensional feature vector extracted from the l th layer of the teacher model, ĥ h h (l) t is the output from the prediction head, and σ(•) is the sigmoid function. As DistilHuBERT has shown excellent results compared to the original HuBERT model, it serves as a starting point for our work where the three proposed innovations are implemented on top.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Modification #1: Data Augmentation",
      "text": "Most SSL models rely on learning speech representations from clean readings of audiobook data, such as LibriSpeech  [10]  or LibriLight  [8] . Even though these models can learn essential characteristics from this type of speech, real-world deployment often involves noisy speech utterances which degrade system performance. Data augmentation has commonly been applied to improve the robustness to unseen conditions  [7] . Here, we propose to perform an online contamination of the data during the distillation process. In particular, the student model receives the noisy data as input, but the network's target is to reconstruct the clean features of the teacher model. At training time, given a batch of clean speech utterances, we uniformly sample one action to be applied to each utterance in the batch:\n\n(a1) No changes are made to the training utterance, (a2) Contaminate the utterance with additive noise with signal-to-noise ratio randomly chosen from [0, 20] dB, (a3) Convolve the speech waveform with a room impulse response representative of either a small, medium, or a large sized room, and (a4) Both (a2) and (a3) actions are jointly applied.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Modification #2: Curriculum/Progressive Learning",
      "text": "Inspired by the ideas of curriculum learning  [3] , we propose a progressive training mechanism where the network gradually receives more deteriorated speech samples during training iterations and not randomly from the beginning of the optimization process. Based on the literature, we expect the models trained on this regime to have better generalization capabilities, which can lead us to reinforce the model behavior in learning speech features disentangled from noise. In our work, we gradually increase the difficulty of the online contamination process by defining a custom sampling procedure instead of allowing every scenario from the beginning. This process is done until we reach half of the training steps, and after, the same regime of the previous section is applied. The pseudocode for this step is illustrated in the Supplementary Material Section.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Modification #3: Multi-Task Enhancement Learning",
      "text": "Speech enhancement is a common processing method used to remove detrimental factors from the speech signal. Motivated by Wang et al.  [13] , we propose a multi-task learning approach, where beyond learning to reconstruct the teacher's representations, we propose an additional enhancement head responsible to rebuild the clean speech waveform from the learned representation. The goal with this step is to enforce the upstream model to carry enough information about the speech itself and not the noise components. Here, a BiLSTM layer followed by seven transposed convolutions and GELU activation functions is used, as well as the following loss function: L = L KD + λL enh , where L enh is the enhancement loss and λ ∈ R is a hyperparameter that controls the importance of the enhancement step on the total loss. Notice that multiple choices can be made about what enhancement loss to use. Here, two enhancement losses are explored: (i) an L1-based reconstruction loss directly on the waveform samples and (ii) an L1-based reconstruction loss applied to the spectral magnitude of the reconstructed signal. These are denote by L L1-wav and L L1-freq , respectively.\n\n3 Experimental Setup",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Datasets",
      "text": "The training data used is the LibriSpeech dataset  [10] , a corpus with 960 hours of audiobook readings with a 16 kHz sampling rate and 16-bit resolution. The training data consists of a subset of 460 hours of clean audio in a studio-like condition, while the other 500 hours do not have the same quality standard. To augment the training set, two additional noise datasets are used, namely MUSAN  [12] , and UrbanSound8K  [11] , as well as the OpenSLR28 dataset  [9]  of room impulse responses (RIR).\n\nThe MUSAN dataset contains 6 hours of noise recordings (16 kHz sample rate) in a wide variety of categories, including office-like noises, babble noise, and natural sounds, such as wind and animals, to name a few. The UrbanSound8K dataset contains approximately 9 hours of audio distributed across ten labels: air conditioner, car horn, children playing, dog bark, drilling, engine idling, gunshot, jackhammer, siren, and street music. We removed the children playing and street music conditions to focus on non-speech like noise sources in this first analysis. The dataset has a sample rate of 44.1 kHz, which was resampled to 16 kHz. Lastly, the openSLR28 dataset contains 325 RIRs from multiple sources, such as small meeting rooms and large churches with up to 570 m 2 .\n\nAt test time, it is important to test the model with unseen conditions. To this end, we rely on two separate datasets to corrupt the test set. The first is the Deep Noise Suppression 4 (DNS4) challenge dataset  [5] , comprised of approximately 180 hours of noise extracted from 150 categories with no presence of speech. For reverberation, we used the OpenSLR26 dataset  [9] , which contains 60,000 simulated room impulse responses corresponding to various small-, medium-, and largesized rooms.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Pre-Training",
      "text": "To gauge the benefits of the different proposed modifications, four different combinations are tested. First, denoted as experiment A, explores the impact of only applying data augmentation on top of DistilHuBERT. Next, experiment B adds the progressive learning approach. Lastly, experiments C1 and C2 explore the further addition of the enhancement head where the loss functions L L1-wav and L L1-freq are used, respectively. We set λ = 10 and λ = 1 for experiments C1 and C2, respectively. In all cases, the training recipe described in  [4]  is followed. Upstream models are trained using a single NVidia A100 GPU. Experiments A and B take approximately 30 hours to train, whereas experiments C1 and C2 took each roughly 43 hours to train. We use the AdamW optimizer, with Table  1 : Experimental results for keyword spotting, intent classification, and emotion recognition under clean (c), noisy (n), reverberation (r), and noise-plus-reveberation (n+r) test conditions.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Keyword Spotting",
      "text": "Intent Classification Emotion Recognition a batch of 24 utterances, for 200k iterations, whereas after 14k updates, the learning rate linearly decays from 2 × 10 -4 to zero. As benchmarks, we use the original HuBERT Base model, its Robust version, and two compressed models: LightHuBERT and DistilHubert.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Downstream Tasks",
      "text": "A subset of three downstream tasks from the SUPERB benchmark  [15]  are chosen to evaluate the robustness of the proposed modifications, namely: keyword spotting, intent classification, and emotion recognition. For all downstream tasks, the evaluation metric is the standard accuracy score; thus, higher values are better. To test the robustness of the methods and models, four evaluation scenarios are considered: clean (c), noise-only (n), reverberation-only (r), and noise-plus-reverberation (n + r). The test set meets the same criteria as SUPERB downstream tasks in the clean setting. For the noisy test conditions, in turn, additive noise with signal-to-noise ratios ranging from [0, 20] dB are added. For the reverberation condition, a random room impulse response is uniformly sampled and applied to the test set utterance. Lastly, for the noise-plus-reverberation condition, both noise and reverberation are jointly applied to the test set, thus representing the most challenging scenario of being in a noisy room. A custom seed is applied to ensure all models are evaluated with the same degradations.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results And Discussion",
      "text": "Table  1  presents the experimental results for the benchmarks and proposed methods on the three downstream tasks. As can be seen, the proposed innovations outperformed the original DistilHu-BERT model for all of the three SUPERB tasks, even in clean conditions. In fact, just adding data augmentation (Expt. A) improved clean speech accuracy for the keyword spotting and intent classification tasks, thus showing the benefits of data augmentation. Progressive learning, in turn, (Expt. B) showed to be particularly important for the noisy/reverberant conditions for keyword spotting and intent classification. The additional enhancement step also showed to provide additional gains, with the waveform-based loss outperforming the magnitude spectral one.\n\nIn fact, for keyword spotting in noise, reverberation, and noise-plus-reverberation conditions, the proposed methods with only 24M parameters outperformed the larger teacher and Robust HuBERT models, each comprised of 95M parameters. For the two other tasks under unseen conditions, the proposed methods outperformed the original teacher model and obtained results in line with those of Robust HuBERT, whilst requiring roughly one quarter of the number of parameters. The proposed multi-task speech enhancement step showed to be particularly useful for when additive noise was present, especially for intent classification and emotion recognition. In the future, other perceptual losses will be explored to see if further improvements can be obtained under reverberant conditions.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "In this work, we propose three modifications to be implemented on top of DistilHubert. The aim is to develop a cross-task speech representation that is compact and, at the same time, robust to unseen noisy conditions. The modifications include addition of data augmentation, progressive learning, and multi-task learning where speech enhancement is jointly performed. Experiments on three SUPERB tasks, namely keyword spotting, intent classification, and emotion recognition, show the proposed model outperforming the original DistilHuBERT across all tested noisy conditions, sometimes even outperforming the original teacher HuBERT model and its robust variant. The obtained results are promising and suggest that the proposed method can be useful for future edge speech applications.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract": "Self-supervised speech representation learning aims to extract meaningful factors"
        },
        {
          "Abstract": "from the speech signal\nthat can later be used across different downstream tasks,"
        },
        {
          "Abstract": "such as speech and/or emotion recognition. Existing models, such as HuBERT,"
        },
        {
          "Abstract": "however, can be fairly large thus may not be suitable for edge speech applications."
        },
        {
          "Abstract": "Moreover, realistic applications typically involve speech corrupted by noise and"
        },
        {
          "Abstract": "room reverberation, hence models need to provide representations that are robust"
        },
        {
          "Abstract": "to such environmental factors.\nIn this study, we build on the so-called DistilHu-"
        },
        {
          "Abstract": "BERT model, which distils HuBERT to a fraction of its original size, with three"
        },
        {
          "Abstract": "modiﬁcations, namely: (i) augment the training data with noise and reverberation,"
        },
        {
          "Abstract": "while the student model needs to distill the clean representations from the teacher"
        },
        {
          "Abstract": "model;\n(ii) introduce a curriculum learning approach where increasing levels of"
        },
        {
          "Abstract": "noise are introduced as the model trains, thus helping with convergence and with"
        },
        {
          "Abstract": "the creation of more robust representations; and (iii) introduce a multi-task learn-"
        },
        {
          "Abstract": "ing approach where the model also reconstructs the clean waveform jointly with"
        },
        {
          "Abstract": "the distillation task,\nthus also acting as an enhancement step to ensure additional"
        },
        {
          "Abstract": "environment\nrobustness to the representation.\nExperiments on three SUPERB"
        },
        {
          "Abstract": "tasks show the advantages of the proposed method not only relative to the original"
        },
        {
          "Abstract": "DistilHuBERT, but also to the original HuBERT, thus showing the advantages of"
        },
        {
          "Abstract": "the proposed method for “in the wild” edge speech applications."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the proposed method for “in the wild” edge speech applications.": "1\nIntroduction"
        },
        {
          "the proposed method for “in the wild” edge speech applications.": "Self-supervised learning (SSL) enables the learning of meaningful and disentangled features from"
        },
        {
          "the proposed method for “in the wild” edge speech applications.": "unlabeled speech data that can be used across different\ntasks.\nThese representations are becom-"
        },
        {
          "the proposed method for “in the wild” edge speech applications.": "ing essential and are now part of several state-of-the-art speech applications [2, 6]. Widely-used"
        },
        {
          "the proposed method for “in the wild” edge speech applications.": "representations include wav2vec 2.0 [2] and HuBERT [6]. The latter is an SSL model that extracts"
        },
        {
          "the proposed method for “in the wild” edge speech applications.": "features directly from the raw speech signal. Via an ofﬂine k-means clustering step, the model learns"
        },
        {
          "the proposed method for “in the wild” edge speech applications.": "combined acoustic and language representations from masked inputs using a BERT-like reconstruc-"
        },
        {
          "the proposed method for “in the wild” edge speech applications.": "tion loss."
        },
        {
          "the proposed method for “in the wild” edge speech applications.": "Existing models, however, have two limitations when edge applications are considered:\n(1) their"
        },
        {
          "the proposed method for “in the wild” edge speech applications.": "large model sizes, and (2) their robustness to environmental conditions not seen during training. For"
        },
        {
          "the proposed method for “in the wild” edge speech applications.": "example, pre-trained models can range from 95 million parameters to 2 billion [1], which makes"
        },
        {
          "the proposed method for “in the wild” edge speech applications.": "training and deployment on edge devices difﬁcult. To overcome this issue, different model compres-"
        },
        {
          "the proposed method for “in the wild” edge speech applications.": "sion schemes have been explored. DistilHuBERT, for example, applied knowledge distillation on"
        },
        {
          "the proposed method for “in the wild” edge speech applications.": "the HuBERT model and was able to reduce the number of parameters to a fraction of the original"
        },
        {
          "the proposed method for “in the wild” edge speech applications.": "[4]. LightHuBERT,\nin turn, proposed a 2-stage distillation approach to also reduce the size of the"
        },
        {
          "the proposed method for “in the wild” edge speech applications.": "original HuBERT [14]."
        },
        {
          "the proposed method for “in the wild” edge speech applications.": "36th Conference on Neural Information Processing Systems (NeurIPS 2022)."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Regarding environmental robustness, commonly with in-the-wild speech applications, there is a shift": "in the distribution of the test data relative to the distribution of the data used to train the models. This"
        },
        {
          "Regarding environmental robustness, commonly with in-the-wild speech applications, there is a shift": "domain shift can be detrimental to downstream task performance. The work in [7], for example, pro-"
        },
        {
          "Regarding environmental robustness, commonly with in-the-wild speech applications, there is a shift": "poses the use of domain adversarial training and data augmentation to improve HuBERT robustness"
        },
        {
          "Regarding environmental robustness, commonly with in-the-wild speech applications, there is a shift": "to unseen conditions; such system is termed Robust HuBERT."
        },
        {
          "Regarding environmental robustness, commonly with in-the-wild speech applications, there is a shift": "In this study, we propose to combine several"
        },
        {
          "Regarding environmental robustness, commonly with in-the-wild speech applications, there is a shift": "also more robust to unseen environmental conditions. In particular, we propose to combine data aug-"
        },
        {
          "Regarding environmental robustness, commonly with in-the-wild speech applications, there is a shift": "mentation, curriculum learning, and multi-task training to ensure that the extracted representation is"
        },
        {
          "Regarding environmental robustness, commonly with in-the-wild speech applications, there is a shift": "robust. As a proof of concept, we implement these modiﬁcations on top of the DistilHuBERT model"
        },
        {
          "Regarding environmental robustness, commonly with in-the-wild speech applications, there is a shift": "to beneﬁt from the compression gains already achieved."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "models trained on this regime to have better generalization capabilities, which can lead us to re-"
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "inforce the model behavior in learning speech features disentangled from noise.\nIn our work, we"
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "gradually increase the difﬁculty of the online contamination process by deﬁning a custom sampling"
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "procedure instead of allowing every scenario from the beginning. This process is done until we"
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "reach half of the training steps, and after,\nthe same regime of the previous section is applied. The"
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "pseudocode for this step is illustrated in the Supplementary Material Section."
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "2.4\nModiﬁcation #3: Multi-task enhancement learning"
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "Speech enhancement is a common processing method used to remove detrimental factors from the"
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "speech signal. Motivated by Wang et al. [13], we propose a multi-task learning approach, where"
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "beyond learning to reconstruct the teacher’s representations, we propose an additional enhancement"
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "head responsible to rebuild the clean speech waveform from the learned representation. The goal"
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "with this step is to enforce the upstream model to carry enough information about the speech itself"
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "and not\nthe noise components. Here, a BiLSTM layer followed by seven transposed convolutions"
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "and GELU activation functions is used, as well as the following loss function: L = LKD + λLenh,"
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "is the enhancement\nloss and λ ∈ R is a hyperparameter that controls the importance\nwhere Lenh"
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "of\nthe enhancement step on the total\nloss. Notice that multiple choices can be made about what"
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "enhancement loss to use. Here, two enhancement losses are explored: (i) an L1-based reconstruction"
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "loss directly on the waveform samples and (ii) an L1-based reconstruction loss applied to the spectral"
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "magnitude of the reconstructed signal. These are denote by LL1-wav and LL1-freq, respectively."
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "3\nExperimental Setup"
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "3.1\nDatasets"
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "The training data used is the LibriSpeech dataset [10], a corpus with 960 hours of audiobook readings"
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "with a 16 kHz sampling rate and 16-bit resolution. The training data consists of a subset of 460 hours"
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "of clean audio in a studio-like condition, while the other 500 hours do not have the same quality"
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "standard. To augment the training set, two additional noise datasets are used, namely MUSAN [12],"
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "and UrbanSound8K [11], as well as the OpenSLR28 dataset [9] of room impulse responses (RIR)."
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "The MUSAN dataset contains 6 hours of noise recordings (16 kHz sample rate)\nin a wide vari-"
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "ety of categories,\nincluding ofﬁce-like noises, babble noise, and natural sounds, such as wind and"
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "animals,\nto name a few. The UrbanSound8K dataset contains approximately 9 hours of audio dis-"
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "tributed across ten labels:\nair conditioner, car horn, children playing, dog bark, drilling, engine"
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "idling, gunshot,\njackhammer, siren, and street music. We removed the children playing and street"
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "music conditions to focus on non-speech like noise sources in this ﬁrst analysis. The dataset has a"
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "sample rate of 44.1 kHz, which was resampled to 16 kHz. Lastly,\nthe openSLR28 dataset contains"
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "325 RIRs from multiple sources, such as small meeting rooms and large churches with up to 570"
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "m2."
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "At\ntest\ntime,\nit\nis\nimportant\nto test\nthe model with unseen conditions.\nTo this end, we rely on"
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "two separate datasets to corrupt\nthe test set.\nThe ﬁrst\nis the Deep Noise Suppression 4 (DNS4)"
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "challenge dataset [5], comprised of approximately 180 hours of noise extracted from 150 categories"
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "with no presence of speech. For reverberation, we used the OpenSLR26 dataset [9], which contains"
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "60,000 simulated room impulse responses corresponding to various small-, medium-, and large-"
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "sized rooms."
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "3.2\nPre-training"
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "To gauge the beneﬁts of the different proposed modiﬁcations, four different combinations are tested."
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "First, denoted as experiment A, explores the impact of only applying data augmentation on top of"
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "DistilHuBERT. Next, experiment B adds the progressive learning approach. Lastly, experiments C1"
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "and C2 explore the further addition of the enhancement head where the loss functions LL1-wav and"
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "LL1-freq are used, respectively. We set λ = 10 and λ = 1 for experiments C1 and C2, respectively."
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "In all cases,\nthe training recipe described in [4]\nis followed. Upstream models are trained using"
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "a single NVidia A100 GPU. Experiments A and B take approximately 30 hours to train, whereas"
        },
        {
          "randomly from the beginning of the optimization process. Based on the literature, we expect\nthe": "experiments C1 and C2 took each roughly 43 hours to train. We use the AdamW optimizer, with"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: Experimentalresults for keywordspotting, intent classification, and emotion recognition",
      "data": [
        {
          "Table 1: Experimental results for keyword spotting,": "",
          "intent classiﬁcation, and emotion recognition": ""
        },
        {
          "Table 1: Experimental results for keyword spotting,": "",
          "intent classiﬁcation, and emotion recognition": "Intent Classiﬁcation"
        },
        {
          "Table 1: Experimental results for keyword spotting,": "Upstream",
          "intent classiﬁcation, and emotion recognition": "(n)"
        },
        {
          "Table 1: Experimental results for keyword spotting,": "HuBERT Base (Teacher)",
          "intent classiﬁcation, and emotion recognition": "80.78"
        },
        {
          "Table 1: Experimental results for keyword spotting,": "Robust HuBERT",
          "intent classiﬁcation, and emotion recognition": "92.75"
        },
        {
          "Table 1: Experimental results for keyword spotting,": "LightHuBERT Small",
          "intent classiﬁcation, and emotion recognition": "80.81"
        },
        {
          "Table 1: Experimental results for keyword spotting,": "DistilHuBERT",
          "intent classiﬁcation, and emotion recognition": "54.23"
        },
        {
          "Table 1: Experimental results for keyword spotting,": "Proposed (Exp. A)",
          "intent classiﬁcation, and emotion recognition": "81.68"
        },
        {
          "Table 1: Experimental results for keyword spotting,": "Proposed (Exp. B)",
          "intent classiﬁcation, and emotion recognition": "83.07"
        },
        {
          "Table 1: Experimental results for keyword spotting,": "Proposed (Exp. C1)",
          "intent classiﬁcation, and emotion recognition": "85.50"
        },
        {
          "Table 1: Experimental results for keyword spotting,": "Proposed (Exp. C2)",
          "intent classiﬁcation, and emotion recognition": "83.39"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "obtained results are promising and suggest that the proposed method can be useful for future edge": "speech applications."
        },
        {
          "obtained results are promising and suggest that the proposed method can be useful for future edge": "References"
        },
        {
          "obtained results are promising and suggest that the proposed method can be useful for future edge": "[1] A. Babu, C. Wang, A. Tjandra, K. Lakhotia, Q. Xu, N. Goyal, K. Singh, P. von Platen, Y. Saraf,"
        },
        {
          "obtained results are promising and suggest that the proposed method can be useful for future edge": "J. Pino, et al. Xls-r: Self-supervised cross-lingual speech representation learning at scale. arXiv"
        },
        {
          "obtained results are promising and suggest that the proposed method can be useful for future edge": "preprint arXiv:2111.09296, 2021."
        },
        {
          "obtained results are promising and suggest that the proposed method can be useful for future edge": "[2] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli.\nwav2vec 2.0: A framework for\nself-"
        },
        {
          "obtained results are promising and suggest that the proposed method can be useful for future edge": "Advances in Neural\nInformation Processing\nsupervised learning of speech representations."
        },
        {
          "obtained results are promising and suggest that the proposed method can be useful for future edge": "Systems, 33:12449–12460, 2020."
        },
        {
          "obtained results are promising and suggest that the proposed method can be useful for future edge": "[3] Y. Bengio, J. Louradour, R. Collobert, and J. Weston. Curriculum learning.\nIn Proceedings of"
        },
        {
          "obtained results are promising and suggest that the proposed method can be useful for future edge": "the 26th annual international conference on machine learning, pages 41–48, 2009."
        },
        {
          "obtained results are promising and suggest that the proposed method can be useful for future edge": "[4] H.-J. Chang, S.-w. Yang, and H.-y. Lee. Distilhubert: Speech representation learning by layer-"
        },
        {
          "obtained results are promising and suggest that the proposed method can be useful for future edge": "wise distillation of hidden-unit bert.\nIn ICASSP 2022-2022 IEEE International Conference on"
        },
        {
          "obtained results are promising and suggest that the proposed method can be useful for future edge": "Acoustics, Speech and Signal Processing (ICASSP), pages 7087–7091. IEEE, 2022."
        },
        {
          "obtained results are promising and suggest that the proposed method can be useful for future edge": "[5] H. Dubey, V. Gopal, et al.\nIcassp 2022 deep noise suppression challenge.\nIn Proc. ICASSP,"
        },
        {
          "obtained results are promising and suggest that the proposed method can be useful for future edge": "2022."
        },
        {
          "obtained results are promising and suggest that the proposed method can be useful for future edge": "[6] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed. Hu-"
        },
        {
          "obtained results are promising and suggest that the proposed method can be useful for future edge": "BERT: Self-supervised speech representation learning by masked prediction of hidden units."
        },
        {
          "obtained results are promising and suggest that the proposed method can be useful for future edge": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:3451–3460, 2021."
        },
        {
          "obtained results are promising and suggest that the proposed method can be useful for future edge": "[7] K. P. Huang, Y.-K. Fu, Y. Zhang, and H.-y. Lee.\nImproving distortion robustness of\nself-"
        },
        {
          "obtained results are promising and suggest that the proposed method can be useful for future edge": "supervised speech processing tasks with domain adaptation. arXiv:2203.16104, 2022."
        },
        {
          "obtained results are promising and suggest that the proposed method can be useful for future edge": "[8]\nJ. Kahn, M. Rivière, et al. Libri-light: A benchmark for ASR with limited or no supervision."
        },
        {
          "obtained results are promising and suggest that the proposed method can be useful for future edge": "In Proc. IEEE ICASSP, pages 7669–7673, 2020."
        },
        {
          "obtained results are promising and suggest that the proposed method can be useful for future edge": "[9] T. Ko, V. Peddinti, D. Povey, M. L. Seltzer, and S. Khudanpur. A study on data augmentation"
        },
        {
          "obtained results are promising and suggest that the proposed method can be useful for future edge": "of reverberant speech for robust speech recognition. In Proc. IEEE ICASSP, pages 5220–5224,"
        },
        {
          "obtained results are promising and suggest that the proposed method can be useful for future edge": "2017."
        },
        {
          "obtained results are promising and suggest that the proposed method can be useful for future edge": "[10] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur.\nLibrispeech:\nan asr corpus based on"
        },
        {
          "obtained results are promising and suggest that the proposed method can be useful for future edge": "public domain audio books.\nIn 2015 IEEE international conference on acoustics, speech and"
        },
        {
          "obtained results are promising and suggest that the proposed method can be useful for future edge": "signal processing (ICASSP), pages 5206–5210. IEEE, 2015."
        },
        {
          "obtained results are promising and suggest that the proposed method can be useful for future edge": "[11]\nJ. Salamon, C. Jacoby, and J. P. Bello.\nA dataset and taxonomy for urban sound research."
        },
        {
          "obtained results are promising and suggest that the proposed method can be useful for future edge": "In 22nd ACM International Conference on Multimedia (ACM-MM’14), pages 1041–1044, Or-"
        },
        {
          "obtained results are promising and suggest that the proposed method can be useful for future edge": "lando, FL, USA, Nov. 2014."
        },
        {
          "obtained results are promising and suggest that the proposed method can be useful for future edge": "[12] D. Snyder, G. Chen, and D. Povey. MUSAN: A Music, Speech, and Noise Corpus, 2015."
        },
        {
          "obtained results are promising and suggest that the proposed method can be useful for future edge": "[13] H. Wang, Y. Qian, X. Wang, Y. Wang, C. Wang, S. Liu, T. Yoshioka, J. Li, and D. Wang."
        },
        {
          "obtained results are promising and suggest that the proposed method can be useful for future edge": "Improving noise robustness of contrastive speech representation learning with speech recon-"
        },
        {
          "obtained results are promising and suggest that the proposed method can be useful for future edge": "struction.\nIn ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and"
        },
        {
          "obtained results are promising and suggest that the proposed method can be useful for future edge": "Signal Processing (ICASSP), pages 6062–6066. IEEE, 2022."
        },
        {
          "obtained results are promising and suggest that the proposed method can be useful for future edge": "[14] R. Wang, Q. Bai, J. Ao, L. Zhou, Z. Xiong, Z. Wei, Y. Zhang, T. Ko, and H. Li. Lighthubert:"
        },
        {
          "obtained results are promising and suggest that the proposed method can be useful for future edge": "Lightweight and conﬁgurable speech representation learning with once-for-all hidden-unit bert."
        },
        {
          "obtained results are promising and suggest that the proposed method can be useful for future edge": "arXiv preprint arXiv:2203.15610, 2022."
        },
        {
          "obtained results are promising and suggest that the proposed method can be useful for future edge": "[15] S.-w. Yang, P.-H. Chi, et al.\nSuperb: Speech processing universal performance benchmark."
        },
        {
          "obtained results are promising and suggest that the proposed method can be useful for future edge": "arXiv preprint arXiv:2105.01051, 2021."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6\nSupplementary Material": "Pseudocode for progressive learning experiment"
        },
        {
          "6\nSupplementary Material": "Algorithm 1 shows the pseudocode to describe the progressive learning strategy adopted in this"
        },
        {
          "6\nSupplementary Material": "work. It is a generalization of the data augmentation strategy described in experiment A. Notice that,"
        },
        {
          "6\nSupplementary Material": "if τ = 0 and t = 1, we recover the augmentation stage without curriculum learning."
        },
        {
          "6\nSupplementary Material": "Algorithm 1: Sample strategy for SNR-Progressive Learning"
        },
        {
          "6\nSupplementary Material": "given a batch of utterances U = {ui}L"
        },
        {
          "6\nSupplementary Material": "i=0, a set of noise ﬁles Un, a set of room impulse"
        },
        {
          "6\nSupplementary Material": "responses Ur, the current iteration it, and the total number of iterations N ;"
        },
        {
          "6\nSupplementary Material": "for each utterance ui do"
        },
        {
          "6\nSupplementary Material": "a ← sample an action from the equiprobable sample space {a1, a2, a3, a4} ;"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "responses Ur, the current iteration it, and the total number of iterations N ;": "for each utterance ui do"
        },
        {
          "responses Ur, the current iteration it, and the total number of iterations N ;": "a ← sample an action from the equiprobable sample space {a1, a2, a3, a4} ;"
        },
        {
          "responses Ur, the current iteration it, and the total number of iterations N ;": "if a == a2 then"
        },
        {
          "responses Ur, the current iteration it, and the total number of iterations N ;": "/* Lower-bound for the SNR value */\nif it ≤ N/2 else 0 ;\nτ ← 20\n1 − 2it"
        },
        {
          "responses Ur, the current iteration it, and the total number of iterations N ;": "N"
        },
        {
          "responses Ur, the current iteration it, and the total number of iterations N ;": "snr ← sample an integer number from a uniform distribution U(τ, 20) ;"
        },
        {
          "responses Ur, the current iteration it, and the total number of iterations N ;": "pn ← sample a random number from a uniform distribution U(0, 1) ;"
        },
        {
          "responses Ur, the current iteration it, and the total number of iterations N ;": "un ← sample from Un if pn ≤ 0.7 else return a narrow-band white noise N (0, 1) ;"
        },
        {
          "responses Ur, the current iteration it, and the total number of iterations N ;": "ui ← applyNoise(ui, un, snr)"
        },
        {
          "responses Ur, the current iteration it, and the total number of iterations N ;": "else if a == a3 then"
        },
        {
          "responses Ur, the current iteration it, and the total number of iterations N ;": "/* Threshold for applying reverberation */\nt ← 2it"
        },
        {
          "responses Ur, the current iteration it, and the total number of iterations N ;": "N if it ≤ N/2 else 1 ;"
        },
        {
          "responses Ur, the current iteration it, and the total number of iterations N ;": "pr ← sample a random number from a uniform distribution U(0, 1) ;"
        },
        {
          "responses Ur, the current iteration it, and the total number of iterations N ;": "if pr ≤ t then"
        },
        {
          "responses Ur, the current iteration it, and the total number of iterations N ;": "ur ← uniformly sample a RIR from Ur ;"
        },
        {
          "responses Ur, the current iteration it, and the total number of iterations N ;": "ui ← applyReverberation(ui, ur) ;"
        },
        {
          "responses Ur, the current iteration it, and the total number of iterations N ;": "end"
        },
        {
          "responses Ur, the current iteration it, and the total number of iterations N ;": "else if a == a4 then"
        },
        {
          "responses Ur, the current iteration it, and the total number of iterations N ;": "ui ← apply the same mechanism from the action a2 ;"
        },
        {
          "responses Ur, the current iteration it, and the total number of iterations N ;": "ui ← apply the same mechanism from the action a3 ;"
        },
        {
          "responses Ur, the current iteration it, and the total number of iterations N ;": "else"
        },
        {
          "responses Ur, the current iteration it, and the total number of iterations N ;": "Do nothing ;"
        },
        {
          "responses Ur, the current iteration it, and the total number of iterations N ;": "end"
        },
        {
          "responses Ur, the current iteration it, and the total number of iterations N ;": "end"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Xls-r: Self-supervised cross-lingual speech representation learning at scale",
      "authors": [
        "A Babu",
        "C Wang",
        "A Tjandra",
        "K Lakhotia",
        "Q Xu",
        "N Goyal",
        "K Singh",
        "P Von Platen",
        "Y Saraf",
        "J Pino"
      ],
      "year": "2021",
      "venue": "Xls-r: Self-supervised cross-lingual speech representation learning at scale",
      "arxiv": "arXiv:2111.09296"
    },
    {
      "citation_id": "2",
      "title": "wav2vec 2.0: A framework for selfsupervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "3",
      "title": "Curriculum learning",
      "authors": [
        "Y Bengio",
        "J Louradour",
        "R Collobert",
        "J Weston"
      ],
      "year": "2009",
      "venue": "Proceedings of the 26th annual international conference on machine learning"
    },
    {
      "citation_id": "4",
      "title": "Distilhubert: Speech representation learning by layerwise distillation of hidden-unit bert",
      "authors": [
        "H.-J Chang",
        "S.-W Yang",
        "H.-Y Lee"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "5",
      "title": "Icassp 2022 deep noise suppression challenge",
      "authors": [
        "H Dubey",
        "V Gopal"
      ],
      "year": "2022",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "6",
      "title": "Hu-BERT: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "7",
      "title": "Improving distortion robustness of selfsupervised speech processing tasks with domain adaptation",
      "authors": [
        "K Huang",
        "Y.-K Fu",
        "Y Zhang",
        "H.-Y Lee"
      ],
      "year": "2022",
      "venue": "Improving distortion robustness of selfsupervised speech processing tasks with domain adaptation",
      "arxiv": "arXiv:2203.16104"
    },
    {
      "citation_id": "8",
      "title": "Libri-light: A benchmark for ASR with limited or no supervision",
      "authors": [
        "J Kahn",
        "M Rivière"
      ],
      "year": "2020",
      "venue": "Proc. IEEE ICASSP"
    },
    {
      "citation_id": "9",
      "title": "A study on data augmentation of reverberant speech for robust speech recognition",
      "authors": [
        "T Ko",
        "V Peddinti",
        "D Povey",
        "M Seltzer",
        "S Khudanpur"
      ],
      "year": "2017",
      "venue": "Proc. IEEE ICASSP"
    },
    {
      "citation_id": "10",
      "title": "Librispeech: an asr corpus based on public domain audio books",
      "authors": [
        "V Panayotov",
        "G Chen",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "11",
      "title": "A dataset and taxonomy for urban sound research",
      "authors": [
        "J Salamon",
        "C Jacoby",
        "J Bello"
      ],
      "year": "2014",
      "venue": "22nd ACM International Conference on Multimedia (ACM-MM'14)"
    },
    {
      "citation_id": "12",
      "title": "MUSAN: A Music, Speech, and Noise Corpus",
      "authors": [
        "D Snyder",
        "G Chen",
        "D Povey"
      ],
      "year": "2015",
      "venue": "MUSAN: A Music, Speech, and Noise Corpus"
    },
    {
      "citation_id": "13",
      "title": "Improving noise robustness of contrastive speech representation learning with speech reconstruction",
      "authors": [
        "H Wang",
        "Y Qian",
        "X Wang",
        "Y Wang",
        "C Wang",
        "S Liu",
        "T Yoshioka",
        "J Li",
        "D Wang"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "14",
      "title": "Lighthubert: Lightweight and configurable speech representation learning with once-for-all hidden-unit bert",
      "authors": [
        "R Wang",
        "Q Bai",
        "J Ao",
        "L Zhou",
        "Z Xiong",
        "Z Wei",
        "Y Zhang",
        "T Ko",
        "H Li"
      ],
      "year": "2022",
      "venue": "Lighthubert: Lightweight and configurable speech representation learning with once-for-all hidden-unit bert",
      "arxiv": "arXiv:2203.15610"
    },
    {
      "citation_id": "15",
      "title": "Superb: Speech processing universal performance benchmark",
      "authors": [
        "S.-W Yang",
        "P.-H Chi"
      ],
      "year": "2021",
      "venue": "Superb: Speech processing universal performance benchmark",
      "arxiv": "arXiv:2105.01051"
    }
  ]
}