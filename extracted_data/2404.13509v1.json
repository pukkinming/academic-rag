{
  "paper_id": "2404.13509v1",
  "title": "Mfhca: Enhancing Speech Emotion Recognition Via Multi-Spatial Fusion And Hierarchical Cooperative Attention",
  "published": "2024-04-21T02:44:17Z",
  "authors": [
    "Xinxin Jiao",
    "Liejun Wang",
    "Yinfeng Yu"
  ],
  "keywords": [
    "Speech emotion recognition",
    "multi-spatial fusion",
    "hierarchical cooperative attention",
    "Hubert features"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition is crucial in humancomputer interaction, but extracting and using emotional cues from audio poses challenges. This paper introduces MFHCA, a novel method for Speech Emotion Recognition using Multi-Spatial Fusion and Hierarchical Cooperative Attention on spectrograms and raw audio. We employ the Multi-Spatial Fusion module (MF) to efficiently identify emotion-related spectrogram regions and integrate Hubert features for higher-level acoustic information. Our approach also includes a Hierarchical Cooperative Attention module (HCA) to merge features from various auditory levels. We evaluate our method on the IEMOCAP dataset and achieve 2.6% and 1.87% improvements on the weighted accuracy and unweighted accuracy, respectively. Extensive experiments demonstrate the effectiveness of the proposed method.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Speech emotion recognition (SER) technology plays a crucial role in intelligent human-machine interaction systems  [1] , as it can identify the speaker's emotional state, thereby enhancing the naturalness of human-machine interaction. Recently, some multimodal SER methods  [2] ,  [3] ,  [22]  have achieved significantly higher accuracy than speech emotion recognition. However, the model can only extract information from the speech signal in specific scenarios, such as voice assistants and phone customer service. Therefore, we focus on extracting emotional information from speech signals.\n\nMany SER models can be seen as a combination of deep feature extractors and classifiers in their structure. In order to attain powerful representational capabilities, numerous researchers have enhanced network models. Xu et al.  [4]  proposed an Attention-based CNN (ACNN) to acquire more effective features, aiming to enhance the performance of SER. Liu et al.  [5]  integrated multi-scale CNN with time-frequency CNN, concurrently modeling local and global information. Chen et al.  [6]  employed a dynamic window transformer to locate significant regions at different time scales, capturing † Corresponding author. essential information. Chen et al.  [7]  devised a Deformable Speech Transformer (DST) that captures multi-granular emotional information through deformable attention windows. Although these methods have achieved higher performance, the intricate structures pose challenges for computational resources.\n\nThe outstanding performance of speech self-supervised learning in downstream tasks such as automatic speech recognition (ASR) has opened up new avenues for developing SER. Xia et al.  [10]  fine-tuned the Wav2Vec model  [11] , leveraging learned features for SER tasks. He et al.  [8]  designed a SER method based on the cross-attention transformer that integrates three acoustic features, one of which is extracted from the Wav2Vec2 model  [9] . Gat I et al.  [12]  proposed a speaker feature normalization framework based on self-supervised feature representation, achieving outstanding performance in SER.\n\nIn this paper, we propose a simple SER method (MFHCA) based on Multi-Spatial Fusion module (MF) and Hierarchical Cooperative Attention module (HCA), the MF is primarily composed of several Global Receptive Field block (GRF). Unlike  [8] ,  [13] , which uses many acoustic features for SER, we only use features from the Hubert model  [14]  and log Mel spectrogram. The Hubert model is trained by performing K-means clustering on MFCC or Hubert features, serving as the training objective. This approach enables the learning of rich feature representations. The log Mel spectrogram better aligns with the auditory characteristics of the human ear. In MFHCA, we employ the MF to extract features in different scale spaces along the temporal and frequency directions, enhancing the model's focus on emotion-related features. Subsequently, the two sets of features are cascaded using the HCA. Finally, a classifier consisting of three fully connected layers is employed to classify the features, completing the emotion recognition task.\n\nOur primary contributions can be summarized as follows:\n\nalse design a Hierarchical Cooperative Attention module (HCA) to integrate the two sets of features interactively. • We propose a novel spectrum-based lightweight feature extraction module, denoted as Multi-Spatial Fusion module (MF), which captures dependencies and positional information in different scale spaces, aiding the network in locating emotional information.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Proposed Method",
      "text": "This section provides an overview of the proposed SER method. In the following subsections, we provide a detailed description of MF, GRF, and HCA.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Overall Architecture",
      "text": "The overall structure of the end-to-end emotion recognition method we proposed is illustrated in Fig.  1 . The method employs two parallel encoders to extract features from the log Mel spectrogram and raw audio. Specifically, MF uses two parallel convolutional layers to capture low-level features from the log Mel spectrogram in both temporal and frequency directions. GRF extracts dependencies and positional information from the features, enhancing the model's ability to learn emotion-related features. In order to save computational time, we utilize Hubert as a feature extractor to obtain feature sequences from the audio directly without fine-tuning Hubert  [12] . HCA hierarchically integrates the two sets of features, and a classifier composed of three fully connected layers utilizes the concatenated features for emotion recognition.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Multi-Spatial Fusion Module",
      "text": "Fig.  1  illustrates the structure of MF, which is primarily composed of a parallel convolutional layer, a pooling layer, and three Global Receptive Field blocks (GRF). We use processed log Mel spectrogram as input. For the first layer of the module, we employ two parallel convolutions with kernel sizes of (10, 2) and  (2, 8) , respectively, to extract features in the time and frequency directions. Not all information in the input features is directly relevant to the SER task. Accurately localizing emotional information enables a more comprehensive representation of emotions. To address this problem, we propose a simple block called GRF, and its structure is illustrated in Fig.  2 .\n\nGRF consists of three parts. Specifically, the first part represents the input features, denoted by X. Global pooling can capture spatial information, but compressing and preserving global spatial information through squeezing may lead to losing positional information. In the second part, we adopt encoding in both the temporal and frequency directions as a replacement for average pooling. We employ global average pooling(GAP) with kernels (H, 1) and (1, W ) to obtain a set of encodings with dependency and positional information. The output for the c-th channel at height h is represented as z c (h), and the output for the c-th channel at width w is denoted as z c (w).\n\nWe utilize 1×1 convolutions to interactively pass positional information between z c (h) and z c (w) to guide the model in locating regions relevant to emotional information. Additionally, this allows for learning relationships between channels.\n\nwhere, [z(h), z(w) T ] represents a cascading operation in the spatial dimension, F denotes a 1×1 convolution operation, BN stands for Batch Normalization, δ means the non-linear activation function swish. Subsequently, we partition feature f in the spatial dimension into two independent tensors, f h and f w .\n\nhere, F h and F w represent 1×1 convolutions, and their role is to adjust the number of channels in f h and f w to be consistent with the input X. σ denotes the sigmoid function. Finally, the outputs of these two parts are used as attention weights applied to the input X. Y a represents the final output.\n\nwhere, ⊙ represents hadamard product.\n\nIn the second part, We employ average pooling to perform downsampling on the input with a rate of r. Assuming the input is a 3D tensor with a shape of (C, H, W ), the shape becomes (C, H/r, W/r) after pooling. Equation  (7)  demonstrates the computation process.\n\nwhere AvgP ool r represents average pooling, F is a convolutional transformation, and U p is a bilinear interpolation operator used to restore features from a smaller-scale space to the original feature space. ⊕ is addition. Y b is the output. The features are transformed into low-dimensional embeddings through downsampling. After transformation by convolutional layers, the low-dimensional embeddings encompass a larger receptive field, enabling the calibration of the convolutional layers in Y a . This communication between the convolutional layers in Y a and Y b expands the receptive field in spatial positions. Each spatial position can also gather context from its surroundings, and dependencies between channels can be learned. Benefiting from this internal information exchange, GRF can generate more discriminative representations. The output of GRF is denoted as Y.\n\nwhere ⊕ represents addition.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Hierarchical Cooperative Attention Module",
      "text": "Research has shown that the hierarchical fusion of different features is helpful in SER tasks  [19] . We employ the Hubert model as a feature extractor without fine-tuning, and the learned features contain rich information. In addition to emotion recognition-related information, information is also relevant to other downstream tasks. Considering these aspects, we designed the HCA module. Spectrogram features are framelevel, and Hubert features contain contextual information. MF can pinpoint the location of emotional information. We use features from MF as guidance for Hubert features, assisting the network in focusing on emotional information within Hubert features and jointly hierarchically obtaining more powerful feature representations. We use f hubert to denote the output of the hidden layer from Hubert, f ′ hubert to represent the output of Hubert features after passing through a BiLSTM, and f spec to denote the spectrogram. We perform the following computations:\n\nwhere ⊗ represents Matrix multiplication, we append a BiL-STM to the end of the Hubert model to obtain a temporal sequence f ′ hubert containing contextual information. A coattention operation is performed between f ′ hubert and f spec to obtain weights f att . Then, a second-level operation is conducted between f att and f hubert , resulting in f ′ att . At this stage, the emotional features in f ′ att are enhanced. Finally, f ′ att is concatenated with f spec to obtain the final feature representation. The emotion classification is achieved through a classifier consisting of three fully connected layers.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. Experiments And Discussion",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Dataset And Implementation Details",
      "text": "We evaluated our approach on the Interactive Emotional dyadic Capture database (IEMOCAP)  [15] , which consists of 10 actors engaged in 5 dyadic sessions, each featuring a unique pair of male and female actors. Following the work of others  [16] -  [18] , we merged happy and excited into a single category labeled as happy, while also considering neutral, sad, happy, and angry emotions.\n\nIn the preprocessing stage, to facilitate comparison with baseline methods  [13] , we also segment the original audio signal into 3-second-long segments. We apply zero padding when the audio segment is less than 3 seconds. Spectrograms are extracted using a Hamming window with a window length of 40ms and a window shift of 10ms. Each windowed block is treated as a frame, and a Discrete Fourier Transform (DFT) of length 800 is applied to transform each frame into the frequency domain. The first 200 DFT points are used as input spectrogram features. Hubert features correspond to the output of the final hidden layer of the Hubert model.\n\nThe model optimizer is Adam, the learning rate is set with 1 × 10 -5 , the training batch size is 32, and early stopping is configured for 10 epochs, the optimization function is Cross Entropy Loss. We employed a 10-fold leave-one-speaker-out cross-validation strategy to assess the model's performance, with evaluation metrics being Unweighted Accuracy (UA) and Weighted Accuracy (WA).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Results And Comparison",
      "text": "Table  1  compares our proposed method and the baselines on WA and UA. We compare our method with these approaches  under two speaker-independent validation strategies. The experimental results demonstrate that our approach achieves the best performance. HNSD  [18]  extracts static and dynamic features for emotion recognition from the log-Mel filter bank feature. The model can achieve better performance when utilizing features from the pre-trained model  [20] . In order to obtain suitable emotion representations, SER systems based on pre-trained models incorporate multiple speech features using methods such as co-attention-based approaches  [13] , crossrepresentation learning  [21] , and cross-attention transformers  [8] . Moreover, our model has 54.26% fewer parameters than the baseline  [13]  while achieving higher performance. The experimental results demonstrate the effectiveness and lightweight of our method.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Ablation Study",
      "text": "In Table  II , we conducted ablation experiments to demonstrate the effectiveness of MF and HCA. \"Hubert\" denotes features extracted using Hubert, and \"W2V2\" represents features extracted using wav2vec2. Specifically, in the experiment involving two features, the absence of HCA implementation does not imply the exclusion of fusion methods. To ensure comparability, we employed baseline fusion methods. We initially conducted experiments using single features and found that MF contributes to the extraction of emotional information in the spectrogram. The pre-trained model used in the baseline is wav2vec2. For a more intuitive comparison, we conducted experiments based on Spec and W2V2. The results indicate that our proposed method performs better in learning emotional information from features compared to the baseline. Subsequently, we further validated our method on Spec and  Hubert, achieving improved recognition accuracy. In Table  III , we employed grid search to investigate the impact of changes in the number of GRFs and channels on the experimental results. In Table  IV , we similarly studied the influence of variations in the scaling factor r on the experimental results.\n\nTo further validate our approach, we employed t-SNE visualization to depict the feature distributions with and without HCA in Fig.  3 . When HCA was used, the dense areas of feature distribution were reduced, and classification boundaries became more distinct.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iv. Conclusion",
      "text": "This paper presents MFHCA, a novel SER approach utilizing MF for spectrogram feature extraction. Our method capitalizes on GRF's advantages to overcome CNN's limitations in capturing global information. HCA fosters interaction between spectral diagrams and Hubert features, jointly harvesting emotional representations from both components. Experimental results on IEMOCAP substantiate the effectiveness of MF and HCA in our approach.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: illustrates the structure of MF, which is primarily",
      "page": 2
    },
    {
      "caption": "Figure 2: GRF consists of three parts. Speciﬁcally, the ﬁrst part rep-",
      "page": 2
    },
    {
      "caption": "Figure 1: The overall architecture of our proposed method.",
      "page": 3
    },
    {
      "caption": "Figure 2: Global Receptive Field block",
      "page": 3
    },
    {
      "caption": "Figure 3: The t-SNE visualization of feature distribution. (a) and (b) are the",
      "page": 5
    },
    {
      "caption": "Figure 3: When HCA was used, the dense areas of",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "element-wise product\naddition\nGlobal Receptive Field block": "Concat\n1×1 Conv\nSigmoid\n1×1 Conv\n1×W GAP\nBN+Swish\n1×1 Conv\nSigmoid\nH×1 GAP\nr\nInterpolate\n3×3 Conv\nAvg Pool"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Models": "Spec",
          "MF": "✗",
          "HCA": "✗\n✗",
          "WA(↑)": "62.13\n62.21",
          "UA(↑)": "62.25\n62.37"
        },
        {
          "Models": "Hubert",
          "MF": "✗",
          "HCA": "✗",
          "WA(↑)": "69.99",
          "UA(↑)": "70.57"
        },
        {
          "Models": "Spec+W2V2",
          "MF": "✗\n✗",
          "HCA": "✗\n✗",
          "WA(↑)": "70.05\n71.73\n70.72\n72.00",
          "UA(↑)": "71.30\n72.32\n72.09\n73.44"
        },
        {
          "Models": "Spec+Hubert",
          "MF": "✗\n✗",
          "HCA": "✗\n✗",
          "WA(↑)": "72.13\n73.72\n73.19\n74.24",
          "UA(↑)": "72.51\n74.53\n73.72\n74.57"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "HNSD [18]\nCo-attention [13]\nSMW CAT [8]\nE2e ASR [20]\nCross-representation [21]\nMFHCA(ours)",
          "Modality": "A\nA\nA\nA+T\nA+T\nA",
          "WA(↑)\nUA(↑)": "70.50\n72.50\n71.64\n72.70\n73.80\n74.25\n71.70\n72.60\n73.00\n73.50\n74.24\n74.57"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion recognition in human-computer interaction[J]",
      "authors": [
        "R Cowie",
        "E Douglas-Cowie",
        "N Tsapatsoulis"
      ],
      "year": "2001",
      "venue": "IEEE Signal processing magazine"
    },
    {
      "citation_id": "2",
      "title": "Key-sparse transformer for multimodal speech emotion recognition",
      "authors": [
        "W Chen",
        "X Xing",
        "X Xu"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "3",
      "title": "Exploring complementary features in multimodal speech emotion recognition",
      "authors": [
        "S Wang",
        "Y Ma",
        "Y Ding"
      ],
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Improve accuracy of speech emotion recognition with attention head fusion[C]//2020 10th annual computing and communication workshop and conference (CCWC)",
      "authors": [
        "M Xu",
        "F Zhang",
        "S Khan"
      ],
      "year": "2020",
      "venue": "Improve accuracy of speech emotion recognition with attention head fusion[C]//2020 10th annual computing and communication workshop and conference (CCWC)"
    },
    {
      "citation_id": "5",
      "title": "Speech emotion recognition with localglobal aware deep representation learning",
      "authors": [
        "J Liu",
        "Z Liu",
        "L Wang"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "6",
      "title": "DWFormer: Dynamic Window Transformer for Speech Emotion Recognition",
      "authors": [
        "S Chen",
        "X Xing",
        "W Zhang"
      ],
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "7",
      "title": "DST: Deformable speech transformer for emotion recognition",
      "authors": [
        "W Chen",
        "X Xing",
        "X Xu"
      ],
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "8",
      "title": "Multiple acoustic features speech emotion recognition using cross-attention transformer",
      "authors": [
        "Y He",
        "N Minematsu",
        "D Saito"
      ],
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations[J]. Advances in neural information processing systems",
      "authors": [
        "A Baevski",
        "Y Zhou"
      ],
      "year": "2020",
      "venue": "wav2vec 2.0: A framework for self-supervised learning of speech representations[J]. Advances in neural information processing systems"
    },
    {
      "citation_id": "10",
      "title": "Temporal Context in Speech Emotion Recognition",
      "authors": [
        "Y Xia",
        "L Chen",
        "A Rudnicky"
      ],
      "year": "2021",
      "venue": "Temporal Context in Speech Emotion Recognition"
    },
    {
      "citation_id": "11",
      "title": "wav2vec: Unsupervised pre-training for speech recognition",
      "authors": [
        "S Schneider",
        "A Baevski",
        "R Collobert"
      ],
      "year": "2019",
      "venue": "wav2vec: Unsupervised pre-training for speech recognition",
      "arxiv": "arXiv:1904.05862"
    },
    {
      "citation_id": "12",
      "title": "Speaker normalization for selfsupervised speech emotion recognition",
      "authors": [
        "I Gat",
        "H Aronowitz",
        "W Zhu"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "Speech emotion recognition with co-attention based multi-level acoustic information",
      "authors": [
        "H Zou",
        "Y Si",
        "C Chen"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W N Hsu",
        "B Bolte",
        "Y H H Tsai"
      ],
      "venue": "Self-supervised speech representation learning by masked prediction of hidden units"
    },
    {
      "citation_id": "15",
      "title": "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "16",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database[J]. Language resources and evaluation",
      "authors": [
        "C Busso",
        "M Bulut",
        "C C Lee"
      ],
      "year": "2008",
      "venue": "IEMOCAP: Interactive emotional dyadic motion capture database[J]. Language resources and evaluation"
    },
    {
      "citation_id": "17",
      "title": "Representation learning with spectrotemporal-channel attention for speech emotion recognition",
      "authors": [
        "L Guo",
        "L Wang",
        "C Xu"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "18",
      "title": "Emotion recognition by fusing time synchronous and time asynchronous representations",
      "authors": [
        "W Wu",
        "C Zhang",
        "P Woodland"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "Hierarchical network based on the fusion of static and dynamic features for speech emotion recognition",
      "authors": [
        "Q Cao",
        "M Hou",
        "B Chen"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "20",
      "title": "Recognizing emotions in spoken dialogue with hierarchically fused acoustic and lexical features",
      "authors": [
        "L Tian",
        "J Moore",
        "C Lai"
      ],
      "year": "2016",
      "venue": "IEEE Spoken Language Technology Workshop"
    },
    {
      "citation_id": "21",
      "title": "Speech sentiment analysis via pre-trained features from end-to-end asr models",
      "authors": [
        "Z Lu",
        "L Cao",
        "Y Zhang"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "Multimodal emotion recognition with high-level speech and text features[C]//2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)",
      "authors": [
        "M Makiuchi",
        "K Uto",
        "K Shinoda"
      ],
      "year": "2021",
      "venue": "Multimodal emotion recognition with high-level speech and text features[C]//2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)"
    },
    {
      "citation_id": "23",
      "title": "WeaveNet: End-to-End Audiovisual Sentiment Analysis",
      "authors": [
        "Y Yu",
        "Z Jia",
        "F Shi"
      ],
      "venue": "International Conference on Cognitive Systems and Signal Processing"
    }
  ]
}