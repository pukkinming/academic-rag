{
  "paper_id": "2403.05234v2",
  "title": "Benchmarking Micro-Action Recognition: Dataset, Methods, And Applications",
  "published": "2024-03-08T11:48:44Z",
  "authors": [
    "Dan Guo",
    "Kun Li",
    "Bin Hu",
    "Yan Zhang",
    "Meng Wang"
  ],
  "keywords": [
    "micro-action",
    "body language",
    "human behavioral"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Micro-action is an imperceptible non-verbal behaviour characterised by low-intensity movement. It offers insights into the feelings and intentions of individuals and is important for human-oriented applications such as emotion recognition and psychological assessment. However, the identification, differentiation, and understanding of micro-actions pose challenges due to the imperceptible and inaccessible nature of these subtle human behaviors in everyday life. In this study, we innovatively collect a new micro-action dataset designated as Microaction-52 (MA-52), and propose a benchmark named microaction network (MANet) for micro-action recognition (MAR) task. Uniquely, MA-52 provides the whole-body perspective including gestures, upper-and lower-limb movements, attempting to reveal comprehensive micro-action cues. In detail, MA-52 contains 52 micro-action categories along with seven body part labels, and encompasses a full array of realistic and natural micro-actions, accounting for 205 participants and 22,422 video instances collated from the psychological interviews. Based on the proposed dataset, we assess MANet and other nine prevalent action recognition methods. MANet incorporates squeeze-andexcitation (SE) and temporal shift module (TSM) into the ResNet architecture for modeling the spatiotemporal characteristics of micro-actions. Then a joint-embedding loss is designed for semantic matching between video and action labels; the loss is used to better distinguish between visually similar yet distinct micro-action categories. The extended application in emotion recognition has demonstrated one of the important values of our proposed dataset and method. In the future, further exploration of human behaviour, emotion, and psychological assessment will be conducted in depth. The dataset and source code are released at https://github.com/VUT-HFUT/Micro-Action.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "E NABLING the computer to intelligently detect and rec- ognize human micro-actions has garnered increasing attention within the artificial intelligence community  [2] ,  [3] ,  [4] ,  [5] . Researches show that micro-actions are the important human actions, generally, they can serve as the nonverbal cues to better reflect a person's mental state compared to general movements  [6] ,  [7] . For example, a slight nod indicates a positive state of agreement, and a small shake in the legs may indicate a negative state such as tension. A strong Micro-action recognition (MAR) system can provide high-quality support for human-oriented technical services and innovations, it has practical implications for various fields, such as medical diagnostics  [8] , smart vehicles  [3] , face expression recognition  [9] ,  [10] , sports competitions  [11] , virtual reality  [12] .\n\nSpecifically, micro-action is characterized by rapid and subtle behaviors, exhibiting extremely low behavioral intensities. It involves whole-body movements, including the head, hands, and upper and lower limbs  [13] ,  [14] . This behavior is often considered a \"silent\" form of communication, operating unconsciously and imperceptibly when individuals express their genuine thoughts, feelings, and intentions. Psychological theory reveals that micro-action provides a more transparent representation of individual intent than do carefully crafted verbal pronouncements  [15] . Further research has also found that human behavior offers a more accurate gauge of emotion than facial expression  [13] ,  [16] . For instance, discerning an athlete's emotional state related to competition outcomes is challenging when relying solely on facial expressions such as weeping. However, it becomes feasible through recognition of postural behaviors  [17] . These observations highlight the emerging effort to employ micro-actions to unveil the individuals' inner spirit or mentality.\n\nMicro-action recognition (MAR) aims to detect and distinguish ephemeral body movements, generally occurring within a temporal span of 1/25s ∼ 1/3s. The MAR task is similar to conventional action recognition in that it uses video instances as input and requires precise and efficient algorithms. However, it is uniquely complex due to lowamplitude fluctuations in gesture and posture. The primary challenges of the MAR task are as follows: 1) Minor visual changes. Micro-actions are subtle and rapid muscle movements that occur across various body parts, such as the head, hands, arms, legs, and feet. Identifying and distinguishing between these subtle variations in movement is a complex task.\n\n2) Approximate inter-class differences. When categorizing samples from the same body part under distinct actions, visual similarity can make differentiation complicated. For example, it can be difficult to distinguish between \"looking up\" and \"nodding\" in a video clip. The former involves elevating the head, whereas the latter is characterized by repeated upward and downward head motions. 3) Long-tailed issue. A longtailed distribution is prevalent when the frequency of sample occurrences varies significantly across different micro-action categories. It is difficult to avoid such data imbalance in wild data collection environments. For example, \"nodding\" is notably more common than \"rubbing eyes\" or \"touching ears.\" 4) Limited data sources. The development of MAR is limited by the scarcity of participant samples and small datasets. These limitations are often worsened by the difficulty in perceiving micro-actions and concerns regarding privacy protection.\n\nExisting achievements in generic action recognition tasks  [18] ,  [19] ,  [20] ,  [21] ,  [22]  focus on categorizing activities at a coarse-grained level (e.g. running and jumping), without exploring the tiny details of action variations (e.g. raising the head and nodding). Recently, there have been reports of new fine-grained action recognition tasks  [23] ,  [11] ,  [24] . However, these tasks are limited to specific scenarios such as gymnastics, basketball, football, makeup, and daily life. These scenarios involve specialized behavioral patterns tailored to each action type.\n\nTo facilitate the micro-action research, we have collected a comprehensive whole-body micro-action dataset of 52 action categories, called Micro-Action-52 (MA-52). We recruited a significant cohort of 205 participants using a specialized faceto-face psychological interview scheme to capture authentic and spontaneous micro-actions for scholarly investigations. The interviewers are provided with access to the participants' SCL-90  [1]  test results. They use open-ended questions to elicit a wide range of micro-actions from the participants. Participants are not compelled to adopt specific gestures or postures. Instead, natural expressions of their genuine spirit or mentality are encouraged. To ensure the comfort and relaxation of the participants, they remain seated during the interviews.\n\nAs discussed in Section II, previous works have focused on collecting upper-limb movements, whereas our dataset includes several lower-limb movements such as \"shaking legs\", \"crossing legs\", \"tiptoe\", and \"scratching feet\".\n\nThe proposed MA-52 dataset comprises 52 action categories and 22,422 video samples. We use high-definition cameras with a resolution of 1920×1080 pixels to record the interview videos. Comparative statistics between MA-52 and the preexisting datasets are provided in Table  I . Compared with prior studies in Table  I , the MA-52 introduces a wealth of leg and foot movements to body language research. Furthermore, the dataset considers a greater variety of bodily interactions, involving the types of body-hand, head-hand, and leg-hand interactions, as shown in Figure  2 . Overall, the MA-52 dataset stands out for its larger number of participants, extensive array of action categories, and greater volume of video instances with diverse lower-body movements and bodily interactions. These naturally occurring micro-actions can serve as reliable indicators of individual characteristics, emotional states, thought processes, and intentions. More details of MA-52 are elaborated in Section III.\n\nBased on the new dataset MA-52, we propose a compatible benchmark network for micro-action recognition, namely Micro-Action Network (MANet). Furthermore, we conduct an extensive review of existing nine micro-action methods and provide a comprehensive performance comparison. To test the practicality of our fundamental data and methodology, we further investigate the application of micro-action recognition in emotion analysis.\n\nThe contributions of our work are summarized as follows:\n\n• A new dataset MA-52 is collected and has made publicly accessible to satisfy academic requirements for microaction analysis. It is a comprehensive whole-body dataset consisting of a large number of video instances  (22, 422) , a diverse participant pool (205), and arrays of body parts  (7)  and action categories  (52) . MA-52 is unique in its data collection method, which involves a specialized interview-based scheme. This allows participants to display natural, spontaneous, and authentic micro-actions during seated interviews, resulting in high-quality data that is rich in lower-body movements and complex bodily interactions. The remainder of this paper is organized as follows. The related works are reviewed in Section II, and the new microaction dataset MA-52 is introduced in Section III. We elaborate on the methodologies and experiments for micro-action recognition in Sections IV and V. An application for emotion analysis is discussed in Section VI. Finally, we discuss future direction in Section VII and conclude this study in Section VIII.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Tasks And Datasets",
      "text": "Human behavior analysis  [25] ,  [26] ,  [27] ,  [28] ,  [29] ,  [30]  has been extensively studied over the past few decades, with several public action datasets being released, such as HMDB51  [26] , ActivityNet  [27] , UCF-101  [18] , Sports 1M  [20] , and Kinetics  [19] . These datasets have contributed significantly to the development of the action recognition field. However, they suffer from data bias  [31]  and do not adequately address the nuances of fine-grained activities, such as distinguishing between high jump, long jump, and triple jump in the jump type. The datasets contain data bias  [31]  due to their coarse-grained action labels, as seen in UCF-101  [18]  and Kinetics  [19] , where actions can be recognized from static frames, such as jumping and running. In this situation, it seems that action recognition no longer necessitates spatiotemporal learning. In contrast, fine-grained action recognition focuses on decomposing activities into their constituent phases and detecting minute variations between closely related actions  [24] . Recently, datasets such as FineGym  [11] , Basketball  [24] , and FineAction  [23]  have gained attention. These datasets aim to distinguish subclasses within broader action categories that typically exhibit reduced inter-class variations. But, these are specialized behavioral patterns tailored to each action type.\n\nIn contrast to the aforementioned efforts, micro-action recognition in the current study requires not only the ability to differentiate fine-grained action categories but also a keen sensitivity to subtle visual alterations. Actions such as \"shrugging\" and \"shaking legs\" exemplify the complexity involved in capturing subtle and rapid changes in movement. The key challenges in micro-action recognition are both the identification of small movements and the differentiation between subtly distinct action categories. Thus, emerging research endeavors have been initiated to advance the state of microaction analysis. Comparative statistics and characteristics of existing action datasets are presented in Table  I . Currently, only a limited number of datasets have focused on humancentered spontaneous micro-action behaviors. For instance, the iMiGUE dataset  [17]  annotates 72 athletes across 32 behavioral classes during sports press conferences, while simultaneously recording their emotional states following the outcome of their matches. This dataset aims to interpret athletes' emotional states by analyzing their micro-gestures. Similarly, the Spontaneous Micro Gesture (SMG) dataset  [32]  incorporates both micro-gesture and emotion recognition but requires 40 participants to narrate both fabricated and factual stories. The limitation here is that predetermined stories may compromise the authenticity of the participants' microgestures. Additionally, the Bodily Behaviors in Social Interaction (BBSI) dataset  [33]  employs a naturalistic multi-view group conversation approach to examine the impact of body language in various social contexts, including leadership and rapport, and only captures 15 different body movements such as \"gesture,\" \"adjusting clothing,\" \"scratching.\" In contrast, our work utilizes a professional face-to-face psychological interview procedure and recruits an extensive cohort of 205 participants to collect genuine micro-actions. The dataset contains 52 distinct micro-action categories captured within the confines of a seated posture, thus facilitating the inclusion of complex leg and foot movements such as \"crossing legs\" and \"stretching feet.\" All the spontaneous movements provide valuable insights into individuals' cognitive processes, emotional states, and intentions. Detailed information about our dataset can be found in Section III.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "B. Approaches",
      "text": "Amid the rapid advancement of large-scale datasets such as ActivityNet  [27]  and Kinetics  [19] , the field of action recognition has seen remarkable progress. Well-known methods such as TSN  [37] , TSM  [38] , TIN  [39] , C3D  [40] , I3D  [41] , SlowFast  [42] , Video-SwinT  [43] , TimeSformer  [44] , and UniFormer  [45]  have achieved exceptionally high accuracy. However, these techniques mainly excel at recognizing generic actions such as jumping and running and deliver less accuracy in finer action categories. The recent focus on fine-grained action recognition has stimulated innovative approaches. Behera et al.  [46]  argue that specific fine-grained actions have local discriminative semantic regions that can be evaluated using the attention mechanism. Consequently, they propose a regional attention network that aggregates multiple contextual regions and focuses on relevant ones. Inspired by the human visual system, Li et al.  [47]  develop a dynamic spatiotemporal specialization module that uses specialized neurons to discriminate subtle differences between fine-grained actions. Xu et al.  [48]  argue that regions of the body where movement occurs, such as the limbs and the trunk, provide rich semantic information. Therefore, they propose a pyramid self-attention polymerization learning framework that uses contrastive learning to jointly understand the representations of body movements and joints at different anatomical levels.\n\nIn contrast, progress in micro-action recognition lags behind that of in generic and fine-grained action recognition due to significant challenges in dataset acquisition and labeling. As shown in Table  I , the iMiGUE dataset  [17]  faces the problem of action class imbalance and long-tailed distribution, especially in uncontrolled and natural settings. This imbalance could cause significant performance degradation even in fully supervised learning models due to extreme label bias. To counteract this, the authors introduce an unsupervised encoderdecoder network to learn the discriminative features of microgestures without relying on labeled data. For the SMG dataset, Chen et al.  [32]  evaluate existing skeleton and RGB-based methodologies and conclude that GCN-based skeleton models with compact network structures outperform their RGBbased counterparts. In addition, the BBSI dataset  [33]  uses   [41] , TSN  [37] , TSM  [38] , Swin Transformer  [49] ) and employs the Pyramid Dilated Attention Network (PDAN)  [50]  to understand temporal relations in social behavior within group conversations.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Applications",
      "text": "The field of micro-action recognition offers considerable possibilities and opportunities, especially in applications focused on human interaction. These techniques offer valuable insights by studying body movements, postures, and facial expressions. Köpüklü et al.  [3]  publish the Driver micro hand gesture (DriverMHG) dataset and a corresponding model to dynamically recognize micro hand gestures in vehicle environments. Gupta et al.  [51]  describe the utility of internet of things terminals equipped with motion sensors for humancomputer interaction, facilitating the capture of users' unique hand micro-movements for authentication. Chandio et al.  [52]  propose the HoloSet dataset for visual-inertial odometry applications in Extended Reality (XR), capturing both macro and micro human actions. Recent scientific efforts have demonstrated that body movements and posture variations crucial for understanding human emotions  [13] ,  [46] ,  [16] . In particular, emotion analysis is a salient application of microaction recognition  [17] ,  [32] ,  [53] ,  [54] ,  [55] . Numerous studies have been conducted to detect human emotional states by observing changes in facial expression, limb movement, and overall body posture. For example, Deng et al.  [53]  capture both micro and macro facial cues for emotion prediction, while Liu et al.  [17]  use spontaneous upper limb actions, and Luo et al.  [54]  design a holistic model that includes body components, inherent intentions, shapes, and spaces for the same purpose.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iii. New Dataset A. Data Collection And Annotation",
      "text": "Nevertheless, the capture of spontaneous micro-actions poses significant challenges, primarily because of their rapid occurrence at low motion intensities and their inherent difficulty in detection and annotation. Owing to limited data and technological barriers, the field of micro-action recognition remains nascent. Current data collection can be categorized into two types. The first type involves the annotation and recording of tiny human behaviors in short film clips or TV shows  [17] ,  [54] , which has the advantage of realistic scenarios, but suffers from issues such as unfavorable camera angles. The second approach involves inducing micro-actions in controlled laboratory settings  [56] ,  [57]  using stimuli such as emotionally intense videos or instructing actors to portray specific emotions and behaviors in simulated environments  [2] ,  [32] . Although these environments can provoke a range of human behaviors, the elicited micro-actions may lack authenticity, being either exaggerated or suppressed. In contrast to these works, this study adopts a communicative interview approach, inviting participants to engage in a conversation with a professional psychological counselor.\n\nIn order to optimally capture unconscious and spontaneous human micro-actions with a high degree of naturalism, certain challenges inherent to the psychological interview setting must be overcome. 1) Elicitation of spontaneous microactions. First, the participants are administered the SCL90-Test  [1] , a comprehensive 90-item psychological assessment questionnaire. A professional counselor then takes on the role of interview facilitator and asks a series of open-ended questions. These questions are designed to provoke deep introspection and detailed self-description, thereby encouraging participants to share rich information about themselves. To gauge participants' genuine feelings, thoughts, and intentions, the counselor provides timely feedback and asks follow-up questions, facilitating the emergence of nonverbal cues. In this study, we focus on their gestures and postures. 2) Collecting whole-body micro-actions. High-definition cameras are used to record unconscious behavior from a whole-body perspective during the interviews. These behaviors encompass movements of various parts of the body, including the head, hands, and both upper and lower limbs, as well as interactive movements between different parts of the body (e.g., head-hand and leghand interactions). To our knowledge, our dataset is unique in capturing participants while seated, thus providing additional data on lower-limb activities such as \"leg shaking,\" \"touching legs,\" and \"patting legs.\" 3) Ensuring annotation quality. A rigorous three-step supervisory process oversees the complete data annotation. The first step involves self-censorship by volunteer annotators, each responsible for independently editing   and annotating one complete video. This step facilitates the identification and correction of potential errors. The second step is cross-checking, which aims to reduce biases and inaccuracies that might arise from the volunteers' subjective interpretations of the annotations. When discrepancies arise, they are reconciled based on a majority consensus. In the final step, a third-party team reviews the data annotations for ultimate verification following the completion of all microaction video annotations.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "B. Dataset Statistics And Properties",
      "text": "The Micro-Action-52 dataset (abbreviated as MA-52) comprises 205 participants and 22,422 video instances of microactions. Each video instance is recorded at a resolution of 1920×1080 pixels and at a frame rate of 30 fps. The duration of these instances ranges from 1s to 7s, with an average duration of 1.9s, cumulating to a total span of 12.29h. The dataset is split into training, validation, and test subsets at a ratio of 2:1:1, consisting of 11,250, 5,586, and 5,586 instances, respectively. Before data collection, we informed each participant of the requirements for data collection and obtained their signed informed consent for academic purposes. We categorized micro-actions based on a two-level labeling system: coarse-grained categories included seven body parts (i.e., body, head, upper limb, lower limb, head-hand, bodyhand, and leg-hand), and fine-grained categories contained 52 distinct micro-action categories.\n\nAs shown by the statistics listed in Table  I , the proposed dataset exhibits the following properties: 1) Large dataset.\n\nThe MA-52 dataset we propose is the largest repository of micro-actions, featuring 205 participants and 22,422 microaction instances. 2) Professional psychological interview. To ensure the acquisition of realistic and authentic micro-actions, we invited professional counselors to host the interview. 3) Whole-body micro-actions. Compared to existing datasets, our data collection surpasses existing strategies by capturing a broader spectrum of whole-body micro-actions, with particular emphasis on lower-limb movements (e.g., crossing legs, shaking legs, and stretching feet). 4) Diverse participants. Conscious efforts are made to achieve balance in gender and age demographics. Although the challenge of long-tailed distribution persists, we attempt to render the dataset as representative as feasible in an uncontrolled setting. It includes 16 children (7.8%), 37 young individuals (18.1%), 97 middleaged participants (47.3%), and 55 elderly individuals (26.8%). Additionally, the MA-52 dataset comprises 100 male (49%) and 105 female (51%) participants. 5) Two-level (coarseto-fine) annotation. The dataset is annotated in a two-level annotation system: 52 micro-action categories are aggregated under seven body labels (i.e., body, head, upper limb, lower limb, head-hand, body-hand, and leg-hand). In terms of the closest relevance, iMiGUE  [17]  offers 32 categories of microgestures distributed across five groups (i.e., body, head, hand, body-hand, and head-hand). Our annotations strive for granularity in distinguishing similar yet distinct micro-actions. For instance, in MA-52, \"nodding head\" and \"bowing head\" are categorized under the coarse-grained \"head\" label. 6) Highresolution data. As the statistics shown in Table  I , our dataset portrays the instances of high-resolution 1920×1080 video. Thus, compared with existing datasets, our dataset delivers significant advantages in terms of action categories, action instances, participants, and video format.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Iv. Methodologies",
      "text": "Owing to the inherent complexity of micro-actions, distinguishing between similar yet distinct categories of microactions remains an arduous challenge. To address this task, this section introduces a methodology established on the ResNet backbone  [59]  combined with squeeze-and-excitation (SE)  [58]  and temporal shift module (TSM)  [38]  for microaction recognition and also examines existing generic action recognition algorithms.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Task Definition",
      "text": "Consider a human micro-action video defined as V = {I 1 , I 2 , . . . , I T }, where T is the length of the video. Our goal is to classify the micro-actions contained in the video by selecting them from a set of micro-action labels Y. We annotate the affiliation relationship between (fine-grained) micro-actions {y 1 , ..., y N F A } and (coarse-grained) body parts {y 1 , ..., y N C A }, where N F A and N C A represent the number of micro-action categories and body parts, respectively. The recognized fine-grained micro-action category serves as an indicator of its corresponding coarse-grained body part.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. General Action Recognition Benchmarks",
      "text": "In this work, we evaluate nine existing algorithms specifically designed for generic action recognition tasks. These algorithms fall into three different categories: 1) 2D CNN-based methods (TSN  [37] , TSM  [38] , and TIN  [39] ); 2) 3D CNNbased methods (C3D  [40] , I3D  [41]  and SlowFast  [42] ), and 3) Transformer-based methods (VSwinT  [43] , TimeSF  [44] , and UniF  [45] ). For implementation, we retain the architecture of these algorithms and append a fully connected (FC) layer to act as an action classifier. The classifier produces a probability vector for all micro-action categories, written as y = W•X+b, where X is the representation vector of the video obtained from the benchmark algorithms, W is a learnable parameter, and b is the bias.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Our Method",
      "text": "Pipeline Overview. The pipeline of our method is illustrated in Figure  3 . We adopt the ResNet architecture as the backbone and incorporate the Temporal Squeeze-and-Excitation (SE)  [58]  and Temporal Shift Module (TSM)  [38]  and into it for micro-action recognition. We refer to this composite model as MANet, which offers the advantages of both SE and TSM: SE extends the spatial feature map via channelwise excitation, while TSM facilitates temporal information exchange by reallocating a subset of channels along the timeline. Thus, each module uniquely addresses either spatial or temporal learning. First, we map the original video V into a feature map F ∈ R T ×H/4×W/4×C through a convolutional layer with a kernel size of 7×7 and a max pooling layer. Then a new ResNet block is equipped with SE  [58]  and TSM  [38] . F is fed into a series of ResNet blocks for spatio-temporal modeling. For each block, the first step is SE-squeeze, which uses an average pooling to compress F ∈ R T ×H/4×W/4×C into z ∈ R T ×1×1×C . The second step involves SE-excitation, which consists of two FC layers. The first FC layer contains C Ratio neurons and maps z into the dimension R T ×1×1× C Ratio ; the second FC layer contains C neurons and reshapes the z back to the original dimension R T ×1×1×C . We set Ra-tio=4. The three steps include performing the element-wise multiplication is performed on the channel weight with the original feature map F ∈ R T ×H/4×W/4×C ; the new feature F ′ is obtained. Furthermore, we conduct the TSM operation. We split F ′ into eight chunks and shift the chunks along the temporal dimension. The process is formulated as follows:\n\nwhere 1/8 denotes the shifted length and the kernel size of the convolutional layer Conv is set to 3 × 3. In the last step of the ResNet block, the feature F is fed to three convolution layers with the kernel sizes of 1 × 1, 3 × 3, and 1 × 1, and then summed up with F. We implement four ResNet blocks and output the video feature X ∈ R T ×H/32×W/32×32C .\n\nEmbedding-based Optimization. The basic objective function employed for this task is the cross-entropy loss, denoted as L cls . This loss constrains the predicted probability corresponding to the correct action. Additionally, an embedding loss L emb is incorporated to align the semantic distance between the video and its ground-truth action label. Specifically, the action labels are transformed into GloVe embedding vectors  [60] . The embedding vector of each action label, denoted as X q ∈ R 300 , is obtained via the average pooling of the action words for each video instance. The averaged visual features of the video are then mapped into a joint embedding space, represented as X z ∈ R 300 . A semantic constraint is imposed by computing the Euclidean distance between X q and X z . The embedding loss formulation is given as follows:\n\nThe total loss function is formulated as follows:\n\nwhere α denotes a balance hyperparameter for the loss terms.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "V. Experiments A. Evaluation Metrics",
      "text": "We adopt standard evaluation metrics for generic action recognition tasks, including Accuracy (Acc) and F1 score  [61] . Specifically, we compute Acc-Top1 and Acc-Top5 under the categories of \"coarse\" and \"fine\" action labels. The F1 metric is particularly relevant in scenarios with unbalanced data and is implemented in variants such as F1 micro and F1 macro . Given the long-tailed distribution of micro-action categories in our data, all F1 metrics are used to provide comprehensive evaluations from multiple perspectives. Importantly, the terms \"micro\" and \"macro\" in F1 micro and F1 macro refer solely to their metric calculation methods and should not be confused with \"finegrained\" and \"coarse-grained\" labels in our dataset.\n\nIn detail, F1 micro evaluates the overall true positives, false negatives, and false positives, while F1 macro calculates the unweighted mean for each action category. F1 micro treats all samples equally and is less influenced by any category with a predominant number of micro-actions. Conversely, F1 macro weights the contributions of each category equally. The metrics are expressed as follows:\n\nwhere N is the number of samples and N C denotes the number of micro-action categories; Pre j denotes the predicted precision for micro-action category j. {TP i }, {FN i }, and {FP i } are the true positives, false negatives, and false positives in the dataset.\n\nWith regard to the two-level (coarse-and fine-grained) action labels in the present dataset, we denote the metrics as F1 coarse macro , F1 fine macro , F1 coarse micro , and F1 fine micro . We calculate a mean F1 as follows:",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "B. Implementation Details",
      "text": "In the MA-52 dataset, the average video duration is 1.9s. Therefore, we empirically sample T = 8 frames from each video sample and resize each frame to 224 × 224. The MANet is constructed on the ResNet  [59]  and equipped with SE  [58]  and TSM  [38] . The SE ratio is set to 4, while the temporal shift length in TSM is set to 1/8. For model optimization, the balancing hyperparameter α in Eq. 3 is set to 5. We set the SGD optimizer with a learning rate of 0.001, a momentum of 0.9, a weight decay of 1e-4, and a batch size of 10 for model training. The learning rate is reduced by a factor of 10 at the 30th and 60th epochs, and the model is trained with 80 epochs. All experiments are run on the PyTorch platform.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Experimental Analysis",
      "text": "Main Comparison. As shown in Table  II , in the context of 2D CNN, 3D CNN, and Transformer models, TSM  [38] , SlowFast  [42] , and UniFormer  [45]  achieve commendable performance, securing F1 mean scores of 61.39%, 63.09%, and 64.43%, respectively. In particular, UniFormer stands out for its exceptional feature representation and global context modeling. Our proposed MANet outperforms UniFormer by 1.16% on F1 mean , thus demonstrating the effectiveness of our methodology. Observing the experimental results, regardless of coarse-or fine-grained labels, our model sets new benchmarks on all major evaluation metrics such as F1 micro , F1 macro , F1 mean , Acc-Top1, and Acc-Top5. For example, MANet outperforms UniFormer by 2.44% on F1 fine micro and outperforms the baseline TSM  [38]  by 4.20% on F1 mean .\n\nFurthermore, we observe that fine-grained micro-action recognition is inherently more challenging than coarse-grained recognition. As shown in Table II again, the results of finegrained evaluations are consistently lower than their coarsegrained counterparts across all methods. Specifically, MANet's F1 macro score is 72.87% in the coarse-grained setting, but drops sharply to 49.22% in the fine-grained setting. This trend is also evident in the Acc-Top1 metric, where the coarse-grained body part Acc-Top1 exceeds 78.95% but falls below 61.33% in the fine-grained micro-action recognition. This phenomenon is consistent with fine-grained generic action recognition  [47] ,  [48] ,  [46] ,  [17] ,  [62] , which is characterized by a wide range of action categories.\n\nAblation Studies. We perform ablation experiments of main components, loss hyperparameter α, and the number of sampling frames per video on all of the evaluation metrics. The experimental results demonstrate the stability of our model. There are three variants of MANet: 1) baseline: TSM  [38] , 2) w/o SE: removing SE  [58]  from MANet, and 3) w/o",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Action Label",
      "text": "Metric TSN  [37]  TSM  [38]  TIN  [39]  C3D  [40]  I3D  [41]  SlowFast  [42]  VSwinT  [43]  TimeSF  [44]    The hyperparameter α is used to balance L cls and L emb . Its effect is to prevent L cls from being overly dominant or L emb from being neglected. It can effectively prevent overfitting training resulting in poor performance. In other terms, L emb is used to strengthen the constraint at the semantic level. Table  IV  shows the results for the parameters α ∈ {0, 0.1, 1, 2, 5, 10}. Observing Table  IV , α=0 indicates that removing emb from MANet (w/o emb) produces the lowest performance for all the metrics. When α=5, the result of F1 mean with {fine, coarse} labels is optimal at 65.59%. The hyperparameter α positively affects the balance between L cls and L emb .\n\nWe conduct the ablation experiment of the setting of T in Table  III . When T is set to 4, the Acc-Top1 metric with fine labels drops 6.76% compared to the setup of T = 8. We speculate that important information is lost when the input frames are too small, resulting in the model being unable to capture the continuous details of micro-actions. When T is set to 16, the performance is dropped by a large margin, e.g., F1 mean is dropped from 65.59 to 64.66. This may be due to the fact that redundant frames interfere with microaction acquisition. Based on the above observations, we set the number of sampling frames to T = 8, which can reduce computing costs while retaining important information to achieve the best micro-action recognition effect.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "D. Visualization Examples",
      "text": "In this subsection, we present visualizations from different perspectives, including feature distribution, confusion matrix, and prediction results, all focused on micro-action recognition.\n\nFeature Distributions. We plot the feature distribution using t-SNE  [63] . Specifically, the features are extracted from the layer preceding the classifier in both MANet and TSM architectures. Figure  5  shows these features categorized into both coarse-and fine-grained labels. Notably, MANet exhibits superior clustering efficiency in distinguishing both coarse-and fine-grained categories as compared to TSM. In Figure  5  (b), samples with micro-action labels \"B1\" and \"B6\" (\"nodding\" and \"head up\") are cleanly segregated into two distinct clusters using our method. Conversely, Figure  5  (a) presents a more amalgamated distribution of micro-action For instance, Figure  4  (e) reveals that MANet accurately predicts the micro-actions \"hands touching fingers\" in conjunction with the body part interaction of \"upper limb.\" Conversely, the TSM model misclassifies the coarse-grained label as \"lower limb\" and incorrectly identifies the fine-grained label as \"crossing legs\" in this specific case. The task of distinguishing between highly similar micro-actions remains a pressing challenge. samples when using TSM. This evidence confirms that MANet can effectively discriminate between micro-actions that are similar but different. Confusion Matrices of the Prediction Results. A2: \"turning around,\" and A4: \"shrugging\") are misclassified, possibly due to approximate inter-class difference and insufficient training data. The long-tailed distribution of micro-action categories remains an issue to be resolved in this field. Visualization Examples. Figure  4  presents six comparative examples between TSM and MANet on coarse-and finegrained recognition. Figures  4 (a ) and (b) highlight the importance of capturing the temporal dynamics of micro-actions. In particular, the distinction between \"tilting head\" and \"turning head\" requires the frequency of the head movement to be determined, as shown in Figure  4  (a), and the distinction between \"stretching feet\" and \"spread legs\" lies in the parts of the movements as shown in Figure  4 (b) . In Figure  4  (c), MANet accurately identifies the micro-action as \"touching nose\", in contrast to TSM, which incorrectly identifies it as \"scratching or touching face\". MANet predicts the correct micro-actions. Figure  4 (d)  demonstrates that MANet provides superior results on the correct body part and the correct microaction category. TSM misclassifies the label \"scratching or touching neck\" and incorrectly recognizes the micro-action \"scratching or touching shoulder\" in this instance. Figure  4  (e) describes MANet's ability to recognize subtle micro-actions such as \"hands touching fingers\". Figure  4  (f) illustrates the importance of temporal information in micro-action recognition. Our model can predict the action as \"patting legs\", while TSM incorrectly predicts the action as \"touching legs\". Distinguishing the extremely small differences between the similar actions remains an urgent challenge for this task.",
      "page_start": 8,
      "page_end": 10
    },
    {
      "section_name": "Tsm-",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Vi. Application In Emotion Analysis",
      "text": "Emotion analysis is a salient direction in human-centered applications. Existing methodologies are mainly focus on facial expression recognition  [64] ,  [65] ,  [55] ,  [66] . However, these approaches are often hindered by various limiting factors such as low resolution, suboptimal illumination, restricted viewing angles, and even privacy protection. Recent efforts in micro-action recognition  [54] ,  [17] ,  [32]  have demonstrated that precise capture and interpretation of micro-action priors can offer advantageous insights for emotion analysis. Despite these advances, the achievements in micro-action recognition remain remarkably and lag significantly behind developments in psychological theories. To assess the practicality of the new data source and methodology introduced in this study, we apply them to the emotion analysis application.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "A. Data Preparation And Methodology",
      "text": "We extend the MA-52-Pro dataset specifically for emotion analysis, where each instance is sequentially annotated with an emotion label followed by a micro-action label. In this study, we collect two micro-action datasets, namely the Microaction 52 dataset (MA-52) for fine-grained and coarse-grained micro-action recognition, and extend an MA-52-Pro dataset for multi-label micro-action recognition and emotion recognition. Our observation shows that shifts in human emotion are frequently accompanied by the simultaneous occurrence of multiple micro-actions. Visualization samples of MA-52-Pro dataset for multi-label micro-action recognition and emotion recognition can be obsevered in Figure  9 .\n\nData Preparation. We collect video instances that capture participants' emotion states to construct the MA-52-Pro dataset. The dataset consists of 7,818 instances containing 29,503 micro-action labels, spanning a total duration of 3,876 minutes. The emotion annotations are based on holistic participant data, including actions, expressions, speech, and tone. Each video instance manifests between 1 and 15 micro-actions with durations ranging from 5s to beyond 100s. Compared to existing bodily emotion datasets  [17] ,  [32] , MA-52-Pro offers advantages such as high resolution data, diverse subjects, spontaneous emotional responses, and comprehensive annotations as well as MA-52. In contrast to iMiGUE  [17]  and SMG  [32] , which categorize emotions dichotomously (i.e., positive/negative and relaxed/stressed), the present dataset includes five emotional categories (i.e., joy, sadness, surprise, fear, and anger). Method Pipeline. As depicted in Figure  7 , we use the developed MANet as a basic module and introduce a unified MANet-based framework to simultaneously manage facial and micro-action branches for emotion recognition. In the facial branch, a pre-trained face detector  [67]  is used to detect and crop facial regions and subsequently obtain facial features. After encoding by MANet, we obtain the final facial feature X f ace ∈ R T ×d . For the micro-action branch, the video input is processed by MANet. Since the extended MA-52-Pro dataset considers the simultaneous occurrence of multiple microactions, a two-layer transformer encoder  [68]  is incorporated to detect long-term temporal associations among micro-actions. Ultimately, we obtain the micro-action features X act ∈ R T ×d . X act is averaged along the timeline and then mapped to the FC-based micro-action classifier for multi-label micro-action prediction. The combination of [X f ace , X act ] is fed into the FC-based emotion classifier. The two classifiers are formulated as follows:\n\nwhere N F A and N E C denote the number of micro-action categories and emotion categories, respectively.\n\nMethod Optimization. We leverage both micro-action and emotion indicators to construct two distinct optimization objectives, i.e., emotion optimization and multilabel micro-action optimization. The emotional loss L emo denotes a normal cross-entropy loss designed for emotion prediction, and the actionness loss L act represents a binary cross-entropy loss designed for multilabel micro-action prediction. The optimization is formulated as L = L act + βL emo , where β indicates a hyperparameter to balance the two losses. β is empirically set to 0.03.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "B. Experiment Analysis Of Application",
      "text": "Comparative analyses are conducted between our approach and emotion recognition methods  [69] ,  [70] . Additionally, we perform ablation studies that investigate both facial and microaction branches. The experimental results are detailed in Table  VI . Metrics such as Acc, F1 weight  [71] , UF1  [72] ,  [73]  and Ideally, the confusion matrices in Figure  10  would prominently highlight the co-occurrence phenomena of multiple actions.\n\nThe proposed method successfully learns to identify such cooccurrences from the head, and upper limb (i.e., labels B and C in Figure  10 (a) ). However, it still encounters challenges in detecting co-occurrences involving full-body, lower limbs, and head-hand actions (i.e., coarse-grained labels A, D and F in Figure  10 (a) ).\n\nQualitative Visualization. Figure  8  provides visual exam- ples from the Face-only branch, the Micro-action-only branch, and the full model. Two illustrative cases show the limitations of relying solely on facial expressions to distinguish between \"anger\" and \"surprise\" or \"joy\" and \"sadness.\" By utilizing micro-action cues, MAENet accurately predicts the correct emotion categories, underlining the robustness of the model. Figure  9  showcases four examples of both micro-action and emotion recognition. Our method performs commendably well in the area of emotion recognition. In terms of multi-label micro-action recognition, it particularly excels in handling video instances characterized by a limited number of microactions, as displayed in Figures  9 (a ) and (b). However, predicting multiple simultaneous micro-actions in extended videos or large sets of actions remains a challenging task, as shown in Figures  9 (c ) and (d). Nevertheless, our method can identify key micro-actions and successfully predict the correct emotion category. Necessity of Lower Limb Labels for Emotion Recognition. We further conduct the experiment on the MA-52-Pro dataset to verify the effectiveness of lower limb micro-actions for emotion recognition. As shown in Table  VII , we report the statistics of the actions of each part in the MA-52-Pro dataset. There are 29,503 action labels under 7 coarse-grained labels including Body (A), Head (B), Upper limb (C), Lower limb (D), Body-hand (E), Head-hand (F), and Leg-hand (G). In fact, the lower limb part contains both \"Lower limb (D)\" and \"Leghand (G)\" micro-actions in our MA-52-Pro dataset, reaching 5,379 labels and 18.23% of the total. Figure  11 (a)  shows that lower limb micro-actions account for a non-negligible proportion of micro-actions in the five categories of emotions, reaching 20% for Joy, 16% for Sadness, 18% for Surprise, 16% for Fear, and 13% for Anger. We then carry out an ablation study of the model that does not use the lower limb part of the micro-actions for emotion recognition. From the experimental results in Table VII, the model without the lower limb part shows performance drops in Acc Avg (1.73%↓), F1 weight (1.59%↓), and UAR (1.23%↓). In addition, we find that the mAP index shows an increase after removing lower limb micro-actions, indicating that the recognition of lower limb micro-actions is still a challenge.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Vii. Further Research Direction",
      "text": "MAR is emerging as a research area of great interest today, which has shown great potential in a variety of application scenarios. Here, we provide an in-depth discussion on the further research directions of micro-action recognition: 1) Composite MAR: In the real world, people's behaviors and actions often involve multiple co-occurring actions, not just",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Viii. Conclusion",
      "text": "To make progress in this particular field of research, we explore video-based human micro-action recognition and compile an expansive, class-rich, and whole-body Micro-Action-52 (MA-52) dataset. This study scrutinizes prevalent generic action recognition methods and introduces a CNN network architecture named MANet. The method integrates SE with the TSM into the established ResNet network to capture nuanced spatio-temporal variations within videos. Experimental results demonstrate both the reliability of the assembled dataset and the effectiveness of the proposed method. This study employs the proposed micro-action recognition method for emotion recognition task, performing multi-task learning for multi-label micro-action recognition and emotion recognition tasks, which verifies the proficiency of MANet in emotion analysis applications. Micro-action recognition serves as a highly practical technology that provides numerous opportunities in various real-world applications, thus opening up various avenues for future academic exploration.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Data collection procedure and samples of our micro-action dataset. We",
      "page": 1
    },
    {
      "caption": "Figure 2: Overall, the MA-52 dataset",
      "page": 2
    },
    {
      "caption": "Figure 2: (a) Details of body part (coarse-grained) and micro-action (fine-grained) labels. (b) Data distribution over the gender and age of respondents, data",
      "page": 5
    },
    {
      "caption": "Figure 3: The architecture of the Micro-action Network (MANet). The core architecture of the MANet integrates the squeeze-and-excitation (SE) [58] and",
      "page": 6
    },
    {
      "caption": "Figure 3: We adopt the ResNet architecture as the back-",
      "page": 6
    },
    {
      "caption": "Figure 5: shows these features categorized",
      "page": 8
    },
    {
      "caption": "Figure 5: (b), samples with micro-action labels “B1” and",
      "page": 8
    },
    {
      "caption": "Figure 5: (a) presents a more amalgamated distribution of micro-action",
      "page": 8
    },
    {
      "caption": "Figure 4: The prediction results of six examples for micro-action recognition examples on the MA-52 dataset are displayed, and the line graphs show the",
      "page": 9
    },
    {
      "caption": "Figure 4: (e) reveals that MANet accurately predicts the micro-actions “hands touching fingers” in conjunction with the body part interaction of",
      "page": 9
    },
    {
      "caption": "Figure 5: t-SNE [63] results of coarse- and fine-grained features on the test",
      "page": 9
    },
    {
      "caption": "Figure 6: presents the confusion matrices for the predictions of MANet",
      "page": 9
    },
    {
      "caption": "Figure 6: Predicted confusion matrices of fine- and coarse-grained categories of",
      "page": 9
    },
    {
      "caption": "Figure 4: presents six comparative",
      "page": 9
    },
    {
      "caption": "Figure 4: (a), and the distinction",
      "page": 9
    },
    {
      "caption": "Figure 4: (b). In Figure 4",
      "page": 9
    },
    {
      "caption": "Figure 4: (d) demonstrates that MANet provides",
      "page": 9
    },
    {
      "caption": "Figure 4: (f) illustrates the",
      "page": 9
    },
    {
      "caption": "Figure 7: MANet-based network architecture for emotion recognition. Both",
      "page": 10
    },
    {
      "caption": "Figure 9: Data Preparation. We collect video instances that cap-",
      "page": 10
    },
    {
      "caption": "Figure 7: , we use the",
      "page": 10
    },
    {
      "caption": "Figure 8: Prediction results of the only Face branch, the only Micro-action branch, and the full model from the MA-52-Pro dataset. Histograms depict the",
      "page": 11
    },
    {
      "caption": "Figure 9: Micro-action and emotion prediction results of four samples from the MA-52-Pro dataset. We list the predicted micro-action results and the ground-truth",
      "page": 11
    },
    {
      "caption": "Figure 10: Predicted confusion matrices of fine-grained and coarse-grained",
      "page": 11
    },
    {
      "caption": "Figure 10: a), whereas it is still challenging to detect",
      "page": 11
    },
    {
      "caption": "Figure 10: would prominently",
      "page": 11
    },
    {
      "caption": "Figure 10: (a)). However, it still encounters challenges",
      "page": 11
    },
    {
      "caption": "Figure 8: provides visual exam-",
      "page": 11
    },
    {
      "caption": "Figure 9: showcases four examples of both micro-action and",
      "page": 12
    },
    {
      "caption": "Figure 11: (a) shows",
      "page": 12
    },
    {
      "caption": "Figure 11: Percentage of micro-action labels in different emotion categories. (a)",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Datasets": "ActivityNet\n[27]\nHACS [34]\nFineAction [23]\nDiving48 [31]\nFineGym [11]\nBasketball\n[24]\nMPII-cooking [35]\nEPIC-KITCHENS [36]",
          "Venue": "CVPR’15\nICCV’19\nTIP’22\nECCV’18\nCVPR’20\nICASSP’20\nCVPR’12\nECCV’18",
          "Resolution\nCategory\nInstance\nDuration (s)\nParticipants": "1280 × 720\n200\n23,064\n49.2\n-\n-\n200\n122,304\n33.2\n-\n-\n106\n103,324\n7.1\n-\n-\n48\n18,404\n5.3\n-\n1920 × 1080\n530\n32,697\n1.7\n-\n-\n26\n3,399\n-\n-\n1624 × 1224\n65\n5,609\n11.1 m\n12\n1920 × 1080\n149\n39,596\n4.9\n32",
          "Source\nCovered Body Area\nAction type": "Web\nWhole Body\nDaily events\nWeb\nWhole Body\nDaily events\nWeb\nWhole Body\nDaily events\nWeb\nWhole Body\nSports\nWeb\nWhole Body\nSports\nWeb\nWhole Body\nSports\nKitchen\nUpper Body\nCooking\nKitchen\nHands\nCooking"
        },
        {
          "Datasets": "BBSI\n[33]",
          "Venue": "MM’22",
          "Resolution\nCategory\nInstance\nDuration (s)\nParticipants": "-\n15\n7,905\n-\n78",
          "Source\nCovered Body Area\nAction type": "Group Conversation\nWhole Body (Sit)\nSocial Behavior"
        },
        {
          "Datasets": "iMiGUE [17]\nSMG [32]\nMA-52 (Ours)",
          "Venue": "CVPR’21\nIJCV’23\n-",
          "Resolution\nCategory\nInstance\nDuration (s)\nParticipants": "1280 × 720\n32\n18,499\n2.6\n72\n1920 × 1080\n16\n3,712\n1.8\n40\n1920 × 1080\n52\n22,422\n1.9\n205",
          "Source\nCovered Body Area\nAction type": "Web\nAbove Chest\n(Sit)\nMicro-gesture\nInterview\nWhole Body (Stand)\nMicro-gesture\nInterview\nWhole Body (Sit)\nMicro-action"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Body (A)": "A1: shaking body",
          "Head (B)": "B1: nodding",
          "Upper limb (C)": "C1: illustrative gestures"
        },
        {
          "Body (A)": "",
          "Head (B)": "B2: shaking head",
          "Upper limb (C)": "C2: other finger movements"
        },
        {
          "Body (A)": "A2: turning around",
          "Head (B)": "",
          "Upper limb (C)": ""
        },
        {
          "Body (A)": "",
          "Head (B)": "",
          "Upper limb (C)": ""
        },
        {
          "Body (A)": "",
          "Head (B)": "B3: turning head",
          "Upper limb (C)": "C3: hands touching fingers"
        },
        {
          "Body (A)": "A3: sitting straightly",
          "Head (B)": "",
          "Upper limb (C)": ""
        },
        {
          "Body (A)": "",
          "Head (B)": "",
          "Upper limb (C)": ""
        },
        {
          "Body (A)": "",
          "Head (B)": "B4: tilting head",
          "Upper limb (C)": "C4: stretching arms"
        },
        {
          "Body (A)": "A4: shrugging",
          "Head (B)": "",
          "Upper limb (C)": ""
        },
        {
          "Body (A)": "",
          "Head (B)": "B5: bowing head",
          "Upper limb (C)": "C5: waving"
        },
        {
          "Body (A)": "",
          "Head (B)": "",
          "Upper limb (C)": ""
        },
        {
          "Body (A)": "A5: rising up",
          "Head (B)": "",
          "Upper limb (C)": ""
        },
        {
          "Body (A)": "",
          "Head (B)": "B6: head up",
          "Upper limb (C)": "C6: scratching arms"
        },
        {
          "Body (A)": "",
          "Head (B)": "",
          "Upper limb (C)": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Lower limb(D)": "D1: tiptoe\nD5: closing legs",
          "Body-hand (E)": "E1: scratching or touching chest",
          "Head-hand (F)": "F1: touching nose\nF6: rubbing eyes",
          "Leg-hand (G)": "G1: touching legs"
        },
        {
          "Lower limb(D)": "",
          "Body-hand (E)": "E2: scratching or touching neck",
          "Head-hand (F)": "F7: scratching or touching forehead\nF2: scratching or touching face",
          "Leg-hand (G)": ""
        },
        {
          "Lower limb(D)": "D2: retracting feet\nD6: spread legs",
          "Body-hand (E)": "",
          "Head-hand (F)": "",
          "Leg-hand (G)": "G2: patting legs"
        },
        {
          "Lower limb(D)": "",
          "Body-hand (E)": "E3: scratching or touching back",
          "Head-hand (F)": "",
          "Leg-hand (G)": ""
        },
        {
          "Lower limb(D)": "",
          "Body-hand (E)": "",
          "Head-hand (F)": "F3: playing or tidying hair\nF8: touching ears",
          "Leg-hand (G)": ""
        },
        {
          "Lower limb(D)": "D3: shaking legs\nD7: curling legs",
          "Body-hand (E)": "E4: arms akimbo",
          "Head-hand (F)": "",
          "Leg-hand (G)": "G3: scratching legs"
        },
        {
          "Lower limb(D)": "",
          "Body-hand (E)": "",
          "Head-hand (F)": "F4: scratching or touching hindbrain\nF9: covering mouth",
          "Leg-hand (G)": ""
        },
        {
          "Lower limb(D)": "",
          "Body-hand (E)": "E5: crossing arms",
          "Head-hand (F)": "",
          "Leg-hand (G)": ""
        },
        {
          "Lower limb(D)": "D4: stretching feet\nD8: crossing legs",
          "Body-hand (E)": "",
          "Head-hand (F)": "",
          "Leg-hand (G)": "G4: scratching feet"
        },
        {
          "Lower limb(D)": "",
          "Body-hand (E)": "E6: scratching or touching shoulder",
          "Head-hand (F)": "F5: pushing glasses\nF10: covering face",
          "Leg-hand (G)": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Action Label": "f ine, coarse",
          "Metric": "F1mean",
          "TSN[37]\nTSM[38]\nTIN[39]": "43.67\n61.39\n58.22",
          "C3D[40]\nI3D[41]\nSlowFast[42]": "58.43\n61.66\n63.09",
          "VSwinT[43]\nTimeSF[44]\nUniF[45]": "61.24\n51.53\n64.43",
          "MANet\n(Ours)": "65.59"
        },
        {
          "Action Label": "coarse\nf ine",
          "Metric": "F1macro\nF1macro",
          "TSN[37]\nTSM[38]\nTIN[39]": "52.50\n70.98\n66.99\n28.52\n40.19\n39.82",
          "C3D[40]\nI3D[41]\nSlowFast[42]": "66.60\n71.56\n70.61\n40.86\n39.84\n44.96",
          "VSwinT[43]\nTimeSF[44]\nUniF[45]": "71.25\n61.90\n71.80\n38.53\n34.38\n48.01",
          "MANet\n(Ours)": "72.87\n49.22"
        },
        {
          "Action Label": "coarse\nf ine",
          "Metric": "F1micro\nF1micro",
          "TSN[37]\nTSM[38]\nTIN[39]": "59.22\n77.64\n73.26\n34.46\n56.75\n52.81",
          "C3D[40]\nI3D[41]\nSlowFast[42]": "74.04\n78.16\n77.18\n52.22\n57.07\n59.60",
          "VSwinT[43]\nTimeSF[44]\nUniF[45]": "79.03\n77.95\n69.17\n57.23\n40.67\n58.89",
          "MANet\n(Ours)": "78.95\n61.33"
        },
        {
          "Action Label": "coarse\nf ine\nf ine",
          "Metric": "Acc-Top1\nAcc-Top1\nAcc-Top5",
          "TSN[37]\nTSM[38]\nTIN[39]": "59.22\n77.64\n73.26\n34.46\n56.75\n52.81\n73.34\n87.47\n85.37",
          "C3D[40]\nI3D[41]\nSlowFast[42]": "74.04\n78.16\n77.18\n52.22\n57.07\n59.60\n86.97\n88.67\n88.54",
          "VSwinT[43]\nTimeSF[44]\nUniF[45]": "79.03\n77.95\n69.17\n57.23\n40.67\n58.89\n87.99\n82.62\n87.29",
          "MANet\n(Ours)": "78.95\n61.33\n88.83"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Action Label": "f ine, coarse\ncoarse\nf ine",
          "Metric": "F1mean\nF1macro\nF1macro",
          "α=0\nα=0.1\nα=1\nα=2\nα=5\nα=10": "65.59\n63.55\n63.65\n63.77\n62.51\n64.11\n72.87\n70.76\n70.70\n71.02\n71.73\n72.18\n49.22\n47.97\n48.38\n48.10\n47.56\n47.16"
        },
        {
          "Action Label": "coarse\nf ine",
          "Metric": "F1micro\nF1micro",
          "α=0\nα=0.1\nα=1\nα=2\nα=5\nα=10": "78.95\n76.94\n77.16\n77.72\n78.12\n78.31\n61.33\n58.54\n58.80\n58.50\n58.96\n59.71"
        },
        {
          "Action Label": "coarse\nf ine\nf ine",
          "Metric": "Acc-Top1\nAcc-Top1\nAcc-Top5",
          "α=0\nα=0.1\nα=1\nα=2\nα=5\nα=10": "78.95\n76.94\n77.17\n77.72\n78.12\n78.31\n61.33\n58.54\n58.80\n58.50\n58.96\n59.71\n89.12\n88.36\n88.47\n88.31\n88.83\n86.74"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Action Label": "f ine, coarse",
          "Metric": "F1mean",
          "Baseline\nw/o SE\nw/o emb\nMANet": "65.59\n61.39\n64.77\n63.55"
        },
        {
          "Action Label": "coarse\nf ine",
          "Metric": "F1macro\nF1macro",
          "Baseline\nw/o SE\nw/o emb\nMANet": "72.87\n70.98\n72.22\n70.76\n49.22\n40.19\n48.07\n47.97"
        },
        {
          "Action Label": "coarse\nf ine",
          "Metric": "F1micro\nF1micro",
          "Baseline\nw/o SE\nw/o emb\nMANet": "78.95\n77.64\n78.48\n76.94\n61.33\n56.75\n60.31\n58.54"
        },
        {
          "Action Label": "coarse\nf ine\nf ine",
          "Metric": "Acc-Top1\nAcc-Top1\nAcc-Top5",
          "Baseline\nw/o SE\nw/o emb\nMANet": "78.95\n77.64\n78.48\n76.94\n61.33\n56.75\n60.31\n58.54\n89.12\n87.47\n89.01\n88.83"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(a)\nTSM\nGT\nMANet\nHead\n100%\n100%\nB4: tilting head\n90%\n75%\nB3: turning head\n50%\n25%\n…\n0%\n0%\n…\n…\nFine-grained prediction (A1-G4) \nCoarse-grained prediction (A-G)": "(c)\nTSM\nGT\nMANet\n100%\n100%\nhead-hand\nF1: touching nose\n75%\n90%\nF2: scratching or \n50%\ntouching face\n25%\n…\n0%\n…\n0%\n…\nCoarse-grained prediction (A-G) \nFine-grained prediction (A1-G4)",
          "(b)\nTSM\nGT\nMANet\n100%\nD4: stretching feet\nlower limb\n100%\n75%\n90%\n50%\nD6: spread legs\n25%\n…\n0%\n…\n0%\nCoarse-grained prediction (A-G) \nFine-grained prediction (A1-G4)": "(d)\nTSM\nGT\nMANet\n100%\n100%\nbody-hand\nE2: scratching or \n75%\n90%\ntouching neck\n50%\nE6: scratching or \n…\n25%\ntouching shoulder\n…\n0%\n0%\nCoarse-grained prediction (A-G) \nFine-grained prediction (A1-G4)"
        },
        {
          "(a)\nTSM\nGT\nMANet\nHead\n100%\n100%\nB4: tilting head\n90%\n75%\nB3: turning head\n50%\n25%\n…\n0%\n0%\n…\n…\nFine-grained prediction (A1-G4) \nCoarse-grained prediction (A-G)": "(e)\nTSM\nGT\nMANet\n100%\n100%\nD8: crossing legs\n75%\nlower limb\n90%\nC3: hands touching fingers\n50%\nupper limb\n25%\n…\n0%\n0%\n…\nCoarse-grained prediction (A-G) \nFine-grained prediction (A1-G4)",
          "(b)\nTSM\nGT\nMANet\n100%\nD4: stretching feet\nlower limb\n100%\n75%\n90%\n50%\nD6: spread legs\n25%\n…\n0%\n…\n0%\nCoarse-grained prediction (A-G) \nFine-grained prediction (A1-G4)": "(f)\nTSM\nGT\nMANet\n100%\n100%\nG1: touching legs\n75%\nleg-hand\n50%\nupper limb\n90%\nG2: patting legs\n…\n25%\n0%\n…\n0%\nCoarse-grained prediction (A-G) \nFine-grained prediction (A1-G4)"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "VAANet\n[69]\nEmotion-FAN [70]",
          "Acc\nUF1\nUAR\nF1weight": "54.63\n48.43\n32.27\n32.06\n56.81\n53.89\n37.42\n36.71",
          "mAP": "-\n-"
        },
        {
          "Method": "Face Branch\nMicro-Action Branch\nFull model",
          "Acc\nUF1\nUAR\nF1weight": "60.01\n55.23\n38.38\n38.18\n58.86\n55.23\n40.30\n41.38\n62.38\n58.49\n41.75\n42.28",
          "mAP": "5.51\n15.35\n15.40"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Face\nAction\nJoy\nSadness\nSurprise\nFear\nAnger\n(a)\n100\n100\n100\nAnger\nAnger\nGT\n75\n75\n75\nGT\n63.99\n58.27\nSurprise Anger\n50\n50\n50\n32.59\n28.33\nGT\n19.80\n25\n25\n25\n16.93\n15.58\n13.59\n13.22\n9.92\n7.82 6.84\n5.8\n4.42\n2.91\n0\n0\n0\nOnly Face\nOnly Micro-action\nFace & Action": "Face\nAction\n(b)\n75\n75\n75\nSadness\nSadness\nSadness\nGT\nGT\nJoy\n47.04\nGT\n46.28\n50\n50\n50\n40.32 37.86\n32.22\n32.08\n25\n25\n25\n17.34\n15.92\n7.37 7.71 6.74\n2.58 1.73\n2.40 1.73\n0\n0\n0\nOnly Face\nOnly Micro-action\nFace & Action"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Joy\n(a)\nGT\n100\n84.3\n75\n50\n25\n6.3\n4.5\n3.9\n1.0\nAction Labels: shaking body, tilting head, nodding, touching legs\n0\nPrediction Results: shaking body, tilting head, nodding, touching legs (4/4)\nEmotion Distribution": "Sadness\n(c)\nGT\n60\n50.8\n40\n18.1\n20\nsitting\nstraightly,\nturning\nhead,\nnodding,\nshaking\nhead,\nAction\nLabels:\n12.6 9.7\n8.7\nstretching arms, hands touching fingers, other finger movements, patting legs,\ntouching legs\n0\nPrediction Results: nodding, stretching arms, patting legs, touching legs (4/9)\nEmotion Distribution",
          "(b)\nSurprise\n100\nGT\n75\n58.7\n50\n31.6\n25\nbowing\nhead,\nhead\nup,\nturning\nhead,\nhands\ntouching\nAction\nLabels:\n6.5\n1.9\n1.3\nfingers\n0\nPrediction Results: bowing head, head up, hands touching fingers (3/4)\nEmotion Distribution": "Fear\n(d)\nGT\n100\n95.8\n75\n50\n25\ntilting head,\nturning head, nodding,\nshaking head, patting\nAction Labels:\n2.4\n0.8\n0.6\n0.4\n0\nlegs, touching legs\nEmotion Distribution\nPrediction Results: nodding, hands touching fingers (1/6)"
        }
      ],
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Scl 90",
      "authors": [
        "L Derogatis",
        "R Lipman",
        "L Covi"
      ],
      "year": "2004",
      "venue": "GROUP"
    },
    {
      "citation_id": "2",
      "title": "Analyze spontaneous gestures for emotional stress state recognition: A micro-gesture dataset and analysis with deep learning",
      "authors": [
        "H Chen",
        "X Liu",
        "X Li",
        "H Shi",
        "G Zhao"
      ],
      "year": "2019",
      "venue": "Proceedings of IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "3",
      "title": "Drivermhg: A multi-modal dataset for dynamic recognition of driver micro hand gestures and a real-time recognition framework",
      "authors": [
        "O Köpüklü",
        "T Ledwon",
        "Y Rong",
        "N Kose",
        "G Rigoll"
      ],
      "year": "2020",
      "venue": "Proceedings of IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "4",
      "title": "Spatiotemporal knowledge distillation for efficient estimation of aerial video saliency",
      "authors": [
        "J Li",
        "K Fu",
        "S Zhao",
        "S Ge"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "5",
      "title": "Detecting deepfake videos with temporal dropout 3dcnn",
      "authors": [
        "D Zhang",
        "C Li",
        "F Lin",
        "D Zeng",
        "S Ge"
      ],
      "year": "2021",
      "venue": "Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "6",
      "title": "Recognizing micro-actions and reactions from paired egocentric videos",
      "authors": [
        "R Yonetani",
        "K Kitani",
        "Y Sato"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "7",
      "title": "Recognizing micro actions in videos: learning motion details via segment-level temporal pyramid",
      "authors": [
        "Y Mi",
        "S Wang"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Multimedia and Expo"
    },
    {
      "citation_id": "8",
      "title": "Fine-grained action recognition using dynamic kernels",
      "authors": [
        "S Yenduri",
        "N Perveen",
        "V Chalavadi"
      ],
      "year": "2022",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "9",
      "title": "Detecting masked faces in the wild with lle-cnns",
      "authors": [
        "S Ge",
        "J Li",
        "Q Ye",
        "Z Luo"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "10",
      "title": "Low-resolution face recognition in the wild via selective knowledge distillation",
      "authors": [
        "S Ge",
        "S Zhao",
        "C Li",
        "J Li"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "11",
      "title": "Finegym: A hierarchical video dataset for fine-grained action understanding",
      "authors": [
        "D Shao",
        "Y Zhao",
        "B Dai",
        "D Lin"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "12",
      "title": "",
      "authors": [
        "A Jourabloo",
        "F De La Torre",
        "J Saragih",
        "S.-E Wei",
        "S Lombardi"
      ],
      "venue": ""
    },
    {
      "citation_id": "13",
      "title": "Robust egocentric photorealistic facial expression transfer for virtual reality",
      "authors": [
        "D Wang",
        "A Belko",
        "H Trimble",
        "Badino"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "14",
      "title": "Survey on emotional body gesture recognition",
      "authors": [
        "F Noroozi",
        "C Corneanu",
        "D Kamińska",
        "T Sapiński",
        "S Escalera",
        "G Anbarjafari"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "15",
      "title": "Hierarchical lstm for sign language translation",
      "authors": [
        "D Guo",
        "W Zhou",
        "H Li",
        "M Wang"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "16",
      "title": "Body cues, not facial expressions, discriminate between intense positive and negative emotions",
      "authors": [
        "H Aviezer",
        "Y Trope",
        "A Todorov"
      ],
      "year": "2012",
      "venue": "Science"
    },
    {
      "citation_id": "17",
      "title": "Emotion recognition from gait analyses: Current research and future directions",
      "authors": [
        "S Xu",
        "J Fang",
        "X Hu",
        "E Ngai",
        "W Wang",
        "Y Guo",
        "V Leung"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "18",
      "title": "imigue: An identity-free video dataset for micro-gesture understanding and emotion analysis",
      "authors": [
        "X Liu",
        "H Shi",
        "H Chen",
        "Z Yu",
        "X Li",
        "G Zhao"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "19",
      "title": "Ucf101: A dataset of 101 human actions classes from videos in the wild",
      "authors": [
        "K Soomro",
        "A Zamir",
        "M Shah"
      ],
      "year": "2012",
      "venue": "Ucf101: A dataset of 101 human actions classes from videos in the wild",
      "arxiv": "arXiv:1212.0402"
    },
    {
      "citation_id": "20",
      "title": "The kinetics human action video dataset",
      "authors": [
        "W Kay",
        "J Carreira",
        "K Simonyan",
        "B Zhang",
        "C Hillier",
        "S Vijayanarasimhan",
        "F Viola",
        "T Green",
        "T Back",
        "P Natsev"
      ],
      "year": "2017",
      "venue": "The kinetics human action video dataset",
      "arxiv": "arXiv:1705.06950"
    },
    {
      "citation_id": "21",
      "title": "Large-scale video classification with convolutional neural networks",
      "authors": [
        "A Karpathy",
        "G Toderici",
        "S Shetty",
        "T Leung",
        "R Sukthankar",
        "L Fei-Fei"
      ],
      "year": "2014",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "22",
      "title": "Dense semanticsassisted networks for video action recognition",
      "authors": [
        "H Luo",
        "G Lin",
        "Y Yao",
        "Z Tang",
        "Q Wu",
        "X Hua"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "23",
      "title": "Spatiotemporal multimodal learning with 3d cnns for video action recognition",
      "authors": [
        "H Wu",
        "X Ma",
        "Y Li"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "24",
      "title": "Fineaction: A finegrained video dataset for temporal action localization",
      "authors": [
        "Y Liu",
        "L Wang",
        "Y Wang",
        "X Ma",
        "Y Qiao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "25",
      "title": "Fine-grained action recognition on a novel basketball dataset",
      "authors": [
        "X Gu",
        "X Xue",
        "F Wang"
      ],
      "year": "2020",
      "venue": "Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "26",
      "title": "Dense temporal convolution network for sign language translation",
      "authors": [
        "D Guo",
        "S Wang",
        "Q Tian",
        "M Wang"
      ],
      "year": "2019",
      "venue": "Proceedings of the 28th International Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "27",
      "title": "Hmdb: a large video database for human motion recognition",
      "authors": [
        "H Kuehne",
        "H Jhuang",
        "E Garrote",
        "T Poggio",
        "T Serre"
      ],
      "year": "2011",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "28",
      "title": "Activitynet: A large-scale video benchmark for human activity understanding",
      "authors": [
        "F Heilbron",
        "V Escorcia",
        "B Ghanem",
        "J Niebles"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "29",
      "title": "Proposal-free video grounding with contextual pyramid network",
      "authors": [
        "K Li",
        "D Guo",
        "M Wang"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "30",
      "title": "Learning semantic-aware spatial-temporal attention for interpretable action recognition",
      "authors": [
        "J Fu",
        "J Gao",
        "C Xu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "31",
      "title": "Vigt: proposal-free video grounding with a learnable token in the transformer",
      "authors": [
        "K Li",
        "D Guo",
        "M Wang"
      ],
      "year": "2023",
      "venue": "Science China Information Sciences"
    },
    {
      "citation_id": "32",
      "title": "Resound: Towards action recognition without representation bias",
      "authors": [
        "Y Li",
        "Y Li",
        "N Vasconcelos"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision"
    },
    {
      "citation_id": "33",
      "title": "Smg: A micro-gesture dataset towards spontaneous body gestures for emotional stress state analysis",
      "authors": [
        "H Chen",
        "H Shi",
        "X Liu",
        "X Li",
        "G Zhao"
      ],
      "year": "2023",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "34",
      "title": "Bodily behaviors in social interaction: Novel annotations and state-of-the-art evaluation",
      "authors": [
        "M Balazia",
        "P Müller",
        "Á Tánczos",
        "A Liechtenstein",
        "F Bremond"
      ],
      "year": "2022",
      "venue": "Proceedings of the ACM International Conference on Multimedia"
    },
    {
      "citation_id": "35",
      "title": "Hacs: Human action clips and segments dataset for recognition and temporal localization",
      "authors": [
        "H Zhao",
        "A Torralba",
        "L Torresani",
        "Z Yan"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "36",
      "title": "A database for fine grained activity detection of cooking activities",
      "authors": [
        "M Rohrbach",
        "S Amin",
        "M Andriluka",
        "B Schiele"
      ],
      "year": "2012",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "37",
      "title": "Scaling egocentric vision: The epic-kitchens dataset",
      "authors": [
        "D Damen",
        "H Doughty",
        "G Farinella",
        "S Fidler",
        "A Furnari",
        "E Kazakos",
        "D Moltisanti",
        "J Munro",
        "T Perrett",
        "W Price"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision"
    },
    {
      "citation_id": "38",
      "title": "Temporal segment networks for action recognition in videos",
      "authors": [
        "L Wang",
        "Y Xiong",
        "Z Wang",
        "Y Qiao",
        "D Lin",
        "X Tang",
        "L Van Gool"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "39",
      "title": "Tsm: Temporal shift module for efficient video understanding",
      "authors": [
        "J Lin",
        "C Gan",
        "S Han"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "40",
      "title": "Temporal interlacing network",
      "authors": [
        "H Shao",
        "S Qian",
        "Y Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "41",
      "title": "Learning spatiotemporal features with 3d convolutional networks",
      "authors": [
        "D Tran",
        "L Bourdev",
        "R Fergus",
        "L Torresani",
        "M Paluri"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "42",
      "title": "Quo vadis, action recognition? a new model and the kinetics dataset",
      "authors": [
        "J Carreira",
        "A Zisserman"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "43",
      "title": "Slowfast networks for video recognition",
      "authors": [
        "C Feichtenhofer",
        "H Fan",
        "J Malik",
        "K He"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "44",
      "title": "Video swin transformer",
      "authors": [
        "Z Liu",
        "J Ning",
        "Y Cao",
        "Y Wei",
        "Z Zhang",
        "S Lin",
        "H Hu"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "45",
      "title": "Is space-time attention all you need for video understanding",
      "authors": [
        "G Bertasius",
        "H Wang",
        "L Torresani"
      ],
      "year": "2021",
      "venue": "Proceedings of International Conference on Machine Learning"
    },
    {
      "citation_id": "46",
      "title": "Uniformer: Unified transformer for efficient spatiotemporal representation learning",
      "authors": [
        "K Li",
        "Y Wang",
        "P Gao",
        "G Song",
        "Y Liu",
        "H Li",
        "Y Qiao"
      ],
      "year": "2022",
      "venue": "Uniformer: Unified transformer for efficient spatiotemporal representation learning",
      "arxiv": "arXiv:2201.04676"
    },
    {
      "citation_id": "47",
      "title": "Regional attention network (ran) for head pose and finegrained gesture recognition",
      "authors": [
        "A Behera",
        "Z Wharton",
        "Y Liu",
        "M Ghahremani",
        "S Kumar",
        "N Bessis"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "48",
      "title": "Dynamic spatio-temporal specialization learning for fine-grained action recognition",
      "authors": [
        "T Li",
        "L Foo",
        "Q Ke",
        "H Rahmani",
        "A Wang",
        "J Wang",
        "J Liu"
      ],
      "year": "2022",
      "venue": "Proceedings of the European Conference on Computer Vision"
    },
    {
      "citation_id": "49",
      "title": "Pyramid self-attention polymerization learning for semi-supervised skeleton-based action recognition",
      "authors": [
        "B Xu",
        "X Shu"
      ],
      "year": "2023",
      "venue": "Pyramid self-attention polymerization learning for semi-supervised skeleton-based action recognition",
      "arxiv": "arXiv:2302.02327"
    },
    {
      "citation_id": "50",
      "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
      "authors": [
        "Z Liu",
        "Y Lin",
        "Y Cao",
        "H Hu",
        "Y Wei",
        "Z Zhang",
        "S Lin",
        "B Guo"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "51",
      "title": "Pdan: Pyramid dilated attention network for action detection",
      "authors": [
        "R Dai",
        "S Das",
        "L Minciullo",
        "L Garattoni",
        "G Francesca",
        "F Bremond"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "52",
      "title": "A survey of human-computer interaction (hci) & natural habitsbased behavioural biometric modalities for user recognition schemes",
      "authors": [
        "S Gupta",
        "C Maple",
        "B Crispo",
        "K Raja",
        "A Yautsiukhin",
        "F Martinelli"
      ],
      "year": "2023",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "53",
      "title": "Holoset-a dataset for visualinertial pose estimation in extended reality: Dataset",
      "authors": [
        "Y Chandio",
        "N Bashir",
        "F Anwar"
      ],
      "year": "2022",
      "venue": "Proceedings of the ACM Conference on Embedded Networked Sensor Systems"
    },
    {
      "citation_id": "54",
      "title": "Mimamo net: Integrating microand macro-motion for video emotion recognition",
      "authors": [
        "D Deng",
        "Z Chen",
        "Y Zhou",
        "B Shi"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "55",
      "title": "Arbee: Towards automated recognition of bodily expression of emotion in the wild",
      "authors": [
        "Y Luo",
        "J Ye",
        "R Adams",
        "J Li",
        "M Newman",
        "J Wang"
      ],
      "year": "2020",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "56",
      "title": "Short and long range relation based spatio-temporal transformer for micro-expression recognition",
      "authors": [
        "L Zhang",
        "X Hong",
        "O Arandjelović",
        "G Zhao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "57",
      "title": "Multimodal emotion recognition using deep learning architectures",
      "authors": [
        "H Ranganathan",
        "S Chakraborty",
        "S Panchanathan"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "58",
      "title": "Recognizing emotions from videos by studying facial expressions, body postures and hand gestures",
      "authors": [
        "M Gavrilescu"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE Conference on Telecommunications Forum Telfor"
    },
    {
      "citation_id": "59",
      "title": "Squeeze-and-excitation networks",
      "authors": [
        "J Hu",
        "L Shen",
        "G Sun"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "60",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "61",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "J Pennington",
        "R Socher",
        "C Manning"
      ],
      "year": "2014",
      "venue": "Proceedings of Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "62",
      "title": "Evidential deep learning for open set action recognition",
      "authors": [
        "W Bao",
        "Q Yu",
        "Y Kong"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "63",
      "title": "Skeleton-based action recognition with focusing-diffusion graph convolutional networks",
      "authors": [
        "J Gao",
        "T He",
        "X Zhou",
        "S Ge"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "64",
      "title": "Visualizing data using t-sne",
      "authors": [
        "L Van Der Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "65",
      "title": "Affective processes: stochastic modelling of temporal context for emotion and facial expression recognition",
      "authors": [
        "E Sanchez",
        "M Tellamekala",
        "M Valstar",
        "G Tzimiropoulos"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "66",
      "title": "Context-aware emotion recognition networks",
      "authors": [
        "J Lee",
        "S Kim",
        "S Kim",
        "J Park",
        "K Sohn"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "67",
      "title": "Dbcface: Towards pure convolutional neural network face detection",
      "authors": [
        "X Li",
        "S Lai",
        "X Qian"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "68",
      "title": "Dlib-ml: A machine learning toolkit",
      "authors": [
        "D King"
      ],
      "year": "2009",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "69",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Proceedings of Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "70",
      "title": "An end-to-end visual-audio attention network for emotion recognition in user-generated videos",
      "authors": [
        "S Zhao",
        "Y Ma",
        "Y Gu",
        "J Yang",
        "T Xing",
        "P Xu",
        "R Hu",
        "H Chai",
        "K Keutzer"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "71",
      "title": "Frame attention networks for facial expression recognition in videos",
      "authors": [
        "D Meng",
        "X Peng",
        "K Wang",
        "Y Qiao"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "72",
      "title": "Dialogxl: All-in-one xlnet for multi-party conversation emotion recognition",
      "authors": [
        "W Shen",
        "J Chen",
        "X Quan",
        "Z Xie"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "73",
      "title": "A systematic review on affective computing: emotion models, databases, and recent advances",
      "authors": [
        "Y Wang",
        "W Song",
        "W Tao",
        "A Liotta",
        "D Yang",
        "X Li",
        "S Gao",
        "Y Sun",
        "W Ge",
        "W Zhang",
        "W Zhang"
      ],
      "year": "2022",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "74",
      "title": "Deep learning for micro-expression recognition: A survey",
      "authors": [
        "Y Li",
        "J Wei",
        "Y Liu",
        "J Kauttonen",
        "G Zhao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "75",
      "title": "Block division convolutional network with implicit deep features augmentation for microexpression recognition",
      "authors": [
        "B Chen",
        "K.-H Liu",
        "Y Xu",
        "Q.-Q Wu",
        "J.-F Yao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    }
  ]
}