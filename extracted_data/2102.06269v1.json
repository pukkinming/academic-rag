{
  "paper_id": "2102.06269v1",
  "title": "Disentanglement For Audio-Visual Emotion Recognition Using Multitask Setup",
  "published": "2021-02-11T20:57:37Z",
  "authors": [
    "Raghuveer Peri",
    "Srinivas Parthasarathy",
    "Charles Bradshaw",
    "Shiva Sundaram"
  ],
  "keywords": [
    "Emotion recognition",
    "multimodal learning",
    "disentanglement",
    "multitask learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Deep learning models trained on audio-visual data have been successfully used to achieve state-of-the-art performance for emotion recognition. In particular, models trained with multitask learning have shown additional performance improvements. However, such multitask models entangle information between the tasks, encoding the mutual dependencies present in label distributions in the real world data used for training. This work explores the disentanglement of multimodal signal representations for the primary task of emotion recognition and a secondary person identification task. In particular, we developed a multitask framework to extract low-dimensional embeddings that aim to capture emotion specific information, while containing minimal information related to person identity. We evaluate three different techniques for disentanglement and report results of up to 13% disentanglement while maintaining emotion recognition performance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotions play an important role in human communication. Humans externalize their reactions to surrounding stimuli through a change in the tone of their voice, facial expressions, hand and body gestures. Therefore, automatic emotion recognition is of interest for building natural interfaces and effective human-machine interaction.  [1] . With regards to human communication, emotion is primarily manifested through speech and facial expressions, each providing complementary information  [2] . Therefore, multimodal techniques have been widely used for reliable emotion prediction  [3, 4, 5] .\n\nSeveral studies have shown that emotion recognition benefits from training with secondary related tasks through multitask learning (MTL). In Parthasarathy and Busso  [6] , predicting the continuous affective attributes of valence, arousal and dominance are treated as the multiple tasks and trained jointly. In Li et al.  [7]  and Kim et al.  [8] , gender prediction as a secondary task improves emotion recognition performance by upto 7.7% as measured by weighted accuracy on a standard corpus. A more comprehensive study involving domain, gender and corpus differences was performed in Zhang et al.  [9] , where cross-corpus evaluations showed that, in general, information sharing across tasks yields improvements in performance of emotion recognition across corpora. These studies indicate that several paralinguistic tasks help generalize shared representations that improve overall performance of the primary task. This motivates us to use person identification as a secondary task to help improve performance on the primary emotion task.\n\nWith MTL the shared representations among tasks retain information pertaining to all the tasks. While this generalizes the overall architecture, it does so by entangling information between multiple tasks  [10, 11, 12] . Since most machine learning models are trained on human-annotated, unconstrained real-world data, several factors that should theoretically be independent end up being dependent. For e.g. in the case of emotions, studies have shown the correlation with demographical information  [13] . Therefore, MTL inherently captures the joint dependencies between different factors in the data. This is problematic as the gains through generalization across tasks may lead to bias and subsequently poor performance on unseen data.\n\nTo address the entanglement of information in MTL, this paper develops a multimodal emotion recognition model, improves its performance using person identification as a secondary task and subsequently disentangles the learned person identity information, while still maintaining the improved emotion recognition performance. As an additional contribution, we analyze how much emotion information is present in the identity representations when models are trained in a MTL setup. For disentanglement, we experiment with three distinct disentanglement techniques to minimize the information transfer between speaker embeddings and emotional labels and vice-versa. We present experiments that make use of alternate adversarial training strategy, gradient reversal based technique adapted from Domain Adversarial Training (DAT) literature and a confusion loss based technique inspired from  [14] . We evaluate the models pre and post disentaglement, showing that disentanglement retains or improves performance on primary tasks upto 2% absolute, while reducing the leakage of information between the tasks with disentanglement upto 13% as measured by F-score.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "In the context of representation learning for emotion recognition, the goal is to extract low dimensional embeddings that are invariant to factors such as domain and speaker. Abdelwahab and Busso  [15]  used gradient reversal (GR) to extract emotion representations that are invariant to domain. Mao et al.  [16]  imposed an explicit orthogonality criterion to encourage the learning of domain invariant and emotion discriminative features. Similarly, to extract speakerinvariant emotion representations, adversarial learning approach was explored in addition to an online data augmentation technique by Tu et al.  [17] . They showed improvements in the emotion recognition performance while testing on speakers unseen during training. More recently Li et al.  [18]  proposed an entropy-based loss function along with GR and showed improved performance compared to  [17] .  Kang et al. [19]  propose channel and emotion invariant speaker embeddings. However, most of these works consider emotion recognition using speech modality alone. Jaiswal and Provost  [20]  explored arXiv:2102.06269v1 [eess.IV] 11 Feb 2021 privacy-preserving multimodal emotion representations, where audio and text modalities were utilized. Our study differs from previous studies by using a secondary task to improve primary emotion recognition performance while being invariant to the auxiliary factors.\n\nWith regards to identity embeddings, Wiliams and King  [12]  have shown that speaker embeddings capture significant amount of affect information. It has been found that differences in the affective states of a person between training and testing conditions can degrade the performance on the task of identity verification from speech  [21, 22] . Techniques have been proposed to compensate this by transforming features from expressive speech to neutral speech domain  [23, 24] . While most of the existing works learn identity representations separately and then try to make them invariant to emotional states, we co-learn identity representations with an emotion recognition task while simultaneously removing emotion information from them. The architectures for this first stage blocks are adopted from  [25] . A second level temporal aggregation block pools the feature representation for audio and video separately over entire clips to fixed dimensional representation. The outputs of the audio and video pooling blocks are concatenated; resulting in independent embedding layers emotion embedding and speaker embedding. The final output layers for task-specific outputs are fully connected layers with a softmax activation function to predict the emotion and person identity labels respectively. Please note that we have used the terms speaker identity and person identity interchangeably throughout the paper.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "Fig.  2  illustrates the addition of auxiliary branches to the baseline multitask architecture. The auxiliary branches are used to assess the amount of emotion information in the speaker embeddings and vice versa. These auxiliary branches are also used for disentanglement as explained in Section 3.2.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Pre-Processing",
      "text": "The input audio and face crop streams from a video clip are first fed into corresponding pre-processing blocks. On the audio stream, pre-processing includes extracting log Mel frequency spectrogram features on overlapping segments of fixed length and stride. This results in one feature vector per segment, with varying number of segments per video clip, depending on the length of the clip. In order to perform efficient batch processing, we pad the features with a constant value to ensure that each video clip contains the same number of segments, N .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Auxiliary Branch For Disentanglement",
      "text": "The multitask outputs are built on top of the common embedding layers for the emotion and person identification tasks respectively. As a result, when training the model, it tends to train an entangled embedding that is optimized for both tasks. This form of entanglement could lead to learning needless dependencies in the train set that may affect the overall generalization. In this work, both for person identification and emotion recognition, the second output or auxiliary task is used to disentangle the emotion information from the speaker embeddings and vice-versa (Fig.  2 ). Disentanglement is achieved using the auxiliary branch. The basic intuition here is similar to domain adversarial training, where the goal is to learn representations that are optimized for the primary task, while simultaneously training it to perform poorly on the auxiliary task. To this end, we experiment with three techniques for disentanglement: (1) gradient reversal, (2) alternate primary-auxiliary training and (2) and confusion loss (CONF).\n\nGradient reversal was originally developed in Ganin and Lempitsky  [26]  to make digit recognition task invariant to domain through adversarial training. As discussed in Section 2, it was adapted to extract speaker-invariant speech emotion representations in Tu et al.  [17] . Gradient reversal is achieved by introducing it in the stages of a network where the auxiliary branch separates from the primary branch. This layer has no effect in the forward pass of training, while in the backward pass the gradients from the auxiliary branch are multipled by a negative value before backpropagating it to the embedding layer.\n\nAlternate training strategy for disentanglement was inspired from adversarial training literature  [27] , where two models are trained with competing objectives. In our setup, for emotion embeddings, the primary task is to predict the emotion labels, while the auxiliary task is to predict person identity labels. Equations 1 and 2 show the loss functions of the primary and auxiliary branch respectively, which are modeled as cross-entropy loss. êprim and ŝprim denote the primary predictions from the emotion and speaker identification branches respectively. Similarly, êaux and ŝaux denote the auxiliary predictions from the speaker identification and emotion recognition branches respectively. etarget and starget denote the groundtruth emotion and speaker identity labels.\n\nL auxiliary =w spk aux * L(êaux, etarget)\n\nAlternate training proceeds in a minimax fashion. The auxiliary branch is trained to minimize L auxiliary , while the primary branch is trained to minimize Lprimary and simultaneously maximize L auxiliary .\n\nConfusion loss for disentanglement has been introduced in Tzeng et al.  [28]  and adapted for disentangling person identity and spoken content representations in Nagrani et al.  [25] . We apply a similar strategy to disentangle the emotion and person identity representations. On a high level, the loss forces the embeddings such that, for the auxiliary task, each class is predicted with the same probability. Similar to  [25] , we implement the confusion loss as the cross-entropy between the predictions and a uniform distribution.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Experimental Framework",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset",
      "text": "For the primary task and disentanglement experiments for multimodal emotion recognition, we use the EmoVox dataset  [29] . The EmoVox dataset comprises of emotional labels on the VoxCeleb dataset obtained by predictions using a strong teacher network over eight emotional states: neutral, happiness, surprise, sadness, anger, disgust, fear and comtempt. Note that the teacher model was trained only using facial features (visual only). Overall, the dataset consists of interview videos from 1251 celebrities spanning a wide range of ages and nationalities. For each video clip, we find the most dominant emotion based on the distribution and use that as our ground-truth label similar to  [29] . The label distribution is heavily skewed towards a few emotion classes because emotions such as disgust, fear, contempt and surprise are rarely exhibited in interviews. Following previous approaches that deal with such imbalanced datasets  [30] , we combine these labels into a single class 'other', resulting in 5 emotion classes. Further, we discard videos corresponding to speakers belonging to the bottom 5 percentile w.r.t the number of segments to reduce the imbalance in the number of speech segments per speaker. We create three splits from the database: EmoVox-Train to train models, EmoVox-Validation for hyperparameter tuning, EmoVox-Test to evaluate models on held out speech segments from speakers present in the train set. The subset EmoVox-Train corresponds to the Train partition in  [29] , whereas the EmoVox-Validation and EmoVox-Test were created from the Heard-Val partition in  [29] .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Experimental Settings",
      "text": "The model architecture for the shared 2D Convolutional layers and the fully connected layers was adapted from  [25]  and modified to suit the dimensions of our inputs and outputs. We use uniform duration videos of 12 seconds each as input to our system. For the audio features we use Da = 40, and for the visual features we use Dv = 224. We fix the emotion embedding dimension to 2048, while varying the speaker embedding dimension 2048, 256 and 64. We use Adam optimizer with an initial learning rate of 1e -4 and 1e -3 for the primary branch and auxiliary branch updates respectively, decaying exponentially with a factor of γ = 0.9. For alternate training (Eqs. 1 and 2), we chose wem prim and w spk prim to be 0.5 each and wem aux and w spk aux to 0.3 each. All parameters were chosen based on preliminary experiments on a subset of EmoVox-Train. The emotion recognition performance was evaluated using unweighted F-score averaged across the 5 emotion classes and for person identity with identification accuracy scores. Disentanglement is measured by combining both the F-score on emotion recognition using speaker embeddings and accuracy on person identification using emotion embeddings. Optimal models were chosen to give the best disentanglement (lowest score) on the EmoVox-Validation set. All results are presented on the EmoVox-Test set.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Baseline Models Without Disentanglement",
      "text": "Emotion Recognition: Figure  3 (a) illustrates the primary emotion recognition results. The blue bars show the performance of all models trained using MTL and the dashed line shows the performance of Single-task learning (STL) setup where the models are not trained on person identification. It is evident that MTL gives substantial gains in performance compared to STL setup. It is also observed that emotion recognition performance improves as the person identification embedding dimension is reduced, which may indicate better regularization with fewer embedding dimensions. Person identification: Table  1  shows the person identification accuracy, trained with varying speaker embedding dimensions. It is worth noting that, despite the reduction in speaker embedding dimension, the models retain performance, pointing to the fact that the task of learning identity representations when both audio and visual modalities are available does not require many degrees of freedom. Identity information in emotion embeddings: Our preliminary experiments showed that the amount of person identity information entangled in emotion embeddings was minimal. Evaluating the person identification task using emotion embeddings produced an accuracy of 0.1%, which was close to random chance performance. Therefore we focus on disentangling emotion information in identity embeddings.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Emotion Information In Identity Embeddings",
      "text": "To baseline the amount of emotion information entangled in the speaker embeddings, we separately train single hidden layer neural network classifiers that predict the emotion labels from speaker embeddings. Figure  3 (b) illustrates the performance. First, it is worth noting that speaker embeddings from models trained for the single task of person identification retain substantial amount of emotion information, as shown by the red dashed line, compared to a random chance F-score of 17.40% if all samples were predicted as 'neutral' class (shown by the green dashed line). Further the blue bars illustrate the performance in the MTL setup where the F-scores are well above random chance as there is more information entanglement. This motivates the need for disentanglement to minimize the emotion information present in speaker embeddings without compromising performance on the emotion recognition, person identification tasks.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Proposed Models With Disentanglement",
      "text": "Next we report the results of the proposed disentanglement techniques and compare them to the baseline models. We trained each disentanglement technique for all three configurations of speaker embedding dimension, 2048, 256 and 64 to investigate their effect on disentanglement performance Emotion Recognition From Fig.  3 (a), we observe that models trained with all three disentanglement strategies outperform the baseline models trained without disentanglement in all but one case. In particular, ALT and CONF methods provide gains consistently across the various embedding dimensions. We performed a Stuart-Maxwell marginal homogeneity test comparing the results and found, with statistical signficance, that all the models with disentanglement were different compared to the baseline models 1 . We also observe that, similar to the baseline models, models trained with disentanglement tend to perform better for reduced speaker embedding dimensions, though with smaller gains.\n\nPerson identification Table  1  shows the person identification accuracy for the models with disentanglement compared to the baseline without disentanglement. We observe that, in general, all models perform better after disentanglement when compared to the baseline without disentanglement. There is no clear evidence of one technique performing better than the other, though GR and ALT seem to 1 H 0 : The predictions from the compared models are the same. Reject H 0 if p < α with α = 0.01 perform marginally better compated to CONF. Emotion information in identity embeddings Fig.  3(b ) illustrates the amount of emotion information in the person identity embeddings after explicit disentanglement. The drop in unweighted average F-score for emotion recognition shows the measure of obtained disentanglement. Compared to the models trained without disentanglement, we observe that the models trained with explicit disentanglement show reduction in F-score of predicting emotions from speaker embeddings. This is noticeable in all the three disentanglement techniques. ALT, CONF training show better disentanglement than GR. Overall, these results show the efficacy of using a separate auxiliary branch to disentangle the emotion information from speaker embeddings. Furthermore, it can be observed that the models trained using the smallest speaker embedding dimension of 64 shows the least amount of emotion information. This is expected because a reduced person identity embedding dimension creates a bottleneck to capture the primary identity information, and thus retains lesser amount of entangled emotion information. Considering the person identity dimension of 64, we see absolute gains of 2% for emotion recognition while ALT training gives 13.5% disentanglement.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "This study analyses disentanglement techniques for emotion recognition in a multitask learning setup, where person identification is the secondary task. We showed with an audio-visual architecture that person identification helps emotion recognition performance. This comes at a cost, as there is significant information transfer between the tasks, which lets us predict emotional categories from speaker embeddings well above chance percentage. To combat this we studied three disentanglement techniques, each reducing the amount of information that is entangled while maintaining or improving performance on the primary task. For our next steps we will explore and validate these methods on other databases which have stronger emotion labels. Furthermore, it is of interest to dig deeper into the reasons for differences in performance across the various disentanglement methods. Finally, this paper shows that there is significant emotional information in the speaker embeddings and the contrary is not necessarily true. Therefore we will explore a hierarchical structure where emotion recognition is more downstream than the person identification task.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Block diagram depitcing the baseline multimodal, multitask",
      "page": 2
    },
    {
      "caption": "Figure 2: Block diagram depicting the baseline model with an auxiliary",
      "page": 2
    },
    {
      "caption": "Figure 1: illustrates the multitask architecture for emotion recog-",
      "page": 2
    },
    {
      "caption": "Figure 2: illustrates the addition of auxiliary branches to the base-",
      "page": 2
    },
    {
      "caption": "Figure 2: ). Disentanglement",
      "page": 2
    },
    {
      "caption": "Figure 3: (a) illustrates the primary emotion",
      "page": 3
    },
    {
      "caption": "Figure 3: (b) illustrates the performance. First, it is worth noting that",
      "page": 3
    },
    {
      "caption": "Figure 3: Unweighted Average F-scores for AER on EmoVox-Test by varying speaker embedding dimension using (a) Emotion embeddings",
      "page": 4
    },
    {
      "caption": "Figure 3: (a), we observe that models",
      "page": 4
    },
    {
      "caption": "Figure 3: (b) illus-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "ABSTRACT"
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": ""
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "Deep learning models trained on audio-visual data have been suc-"
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": ""
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "cessfully used to achieve state-of-the-art performance for emotion"
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": ""
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "recognition.\nIn particular, models trained with multitask learning"
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": ""
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "have shown additional performance improvements. However, such"
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": ""
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "multitask models entangle information between the tasks, encoding"
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": ""
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "the mutual dependencies present\nin label distributions\nin the real"
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": ""
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "world data used for training. This work explores the disentanglement"
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": ""
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "of multimodal signal representations for the primary task of emotion"
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": ""
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "recognition and a secondary person identiﬁcation task.\nIn particu-"
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": ""
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "lar, we developed a multitask framework to extract low-dimensional"
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": ""
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "embeddings that aim to capture emotion speciﬁc information, while"
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": ""
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "containing minimal information related to person identity. We eval-"
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": ""
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "uate three different techniques for disentanglement and report results"
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": ""
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "of up to 13% disentanglement while maintaining emotion recogni-"
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": ""
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "tion performance."
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": ""
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "Index Terms— Keywords: Emotion recognition, multimodal"
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "learning, disentanglement, multitask learning"
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": ""
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": ""
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "1.\nINTRODUCTION"
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": ""
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": ""
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "Emotions play an important role in human communication. Humans"
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": ""
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "externalize their reactions to surrounding stimuli\nthrough a change"
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": ""
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "in the tone of\ntheir voice,\nfacial expressions, hand and body ges-"
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": ""
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "tures.\nTherefore, automatic emotion recognition is of\ninterest\nfor"
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": ""
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "building natural interfaces and effective human-machine interaction."
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": ""
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "[1]. With regards to human communication, emotion is primarily"
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": ""
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "manifested through speech and facial expressions, each providing"
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "complementary information [2]. Therefore, multimodal\ntechniques"
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "have been widely used for reliable emotion prediction [3, 4, 5]."
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": ""
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "Several\nstudies have shown that emotion recognition beneﬁts"
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "from training with secondary related tasks through multitask learn-"
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": ""
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "ing (MTL). In Parthasarathy and Busso [6], predicting the continu-"
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": ""
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "ous affective attributes of valence, arousal and dominance are treated"
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": ""
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "as the multiple tasks and trained jointly. In Li et al. [7] and Kim et al."
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": ""
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "[8], gender prediction as a secondary task improves emotion recog-"
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": ""
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "nition performance by upto 7.7% as measured by weighted accuracy"
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": ""
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "on a standard corpus. A more comprehensive study involving do-"
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": ""
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "main, gender and corpus differences was performed in Zhang et al."
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": ""
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "[9], where cross-corpus evaluations showed that,\nin general,\ninfor-"
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": ""
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "mation sharing across tasks yields improvements in performance of"
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": ""
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "emotion recognition across corpora. These studies indicate that sev-"
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": ""
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "eral paralinguistic tasks help generalize shared representations that"
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": ""
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "improve overall performance of\nthe primary task.\nThis motivates"
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": ""
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "us to use person identiﬁcation as a secondary task to help improve"
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": ""
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": "performance on the primary emotion task."
        },
        {
          "(cid:63)Amazon Inc., Sunnyvale, CA, USA": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "privacy-preserving multimodal emotion representations, where au-": "dio and text modalities were utilized. Our study differs from previ-",
          "representation is extracted for both audio and video independently.": "The architectures for this ﬁrst stage blocks are adopted from [25]. A"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "ous studies by using a secondary task to improve primary emotion",
          "representation is extracted for both audio and video independently.": "second level temporal aggregation block pools the feature represen-"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "recognition performance while being invariant\nto the auxiliary fac-",
          "representation is extracted for both audio and video independently.": "tation for audio and video separately over entire clips to ﬁxed dimen-"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "tors.",
          "representation is extracted for both audio and video independently.": "sional\nrepresentation. The outputs of\nthe audio and video pooling"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "With regards\nto identity embeddings, Wiliams and King [12]",
          "representation is extracted for both audio and video independently.": "blocks are concatenated; resulting in independent embedding layers"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "have shown that speaker embeddings capture signiﬁcant amount of",
          "representation is extracted for both audio and video independently.": "emotion embedding and speaker embedding. The ﬁnal output layers"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "affect\ninformation.\nIt has been found that differences in the affec-",
          "representation is extracted for both audio and video independently.": "for\ntask-speciﬁc outputs are fully connected layers with a softmax"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "tive states of a person between training and testing conditions can",
          "representation is extracted for both audio and video independently.": "activation function to predict the emotion and person identity labels"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "degrade the performance on the task of\nidentity veriﬁcation from",
          "representation is extracted for both audio and video independently.": "respectively. Please note that we have used the terms speaker iden-"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "speech [21, 22]. Techniques have been proposed to compensate this",
          "representation is extracted for both audio and video independently.": "tity and person identity interchangeably throughout the paper."
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "by transforming features from expressive speech to neutral speech",
          "representation is extracted for both audio and video independently.": "Fig. 2 illustrates the addition of auxiliary branches to the base-"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "domain [23, 24]. While most of the existing works learn identity rep-",
          "representation is extracted for both audio and video independently.": "line multitask architecture. The auxiliary branches are used to assess"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "resentations separately and then try to make them invariant\nto emo-",
          "representation is extracted for both audio and video independently.": "the amount of emotion information in the speaker embeddings and"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "tional states, we co-learn identity representations with an emotion",
          "representation is extracted for both audio and video independently.": "vice versa. These auxiliary branches are also used for disentangle-"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "recognition task while simultaneously removing emotion informa-",
          "representation is extracted for both audio and video independently.": "ment as explained in Section 3.2."
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "tion from them.",
          "representation is extracted for both audio and video independently.": ""
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "",
          "representation is extracted for both audio and video independently.": "3.1. Pre-processing"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "3. METHODOLOGY",
          "representation is extracted for both audio and video independently.": ""
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "",
          "representation is extracted for both audio and video independently.": "The input audio and face crop streams from a video clip are ﬁrst"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "",
          "representation is extracted for both audio and video independently.": "fed into corresponding pre-processing blocks. On the audio stream,"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "Speaker Identification\nEmotion label",
          "representation is extracted for both audio and video independently.": "pre-processing includes extracting log Mel\nfrequency spectrogram"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "",
          "representation is extracted for both audio and video independently.": "features on overlapping segments of ﬁxed length and stride.\nThis"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "",
          "representation is extracted for both audio and video independently.": "results in one feature vector per segment, with varying number of"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "Fully-connected\nFully-connected",
          "representation is extracted for both audio and video independently.": ""
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "",
          "representation is extracted for both audio and video independently.": "segments per video clip, depending on the length of the clip.\nIn or-"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "",
          "representation is extracted for both audio and video independently.": "der to perform efﬁcient batch processing, we pad the features with"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "Speaker Embedding\nEmotion Embedding",
          "representation is extracted for both audio and video independently.": "a constant value to ensure that each video clip contains the same"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "Temporal \nConcatenation",
          "representation is extracted for both audio and video independently.": "number of segments, N . The resulting features have the dimensions"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "Concatenation",
          "representation is extracted for both audio and video independently.": ""
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "aggregation",
          "representation is extracted for both audio and video independently.": ""
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "Convolution \nConvolution \nConvolution \nConvolution",
          "representation is extracted for both audio and video independently.": "B ∗ N ∗ Da where B is the minibatch size and Da is the dimension"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "+  temporal \n+ temporal \n+  temporal \n+ temporal \npooling block\npooling block\npooling block\npooling block",
          "representation is extracted for both audio and video independently.": "of the Mel spectrogram features. On the face crops, pre-procesing"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "(video)\n(audio)\n(video)\n(audio)",
          "representation is extracted for both audio and video independently.": "includes resizing them into a ﬁxed size of Dv ∗ Dv pixels and rescal-"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "1D conv",
          "representation is extracted for both audio and video independently.": "ing the values to between −1 and 1. The resulting face crops have"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "2D Conv\n3D Conv",
          "representation is extracted for both audio and video independently.": "the dimensions B ∗ N ∗ Dv ∗ Dv."
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "(Audio)\n(Video)",
          "representation is extracted for both audio and video independently.": ""
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "Video",
          "representation is extracted for both audio and video independently.": ""
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "Pre-processing\nPre-processing\nAudio",
          "representation is extracted for both audio and video independently.": "3.2. Auxiliary branch for disentanglement"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "",
          "representation is extracted for both audio and video independently.": "The multitask outputs are built on top of\nthe common embedding"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "Overlapping",
          "representation is extracted for both audio and video independently.": ""
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "segments",
          "representation is extracted for both audio and video independently.": "layers for\nthe emotion and person identiﬁcation tasks respectively."
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "Length-3.6sec,",
          "representation is extracted for both audio and video independently.": ""
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "stride-0.72sec",
          "representation is extracted for both audio and video independently.": "As a result, when training the model,\nit\ntends to train an entangled"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "",
          "representation is extracted for both audio and video independently.": "embedding that\nis optimized for both tasks. This form of entangle-"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "Fig. 1. Block diagram depitcing the baseline multimodal, multitask",
          "representation is extracted for both audio and video independently.": "ment could lead to learning needless dependencies in the train set"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "training",
          "representation is extracted for both audio and video independently.": "that may affect\nthe overall generalization.\nIn this work, both for"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "",
          "representation is extracted for both audio and video independently.": "person identiﬁcation and emotion recognition,\nthe second output or"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "",
          "representation is extracted for both audio and video independently.": "auxiliary task is used to disentangle the emotion information from"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "",
          "representation is extracted for both audio and video independently.": "the speaker embeddings and vice-versa (Fig.\n2). Disentanglement"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "INPUT LAYER\nEMBEDDING EXTRACTION\nOUTPUT LAYER\nTASK\nTYPE",
          "representation is extracted for both audio and video independently.": "is achieved using the auxiliary branch. The basic intuition here is"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "",
          "representation is extracted for both audio and video independently.": "similar to domain adversarial training, where the goal is to learn rep-"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "Disentanglement\nSpeaker Identification",
          "representation is extracted for both audio and video independently.": ""
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "AUDIO\nEmotion embedding\n2D Conv",
          "representation is extracted for both audio and video independently.": "resentations that are optimized for the primary task, while simulta-"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "AUXILLARY\n(Audio)",
          "representation is extracted for both audio and video independently.": ""
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "Fully-connected\nEmotion Recognition",
          "representation is extracted for both audio and video independently.": "neously training it\nto perform poorly on the auxiliary task. To this"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "VIDEO\n3D Conv",
          "representation is extracted for both audio and video independently.": ""
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "PRIMARY\n(Video)",
          "representation is extracted for both audio and video independently.": "end, we experiment with three techniques for disentanglement:\n(1)"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "Fully-connected\nSpeaker identification",
          "representation is extracted for both audio and video independently.": ""
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "Speaker embedding",
          "representation is extracted for both audio and video independently.": "gradient reversal, (2) alternate primary-auxiliary training and (2) and"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "Disentanglement\nEmotion Recognition",
          "representation is extracted for both audio and video independently.": "confusion loss (CONF)."
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "",
          "representation is extracted for both audio and video independently.": "Gradient reversal was originally developed in Ganin and Lem-"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "",
          "representation is extracted for both audio and video independently.": "pitsky\n[26]\nto make\ndigit\nrecognition\ntask\ninvariant\nto\ndomain"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "",
          "representation is extracted for both audio and video independently.": "through adversarial\ntraining.\nAs discussed in Section 2,\nit was"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "Fig. 2. Block diagram depicting the baseline model with an auxiliary",
          "representation is extracted for both audio and video independently.": ""
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "",
          "representation is extracted for both audio and video independently.": "adapted to extract speaker-invariant speech emotion representations"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "disentanglement task",
          "representation is extracted for both audio and video independently.": ""
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "",
          "representation is extracted for both audio and video independently.": "in Tu et al.\n[17]. Gradient reversal\nis achieved by introducing it\nin"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "",
          "representation is extracted for both audio and video independently.": "the stages of a network where the auxiliary branch separates from"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "Fig.\n1 illustrates the multitask architecture for emotion recog-",
          "representation is extracted for both audio and video independently.": "the primary branch. This layer has no effect\nin the forward pass of"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "nition and person identiﬁcation. The inputs to the model are both",
          "representation is extracted for both audio and video independently.": "training, while in the backward pass the gradients from the auxiliary"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "audio and video frames that are time-synchronized. The ﬁrst step is",
          "representation is extracted for both audio and video independently.": "branch are multipled by a negative value before backpropagating it"
        },
        {
          "privacy-preserving multimodal emotion representations, where au-": "a shared convolutional feature extraction stage where a data-driven",
          "representation is extracted for both audio and video independently.": "to the embedding layer."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Alternate training strategy for disentanglement was\ninspired": "from adversarial\ntraining\nliterature\n[27], where\ntwo models\nare",
          "4.2. Experimental Settings": ""
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "",
          "4.2. Experimental Settings": "The model architecture for the shared 2D Convolutional\nlayers and"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "trained with competing objectives. In our setup, for emotion embed-",
          "4.2. Experimental Settings": ""
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "",
          "4.2. Experimental Settings": "the fully connected layers was adapted from [25] and modiﬁed to"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "dings,\nthe primary task is to predict\nthe emotion labels, while the",
          "4.2. Experimental Settings": ""
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "",
          "4.2. Experimental Settings": "suit\nthe dimensions of our inputs and outputs. We use uniform du-"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "auxiliary task is to predict person identity labels. Equations 1 and 2",
          "4.2. Experimental Settings": ""
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "",
          "4.2. Experimental Settings": "ration videos of 12 seconds each as input\nto our system.\nFor\nthe"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "show the loss functions of the primary and auxiliary branch respec-",
          "4.2. Experimental Settings": ""
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "",
          "4.2. Experimental Settings": "audio features we use Da = 40, and for the visual features we use"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "tively, which are modeled as cross-entropy loss.\neprim and ˆsprim",
          "4.2. Experimental Settings": ""
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "",
          "4.2. Experimental Settings": "Dv = 224. We ﬁx the emotion embedding dimension to 2048,"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "denote the primary predictions from the emotion and speaker identi-",
          "4.2. Experimental Settings": ""
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "",
          "4.2. Experimental Settings": "while varying the speaker embedding dimension 2048, 256 and 64."
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "ﬁcation branches respectively. Similarly, ˆeaux and ˆsaux denote the",
          "4.2. Experimental Settings": ""
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "",
          "4.2. Experimental Settings": "We use Adam optimizer with an initial\nlearning rate of 1e − 4 and"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "auxiliary predictions\nfrom the speaker\nidentiﬁcation and emotion",
          "4.2. Experimental Settings": ""
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "",
          "4.2. Experimental Settings": "1e − 3 for the primary branch and auxiliary branch updates respec-"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "recognition branches\nrespectively.\netarget\nand starget denote the",
          "4.2. Experimental Settings": ""
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "",
          "4.2. Experimental Settings": "tively, decaying exponentially with a factor of γ = 0.9. For alternate"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "groundtruth emotion and speaker identity labels.",
          "4.2. Experimental Settings": ""
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "",
          "4.2. Experimental Settings": "training (Eqs. 1 and 2), we chose wem prim and wspk prim to be 0.5"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "",
          "4.2. Experimental Settings": "each and wem aux and wspk aux to 0.3 each. All parameters were"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "Lprimary =wem prim ∗ L(ˆeprim, etarget)",
          "4.2. Experimental Settings": ""
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "(1)",
          "4.2. Experimental Settings": "chosen based on preliminary experiments on a subset of EmoVox-"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "+ wspk prim ∗ L(ˆsprim, starget)",
          "4.2. Experimental Settings": ""
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "",
          "4.2. Experimental Settings": "Train.\nThe emotion recognition performance was evaluated using"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "",
          "4.2. Experimental Settings": "unweighted F-score averaged across the 5 emotion classes and for"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "Lauxiliary =wspk aux ∗ L(ˆeaux, etarget)",
          "4.2. Experimental Settings": "person identity with identiﬁcation accuracy scores. Disentanglement"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "(2)",
          "4.2. Experimental Settings": "is measured by combining both the F-score on emotion recognition"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "+ wem aux ∗ L(ˆsaux, starget)",
          "4.2. Experimental Settings": ""
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "",
          "4.2. Experimental Settings": "using speaker embeddings and accuracy on person identiﬁcation us-"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "Alternate training proceeds in a minimax fashion.\nThe auxil-",
          "4.2. Experimental Settings": "ing emotion embeddings. Optimal models were chosen to give the"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "iary branch is\ntrained to minimize Lauxiliary, while the primary",
          "4.2. Experimental Settings": "best disentanglement\n(lowest score) on the EmoVox-Validation set."
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "branch is trained to minimize Lprimary and simultaneously maxi-",
          "4.2. Experimental Settings": "All results are presented on the EmoVox-Test set."
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "mize Lauxiliary.",
          "4.2. Experimental Settings": ""
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "Confusion loss\nfor\ndisentanglement\nhas\nbeen\nintroduced\nin",
          "4.2. Experimental Settings": "5. RESULTS"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "Tzeng et al.\n[28] and adapted for disentangling person identity and",
          "4.2. Experimental Settings": ""
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "spoken content representations in Nagrani et al.\n[25]. We apply a",
          "4.2. Experimental Settings": "5.1. Baseline models without disentanglement"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "similar strategy to disentangle the emotion and person identity rep-",
          "4.2. Experimental Settings": ""
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "",
          "4.2. Experimental Settings": "Emotion Recognition: Figure 3(a) illustrates the primary emotion"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "resentations. On a high level,\nthe loss forces the embeddings such",
          "4.2. Experimental Settings": ""
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "",
          "4.2. Experimental Settings": "recognition results. The blue bars show the performance of all mod-"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "that,\nfor\nthe auxiliary task, each class\nis predicted with the same",
          "4.2. Experimental Settings": ""
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "",
          "4.2. Experimental Settings": "els trained using MTL and the dashed line shows the performance of"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "probability. Similar to [25], we implement the confusion loss as the",
          "4.2. Experimental Settings": ""
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "",
          "4.2. Experimental Settings": "Single-task learning (STL) setup where the models are not\ntrained"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "cross-entropy between the predictions and a uniform distribution.",
          "4.2. Experimental Settings": ""
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "",
          "4.2. Experimental Settings": "on person identiﬁcation.\nIt\nis evident\nthat MTL gives\nsubstantial"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "",
          "4.2. Experimental Settings": "gains in performance compared to STL setup.\nIt\nis also observed"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "4. EXPERIMENTAL FRAMEWORK",
          "4.2. Experimental Settings": "that emotion recognition performance improves as the person identi-"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "",
          "4.2. Experimental Settings": "ﬁcation embedding dimension is reduced, which may indicate better"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "",
          "4.2. Experimental Settings": "regularization with fewer embedding dimensions."
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "4.1. Dataset",
          "4.2. Experimental Settings": ""
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "",
          "4.2. Experimental Settings": "Person identiﬁcation: Table 1 shows the person identiﬁcation ac-"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "For\nthe primary task and disentanglement experiments\nfor multi-",
          "4.2. Experimental Settings": "curacy,\ntrained with varying speaker embedding dimensions.\nIt\nis"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "modal emotion recognition, we use the EmoVox dataset [29]. The",
          "4.2. Experimental Settings": "worth noting that, despite the reduction in speaker embedding di-"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "EmoVox dataset\ncomprises of\nemotional\nlabels on the VoxCeleb",
          "4.2. Experimental Settings": "mension, the models retain performance, pointing to the fact that the"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "dataset obtained by predictions using a strong teacher network over",
          "4.2. Experimental Settings": "task of learning identity representations when both audio and visual"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "eight emotional states: neutral, happiness, surprise, sadness, anger,",
          "4.2. Experimental Settings": "modalities are available does not require many degrees of freedom."
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "disgust, fear and comtempt. Note that the teacher model was trained",
          "4.2. Experimental Settings": "Identity information in emotion embeddings: Our preliminary"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "only using facial\nfeatures (visual only). Overall,\nthe dataset con-",
          "4.2. Experimental Settings": "experiments showed that\nthe amount of person identity information"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "sists of\ninterview videos\nfrom 1251 celebrities\nspanning a wide",
          "4.2. Experimental Settings": "entangled in emotion embeddings was minimal.\nEvaluating the"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "range of ages and nationalities.\nFor each video clip, we ﬁnd the",
          "4.2. Experimental Settings": "person identiﬁcation task using emotion embeddings produced an"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "most dominant emotion based on the distribution and use that as",
          "4.2. Experimental Settings": "accuracy of 0.1%, which was close to random chance performance."
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "our ground-truth label\nsimilar\nto [29].\nThe\nlabel distribution is",
          "4.2. Experimental Settings": "Therefore we focus on disentangling emotion information in identity"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "heavily skewed towards a few emotion classes because emotions",
          "4.2. Experimental Settings": "embeddings."
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "such as disgust,\nfear, contempt and surprise are rarely exhibited in",
          "4.2. Experimental Settings": "Emotion information in identity\nembeddings To\nbaseline\nthe"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "interviews. Following previous approaches that deal with such im-",
          "4.2. Experimental Settings": "amount of emotion information entangled in the speaker embed-"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "balanced datasets [30], we combine these labels into a single class",
          "4.2. Experimental Settings": "dings, we separately train single hidden layer neural network clas-"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "‘other‘,\nresulting in 5 emotion classes.\nFurther, we discard videos",
          "4.2. Experimental Settings": "siﬁers\nthat predict\nthe\nemotion labels\nfrom speaker\nembeddings."
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "corresponding to speakers belonging to the bottom 5 percentile w.r.t",
          "4.2. Experimental Settings": "Figure 3(b) illustrates the performance. First, it is worth noting that"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "the number of\nsegments\nto reduce\nthe\nimbalance\nin the number",
          "4.2. Experimental Settings": "speaker embeddings from models trained for the single task of per-"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "of\nspeech segments per\nspeaker. We create three splits\nfrom the",
          "4.2. Experimental Settings": "son identiﬁcation retain substantial amount of emotion information,"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "database:\nEmoVox-Train to train models, EmoVox-Validation for",
          "4.2. Experimental Settings": "as\nshown by the red dashed line,\ncompared to a random chance"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "hyperparameter tuning, EmoVox-Test to evaluate models on held out",
          "4.2. Experimental Settings": "F-score of 17.40% if all samples were predicted as ‘neutral‘ class"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "speech segments from speakers present\nin the train set. The subset",
          "4.2. Experimental Settings": "(shown by the green dashed line). Further the blue bars illustrate the"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "EmoVox-Train corresponds to the Train partition in [29], whereas",
          "4.2. Experimental Settings": "performance in the MTL setup where the F-scores are well above"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "the EmoVox-Validation\nand EmoVox-Test were\ncreated\nfrom the",
          "4.2. Experimental Settings": "random chance as\nthere is more information entanglement.\nThis"
        },
        {
          "Alternate training strategy for disentanglement was\ninspired": "Heard-Val partition in [29].",
          "4.2. Experimental Settings": "motivates\nthe need for disentanglement\nto minimize the emotion"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "41.00%": ""
        },
        {
          "41.00%": ""
        },
        {
          "41.00%": ""
        },
        {
          "41.00%": "40.00%"
        },
        {
          "41.00%": ""
        },
        {
          "41.00%": ""
        },
        {
          "41.00%": "39.00%"
        },
        {
          "41.00%": ""
        },
        {
          "41.00%": ""
        },
        {
          "41.00%": "38.00%"
        },
        {
          "41.00%": ""
        },
        {
          "41.00%": ""
        },
        {
          "41.00%": "37.00%"
        },
        {
          "41.00%": ""
        },
        {
          "41.00%": ""
        },
        {
          "41.00%": "36.00%"
        },
        {
          "41.00%": ""
        },
        {
          "41.00%": ""
        },
        {
          "41.00%": "35.00%"
        },
        {
          "41.00%": ""
        },
        {
          "41.00%": ""
        },
        {
          "41.00%": "34.00%"
        },
        {
          "41.00%": "33.00%"
        },
        {
          "41.00%": "32.00%"
        },
        {
          "41.00%": "31.00%"
        },
        {
          "41.00%": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "31.00%": "2048\n256\n64",
          "0.00%": "2048\n256\n64"
        },
        {
          "31.00%": "Baseline (multitask)\nALT\nGR\nCONF",
          "0.00%": "Baseline (multitask)\nALT\nGR\nCONF"
        },
        {
          "31.00%": "Raghuveer Peri",
          "0.00%": "Raghuveer Peri"
        },
        {
          "31.00%": "* All models were found to be different than baseline, based on Stuart-Maxwell test for marginal homogeneity, with p<0.05",
          "0.00%": ""
        },
        {
          "31.00%": "",
          "0.00%": ""
        },
        {
          "31.00%": "",
          "0.00%": ""
        },
        {
          "31.00%": "",
          "0.00%": ""
        },
        {
          "31.00%": "",
          "0.00%": "perform marginally better compated to CONF."
        },
        {
          "31.00%": "Table 1. Person identiﬁcation accuracy (%) comparing models with",
          "0.00%": ""
        },
        {
          "31.00%": "",
          "0.00%": "Emotion\ninformation\nin\nidentity\nembeddings\nFig.\n3(b)\nillus-"
        },
        {
          "31.00%": "varying speaker embedding dimensions without and with disentan-",
          "0.00%": ""
        },
        {
          "31.00%": "",
          "0.00%": "trates\nthe\namount of\nemotion information in the person identity"
        },
        {
          "31.00%": "glement on EmoVox-Test",
          "0.00%": ""
        },
        {
          "31.00%": "",
          "0.00%": "embeddings after explicit disentanglement. The drop in unweighted"
        },
        {
          "31.00%": "",
          "0.00%": "average F-score for emotion recognition shows the measure of ob-"
        },
        {
          "31.00%": "Emb Dim\nBaseline\nALT\nGR\nCONF",
          "0.00%": ""
        },
        {
          "31.00%": "",
          "0.00%": "tained disentanglement.\nCompared to the models trained without"
        },
        {
          "31.00%": "2048\n90.98\n92.40\n93.19\n93.12",
          "0.00%": "disentanglement, we observe that\nthe models trained with explicit"
        },
        {
          "31.00%": "",
          "0.00%": "disentanglement show reduction in F-score of predicting emotions"
        },
        {
          "31.00%": "256\n94.75\n95.04\n95.86\n95.42",
          "0.00%": ""
        },
        {
          "31.00%": "",
          "0.00%": "from speaker embeddings. This is noticeable in all\nthe three disen-"
        },
        {
          "31.00%": "64\n90.62\n92.83\n91.17\n90.75",
          "0.00%": ""
        },
        {
          "31.00%": "",
          "0.00%": "tanglement\ntechniques. ALT, CONF training show better disentan-"
        },
        {
          "31.00%": "",
          "0.00%": "glement\nthan GR. Overall,\nthese results show the efﬁcacy of using"
        },
        {
          "31.00%": "",
          "0.00%": "a separate auxiliary branch to disentangle the emotion information"
        },
        {
          "31.00%": "information present\nin speaker embeddings without compromising",
          "0.00%": "from speaker embeddings. Furthermore,\nit can be observed that\nthe"
        },
        {
          "31.00%": "performance on the emotion recognition, person identiﬁcation tasks.",
          "0.00%": "models trained using the smallest speaker embedding dimension of"
        },
        {
          "31.00%": "",
          "0.00%": "64 shows the least amount of emotion information. This is expected"
        },
        {
          "31.00%": "",
          "0.00%": "because\na\nreduced person identity embedding dimension creates"
        },
        {
          "31.00%": "5.2. Proposed models with disentanglement",
          "0.00%": ""
        },
        {
          "31.00%": "",
          "0.00%": "a bottleneck to capture the primary identity information, and thus"
        },
        {
          "31.00%": "Next we report\nthe results of\nthe proposed disentanglement\ntech-",
          "0.00%": "retains\nlesser amount of entangled emotion information.\nConsid-"
        },
        {
          "31.00%": "niques and compare them to the baseline models. We trained each",
          "0.00%": "ering the person identity dimension of 64, we see absolute gains"
        },
        {
          "31.00%": "disentanglement\ntechnique for all\nthree conﬁgurations of\nspeaker",
          "0.00%": "of 2% for\nemotion recognition while ALT training gives 13.5%"
        },
        {
          "31.00%": "embedding dimension, 2048, 256 and 64 to investigate their effect",
          "0.00%": "disentanglement."
        },
        {
          "31.00%": "on disentanglement performance",
          "0.00%": ""
        },
        {
          "31.00%": "Emotion Recognition From Fig.\n3(a), we\nobserve\nthat models",
          "0.00%": "6. CONCLUSIONS"
        },
        {
          "31.00%": "trained with\nall\nthree\ndisentanglement\nstrategies\noutperform the",
          "0.00%": ""
        },
        {
          "31.00%": "",
          "0.00%": "This study analyses disentanglement\ntechniques for emotion recog-"
        },
        {
          "31.00%": "baseline models\ntrained without\ndisentanglement\nin\nall\nbut\none",
          "0.00%": ""
        },
        {
          "31.00%": "",
          "0.00%": "nition in a multitask learning setup, where person identiﬁcation is the"
        },
        {
          "31.00%": "case.\nIn particular, ALT and CONF methods provide gains consis-",
          "0.00%": ""
        },
        {
          "31.00%": "",
          "0.00%": "secondary task. We showed with an audio-visual architecture that"
        },
        {
          "31.00%": "tently across the various embedding dimensions. We performed a",
          "0.00%": ""
        },
        {
          "31.00%": "",
          "0.00%": "person identiﬁcation helps emotion recognition performance. This"
        },
        {
          "31.00%": "Stuart-Maxwell marginal homogeneity test comparing the results",
          "0.00%": ""
        },
        {
          "31.00%": "",
          "0.00%": "comes at a cost, as there is signiﬁcant\ninformation transfer between"
        },
        {
          "31.00%": "and found, with statistical signﬁcance,\nthat all\nthe models with dis-",
          "0.00%": ""
        },
        {
          "31.00%": "",
          "0.00%": "the tasks, which lets us predict emotional categories from speaker"
        },
        {
          "31.00%": "entanglement were different compared to the baseline models 1. We",
          "0.00%": ""
        },
        {
          "31.00%": "",
          "0.00%": "embeddings well above chance percentage. To combat this we stud-"
        },
        {
          "31.00%": "also observe that,\nsimilar\nto the baseline models, models\ntrained",
          "0.00%": ""
        },
        {
          "31.00%": "",
          "0.00%": "ied three disentanglement\ntechniques, each reducing the amount of"
        },
        {
          "31.00%": "with disentanglement\ntend to perform better\nfor\nreduced speaker",
          "0.00%": ""
        },
        {
          "31.00%": "",
          "0.00%": "information that\nis entangled while maintaining or\nimproving per-"
        },
        {
          "31.00%": "embedding dimensions, though with smaller gains.",
          "0.00%": ""
        },
        {
          "31.00%": "",
          "0.00%": "formance on the primary task.\nFor our next steps we will explore"
        },
        {
          "31.00%": "Person identiﬁcation Table 1 shows the person identiﬁcation accu-",
          "0.00%": ""
        },
        {
          "31.00%": "",
          "0.00%": "and validate these methods on other databases which have stronger"
        },
        {
          "31.00%": "racy for the models with disentanglement compared to the baseline",
          "0.00%": ""
        },
        {
          "31.00%": "",
          "0.00%": "emotion labels. Furthermore,\nit\nis of interest\nto dig deeper into the"
        },
        {
          "31.00%": "without disentanglement. We observe that,\nin general, all models",
          "0.00%": ""
        },
        {
          "31.00%": "",
          "0.00%": "reasons for differences in performance across the various disentan-"
        },
        {
          "31.00%": "perform better after disentanglement when compared to the baseline",
          "0.00%": ""
        },
        {
          "31.00%": "",
          "0.00%": "glement methods. Finally,\nthis paper shows that\nthere is signiﬁcant"
        },
        {
          "31.00%": "without disentanglement.\nThere is no clear evidence of one tech-",
          "0.00%": ""
        },
        {
          "31.00%": "",
          "0.00%": "emotional information in the speaker embeddings and the contrary is"
        },
        {
          "31.00%": "nique performing better than the other, though GR and ALT seem to",
          "0.00%": ""
        },
        {
          "31.00%": "",
          "0.00%": "not necessarily true. Therefore we will explore a hierarchical struc-"
        },
        {
          "31.00%": "1H0: The predictions from the compared models are the same. Reject",
          "0.00%": "ture where emotion recognition is more downstream than the person"
        },
        {
          "31.00%": "H0 if p < α with α = 0.01",
          "0.00%": "identiﬁcation task."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "7. REFERENCES": "",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "emotion-discriminative and domain-invariant\nfeatures for do-"
        },
        {
          "7. REFERENCES": "[1] M. Pantic and J. M. L. Rothkrantz, “Toward an affect-sensitive",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "main adaptation in speech emotion recognition,” Speech Com-"
        },
        {
          "7. REFERENCES": "the\nmultimodal human-computer interaction,” Proceedings of",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "munication, vol. 93, pp. 1–10, 2017."
        },
        {
          "7. REFERENCES": "IEEE, vol. 91, no. 9, pp. 1370–1390, 2003.",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "[17] M. Tu, Y. Tang, J. Huang, X. He, and B. Zhou, “Towards adver-"
        },
        {
          "7. REFERENCES": "",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "sarial\nlearning of speaker-invariant\nrepresentation for speech"
        },
        {
          "7. REFERENCES": "[2] A. Mehrabian,\n“Communication without words,” Communi-",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": ""
        },
        {
          "7. REFERENCES": "",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "emotion recognition,” arXiv preprint arXiv:1903.09606, 2019."
        },
        {
          "7. REFERENCES": "cation theory, vol. 6, pp. 193–200, 2008.",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": ""
        },
        {
          "7. REFERENCES": "",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "[18] H. Li, M. Tu,\nJ. Huang,\nS. Narayanan,\nand P. Georgiou,"
        },
        {
          "7. REFERENCES": "[3] Y. Kim, H. Lee, and E. M. Provost,\n“Deep learning for\nro-",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": ""
        },
        {
          "7. REFERENCES": "",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "“Speaker-invariant\naffective\nrepresentation\nlearning\nvia\nad-"
        },
        {
          "7. REFERENCES": "bust feature generation in audiovisual emotion recognition,” in",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": ""
        },
        {
          "7. REFERENCES": "",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "ICASSP 2020-2020\nIEEE Interna-\nversarial\ntraining,”\nin"
        },
        {
          "7. REFERENCES": "2013 IEEE international conference on acoustics, speech and",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": ""
        },
        {
          "7. REFERENCES": "",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "tional Conference on Acoustics, Speech and Signal Processing"
        },
        {
          "7. REFERENCES": "signal processing, Vancouver, BC, Canada, 2013,\nIEEE, pp.",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": ""
        },
        {
          "7. REFERENCES": "",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "(ICASSP), Barcelona, Spain, 2020, IEEE, pp. 7144–7148."
        },
        {
          "7. REFERENCES": "3687–3691.",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": ""
        },
        {
          "7. REFERENCES": "",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "[19] W. H. Kang, S. H. Mun, M. H. Han, and N. S. Kim,\n“Dis-"
        },
        {
          "7. REFERENCES": "[4] Y. Wang and L. Guan,\n“Recognizing human emotional state",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "entangled speaker and nuisance attribute embedding for\nro-"
        },
        {
          "7. REFERENCES": "from audiovisual signals,”\nIEEE transactions on multimedia,",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "bust speaker veriﬁcation,”\nIEEE Access, vol. 8, pp. 141838–"
        },
        {
          "7. REFERENCES": "vol. 10, no. 5, pp. 936–946, 2008.",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "141849, 2020."
        },
        {
          "7. REFERENCES": "",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "[20] M. Jaiswal and E. M. Provost, “Privacy enhanced multimodal"
        },
        {
          "7. REFERENCES": "[5] M. Song,\nJ. Bu, C. Chen,\nand N. Li,\n“Audio-visual based",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": ""
        },
        {
          "7. REFERENCES": "",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "neural\nrepresentations\nfor\nemotion recognition.,”\nin AAAI,"
        },
        {
          "7. REFERENCES": "the\nemotion recognition-a new approach,”\nin Proceedings of",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": ""
        },
        {
          "7. REFERENCES": "",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "2020, pp. 7985–7993."
        },
        {
          "7. REFERENCES": "2004 IEEE Computer Society Conference on Computer Vision",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": ""
        },
        {
          "7. REFERENCES": "and Pattern Recognition, 2004. CVPR 2004., Washington, DC,",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "[21]\nS. Parthasarathy, C. Zhang,\nJ. H. L. Hansen, and C. Busso,"
        },
        {
          "7. REFERENCES": "USA, 2004, IEEE, vol. 2, pp. II–II.",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "“A study of speaker veriﬁcation performance with expressive"
        },
        {
          "7. REFERENCES": "",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "speech,” in 2017 IEEE International Conference on Acoustics,"
        },
        {
          "7. REFERENCES": "[6]\nS. Parthasarathy and C. Busso,\n“Jointly predicting arousal,",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": ""
        },
        {
          "7. REFERENCES": "",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "Speech and Signal Processing (ICASSP), New Orleans, LA,"
        },
        {
          "7. REFERENCES": "valence and dominance with multi-task learning.,”\nin Inter-",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": ""
        },
        {
          "7. REFERENCES": "",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "USA, 2017, IEEE, pp. 5540–5544."
        },
        {
          "7. REFERENCES": "speech, Stockholm, Sweden, 2017, pp. 1103–1107.",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": ""
        },
        {
          "7. REFERENCES": "",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "[22] W. Wu, T. F. Zheng, M. X. Xu, and H.\nJ. Bao,\n“Study on"
        },
        {
          "7. REFERENCES": "[7] Y. Li, T. Zhao, and T. Kawahara, “Improved end-to-end speech",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "speaker veriﬁcation on emotional speech,”\nin Ninth Interna-"
        },
        {
          "7. REFERENCES": "emotion recognition using self attention mechanism and multi-",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "tional Conference on Spoken Language Processing, Pittsburgh,"
        },
        {
          "7. REFERENCES": "task learning.,” in Interspeech, Graz, Austria, 2019, pp. 2803–",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "PA, USA, 2006, pp. 2102–2105."
        },
        {
          "7. REFERENCES": "2807.",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": ""
        },
        {
          "7. REFERENCES": "",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "[23] H. Bao, M. X. Xu, and T. F. Zheng, “Emotion attribute projec-"
        },
        {
          "7. REFERENCES": "[8]\nJ. Kim, G. Englebienne, P. K. Truong, and V. Evers,\n“To-",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "tion for speaker recognition on emotional speech,”\nin Eighth"
        },
        {
          "7. REFERENCES": "",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "Annual Conference of\nthe International Speech Communica-"
        },
        {
          "7. REFERENCES": "wards speech emotion recognition” in the wild” using aggre-",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": ""
        },
        {
          "7. REFERENCES": "arXiv preprint\ngated corpora and deep multi-task learning,”",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "tion Association, Antwerp, Belgium, 2007, pp. 758–761."
        },
        {
          "7. REFERENCES": "arXiv:1708.03920, 2017.",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "[24]\nS. R. Krothapalli,\nJ. Yadav, S. Sarkar, G. S. Koolagudi, and"
        },
        {
          "7. REFERENCES": "",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "A. K. Vuppala, “Neural network based feature transformation"
        },
        {
          "7. REFERENCES": "[9] B. Zhang, E. M. Provost, and G. Essl, “Cross-corpus acoustic",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": ""
        },
        {
          "7. REFERENCES": "",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "for emotion independent speaker identiﬁcation,” International"
        },
        {
          "7. REFERENCES": "emotion recognition with multi-task learning:\nSeeking com-",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": ""
        },
        {
          "7. REFERENCES": "",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "Journal of Speech Technology, vol. 15, no. 3, pp. 335–349,"
        },
        {
          "7. REFERENCES": "mon ground while preserving differences,” IEEE Transactions",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": ""
        },
        {
          "7. REFERENCES": "",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "2012."
        },
        {
          "7. REFERENCES": "on Affective Computing, vol. 10, no. 1, pp. 85–99, 2017.",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": ""
        },
        {
          "7. REFERENCES": "",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "[25] A. Nagrani,\nJ. S . Chung, S. Albanie,\nand A. Zisserman,"
        },
        {
          "7. REFERENCES": "[10]\nJ. Liang, Z. Liu, J. Zhou, X. Jiang, C .Zhang, and F. Wang,",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": ""
        },
        {
          "7. REFERENCES": "",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "“Disentangled\nspeech\nembeddings\nusing\ncross-modal\nself-"
        },
        {
          "7. REFERENCES": "IEEE Transactions on\n“Model-protected multi-task learning,”",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": ""
        },
        {
          "7. REFERENCES": "",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "supervision,”\nin ICASSP 2020-2020 IEEE International Con-"
        },
        {
          "7. REFERENCES": "Pattern Analysis and Machine Intelligence, pp. 1–1, 2020.",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": ""
        },
        {
          "7. REFERENCES": "",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "ference on Acoustics, Speech and Signal Processing (ICASSP),"
        },
        {
          "7. REFERENCES": "",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "Barcelona, Spain, 2020, IEEE, pp. 6829–6833."
        },
        {
          "7. REFERENCES": "[11] L. Xiao, H. Zhang, W. Chen, Y. Wang, and Y. Jin,\n“Learn-",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": ""
        },
        {
          "7. REFERENCES": "ing what\nto share: Leaky multi-task network for text classiﬁ-",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "[26] Y. Ganin and V. Lempitsky, “Unsupervised domain adaptation"
        },
        {
          "7. REFERENCES": "the 27th International Conference\ncation,”\nin Proceedings of",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "by backpropagation,”\nin International conference on machine"
        },
        {
          "7. REFERENCES": "on Computational Linguistics, Santa Fe, NM, USA, 2018, pp.",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "learning, Lille, France, 2015, PMLR, pp. 1180–1189."
        },
        {
          "7. REFERENCES": "2055–2065.",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "[27]\nI. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-"
        },
        {
          "7. REFERENCES": "",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "Farley, S. Ozair, A. Courville, and Y. Bengio, “Generative ad-"
        },
        {
          "7. REFERENCES": "[12]\nJ. Williams and S. King,\n“Disentangling style factors\nfrom",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": ""
        },
        {
          "7. REFERENCES": "",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "information processing\nversarial nets,”\nin Advances in neural"
        },
        {
          "7. REFERENCES": "speaker\nrepresentations.,”\nin INTERSPEECH, Graz, Austria,",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": ""
        },
        {
          "7. REFERENCES": "",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "systems, Montreal, Canada, 2014, pp. 2672–2680."
        },
        {
          "7. REFERENCES": "2019, pp. 3945–3949.",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": ""
        },
        {
          "7. REFERENCES": "",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "[28] E. Tzeng, J. Hoffman, T. Darrell, and K. Saenko, “Simultane-"
        },
        {
          "7. REFERENCES": "[13] T. M. Chaplin,\n“Gender and emotion expression: A develop-",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": ""
        },
        {
          "7. REFERENCES": "",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "ous deep transfer across domains and tasks,” in Proceedings of"
        },
        {
          "7. REFERENCES": "mental contextual perspective,” Emotion Review, vol. 7, no. 1,",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": ""
        },
        {
          "7. REFERENCES": "",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "the IEEE International Conference on Computer Vision, Santi-"
        },
        {
          "7. REFERENCES": "pp. 14–21, 2015.",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": ""
        },
        {
          "7. REFERENCES": "",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "ago, Chile, pp. 4068–4076."
        },
        {
          "7. REFERENCES": "[14] M. Alvi, A. Zisserman, and C. Nell˚aker, “Turning a blind eye:",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "[29]\nS. Albanie, A. Nagrani, A. Vedaldi, and A. Zisserman, “Emo-"
        },
        {
          "7. REFERENCES": "Explicit removal of biases and variation from deep neural net-",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "tion recognition in speech using cross-modal\ntransfer\nin the"
        },
        {
          "7. REFERENCES": "the European Confer-\nwork embeddings,”\nin Proceedings of",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "the 26th ACM international confer-\nwild,”\nin Proceedings of"
        },
        {
          "7. REFERENCES": "ence on Computer Vision (ECCV), Munich, Germany, 2018,",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "ence on Multimedia, Seoul, Republic of Korea, pp. 292–301."
        },
        {
          "7. REFERENCES": "pp. 556–572.",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "[30] C. Busso, S. Parthasarathy, A. Burmania, M. AbdelWahab,"
        },
        {
          "7. REFERENCES": "",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "N. Sadoughi, and E. M. Provost, “Msp-improv: An acted cor-"
        },
        {
          "7. REFERENCES": "[15] M. Abdelwahab and C. Busso, “Domain adversarial for acous-",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": ""
        },
        {
          "7. REFERENCES": "",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "pus of dyadic interactions to study emotion perception,” IEEE"
        },
        {
          "7. REFERENCES": "IEEE/ACM Transactions on Audio,\ntic emotion recognition,”",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": ""
        },
        {
          "7. REFERENCES": "",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "Transactions on Affective Computing, vol. 8, no. 1, pp. 67–80,"
        },
        {
          "7. REFERENCES": "Speech, and Language Processing, vol. 26, no. 12, pp. 2423–",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": ""
        },
        {
          "7. REFERENCES": "",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": "2016."
        },
        {
          "7. REFERENCES": "2435, 2018.",
          "[16] Q. Mao, G. Xu, W. Xue,\nJ. Gou,\nand Y. Zhan,\n“Learning": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Toward an affect-sensitive multimodal human-computer interaction",
      "authors": [
        "M Pantic",
        "J Rothkrantz"
      ],
      "year": "2003",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "3",
      "title": "Communication without words",
      "authors": [
        "A Mehrabian"
      ],
      "year": "2008",
      "venue": "Communication theory"
    },
    {
      "citation_id": "4",
      "title": "Deep learning for robust feature generation in audiovisual emotion recognition",
      "authors": [
        "Y Kim",
        "H Lee",
        "E Provost"
      ],
      "year": "2013",
      "venue": "2013 IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "5",
      "title": "Recognizing human emotional state from audiovisual signals",
      "authors": [
        "Y Wang",
        "L Guan"
      ],
      "year": "2008",
      "venue": "IEEE transactions on multimedia"
    },
    {
      "citation_id": "6",
      "title": "Audio-visual based emotion recognition-a new approach",
      "authors": [
        "M Song",
        "J Bu",
        "C Chen",
        "N Li"
      ],
      "year": "2004",
      "venue": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "7",
      "title": "Jointly predicting arousal, valence and dominance with multi-task learning.,\" in Interspeech",
      "authors": [
        "S Parthasarathy",
        "C Busso"
      ],
      "year": "2017",
      "venue": "Jointly predicting arousal, valence and dominance with multi-task learning.,\" in Interspeech"
    },
    {
      "citation_id": "8",
      "title": "Improved end-to-end speech emotion recognition using self attention mechanism and multitask learning",
      "authors": [
        "Y Li",
        "T Zhao",
        "T Kawahara"
      ],
      "year": "2019",
      "venue": "Improved end-to-end speech emotion recognition using self attention mechanism and multitask learning"
    },
    {
      "citation_id": "9",
      "title": "Towards speech emotion recognition\" in the wild\" using aggregated corpora and deep multi-task learning",
      "authors": [
        "J Kim",
        "G Englebienne",
        "P Truong",
        "V Evers"
      ],
      "year": "2017",
      "venue": "Towards speech emotion recognition\" in the wild\" using aggregated corpora and deep multi-task learning",
      "arxiv": "arXiv:1708.03920"
    },
    {
      "citation_id": "10",
      "title": "Cross-corpus acoustic emotion recognition with multi-task learning: Seeking common ground while preserving differences",
      "authors": [
        "B Zhang",
        "E Provost",
        "G Essl"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "Model-protected multi-task learning",
      "authors": [
        "J Liang",
        "Z Liu",
        "J Zhou",
        "X Jiang",
        "C Zhang",
        "F Wang"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "12",
      "title": "Learning what to share: Leaky multi-task network for text classification",
      "authors": [
        "L Xiao",
        "H Zhang",
        "W Chen",
        "Y Wang",
        "Y Jin"
      ],
      "year": "2018",
      "venue": "Proceedings of the 27th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "13",
      "title": "Disentangling style factors from speaker representations.,\" in INTERSPEECH",
      "authors": [
        "J Williams",
        "S King"
      ],
      "year": "2019",
      "venue": "Disentangling style factors from speaker representations.,\" in INTERSPEECH"
    },
    {
      "citation_id": "14",
      "title": "Gender and emotion expression: A developmental contextual perspective",
      "authors": [
        "T Chaplin"
      ],
      "year": "2015",
      "venue": "Emotion Review"
    },
    {
      "citation_id": "15",
      "title": "Turning a blind eye: Explicit removal of biases and variation from deep neural network embeddings",
      "authors": [
        "M Alvi",
        "A Zisserman",
        "C Nellåker"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "16",
      "title": "Domain adversarial for acoustic emotion recognition",
      "authors": [
        "M Abdelwahab",
        "C Busso"
      ],
      "year": "2018",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "17",
      "title": "Learning emotion-discriminative and domain-invariant features for domain adaptation in speech emotion recognition",
      "authors": [
        "Q Mao",
        "G Xu",
        "W Xue",
        "J Gou",
        "Y Zhan"
      ],
      "year": "2017",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "18",
      "title": "Towards adversarial learning of speaker-invariant representation for speech emotion recognition",
      "authors": [
        "M Tu",
        "Y Tang",
        "J Huang",
        "X He",
        "B Zhou"
      ],
      "year": "2019",
      "venue": "Towards adversarial learning of speaker-invariant representation for speech emotion recognition",
      "arxiv": "arXiv:1903.09606"
    },
    {
      "citation_id": "19",
      "title": "Speaker-invariant affective representation learning via adversarial training",
      "authors": [
        "H Li",
        "M Tu",
        "J Huang",
        "S Narayanan",
        "P Georgiou"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "20",
      "title": "Disentangled speaker and nuisance attribute embedding for robust speaker verification",
      "authors": [
        "W Kang",
        "S Mun",
        "M Han",
        "N Kim"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "21",
      "title": "Privacy enhanced multimodal neural representations for emotion recognition",
      "authors": [
        "E Provost"
      ],
      "year": "2020",
      "venue": "AAAI"
    },
    {
      "citation_id": "22",
      "title": "A study of speaker verification performance with expressive speech",
      "authors": [
        "S Parthasarathy",
        "C Zhang",
        "J Hansen",
        "C Busso"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "23",
      "title": "Study on speaker verification on emotional speech",
      "authors": [
        "W Wu",
        "T Zheng",
        "M Xu",
        "H Bao"
      ],
      "year": "2006",
      "venue": "Ninth International Conference on Spoken Language Processing"
    },
    {
      "citation_id": "24",
      "title": "Emotion attribute projection for speaker recognition on emotional speech",
      "authors": [
        "H Bao",
        "M Xu",
        "T Zheng"
      ],
      "year": "2007",
      "venue": "Eighth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "25",
      "title": "Neural network based feature transformation for emotion independent speaker identification",
      "authors": [
        "S Krothapalli",
        "J Yadav",
        "S Sarkar",
        "G Koolagudi",
        "A Vuppala"
      ],
      "year": "2012",
      "venue": "International Journal of Speech Technology"
    },
    {
      "citation_id": "26",
      "title": "Disentangled speech embeddings using cross-modal selfsupervision",
      "authors": [
        "A Nagrani",
        "J Chung",
        "S Albanie",
        "A Zisserman"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "27",
      "title": "Unsupervised domain adaptation by backpropagation",
      "authors": [
        "Y Ganin",
        "V Lempitsky"
      ],
      "year": "2015",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "28",
      "title": "Generative adversarial nets",
      "authors": [
        "I Goodfellow",
        "J Pouget-Abadie",
        "M Mirza",
        "B Xu",
        "D Warde-Farley",
        "S Ozair",
        "A Courville",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "29",
      "title": "Simultaneous deep transfer across domains and tasks",
      "authors": [
        "E Tzeng",
        "J Hoffman",
        "T Darrell",
        "K Saenko"
      ],
      "venue": "Proceedings of the IEEE International Conference on Computer Vision, Santiago"
    },
    {
      "citation_id": "30",
      "title": "Emotion recognition in speech using cross-modal transfer in the wild",
      "authors": [
        "S Albanie",
        "A Nagrani",
        "A Vedaldi",
        "A Zisserman"
      ],
      "venue": "Proceedings of the 26th ACM international conference on Multimedia"
    },
    {
      "citation_id": "31",
      "title": "Msp-improv: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "C Busso",
        "S Parthasarathy",
        "A Burmania",
        "M Abdelwahab",
        "N Sadoughi",
        "E Provost"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    }
  ]
}