{
  "paper_id": "2509.21381v1",
  "title": "Toward A Realistic Encoding Model Of Auditory Affective Understanding In The Brain",
  "published": "2025-09-23T14:52:11Z",
  "authors": [
    "Guandong Pan",
    "Yaqian Yang",
    "Shi Chen",
    "Xin Wang",
    "Longzhao Liu",
    "Hongwei Zheng",
    "Shaoting Tang"
  ],
  "keywords": [
    "Neural encoding",
    "Naturalistic Auditory Stimuli",
    "Neural Synchrony",
    "Affective Understanding",
    "Electroencephalogram (EEG)"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In affective neuroscience and emotion-aware AI, understanding how complex auditory stimuli drive emotion arousal dynamics remains unresolved. This study introduces a neurobiologically informed computational framework to model the brain's encoding of naturalistic auditory inputs into dynamic behavioral/neural responses across three datasets (SEED, LIRIS, self-collected BAVE). Guided by neurobiological principles of parallel auditory hierarchy, we decompose audio into multilevel auditory features (through classical algorithms and wav2vec 2.0/Hubert) from the original and isolated human voice/background soundtrack elements, mapping them to emotion-related responses via cross-dataset analyses. Our analysis reveals that high-level semantic representations (derived from the final layer of wav2vec 2.0/Hubert) exert a dominant role in emotion encoding, outperforming low-level acoustic features with significantly stronger mappings to behavioral annotations and dynamic neural synchrony across most brain regions (p < 0.05). Notably, middle layers of wav2vec 2.0/hubert (balancing acoustic-semantic information) surpass the final layers in emotion induction across datasets. Moreover, human voices and soundtracks show datasetdependent emotion-evoking biases aligned with stimulus energy distribution (e.g., LIRIS favors soundtracks due to higher background energy), with neural analyses indicating voices dominate prefrontal/temporal activity while soundtracks excel in limbic regions. By integrating affective computing and neuroscience, this work uncovers hierarchical mechanisms of auditory-emotion encoding, providing a foundation for adaptive emotion-aware systems and cross-disciplinary explorations of audio-affective interactions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotion serves as a critical window through which humans engage with the world. Quantifying how environmental stimuli drive emotion dynamics is vital for advancing affective neuroscience and emotion-aware AI  [1] -  [3] . As a well-established approach, the naturalistic paradigm employs real-world audiovisual inputs to elicit time-varying emotional responses in the brain  [4] -  [7] . Under naturalistic conditions, recent studies have made significant progress in recognizing/predicting emotions (e.g., discrete categories like positive or negative) from multimedia or physiological signals  [4] -  [7] . However, understanding how complex, naturalistic auditory stimuli dynamically modulate the fluctuations of emotion arousal is still a critical gap, which has been overlooked by traditional emotion recognition frameworks. Here, by integrating neurobiological principles of auditory processing with advanced computational modeling and multimodal data (stimuli data, behavioral annotations, EEG), we investigate how the brain encodes naturalistic auditory inputs into emotion arousal dynamics.\n\nThe audio decomposition strategies employed in this work are grounded in the principles of hierarchical and parallel auditory processing  [8] ,  [9] , and attentional mechanisms  [10] ,  [11]  in the human brain. Neuroscience research under the naturalistic paradigm has illuminated how the brain processes complex real-world inputs  [12] . For example, studies of speech processing  [8]  reveal a multi-level auditory pathway-from subcortical structures like the auditory nerve to primary and non-primary auditory cortices-where lowerlevel regions (e.g., subcortical, primary cortex) handle basic acoustic features, while non-primary cortices specialize in abstract linguistic processing (semantics, grammar). Parallel processing of sound information across these hierarchical representations has also been demonstrated  [9] . Additional works on attentional focus and the global workspace theory  [10] ,  [11]  highlight that in complex acoustic environments, neural activity correlates with selectively attended elements. Yet, how specific levels/elements of audio information contribute to emotion induction-both in terms of mechanism and magnitude-remains understudied. Building on classical feature extraction algorithms  [13] -  [15]  and deep learningbased methods  [16] -  [18] , we decompose audio into hierarchical (from acoustic to semantic) and multi-element (original audio, isolated voice, isolated soundtrack) components, then quantitatively model their associations with emotion-related responses while excluding the effect of visual features, as outlined in our research framework (Fig.  1 ).\n\nRecent advanced deep neural network (DNN) models have provided powerful capabilities of semantic information representation and factor decomposition, and have been widely used in the encoding study for brain information processing  [20] . Researchers  [21] -  [23]  have provided evidence that text-based language models and brain language processing both follow the predictive coding theory. On the basis of these studies, Fig.  1 . Framework for quantitative modeling of auditory-driven emotion arousal encoding in the brain. We integrate naturalistic stimuli from datasets (SEED, LIRIS, BAVE; total 9h) and analyze emotion-elicitation mechanisms of hierarchical/multi-element audio features (original audio, isolated human voice or background soundtrack) extracted via classical algorithms and deep networks (Wav2vec 2.0/Hubert). To isolate audio-specific effects, visual features (lowlevel attributes and high-level CLIP features  [18] ) are first incorporated as covariates during model training and excluded during prediction (following  [19] ). Using raw and source-localized EEG, we compute group-level emotion-related dynamic neural synchrony for each electrode/brain region. Finally, we map decomposed audio factors to human-annotated continuous emotion arousal or dynamic neural synchrony (evaluated via cross-validated Pearson correlation) to reveal how the brain encodes auditory information into emotion arousal dynamics.\n\nLi et al.  [8]  and Millet et al.  [24]  explored the commonalities between the processing of speech in the human brain and DNN models, and revealed that the hierarchy of DNN models is similar to the hierarchical speech processing mechanisms in the human brain. Therefore, it is of great significance to combine the powerful representational capacity of DNN models for the study of the encoding process of emotions. This work will utilize the advanced tool spleeter  [17]  to decouple audio stimuli into character voices and background soundtrack, and integrate the cutting-edge audio model wav2vec 2.0  [16]  and Hubert  [25]  to represent the hierarchical audio information.\n\nSecond-scale dynamics of emotion arousal are a prerequisite to investigating how specific components within naturalistic stimuli exert their influence on emotional responses. Moreover, under group-level analysis, neural activities or behavioral data can disregard individual variations  [26] , facilitating our comprehension of the general mechanisms underlying the affective encoding of audio stimuli. In the field of neuroscience, neural synchrony has been demonstrated to be closely associated with collective emotional experience  [27] ,  [28] . The intersubject correlation (ISC), initially introduced by Hasson et al.  [29] , has become a fundamental method for identifying neural synchrony among individuals. Compared with other imaging modalities such as functional magnetic resonance imaging (fMRI), electroencephalography (EEG) presents advantages owing to its high temporal resolution  [4] , which is crucial for capturing subtle changes in neural responses. Multiple studies  [30] -  [32]  have verified that EEG-based neural synchrony is closely related to the audience preference or the structure of stimuli. Dmochowski et al.  [30] ,  [33]  explored dynamic neural synchrony and successfully linked it to movie scenes. Pan et al.  [34]  identified the reliability of single-electrodebased dynamic neural synchrony, allowing for a comparative analysis of regional differences in neural synchrony. Moreover, the raw EEG signals recorded from the scalp are a complex mixture of electrical activities originating from multiple neural sources within the brain. Source localization techniques can estimate the actual neural activities in different brain regions  [35] , which is beneficial for more accurately uncovering the neural mechanisms underlying emotion generation. Therefore, in this paper, we extract dynamic neural synchrony from neural activities of different channels/regions (raw EEG electrodes or estimated brain regions) respectively to obtain emotionrelated neural responses. The dynamic neural data of each channel/region and behavioral data at the group level are respectively modeled with audio features to study the general emotion arousal encoding process.\n\nThis study makes three key contributions by constructing a realistic encoding framework for the brain's auditory affective understanding. First, we demonstrate that semantic auditory representations (derived from the final layer of wav2vec 2.0/Hubert) exert a more dominant role in emotion encoding than low-level acoustic features, as evidenced by stronger mappings to behavioral annotations and dynamic neural synchrony across most brain regions. Second, middle layers of wav2vec 2.0 and hubert (balancing acoustic and semantic information) outperform the final layer in emotion induction, a finding robust across three datasets. Third, we show heterogeneous emotion-evoking effects of human voices and background soundtracks across datasets, with element dominance linked to stimulus-specific attributes (e.g., energy distribution). These findings not only deepen our understanding of the underlying mechanisms but also pave the way for future research in this area, setting the stage for further exploration and application of audio-emotion interactions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Materials And Methods",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Datasets",
      "text": "This work utilizes two emotion-related public datasets, one self-collected annotation dataset, and one self-collected EEG dataset. Public datasets: (i) the SEED EEG dataset  [4] , containing 62-channel EEG data from 45 experiments with 15 approximately 4-minute video clips, and (ii) the LIRIS video dataset  [36]  with 30 full movies (10 minutes-1 hour) annotated for arousal by 5 annotators. For emotion arousal annotation of SEED video clips, we recruited 1061 individuals to participate in the experiment. This resulted in 600 valid annotation sequences, with each movie clip getting 40 continuous annotations. Additionally, the EEG dataset (Beihang Audio-Visual EEG dataset, BAVE) was self-collected from 65 subjects (3 excluded due to data quality issues) while they viewed three comedy videos: a cross-talk (audio-only), a mime (visual-only) and a skit (audio-visual), each approximately 15 minutes in duration. Each participant in the experiment received a reward of 150 yuan. Only the cross-talk and skit are utilized in this study. The used sample comprised 43 males and 19 females, with ages ranging from 18 to 34 years (M = 22.16, SD = 9.15). EEG signals were recorded at a 1000 Hz sampling rate using a 64-channel ESI Neuroscan System (with M1 and M2 as reference channels, resulting 62-channel EEG data), along with resting-state sessions and post-video self-assessment questionnaires. Fig.  2  shows the attention level and emotion arousal level of the audience's ratings for the two videos. The average scores higher than 3 indicate that the two videos have high attractiveness. All procedures were ethically approved. Detailed collection procedure see Supplementary Appendix I.A.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. Eeg Data Preprocessing",
      "text": "The preprocessed EEG data collection of SEED is used in our work, which has been sampled at 200 Hz and filtered from 0-75 Hz. We have performed the same operation on our selfcollected BAVE EEG dataset. Moreover, we conduct further preprocessing using EEGLAB toolbox  [37]  in MATLAB. A notch filter (48-52 Hz) is applied to remove power line interference. Eye movements and muscle artifacts are decomposed and removed by the independent component analysis (ICA) technique in EEGLAB.\n\nSource localization. Brain source localization maps scalprecorded EEG signals to underlying cortical activity, enabling emotion-related neural correlates to be localized in brain regions. We perform source localization using the MNE library  [38] , following a standardized pipeline: 1) constructing the source space and boundary element model (BEM) for brain tissue conductivity; 2) configuring electrode montages and reference channels for each EEG dataset; 3) computing forward solutions to link brain sources with scalp EEG signals, estimating noise covariance matrices, generating inverse operators, and applying dynamic statistical parametric mapping (dSPM) to raw data for source activity estimation. Anatomical timeseries are extracted using the aparc parcellation scheme  [39] , yielding 68 left/right hemisphere regions. For visualizations of electrode positioning and MRI alignment across datasets, see Supplementary Appendix I.F.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Emotion-Related Response Acquisition",
      "text": "Dynamic neural synchrony extraction. We utilize the state-of-the-art dynamic neural synchrony method in EEG  [34] , which measures the similarity of single-channel brain activities among the subjects in the present study (exposed to identical naturalistic stimuli). This approach has the advantage of capturing shared cognitive information without predetermined hypotheses and isolating heterogeneous cognitive processes from different channels/cerebral cortices for regional comparison. Previous studies have shown that neural synchrony is closely related to emotion  [27] ,  [30] , hence we use dynamic neural synchrony here to reflect changes related to emotion arousal.\n\nWe apply this technique to both EEG datasets, each including both the raw EEG data (eeg) and the time-series of source activity (brain). The implementation method is briefly introduced below; for details, refer to  [34] . We measure the similarity of each pairwise time sequences under each window using a sliding window method for each video clip and each channel. By averaging the dynamic correlations of all pairwise subjects under the corresponding windows, we obtain a population-level dynamic neural synchrony sequence for each clip. Pearson correlation coefficient (PCC) is used for similarity measurement, and the window size is set to 10 seconds (see Supplementary Appendix II.A for sensitivity analysis of the sliding window size), and the step size is 1 second. Before correlation, EEG is processed as the first-order differential feature to improve performance, following  [34] . The Python library taichi  [40]  is used to compute sequences in parallel to accelerate the computation.\n\nHuman annotated dynamic emotion arousal. LIRIS dataset provides a 1 Hz continuous emotion arousal label for each movie clip by 5 participants. For the SEED dataset, we used the online platform Naodao to publicly recruit a group of Chinese native people to watch movie materials in SEED twice and collect their annotation in the second time, namely the post-hoc manner  [41] . The annotators reported their dynamic emotion arousal using a slider in the website. The experiment program was designed by Psychopy  [42] . Each movie clip was reported by 40 different annotators. We paid each annotator 3 yuan for each 4-min around film clip. An annotation quality check module was used to ensure data quality. We average annotation sequences of each clip across annotators to gain the final continuous emotional responses. Detailed collection procedure and data processing are available in Supplementary Appendix I.A.\n\nSplit-half similarity of emotion-related responses. To validate group-level reliability of emotion-related responses, we perform 100 rounds of randomly splitting the subjects into two separate groups, comparing intergroup similarity of behavioral/neural dynamics. All measures-human annotations, raw EEG, and brain source activity-show non-zero correlations (p < 10 -9 , Fig.  3 ), confirming emotion-related response consistency across subjects.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "D. Characteristics Of Naturalistic Stimuli",
      "text": "Features from classical feature extraction algorithms. Acoustic features reflect core sound properties (e.g., pitch, timbre), and their capacity to discriminate emotional information has been widely acknowledged in prior speech emotion recognition research  [14] . In this work, we extract 12 low-level descriptors (LLDs) to characterize acoustic information: logarithm energy spectrum (LES), short-time energy (STE), zerocrossing rate (ZCR), sound pressure level (SPL), along with their first-order difference features and second-order difference features. These LLDs are computed for each one-second frame. Detailed mathematical formulations of the LLDs are provided in Supplementary Appendix I.B. The considerations for feature selection are also provided in Supplementary Appendix I.C.\n\nRepresentations derived from deep learning. Previous research has shown that activations of DNNs can deliver rich semantic information compared to traditional feature extraction methods  [19] ,  [23] ,  [43] . Recent studies  [24] ,  [44]  based on the self-supervised speech model (e.g., wav2vec 2.0  [16]  and Hubert  [25] ) have established links between model activations and brain dynamics. Following this paradigm, we employ both wav2vec 2.0 and Hubert to map audio representations to emotions, using pre-trained weights from  [45] . Both models share a similar architecture: wav2vec 2.0 consists of a 7-layer convolutional feature encoder and 12 transformer blocks f : X → C, taking raw audio X (16 kHz waveform) as input and outputting semantic representations c 1 , c 2 , ..., c T  [16] ; Hubert follows a comparable design but with enhanced contrastive learning for speech units  [25] . The transformer layer output C has a 768-dimensional feature space (50 Hz frequency, 20 ms stride), while the convolutional layer yields 512-dimensional features. To ensure fair comparison with low-level descriptors (LLDs), we apply PCA to reduce semantic features to 12 dimensions (see Supplementary Appendix II.B for sensitivity analysis of dimensionality reduction). Supplementary Appendix I.D describes the characterization of wav2vec 2.0 representations in detail. Briefly, semantic features tend to derive primarily from the 12-layer Transformer blocks, while their 7-layer convolutional encoders appear to capture lowlevel acoustic representations.\n\nElement separation. Guided by the neurobiological principle of parallel processing  [9] , we decompose audio into human voice and background elements, as the brain's auditory cortex processes speech and non-speech sounds through distinct neural pathways. To isolate human voices and background soundtracks from audio clips across the SEED, LIRIS, and BAVE datasets, we employ the pre-trained 2-stem model of the Python library Spleeter [17]-a state-of-the-art tool specialized for vocal-background audio separation. Specifically, human voice components encompass character dialogues, monologues, and voice-overs, while background soundtracks comprise accompanying music and sound effects. The robustness of this separation is validated via qualitative audio demonstrations, human perceptual evaluations (e.g., Likertscale clarity ratings), and cross-model comparisons (vs. alternative separators like Open-Unmix  [46] ) on the SEED dataset, with detailed results provided in Supplementary Appendix I.E. Multilevel features (as introduced in Sections on classical algorithm-derived features and deep learning-derived representations) are further extracted from the two separated audio elements for subsequent emotion encoding analyses.\n\nVisual features. To better isolate audio-specific effects and enable an unbiased analysis, we extract both classical low-level visual features and dimensionality-reduced high-level visual features, incorporating them as covariates for model estimation to account for their contribution to emotion-related responses. These visual covariates are subsequently excluded during the prediction phase to emphasize the independent influence of auditory features. This methodological approach is consistent with the strategy utilized by Huth et al.  [19] . For low-level visual features, we compute classical attributes on a per-second basis, including color, brightness, and texture features, among others  [47] ; detailed extraction procedures are provided in Supplementary Appendix I.B.\n\nFor high-level visual features, we utilize the CLIP model (openai/clip-vit-base-patch32)  [18] , a vision-language model proven effective in capturing abstract visual semantics. The 768-dimensional high-level visual features extracted by this model are reduced to 12 dimensions via PCA, matching the dimensionality of our high-level audio features (such as H P CA ). These reduced high-level visual features are concatenated with the low-level visual features to form a combined covariate set, which is included during model training. In the prediction phase, however, our focus remains exclusively on the contribution of audio features.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "E. Emotion Score",
      "text": "After extracting auditory features, visual features and emotion-related responses, we explore how audio factors are encoded into emotion arousal dynamics. The emotion score, calculated through systematic methodological steps, serves as a pivotal indicator in this context: it directly quantifies the performance of audio information encoding by measuring the correlation between predicted emotion-related responses (derived from model prediction) and actual emotion-related responses (i.e., behavioral annotations or raw/source-localized dynamic neural synchrony). This metric is motivated by the need to rigorously assess how well auditory features capture and translate into emotional content, with its validity underpinned by a series of methodological procedures and related encoding literature  [19] ,  [23] ,  [24] .\n\nTemporal alignment. Dynamic neural synchrony uses a 10s sliding window, which results in its sequence length being smaller than the video length. Therefore, to ensure the same sample length, we perform the sliding window process on all other kinds of data and average the data of each window. Step size is set to 1 second. Among them, high-level features are aligned by intercepting the input audio with n sliding windows and averaging the output representation of each window across the time scale.\n\nData partition. In this work, we mainly use the stratified five-fold cross-validation method to evaluate the performance of the encoding. Specifically, in each evaluation, the stratified partition randomly divides one-fifth of multi-modal samples of each video into one of the five subsets. Each subset is used as a test set once, while the other four subsets are used as the training set. The average score of all 5 subsets as test set can be regarded as one emotion score.\n\nPenalized linear regression. Following  [19] ,  [23] ,  [24] , we build the linear mapping with l 2 -penalized standard linear model (Ridge Regression) to encode emotion-related responses given the audio features and visual features (see Supplementary Appendix II.D for linear-nonlinear comparison). These emotion-related responses include behavioral data, dynamic neural synchrony for each electrode, and dynamic neural synchrony for each brain region after source localization, with each computed separately. Data standardization is applied to the samples. The RidgeCV function  [48]  is used with the penalization parameter λ ranging from 10 to 10 8 (20 values scaled logarithmically). λ is independently chosen for each model with a nested five-fold cross-validation over the train set. The formula of the optimization is the following:\n\nwhere V denotes the weight vector of the linear model, Xi represents the concatenated feature vector (auditory + visual features) of the i-th sample, and y i is the corresponding emotion-related response (behavioral annotation or dynamic neural synchrony).\n\nEvaluation. Assuming the linear mapping as W , we quantify the correlation between W • X test and Y test to evaluate the encoding ability from auditory features to emotion-related responses. Specifically, we extract visual features and incorporate them as covariates in our encoding models: during training, visual features are concatenated with auditory features to account for their potential influence; during prediction, visual feature values are set to 0 to isolate the unique contribution of auditory features. This approach, aligned with Huth et al.  [19] , directly quantifies audio-specific effects by controlling visual confounds. To further validate the independence of auditory contributions and characterize visual modality's impact on emotion encoding, we designed ablation experiments across four modality combinations (detailed in Supplementary Appendix II.E), comparing auditory-only, visual-only, and covariate-integrated models.\n\nwhere PCC means Pearson correlation coefficient.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "F. Stepwise Regression",
      "text": "In the joint modeling of acoustic and semantic features from the original audio, along with semantic features from character voices and soundtracks, we employ stepwise regression to systematically quantify the incremental contributions of these features to encoding capabilities of emotion arousal. Given the high dimensionality of semantic features, we first apply PCA to the semantic features (to reduce dimensionality to 12 dimensions, see Supplementary Appendix II.B for sensitivity analysis of dimensionality reduction) prior to conducting the analyses.\n\nSpecifically, we concatenate the two corresponding feature groups (acoustic and semantic, or voices and soundtracks) and train the model iteratively by incrementally adding one feature at a time, starting from a single feature. Each training iteration utilizes five-fold cross-validation to compute emotion scores.\n\nTo mitigate the risk of spurious significance arising from arbitrary variable ordering, a known concern with stepwise regression  [49] , we randomize the order of variables within each feature group and repeat the entire stepwise analysis 100 times. This procedure aligns with similar practices in neuroscience for dissecting incremental feature effects  [50] .\n\nFurthermore, manipulating the order in which the two feature groups are added to the encoding model allows for explicit quantification of the incremental contributions of hierarchical features, as well as comparisons between voice and soundtrack features.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "G. Statistics",
      "text": "In split-half experiments examining emotion-related responses, we employ the Wilcoxon signed-rank test to determine if the distribution of split-half similarity (e.g., intergroup correlation coefficients) deviates significantly from zero. A non-significant result would suggest that the observed consistency in emotion-related responses could be ascribed to random chance. Conversely, a significant outcome corroborates that the similarity across groups reflects non-random reliability inherent to emotion-related processing.\n\nFor significant-mapping experiments, the Mann-Whitney U test is utilized to identify annotations/EEG channels/sourcelocalized brain regions where the empirical distribution (real data) differs significantly from the null distribution. Regarding neural mappings, to account for the increased likelihood of Type I errors due to multiple comparisons, we apply the Benjamini/Hochberg False Discovery Rate (FDR) correction  [51] . This approach balances the need to detect true effects while controlling the proportion of falsely rejected null hypotheses, making it well-suited for exploratory analyses where identifying potential effects is prioritized. In experiments comparing score differences across brain regions, the Mann-Whitney U test is used to assess between-group differences in emotion scores for each region. Here, we employ the Family-Wise Error Rate (FWER) correction to control the probability of making at least one Type I error across all tested regions.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Iii. Results",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Multilevel Auditory Features Of The Original Audio And The Isolated Elements Can Be Significantly Mapped To Emotionrelated Responses",
      "text": "We conducted cross-dataset experiments to explore whether multilevel auditory features associate with behavioral and neural emotion-related responses. For dynamic neural synchrony, Fig.  4a  and Fig.  6a  visualize emotion scores quantifying the link between auditory features (low-level descriptors, L, the reduced last layer representations of the wav2vec 2.0 model, H P CA ) and dynamic neural synchrony of raw EEG signals across 62 electrodes. Electrodes with empirical scores exceeding null distributions (model training with shuffled labels, p < 10 -3 , FDR-corrected) are denoted by solid circles; others appear as hollow circles. The top 15 electrodes (by score) in each subplot are labeled, highlighting significant activity in temporal, prefrontal, and occipital regions. Source localization analyses in Supplementary Fig.  5  and Fig.  6  further show colored brain regions with emotion scores differing significantly from null distributions, demonstrating widespread neural engagement with auditory features.\n\nFor behavioral annotations, Fig.  4b  presents the results of modeling three audio feature types (L, H and H P CA ) from original audios alongside continuous arousal ratings or shuffled ratings in SEED/LIRIS, with emotion scores derived through this modeling process. In contrast, Fig.  6b  compares the emotion scores of voice/background elements against their respective null distributions to assess statistical significance. All feature types exhibit significant encoding of emotional signals, with behavioral scores surpassing null expectations (p < 10 -5 for both datasets and three audio elements).\n\nIn addition to the final layer of wav2vec 2.0 model (H and H P CA ), we also investigated the emotion-related associations of audio features from each layer of wav2vec 2.0 (fulldimensional, unreduced) with behavioral and neural responses. Behavioral results (Fig.  5a ) and source-localized dynamic neural synchrony (Fig.  5c, 5d ) depict emotion scores across model layers in the original audio, with null distributions visualized as gray bar plots/violin plots. Layerwise wav2vec 2.0 features extracted from human voice and background music elements are presented in Supplementary Fig.  7 . Results show significant emotion scores compared with null distribution across all layers in three audio elements, confirming hierarchical emotional representation where both shallow acoustic and deep semantic features contribute to emotionrelated neural/behavioral responses.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "B. The Dominant Role Of Semantic Representations Over Acoustic Features In Emotion Encoding Across Most Brain Regions",
      "text": "Considering the hierarchical and parallel processing mechanisms of auditory perception  [8] ,  [9] , we intend to investigate which level of auditory information has a stronger effect on inducing emotions. Here, we first compared the manually extracted low-level features (L) with the semantic features of the last layer of the wav2vec 2.0 model (H, H P CA ). Supplementary Fig.  4  shows that reducing wav2vec 2.0 features from 768 to 12 dimensions preserves critical variance information. Fig.  4b  shows that both full-dimension and PCAreduced semantic features (H, H P CA ) exhibit significantly higher emotion scores than acoustic features (L) on the behavioral mapping, with p < 0.001 for both SEED and LIRIS datasets. Additionally, across SEED and BAVE datasets, we calculated score differences between semantic and acoustic features for each source-localized brain region, statistically assessing significance (p < 0.05, FWER-corrected). Fig.  4c  highlights regions with significant differences: in most cortical areas, semantic features outperform acoustic features. The right hemisphere's rostral anterior cingulate cortex (rACC-rh, ∆score = 0.15) and frontal pole (FP-rh, ∆score = 0.62) show the highest score differences in SEED and BAVE respectively, prompting their selection for subsequent analysis.\n\nThen, we used stepwise regression (detailed in Section II-F)  [50]  to explicitly quantify the incremental contribution of hierarchical features on emotional encoding. Briefly, we concatenated two feature groups (L and H P CA ) and iteratively added features to compute emotion scores. The order of variables within each feature group were randomized and repeated 100 times. Fig.  4d -f depict the results between joint hierarchical features and emotion-related responses across three datasets. Results show that when acoustic features are added later, emotion scores plateau rapidly in both behavioral and neural mapping analyses (small blue shaded areas), whereas adding semantic features later lead to significant improvements (red shaded areas). This pattern indicates that semantic features drive emotion encoding beyond acoustic cues.\n\nAdditionally, we extended our analysis to compare emotion scores between the full-dimensional final layer and first layer of the wav2vec 2.0 model. Consistent with prior studies showing that wav2vec 2.0's shallow layers prioritize acoustic features while deep layers encode semantic information  [52] ,  [53] , we hypothesized that deep-layer representations would outperform shallow-layer features in emotional encoding. Comparative results in Fig.  5e  and Fig.  5f  demonstrate that average emotion scores from semantic representations (final layer) across brain regions are significantly higher than those from acoustic representations (first layer) at least 25% across all datasets.\n\nCollectively, these results demonstrate the dominant role of semantic features in emotional encoding than low-level acoustic features.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "C. Middle Layers Of Wav2Vec 2.0 And Hubert Demonstrate Enhanced Emotion Induction Compared To The Final Layer",
      "text": "Having confirmed that high-level semantic features (the final layer of wav2vec 2.0) outperform acoustic features (either handcrafted or from the model's first layer) in emotion encoding, we aim to probe deeper into the hierarchical representation space with the full feature dimension. This exploration is motivated by the nature of self-supervised models like wav2vec 2.0, which progressively transition from encoding primarily acoustic to semantic information across layers  [44] ,  [52] ,  [53] . Given this hierarchical trajectory, the question of which specific auditory information optimally supports emotion encoding remains unresolved.\n\nLayer-wise analysis of wav2vec 2.0 across all 19 layers (Fig.  5a, 5c, 5d ) shows that emotion scores follow a non-linear path (see Supplementary Fig.  7  for the isolated elements, see Supplementary Fig.  12  for hierarchical effects of categorized lobes). They are low in shallow, acoustic-dominated layers,\n\n, reflecting neural preference for multi-scale integration. c,d Neural mapping (SEED, BAVE) shows emotion scores across layers (gray violins as null distributions, all p < 0.001), with non-linear trajectories matching behavioral results. e,f Layerwise score comparison shows middle layers (e.g., layer 8) outperform deep layers (e.g., layer 8 scores 48.2% higher than layer 0 vs. layer 18 scores 35.1% higher than layer 0 in SEED), highlighting them as a \"synergistic zone\" for emotion encoding.\n\nrise sharply in middle layers (e.g., layers 7-14), and plateau in deep, semantic-dominated ones. To validate the generalizability of this layer-wise trend across self-supervised audio models, layer-wise analysis of Hubert (Supplementary Fig.  10 ) also exhibits a consistent trend, with emotion scores showing analogous non-linear dynamics across its layers. Further analysis in Supplementary Fig.  11  controls for the full visual feature space (not just low-level visual features and high-level dimension-reduced subsets), confirming consistent auditory layer trends. Looking at the top 2 layers in 68 source-localized brain regions (Fig.  5b ), middle layers (especially 8, 10 and 14) are dominant. Layer 8, 10, and 14 rank top in over 25 regions, showing neural preference for these layers in integrating multiscale auditory information (see Supplementary Fig.  13  for similar results of top 1-5 layers distribution). Moreover, layerwise score comparisons (Fig.  5e , 5f), normalizing to layer 0 (layer 0 refers to the first layer of the convolutional feature encoder, following  [24] ), reveal middle layers (e.g., layer 8) outdo the final layer (layer 18). In SEED behavioral mapping, layer 8 scores are 48.2% higher than layer 0, beating layer 18's 35.1%, a pattern consistent across datasets. Notably, the middle layers (7-14) of wav2vec 2.0/Hubert maintain the same feature dimensions to the final layer (768D), ruling out dimensionality as a confounding factor for their superior emotion induction.\n\nBriefly, wav2vec 2.0's middle layers (7-14) balance acous-tic detail and semantic abstraction, driving stronger emotion encoding in behavioral and neural responses, even surpassing the deepest semantic layers. This refines our understanding: self-supervised models encode emotions most powerfully in intermediate, integrative representations, a key insight for future speech-emotion and neurocomputational work.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "D. The Representations Of Human Voices And Background Soundtracks Exhibit Heterogeneous Emotion-Evoking Effects On Three Datasets",
      "text": "When the human brain processes complex naturalistic auditory stimuli, multiple elements may contribute to emotional induction; however, the relative dominance of specific elements in driving emotional responses remains unclear. To address this, we first employed the deep neural network model Spleeter  [17]  to isolate two primary components: human voices and background soundtracks. We then extracted audio representations from these isolated elements and modeled their relationships with emotion-related responses to compare their differential capacities for emotion induction.\n\nBased on our prior finding that layer 8 of wav2vec 2.0 optimally balances acoustic and semantic information (Section III-C), we focus on layer 8 features to compare the emotionevoking effects of voices and soundtracks (full dimension H8 or with PCA reduction H8 P CA ). Comparative results for the model's final layer across these elements are provided in Supplementary Fig.  14 .\n\nBehavioral data analysis reveals contrasting patterns: in the LIRIS dataset, background soundtracks outperform voices in both H8 and H8 P CA (Fig.  6b ) and joint stepwise analysis (Fig.  6f ); While in SEED, the voice element yields higher emotion scores (Fig.  6b, 6f ). Neural modeling of sourcelevel activity of SEED (Fig.  6c ) shows voices dominate in regions like the language-related temporal cortex, with joint stepwise analyses highlighting voice primacy in cognitionrelated annotations and neural data of FP-rh (Fig.  6f, 6g ). However, the limbic rACC-rh favors background sounds (Fig.  6g ), aligning with its role in unconscious emotion arousal from ambient acoustic contexts  [28] . In the BAVE dataset, voices elicit stronger emotion-related neural responses across most brain regions, particularly in the prefrontal cortex (Fig.  6c, 6h ).\n\nDataset-specific dominance of voice or soundtrack elements prompts an investigation into stimulus-related factors. Computing root-mean-square energy of separated elements (Fig.  6d ) reveals LIRIS audios have higher background sound energy, SEED balances the energy of voice/sound, and BAVE minimizes background content. To further validate the link between audio attributes and encoding effects on audiolevel, we applied Leave-one-out (LOO) method to predict the element effect for each audio. Using human annotations for SEED/LIRIS and FP-rh neural data for BAVE, Fig.  6e  shows BAVE audios mainly trigger voice-mediated emotions, while LIRIS relies more on background sounds. The result of Leave-one-out effect predictions mirrors the trends of stimulus composition, linking element dominance to audio attribute heterogeneity.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Iv. Discussion",
      "text": "Unlike prior works in affective computing mainly focusing on static emotion recognition/prediction from physiological signals or stimulus features  [4] ,  [14] ,  [47] ,  [54] -  [56] , our study offers novel insights into the analysis of induction mechanisms using emotional dynamics. Conventional EEGbased emotion recognition datasets provide millisecond-level EEG data paired with film-level emotion labels (e.g., positive, neutral, negative)  [4] ,  [7] . Many studies assigned the same emotional label to the entire EEG data corresponding to a film clip, segmenting it into shorter samples for model training  [56] -  [58] . This approach ignores the dynamic changes in emotions during viewing, limiting the practical application of computational models. In contrast, our study employs behavioral markers and dynamic neural synchrony techniques to achieve multimodal dynamic emotion measurement, enabling cross-modal investigation with consistent results. Moreover, while previous work  [56] -  [58]  has emphasized the importance of EEG signals from the temporal lobe in emotion recognition, this region's dual role in primary auditory function and high-level language processing, combined with the complex elements (e.g., dialogue, background music) in the audios, necessitates a more nuanced approach. Our study integrates audio stimulus representation and decomposition techniques to elucidate the emotion-elicitation mechanisms of various auditory elements, thereby enhancing our understanding of the brain's emotional encoding processes.\n\nIn addition, our work adds a complementary dimension to affective neuroscience by shifting the focus from where emotions are represented in the brain to how naturalistic auditory stimuli drive dynamic emotional responses. Prior neuroimaging studies  [59] ,  [60]  have illuminated cortical emotion maps, characterizing spatial distributions of emotional states across the brain. However, these studies often lack mechanistic insights into how sensory features and stimulus components translate into neural and behavioral emotion dynamics. Building on DNN encoding frameworks in vision and language  [8] ,  [23] ,  [24] ,  [26] , we introduce a hierarchical and parallel modeling approach to quantify how distinct stimulus components drive emotion-related responses, a dimension understudied in prior emotion neuroscience. Specifically, our work uniquely decomposes audio stimuli using Spleeter  [17] , wav2vec 2.0  [16]  and Hubert  [25]  to reveal: (1) auditory features from middle model layers outperform shallow acoustic cues in emotion encoding (Fig.  5 , Supplementary Fig.  10 ), demonstrating prosodic and semantic abstraction as a key driver of emotional induction; (2) voices and soundtracks exhibit dataset-dependent emotion-evoking effects linked to stimulus energy distribution (Fig.  6 ), bridging complex auditory scene analysis with emotion science. By integrating multi-scale feature mapping (handcrafted + DNN-derived), cross-dataset validation (SEED/LIRIS/BAVE), and neural-behavioral convergence (dynamic neural synchrony/behavioral annotations), our framework provides a stimulus-driven perspective on emotion arousal-complementing prior cortical emotion mapping studies by illuminating the transition from sensory input to neural/behavioral output in auditory-emotional processing.\n\nBuilding on these scientific insights, the mechanistic framework developed here also holds practical implications for emotion-aware AI. For emotion-aware AI, affective understanding-unlike static recognition-requires quantifying how cognitive elements combine to elicit emotions. Our framework, which decomposes audio into different stimuli components to model their differential emotional contributions, provides a foundation for fine-grained emotion analysis in industries like advertising. For example, such models could optimize audio design by disentangling the emotional impacts of vocal prosody vs. musical elements, aligning with the need for expert systems that inform content creation strategies. However, translating these insights into real-world applications faces critical methodological challenges. First, the scarcity of time-aligned stimulus data and dynamic emotion annotation pairs hinders model training-an issue we address by expanding the annotation dataset at the behavioral level and using dynamic neural synchrony as a potential indicator of emotion arousal. Second, annotating the independent emotional impact of individual stimulus components (e.g., musical instruments in complex audio) remains prohibitively laborintensive, necessitating computational approaches to infer such contributions indirectly. These challenges underscore the need for hybrid frameworks that integrate data-driven modeling with theoretical priors, while highlighting opportunities for cross- ) or behavioral annotations (p < 10 -5 ) on three datasets (see Supplementary Fig.  14  for results of wav2vec 2.0 final layer). Significance computation and illustration is similar to Fig.  4a-b . c shows the score differences between two elements, similar to Fig.  4c . Only significant differences are colored (p < 0.05, corrected by FWER). d,e show element root-mean-square energy ratio or the predicted element emotion-induction effect of each audio in three datasets. Each line connects values of the two elements for an audio. If the proportion of the human-voice element is higher than that of the background-sound element, it is represented in red; otherwise, it is represented in blue. The prediction model is trained by the leave one out method. f-h Joint stepwise regression of voice and soundtrack features predicting emotion scores (behavioral: f; neural data in SEED: g; neural data in BAVE: h), using the same methodology as Fig.  4d -f. Whiskers reflect variability (box-plot conventions), showing dataset/region-specific dominance of voice vs. soundtrack in emotion encoding.\n\ndisciplinary innovation in emotion science.\n\nBeyond these broader field challenges, the present work has specific limitations worthy of note. First, our analysis focuses on group-level averages, overlooking individual differences in auditory emotion encoding-an oversight highlighted by the dataset demographics: while BAVE/SEED primarily include Chinese participants and LIRIS features French speakers, cross-cultural generalizability remains untested. Second, although we decomposed audio into voices/soundtracks, we did not systematically control for stimulus categories (e.g., comedy vs. tragedy in SEED/BAVE, six genres in LIRIS). Whether emotion encoding mechanisms vary across film types-such as the distinct roles of vocal humor in comedies vs. orchestral cues in tragedies-requires dedicated investigation. Additionally, the spatial resolution of EEG source localization (1 cm) may limit precise neural regionalization, though our use of both raw EEG and source-localized activity (68 regions) mitigates this limitation. Future research could incorporate diverse cultural cohorts, model individual differences, and design genre-controlled experiments to further investigate the auditory emotion encoding process.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "V. Conclusion",
      "text": "This study integrates affective neuroscience and computing to unveil hierarchical mechanisms of auditory-emotion encoding, constructing a neurobiologically informed computational framework that maps naturalistic auditory inputs to dynamic behavioral/neural responses across SEED, LIRIS, and BAVE datasets. Guided by neurobiological principles, we decompose audio into multilevel features (via classical feature extraction algorithms or wav2vec 2.0/Hubert) from the original or the isolated human voice/background soundtrack elements, yielding three key contributions. First, semantic representations dominate emotion encoding, outperforming acoustic features with stronger mappings to behavioral annotations and dynamic neural synchrony across most brain regions (p < 0.05), particularly in right hemisphere areas like the rostral anterior cingulate cortex and the frontal pole cortex. Second, wav2vec 2.0/Hubert's middle layers  (7) (8) (9) (10) (11) (12) (13) (14)  balance acoustic-semantic information to surpass final layers in emotion induction robustness. Third, voices and soundtracks exhibit dataset-dependent biases aligned with energy distribution (e.g., LIRIS favors soundtracks), with voices dominating prefrontal/temporal activity and soundtracks excelling in limbic regions.\n\nThese findings not only elucidate how the brain integrates auditory hierarchy and stimulus ecology to drive emotion arousal dynamics but also advance emotion-aware AI. As highlighted by Picard's vision for affective computing  [2] , this work provides a neurocomputational foundation for adaptive systems, bridging artificial intelligence with human emotion science to enable more sophisticated modeling of audioaffective interactions.\n\nLongzhao Liu received a bachelor's degree and then a doctoral degree from the School of Mathematical Sciences, Beihang University. He had an exchange at the Center for Complex Systems Research, Northwestern University, US, from 2019 to 2020. Currently, he works as an assistant professor and master's supervisor at the Institute of Artificial Intelligence, Beihang University. He has published 6 papers in journals like New J. Phys. and J. Stat. Mech.-Theory Exp. His research interests focus on complex systems, network science, group behavior and idea evolution, swarm intelligence, and dissemination dynamics, exploring microscopic dynamics to explain macroscopic phenomena in complex systems.\n\nHongwei Zheng earned his Doctor degree from the Ohio State University, department of economics. He is a senior researcher at the Beijing Academy of Block-Chain and Edge Computing. He has built many IT platforms using block-chain and distributed ledger technology with an emphasis on serving carbon neutrality and privacy preserving. He's research interest includes private computing, complex networks and big data. He is a member of IEEE and CIC.",
      "page_start": 10,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Framework for quantitative modeling of auditory-driven emotion arousal encoding in the brain. We integrate naturalistic stimuli from datasets",
      "page": 2
    },
    {
      "caption": "Figure 2: Subject ratings for the used stimuli in BAVE dataset. The scoring",
      "page": 3
    },
    {
      "caption": "Figure 2: shows the attention level",
      "page": 3
    },
    {
      "caption": "Figure 3: ), confirming emotion-related response",
      "page": 4
    },
    {
      "caption": "Figure 3: Split-half similarity of emotion-related responses across datasets",
      "page": 4
    },
    {
      "caption": "Figure 4: a and Fig. 6a visualize emotion scores quantifying",
      "page": 6
    },
    {
      "caption": "Figure 5: and Fig. 6 fur-",
      "page": 6
    },
    {
      "caption": "Figure 4: b presents the results",
      "page": 6
    },
    {
      "caption": "Figure 6: b compares",
      "page": 6
    },
    {
      "caption": "Figure 5: a) and source-localized dynamic",
      "page": 6
    },
    {
      "caption": "Figure 5: c, 5d) depict emotion scores across",
      "page": 6
    },
    {
      "caption": "Figure 4: shows that reducing wav2vec 2.0 fea-",
      "page": 6
    },
    {
      "caption": "Figure 4: b shows that both full-dimension and PCA-",
      "page": 6
    },
    {
      "caption": "Figure 4: The dominant role of semantic representations compared to acoustic features. a and b illustrate the significant mappings from classical algorithms",
      "page": 7
    },
    {
      "caption": "Figure 4: d-f depict the results between joint hierarchical",
      "page": 7
    },
    {
      "caption": "Figure 5: e and Fig. 5f demonstrate that",
      "page": 7
    },
    {
      "caption": "Figure 5: a, 5c, 5d) shows that emotion scores follow a non-linear",
      "page": 7
    },
    {
      "caption": "Figure 7: for the isolated elements, see",
      "page": 7
    },
    {
      "caption": "Figure 12: for hierarchical effects of categorized",
      "page": 7
    },
    {
      "caption": "Figure 5: Layerwise exploration of wav2vec 2.0 features (full dimensions) from the original audio in emotional encoding, integrating behavioral,",
      "page": 8
    },
    {
      "caption": "Figure 11: controls for the full visual",
      "page": 8
    },
    {
      "caption": "Figure 5: b), middle layers (especially 8, 10 and 14)",
      "page": 8
    },
    {
      "caption": "Figure 5: e, 5f), normalizing to layer 0",
      "page": 8
    },
    {
      "caption": "Figure 14: Behavioral data analysis reveals contrasting patterns: in the",
      "page": 9
    },
    {
      "caption": "Figure 6: b) and joint stepwise analysis",
      "page": 9
    },
    {
      "caption": "Figure 6: f); While in SEED, the voice element yields higher",
      "page": 9
    },
    {
      "caption": "Figure 6: b, 6f). Neural modeling of source-",
      "page": 9
    },
    {
      "caption": "Figure 6: c) shows voices dominate in",
      "page": 9
    },
    {
      "caption": "Figure 6: g), aligning with its role in unconscious emotion arousal from",
      "page": 9
    },
    {
      "caption": "Figure 6: d) reveals LIRIS audios have higher background sound",
      "page": 9
    },
    {
      "caption": "Figure 5: , Supplementary Fig. 10), demon-",
      "page": 9
    },
    {
      "caption": "Figure 6: ), bridging complex auditory scene",
      "page": 9
    },
    {
      "caption": "Figure 6: Heterogeneous induction effect of the human voice and soundtracks. a and b show the significant mappings of two isolated elements between",
      "page": 10
    },
    {
      "caption": "Figure 14: for results of wav2vec 2.0 final layer). Significance computation and illustration is similar to Fig. 4a–b. c shows",
      "page": 10
    },
    {
      "caption": "Figure 4: c. Only significant differences are colored (p < 0.05, corrected by FWER). d,e show element",
      "page": 10
    },
    {
      "caption": "Figure 4: d–f. Whiskers reflect variability (box-plot",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "trans": "convs"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "trans": "convs"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Descartes' Error: Emotion, Reason, and the Human Brain",
      "authors": [
        "A Damasio"
      ],
      "year": "2005",
      "venue": "Descartes' Error: Emotion, Reason, and the Human Brain"
    },
    {
      "citation_id": "2",
      "title": "",
      "authors": [
        "R Picard"
      ],
      "year": "2000",
      "venue": ""
    },
    {
      "citation_id": "3",
      "title": "Emotion ai, explained",
      "authors": [
        "Meredith Somers"
      ],
      "year": "2019",
      "venue": "Emotion ai, explained"
    },
    {
      "citation_id": "4",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    },
    {
      "citation_id": "5",
      "title": "Multimodal adaptive emotion transformer with flexible modality inputs on a novel dataset with continuous labels",
      "authors": [
        "W.-B Jiang",
        "X.-H Liu",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "6",
      "title": "Liris-accede: A video database for affective content analysis",
      "authors": [
        "Y Baveye",
        "E Dellandrea",
        "C Chamaret",
        "Liming Chen"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "7",
      "title": "Deap: A database for emotion analysis using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "Jong-Seok Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "8",
      "title": "Dissecting neural computations in the human auditory pathway using deep neural networks for speech",
      "authors": [
        "Y Li",
        "G Anumanchipalli",
        "A Mohamed",
        "P Chen",
        "L Carney",
        "J Lu",
        "J Wu",
        "E Chang"
      ],
      "year": "2023",
      "venue": "Nature Neuroscience"
    },
    {
      "citation_id": "9",
      "title": "Parallel and distributed encoding of speech across human auditory cortex",
      "authors": [
        "L Hamilton",
        "Y Oganian",
        "J Hall",
        "E Chang"
      ],
      "year": "2021",
      "venue": "Cell"
    },
    {
      "citation_id": "10",
      "title": "Eeg-based intersubject correlations reflect selective attention in a competing speaker scenario",
      "authors": [
        "M Rosenkranz",
        "B Holtze",
        "M Jaeger",
        "S Debener"
      ],
      "year": "2021",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "11",
      "title": "Adversarial testing of global neuronal workspace and integrated information theories of consciousness",
      "authors": [
        "O Ferrante",
        "U Gorska-Klimowska",
        "S Henin",
        "R Hirschhorn",
        "A Khalaf",
        "A Lepauvre",
        "L Liu",
        "D Richter",
        "Y Vidal",
        "N Bonacchi",
        "T Brown",
        "P Sripad",
        "M Armendariz",
        "K Bendtz",
        "T Ghafari",
        "D Hetenyi",
        "J Jeschke",
        "C Kozma",
        "D Mazumder",
        "S Montenegro",
        "A Seedat",
        "A Sharafeldin",
        "S Yang",
        "S Baillet",
        "D Chalmers",
        "R Cichy",
        "F Fallon",
        "T Panagiotaropoulos",
        "H Blumenfeld",
        "F De Lange",
        "S Devore",
        "O Jensen",
        "G Kreiman",
        "H Luo",
        "M Boly",
        "S Dehaene",
        "C Koch",
        "G Tononi",
        "M Pitts",
        "L Mudrik",
        "L Melloni"
      ],
      "year": "2025",
      "venue": "Nature"
    },
    {
      "citation_id": "12",
      "title": "The default network dominates neural responses to evolving movie stories",
      "authors": [
        "E Yang",
        "F Milisav",
        "J Kopal",
        "A Holmes",
        "G Mitsis",
        "B Misic",
        "E Finn",
        "D Bzdok"
      ],
      "year": "2023",
      "venue": "Nature Communications"
    },
    {
      "citation_id": "13",
      "title": "Affective understanding in film",
      "authors": [
        "H Wang",
        "L.-F Cheong"
      ],
      "year": "2006",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "14",
      "title": "Video affective content analysis: A survey of stateof-the-art methods",
      "authors": [
        "S Wang",
        "Q Ji"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "15",
      "title": "Eeg microstate correlates of emotion dynamics and stimulation content during video watching",
      "authors": [
        "W Hu",
        "Z Zhang",
        "H Zhao",
        "L Zhang",
        "L Li",
        "G Huang",
        "Z Liang"
      ],
      "year": "2023",
      "venue": "Cerebral Cortex"
    },
    {
      "citation_id": "16",
      "title": "Wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "17",
      "title": "Spleeter: A fast and efficient music source separation tool with pre-trained models",
      "authors": [
        "R Hennequin",
        "A Khlif",
        "F Voituret",
        "M Moussallam"
      ],
      "year": "2020",
      "venue": "Journal of Open Source Software"
    },
    {
      "citation_id": "18",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "C Hallacy",
        "A Ramesh",
        "G Goh",
        "S Agarwal",
        "G Sastry",
        "A Askell",
        "P Mishkin",
        "J Clark",
        "G Krueger",
        "I Sutskever"
      ],
      "year": "2021",
      "venue": "Proceedings of the 38th International Conference on Machine Learning"
    },
    {
      "citation_id": "19",
      "title": "Natural speech reveals the semantic maps that tile human cerebral cortex",
      "authors": [
        "A Huth",
        "W De Heer",
        "T Griffiths",
        "F Theunissen",
        "J Gallant"
      ],
      "year": "2016",
      "venue": "Nature"
    },
    {
      "citation_id": "20",
      "title": "Deep neural networks and brain alignment: Brain encoding and decoding (survey)",
      "authors": [
        "S Oota",
        "Z Chen",
        "M Gupta",
        "R Bapi",
        "G Jobard",
        "F Alexandre",
        "X Hinaut"
      ],
      "year": "2024",
      "venue": "Deep neural networks and brain alignment: Brain encoding and decoding (survey)"
    },
    {
      "citation_id": "21",
      "title": "The neural architecture of language: Integrative modeling converges on predictive processing",
      "authors": [
        "M Schrimpf",
        "I Blank",
        "G Tuckute",
        "C Kauf",
        "E Hosseini",
        "N Kanwisher",
        "J Tenenbaum",
        "E Fedorenko"
      ],
      "year": "2021",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "22",
      "title": "Shared computational principles for language processing in humans and deep language models",
      "authors": [
        "A Goldstein",
        "Z Zada",
        "E Buchnik",
        "M Schain",
        "A Price",
        "B Aubrey",
        "S Nastase",
        "A Feder",
        "D Emanuel",
        "A Cohen",
        "A Jansen",
        "H Gazula",
        "G Choe",
        "A Rao",
        "C Kim",
        "C Casto",
        "L Fanda",
        "W Doyle",
        "D Friedman",
        "P Dugan",
        "L Melloni",
        "R Reichart",
        "S Devore",
        "A Flinker",
        "L Hasenfratz",
        "O Levy",
        "A Hassidim",
        "M Brenner",
        "Y Matias",
        "K Norman",
        "O Devinsky",
        "U Hasson"
      ],
      "year": "2022",
      "venue": "Nature Neuroscience"
    },
    {
      "citation_id": "23",
      "title": "Evidence of a predictive coding hierarchy in the human brain listening to speech",
      "authors": [
        "C Caucheteux",
        "A Gramfort",
        "J.-R King"
      ],
      "year": "2023",
      "venue": "Nature Human Behaviour"
    },
    {
      "citation_id": "24",
      "title": "Toward a realistic model of speech processing in the brain with self-supervised learning",
      "authors": [
        "J Millet",
        "C Caucheteux",
        "P Orhan",
        "Y Boubenec",
        "A Gramfort",
        "E Dunbar",
        "C Pallier",
        "J.-R King"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "25",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "26",
      "title": "Cortical response to naturalistic stimuli is largely predictable with deep neural networks",
      "authors": [
        "M Khosla",
        "G Ngo",
        "K Jamison",
        "A Kuceyeski",
        "M Sabuncu"
      ],
      "year": "2021",
      "venue": "Science Advances"
    },
    {
      "citation_id": "27",
      "title": "Emotions promote social interaction by synchronizing brain activity across individuals",
      "authors": [
        "L Nummenmaa",
        "E Glerean",
        "M Viinikainen",
        "I Jääskeläinen",
        "R Hari",
        "M Sams"
      ],
      "year": "2012",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "28",
      "title": "Emotional speech synchronizes brains across listeners and engages large-scale dynamic brain networks",
      "authors": [
        "L Nummenmaa",
        "H Saarimäki",
        "E Glerean",
        "A Gotsopoulos",
        "I Jääskeläinen",
        "R Hari",
        "M Sams"
      ],
      "year": "2014",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "29",
      "title": "Intersubject synchronization of cortical activity during natural vision",
      "authors": [
        "U Hasson",
        "Y Nir",
        "I Levy",
        "G Fuhrmann",
        "R Malach"
      ],
      "year": "2004",
      "venue": "Science"
    },
    {
      "citation_id": "30",
      "title": "Audience preferences are predicted by temporal reliability of neural processing",
      "authors": [
        "J Dmochowski",
        "M Bezdek",
        "B Abelson",
        "J Johnson",
        "E Schumacher",
        "L Parra"
      ],
      "year": "2014",
      "venue": "Nature Communications"
    },
    {
      "citation_id": "31",
      "title": "Extracting multidimensional stimulus-response correlations using hybrid encoding-decoding of neural activity",
      "authors": [
        "J Dmochowski",
        "J Ki",
        "P Deguzman",
        "P Sajda",
        "L Parra"
      ],
      "year": "2018",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "32",
      "title": "Natural music evokes correlated eeg responses reflecting temporal structure and beat",
      "authors": [
        "B Kaneshiro",
        "D Nguyen",
        "A Norcia",
        "J Dmochowski",
        "J Berger"
      ],
      "year": "2020",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "33",
      "title": "Correlated components of ongoing eeg point to emotionally laden attention -a possible marker of engagement?",
      "authors": [
        "J Dmochowski",
        "P Sajda",
        "J Dias",
        "L Parra"
      ],
      "year": "2012",
      "venue": "Frontiers in Human Neuroscience"
    },
    {
      "citation_id": "34",
      "title": "Potential indicator for continuous emotion arousal by dynamic neural synchrony",
      "authors": [
        "G Pan",
        "Z Wu",
        "Y Yang",
        "X Wang",
        "L Liu",
        "Z Zheng",
        "S Tang"
      ],
      "year": "2024",
      "venue": "ser. Communications in Computer and Information Science"
    },
    {
      "citation_id": "35",
      "title": "Identifying cortical brain directed connectivity networks from high-density eeg for emotion recognition",
      "authors": [
        "H Wang",
        "X Wu",
        "L Yao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "36",
      "title": "Continuous arousal self-assessments validation using real-time physiological responses",
      "authors": [
        "T Li",
        "Y Baveye",
        "C Chamaret",
        "E Dellandréa",
        "L Chen"
      ],
      "year": "2015",
      "venue": "Proceedings of the 1st International Workshop on Affect & Sentiment in Multimedia, ser. ASM '15"
    },
    {
      "citation_id": "37",
      "title": "Eeglab: An open source toolbox for analysis of single-trial eeg dynamics including independent component analysis",
      "authors": [
        "A Delorme",
        "S Makeig"
      ],
      "year": "2004",
      "venue": "Journal of neuroscience methods"
    },
    {
      "citation_id": "38",
      "title": "Meg and eeg data analysis with mne-python",
      "authors": [
        "A Gramfort",
        "M Luessi",
        "E Larson",
        "D Engemann",
        "D Strohmeier",
        "C Brodbeck",
        "R Goj",
        "M Jas",
        "T Brooks",
        "L Parkkonen",
        "M Hämäläinen"
      ],
      "year": "2013",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "39",
      "title": "Cortical surface-based analysis: I. segmentation and surface reconstruction",
      "authors": [
        "A Dale",
        "B Fischl",
        "M Sereno"
      ],
      "year": "1999",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "40",
      "title": "Taichi: A language for high-performance computation on spatially sparse data structures",
      "authors": [
        "Y Hu",
        "T.-M Li",
        "L Anderson",
        "J Ragan-Kelley",
        "F Durand"
      ],
      "year": "2019",
      "venue": "ACM Transactions on Graphics (TOG)"
    },
    {
      "citation_id": "41",
      "title": "Inter-brain eeg feature extraction and analysis for continuous implicit emotion tagging during video watching",
      "authors": [
        "Y Ding",
        "X Hu",
        "Z Xia",
        "Y.-J Liu",
        "D Zhang"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "42",
      "title": "Psychopy2: Experiments in behavior made easy",
      "authors": [
        "J Peirce",
        "J Gray",
        "S Simpson",
        "M Macaskill",
        "R Höchenberger",
        "H Sogo",
        "E Kastman",
        "J Lindeløv"
      ],
      "year": "2019",
      "venue": "Behavior Research Methods"
    },
    {
      "citation_id": "43",
      "title": "Brains and algorithms partially converge in natural language processing",
      "authors": [
        "C Caucheteux",
        "J.-R King"
      ],
      "year": "2022",
      "venue": "Communications Biology"
    },
    {
      "citation_id": "44",
      "title": "Self-supervised models of audio effectively explain human cortical responses to speech",
      "authors": [
        "A Vaidya",
        "S Jain",
        "A Huth"
      ],
      "year": "2022",
      "venue": "Proceedings of the 39th International Conference on Machine Learning"
    },
    {
      "citation_id": "45",
      "title": "Wenetspeech: A 10000+ hours multi-domain mandarin corpus for speech recognition",
      "authors": [
        "B Zhang",
        "H Lv",
        "P Guo",
        "Q Shao",
        "C Yang",
        "L Xie",
        "X Xu",
        "H Bu",
        "X Chen",
        "C Zeng",
        "D Wu",
        "Z Peng"
      ],
      "year": "2022",
      "venue": "Wenetspeech: A 10000+ hours multi-domain mandarin corpus for speech recognition"
    },
    {
      "citation_id": "46",
      "title": "Open-unmix -a reference implementation for music source separation",
      "authors": [
        "F.-R Stöter",
        "S Uhlich",
        "A Liutkus",
        "Y Mitsufuji"
      ],
      "year": "2019",
      "venue": "Journal of Open Source Software"
    },
    {
      "citation_id": "47",
      "title": "Affective video content analysis: A multidisciplinary insight",
      "authors": [
        "Y Baveye",
        "C Chamaret",
        "E Dellandrea",
        "L Chen"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "48",
      "title": "Scikit-learn: Machine learning in python",
      "authors": [
        "F Pedregosa",
        "G Varoquaux",
        "A Gramfort",
        "V Michel",
        "B Thirion",
        "O Grisel",
        "M Blondel",
        "P Prettenhofer",
        "R Weiss",
        "V Dubourg"
      ],
      "year": "2011",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "49",
      "title": "Stepwise model fitting and statistical inference: Turning noise into signal pollution",
      "authors": [
        "R Mundry",
        "C Nunn"
      ],
      "year": "2009",
      "venue": "The American Naturalist"
    },
    {
      "citation_id": "50",
      "title": "Enhanced brain structure-function tethering in transmodal cortex revealed by high-frequency eigenmodes",
      "authors": [
        "Y Yang",
        "Z Zheng",
        "L Liu",
        "H Zheng",
        "Y Zhen",
        "Y Zheng",
        "X Wang",
        "S Tang"
      ],
      "year": "2023",
      "venue": "Nature Communications"
    },
    {
      "citation_id": "51",
      "title": "Discovering the false discovery rate",
      "authors": [
        "Y Benjamini"
      ],
      "year": "2010",
      "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology)"
    },
    {
      "citation_id": "52",
      "title": "Layer-wise analysis of a selfsupervised speech representation model",
      "authors": [
        "A Pasad",
        "J.-C Chou",
        "K Livescu"
      ],
      "year": "2022",
      "venue": "Layer-wise analysis of a selfsupervised speech representation model"
    },
    {
      "citation_id": "53",
      "title": "What all do audio transformer models hear? probing acoustic representations for language delivery and its structure",
      "authors": [
        "J Shah",
        "Y Singla",
        "C Chen",
        "R Shah"
      ],
      "year": "2021",
      "venue": "What all do audio transformer models hear? probing acoustic representations for language delivery and its structure"
    },
    {
      "citation_id": "54",
      "title": "Emotions recognition using eeg signals: A survey",
      "authors": [
        "S Alarcao",
        "M Fonseca"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "55",
      "title": "Seed-vii: A multimodal dataset of six basic emotions with continuous labels for emotion recognition",
      "authors": [
        "W.-B Jiang",
        "X.-H Liu",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "56",
      "title": "Identifying stable patterns over time for emotion recognition from eeg",
      "authors": [
        "W.-L Zheng",
        "J.-Y Zhu",
        "B.-L Lu"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "57",
      "title": "Classification of five emotions from eeg and eye movement signals: Complementary representation properties",
      "authors": [
        "L.-M Zhao",
        "R Li",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2019",
      "venue": "The 9th International IEEE/EMBS Conference on Neural Engineering (NER)"
    },
    {
      "citation_id": "58",
      "title": "Contrastive learning of subject-invariant eeg representations for cross-subject emotion recognition",
      "authors": [
        "X Shen",
        "X Liu",
        "X Hu",
        "D Zhang",
        "S Song"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "59",
      "title": "Distinct dimensions of emotion in the human brain and their representation on the cortical surface",
      "authors": [
        "N Koide-Majima",
        "T Nakai",
        "S Nishimoto"
      ],
      "year": "2020",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "60",
      "title": "The neural representation of visually evoked emotion is high-dimensional, categorical, and distributed across transmodal brain regions",
      "authors": [
        "T Horikawa",
        "A Cowen",
        "D Keltner",
        "Y Kamitani"
      ],
      "year": "2020",
      "venue": "iScience"
    }
  ]
}