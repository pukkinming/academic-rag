{
  "paper_id": "2207.10333v1",
  "title": "Jointly Predicting Emotion, Age, And Country Using Pre-Trained Acoustic Embedding",
  "published": "2022-07-21T07:04:41Z",
  "authors": [
    "Bagus Tris Atmaja",
    "Zanjabila",
    "Akira Sasou"
  ],
  "keywords": [
    "speech emotion recognition",
    "affective computing",
    "acoustic embedding",
    "multitask learning",
    "age prediction",
    "country prediction"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper, we demonstrated the benefit of using pre-trained model to extract acoustic embedding to jointly predict (multitask learning) three tasks: emotion, age, and native country. The pre-trained model was trained with wav2vec 2.0 large robust model on the speech emotion corpus. The emotion and age tasks were regression problems, while country prediction was a classification task. A single harmonic mean from three metrics was used to evaluate the performance of multitask learning. The classifier was a linear network with two independent layers and shared layers, including the output layers. This study explores multitask learning on different acoustic features (including the acoustic embedding extracted from a model trained on an affective speech dataset), seed numbers, batch sizes, and normalizations for predicting paralinguistic information from speech.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Research on computational paralinguistics advances with the advent of artificial intelligence, big data, and speed up internet access. Among many issues, computational paralinguistics research still relies on limited small data for experiments  [1] . The need to conduct research on big data is needed for generalization and performance improvements.\n\nA large-scale dataset was made for accelerating research on computational paralinguistics, the HUME-VB dataset  [2] . The dataset contains more than 36 hours of audio data with a 48 kHz of the sampling rate. The samples in the dataset were labeled with three pieces of information: intensity rating for ten different expressed emotions (float in ranges [0, 1]), age (integer), and country (string). This dataset was created to support the following tasks: multitask learning, emotion generation, and few-shot learning. Multitask learning includes emotion recognition from vocalizations.\n\nSpeech emotion recognition is a branch of computational paralinguistics that deal with the accurate prediction of emotional score/class from speech. Since the expression of speakers' emotions can be perceived by the human ears of listeners, it is also possible for the computer to have the same ability: recognize emotion from the sound. Instead of speech, which has semantic or linguistic meanings, the burst of brief vocalizations is an interesting source for predicting expressed emotion in human social life since it contains rich information on emotion  [3] .\n\nHumans communicate emotion through two different kinds of vocalizations, prosody and vocal burst  [4] . Prosody interacts with the words (linguistic) to convey feelings and attitudes via speech, while vocal burst just occurs without linguistic meanings. Examples are laughs, cries, sighs, shrieks, growls, hollers, roars, and oohs. While the study of the prosody of speech for speech emotion recognition is widely conducted (indicated by the number of available datasets), the study of the vocal burst emotion recognition is currently undergoing with support of new vocalization datasets, e.g.,  [5] ,  [2] .\n\nIt is not the only emotion that can be recognized in the human's voice. Age, gender, and nationality could also be detected from voices. The change of voice in age is recognizable by both humans and computers, although it is the hardest among emotion and gender  [6] . By transferring information about age and gender, the recognition of dimensional could be improved  [7] . The task of combining several tasks together based on the same or different inputs is known as multitask learning.\n\nAt the ICML Expressive Vocalizations (ExVo) Workshop and Competition, a multitask learning task is held to predict the average intensity of each of 10 emotions perceived in vocal bursts, the speaker's age, and the speaker's Native-Country. The challenge utilized a large vocalization dataset  [3]  which was shared across three different tasks. The emotion and predictions are regression problems; native-country prediction is a classification problem. The participants were evaluated by a single score of the harmonic mean from these three problems.\n\nThis paper contributes to evaluating the pre-trained speech embeddings extractor, which was trained specifically on an affective speech dataset, to jointly predict emotion, age, and the country as in the ExVo challenge. We hypothesize that using this kind of acoustic feature extractor will lead to better results than traditional feature extractors, which extract physical information of audio signals. The rest of this paper explains related work, methods, results and discussion, and conclusion.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Research on multitask learning, predicting several tasks simultaneously using unified architecture, progressively increased due to its effectiveness in predicting several outputs arXiv:2207.10333v1 [eess.AS] 21 Jul 2022 with the same or similar inputs. Speech processing is an ideal test bed for multitask learning. Several pieces of information could be extracted from the same speech input, whether it is text (automatic speech recognition, ASR), gender, age, nationality, emotion, language, and disease. The following is a resume of the work of multitask learning done in the past with key differences from this study presented at the end of this section.\n\nParthasarathy and Busso  [8]  proposed two schemes of multitask learning architectures for evaluating valence, arousal, and dominance simultaneously. The first architecture is without an independent layer for each task (shared layers only), while the second architecture is with independent layers for each task. They found the second architecture with independent layers performed better than the first one without independent layers. The scores (measured in concordance correlation coefficient) also showed improvements from the baseline architecture with single-task learnings.\n\nLee  [9]  also evaluated two architectures of multitask learning optimized for emotion recognition. The first architecture is composed of three task-specific softmax layers to predict gender, emotion, and language. The second architecture is composed of one softmax layer containing two tasks, language and emotion. The results revealed a better generalization of the second architecture to predict emotion categories.\n\nKim et al.  [10]  proposed to use gender and naturalness information to minimize the large mismatch between the training and test data in speech emotion recognition in the wild. The method employed traditional acoustic features (f o , voice probability, zero-crossing-rate, MFCC with their energies and first-time derivatives) extracted on frame-level and calculated the high-level features on top of it. The high-level features then were fed into an extreme learning machine to predict the categories of emotion. They obtain significant improvement over the baseline with single-task learning. The method was evaluated on five different datasets for generalization.\n\nAtmaja and Akagi  [11]  evaluated different loss functions -CCC, MSE, and MAE -for multitask learning dimensional emotion recognition. The traditional approach of deep learning commonly employed MSE loss to minimize the error and training stage. Since the goal is to maximize CCC, they proposed to use CCC loss as the loss function to replace MSE. The results on two datasets (IEMOCAP and MSP-IMPROV) and two acoustic features (GeMAPS and python Audio Analysis) showed the consistency that CCC loss is superior to MSE and MAE.\n\nLi et al.  [12]  proposed additional information on age, gender, and emotion for speaker verification using multitask learning and domain adversarial training. The multitask learning part minimizes losses of three variables (speaker, gender, and nationality). The domain adversarial training, which also employs multitask learning, minimizes losses of speaker and emotion. The results showed that multitask improved the performance from the baseline by about 16% while the domain adversarial training improved the performance from the baseline by about 22%. The baseline used ResNet networks.\n\nCai et al.  [13]  employed multitask learning by predicting text characters and predicting emotion in the training phase. The model is trained to minimize the loss of categorical emotion (cross-entropy) and loss of character recognition (connectionist temporal classification). The inference phase removes the character recognition path to predicting emotion categories only. The proposed multitask learning achieved an accuracy of 78% compared to 72% of the baseline method with capsule networks.\n\nAtmaja et al.  [14]  evaluated multitask learning of emotion recognition (dimensional) naturalness scores from speech. They evaluated two different architectures with and without independent layers. The architecture without independent layers (shared layers only) exhibits the best performance in predicting valence, arousal, and dominance scores. The shared layers have been built using three layers of fully connected networks with nodes of 512, 256, and 128. However, the scores for naturalness recognition in multitask learning is lower than in single-task learning.\n\nInstead of focusing on the single task evaluation on multitask learning (e.g., only predicting emotion in multitask emotion and transcription  [13] , or emotion and language  [9] ), this study focused on the all tasks evaluated on the multitask learning. We employed the harmonic mean evaluation from three metrics for three tasks and used this harmonic mean as the final evaluation. This multitask learning evaluation using all tasks was not evaluated on the previous tasks, where the authors only focused on emotion recognition or speaker verification.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Iii. Methods",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Datasets",
      "text": "This study relies on the HUME-VB dataset, which is used at the ICML Expressive Vocalization (ExVo) Competition 2022. The dataset is a large-scale emotional non-linguistic vocalization known as vocal burst. An example of this vocal burst is \"argh!\" to express distress emotion. There are ten emotions rated in continuous scores. These emotions are Amusement, Awe, Awkwardness, Distress, Excitement, Fear, Horror, Sadness, Surprise, and Triumph. The data were collected from 1702 speakers aged 20-39 years old. The collection locations are China, South Africa, Venezuela, and the US. The total duration of the dataset is almost 37 hours (36 hours 47 minutes). Although the data were split into train, validation, and test, the test set was closed by the organizer of the competition. Hence, we evaluated our methods mostly on the validation set (except in the last part, where test results are reported). More details about the dataset can be found in the  [2] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Pre-Trained Acoustic Embedding",
      "text": "We evaluated a pre-trained model finetuned on an affective speech dataset. The model  [15] ,  [16]  is based on wav2vec2-large-robust model  [17] . The model is trained on MSP-Podcasts dataset  [18] , a large affective speech corpus derived from YouTube with valence, arousal, and dominance scores. For this finetuning, the combined samples on the MSP-Podcasts dataset have a combined length of roughly 21 hours. The model extracted the speech embedding from the dataset (HUME-VB) with a size of 1024 dimensions for each utterance. The output layers of the model, i.e., the logits, are the scores of arousal, dominance, and valence, in ranges [0, 1]. We experimented with two variants of speech embedding with this model. The first embedding is the hidden states of the last layer before the output layer (1024-dim), and the second embedding is the concatenation of hidden states with the logits (1027-dim). We named the first embedding \"w2v2-R-er\" (wav2vec 2.0 robust emotion recognition), and the second embedding\"w2v2-R-vad\" (wav2vec 2.0 robust emotion recognition with valence, arousal, and dominance). Notice the term of acoustic embedding (extracted from vocal bursts) here is used instead of (traditonal) acoustic features or speech embedding (extracted from speech).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Classifier",
      "text": "We employed a model of multitask learning adopted from  [2] . The architecture of the model is shown in Figure  1 . The model accepts the input of acoustic features depending on the size of the features. For instance, the dimension for w2v2-R-er is 1024. Then, two linear layers are stacked as shared layers. The number of nodes for each layer is 128 and 64, respectively. We applied layer normalization  [19]  for each linear layer, followed by LeakyReLU activation functions. A group of layers builds up the independent layers for each task. For emotion and country prediction, there is only a layer followed by output layers. For age prediction, we used two independent layers to bridge the gap in the big number of nodes from the shared layer (64) to the single-node output layer. The nodes for these independent layers are 32 and 16, respectively.\n\nThe loss function minimizes losses of three tasks: MSE for emotion recognition age prediction, cross-entropy (CE) for country prediction. The total loss function (L T ) is weighting sum of three losses given by the following formula,\n\nThe coefficient of α is set to be 0.34, 0.33, and 0.33 for emotion, country, and age, respectively.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Evaluation Metrics",
      "text": "The evaluation of three paralinguistic tasks in this study used specific metric for each task. The emotion recognition is evaluated in concordance correlation coefficient (CCC), age recognition is evaluated in mean absolute error (MAE), and nationality prediction is evaluated in unweighted average recall (UAR). These metrics are described below,\n\nThe average CCC scores for ten emotion categories is the mean values,\n\nCCC is in range [-1, 1] with -1 for perfect disconcordance, 0 for absence of concordance/disconcordance, and 1 for perfect concordance.\n\nNext is metric to evaluate the performance of nationality/country prediction that is unweighted accuracy, also known as unweighted average recall (UAR) and balanced accuracy. Unweighted accuracy is formulated as,\n\nwhere i is the corresponding country class, and 4 is the number of countries (USA, China, South Africa, and Venezuela). UAR ranges in 0-100 in % or 0-1 in normalized score. The metric to evaluate the last task, age prediction, is mean absolute error (MAE). MAE is a common metric for evaluating regression, and it is scale-dependent. The lower the scores, the better the age prediction. The formulation of MAE is given by\n\nwhere n is the number of samples in the evaluation or test sets (which one is used to calculate the score).\n\nSince MAE is scale-dependent, we inverted the MAE scores for consistency with the previous two metrics,\n\nNow, for all metrics (CCC, UAR, 1/MAE), the higher scores, the better predictions of emotion, country, and age. Finally, for calculating overall performance, we used the harmonic mean of three metrics above  [2] ,\n\nS M T L is our main metric to judge the performance of the evaluated methods (hyperparameters, features, normalization). Other previous metrics are used to determine the performance of the corresponding method for individual tasks in multitask learning.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iv. Results And Discussion",
      "text": "We presented our results in different ablation studies: choosing the right seed, comparing different acoustic features, effects of normalization, and test results. For each study, we run the experiment five times for each setting. For instance, in choosing the right seed, we run the experiment five times on seed \"101\". The reported results are the average scores, except for comparing acoustic features. The reported scores for w2v2-R-er and w2v2-R-vad are chosen from the best from five different runs, similar to the baseline. For the baseline results, we quote the scores from the source  [2] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Choosing The Right Seed",
      "text": "Seed initialization is an important step in deep learning methods  [20] ,  [21] . Here, as a first step, we choose the seven different seed values to choose the best one. We evaluated seed values 42, 101, 102, 103, 104, 105, 106. The reason for adding seed 42 to the other six seed values given in the baseline is due to its common use in the deep learning community.\n\nTable  I  shows the results of using different seed values for multitask learning of emotion, age, and country on the validation set. The results are average scores of five trials with their standard deviation. Since the values of STD are similar in other ablation tests in this study, we only report these STD scores in Table  I . The following reported scores, except stated, used the seed values of \"106\" which obtained the highest S M T L score from seed values evaluation.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Comparison Of Different Features",
      "text": "The main focus of this study is to evaluate the acoustic embedding extracted using pre-trained models trained on the emotional speech dataset. Most speech processing tasks, especially speech emotion recognition, are trained using handcrafted acoustic feature extractors (e.g., MFCC, spectrogram, or mel-spectrogram)  [22] . Others are using pre-trained models but trained on neutral speech (e.g., wav2vec2 2.0, HuBERT, WavLM). In this study, we utilized a pre-trained model  [16] ,  [16]  built using wav2vec 2.0 Robust on affective speech dataset.\n\nTable  II  proves our presumption that our model will surpass the baseline scores. Both w2v2-R-er and w2v2-R-vad obtain higher S M T L scores than the baseline scores on the same configuration (batch size, seed, and other hyperparameters). Specifically, these scores obtained by two acoustic embeddings show the most remarkable improvement in emotion recognition score, in which the model to extract the acoustic embedding is trained. Not only for the emotion recognition task but both scores for age and country predictions were also improved. The pre-trained model is shown to be helpful on other tasks probably due to the similarity of the task (paralinguistics and non-linguistic tasks), and the data trained to build the model contains the age and country information embedded on the extracted acoustic embedding. The data to train the model is MSP-Podcast in the English language. Although the dataset contains English only language, the pretrained model may be able to discriminate between English with non-English language (related to country prediction) as in anomaly detection problems.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "C. Evaluation Of Different Batch Sizes",
      "text": "We evaluated five different batch sizes since there is evidence the influence of batch size, particularly for emotion recognition task  [23] . As found in the  [23] , we also found that the smallest evaluated batch size in this study resulted in the best performance. As the batch size increases, the performance decreases. For the best performance on the use",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "D. Effect Of Waveform Normalization",
      "text": "Normalization may affect the performance of deep learning, particularly in speech processing. This effect is due to the model usually only working on the standard input and being prone to high deviation. It has been proved that such normalizations are effective for deep learning, e.g., batch normalization  [24] , group normalization  [23] , and layer normalization  [19] . Aside from layer normalization, we also evaluated the model with waveform normalization. In this case, we utilize librosa toolkit  [25]  to normalize the audio array (the amplitude of waveform), i.e., the output is in the range [-1, 1]. Then, two acoustic embeddings are created with these normalizations, namely w2v2-R-er-norm and w2v2-Rvad-norm. The results are shown in Table  IV .\n\nAs shown in Table  IV , there are no such improvements by normalizing the waveform of speech. This finding may be explained by the fact that emotion, age, and gender are related to the loudness of the waveform. Hence, such normalization of the waveform will remove important information that discriminates these paralinguistics labels. The results for two acoustic emebddings, w2v2-R-er and w2v2-R-vad, are consistent with and without normalization, highlighting the unnecessary processing of normalizing the speech waveform.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "E. Test Results",
      "text": "We presented the results of our predictions from the previous validation evaluation for the closed test set evaluation. Note that the best result from the validation set reported in this study was not submitted to obtain the test scores due to time limitations (for obtaining the test scores as part of The ICML 2022 Expressive Vocalizations Workshop and Competition). In the submitted results, we evaluated two predictions from both w2v2-R-er features set with different seed and batch sizes. The first prediction is with a batch size of 8 and a seed of 42. The second prediction is with a batch size of 4 and a seed of 106. Note that this last prediction is submitted with the original architecture  [2]  with one hidden layer for all independent layers. Table  V  shows the score of harmonic mean (S M T L ) from our predictions (last two rows) and the baseline  [2] . Although we did not use the best-reported validation scores in this study, our submitted results are still higher than the baseline results. These scores reveal the effectiveness of the evaluated acoustic embedding, w2v2-R-er, which were extracted using the robust version of wav2vec 2.0  [26]  on the MSP-Podcast emotional speech dataset  [18] . We believe that the test score will be even higher for w2v2-R-vad with batch size 2 since there are remarkable improvements on the validation set by using this configuration (the best validation S M T L = 0.401).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "V. Conclusions",
      "text": "In this paper, we reported evaluations of multitask learning to jointly predict emotion, age, and country by using acoustic emebdding extracted from a pre-trained model. The model is finetuned on an affective speech dataset. The extracted acoustic emebddings were fed to an architecture consisting of shared layers and independent layers for these three tasks. The results showed improvements over the baseline methods with the common speech representations (ComParE, eGeMAPS, BoAW, and DeepSpectrum). Two variants of acoustic embeddings are evaluated with original hidden states and concatenation of hidden states with logits. The latter performed best on the validation set. We conducted ablation studies on different seeds, batch sizes, and normalization. The study finds the optimum seed and batch size of the evaluated ranges and finds no improvement in performing waveform normalizations. While this study treated all emotions in the same weights, future studies may be directed to adjust these weights for optimum emotion recognition, as well as to improve overall harmonic mean evaluation for all tasks in Multitask learning.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Architecture of multitask learning (MTL) for predicting emotion, age, and country. Emotion and age tasks are regressions; country prediction is a",
      "page": 4
    },
    {
      "caption": "Figure 1: , pp. 1–12,",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "† Sepuluh Nopember\nInstitute of Technology,\nIndonesia"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "Abstract—In this paper, we demonstrated the beneﬁt of using\nHumans communicate emotion through two different kinds"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "pre-trained model\nto\nextract\nacoustic\nembedding\nto\njointly"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "of\nvocalizations,\nprosody and\nvocal burst\n[4]. Prosody\nin-"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "predict (multitask learning) three tasks: emotion, age, and native"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "teracts with\nthe words\n(linguistic)\nto\nconvey\nfeelings\nand"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "country. The pre-trained model was\ntrained with wav2vec 2.0"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "attitudes via speech, while vocal burst\njust occurs without lin-"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "large robust model on the speech emotion corpus. The emotion"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "guistic meanings. Examples are laughs, cries, sighs, shrieks,\nand age tasks were regression problems, while country prediction"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "was a classiﬁcation task. A single harmonic mean from three\ngrowls,\nhollers,\nroars,\nand\noohs. While\nthe\nstudy\nof\nthe"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "metrics was\nused\nto\nevaluate\nthe\nperformance\nof multitask"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "prosody of speech for speech emotion recognition is widely"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "learning. The\nclassiﬁer was\na\nlinear network with two\ninde-"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "conducted (indicated by the number of\navailable datasets),"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "pendent\nlayers and shared layers,\nincluding the output\nlayers."
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "the study of\nthe vocal burst emotion recognition is currently"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "This\nstudy\nexplores multitask\nlearning\non\ndifferent\nacoustic"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "undergoing with support of new vocalization datasets,\ne.g.,\nfeatures\n(including\nthe\nacoustic\nembedding\nextracted from a"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "model\ntrained on an affective\nspeech dataset),\nseed numbers,\n[5],\n[2]."
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "batch\nsizes,\nand\nnormalizations\nfor\npredicting\nparalinguistic"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "It\nis not\nthe only emotion that can be\nrecognized in the"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "information from speech."
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "human’s voice. Age, gender,\nand nationality could also be"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "Index Terms—speech emotion recognition, affective comput-"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "detected from voices. The change of voice in age is recogniz-"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "ing,\nacoustic\nembedding, multitask\nlearning,\nage\nprediction,"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "able by both humans and computers, although it is the hardest"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "country prediction"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "among emotion and gender\n[6]. By transferring information"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "about age and gender,\nthe recognition of dimensional could"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "I.\nINTRODUCTION"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "be improved [7]. The task of combining several tasks together"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "Research on computational paralinguistics\nadvances with\nbased on the same or different\ninputs is known as multitask"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "the\nadvent of\nartiﬁcial\nintelligence, big data,\nand speed up\nlearning."
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "internet access. Among many issues, computational paralin-\nAt\nthe ICML Expressive Vocalizations\n(ExVo) Workshop"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "guistics research still\nrelies on limited small data for experi-\nand Competition, a multitask learning task is held to predict"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "ments [1]. The need to conduct research on big data is needed\nthe\naverage\nintensity of\neach of 10 emotions perceived in"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "for generalization and performance improvements.\nvocal\nbursts,\nthe\nspeaker’s\nage,\nand\nthe\nspeaker’s Native-"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "A large-scale dataset was made for accelerating research\nCountry. The challenge utilized a large vocalization dataset"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "on computational paralinguistics,\nthe HUME-VB dataset\n[2].\n[3] which was shared across three different\ntasks. The emo-"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "The dataset contains more than 36 hours of audio data with\ntion and predictions are regression problems; native-country"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "a 48 kHz of\nthe sampling rate. The samples\nin the dataset\nprediction is a classiﬁcation problem. The participants were"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "were labeled with three pieces of information: intensity rating\nevaluated by a single score of the harmonic mean from these"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "for\nten different expressed emotions\n(ﬂoat\nin ranges\n[0, 1]),\nthree problems."
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "age (integer), and country (string). This dataset was created\nThis paper contributes to evaluating the pre-trained speech"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "to support\nthe\nfollowing tasks: multitask learning,\nemotion\nembeddings extractor, which was\ntrained speciﬁcally on an"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "generation, and few-shot learning. Multitask learning includes\naffective speech dataset,\nto jointly predict emotion, age, and"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "emotion recognition from vocalizations.\nthe country as\nin the ExVo challenge. We hypothesize that"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "Speech emotion recognition is a branch of computational\nusing\nthis\nkind\nof\nacoustic\nfeature\nextractor will\nlead\nto"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "paralinguistics\nthat\ndeal with\nthe\naccurate\nprediction\nof\nbetter results than traditional feature extractors, which extract"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "emotional\nscore/class\nfrom speech. Since\nthe\nexpression of\nphysical\ninformation of audio signals. The rest of\nthis paper"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "speakers’ emotions can be perceived by the human ears of\nexplains\nrelated work, methods,\nresults and discussion, and"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "listeners,\nit\nis also possible for the computer to have the same\nconclusion."
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "ability: recognize emotion from the sound. Instead of speech,"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "II. RELATED WORK"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "which has semantic or\nlinguistic meanings,\nthe burst of brief"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "vocalizations is an interesting source for predicting expressed\nResearch\non multitask\nlearning,\npredicting\nseveral\ntasks"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "emotion in human social life since it contains rich information\nsimultaneously\nusing\nuniﬁed\narchitecture,\nprogressively\nin-"
        },
        {
          "∗ National\nInstitute of Advanced Industrial Science and Technology, Japan": "on emotion [3].\ncreased due to its effectiveness in predicting several outputs"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "with the same or similar inputs. Speech processing is an ideal": "test bed for multitask learning. Several pieces of\ninformation",
          "baseline by about 22%. The baseline used ResNet networks.": "Cai et al.\n[13] employed multitask learning by predicting"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "could be\nextracted from the\nsame\nspeech input, whether\nit",
          "baseline by about 22%. The baseline used ResNet networks.": "text characters and predicting emotion in the training phase."
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "is\ntext\n(automatic\nspeech\nrecognition, ASR),\ngender,\nage,",
          "baseline by about 22%. The baseline used ResNet networks.": "The model\nis\ntrained\nto minimize\nthe\nloss\nof\ncategorical"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "nationality, emotion,\nlanguage, and disease. The following is",
          "baseline by about 22%. The baseline used ResNet networks.": "emotion\n(cross-entropy)\nand\nloss\nof\ncharacter\nrecognition"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "a resume of\nthe work of multitask learning done in the past",
          "baseline by about 22%. The baseline used ResNet networks.": "(connectionist\ntemporal\nclassiﬁcation). The\ninference phase"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "with key differences from this study presented at\nthe end of",
          "baseline by about 22%. The baseline used ResNet networks.": "removes the character recognition path to predicting emotion"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "this section.",
          "baseline by about 22%. The baseline used ResNet networks.": "categories only. The proposed multitask learning achieved an"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "Parthasarathy and Busso [8] proposed two schemes of mul-",
          "baseline by about 22%. The baseline used ResNet networks.": "accuracy of 78% compared to 72% of\nthe baseline method"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "titask learning architectures\nfor\nevaluating valence,\narousal,",
          "baseline by about 22%. The baseline used ResNet networks.": "with capsule networks."
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "and dominance simultaneously. The ﬁrst architecture is with-",
          "baseline by about 22%. The baseline used ResNet networks.": "Atmaja et al.\n[14] evaluated multitask learning of emotion"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "out an independent\nlayer\nfor each task (shared layers only),",
          "baseline by about 22%. The baseline used ResNet networks.": "recognition\n(dimensional)\nnaturalness\nscores\nfrom speech."
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "while the second architecture is with independent\nlayers for",
          "baseline by about 22%. The baseline used ResNet networks.": "They evaluated two different architectures with and without"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "each\ntask. They\nfound\nthe\nsecond\narchitecture with\ninde-",
          "baseline by about 22%. The baseline used ResNet networks.": "independent\nlayers.\nThe\narchitecture without\nindependent"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "pendent\nlayers performed better\nthan the ﬁrst one without",
          "baseline by about 22%. The baseline used ResNet networks.": "layers\n(shared layers only) exhibits\nthe best performance in"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "independent\nlayers. The\nscores\n(measured\nin\nconcordance",
          "baseline by about 22%. The baseline used ResNet networks.": "predicting valence, arousal, and dominance scores. The shared"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "correlation coefﬁcient) also showed improvements\nfrom the",
          "baseline by about 22%. The baseline used ResNet networks.": "layers have been built using three layers of\nfully connected"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "baseline architecture with single-task learnings.",
          "baseline by about 22%. The baseline used ResNet networks.": "networks with\nnodes\nof\n512,\n256,\nand\n128. However,\nthe"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "Lee [9] also evaluated two architectures of multitask learn-",
          "baseline by about 22%. The baseline used ResNet networks.": "scores\nfor\nnaturalness\nrecognition\nin multitask\nlearning\nis"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "ing optimized for emotion recognition. The ﬁrst architecture is",
          "baseline by about 22%. The baseline used ResNet networks.": "lower\nthan in single-task learning."
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "composed of three task-speciﬁc softmax layers to predict gen-",
          "baseline by about 22%. The baseline used ResNet networks.": "Instead of\nfocusing on the single task evaluation on mul-"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "der, emotion, and language. The second architecture is com-",
          "baseline by about 22%. The baseline used ResNet networks.": "titask\nlearning\n(e.g.,\nonly\npredicting\nemotion\nin multitask"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "posed of one softmax layer containing two tasks,\nlanguage",
          "baseline by about 22%. The baseline used ResNet networks.": "emotion and transcription [13], or emotion and language [9]),"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "and emotion. The results\nrevealed a better generalization of",
          "baseline by about 22%. The baseline used ResNet networks.": "this study focused on the all\ntasks evaluated on the multitask"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "the second architecture to predict emotion categories.",
          "baseline by about 22%. The baseline used ResNet networks.": "learning. We employed the harmonic mean evaluation from"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "Kim et\nal.\n[10] proposed to use gender\nand naturalness",
          "baseline by about 22%. The baseline used ResNet networks.": "three metrics\nfor\nthree\ntasks\nand used this harmonic mean"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "information to minimize the large mismatch between the train-",
          "baseline by about 22%. The baseline used ResNet networks.": "as\nthe ﬁnal\nevaluation. This multitask\nlearning\nevaluation"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "ing and test data in speech emotion recognition in the wild.",
          "baseline by about 22%. The baseline used ResNet networks.": "using all\ntasks was not evaluated on the previous tasks, where"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "The method employed traditional acoustic features (fo, voice",
          "baseline by about 22%. The baseline used ResNet networks.": "the authors only focused on emotion recognition or\nspeaker"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "probability, zero-crossing-rate, MFCC with their energies and",
          "baseline by about 22%. The baseline used ResNet networks.": "veriﬁcation."
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "ﬁrst-time derivatives) extracted on frame-level and calculated",
          "baseline by about 22%. The baseline used ResNet networks.": ""
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "",
          "baseline by about 22%. The baseline used ResNet networks.": "III. METHODS"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "the high-level\nfeatures on top of\nit. The high-level\nfeatures",
          "baseline by about 22%. The baseline used ResNet networks.": ""
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "then were fed into an extreme learning machine to predict\nthe",
          "baseline by about 22%. The baseline used ResNet networks.": "A. Datasets"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "categories of emotion. They obtain signiﬁcant\nimprovement",
          "baseline by about 22%. The baseline used ResNet networks.": "This study relies on the HUME-VB dataset, which is used"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "over\nthe baseline with single-task learning. The method was",
          "baseline by about 22%. The baseline used ResNet networks.": "at\nthe\nICML Expressive Vocalization\n(ExVo) Competition"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "evaluated on ﬁve different datasets for generalization.",
          "baseline by about 22%. The baseline used ResNet networks.": "2022. The dataset\nis\na\nlarge-scale\nemotional non-linguistic"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "Atmaja and Akagi [11] evaluated different\nloss functions –",
          "baseline by about 22%. The baseline used ResNet networks.": "vocalization known as vocal burst. An example of\nthis vo-"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "CCC, MSE, and MAE – for multitask learning dimensional",
          "baseline by about 22%. The baseline used ResNet networks.": "cal burst\nis\n“argh!”\nto express distress\nemotion. There\nare"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "emotion recognition. The traditional approach of deep learn-",
          "baseline by about 22%. The baseline used ResNet networks.": "ten\nemotions\nrated\nin\ncontinuous\nscores. These\nemotions"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "ing\ncommonly\nemployed MSE loss\nto minimize\nthe\nerror",
          "baseline by about 22%. The baseline used ResNet networks.": "are Amusement, Awe, Awkwardness, Distress, Excitement,"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "and training stage. Since the goal\nis to maximize CCC,\nthey",
          "baseline by about 22%. The baseline used ResNet networks.": "Fear, Horror, Sadness, Surprise, and Triumph. The data were"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "proposed to use CCC loss\nas\nthe\nloss\nfunction to replace",
          "baseline by about 22%. The baseline used ResNet networks.": "collected\nfrom 1702\nspeakers\naged\n20-39\nyears\nold. The"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "MSE. The\nresults\non\ntwo\ndatasets\n(IEMOCAP and MSP-",
          "baseline by about 22%. The baseline used ResNet networks.": "collection locations are China, South Africa, Venezuela, and"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "IMPROV) and two acoustic features\n(GeMAPS and python",
          "baseline by about 22%. The baseline used ResNet networks.": "the US. The total duration of\nthe dataset\nis almost 37 hours"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "Audio Analysis)\nshowed the\nconsistency that CCC loss\nis",
          "baseline by about 22%. The baseline used ResNet networks.": "(36 hours 47 minutes). Although the data were split into train,"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "superior\nto MSE and MAE.",
          "baseline by about 22%. The baseline used ResNet networks.": "validation, and test,\nthe test set was closed by the organizer"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "Li\net\nal.\n[12]\nproposed\nadditional\ninformation\non\nage,",
          "baseline by about 22%. The baseline used ResNet networks.": "of the competition. Hence, we evaluated our methods mostly"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "gender, and emotion for speaker veriﬁcation using multitask",
          "baseline by about 22%. The baseline used ResNet networks.": "on the validation set (except\nin the last part, where test results"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "learning and domain adversarial training. The multitask learn-",
          "baseline by about 22%. The baseline used ResNet networks.": "are reported). More details about\nthe dataset can be found in"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "ing part minimizes losses of three variables (speaker, gender,",
          "baseline by about 22%. The baseline used ResNet networks.": "the [2]."
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "and nationality). The domain adversarial\ntraining, which also",
          "baseline by about 22%. The baseline used ResNet networks.": ""
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "",
          "baseline by about 22%. The baseline used ResNet networks.": "B. Pre-trained Acoustic Embedding"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "employs multitask learning, minimizes losses of speaker and",
          "baseline by about 22%. The baseline used ResNet networks.": ""
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "emotion. The\nresults\nshowed\nthat multitask\nimproved\nthe",
          "baseline by about 22%. The baseline used ResNet networks.": "We\nevaluated\na\npre-trained model\nﬁnetuned\non\nan\naf-"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "performance from the baseline by about 16% while the do-",
          "baseline by about 22%. The baseline used ResNet networks.": "fective\nspeech\ndataset. The model\n[15],\n[16]\nis\nbased\non"
        },
        {
          "with the same or similar inputs. Speech processing is an ideal": "main adversarial\ntraining improved the performance from the",
          "baseline by about 22%. The baseline used ResNet networks.": "wav2vec2-large-robust model\n[17]. The model\nis\ntrained on"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "derived from YouTube with valence, arousal, and dominance"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": ""
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": ""
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "scores.\nFor\nthis\nﬁnetuning,\nthe\ncombined\nsamples\non\nthe"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "MSP-Podcasts dataset have a combined length of roughly 21"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "hours. The model extracted the speech embedding from the"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": ""
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "dataset (HUME-VB) with a size of 1024 dimensions for each"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": ""
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "utterance. The output\nlayers of\nthe model,\ni.e.,\nthe logits, are"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": ""
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "the scores of arousal, dominance, and valence,\nin ranges [0,"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": ""
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": ""
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "1]. We experimented with two variants of speech embedding"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": ""
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "with\nthis model. The ﬁrst\nembedding\nis\nthe\nhidden\nstates"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "of\nthe\nlast\nlayer\nbefore\nthe\noutput\nlayer\n(1024-dim),\nand"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": ""
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "the second embedding is\nthe concatenation of hidden states"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": ""
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "with the\nlogits\n(1027-dim). We named the ﬁrst\nembedding"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": ""
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "“w2v2-R-er” (wav2vec 2.0 robust emotion recognition), and"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": ""
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "the\nsecond\nembedding“w2v2-R-vad”\n(wav2vec\n2.0\nrobust"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": ""
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "emotion recognition with valence, arousal, and dominance)."
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": ""
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "Notice the term of acoustic embedding (extracted from vocal"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": ""
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "bursts) here is used instead of\n(traditonal) acoustic features"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "or speech embedding (extracted from speech)."
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": ""
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": ""
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "C. Classiﬁer"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": ""
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "We employed a model of multitask learning adopted from"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": ""
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "[2]. The architecture of\nthe model\nis shown in Figure 1. The"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": ""
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "model accepts the input of acoustic features depending on the"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": ""
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "size of\nthe features. For\ninstance,\nthe dimension for w2v2-"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "R-er\nis 1024. Then,\ntwo linear\nlayers are stacked as\nshared"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "layers. The number of nodes\nfor each layer\nis 128 and 64,"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "respectively. We\napplied layer normalization [19]\nfor\neach"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "linear\nlayer,\nfollowed by LeakyReLU activation functions. A"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "group of\nlayers builds up the\nindependent\nlayers\nfor\neach"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "task. For emotion and country prediction, there is only a layer"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "followed by output\nlayers. For age prediction, we used two"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": ""
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "independent\nlayers\nto bridge\nthe gap in the big number of"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": ""
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "nodes\nfrom the shared layer\n(64)\nto the single-node output"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "layer. The nodes for\nthese independent\nlayers are 32 and 16,"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": ""
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "respectively."
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": ""
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "The loss function minimizes losses of three tasks: MSE for"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": ""
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "emotion recognition age prediction,\ncross-entropy (CE)\nfor"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": ""
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "country prediction. The total\nis weighting\nloss function (LT )"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "sum of\nthree losses given by the following formula,"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": ""
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": ""
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "(cid:19)"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "(cid:18) Li"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "α 2\n.\n(1)"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "3(cid:88) i\nLT =\n2eα +"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "=1"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": ""
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "The\ncoefﬁcient of α is\nset\nto be 0.34, 0.33,\nand 0.33 for"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": ""
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "emotion, country, and age,\nrespectively."
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": ""
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": ""
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "D. Evaluation Metrics"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": ""
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "The\nevaluation of\nthree paralinguistic\ntasks\nin this\nstudy"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "used speciﬁc metric for each task. The emotion recognition"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "is\nevaluated\nin\nconcordance\ncorrelation\ncoefﬁcient\n(CCC),"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "age recognition is evaluated in mean absolute error\n(MAE),"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "and nationality prediction is evaluated in unweighted average"
        },
        {
          "MSP-Podcasts dataset\n[18],\na\nlarge\naffective\nspeech corpus": "recall\n(UAR). These metrics are described below,"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "Cou-UAR"
        },
        {
          "TABLE I": "0.520 ± 0.006"
        },
        {
          "TABLE I": "0.520 ± 0.004"
        },
        {
          "TABLE I": "0.531 ± 0.003"
        },
        {
          "TABLE I": "0.512 ± 0.005"
        },
        {
          "TABLE I": "0.527 ± 0.006"
        },
        {
          "TABLE I": "0.518 ± 0.006"
        },
        {
          "TABLE I": "0.527 ± 0.006"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "106": "",
          "0.537 ± 0.010": "",
          "0.527 ± 0.006": "",
          "0.247 ± 0.005": "",
          "0.3844 ± 0.003": ""
        },
        {
          "106": "",
          "0.537 ± 0.010": "",
          "0.527 ± 0.006": "",
          "0.247 ± 0.005": "be explained by the fact",
          "0.3844 ± 0.003": ""
        },
        {
          "106": "",
          "0.537 ± 0.010": "",
          "0.527 ± 0.006": "",
          "0.247 ± 0.005": "",
          "0.3844 ± 0.003": "related to the loudness of the waveform. Hence, such normal-"
        },
        {
          "106": "",
          "0.537 ± 0.010": "",
          "0.527 ± 0.006": "",
          "0.247 ± 0.005": "",
          "0.3844 ± 0.003": ""
        },
        {
          "106": "",
          "0.537 ± 0.010": "",
          "0.527 ± 0.006": "",
          "0.247 ± 0.005": "ization of",
          "0.3844 ± 0.003": "the waveform will"
        },
        {
          "106": "",
          "0.537 ± 0.010": "",
          "0.527 ± 0.006": "",
          "0.247 ± 0.005": "",
          "0.3844 ± 0.003": "that discriminates these paralinguistics labels. The results for"
        },
        {
          "106": "UAR",
          "0.537 ± 0.010": "1/MAE",
          "0.527 ± 0.006": "SM T L",
          "0.247 ± 0.005": "",
          "0.3844 ± 0.003": ""
        },
        {
          "106": "0.506",
          "0.537 ± 0.010": "0.237",
          "0.527 ± 0.006": "0.349",
          "0.247 ± 0.005": "two acoustic",
          "0.3844 ± 0.003": "emebddings, w2v2-R-er"
        },
        {
          "106": "0.423",
          "0.537 ± 0.010": "0.249",
          "0.527 ± 0.006": "0.324",
          "0.247 ± 0.005": "",
          "0.3844 ± 0.003": "consistent with and without normalization, highlighting the"
        },
        {
          "106": "0.417",
          "0.537 ± 0.010": "0.234",
          "0.527 ± 0.006": "0.311",
          "0.247 ± 0.005": "",
          "0.3844 ± 0.003": ""
        },
        {
          "106": "",
          "0.537 ± 0.010": "",
          "0.527 ± 0.006": "",
          "0.247 ± 0.005": "",
          "0.3844 ± 0.003": "unnecessary processing of normalizing the speech waveform."
        },
        {
          "106": "0.423",
          "0.537 ± 0.010": "0.238",
          "0.527 ± 0.006": "0.319",
          "0.247 ± 0.005": "",
          "0.3844 ± 0.003": ""
        },
        {
          "106": "0.432",
          "0.537 ± 0.010": "0.218",
          "0.527 ± 0.006": "0.314",
          "0.247 ± 0.005": "",
          "0.3844 ± 0.003": ""
        },
        {
          "106": "",
          "0.537 ± 0.010": "",
          "0.527 ± 0.006": "",
          "0.247 ± 0.005": "",
          "0.3844 ± 0.003": ""
        },
        {
          "106": "0.438",
          "0.537 ± 0.010": "0.225",
          "0.527 ± 0.006": "0.321",
          "0.247 ± 0.005": "",
          "0.3844 ± 0.003": ""
        },
        {
          "106": "",
          "0.537 ± 0.010": "",
          "0.527 ± 0.006": "",
          "0.247 ± 0.005": "",
          "0.3844 ± 0.003": "COMPARISON OF THE ACOUSTIC EMBEDDINGS WITH AND WITHOUT"
        },
        {
          "106": "0.456",
          "0.537 ± 0.010": "0.227",
          "0.527 ± 0.006": "0.322",
          "0.247 ± 0.005": "",
          "0.3844 ± 0.003": ""
        },
        {
          "106": "",
          "0.537 ± 0.010": "",
          "0.527 ± 0.006": "",
          "0.247 ± 0.005": "",
          "0.3844 ± 0.003": "NORMALIZATIONS ON VALIDATION SET"
        },
        {
          "106": "0.523",
          "0.537 ± 0.010": "0.252",
          "0.527 ± 0.006": "0.386",
          "0.247 ± 0.005": "",
          "0.3844 ± 0.003": ""
        },
        {
          "106": "0.525",
          "0.537 ± 0.010": "0.253",
          "0.527 ± 0.006": "0.388",
          "0.247 ± 0.005": "",
          "0.3844 ± 0.003": ""
        },
        {
          "106": "",
          "0.537 ± 0.010": "",
          "0.527 ± 0.006": "",
          "0.247 ± 0.005": "Feature",
          "0.3844 ± 0.003": "Batch size"
        },
        {
          "106": "",
          "0.537 ± 0.010": "",
          "0.527 ± 0.006": "",
          "0.247 ± 0.005": "w2v2-R-er",
          "0.3844 ± 0.003": "8"
        },
        {
          "106": "",
          "0.537 ± 0.010": "",
          "0.527 ± 0.006": "",
          "0.247 ± 0.005": "w2v2-R-er-norm",
          "0.3844 ± 0.003": "8"
        },
        {
          "106": "",
          "0.537 ± 0.010": "",
          "0.527 ± 0.006": "",
          "0.247 ± 0.005": "w2v2-R-vad",
          "0.3844 ± 0.003": "2"
        },
        {
          "106": "",
          "0.537 ± 0.010": "",
          "0.527 ± 0.006": "",
          "0.247 ± 0.005": "",
          "0.3844 ± 0.003": ""
        },
        {
          "106": "",
          "0.537 ± 0.010": "",
          "0.527 ± 0.006": "",
          "0.247 ± 0.005": "w2v2-R-vad-norm",
          "0.3844 ± 0.003": "2"
        },
        {
          "106": "",
          "0.537 ± 0.010": "",
          "0.527 ± 0.006": "",
          "0.247 ± 0.005": "",
          "0.3844 ± 0.003": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "vious validation evaluation for": "",
          "the closed test set evaluation.": ""
        },
        {
          "vious validation evaluation for": "Note that the best result from the validation set reported in this",
          "the closed test set evaluation.": ""
        },
        {
          "vious validation evaluation for": "",
          "the closed test set evaluation.": ""
        },
        {
          "vious validation evaluation for": "study was not submitted to obtain the test scores due to time",
          "the closed test set evaluation.": ""
        },
        {
          "vious validation evaluation for": "limitations (for obtaining the test scores as part of The ICML",
          "the closed test set evaluation.": ""
        },
        {
          "vious validation evaluation for": "",
          "the closed test set evaluation.": ""
        },
        {
          "vious validation evaluation for": "2022 Expressive Vocalizations Workshop and Competition).",
          "the closed test set evaluation.": ""
        },
        {
          "vious validation evaluation for": "",
          "the closed test set evaluation.": ""
        },
        {
          "vious validation evaluation for": "In the submitted results, we evaluated two predictions",
          "the closed test set evaluation.": "from"
        },
        {
          "vious validation evaluation for": "",
          "the closed test set evaluation.": ""
        },
        {
          "vious validation evaluation for": "features",
          "the closed test set evaluation.": "seed and batch"
        },
        {
          "vious validation evaluation for": "",
          "the closed test set evaluation.": ""
        },
        {
          "vious validation evaluation for": "sizes. The ﬁrst prediction is with a batch size of 8 and a",
          "the closed test set evaluation.": ""
        },
        {
          "vious validation evaluation for": "",
          "the closed test set evaluation.": ""
        },
        {
          "vious validation evaluation for": "seed of 42. The second prediction is with a batch size of 4",
          "the closed test set evaluation.": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "RESULTS OF THE SUBMITTED PREDICTIONS ON THE TEST SET; ’BEST’ IN"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "SEED COLUMN IS OBTAINED FROM VALUES IN RANGE [101, 102, 103,"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "104, 105, 106]."
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "Feature\nBatch size\nSeed\nSM T L"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "ComParE\n8\nbest\n0.335"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "eGeMAPS\n8\nbest\n0.214"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "BoAW-125\n8\nbest\n0.299"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "BoAW-250\n8\nbest\n0.305"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "BoAW-500\n8\nbest\n0.302"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "BoAW-1000\n8\nbest\n0.307"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "BoAW-2000\n8\nbest\n0.303"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "DeepSpec\n8\nbest\n0.305"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "w2v2-R-er\n8\n42\n0.358"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "0.378\nw2v2-R-er\n4\n106"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "emebdding extracted from a pre-trained model. The model"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "is ﬁnetuned\non\nan\naffective\nspeech\ndataset. The\nextracted"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "acoustic emebddings were\nfed to an architecture\nconsisting"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "of\nshared\nlayers\nand\nindependent\nlayers\nfor\nthese\nthree"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "tasks. The\nresults\nshowed\nimprovements\nover\nthe\nbaseline"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "methods with the common speech representations (ComParE,"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "eGeMAPS, BoAW,\nand DeepSpectrum).\nTwo\nvariants\nof"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "acoustic embeddings are evaluated with original hidden states"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "and\nconcatenation\nof\nhidden\nstates with\nlogits. The\nlatter"
        },
        {
          "TABLE V": "performed best on the validation set. We conducted ablation"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "studies on different seeds, batch sizes, and normalization. The"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "study ﬁnds the optimum seed and batch size of the evaluated"
        },
        {
          "TABLE V": "ranges\nand ﬁnds no improvement\nin performing waveform"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "normalizations. While this\nstudy treated all emotions\nin the"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "same weights,\nfuture studies may be directed to adjust\nthese"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "weights\nfor\noptimum emotion\nrecognition,\nas well\nas\nto"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "improve overall harmonic mean evaluation for\nall\ntasks\nin"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "Multitask learning."
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "ACKNOWLEDGMENT"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "This\npaper\nis\nbased\non\nresults\nobtained\nfrom a\nproject,"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "JPNP20006, commissioned by the New Energy and Industrial"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "Technology Development Organization (NEDO), Japan."
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "REFERENCES"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "[1] A. Batliner, S. Hantke, and B. W. Schuller, “Ethics and Good Practice"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "in Computational Paralinguistics,”\nIEEE Trans. Affect. Comput., vol."
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "3045, no. c, pp. 1–1, 2020."
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "[2] A.\nBaird,\nP.\nTzirakis, G. Gidel, M.\nJiralerspong,\nE.\nB. Muller,"
        },
        {
          "TABLE V": "K. Mathewson, B. Schuller, E. Cambria, D. Keltner, and A. Cowen,"
        },
        {
          "TABLE V": "“The ICML 2022 Expressive Vocalizations Workshop and Competition:"
        },
        {
          "TABLE V": "Recognizing, Generating, and Personalizing Vocal Bursts,” 2022."
        },
        {
          "TABLE V": "[3] A. S. Cowen, H. A. Elfenbein, P. Laukka, and D. Keltner, “Mapping 24"
        },
        {
          "TABLE V": "emotions conveyed by brief human vocalization.” Am. Psychol., vol. 74,"
        },
        {
          "TABLE V": "no. 6, pp. 698–712, sep 2019."
        },
        {
          "TABLE V": "[4] K. R. Scherer, “Vocal affect expression: a review and a model for future"
        },
        {
          "TABLE V": "research.” Psychol. Bull., vol. 99, no. 2, pp. 143–165, mar 1986."
        },
        {
          "TABLE V": "[5] B. W. Schuller, A. Batliner, S. Amiriparian, C. Bergler, M. Gerczuk,"
        },
        {
          "TABLE V": "N. Holz, P. Larrouy-Maestri, S. P. Bayerl, K. Riedhammer, A. Mallol-"
        },
        {
          "TABLE V": "Ragolta, M. Pateraki, H. Coppock, I. Kiskin, M. Sinka, and S. Roberts,"
        },
        {
          "TABLE V": "“The ACM Multimedia 2022 Computational Paralinguistics Challenge:"
        },
        {
          "TABLE V": "Vocalisations, Stuttering, Activity, & Mosquitoes,” 2022."
        },
        {
          "TABLE V": "[6] H. Kaya, A. A.\nSalah, A. Karpov, O.\nFrolova, A. Grigorev,\nand"
        },
        {
          "TABLE V": "E. Lyakso, “Emotion, age, and gender classiﬁcation in children’s speech"
        },
        {
          "TABLE V": "by humans and machines,” Comput. Speech Lang., vol. 46, pp. 268–283,"
        },
        {
          "TABLE V": "2017.\n[Online]. Available: http://dx.doi.org/10.1016/j.csl.2017.06.002"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "RESULTS OF THE SUBMITTED PREDICTIONS ON THE TEST SET; ’BEST’ IN"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "SEED COLUMN IS OBTAINED FROM VALUES IN RANGE [101, 102, 103,"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "104, 105, 106]."
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "Feature\nBatch size\nSeed\nSM T L"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "ComParE\n8\nbest\n0.335"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "eGeMAPS\n8\nbest\n0.214"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "BoAW-125\n8\nbest\n0.299"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "BoAW-250\n8\nbest\n0.305"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "BoAW-500\n8\nbest\n0.302"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "BoAW-1000\n8\nbest\n0.307"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "BoAW-2000\n8\nbest\n0.303"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "DeepSpec\n8\nbest\n0.305"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "w2v2-R-er\n8\n42\n0.358"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "0.378\nw2v2-R-er\n4\n106"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "emebdding extracted from a pre-trained model. The model"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "is ﬁnetuned\non\nan\naffective\nspeech\ndataset. The\nextracted"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "acoustic emebddings were\nfed to an architecture\nconsisting"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "of\nshared\nlayers\nand\nindependent\nlayers\nfor\nthese\nthree"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "tasks. The\nresults\nshowed\nimprovements\nover\nthe\nbaseline"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "methods with the common speech representations (ComParE,"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "eGeMAPS, BoAW,\nand DeepSpectrum).\nTwo\nvariants\nof"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "acoustic embeddings are evaluated with original hidden states"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "and\nconcatenation\nof\nhidden\nstates with\nlogits. The\nlatter"
        },
        {
          "TABLE V": "performed best on the validation set. We conducted ablation"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "studies on different seeds, batch sizes, and normalization. The"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "study ﬁnds the optimum seed and batch size of the evaluated"
        },
        {
          "TABLE V": "ranges\nand ﬁnds no improvement\nin performing waveform"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "normalizations. While this\nstudy treated all emotions\nin the"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "same weights,\nfuture studies may be directed to adjust\nthese"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "weights\nfor\noptimum emotion\nrecognition,\nas well\nas\nto"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "improve overall harmonic mean evaluation for\nall\ntasks\nin"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "Multitask learning."
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "ACKNOWLEDGMENT"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "This\npaper\nis\nbased\non\nresults\nobtained\nfrom a\nproject,"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "JPNP20006, commissioned by the New Energy and Industrial"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "Technology Development Organization (NEDO), Japan."
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "REFERENCES"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "[1] A. Batliner, S. Hantke, and B. W. Schuller, “Ethics and Good Practice"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "in Computational Paralinguistics,”\nIEEE Trans. Affect. Comput., vol."
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "3045, no. c, pp. 1–1, 2020."
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "[2] A.\nBaird,\nP.\nTzirakis, G. Gidel, M.\nJiralerspong,\nE.\nB. Muller,"
        },
        {
          "TABLE V": "K. Mathewson, B. Schuller, E. Cambria, D. Keltner, and A. Cowen,"
        },
        {
          "TABLE V": "“The ICML 2022 Expressive Vocalizations Workshop and Competition:"
        },
        {
          "TABLE V": "Recognizing, Generating, and Personalizing Vocal Bursts,” 2022."
        },
        {
          "TABLE V": "[3] A. S. Cowen, H. A. Elfenbein, P. Laukka, and D. Keltner, “Mapping 24"
        },
        {
          "TABLE V": "emotions conveyed by brief human vocalization.” Am. Psychol., vol. 74,"
        },
        {
          "TABLE V": "no. 6, pp. 698–712, sep 2019."
        },
        {
          "TABLE V": "[4] K. R. Scherer, “Vocal affect expression: a review and a model for future"
        },
        {
          "TABLE V": "research.” Psychol. Bull., vol. 99, no. 2, pp. 143–165, mar 1986."
        },
        {
          "TABLE V": "[5] B. W. Schuller, A. Batliner, S. Amiriparian, C. Bergler, M. Gerczuk,"
        },
        {
          "TABLE V": "N. Holz, P. Larrouy-Maestri, S. P. Bayerl, K. Riedhammer, A. Mallol-"
        },
        {
          "TABLE V": "Ragolta, M. Pateraki, H. Coppock, I. Kiskin, M. Sinka, and S. Roberts,"
        },
        {
          "TABLE V": "“The ACM Multimedia 2022 Computational Paralinguistics Challenge:"
        },
        {
          "TABLE V": "Vocalisations, Stuttering, Activity, & Mosquitoes,” 2022."
        },
        {
          "TABLE V": "[6] H. Kaya, A. A.\nSalah, A. Karpov, O.\nFrolova, A. Grigorev,\nand"
        },
        {
          "TABLE V": "E. Lyakso, “Emotion, age, and gender classiﬁcation in children’s speech"
        },
        {
          "TABLE V": "by humans and machines,” Comput. Speech Lang., vol. 46, pp. 268–283,"
        },
        {
          "TABLE V": "2017.\n[Online]. Available: http://dx.doi.org/10.1016/j.csl.2017.06.002"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A. Weiss,": "https://zenodo.org/record/3606573",
          "“librosa/librosa:\n0.7.2,”\njan\n2020.\n[Online]. Available:": ""
        },
        {
          "A. Weiss,": "[26] A. Baevski, H. Zhou, A. Mohamed,",
          "“librosa/librosa:\n0.7.2,”\njan\n2020.\n[Online]. Available:": "and M. Auli,\n“wav2vec 2.0: A"
        },
        {
          "A. Weiss,": "",
          "“librosa/librosa:\n0.7.2,”\njan\n2020.\n[Online]. Available:": "framework for self-supervised learning of speech representations,” Adv."
        },
        {
          "A. Weiss,": "Neural",
          "“librosa/librosa:\n0.7.2,”\njan\n2020.\n[Online]. Available:": "Inf. Process. Syst., vol. 2020-Decem, no. Figure 1, pp. 1–12,"
        },
        {
          "A. Weiss,": "2020.",
          "“librosa/librosa:\n0.7.2,”\njan\n2020.\n[Online]. Available:": ""
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Ethics and Good Practice in Computational Paralinguistics",
      "authors": [
        "A Batliner",
        "S Hantke",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "2",
      "title": "The ICML 2022 Expressive Vocalizations Workshop and Competition: Recognizing, Generating, and Personalizing Vocal Bursts",
      "authors": [
        "A Baird",
        "P Tzirakis",
        "G Gidel",
        "M Jiralerspong",
        "E Muller",
        "K Mathewson",
        "B Schuller",
        "E Cambria",
        "D Keltner",
        "A Cowen"
      ],
      "year": "2022",
      "venue": "The ICML 2022 Expressive Vocalizations Workshop and Competition: Recognizing, Generating, and Personalizing Vocal Bursts"
    },
    {
      "citation_id": "3",
      "title": "Mapping 24 emotions conveyed by brief human vocalization",
      "authors": [
        "A Cowen",
        "H Elfenbein",
        "P Laukka",
        "D Keltner"
      ],
      "year": "2019",
      "venue": "Am. Psychol"
    },
    {
      "citation_id": "4",
      "title": "Vocal affect expression: a review and a model for future research",
      "authors": [
        "K Scherer"
      ],
      "year": "1986",
      "venue": "Psychol. Bull"
    },
    {
      "citation_id": "5",
      "title": "The ACM Multimedia 2022 Computational Paralinguistics Challenge: Vocalisations, Stuttering, Activity, & Mosquitoes",
      "authors": [
        "B Schuller",
        "A Batliner",
        "S Amiriparian",
        "C Bergler",
        "M Gerczuk",
        "N Holz",
        "P Larrouy-Maestri",
        "S Bayerl",
        "K Riedhammer",
        "A Mallol-Ragolta",
        "M Pateraki",
        "H Coppock",
        "I Kiskin",
        "M Sinka",
        "S Roberts"
      ],
      "year": "2022",
      "venue": "The ACM Multimedia 2022 Computational Paralinguistics Challenge: Vocalisations, Stuttering, Activity, & Mosquitoes"
    },
    {
      "citation_id": "6",
      "title": "Emotion, age, and gender classification in children's speech by humans and machines",
      "authors": [
        "H Kaya",
        "A Salah",
        "A Karpov",
        "O Frolova",
        "A Grigorev",
        "E Lyakso"
      ],
      "year": "2017",
      "venue": "Comput. Speech Lang",
      "doi": "10.1016/j.csl.2017.06.002"
    },
    {
      "citation_id": "7",
      "title": "Transferring Age and Gender Attributes for Dimensional Emotion Prediction from Big Speech Data Using Hierarchical Deep Learning",
      "authors": [
        "H Zhao",
        "N Ye",
        "R Wang"
      ],
      "year": "2018",
      "venue": "2018 IEEE 4th Int. Conf. Big Data Secur. Cloud (BigDataSecurity), IEEE Int. Conf. High Perform. Smart Comput. IEEE Int. Conf. Intell. Data Secur"
    },
    {
      "citation_id": "8",
      "title": "Jointly Predicting Arousal, Valence and Dominance with Multi-Task Learning",
      "authors": [
        "S Parthasarathy",
        "C Busso"
      ],
      "year": "2017",
      "venue": "Interspeech 2017. ISCA: ISCA"
    },
    {
      "citation_id": "9",
      "title": "The Generalization Effect for Multilingual Speech Emotion Recognition across Heterogeneous Languages",
      "authors": [
        "S.-W Lee"
      ],
      "year": "2019",
      "venue": "ICASSP 2019 -2019 IEEE Int. Conf. Acoust. Speech Signal Process"
    },
    {
      "citation_id": "10",
      "title": "Towards Speech Emotion Recognition \"in the Wild\" Using Aggregated Corpora and Deep Multi-Task Learning",
      "authors": [
        "J Kim",
        "G Englebienne",
        "K Truong",
        "V Evers"
      ],
      "year": "2017",
      "venue": "Interspeech 2017. ISCA"
    },
    {
      "citation_id": "11",
      "title": "Evaluation of Error and Correlation-Based Loss Functions For Multitask Learning Dimensional Speech Emotion Recognition",
      "authors": [
        "B Atmaja",
        "M Akagi"
      ],
      "year": "2020",
      "venue": "J. Phys. Conf. Ser"
    },
    {
      "citation_id": "12",
      "title": "Segment-level Effects of Gender , Nationality and Emotion Information on Text-independent Speaker Verification",
      "authors": [
        "K Li",
        "M Akagi",
        "Y Wu",
        "J Dang"
      ],
      "year": "2020",
      "venue": "Segment-level Effects of Gender , Nationality and Emotion Information on Text-independent Speaker Verification"
    },
    {
      "citation_id": "13",
      "title": "Speech Emotion Recognition with Multi-Task Learning",
      "authors": [
        "X Cai",
        "J Yuan",
        "R Zheng",
        "L Huang",
        "K Church"
      ],
      "year": "2021",
      "venue": "Interspeech 2021. ISCA: ISCA"
    },
    {
      "citation_id": "14",
      "title": "Speech Emotion and Naturalness Recognitions with Multitask and Single-task Learnings",
      "authors": [
        "B Atmaja",
        "A Sasou",
        "M Akagi"
      ],
      "year": "2022",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "15",
      "title": "Model for Dimensional Speech Emotion Recognition based on Wav2vec 2.0 (1.1.0)",
      "authors": [
        "B Wagner",
        "Johannes",
        "Triantafyllopoulos",
        "Andreas",
        "Wierstorf",
        "Hagen",
        "Maximilian Schmitt",
        "Eyben",
        "Florian",
        "Schuller"
      ],
      "venue": "Model for Dimensional Speech Emotion Recognition based on Wav2vec 2.0 (1.1.0)",
      "doi": "10.5281/zenodo.6221127"
    },
    {
      "citation_id": "16",
      "title": "Dawn of the transformer era in speech emotion recognition: closing the valence gap",
      "authors": [
        "J Wagner",
        "A Triantafyllopoulos",
        "H Wierstorf",
        "M Schmitt",
        "F Burkhardt",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "Dawn of the transformer era in speech emotion recognition: closing the valence gap"
    },
    {
      "citation_id": "17",
      "title": "Robust wav2vec 2.0: Analyzing domain shift in self-supervised pre-training",
      "authors": [
        "W Hsu",
        "A Sriram",
        "A Baevski",
        "T Likhomanenko",
        "Q Xu",
        "V Pratap",
        "J Kahn",
        "A Lee",
        "R Collobert",
        "G Synnaeve",
        "M Auli"
      ],
      "year": "2021",
      "venue": "Proc. Annu. Conf. Int"
    },
    {
      "citation_id": "18",
      "title": "Building Naturalistic Emotionally Balanced Speech Corpus by Retrieving Emotional Speech from Existing Podcast Recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "19",
      "title": "Layer Normalization",
      "authors": [
        "J Ba",
        "J Kiros",
        "G Hinton"
      ],
      "year": "2015",
      "venue": "Layer Normalization",
      "arxiv": "arXiv:1607.06450v1"
    },
    {
      "citation_id": "20",
      "title": "Fusion Approaches for Emotion Recognition from Speech Using Acoustic and Text-Based Features",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer",
        "A Gravano"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE Int. Conf. Acoust. Speech Signal Process"
    },
    {
      "citation_id": "21",
      "title": "Multi-corpus Experiment on Continuous Speech Emotion Recognition: Convolution or Recurrence?\" in Int. Conf. Speech Comput",
      "authors": [
        "M Macary",
        "M Lebourdais",
        "M Tahon",
        "Y Estève",
        "A Rousseau"
      ],
      "year": "2020",
      "venue": "Multi-corpus Experiment on Continuous Speech Emotion Recognition: Convolution or Recurrence?\" in Int. Conf. Speech Comput"
    },
    {
      "citation_id": "22",
      "title": "Survey on bimodal speech emotion recognition from acoustic and linguistic information fusion",
      "authors": [
        "B Atmaja",
        "A Sasou",
        "M Akagi"
      ],
      "year": "2022",
      "venue": "Speech Commun"
    },
    {
      "citation_id": "23",
      "title": "Group Normalization",
      "authors": [
        "Y Wu",
        "K He"
      ],
      "year": "2018",
      "venue": "Group Normalization"
    },
    {
      "citation_id": "24",
      "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "authors": [
        "S Ioffe",
        "C Szegedy"
      ],
      "year": "2015",
      "venue": "32nd Int. Conf. Mach. Learn. ICML 2015"
    },
    {
      "citation_id": "25",
      "title": "",
      "authors": [
        "B Mcfee",
        "V Lostanlen",
        "M Mcvicar",
        "A Metsai",
        "S Balke",
        "C Thomé",
        "C Raffel",
        "A Malek",
        "D Lee",
        "F Zalkow",
        "K Lee",
        "O Nieto",
        "J Mason",
        "D Ellis",
        "R Yamamoto",
        "S Seyfarth",
        "E Battenberg",
        "V Morozov",
        "R Bittner",
        "K Choi",
        "J Moore",
        "Z Wei",
        "S Hidaka",
        "P Nullmightybofo",
        "F.-R Friesch",
        "D Stöter",
        "T Hereñú",
        "M Kim",
        "A Vollrath",
        "Weiss"
      ],
      "year": "2020",
      "venue": ""
    },
    {
      "citation_id": "26",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "H Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Adv. Neural Inf. Process. Syst"
    }
  ]
}