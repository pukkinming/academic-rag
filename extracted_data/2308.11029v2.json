{
  "paper_id": "2308.11029v2",
  "title": "Rba-Gcn: Relational Bilevel Aggregation Graph Convolutional Network For Emotion Recognition",
  "published": "2023-08-18T11:29:12Z",
  "authors": [
    "Lin Yuan",
    "Guoheng Huang",
    "Fenghuan Li",
    "Xiaochen Yuan",
    "Chi-Man Pun",
    "Guo Zhong"
  ],
  "keywords": [
    "Emotion recognition",
    "multimodal fusion",
    "context modeling",
    "similarity cluster"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition in conversation (ERC) has received increasing attention from researchers due to its wide range of applications. As conversation has a natural graph structure, numerous approaches used to model ERC based on graph convolutional networks (GCNs) have yielded significant results. However, the aggregation approach of traditional GCNs suffers from the node information redundancy problem, leading to node discriminant information loss. Additionally, single-layer GCNs lack the capacity to capture long-range contextual information from the graph. Furthermore, the majority of approaches are based on textual modality or stitching together different modalities, resulting in a weak ability to capture interactions between modalities. To address these problems, we present the relational bilevel aggregation graph convolutional network (RBA-GCN), which consists of three modules: the graph generation module (GGM), similarity-based cluster building module (SCBM) and bilevel aggregation module (BiAM). First, GGM constructs a novel graph to reduce the redundancy of target node information. Then, SCBM calculates the node similarity in the target node and its structural neighborhood, where noisy information with low similarity is filtered out to preserve the discriminant information of the node. Meanwhile, BiAM is a novel aggregation method that can preserve the information of nodes during the aggregation process. This module can construct the interaction between different modalities and capture longrange contextual information based on similarity clusters. On both the IEMOCAP and MELD datasets, the weighted average F1 score of RBA-GCN has a 2.17∼5.21% improvement over that of the most advanced method. Our code is available at https://github.com/luftmenscher/RBA-GCN and our article \"RBA-GCN:",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "T HE purpose of emotion recognition in conversation (ERC) is to assign each sentence in a conversation to a specific emotion category. ERC is becoming an important research topic due to its broad applications in various scenarios, such as chatbots and mental health services  [1] ,  [2] . Cambria et al.  [3]  consider understanding emotions to be an important aspect of personal development and growth; as such, it is key for the emulation of human intelligence.\n\nThe ERC task differs from traditional emotion recognition of individual isolated utterances in that it requires a combination of conversational intent, topic and context  [4] . Previous models are mainly tested by means of contextual information, e.g., bias compensation-long short-term memory (BC-LSTM)  [5] , conversational memory network (CMN)  [6] , and dialogue recurrent neural network (DialogueRNN)  [7] . However, these models cannot effectively capture long-range contextual information in a multiperson conversation scenario.\n\nTo address the shortcomings of the above models, some ERC models based on graph convolutional networks (GCNs), such as DialogueGCN  [8]  and multimodal fusion via deep graph convolution network (MMGCN)  [9] , have been proposed. The DialogueGCN model captures the dependencies between speakers by forming utterances in a conversation into a fully connected graph. Different from DialogueGCN, which utilizes only textual information, MMGCN further leverages multimodal information for emotion recognition. Similarly, it uses all the different modalities of utterances in a conversation as nodes to form a fully connected graph and applies multilayer GCNs to capture long-range contextual information. Although previously developed ERC methods have achieved great progress, they mainly exploit GCNs based on message passing neural networks (MPNNs)  [10] ,  [11] . Consequently, such models possess several shortcomings. First, single-layer GCNs aggregate only neighboring nodes. In a conversation, utterance nodes that are far from each other may also have high structural similarity. However, due to the influence of graph generation methods, a single-layer GCN may be unable to capture such utterance node information. To solve this problem, multilayer GCNs are often used to capture longrange contextual information. However, GCNs simply sum the \"messages\" from all neighborhoods. After aggregating the neighboring information via multilayer GCNs, the information possessed by similar nodes at distant locations may be disturbed by a large amount of irrelevant noisy information acquired from the nodes that are proximal to the prediction target. This leads to a situation where long-range contextual information cannot be efficiently extracted. Velickovic et al.  [12] . and Ishiwatari et al.  [13]  adopted an attention mechanism to reduce the interference of irrelevant noise information by assigning corresponding weights to adjacent nodes. In contrast, we take a different approach. We utilize the cosine similarity function to calculate the similarity between nodes, filter nodes with low similarity, and then map them to corresponding clusters according to their similarity levels. With this approach, we can effectively eliminate redundant information and preserve the discriminant information of the node. As shown in Figure  1  1 , we first consider long-range contextual information. Although U 2 and U 11 are far away, they both express excitement because they are related to the topic of eating, which can illustrate the importance of long-range contextual information for ERC. The traditional aggregation methods indiscriminately aggregate the target node U 7 and its neighboring nodes U 6 and U 8 . In contrast, our method first filters the redundant information. Then, the information within each cluster is aggregated. Finally, the information between clusters is aggregated, thus avoiding the disturbance caused by the noise of the proximal nodes and better preserving the discriminant information of the target node. Here, we define the target node as the node in the graph that currently needs to be predicted.\n\nIn summary, a relational bilevel aggregation graph convolutional network (RBA-GCN) is presented in this paper, which can capture long-range contextual information in a single-layer architecture and improve the ability to capture interactions between different modalities. Different from DialogueGCN and MMGCN, we leverage the disconnected neighborhood to handle long-range contextual information and the connected neighborhood to handle multimodal interactions. First, we model the contextual information via bidirectional long short-1 \"Friends\" Season 5 ep7: http://www.livesinabox.com/friends/scripts.shtml term memory (Bi-LSTM) with the extracted features of different modalities. Based on this, we propose to connect nodes of the same modality in the same conversation in order of conversation and connect different modalities in the same utterance. We compute the similarity between the target node and the nodes in its structural neighborhood by corner similarity and map these nodes to different clusters. In particular, we remove the nodes with low similarity in the relation definition to effectively filter out the interference of noisy information. To allow RBA-GCN to be applied to input data in different orders, making the model more robust and general, we introduce the design consideration of permutation invariance. To ensure the permutation invariance of the graph-structure data, we utilize the bilevel aggregation module (BiAM) to renew the feature representation of the node, thereby generating the final classification features of the target node. Finally, we pass the final classification features of the target node through an emotion classifier to facilitate emotion prediction.\n\nThe contributions of this paper can be summarized as follows:\n\n• A novel ERC framework (RBA-GCN) is proposed to comprehensively consider the relevance between nodes on the basis of graphs. The proposed RBA-GCN can capture long-range context information and interactions between modalities under a single-layer architecture. • To reduce the redundancy of the target node information, we present a novel graph generation module (GGM).\n\nBased on the GGM, we propose the similarity-based cluster building module (SCBM), which considers the correlation between nodes, to enhance the interclass relationship based on the similarity metric. • We present a novel graph convolution aggregation method, BiAM, to aggregate the feature representations of distant nodes through a cluster neighborhood and perform multimodal feature fusion. The proposed BiAM can preserve the discriminant information of nodes during the aggregation process. • To verify the performance of our approach, experiments on both the IEMOCAP and MELD datasets are conducted. On both datasets, the weighted average F1 score of our approach is improved by 2.17∼5.21% over that of the state-of-the-art method.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "In this section, we briefly introduce recent deep learningbased methods for ERC tasks  [14] . The specific methods are described as follows:\n\nContextual Modeling Emotion Recognition: Several advances have been made in ERC research, as the number of open-source datasets available for ERC has increased  [15] ,  [16] . First, Hazarika et al.  [6]  presented a CMN that utilized the different memories of each speaker to model the specific context of the speaker. Second, Hazarika et al.  [17]  presented an interactive conversational memory network (ICON) that accounted for the influence of interpersonal relationships in a conversation and modeled the affective influence between self and speaker hierarchically as a global memory. Then, Fig.  2 . The overall framework. First, we encode contextual information for each modality feature of the utterance, using Bi-LSTM to obtain the contextual embedding of each node. Then, we apply the RBA-GCN to filter out noisy information and reduce information redundancy, while effectively capturing the interactions between modalities and long-range contextual information. Finally, classifiers are applied to implement emotion prediction.\n\nMajumder et al.  [7]  presented the DialogueRNN, which utilized three GRU modules interacting with each other to model conversational information. In recent years, due to the outstanding ability of GCNs to process contextual information, GCNs have been used extensively in emotion recognition. For example, DialogueGCN  [8]  constructs a fully connected graph by referring to each utterance in the conversation, while the edges between two nodes form speaker dependencies. This is the first GCN-based model for emotion recognition, and good results have been obtained. Tu et al.  [18]  presented a context and emotion-aware framework, termed Sentic GAT, which tends to select common sense knowledge consistent with the context semantics and emotion of the target utterance. This approach has also achieved good results. Finally, since the Di-alogueGCN considers only the textual modality, the MMGCN  [9]  builds upon it by exploiting information from multiple modalities and encoding the information of the speaker. This method uses multilayer GCNs to capture long-range contextual information and realizes the best performance. The above GCNs applied to ERC are all part of MPNNs. To capture longrange contextual information, the multilayer GCN strategy is typically used. However, when applying multilayer GCNs, more computer resources are consumed, and the updated node information after aggregation contains a considerable amount of irrelevant information. Thus, the discriminant information of the target node is lost, and the ability to capture the context information remotely is diminished. To resolve the problems in the ERC mission, we present an RBA-GCN, which is inspired by the GEOM-GCN  [19] . GEOM-GCN maps nodes into a continuous latent space, followed by the construction of a structural neighborhood for aggregation using the geometric relationships defined in the latent space.\n\nMultimodal Emotion Recognition: Based on textual modality development and the increasing number of multimodal emotion recognition datasets  [20] ,  [21] , more researchers have been focusing on the exploitation of multimodal information. Hazarika et al.  [22] ,  [23]  simply concatenated the features of the three modalities in series for multimodal fusion with no established intermodal interactions. Chen et al.  [24]  performed word multimodal fusion for emotion recognition in solitary utterances. Zadeh et al.  [25]  proposed an MFN to fuse multiview information, which can satisfactorily coordinate features of different modalities. However, the feature fusion technique of these methods is the simple splicing of features  [26] ,  [27] . Lian et al.  [28]  proposed CTNet using a transformerbased structure to model fusion between multimodal features. Chen et al.  [29]  proposed a novel time and semantic interaction network (TSIN) to conduct emotional parsing and emotion refinement by performing fine-grained temporal alignment and cross-modal semantic interaction. Although these methods achieve some improvement in performance, the problem of data sparsity can easily occur with high-dimensional features  [30] . Recently, Zhang et al.  [31]  proposed a novel multimodal emotion recognition model for conversational videos based on reinforcement learning and domain knowledge (ERLDK); this model introduces reinforcement learning algorithms for real-time ERC with the occurrence of conversations. Yang et al.  [32]  proposed a multimodal framework named two-phase multitask emotion analysis (TPMSA). This method applies a two-stage training strategy to leverage pretrained models and a novel multitask learning strategy to investigate classification capabilities. In contrast to existing studies, our proposed graph method can preserve multimodal information and effectively capture the interactions between modalities.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Proposed Method",
      "text": "Our approach is described throughout this section. The framework of our proposed model, which is displayed in Figure  2 , is composed of a contextual encoder, an RBA-GCN, and an emotion classifier. In the contextual encoder part, the extracted features are passed into the Bi-LSTM layer to generate contextual information of the utterance. Then, the proposed RBA-GCN is applied to capture both longrange contextual information and multimodal information. The information of nodes is preserved during the aggregation. In the emotion classifier part, the node features updated by RBA-GCN are used as features for the final classification.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "A. Problem Definition",
      "text": "First, a series of utterances {u 1 , u 2 , . . . , u N } composes a conversation, where N represents the number of utterances in a conversation. The objective of ERC is to identify emotional labels (\"happy\", \"excited\", \"sad\", \"frustrated\", \"neutral\", \"angry\") for each utterance. Each utterance contains three modalities of data, namely, textual (t), visual (v), and acoustic (a), which are represented as follows:\n\nwhere u t i , u v i , and u a i represent the original feature representations of the textual, visual, and acoustic modalities of utterance u i , respectively.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Contextual Encoder",
      "text": "Context refers mainly to factors such as time, occasion, and place in which language activities occur. Contextual information is essential for ERC, especially during some short utterances, which are very important for predicting emotional labels. Therefore, we encode contextual information for each modality feature of the utterance. We input the per-modality features of an utterance into a Bi-LSTM network to encode orderly contextual information of each modality. The contextual information feature encoding is implemented as follows:\n\nwhere u x i represents a context-independent arbitrary modality raw feature representation for utterance i and x ∈ {t, v, a} represents an arbitrary modality of an utterance.\n\n--→ g x i-1 is the hidden vector obtained before processing the current sentence, and ← -g x i+1 is obtained after processing the current sentence. After the original features pass through the Bi-LSTM network, the context encoder outputs context-aware feature encodings g t i , g v i , and g a i accordingly.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Relational Bilevel Aggregation Gcn (Rba-Gcn)",
      "text": "Our proposed RBA-GCN can filter out noisy information and reduce the redundancy of target node information. Longrange contextual information and interactions between modalities can be effectively captured. RBA-GCN consists of three modules: GGM, SCBM, and BiAM.\n\n1) Graph Generation Module (GGM): Previous graph convolution models for emotion recognition typically construct all nodes in a conversation as a fully connected graph. However, this method has the following drawbacks: First, in this approach, the graph network is very large, which makes the training of the model difficult. To address this problem, sliding windows are used by the models of the graph generation approach to aggregate and update target node, but the ability to capture long-range contextual information is lacking. Second, GCNs simply sum all the \"messages\" connected to the target\n\nThe construction process of similarity clusters.\n\nnode, whereas construction using fully connected graphs leads to redundant node information. Thus, we do not know which nodes contribute to the final aggregation. To address these issues, we adopt an effective graph generation method. The specific implementation details are as follows:\n\nWe construct each conversation containing N utterances as an undirected graph G = (V, E), where V (|V | = 3N ) represents the nodes of three modalities in each utterance. E represents the edges between every two relation nodes. The graph is constructed as follows: Nodes: We represent each modality of each utterance in the conversation as a node of a graph, and the nodes of the three modalities of each utterance are represented as n t i , n v i and n a i . The nodes are initialized with the outputs from the contextual encoders: g t i , g v i and g a i . Therefore, for a conversation with N utterances, the graph has 3N nodes. Edges: To exploit multimodal information more effectively and capture long-range contextual information, we connect nodes of the same modality in the conversation sequentially according to the conversation order. Nodes of several modalities of the same utterance are connected in the same conversation. For example, in the graph, we connect n t i , n v i and n a i to each other.\n\n2) Similarity-Based Cluster Building Module (SCBM): We first calculate the node similarity in the target node and its structure neighborhood. We consider nodes with low similarity to the target node to have opposite or different labels from the target node, and such nodes are filtered. Nodes with high similarity to the target node are considered to have similar features or the same label as the target node. We map these nodes to different clusters based on the similarity between the nodes.\n\nIn this paper, we leverage the disconnected neighborhood to handle long-range contextual information and the leverage connected neighborhood to handle multimodal interactions. First, we construct the structural neighborhood N (o) on the basis of the GGM. Second, for the relationship between two nodes, we assume that the higher the similarity between them, the more similar the information between them and the higher the level of the relationship. Nodes in the same cluster have a certain similarity, and we believe that the aggregation operations of nodes in the same cluster can have a certain feature enhancement effect. Thus, we define the structural neighborhood N (o) as follows:\n\nwhere C g (o) is the connected neighborhood in the graph and D g (o) is the disconnected neighborhood in the graph.\n\nThe connected neighborhood C g (o) in the graph is defined below:\n\nThe disconnected neighborhood D g (o) in the graph is defined below:\n\nwhere u and the target node o belong to the same modality.\n\nThe similarity metric function s (u, o) is defined below:\n\nwhere sim (•, •) is the cosine similarity function. f u and f o represent the features of nodes u and o on the graph, respectively. We define the cluster operator τ through similarity mapping and specifically define the clusters as follows:\n\nwhere γ and ρ are hyperparameters. ρ is the threshold value for filtering noisy information from the clusters. ⌊•⌋ is the rounding down operation. When the similarity is less than ρ and u is in the disconnected neighborhood, we filter out this node to reduce information redundancy. We set γ to an integer so that we can obtain γ+1 clusters. Clusters is a set of all clusters. r refers to the id of the cluster and Clusters (r) refers to the r-th cluster. The process of mapping nodes to clusters is shown in Figure  3 .\n\n3) Bilevel Aggregation Module (BiAM): On the basis of the similarity clusters, we construct the cluster neighborhood S s (o), which is later used to aggregate and update the features of the target node. The cluster neighborhood S s (o) is specifically defined as follows:\n\nTo ensure the permutation invariance of graph-structure data, we apply the bilevel aggregation scheme for the cluster neighborhood S s (o) to renew the characteristics of nodes. At the first level, the nodes in the same cluster are aggregated into a virtual node by means of an aggregation function. At the second level, we aggregate and update the virtual nodes aggregated in the first level together with the target node into the final node feature representation. The cluster is obtained by performing similarity mapping between the nodes in the graph and the target node. The similarity between nodes does not change with the order of the nodes in the graph, so the order of the nodes in the graph does not affect the clustering result. When the number of nodes in the cluster is constant, the mean aggregator satisfies permutation invariance. In addition, the result of first-level aggregation is the input of the secondlevel aggregation process and remains unchanged, thus making the entire bilevel aggregator satisfy permutation invariance. We utilize the mean aggregation function in the first-level aggregation step, and e o (r) is the final feature representation obtained after the first-level aggregation process, which is defined as follows:\n\nwhere |Clusters (r) | denotes the number of nodes that belong to the r-th cluster. u is a node in the cluster neighborhood, and g u is the value of node u.\n\nWe define the linear transformation function σ (r) (x) as follows:\n\nwhere W (r) is the weight matrix and x is the feature representation of a node in the cluster neighborhood. b (r) represents the bias vector. We specifically implement the δ (τ (o, u) , r) function as follows:\n\nwhere δ (•, •) is the Kronecker delta function, which takes only nodes in the same cluster into account. This function is employed to separate different clusters for aggregation operations. The detailed process of the first-level aggregation is shown in Figure  4 .\n\nWe perform further aggregation operations based on all virtual nodes e o (r) and the target node. h i is the final feature representation obtained after the second-level aggregator updates the target node. We define it as follows:\n\nHere, we implement σ (•) as a ReLU function. i represents the i-th utterance in the conversation. matrix of the node feature transformation. The detailed process of graph aggregation via bilevel aggregation according to the cluster neighborhood is shown in Figure  4 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Emotion Classifier",
      "text": "We take the updated feature representation of each node through the bilevel aggregated as the input for the final predicted label. Then, the following methods are used to predict the emotion labels of the nodes:\n\nwhere h i represents the final feature of the target node, which contains multimodal information. p i represents the probability vector of the emotion class for utterance i. W l , b l , W smax and b smax are all trainable parameters. The category with the highest calculated probability is used as the prediction label, and the emotion label computation is defined as follows:",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "E. Model Training",
      "text": "We choose the categorical cross-entropy loss function during training. The calculation process is as follows:\n\nwhere K is the number of conversations, and m indicates the category of the emotion label. y (m) i,j is the golden label for utterance i, p (m) i,j is the predicted output for utterance i, and N i is the number of utterances in the conversation.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Iv. Experimental Databases And Setup",
      "text": "In this section, first, we introduce the datasets used for the experiments. Second, we describe details of the implementation of our method. Finally, we present the methodology for model evaluation and some state-of-the-art baselines.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Datasets",
      "text": "Both benchmark IEMOCAP  [20]  and MELD  [21]  datasets are used to measure the performance of RBA-GCN, and both contain three modalities: acoustic, visual and textual. Table  I  presents the detailed information of the two datasets, including the detailed distribution of each emotion and the number of utterances used for training, validation and testing. IEMOCAP: The University of Southern California has produced the IEMOCAP  [20]  dataset. It contains up to 12 hours of multimodal audiovisual data, and there are 5 sessions in total, each consisting of a conversation between a man and a woman. The conversation is divided into two parts, namely, the fixed script and the free form, in a given thematic scene. The dataset has 151 conversations with a total of 7433 utterances and is labeled with 6 types of emotions: \"neutral\", \"happy\", \"sad\", \"angry\", \"frustrated\" and \"excited\", with non-neutral emotions accounting for 77%. MELD: The MELD  [21]  dataset is an extension of the EmotionLines Friends section of the plain text modality, and it is presented as a multiperson conversation, unlike the binary conversations in IEMOCAP. It contains 1433 conversations with 13708 utterances and is labeled with seven types of emotions, i.e., \"anger\", \"disgust\", \"fear\", \"joy\", \"neutral\", \"sadness\", and \"surprise\", which are categorized into three categories, i.e., positive, negative and neutral, with nonneutral emotions accounting for 53%.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Data Preprocessing",
      "text": "During data preprocessing, TextCNN  [33]  is utilized to extract raw textual features, the openSMILE toolkit with IS10  [34]  configuration is utilized to extract raw acoustic features, and DenseNet  [35]  is utilized to extract raw visual facial expression features.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "C. Implementation Details",
      "text": "In this subsection, we focus on the specific details of the RBA-GCN. We utilize the Adam optimizer to train the RBA-GCN. The model is configured with a dropout rate of 0.5, a ρ parameter of 0.3, a learning rate of 0.0009, and a γ parameter of 8. The model is trained for up to 1500 epochs.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "D. Evaluation Metrics",
      "text": "As shown in Table  I , there are inherent data imbalances in the IEMOCAP and MELD datasets. Considering that the weighted average F1 score has good ability to handle unbalanced classes, in the following experiments, we employ it as our metric for the evaluation of our proposed RBA-GCN. The weighted average F1 is shown below:\n\nwhere the number of emotion categories in the dataset is denoted by M and the sample size of a category is denoted by N j . F 1 j is the f 1 score of samples in a category.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "E. State-Of-The-Art Baselines",
      "text": "In this subsection, we present several of the most advanced baseline methods. To highlight the superiority of RBA-GCN, we compare our proposed method with these baselines. BC-LSTM  [5] : A context-aware utterance representation for emotion classification is utilized, and the model aims to capture contextual information through a Bi-LSTM layer. CMN  [6] : The context of a particular speaker is modeled by the different memories of each speaker, and the historical utterance of each speaker is modeled separately by GRU as a memory unit. ICON  [17] : The attention mechanism is used to obtain the result fusion of memory units with the current utterance representation for utterance emotion classification. GAT  [12] : The model applies an attention mechanism to characterize the importance of neighboring nodes to nodes and updates node features for emotion recognition using different edge weights. DialogueRNN  [7] : To capture speaker information, the context of previous utterances and affective information, three types of states, namely, speaker state, global state, and emotional state, are employed. DialogueGCN  [8] : This is the first time that GCNs are applied to an emotion recognition scenario in a conversation. It can effectively model the contextual information and speaker information in a conversation. MTAG  [36] : This method converts unaligned multimodal sequence data into a graph with heterogeneous nodes and edges to capture the rich interactions across modalities and through time. ConGCN  [37] : This method constructs the entire dataset as a graph and uses subgraphs in the larger graph to represent each conversation. Speaker nodes are also connected to corresponding utterance nodes, which are used to model speaker-sensitive dependencies. MMGCN  [9] : This approach utilizes multimodal information based on DialogueGCN. The model uses spectral domain GCNs to encode the multimodal graph, which makes it possible for multilayer GCNs to capture more distant contextual information. However, it does not consider the relationships between nodes in the graph.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "V. Results And Discussions",
      "text": "In this section, first, we compare our proposed method with all the baseline methods mentioned in Subsection IV-E to verify the superiority of our approach. Second, we perform a case study to further validate our approach. Then, we evaluate the effectiveness of the three modules in RBA-GCN. Finally, we explore the importance of effectively capturing the interactions between different models.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Comparison With State-Of-The-Art Baselines",
      "text": "We compare the RBA-GCN with the baseline methods on IEMOCAP and MELD in Subsection IV-E. Table  II  and Table  III  show the comparison results. The experimental results indicate that our method significantly outperforms all the baseline methods. On the IEMOCAP dataset, the RBA-GCN achieves a WAF1 score of 71.43%, which is 5 points higher than that of the most advanced existing method. In addition, it achieves a WAF1 score of 62.67% on the MELD dataset, which is 4 points higher than that of the best baseline method. Furthermore, we compare the proposed approach with the GAT and MTAG models. Unlike the graph attention mechanism, which automatically removes edges with low weights or directly assigns low weights during aggregation, RBA-GCN employs similarity measures to filter out redundant information and map nodes to different clusters. Finally, intracluster aggregation and intercluster aggregation are performed. We conduct additional experiments to compare the performance of different graph generation methods on the GAT. \"GAT\" involves using our graph generation method, and \"GAT-fully\" represents the fully graph connected method. The experimental results are shown in Tables  II  and III . The overall performance of GAT-fully is better than that of the GAT. This is because GAT-fully is better than GAT at capturing contextual information. The comparison results show that our proposed RBA-GCN performs better than both the GAT and GAT-fully, which indicates the superiority of the RBA-GCN.\n\nThe experimental results for the IEMOCAP dataset are shown in Table  II . These results demonstrate that our method obtains the best scores on almost all the labels. Undoubtedly, our method achieves the most advanced weighted average F1 score. Because the IEMOCAP dataset has more than 70 conversations and the average conversation length exceeds 50 utterances, DialogueGCN and MMGCN use sliding windows in the composition to reduce the complexity of the graph. Although this approach reduces the complexity of the model, it loses the context dependency of the target node over long ranges. Remarkably, our RBA-GCN method achieves the best prediction result, with a WAF1 score of 71.66% for the \"happy\" label, which is almost 30% higher than that of the best performing DialogueGCN model on this label. Data with a \"happy\" label in the IEMOCAP dataset account for only 7% of the whole dataset. This means that the probability of \"happy\" appearing in a conversation is minimal and a correct prediction for such an utterance is difficult. As a result, the prediction accuracy of such labels is very low. The DialogueGCN model uses GCNs to aggregate node information, which improves the prediction accuracy of such nodes. Due to the complexity of graph generation, the prediction of MMGCN for such node labels is not satisfactory. In our method, for such nodes, we first filter the noisy information using clusters to reduce the redundancy of the target node information. Then, we enhance the favorable features in each cluster to improve the classification effect. Finally, bilevel aggregation effectively captures the long-range contextual information and makes excellent use of the interaction between multiple modalities, thereby improving the prediction accuracy of such nodes more effectively.\n\nThe experimental results of the MELD dataset are displayed in Table  III . These results indicate that our method achieves the optimal scores on almost every label. The MELD dataset consists of multiperson conversations, which are briefer and have few specific emotional expressions compared to the IEMOCAP dataset. In addition, the average conversation length of the MELD dataset is more than 10 utterances. Since there are more than 4 speakers in many conversations, only a few utterances are available for most speakers in a conversation. These factors make it more difficult to improve the classification accuracy. However, the prediction results of our model on the MELD dataset are also improved by at least 4 points compared to that of ConGCN. The substantial improvement is due to our cluster and bilevel aggregation approach.\n\nThe confusion matrix of our RBA-GCN method with respect to the IEMOCAP and MELD datasets is shown in Figure  5 , which illustrates the effectiveness of our method more distinctly. For the IEMOCAP dataset, the weighted average F1 scores of all classes are relatively balanced, with the \"sad\" category having the highest weighted average F1 score of 86.95%. For the MELD dataset, we find from Table  I  that the training and test sets for the three categories of \"disgust\", \"fear\" and \"sadness\" are relatively small compared to those of other categories. Although the MELD dataset has obvious class imbalance, leading to more difficulty in model training, RBA-GCN is significantly improved. Thus, RBA-GCN can filter out noisy information, reduce the redundancy of target node information, and better retain node discriminant information. Additionally, our model can effectively capture long-range contextual information and interactions between modalities.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Rba-Gcn Under Various Modality Settings",
      "text": "We experimentally compare the performance between single-modality and multimodality settings to verify the effectiveness of RBA-GCN for multimodal interactions. The performance of our proposed method in various modality settings is shown in Table  IV .\n\nAccording to the results in Table  IV , there are some differences in the performance of each modality under the single-modality setting, with the textual modality performing best. We argue that textual features can express emotions more intuitively than acoustic and visual features in a conversational emotion recognition task. With few exceptions, the words for emotional expression are in the utterance.\n\nIn a multimodal setting, the performance of multiplemodality fusion is better than that of individual modalities, but the best performance is obtained with the fusion of three modalities. We believe that multiple modality features can complement each other compared to a single modality. Similar to communication with people in reality, we can combine facial expressions, voice and conversation content to determine mood fluctuations of the speaker. The experimental results indicate that the RBA-GCN achieves a significant improvement on most modal combinations compared to the multimodal fusion method from MMGCN. This indicates that  our multimodal fusion method can fuse sufficient information effectively. Meanwhile, the node discriminant information can be retained after multimodal fusion, which makes the emotion recognition more accurate.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "C. Comparison With Other Fusion Methods",
      "text": "A performance comparison of RBA-GCN and the most advanced baseline methods is shown in Table  V . We compare our proposed method with other multimodal fusion methods, including other representative fusion methods such as MFN, MMGCN and CTNet, to illustrate the superiority of the RBA-GCN.\n\nOur method outperforms other multimodal fusion methods on both datasets, as shown in Table  V . On the IEMOCAP dataset, it outperforms the most advanced graph convolution fusion method (MMGCN) by more than 5% and is nearly 4% higher than the current most advanced fusion method (CTNet). On the MELD dataset, our method outperforms the most advanced graph convolution fusion method (MMGCN) by more than 4% and is nearly 2% higher than the most advanced fusion method (CTNet). This reflects the superiority of our proposed multimodal fusion method anchored on relational bilevel aggregation, which can effectively capture the interactions between modalities.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "D. Ablation Study",
      "text": "Ablation studies are conducted to demonstrate the effectiveness of the various components of our proposed method (RBA-GCN).\n\n1) The effectiveness of the graph generation module (GGM): To verify the effectiveness of our GGM, we compare Multimodal fusion method IEMOCAP MELD MFN  [38]  0.6277 0.5470 MulT  [39]  0.6237 0.5649 MMGCN  [9]  0.6622 0.5865 CTNet  [28]  0  Our graph generation method performs better on the IEMO-CAP dataset than other methods with fully connected graphs by nearly 1%. On the MELD dataset, our graph generation method outperforms the fully connected graph generation method employed by other models by 5%. To explain the superior performance of our graph generation method on the MELD dataset, we argue that only a small number of utterances per conversation by most participants in this dataset lead to increased information redundancy. However, our graph generation method can effectively reduce the information redundancy of the target node and retain the discriminant information of the node. This leads to a significant improvement in the experimental results.\n\n2) Effectiveness of the similarity-based cluster building module (SCBM) : To verify the effectiveness of our clusters and demonstrate that the clusters can effectively filter information irrelevant to the target node, an ablation study is performed. According to the data in Table VII, the clusters significantly influence the final classification result of the model. Consequently, irrelevant information can be effectively filtered so that the discriminant information of the target node is better retained.\n\nWe further compare the performance with and without clusters under different combinations of modalities. As shown in Table  VII , the performance with clusters is better than that without clusters for different combinations of modalities. This demonstrates the effectiveness of clusters for multimodal interactions. The number of clusters γ is a key hyperparameter for bilevel aggregation. Intuitively, the final classification performance of RBA-GCN is influenced by the value of γ. Our aggregation uses clusters to perform the first-level aggregation operation because the value of γ affects the cluster number. Therefore, to study the effect of clusters on model performance, we choose γ in {2, 4, 6, 8, 10}.\n\nOur experiments show that with increasing cluster number within a certain range, a consistent improvement is observed. As shown in Figure  6 , RBA-GCN has the best classification performance when γ = 8. The weighted average F1 scores are 71.43% and 62.67% on the IEMOCAP and MELD datasets, respectively. The increase in the cluster number allows for more detailed differentiation of other nodes so that similar nodes can be better aggregated. However, the model classification performance decreases when γ > 8. We believe that the number of clusters impacts the second-level aggregation. The greater the number of clusters is, the more virtual nodes are aggregated, which destroys the target node information.\n\nTo further investigate the effectiveness of the SCBM, a finergrained ablation study is performed. The experimental results in Table  VIII  show that our method achieves the best results. This proves the effectiveness of our application of connected neighborhood C g (o) to retain multimodal information and disconnected neighborhood D g (o) with filter s(u, o) to retain long-range contextual information.\n\n3) Effect of GCN layers on RBA-GCN : We conduct a comparison study on the number of GCN layers on RBA-GCN. The experimental results are shown in Figure  7 , where the best results are achieved when we apply only bilevel aggregation. The model performance begins to degrade when  we scale up the number of GCN layers. This further validates that after multilayer GCNs aggregation, the nodes in the graph become very similar and may lose the discriminant information of the node.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "E. Complexity Analysis",
      "text": "The temporal complexity of GCNs is very important because certain conversations in the real world tend to be relatively long. Therefore, the graphs composed of these conversations are very large and have a very complex structure. In this subsection, we compare the temporal complexity of our method with that of the methods in Section IV-E. We compare the actual runtime (1500 epochs) of the Dia-logueGCN, MMGCN, DialogueRNN and RBA-GCN models on all datasets using the hyperparameters described in Section IV-C. According to the data in Figure  8 , DialogueRNN takes the least time, while our method comes in second place. We believe that our model is computationally complex and tedious compared to traditional neural network models, which is a significant reason why it consumes more time. Next is DialogueGCN, and MMGCN is the slowest. Although these methods employ some computational optimization techniques, such as sliding windows, they have not significantly reduced their computational cost. Due to the tremendous number of conversations in real life every day, the graph is large. Therefore, in future work, we will consider how to reduce the training time and enhance the robustness of the model.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "F. Case Study",
      "text": "For a more intuitive comparative analysis of our method and more advanced methods, we perform a case study. Table  IX  shows the results of our analysis for one case on the IEMOCAP dataset, where the results in red indicate incorrect predictions and the results in green indicate correct predictions. According to the prediction results, our method clearly outperforms the other methods. We think that most of the utterances in this conversation are \"neutral\", while some other emotion-labeled utterances are mixed into the conversation. Since traditional graph convolution methods aggregate the information of neighboring nodes, this leads to target node discriminant information loss and prediction errors. In this case, the prediction results of other methods for these utterances are wrong, while our model handles these cases well. In particular, the fifth utterance is predicted as \"neutral\" by other models, while our model produces the correct label \"happy\". This is because our method can effectively capture long-range contextual information and interactions between modalities by considering the relevance between nodes and filtering out noisy information.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Vi. Conclusion And Future Work",
      "text": "We propose a model named RBA-GCN for ERC. RBA-GCN considers the correlation between nodes on the basis of graphs and has the ability to capture long-range contextual information as well as interactions between modalities in a single-layer architecture. Our GGM is a novel graph generation method used to reduce the redundancy of target node information. Based on the GGM, we present SCBM to calculate the node similarity in the target node and its structural neighborhood, where noisy information with low similarity is filtered out to preserve the discriminant information of the nodes. Finally, our propose BiAM has the capability to capture In future work, first, we will conduct further research on clusters, such as calculating the relationship between nodes by an attention mechanism and mapping them into a cluster. Second, we will explore developing acceleration techniques to address the scalability issue of RBA-GCN.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (a) An example of a conversation with different aggregation methods.",
      "page": 1
    },
    {
      "caption": "Figure 1: 1, we first consider long-range contextual",
      "page": 2
    },
    {
      "caption": "Figure 2: The overall framework. First, we encode contextual information for each modality feature of the utterance, using Bi-LSTM to obtain the contextual",
      "page": 3
    },
    {
      "caption": "Figure 2: , is composed of a contextual encoder, an RBA-",
      "page": 3
    },
    {
      "caption": "Figure 3: The construction process of similarity clusters.",
      "page": 4
    },
    {
      "caption": "Figure 3: 3) Bilevel Aggregation Module (BiAM): On the basis of",
      "page": 5
    },
    {
      "caption": "Figure 4: We perform further aggregation operations based on all",
      "page": 5
    },
    {
      "caption": "Figure 4: The detailed process of the bilevel aggregation polymerization.",
      "page": 6
    },
    {
      "caption": "Figure 4: D. Emotion Classifier",
      "page": 6
    },
    {
      "caption": "Figure 5: , which illustrates the effectiveness of our method more",
      "page": 8
    },
    {
      "caption": "Figure 5: Confusion matrix of proposed RBA-GCN on: (a) IEMOCAP dataset, and (b) MELD dataset. Note: x-axis is the correct label, y-axis is the predicted",
      "page": 9
    },
    {
      "caption": "Figure 6: (a) Representation of different quantitative clusters on the IEMOCAP dataset; (b) Representation of different quantitative clusters on the MELD",
      "page": 9
    },
    {
      "caption": "Figure 6: , RBA-GCN has the best classification",
      "page": 10
    },
    {
      "caption": "Figure 7: Performance with the different number of GCN layers on",
      "page": 11
    },
    {
      "caption": "Figure 8: , DialogueRNN takes",
      "page": 11
    },
    {
      "caption": "Figure 8: Running time comparison of four models.",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "emotion": "",
          "IEMOCAP": "train+val",
          "MELD": "train+val"
        },
        {
          "emotion": "Anger\nHappiness/Joy\nSadness\nNeutral\nExcitement\nFrustration\nDisgust\nSurprise\nFear",
          "IEMOCAP": "869\n460\n877\n1387\n828\n1478\n–\n–\n–",
          "MELD": "1262\n1906\n794\n5180\n–\n–\n293\n1355\n308"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Models": "",
          "IEMOCAP": "happy"
        },
        {
          "Models": "BC-LSTM [5]",
          "IEMOCAP": "0.3443"
        },
        {
          "Models": "CMN [6]",
          "IEMOCAP": "0.3038"
        },
        {
          "Models": "ICON [17]",
          "IEMOCAP": "0.2991"
        },
        {
          "Models": "DialogueRNN [7]",
          "IEMOCAP": "0.3318"
        },
        {
          "Models": "DialogueGCN [8]",
          "IEMOCAP": "0.4275"
        },
        {
          "Models": "GAT [12]",
          "IEMOCAP": "0.4761"
        },
        {
          "Models": "GAT-fully [12]",
          "IEMOCAP": "0.4720"
        },
        {
          "Models": "MTAG [36]",
          "IEMOCAP": "0.3603"
        },
        {
          "Models": "MMGCN [9]",
          "IEMOCAP": "0.4234"
        },
        {
          "Models": "Ours",
          "IEMOCAP": "0.7166"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Models": "",
          "MELD": "anger"
        },
        {
          "Models": "BC-LSTM [5]",
          "MELD": "0.445"
        },
        {
          "Models": "CMN [6]",
          "MELD": "0.447"
        },
        {
          "Models": "ICON [17]",
          "MELD": "0.448"
        },
        {
          "Models": "GAT [12]",
          "MELD": "0.4262"
        },
        {
          "Models": "GAT-fully [12]",
          "MELD": "0.4323"
        },
        {
          "Models": "MTAG [36]",
          "MELD": "0.4742"
        },
        {
          "Models": "DialogueRNN [7]",
          "MELD": "0.415"
        },
        {
          "Models": "ConGCN [37]",
          "MELD": "0.468"
        },
        {
          "Models": "Ours",
          "MELD": "0.5000"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Modality": "",
          "IEMOCAP": "happy"
        },
        {
          "Modality": "T",
          "IEMOCAP": "0.7090"
        },
        {
          "Modality": "V",
          "IEMOCAP": "0.6326"
        },
        {
          "Modality": "A",
          "IEMOCAP": "0.6034"
        },
        {
          "Modality": "T+V",
          "IEMOCAP": "0.7049"
        },
        {
          "Modality": "T+A",
          "IEMOCAP": "0.7288"
        },
        {
          "Modality": "V+A",
          "IEMOCAP": "0.6724"
        },
        {
          "Modality": "T+V+A",
          "IEMOCAP": "0.7166"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Multimodal\nfusion method": "MFN [38]\nMulT [39]\nMMGCN [9]\nCTNet\n[28]\nOurs",
          "IEMOCAP": "0.6277\n0.6237\n0.6622\n0.6750\n0.7143",
          "MELD": "0.5470\n0.5649\n0.5865\n0.6050\n0.6267"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "RBA-GCN": "w/o Clusters",
          "Modalities": "A+T",
          "IEMOCAP": "0.6265",
          "MELD": "0.5200"
        },
        {
          "RBA-GCN": "",
          "Modalities": "A+V",
          "IEMOCAP": "0.6076",
          "MELD": "0.4933"
        },
        {
          "RBA-GCN": "",
          "Modalities": "T+V",
          "IEMOCAP": "0.6368",
          "MELD": "0.5333"
        },
        {
          "RBA-GCN": "",
          "Modalities": "A+T+V",
          "IEMOCAP": "0.6489",
          "MELD": "0.5467"
        },
        {
          "RBA-GCN": "w Clusters",
          "Modalities": "A+T",
          "IEMOCAP": "0.6850",
          "MELD": "0.5357"
        },
        {
          "RBA-GCN": "",
          "Modalities": "A+V",
          "IEMOCAP": "0.6162",
          "MELD": "0.5067"
        },
        {
          "RBA-GCN": "",
          "Modalities": "T+V",
          "IEMOCAP": "0.6988",
          "MELD": "0.5779"
        },
        {
          "RBA-GCN": "",
          "Modalities": "A+T+V",
          "IEMOCAP": "0.7143",
          "MELD": "0.6267"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "RBA-GCN": "Cg(o)",
          "WAF1": "0.6024"
        },
        {
          "RBA-GCN": "Dg(o)",
          "WAF1": "0.6093"
        },
        {
          "RBA-GCN": "Cg(o) with s(u, o)",
          "WAF1": "0.5921"
        },
        {
          "RBA-GCN": "Dg(o) with s(u, o)",
          "WAF1": "0.6231"
        },
        {
          "RBA-GCN": "Cg(o) + Dg(o)",
          "WAF1": "0.6813"
        },
        {
          "RBA-GCN": "Cg(o) with s(u, o) + Dg(o)",
          "WAF1": "0.6489"
        },
        {
          "RBA-GCN": "Cg(o) with s(u, o) + Dg(o) with s(u, o)",
          "WAF1": "0.6895"
        },
        {
          "RBA-GCN": "Cg(o) + Dg(o) with s(u, o) (ours)",
          "WAF1": "0.7143"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Turn": "1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14",
          "Utterances": "A: Did we bring something less? You forgot\nto bring\nthe baby’s anvil?\nB: Women like babies it’s common knowledge, okay?\nB: Women like men who like babies.\nB: Quick, point him towards that group of beautiful women.\nB: No, no, wait,\nto get\nthem, we got one, on the left.\nB: Well, give me the baby.\nA: No,\nI got him.\nA: Oh, you really wanted him?\nB: Hi.\nA: Well, don’t\nthink I’m not being modest, but, me?\nB: Do you want\nto smell him?\nB: Oh, yeah. He has that baby smell.\nB: What have I\ntold you? What have I\ntold you?\nA: Well, we are great guys.",
          "Label": "neutral\nneutral\nneutral\nneutral\nhappy\nneutral\nneutral\nexcited\nneutral\nexcited\nneutral\nhappy\nhappy\nneutral",
          "DialogueRNN [7]": "neutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nhappy\nneutral\nneutral",
          "DialogueGCN [8]": "neutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nexcited\nneutral\nneutral\nneutral\nhappy\nneutral\nneutral",
          "MMGCN [9]": "neutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nexcited\nneutral\nexcited\nneutral\nhappy\nneutral\nneutral",
          "Ours": "neutral\nneutral\nneutral\nneutral\nhappy\nneutral\nneutral\nexcited\nneutral\nexcited\nneutral\nhappy\nhappy\nneutral"
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Real-time emotion recognition via attention gated hierarchical memory network",
      "authors": [
        "W Jiao",
        "M Lyu",
        "I King"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "2",
      "title": "Towards discriminative representation learning for speech emotion recognition",
      "authors": [
        "R Li",
        "Z Wu",
        "J Jia",
        "Y Bu",
        "S Zhao",
        "H Meng"
      ],
      "year": "2019",
      "venue": "IJCAI"
    },
    {
      "citation_id": "3",
      "title": "Affective computing and sentiment analysis",
      "authors": [
        "E Cambria"
      ],
      "year": "2016",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "4",
      "title": "Sentiment classification in customer service dialogue with topic-aware multi-task learning",
      "authors": [
        "J Wang",
        "J Wang",
        "C Sun",
        "S Li",
        "X Liu",
        "L Si",
        "M Zhang",
        "G Zhou"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "5",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "N Majumder",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "6",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "D Hazarika",
        "S Poria",
        "A Zadeh",
        "E Cambria",
        "L.-P Morency",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the conference"
    },
    {
      "citation_id": "7",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "8",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya",
        "A Gelbukh"
      ],
      "year": "2020",
      "venue": "Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference"
    },
    {
      "citation_id": "9",
      "title": "Mmgcn: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "J Hu",
        "Y Liu",
        "J Zhao",
        "Q Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "10",
      "title": "Semi-supervised classification with graph convolutional networks",
      "authors": [
        "T Kipf",
        "M Welling"
      ],
      "year": "2017",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "11",
      "title": "The graph neural network model",
      "authors": [
        "F Scarselli",
        "M Gori",
        "A Tsoi",
        "M Hagenbuchner",
        "G Monfardini"
      ],
      "year": "2008",
      "venue": "IEEE transactions on neural networks"
    },
    {
      "citation_id": "12",
      "title": "Graph attention networks",
      "authors": [
        "P Velickovic",
        "G Cucurull",
        "A Casanova",
        "A Romero",
        "P Lio",
        "Y Bengio"
      ],
      "year": "2017",
      "venue": "stat"
    },
    {
      "citation_id": "13",
      "title": "Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations",
      "authors": [
        "T Ishiwatari",
        "Y Yasuda",
        "T Miyazaki",
        "J Goto"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "14",
      "title": "Enhancing segment-based speech emotion recognition by iterative self-learning",
      "authors": [
        "S Mao",
        "P Ching",
        "T Lee"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "15",
      "title": "Dailydialog: A manually labelled multi-turn dialogue dataset",
      "authors": [
        "Y Li",
        "H Su",
        "X Shen",
        "W Li",
        "Z Cao",
        "S Niu"
      ],
      "year": "2017",
      "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "16",
      "title": "Emotion detection on tv show transcripts with sequence-based convolutional neural networks",
      "authors": [
        "S Zahiri",
        "J Choi"
      ],
      "year": "2018",
      "venue": "Workshops at the thirty-second aaai conference on artificial intelligence"
    },
    {
      "citation_id": "17",
      "title": "Icon: Interactive conversational memory network for multimodal emo-tion detection",
      "authors": [
        "D Hazarika",
        "S Poria",
        "R Mihalcea",
        "E Cambria",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "18",
      "title": "Context-and sentiment-aware networks for emotion recognition in conversation",
      "authors": [
        "G Tu",
        "J Wen",
        "C Liu",
        "D Jiang",
        "E Cambria"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Artificial Intelligence"
    },
    {
      "citation_id": "19",
      "title": "Geom-gcn: Geometric graph convolutional networks",
      "authors": [
        "H Pei",
        "B Wei",
        "-C Chang",
        "Y Lei",
        "B Yang"
      ],
      "year": "2020",
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "20",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "21",
      "title": "The model for end-stage liver disease (meld)",
      "authors": [
        "P Kamath",
        "W Kim"
      ],
      "year": "2007",
      "venue": "The model for end-stage liver disease (meld)"
    },
    {
      "citation_id": "22",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "N Majumder",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "23",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "D Hazarika",
        "S Poria",
        "A Zadeh",
        "E Cambria",
        "L.-P Morency",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the conference"
    },
    {
      "citation_id": "24",
      "title": "Multimodal sentiment analysis with word-level fusion and reinforcement learning",
      "authors": [
        "M Chen",
        "S Wang",
        "P Liang",
        "T Baltrušaitis",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "25",
      "title": "Memory fusion network for multi-view sequential learning",
      "authors": [
        "A Zadeh",
        "P Liang",
        "N Mazumder",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "26",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "N Majumder",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "27",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "D Hazarika",
        "S Poria",
        "A Zadeh",
        "E Cambria",
        "L.-P Morency",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the conference"
    },
    {
      "citation_id": "28",
      "title": "Ctnet: Conversational transformer network for emotion recognition",
      "authors": [
        "Z Lian",
        "B Liu",
        "J Tao"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "29",
      "title": "Multimodal emotion recognition with temporal and semantic consistency",
      "authors": [
        "B Chen",
        "Q Cao",
        "M Hou",
        "Z Zhang",
        "G Lu",
        "D Zhang"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "30",
      "title": "Survey on audiovisual emotion recognition: databases, features, and data fusion strategies",
      "authors": [
        "C.-H Wu",
        "J.-C Lin",
        "W.-L Wei"
      ],
      "year": "2014",
      "venue": "APSIPA transactions on signal and information processing"
    },
    {
      "citation_id": "31",
      "title": "Real-time video emotion recognition based on reinforcement learning and domain knowledge",
      "authors": [
        "K Zhang",
        "Y Li",
        "J Wang",
        "E Cambria",
        "X Li"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "32",
      "title": "Multimodal sentiment analysis with two-phase multi-task learning",
      "authors": [
        "B Yang",
        "L Wu",
        "J Zhu",
        "B Shao",
        "X Lin",
        "T.-Y Liu"
      ],
      "year": "2022",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "33",
      "title": "Large-scale video classification with convolutional neural networks",
      "authors": [
        "A Karpathy",
        "G Toderici",
        "S Shetty",
        "T Leung",
        "R Sukthankar",
        "L Fei-Fei"
      ],
      "year": "2014",
      "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "34",
      "title": "Recognising realistic emotions and affect in speech: State of the art and lessons learnt from the first challenge",
      "authors": [
        "B Schuller",
        "A Batliner",
        "S Steidl",
        "D Seppi"
      ],
      "year": "2011",
      "venue": "Speech communication"
    },
    {
      "citation_id": "35",
      "title": "Densely connected convolutional networks",
      "authors": [
        "G Huang",
        "Z Liu",
        "L Van Der Maaten",
        "K Weinberger"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "36",
      "title": "Mtag: Modal-temporal attention graph for unaligned human multimodal language sequences",
      "authors": [
        "J Yang",
        "Y Wang",
        "R Yi",
        "Y Zhu",
        "A Rehman",
        "A Zadeh",
        "S Poria",
        "L.-P Morency"
      ],
      "year": "2021",
      "venue": "NAACL"
    },
    {
      "citation_id": "37",
      "title": "Modeling both context-and speaker-sensitive dependence for emotion detection in multi-speaker conversations",
      "authors": [
        "D Zhang",
        "L Wu",
        "C Sun",
        "S Li",
        "Q Zhu",
        "G Zhou"
      ],
      "year": "2019",
      "venue": "IJCAI"
    },
    {
      "citation_id": "38",
      "title": "Memory fusion network for multi-view sequential learning",
      "authors": [
        "A Zadeh",
        "P Liang",
        "N Mazumder",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "39",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Y.-H Tsai",
        "S Bai",
        "P Liang",
        "J Kolter",
        "L.-P Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the conference. Association for Computational Linguistics. Meeting"
    }
  ]
}