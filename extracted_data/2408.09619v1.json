{
  "paper_id": "2408.09619v1",
  "title": "Statistical Inference For Regression With Imputed Binary Covariates With Application To Emotion Recognition",
  "published": "2024-08-19T00:17:46Z",
  "authors": [
    "Ziqian Lin",
    "Danyang Huang",
    "Ziyu Xiong",
    "Hansheng Wang"
  ],
  "keywords": [
    "Auxiliary Feature",
    "Emotion Recognition",
    "Live Streaming",
    "Pilot Sample",
    "Regression Imputation"
  ],
  "sections": [
    {
      "section_name": "Statistical Inference For Imputed Estimator.",
      "text": "2.1. The Model Setup. Let (Z i , X i , Y i ) be the i-th (1 ≤ i ≤ N ) observation independently collected from the joint distribution of (Z, X, Y ). Here Y i ∈ R is the response, Z i = (Z i1 , . . . , Z ip ) ⊤ ∈ {0, 1} p is the vector of interested binary features, and X i = (X i1 , . . . , X iq ) ⊤ ∈ R q is the vector of the fully observed control variables. To model the relationship between Y i and (Z i , X i ), we assume the following correctly specified linear regression model (2.1)\n\nis the interested coefficient vector, γ = (γ 1 , . . . , γ q ) ⊤ ∈ R q is the coefficient vector of the control variables, and ε i ∈ R is the independent random noise satisfying E(ε i ) = 0 and var(ε i ) = σ 2 . For convenience, we refer to this model as a substantive model. To estimate the unknown coefficients β and γ, a standard OLS approach can be applied. Specifically, we minimize the least squares loss function\n\nThen the OLS estimator is given as ( β ols , γ ols ) = argmin β,γ L y,lse (β, γ), where β ols = ( β ols,1 , . . . , β ols,p ) ⊤ ∈ R p and γ ols = ( γ ols,1 , . . . , γ ols,q\n\nIf all the variables are fully observed, the OLS estimator will be consistent and asymptotically normal under mild regularity conditions  (Rao, 1973; Shao, 2003) .\n\nUnfortunately, in this interested application, the binary features Z i are largely missing because of the high collection cost. To solve the problem, we assume an auxiliary variable W i = (W i1 , . . . , W ir ) ⊤ ∈ R r could be collected for the ith observation. We typically expect W i to be highly related to Z i and to be much easier to collect. To model the relationship between Z i and W i , a logistic regression model is correctly specified as follows\n\nwhere α j = (α j1 , . . . , α jr ) ⊤ ∈ R r (1 ≤ j ≤ p) is the coefficient vector and p(x) = exp(x)/{1+ exp(x)} is the sigmoid function. For convenience, we refer to this model as an imputation model. We also assume that Z ij1 and Z ij2 are conditionally independent on W i for j 1 ̸ = j 2 . If a consistent estimator for α j with the desired statistical accuracy can be obtained, the binary vector Z i could be predicted with reasonable accuracy. More specifically, we first extract a subsample out of the whole dataset. Without loss of generality, we assume the first n observations are selected. We typically expect n to be much smaller than N in the sense n/N → 0 as N → ∞. We assume that for each 1 ≤ i ≤ n the accurate value of Z i could be collected.\n\nObviously, this leads to a considerable data collection cost. However, as n ≪ N , the cost for collecting Z i with 1 ≤ i ≤ n is expected to be practically acceptable. For convenience, we refer to\n\nWith the help of the pilot sample S 0 , the unknown regression coefficient α j can be consistently estimated by maximizing the log-likelihood function\n\nThen the maximum likelihood estimator for α j is given as α j = argmax αj L j,z (α j ). Under appropriate regularity conditions, the maximum likelihood estimator α j is consistent and asymptotically normal  (Shao, 2003; Lehmann and Casella, 2006) . We impute the binary covariate Z i by its predicted probability Z i , where the jth component of Z i is given as\n\nHere we do not impute Z ij by its binary prediction results (e.g.\n\n) due to the challenge of threshold value selection. As pointed out by  Qiao and Liu (2009) , when Z i is highly imbalanced, the threshold value 0.5 is unlikely to be the optimal choice. In the meanwhile, what is the optimal choice about the threshold value is not immediately clear.\n\nAfter imputation, we obtain an imputed whole sample as\n\nNext a loss function can be constructed based on the imputed whole sample as\n\nThen, an imputed whole sample based estimator can be easily obtained as\n\nFor convenience, we refer to the estimator\n\nThe asymptotic properties of the estimator are to be studied subsequently. We investigate the theoretical properties in the regular case first and then discuss the two special cases, which are the highly imbalanced and highly predictable cases.\n\nIt is remarkable that two models are included here. The first one is the true model (2.1), where Z i 's are all binary. The second one is the working model (2.2), where the binary Z i 's are partially imputed by the estimated response probabilities. The imputed response probability is continuous. The regression coefficient β should be interpreted with respect to the true model, where the Z i 's are all binary. The working model (2.2) is developed solely for the purpose of parameter estimation rather than interpretation.",
      "page_start": 1,
      "page_end": 6
    },
    {
      "section_name": "The Asymptotic Theory For Regular Case.",
      "text": "To study the asymptotic properties of the imputed estimator θ imp , we start with the regular case with fixed α j 's, which means the binary covariates are relatively balanced and the prediction accuracy of the covariates is not extremely high. Accordingly, we should never have p(W ⊤ i α j ) = Z ij or even approximately in any sense. In other words, even if the true parameter α j is given, we can never predict Z i consistently. Therefore, the consistency of θ imp becomes skeptical  (White and Carlin, 2010) . Let p+q) . To investigate the asymptotic behavior of θ imp , the following assumptions are required.\n\n(C1) (Sample divergence rate) As N → ∞, we assume that n → ∞ and n/N → 0. (C2) (Nonsingular matrices) Assume that the matrices\n\n] are finite and positive definite. For condition (C1), we should expect the pilot sample size n to be much smaller than the total sample size N in the sense that n/N → 0 as N → ∞. This indicates that the data we have labeled is only a small portion of the entire dataset. Condition (C2) is a classical assumption to ensure the nonsingularity of both the Fisher information matrix in the imputation model and the covariance matrix in the substantive model. Under these conditions, we have the following theorem. THEOREM 1. Assume conditions (C1) and (C2), we have\n\nThe detailed proof of Theorem 1 is provided in Appendix B in the supplementary material. By Theorem 1, we know that the imputed estimator θ imp is consistent. Moreover, the estimation error θ imp -θ can be decomposed into two parts. The first part ζ 1 is a linear transformation of α j -α j . Therefore, its asymptotic behavior is fully determined by the estimation error due to the imputation model. Consequently, we know that ζ 1 is an O p (n -1/2 ) term and is asymptotically normal. The second part ζ 2 involves errors due to both imputation and substantive models. Under the assumption n/N → 0, the second term\n\n) term and therefore is asymptotically negligible when compared with the first term ζ 1 . Combining these two facts, we know that θ imp is √ n-consistent for θ. This is a convergence rate the same as that of the OLS estimator obtained by using the pilot sample only, that is\n\n, where β pilot = ( β pilot,1 , . . . , β pilot,p ) ⊤ ∈ R p and γ pilot = ( γ pilot,1 , . . . , γ pilot,q ) ⊤ ∈ R q . For convenience, we refer to this estimator as the pilot estimator. Under appropriate regularity conditions, we can verify that\n\nNote that the two estimators θ imp and θ pilot have the same convergence rate, but very different asymptotic covariance matrices. Then a natural question is: Which estimator is better? We are particularly interested in the situation when θ imp outperforms θ pilot , as this is the case, the imputation efforts are not wasted. By Theorem 1, we know that the asymptotic efficiency of θ imp is mainly determined by that of A. By contrast, that of θ pilot is significantly affected by the variance of the random noise σ 2 . It is remarkable that this quantity σ 2 is not involved in the leading asymptotic covariance of θ imp . In fact, the variance σ 2 affects the asymptotic variance of θ imp through ζ 2 . See Step 5 of Appendix B for the detailed expression for ζ 2 . However, ζ 2 = o p (ζ 1 ) under condition (C1) is thus ignorable. In this way, σ 2 does not affect the leading covariance of θ imp . This discussion suggests that θ imp should be a better choice than θ pilot if σ 2 is relatively large. Otherwise, it might not be a good choice for a larger computational cost. In the extreme case with σ 2 = 0, we should have θ pilot = θ. Then no imputation is needed.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Special Case I:",
      "text": "The Highly Imbalanced Case. Theorem 1 studies the asymptotic behavior of θ imp for regular cases, where the response probability needs to be relatively balanced. In other words, we should have both min j E{p(W ⊤ i α j )} and 1max j E{p(W ⊤ i α j )} well bounded above zero. However, in real practice, we often encounter the situation where positive cases in Z are rare. For example, for the pharmacokinetic analysis of tacrolimus, the percentage of immunoglobulin treatment accounts for approximately only 7.9% of the concentration records  (Chen et al., 2017) . Theoretically, this suggests that it might be more appropriate to assume that P (Z ij = 1) → 0 as the sample size N → ∞.\n\nWe next study the asymptotic behavior of θ imp in this case in this subsection. To this end, we follow the theoretical framework of  Wang (2020)  and  Wang, Zhang and Wang (2021)  and write\n\n) be the positive proportion for the binary variable Z ij , j = 1, . . . , p. Let r N,max = max j r N j = max j exp(α N j ) and r N,min = min j r N j = min j exp(α N j ) be the maximum and minimum positive proportion for the binary vector Z i , respectively. To investigate the asymptotic behavior of the estimators, the following conditions are needed.\n\n(C3) (Imbalance effect) As N → ∞, we assume that r N j → 0, nr N j → ∞, and r N j /r N,max → c j for some constants\n\n} are finite and positive definite.\n\nCondition (C3) formulates the case where all the binary covariates have rare positive cases. By assuming r N j → 0, we know that\n\nTherefore, the imbalanced phenomenon can be theoretically described. Moreover, by assuming nr N j → ∞, we know that\n\nThis suggests that the number of positive cases should diverge to infinity, even if its sample percentage is low. This allows us to develop rigorous asymptotic theory. Condition (C4) requires the existence of the moment generating function for W i . This condition ensures the existence of a dominated function when applying the dominated convergence theorem in the proof. Condition (C5) is similar to condition (C2), which ensures the positive definiteness of the asymptotic covariance matrices. It is remarkable that condition (C3) is about the intercept parameter, which is the scalar\n\n, which is basically a technical condition for regular feature vectors X i and W i . Note that X i and W i are regular feature vectors with finite dimensions. As a result, the positive definiteness of the covariance matrices in (C5) is naturally satisfied. Under the aforementioned assumptions, we further define p+q) , where I q is a q × q identity matrix. We then obtain the following theorem about the theoretical properties of both θ pilot and θ imp . THEOREM 2. (1) (Asymptotic distribution of the pilot estimator.) Assume conditions (C1) and (C3)-(C5), we then have\n\nwith the matrices\n\nThe detailed proof of Theorem 2 is provided in Appendix C in the supplementary material. By Theorem 2, we know that the data imbalance does play a critical role in the convergence rate of the corresponding estimator. We discuss this for the estimation of β and γ separately.\n\n(1) Estimation of β. First, for the pilot estimator, by Theorem 2, we know that β pilot,j -\n\n). Therefore, we know that in the highly imbalanced case, the convergence rate of β pilot will be slower than the classical parametric rate √ n. Second, for the imputed estimator, we know that β imp,j -\n\n). The convergence rate can be studied according to two cases. Case 1: If the Z ij s are typically balanced with c j > 0, then we have β imp,j -β j = O p (n -1/2 r -1/2 N j ), which is the same convergence rate as that of β pilot,j . Case 2: If the Z ij s are extremely imbalanced with c j = 0, then we have\n\n, which is a convergence rate slower than that of β pilot,j . To gain some intuitive understanding about Case 2, we consider here a highly simplified example as\n\nFor simplicity, we write r N = r N 2 in this simplified example. Next we compare the performance of pilot and imputed estimators. We have Z i2 actually observed for the pilot sample. In this case, the information contained in Z i2 , as measured by its variance, is given by var\n\n, which is a smaller order term as compared with var(Z i2 ). Intuitively, this suggests that a significant amount of variability about Z i2 is lost with Z i2 , if the interested case is a highly imbalanced one. That intuitively explains the slower convergence rate of β 2,imp .\n\n(2) Estimation of γ. We next study the asymptotic behavior of γ imp and γ pilot . From Theorem 2, we know that the convergence rate of γ pilot is O p (n -1/2 ), which is the same as the regular case. By contrast, we have γ imp -γ = O p (n -1/2 r 1/2 N,max ), which is a convergence rate faster than that of γ pilot since r N,max = max j r N j → 0 as N → ∞. Consequently, we should expect that γ imp has a smaller variance than γ pilot .\n\nTo gain some intuitive understanding of the faster convergence rate of γ imp , we consider here a highly simplified model as Y i = Z i β + X i γ + ε i , where Z i is a highly imbalanced binary variable with P (Z i = 1) ≈ r N E{exp( W ⊤ i α * )} → 0 with r N = exp(α N ) and X i is a standard normal random variable independent of Z i . With imputed feature Z i , we can then rewrite the regression model as\n\nOne can verify that the side effect for γ imp due to imputation error is mainly caused by the term {N\n\nFor the binary variable Z i , the more imbalanced it is, the larger estimation error it suffers from αα = O p (1/ √ nr N ). Nevertheless, this side effect can be significantly discounted by a factor of r N due to the fact that var\n\nTo fix the idea, consider, for example, the most extreme situation with P (Z i = 0) = 1. We then have var\n\nIn this case, no imputation error can be transfer from α to γ imp . Therefore, the overall side effect due to imputation also disappears. This explains why overall speaking less imbalanced terms are more influential in affecting the convergence rate of γ imp .\n\n2.4. Special Case II: The Highly Predictable Case. Other than the regular case studied in the previous subsection, we often encounter practical cases with extremely high prediction accuracy, which means either p(W ⊤ i α j ) → 0 or p(W ⊤ i α j ) → 1. In this case, the difference between the estimated probability p(W ⊤ i α j ) and the actual binary feature Z i is asymptotically ignorable. Mathematically, this can be represented by imposing a technical condition that p(W ⊤ i α j ){1 -p(W ⊤ i α j )} → 0 in the sense that N → ∞. A sufficient but necessary condition for this condition is that ∥α j ∥ → ∞ at an appropriate speed as N → ∞.\n\nWe are inspired to consider this interesting case mainly because, for many modern machine learning related classification problems, the out-of-sample prediction accuracy can be extremely high. For illustration, consider for example the CIFAR-10 classification problem with 10 different classes  (Krizhevsky et al., 2009 ). An excellent prediction accuracy of 99.50% has been achieved by  Dosovitskiy et al. (2020) . As another example, consider the Oxford Flower dataset with a total of 102 classes  (Nilsback and Zisserman, 2008 ). An outstanding forecasting accuracy of 99.76% has been achieved by  Hassani et al. (2021) . For these applications, we should always have\n\nextremely close to 0. This makes the asymptotic behavior of the related statistical estimators, such as the maximum likelihood estimator for the logistic regression, somewhat different from the classical cases.\n\nTherefore, we are motivated to study the asymptotic behavior of the proposed imputed estimator under this highly predictable case. Define ω n = max j E[p(W ⊤ i α j ){1 -p(W ⊤ i α j )}] to be the maximum prediction variation of Z i . Then a smaller value of ω n implies a higher prediction accuracy. One can verify that if ∥α j ∥ → ∞ for all 1 ≤ j ≤ p, we have ω n → 0 as N → ∞. Specifically, the following assumptions are required.\n\n(C6) (Predictability of binary covariates) As N → ∞, we assume that ω n → 0, n/(N ω n ) → ∞, and nω n → ∞. (C7) (Local dominance function) Define B(α, r) = { α : ∥ α -α∥ ≤ r} as a compact local ball with α as the center and r > 0 as the radius. Then we assume that there exists a sufficiently small but fixed r such that\n\n) ≥ C min ω n for all 1 ≤ j ≤ p, where λ min (A) represents the minimum eigenvalue of an arbitrary symmetric matrix A.\n\nRecall that the pilot sample size n is assumed to be much smaller than the total sample size N in the sense that n/N → 0 as N → ∞ in condition (C1). To ensure n/(N ω n ) → ∞ as N → ∞ in (C6), we must have ω n → 0 at a sufficiently fast speed. This condition further implies that for an arbitrary case with a feature vector W i , we should have p(W ⊤ i α j ) very close to either 1 or 0. Otherwise, we cannot have p(W ⊤ i α j ){1 -p(W ⊤ i α j )} close to 0. However, the assumption nω n → ∞ as N → ∞ in (C6) constrains the convergence rate of ω n toward 0 to not be too fast either. Otherwise the assumption nω n → ∞ as N → ∞ would be violated. This assumption implies that the number of both positive and negative cases in the pilot sample should diverge to infinity as n → ∞. To see this, note that the expected number of positive cases in the pilot sample is given by nE{p\n\nSimilarly, the expected number of negative cases in the pilot sample is given by nE{1\n\nThen by the condition that nω n → ∞ as N → ∞ in (C6), we know that both the number of positive and negative cases in the pilot sample should diverge to infinity as N → ∞. This is also a reasonable assumption. Otherwise, no statistically consistent estimator can be computed based on the pilot sample. Combining the conditions n/(N ω n ) → ∞ and nω n → ∞ as N → ∞, we implicitly assume that n 2 /N → ∞. In this way, this assumption further requires that the pilot sample size cannot be too small either.\n\nCondition (C7) is basically a moment type condition. It constrains the convergence rate of some p(W ⊤ i α j ){1 -p(W ⊤ i α j )} related moments to be no slower than ω n . The last condition (C8) constrains the information contained in the information matrix\n\nto not be too small. With the help of these assumptions, we then have the following theorem about θ imp . THEOREM 3. Under conditions (C1)-(  C2 ) and (  C6 )-(C8), we have\n\nThe detailed proof of Theorem 3 is provided in Appendix D in the supplementary material. By Theorem 3, we know that, if we can impute the missing variable Z i with sufficiently high accuracy, the asymptotic behavior of θ imp could be the same as that of the oracle OLS estimator θ ols , which is the estimator obtained with all Z i values observed. It is then of great interest to further explore the intrinsic relationship between Theorems 1 and 3. By Theorem 1, we know that θ imp -θ can be decomposed as\n\nIf the same technical condition as in Theorem 3 can be assumed, we then have (1)\n\nThe verification details are given in Appendix D. It follows then that\n\n, which is the same conclusion as given in Theorem 3.\n\n2.5. Unified Covariance Matrix Estimation. We next consider how to estimate the asymptotic covariance of θ imp . By Theorem 1, we know that the asymptotic covariance matrix of the imputed estimator is considerably more complicated than that of a standard estimator without imputation. By treating the imputed values as if they were the truth, the estimated asymptotic covariance will be seriously biased, since we are trying to estimate the asymptotic covariance of the oracle OLS estimator θ ols . As a result, the standard covariance estimator assuming imputed values are true fails. In the meanwhile, the previous analysis suggests that different cases of belonging lead to different analytical formulas according to the case that the intended application belongs to. For convenience, it is of great interest to have a unified asymptotic covariance estimator, which works well for all the cases. Specifically, we start with the asymptotic covariance estimation for the regular case. By Theorem 1, we know that the asymptotic covariance of θ imp is mainly determined by the following quantities. They are\n\n, Ω and the asymptotic covariance matrix of A -A, which is given by I\n\n. Accordingly, they can be estimated based on the pilot sample as\n\nand\n\nThis leads to a natural estimator for the asymptotic covariance of θ imp as\n\nFrom Theorem 1, we know that the asymptotic covariance of θ imp is given as cov(\n\nunder the same conditions as in Theorem 1. This seems to be an unsurprising result as Σ is constructed accurately to the analytical formula as given by Theorem 1. However, whether it is also a consistent estimator for cov( θ imp ) under the conditions of Theorems 2 and 3 is not immediately straightforward. We then have the following theorem.\n\nTHEOREM 4. (1) Under the same conditions as in Theorem 2, we have nr\n\n(2) Under the same conditions as in Theorem 3, we have\n\nThe detailed proof of Theorem 4 is provided in Appendix E in the supplementary material.\n\nBy Theorem 4, we find that the covariance matrix estimator is still consistent under the same technical conditions as in Theorems 2 and 3. Specifically, the convergence rate of Σ matches that of θ imp . In the highly imbalanced case, the estimated covariance of β imp,j is of order O p (n -1 r N,max r -2 N j ) and that of γ imp is of order O p (n -1 r N,max ), whereas in the highly predictable case, the estimated covariance of θ imp is of order O p (N -1 ). Then we can conduct a unified hypothesis test or the unified confidence region to examine the significance of the interested coefficients.\n\n2.6. A Further Improved Estimator for the Regular Case. As we have discussed in Theorem 1, the imputed estimator θ imp might perform worse than the pilot estimator θ pilot , when σ is relative small for the regular case. Then a natural question is: can we find an estimator, which combines the strength of both the imputed and the pilot estimators, so that the new estimator can outperform both the pilot and imputed estimators uniformly? To this end, we consider a weighted estimator as θ w = w θ pilot + (1 -w) θ imp . Then the pilot and impute estimators are both special cases of θ w . Specifically, when w = 0, we have θ 0 = θ pilot . When w = 1, we have θ 1 = θ imp . We next search for the optimal w ∈ [0, 1] for the best asymptotic performance. To this end, we adopt the idea of A-optimality  (Kiefer, 1959)  and define an objective function as tr{avar( θ w )} = w 2 tr{avar( θ pilot )} + 2w(1 -w)tr{acov( θ imp , θ imp )} + (1 -w) 2 tr{avar( θ imp )}, where avar( θ) is the asymptotic variance of the estimator θ and acov( θ 1 , θ 2 ) is the asymptotic covariance matrix between θ 1 and θ 2 . By minimizing the loss function, the optimal weight w * can be calculated as\n\n. By replacing the unknown parameters by their sample counterparts, we can estimate w * by\n\n,\n\nwhere\n\nWe then have the following theorem for the asymptotic efficiency of θ w . THEOREM 5. Under the same conditions as in Theorem 1, we have Based on these settings, we consider two examples to demonstrate the imbalance effect and predictability effect for different estimators, which correspond to the two special cases discussed in Section 2. Example 1: Imbalance effect. We first study the imbalance effect with σ = 1. Consider a logistic regression model with different intercepts. Specifically, set α 1 = (α N , 3/2, 0, 0, 3/4, 0, 0, -2, 0) ⊤ ∈ R 9 and α 2 = (tα N , 1, 1, 1, -3 √ 2/2, 1/3, 0, 0, 0) ⊤ ∈ R 9  (Fan and Li, 2001; Fan, Samworth and Wu, 2009)  with α N = -C log(n), where C ∈ {0, 0.15, 0.25, 0.45} and t ∈ {2, 3, 4} are two constants controlling the maximum positive proportion r N,max and the imbalance between Z i1 and Z i2 , respectively. It could be calculated that r N,max = max j r N j = exp(α N 1 ) = n -C and r N 2 = n -Ct . Then an increase in C leads to a decrease in r N,max and an increase in t results in a decrease in r N 2 . Specifically, here C = 0 represents the regular case with P (Z i1 = 1) ≈ 50.0% and P (Z i2 = 1) ≈ 50.0%; and C = 0.45 represents the highly imbalanced case with P (Z i1 = 1) ≈ 11.0% and P (Z i2 = 1) ≤ 1%. The MSE values for β 1 , β 2 and γ are then log-transformed and boxplotted in Figure  3 .\n\nWe can draw several conclusions from Figure  3 . First, the estimation errors of β pilot,j , β imp,j , and β ols,j increase as C increases with respect to the increase in the medians of the corresponding boxes. This is because the increase in C leads to a decrease in r N,max , which further leads to a slower convergence rate for all the three estimators. This is consistent with the theoretical results in Theorem 2. Second, the estimation errors of γ pilot and γ ols are relatively stable when C and r N,max change in the observation that the boxes are located approximately at the same position. By contrast, the estimation error of γ imp decreases with the decreasing medians and narrower boxes in the boxplot as C increases and r N,max decreases. This is consistent with the theoretical claims of Theorem 2. From Theorem 2, we know that γ pilot and γ ols are not much affected by the imbalance level. By contrast, γ imp converges faster as C increases and r N,max decreases. Third, when r N,max is fixed and r N 2 decreases, the estimation error of β 2 increases while those of the other estimators remain relatively stable. This is also as expected, as the convergence rate of β 2 is much affected by r N 2 . This explains the worse performance of β 2 . By contrast, the performances of other estimators are not much affected as r N,max is stable in this case.\n\nExample 2: Predictability effect. We next study examples with different prediction accuracy. Consider a logistic regression model with different coefficient norms. Specifically, set α 1 = k(0, 3/2, 0, 0, 3/4, 0, 0, -2, 0) ⊤ ∈ R 9 and α 2 = k(0, 1, 1, 1, -3 √ 2/2, 1/3, 0, 0, 0) ⊤ ∈ R 9 , where k ∈ {1, 5, 15} is some positive constant controlling the size of ∥α j ∥. Specifically, ω n ≈ 0.132, 0.032, 0.011 for k = 1, 5, 15. Then we vary σ ∈ {4, 2, 1, 0.5}. For different σ values, k = 1 represents the regular case and k = 15 represents the highly predictable case. For different k values, σ = 4 represents cases with high noise level and σ = 1 represents cases with low noise level. In addition to the aforementioned three estimators, we also consider the weighted estimator θ w in this example. The MSE values for θ are then log-transformed and boxplotted in Figure  4 .\n\nWe can obtain the following conclusions from Figure  4 . First, as k increases and ω n decreases, we find that the log(MSE) of θ imp approaches that of the oracle estimator θ ols with a smaller difference between the two estimators in the median values of log(MSE) in both low and high noise level cases. This finding is consistent with the theoretical claim of Theorem 3. As shown in Appendix E, we have\n\n). In this way, the decrease in ω n leads to the decrease in ζ 1 . Consequently, θ imp becomes closer to ζ 2 , whose asymptotic variance is approaching the same as that of θ ols . Second, for both the regular and highly predictable cases, the noise level (as controlled by σ) does affect the finite sample performance of the pilot sample and oracle estimators significantly. For the cases with high noise level (σ = 4 in the simulation), the performances of the imputed estimators are fairly close to those of the oracle estimator θ ols . This is as expected as the estimation error from the imputation model is relatively small compared with the regression error in the substantive model. Whereas for the case with low noise level (σ = 1 in the simulation), θ imp performs poorly and could be even worse than θ pilot in the sense of the MSE in the regular case. This is also as expected, because by Theorem 1 the regression error will be dominated by the estimation error from the pilot sample size based logistic regression. By contrast, θ imp performs much better than θ pilot in the highly predictable case even though the noise low level is low. This is also expected, because by Theorem 3 the convergence rate of θ imp is faster than that of θ pilot . All these numerical findings corroborate the theoretical claims of Theorem 3 very well. Lastly, the values of log(MSE) for the weighted estimator θ w are always smaller than those of θ imp and θ pilot in all cases, which is consistent with the results in Theorem 5.\n\n3.2. Finite Sample Performance of the Unified Covariance Estimator. Next, we verify the finite sample performance of the unified covariance estimator Σ. In this section, we consider three different cases, which are listed as follows.\n\nCase 1. Regular case. We consider C = 0 in Example 1. Case 2. Highly imbalanced case. We set C = 0.45 and t = 2 in Example 1. Case 3. Highly predictable case. We let k = 15 and σ = 1 in Example 2.\n\nFor the sample sizes, let (n, N ) ∈ {(6000, 140000), (8000, 200000)}. For each θ j , the true standard error is estimated by\n\nj be the estimated standard error of θ (b) j using the unified covariance estimator (2.3) from the bth replication. Define SE\n\nj . We also compute the empirical coverage probability of the 95% confidence interval, which is given as\n\n. We consider two types of estimators here. The first type of estimators are SE obs j and CP obs . They are, respectively, the standard error estimate obtained by treating the imputed values as if they were the true values and the coverage probability of the resulting confidence interval. The second type of estimators are SE imp j and CP imp . They are, respectively, the standard error estimate obtained by using the unified covariance estimator (2.3) and the coverage probability of the resulting confidence interval. The simulation results for different cases are presented in Table  1 .\n\nFrom Table  1 , we find that the unified covariance estimators work fairly well for different cases. First, for all the settings, the values of SE imp are quite close to the true SE. In contrast, the values of SE obs are very different from those of the true SE. Meanwhile, as n and N increase, the differences between SE imp and SE shrink towards 0. This result suggests that SE imp is a consistent estimator for the true SE. This corroborates the theoretical claims of Theorem 4 very well. As a consequence, we find that the empirical coverage probability (i.e., CP) of the confidence interval constructed by using the unified covariance estimator is fairly close to their nominal level of 95%. In contrast, those of CP obs are far from 95%. This further confirms that SE obs j is not a consistent estimator for the true SE at all. Second, compared with the regular case, the SE and SE values of β imp,1 and β imp,2 increase, while those of γ decrease slightly in the highly imbalanced case. This is also consistent with the theoretical claims in Theorems 2 and 4. By Theorems 2 and 4, we know that the variance and the estimated variance of β imp,j are all of order O(n -1 r -1 N,max ) and those of γ imp are of order O(n -1 r N,max ). Consequently, the increase in the imbalance level will result in a larger variance in β imp and a smaller variance in γ imp . Third, compared with the regular case, the SE and SE values of β imp and γ imp also decrease for the highly predictable case. This is consistent with the theoretical findings in Theorems 3 and 4. As shown in Theorems 3 and 4, in the highly predictable case, the variance and estimated variance of θ imp are of order O(N -1 ). Under the assumption n/N → 0, the performance of θ imp in the highly predictable case will become much better than the regular case. All the simulation results demonstrate the empirical effectiveness of the unified covariance estimator.",
      "page_start": 8,
      "page_end": 17
    },
    {
      "section_name": "Imputation By Deep Neural Networks.",
      "text": "In this subsection, we present an experiment with the binary variables Z ij 's imputed by some sophisticated deep neural network models. More specifically, if we extract the W i -feature from a pre-trained neural network model without fine tuning, then our method for W i and Z i is a standard logistic regression. It is remarkable that fine tuning refers to making small adjustments to the parameter estimates of the pre-trained model by the additional information provided by the target dataset, so that the resulting statistical performance can be further improved; see  Oquab et al. (2014)  and  Rebuffi, Bilen and Vedaldi (2017)  for a more detailed discussion. However, if we allow the parameter in the neural network for W i -feature extraction to be fine tuned based on our data, our method becomes a deep neural network model immediately. We consider two different simulation models here. Model 1. The true regression model between W i and Z i is assumed to be a simple logistic regression model. More specifically, the generation of W i and Z i is the same as that in Example 3.2 with k = 1.\n\nModel 2. We assume to model the relationship between W i and Z i by a highly nonlinear regression model. First, W i = (1, W ⊤ i ) ⊤ ∈ R 9 is generated in the same way as before. Next we generate Z i1 and Z i2 by P\n\n. The data generation of X i and ε i is the same as Section 3.1. Once Z i and X i are given, we have Y i generated in the same way as Example 3.2 with σ = 1.\n\nWe next consider estimating the regression relationship between W i and Z i by a deep neural network regardless of the fact whether the true regression relationship is a single logistic regression model or a highly nonlinear one. Consider a full connected neural network with a total of 4-layers. The GELU function is used for each hidden neural activation  (Hendrycks and Gimpel, 2016) . The width of each hidden layer is fixed to be 16, 32 and 16, respectively. To avoid overfitting, a dropout layer with a 50% dropout rate is used between the second and third layers for training  (Srivastava et al., 2014) . This leads to a total of 1,304 parameters. We next vary the sample size to be (n, N ) ∈ {(1000, 25000),  (2000, 60000) , (3000, 100000)}. For each simulation example, the experiment is replicated for a total of B = 200 times. This leads to a total of 200 MSE values. They are then boxplotted in Figure  5 . For comparison purpose, the binary covariates imputed by the logistic regression model are also reported.\n\nFrom Figure  5 , we find that as (n, N ) increases, the log(MSE) of θ imp becomes smaller for both models and both imputation methods. In the meanwhile, the log(MSE) of θ imp imputed by logistic regression model is smaller than that of θ imp imputed by neural networks, if the true model is logistic regression model. In contrast, the estimator θ imp imputed by neural network outperforms the one with missing features imputed by the logistic regression model, if the relationship between W i and Z i is highly non-linear.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Emotion Recognition Analysis.",
      "text": "We present here a real data example to demonstrate the practical usefulness of the proposed procedure. It is about the aforementioned audio record dataset. The raw data contains a total of 3,262 live streaming video records. These video records are generated by automobile dealers promoting their automobile products on the DouYin platform in China. Each video record lasts for about 30 to 150 minutes, which leads to approximately 253,459 minutes video records in total. These video records are then cut into a total of 2,744,173 video clips with each clip lasting 5 seconds. Each clip is then treated as one observation leading to a large sample size of N = 2, 744, 173. For each observation, the response of interest Y i is the increment of the number of likes during the live streaming clip in log scale. Intuitively, the number of likes reflects the popularity and quantities of the live streaming and thus is of great practical importance. Therefore, it makes sense to examine the factors influencing this response.\n\nAn important factor is the live streamer's emotional state. Intuitively, if the live streamer demonstrates strong passion and confidence, the audience are more likely to generate likes. However, this intuition has never been empirically verified by rigorous statistical analysis. To empirically test this hypothesis, we might need to code each observation (i.e., each 5 second video clip) for two binary indicators Z i1 and Z i2 to describe the streamer's emotional state. Here Z i1 is the valance of the live streamer's emotion with Z i1 = 1 representing positive and Z i1 = 0 otherwise. Moreover, Z i2 is the arousal of the live streamer's emotion with Z i2 = 1 representing a strong emotion and Z i2 = 0 otherwise  (Russell, 1980) . Unfortunately, this is a task relying on human effort (i.e., field experts). Therefore, it will be extremely expensive to process the whole sample N = 2, 744, 173. To alleviate the data labeling cost, we then have to rely on the imputation method developed in this work with a pilot sample of size n = 6, 568 according to our limited budget. The emotional state of each pilot sample is then manually coded by field experts. Next, for the rest uncoded observations, the missing emotional state is imputed with an r-dimensional feature vector W i = (1, W ⊤ i ) ⊤ ∈ R r with r = 7. The feature vector W i is constructed using the Mel spectrogram  (Rabiner and Schafer, 2010)  together with a multi-task trained VGG16 model with the weights pretrained on ImageNet  (LeCun et al., 1989; Caruana, 1997; Simonyan and Zisserman, 2014) . Then the first 6 principal components W i are extracted from the output of the last layer of the neural network. Thereafter, a logistic regression model from W i to each Z ij can be constructed. This leads to an imputation model with an out-of-sample AUC 64.27% for Z i1 and 81.74% for imputing Z i2 . Moreover, the cases with Z i1 = 1 accounts for approximately 5.36% in the pilot sample whereas Z i2 = 1 accounts for about 17.44%. Therefore, this particular dataset seems to fall into the class of regular cases.\n\nOther than emotional state, we also collect for each observation a set of three explanatory variables as X i , which are (1) the number of total followers before the live streaming; (2) the cumulative likes before this period; (3) the number of danmaku characters, see  Wang et al. (2022)  and  Zhou and Tong (2022) . All the covariates are log-transformed and plus 1 before the transformation to avoid zero values. Subsequently, our method is applied and the detailed estimation results are given in Table  2 . For comparison purposes, those of the pilot estimate are also presented. We find that the estimated standard errors of both β pilot and γ pilot are significantly larger than those of β imp and γ imp , respectively. This suggests that a statistically more accurate estimator can be obtained using the imputation method. Both models suggest that both positive and strong emotions have a significantly positive effect on the number of likes. In addition to that, the number of followers, cumulative likes, and danmaku characters all have significantly positive effects on the current number of likes. All results are consistent To gain some intuitive understanding about the practical significance of both Z i -features, we compute the standard deviation of the response, which is given by 0.571. We then use this as a reference. Note that the estimated coefficient of Z i1 and Z i2 based on imputed data are given by 0.040 and 0.038, respectively. As a result, the relative effect size of the total effect due to emotion measured by the ratio between the coefficients and the standard deviation of the response is given by (0.040 + 0.038)/0.571 = 13.7%.\n\nWe next randomly split the datasets into two parts with 80% of the observations for training and the rest for testing. Then we estimate the coefficient on the training data and compute the RMSE on the testing one. We compare two models. The first model is the linear regression model (2.1) with missing Z i -feature imputed by Z i . The second model is almost the same as the first one but with the feature vector W i also included. We find that the out-of-sample prediction accuracy as measured by RMSE is almost unchanged with 0.5436 for the first model and 0.5434 for the second one. This suggests that W i might have little extra contribution for improving prediction accuracy.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Concluding Remarks.",
      "text": "In this article, we study the theoretical properties of imputed estimator of linear regression with imputed binary covariates. Rigorous asymptotic theory is established for the regular case. Different special cases (i.e., the highly imbalanced and highly predictable cases) are also investigated and theoretically discussed. A unified covariance matrix estimator is also developed for statistical inference. Extensive numerical studies are presented to demonstrate the proposed method. To conclude the article, we discuss here a number of interesting topics for future research. First, this study assumes a fixed-dimensional feature. Then, how to handle high dimensional features becomes an interesting problem. Second, this study uses a standard linear regression model. This leaves many other important models (e.g., generalized linear models) open for study. Third, for the highly imbalanced case, the convergence rate of the imputed estimator is unfortunately slower than that of the pilot estimator. Then how to obtain a better estimator remains an important and challenging problem. Lastly, our working imputation model is also related to a single index model as Y i = p j=1 β j σ(W ⊤ i α j ) + X ⊤ i β + ε i , where σ(•) is the sigmoid function. If this is assumed to be true population model, how to conduct imputation and then valid statistical inference is also an interesting problem for further study.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Supplementary Material",
      "text": "",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Technical Proofs And Additional Numerical Results",
      "text": "The Supplemental Material contains some useful lemmas, the technical proofs of Theorem 1-5, the verification of the results in Section 2.4 and some additional numerical results.",
      "page_start": 20,
      "page_end": 20
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ). The emotional state of the streamers (i.e., 1",
      "page": 1
    },
    {
      "caption": "Figure 1: Example of live streaming on TikTok. The left figure represents the live streaming",
      "page": 2
    },
    {
      "caption": "Figure 2: We show theoretically",
      "page": 4
    },
    {
      "caption": "Figure 2: Framework of regression with imputed binary covariates. The pilot sample contains",
      "page": 5
    },
    {
      "caption": "Figure 3: Boxplots of the log-transformed MSE values of the pilot estimator, the imputed esti-",
      "page": 14
    },
    {
      "caption": "Figure 3: We can draw several conclusions from Figure 3. First, the estimation errors of bβpilot,j,",
      "page": 14
    },
    {
      "caption": "Figure 4: Boxplots of the log-transformed MSE values of the pilot estimator bθpilot, the imputed",
      "page": 15
    },
    {
      "caption": "Figure 4: We can obtain the following conclusions from Figure 4. First, as k increases and ωn de-",
      "page": 15
    },
    {
      "caption": "Figure 5: Boxplots of the log-transformed MSE values of the imputed estimator bθimp under",
      "page": 18
    },
    {
      "caption": "Figure 5: For comparison",
      "page": 18
    },
    {
      "caption": "Figure 5: , we find that as (n,N) increases, the log(MSE) of bθimp becomes smaller for",
      "page": 18
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Pilot\nEstimate\nS.E.": "−0.323∗∗∗\n0.0527\n0.018∗∗∗\n0.0048\n0.081∗∗∗\n0.0042\n0.067∗∗∗\n0.0062",
          "Imputed\nEstimate\nS.E.": "−0.348∗∗∗\n0.0026\n0.021∗∗∗\n0.0002\n0.079∗∗∗\n0.0002\n0.065∗∗∗\n0.0003"
        },
        {
          "Pilot\nEstimate\nS.E.": "0.014\n0.0295\n0.043∗∗\n0.0183",
          "Imputed\nEstimate\nS.E.": "0.040∗∗∗\n0.0082\n0.038∗∗∗\n0.0025"
        }
      ],
      "page": 20
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Multiple imputation of covariates by fully conditional specification: Accommodating the substantive model",
      "authors": [
        "J Bartlett",
        "S Seaman",
        "I White",
        "J Carpenter",
        "A Initiative"
      ],
      "year": "2015",
      "venue": "Statistical Methods in Medical Research"
    },
    {
      "citation_id": "2",
      "title": "Multitask learning",
      "authors": [
        "R Caruana"
      ],
      "year": "1997",
      "venue": "Machine Learning"
    },
    {
      "citation_id": "3",
      "title": "Population pharmacokinetic analysis of tacrolimus in Chinese myasthenia gravis patients",
      "authors": [
        "Y.-S Chen",
        "Z.-Q Liu",
        "R Chen",
        "L Wang",
        "L Huang",
        "X Zhu",
        "T.-Y Zhou",
        "W Lu",
        "P Ma"
      ],
      "year": "2017",
      "venue": "Acta Pharmacologica Sinica"
    },
    {
      "citation_id": "4",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M De-Hghani",
        "M Minderer",
        "G Heigold",
        "S Gelly"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "5",
      "title": "Variable selection via nonconcave penalized likelihood and its oracle properties",
      "authors": [
        "J Fan",
        "R Li"
      ],
      "year": "2001",
      "venue": "Journal of the American Statistical Association"
    },
    {
      "citation_id": "6",
      "title": "Ultrahigh dimensional feature selection: beyond the linear model",
      "authors": [
        "J Fan",
        "R Samworth",
        "Y Wu"
      ],
      "year": "2009",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "7",
      "title": "A pseudo empirical likelihood approach for stratified samples with nonresponse",
      "authors": [
        "F Fang",
        "Q Hong",
        "J Shao"
      ],
      "year": "2009",
      "venue": "The Annals of Statistics"
    },
    {
      "citation_id": "8",
      "title": "Escaping the big data paradigm with compact transformers",
      "authors": [
        "A Hassani",
        "S Walton",
        "N Shah",
        "A Abuduweili",
        "J Li",
        "H Shi"
      ],
      "year": "2021",
      "venue": "Escaping the big data paradigm with compact transformers",
      "arxiv": "arXiv:2104.05704"
    },
    {
      "citation_id": "9",
      "title": "Gaussian error linear units (gelus)",
      "authors": [
        "D Hendrycks",
        "K Gimpel"
      ],
      "year": "2016",
      "venue": "Gaussian error linear units (gelus)",
      "arxiv": "arXiv:1606.08415"
    },
    {
      "citation_id": "10",
      "title": "Optimum experimental designs",
      "authors": [
        "J Kiefer"
      ],
      "year": "1959",
      "venue": "Journal of the Royal Statistical Society: Series B (Methodological)"
    },
    {
      "citation_id": "11",
      "title": "Handwritten digit recognition with a back-propagation network",
      "authors": [
        "A Krizhevsky",
        "G Hinton"
      ],
      "year": "1989",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "12",
      "title": "Theory of point estimation",
      "authors": [
        "E Lehmann",
        "G Casella"
      ],
      "year": "2006",
      "venue": "Theory of point estimation"
    },
    {
      "citation_id": "13",
      "title": "Estimating and exploiting the impact of photo layout: A structural approach",
      "authors": [
        "H Li",
        "D Simchi-Levi",
        "M Wu",
        "W Zhu"
      ],
      "year": "2023",
      "venue": "Management Science"
    },
    {
      "citation_id": "14",
      "title": "Statistical analysis with missing data 793",
      "authors": [
        "R Little",
        "D Rubin"
      ],
      "year": "2019",
      "venue": "Statistical analysis with missing data 793"
    },
    {
      "citation_id": "15",
      "title": "Large-scale cross-category analysis of consumer review content on sales conversion leveraging deep learning",
      "authors": [
        "X Liu",
        "D Lee",
        "K Srinivasan"
      ],
      "year": "2019",
      "venue": "Journal of Marketing Research"
    },
    {
      "citation_id": "16",
      "title": "Automated flower classification over a large number of classes",
      "authors": [
        "M.-E Nilsback",
        "A Zisserman"
      ],
      "year": "2008",
      "venue": "Sixth Indian Conference on Computer Vision, Graphics & Image Processing"
    },
    {
      "citation_id": "17",
      "title": "Learning and transferring mid-level image representations using convolutional neural networks",
      "authors": [
        "M Oquab",
        "L Bottou",
        "I Laptev",
        "J Sivic"
      ],
      "year": "2014",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "18",
      "title": "A note on distributed quantile regression by pilot sampling and one-step updating",
      "authors": [
        "R Pan",
        "T Ren",
        "B Guo",
        "F Li",
        "G Li",
        "H Wang"
      ],
      "year": "2022",
      "venue": "Journal of Business & Economic Statistics"
    },
    {
      "citation_id": "19",
      "title": "Adaptive weighted learning for unbalanced multicategory classification",
      "authors": [
        "X Qiao",
        "Y Liu"
      ],
      "year": "2009",
      "venue": "Biometrics"
    },
    {
      "citation_id": "20",
      "title": "Efficient and doubly robust imputation for covariate-dependent missing responses",
      "authors": [
        "J Qin",
        "J Shao",
        "B Zhang"
      ],
      "year": "2008",
      "venue": "Journal of the American Statistical Association"
    },
    {
      "citation_id": "21",
      "title": "Theory and applications of digital speech processing",
      "authors": [
        "L Rabiner",
        "R Schafer"
      ],
      "year": "2010",
      "venue": "Theory and applications of digital speech processing"
    },
    {
      "citation_id": "22",
      "title": "Linear statistical inference and its applications 2",
      "authors": [
        "C Rao"
      ],
      "year": "1973",
      "venue": "Linear statistical inference and its applications 2"
    },
    {
      "citation_id": "23",
      "title": "Learning multiple visual domains with residual adapters",
      "authors": [
        "S.-A Rebuffi",
        "H Bilen",
        "A Vedaldi"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems 30"
    },
    {
      "citation_id": "24",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "25",
      "title": "Mathematical statistics",
      "authors": [
        "J Shao"
      ],
      "year": "2003",
      "venue": "Mathematical statistics"
    },
    {
      "citation_id": "26",
      "title": "Sample correlation coefficients based on survey data under regression imputation",
      "authors": [
        "J Shao",
        "H Wang"
      ],
      "year": "2002",
      "venue": "Journal of the American Statistical Association"
    },
    {
      "citation_id": "27",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "28",
      "title": "Dropout: a simple way to prevent neural networks from overfitting",
      "authors": [
        "N Srivastava",
        "G Hinton",
        "A Krizhevsky",
        "I Sutskever",
        "R Salakhutdinov"
      ],
      "year": "2014",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "29",
      "title": "Identifying customer needs from user-generated content",
      "authors": [
        "A Timoshenko",
        "J Hauser"
      ],
      "year": "2019",
      "venue": "Marketing Science"
    },
    {
      "citation_id": "30",
      "title": "Logistic regression for massive data with rare events",
      "authors": [
        "H Wang"
      ],
      "year": "2020",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "31",
      "title": "Multiple imputation for M-regression with censored covariates",
      "authors": [
        "H Wang",
        "X Feng"
      ],
      "year": "2012",
      "venue": "Journal of the American Statistical Association"
    },
    {
      "citation_id": "32",
      "title": "Nonuniform negative sampling and log odds correction with rare events data",
      "authors": [
        "H Wang",
        "A Zhang",
        "C Wang"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "33",
      "title": "The impact of broadcasters on consumer's intention to follow livestream brand community",
      "authors": [
        "W Wang",
        "M Huang",
        "S Zheng",
        "L Lin",
        "L Wang"
      ],
      "year": "2022",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "34",
      "title": "Multiple imputation in quantile regression",
      "authors": [
        "Y Wei",
        "Y Ma",
        "R Carroll"
      ],
      "year": "2012",
      "venue": "Biometrika"
    },
    {
      "citation_id": "35",
      "title": "Bias and efficiency of multiple imputation compared with complete-case analysis for missing covariate values",
      "authors": [
        "I White",
        "J Carlin"
      ],
      "year": "2010",
      "venue": "Statistics in Medicine"
    },
    {
      "citation_id": "36",
      "title": "Imputing missing covariate values for the Cox model",
      "authors": [
        "I White",
        "P Royston"
      ],
      "year": "2009",
      "venue": "Statistics in Medicine"
    },
    {
      "citation_id": "37",
      "title": "Can consumer-posted photos serve as a leading indicator of restaurant survival? Evidence from Yelp",
      "authors": [
        "M Zhang",
        "L Luo"
      ],
      "year": "2023",
      "venue": "Management Science"
    },
    {
      "citation_id": "38",
      "title": "What makes a good image? Airbnb demand analytics leveraging interpretable image features",
      "authors": [
        "S Zhang",
        "D Lee",
        "P Singh",
        "K Srinivasan"
      ],
      "year": "2022",
      "venue": "Management Science"
    },
    {
      "citation_id": "39",
      "title": "A study on the influencing factors of consumers' purchase intention during livestreaming e-commerce: the mediating effect of emotion",
      "authors": [
        "R Zhou",
        "L Tong"
      ],
      "year": "2022",
      "venue": "Frontiers in Psychology"
    }
  ]
}