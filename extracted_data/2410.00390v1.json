{
  "paper_id": "2410.00390v1",
  "title": "Multi-Scale Temporal Transformer For Speech Emotion Recognition",
  "published": "2024-10-01T04:22:10Z",
  "authors": [
    "Zhipeng Li",
    "Xiaofen Xing",
    "Yuanbo Fang",
    "Weibin Zhang",
    "Hengsheng Fan",
    "Xiangmin Xu"
  ],
  "keywords": [
    "Multi-Scale",
    "Transformer",
    "Speech Emotion Recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition plays a crucial role in humanmachine interaction systems. Recently various optimized Transformers have been successfully applied to speech emotion recognition. However, the existing Transformer architectures focus more on global information and require large computation. On the other hand, abundant speech emotional representations exist locally on different parts of the input speech. To tackle these problems, we propose a Multi-Scale TRansfomer (MSTR) for speech emotion recognition. It comprises of three main components: (1) a multi-scale temporal feature operator, (2) a fractal self-attention module, and (3) a scale mixer module. These three components can effectively enhance the transformer's ability to learn multi-scale local emotion representations. Experimental results demonstrate that the proposed MSTR model significantly outperforms a vanilla Transformer and other state-of-the-art methods across three speech emotion datasets: IEMOCAP, MELD and, CREMA-D. In addition, it can greatly reduce the computational cost.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In recent years, with the development of artificial intelligence and robotics, affective computing has become more and more important in human-computer interaction. Human emotions and intentions are well contained in the speech. Speech emotion recognition has a wide range of applications in spoken dialogue systems, call-center conversation analysis, etc. It can also be potentially used in a smart device. Despite great progress that has been made in speech processing, natural emotion understanding is still a challenging task for many smart systems.\n\nWith the advancement of deep neural networks, several attempts have been made to classify the emotional utterances through recurrent neural networks and convolutional neural networks. Guo et al.  [1]  proposed a CNN based on a spectrotemporal-channel attention module to improve emotion representation learning ability. Some algorithms  [2, 3, 4]  used RNN to model temporal sequence and attention mechanism for temporal tokens weighting have helped emotion representation extraction from speech.\n\nSelf-attention-based Transformers  [5, 6, 7, 8]  have become the main backbone in natural language processing(NLP) and computer vision(CV). Inspired by the success of NLP, researchers have tried to replace the entire CNN or RNN with a Transformer. Significant progress has been made in many speech-related tasks such as automatic speech recognition  [9, 10, 11] , speech enhancement  [12, 13, 14] . However, the computational resources that a Transformer with the fullattention mechanism is quadratic to the sequence duration, making it difficult to run on mobiles and embedded devices. In addition, its applications to the speech emotion recognition (SER) task remains limited since human emotions are inherently complex and ambiguous. Some authors have proposed Transformerbased sparse attention mechanisms, such as BigBird  [15]  in NLP, Image transformer  [16] , Swin transformer  [17]  in CV, but these are not well suited for speech emotion tasks, since emotion is embedded in a long segment of continuous speech. So, it is necessary for us to design specific transformers for SER. Chen et al.  [18]  proposed a Transformer based algorithm to capture all emotion features through a single fixed-scale size feature extractor. This might be inappropriate since human emotions can be expressed in different parts of the speech with different time spans. Zhu et al.  [19]  used two different convolutional kernel sizes to simulate the extraction of multi-scale emotions from speech. We believe that using more different speech time scale information will be more helpful for emotion extraction. Emotional cues are multi-grained in nature, so a efficient light-weight Transformer that can utilize multiple granule levels of the acoustic features is more suitable for the speech emotion recognition (SER) task.   • Experiments also show that the proposed model achieves comparable results with other state-of-the-art methods on both IEMOCAP  [20] , Meld  [21]  and CREMA-D  [22]  SER benchmark datasets, but with much less computation requirement.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "The overall architecture of the proposed MSTR model is shown in Figure  2 . As can be seen, different from a vanilla Transformer, the basic block of the proposed MSTR network mainly contains three components: a multi-scale temporal feature operator, a fractal self-attention module, and a scale mixer module. The multi-scale temporal features operator takes raw acoustic feature or output from the lower layer as input and produces multiple output features with different temporal scales.\n\nThe fractal self-attention module is used to efficiently model the temporal relations between different frames within a fixedlength window. Finally, the scale mixer module effectively fuses features at different temporal scales, to create a unified and mixed emotional feature representation. Compared to the original full attention mechanism, fractal attention is more effective in learning multi-grained features while greatly reducing model redundancy. Other modules like feed forward network remain the same as the original vanilla Transformer. The transformer's output will be fed to a classifier with three fully connected layers for sentiment classification. Details about these components are presented in the following sections.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Multi-Scale Temporal Feature Operator",
      "text": "We proposed a multi-scale temporal feature operator to parallelly extract multi-scale feature representations from raw acous-tic features or output from lower layer. It takes a sequential feature X ∈ R T ×F as input, where T is the number of input frames and F indicates the feature dimension. Similar to a vanilla Transformer, we first obtain the Query, Key, and Value. Specifically, the input X is projected in to\n\nAs shown in Figure  2 , the obtained {Q, K, V } are then fed into an average pooling module separately to get features at different time scales. Specifically, a scaling factor S k = p k-1 is designed for the k th scale level, where p is the fractal factor and k ∈ {1, 2, ..., L}. The k th scaling level operates on top of the k -1 th level by averaging p adjacent frames. In a word, the input feature set {Q, K, V } goes through the average pooling operator to obtain temporal scale feature sets at different time scale\n\n×F . The new feature set X k will be fed into the fractal self-attention module to model the features' temporal relationship. It is worth mentioning that use of the pooling operator rather than the convolution can perform better, maintain the original timing structure, and wouldn't add any extra parameters.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Fractal Self-Attention",
      "text": "A vanilla Transformer uses global self-attention and thus requires large amount of computation. In the proposed MSTR model, we propose to calculate the self-attention within a fixedlength window since we already have features at different time scales. In our implementation, the length of the window is set to p, i.e. same with the fractal factor. Thus we call the selfattention module the fractal self-attention. Specifically, the feature set at the k-th scale level X k = Q k , K k , V k is divided by the window size p. Take Q k for example , we will have\n\nFor data in the i-th window block, self-attention is computed, i.e.\n\nwhere t means transpose and\n\nFinally, the output matrices from the self-attention computed at different windows are concatenated along the time dimension to produce new features\n\nThe output Y k from the k-th level scale will then be fed to the following scale mixer module. In Equation  1 , a single-head attention with full dimensionality is used for simplify. The multihead mechanism described in  [5]  can also be applied straightforwardly. The window length p applied to self-attention calculation help substantially reduce computation, especially when the length of the input data is large.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Scale Mixer Module",
      "text": "Fractal self-attention module can greatly reduce the computational complexity, but it also leads to the problem that the model might excessively focus on local semantics and ignore global information. We thus propose a scale mixer module to aggregate multi-scale data to get a unified emotion representation. The first step is to interpolate data at different time scales to have the original temporal sequence length T. This is achieved by performing the nearest up-sample operation. Then the upsampled data go through an activation function Gelu before we simply add all of them up. Finally, a linear projection W O is applied to get the final multi-scale emotion representation.\n\nThe local semantic information from fine-grained features and the global semantic information from coarse-grained features will be re-unified into a single representation. The scale mixer module effectively complements the deficiency of only being able to extract salient local information within a short window in a specific scale caused by the fractal self-attention. Other more complicated fusion methods like scale attention to select important emotional information may also be used to aggregate information from multi-scale features. But the simple method described above performs the best among all the fusion methods we explored in our experiments.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Computational Complexity Analyse",
      "text": "In a MSTR model, the self-attention is computed within a window of length p. The computation complexities of the selfattention from a vanilla Transformer(VTR) and that from the fractal self-attention in a MSTR model are given below:\n\nHere we ignore the computational effort of pooling and upsampling because they are much smaller than the computational effort of the self-attentive module. As can be seen, the computation for the self-attention layer in a vanilla Transformer is quadratic to the input sequence length, while the computation for the fractal self-attention in a MSTR model is liner.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "Details about the experimental settings as well as the evaluation metrics used for three datasets are listed in Table  1 . We used a pre-trained Hubert(-large)  [25] model to extract raw acoustic features. For both the MSTR model and the baseline model-vanilla Transformer, cross-entropy is employed as aloss function, and Adam  [26]  optimizer is employed, the number of basic modeling blocks is 4, the number of heads is 16 and the batch size is 32. In our implementation, Fractal number p is 3 and the number of scale layers L is 4. In order to eliminate the effects of randomness, we trained and evaluated the models 10 times (setting 10 different seeds from 0 to 9), and report the average scores in the following.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Result And Discussion",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Comparison Analyse",
      "text": "The results of different methods are shown, in The MSTR model also outperforms some well-known systems on the three corpora. On IEMOCAP, MSTR achieves comparable performances to  [23] : +0.80%WA and +0.55%UA. In terms of computation, it only requires about 1.5% of the com- putation of the model in  [24] . On Meld, the MSTR model outperforms the previous methods by a large margin: +3.43%WF1 over  [32] . In summary, the MSTR architecture achieves stateof-the-art performance and computational efficiency in three popular benchmark datasets in speech emotion recognition.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Significant Hyperparameter Analysis",
      "text": "We also did experiments to evaluate the influence of the two important hyperparameters: the fractal factor p and the number of scale layers L. Figure  3  shows the results compared with baseline model. As can be seen from Figure  3a , setting p to 3 achieves the best performance in all three speech emotion datasets. From Figure  3b , we can clearly see that the model can substantially benefit from the multi-scale configurations.\n\nThe performance of the MSTR model drops on all the corpus when the number of scale levels goes from 4 to 1, which demonstrates the effectiveness of multi-scale temporal transformer. When p set 3 and L set 1, the fractal attention mechanism degenerates to general window-based attention, compared with baseline model, MSTR does not perform well, and that means the window-based attention mechanism does not apply to all speech emotion datasets, and multi-scale attention mechanism can achieve great performance with low computing volume .This in turn confirms that human emotion indeed exists in features with different time-scales and the fact that multigrained emotion representations are essential. Rational use of emotion representations implied by speech in different time scales is the key to the speech emotion recognition task.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Compared with different attention mechanisms. Full",
      "page": 1
    },
    {
      "caption": "Figure 2: Overall architecture of the proposed Multi-Scale Transformer (MSTR) for speech emotion recognition. Each basic block of",
      "page": 2
    },
    {
      "caption": "Figure 2: As can be seen, different from a vanilla Trans-",
      "page": 2
    },
    {
      "caption": "Figure 2: , the obtained {Q, K, V } are then fed",
      "page": 2
    },
    {
      "caption": "Figure 3: shows the results compared with",
      "page": 4
    },
    {
      "caption": "Figure 3: a, setting p to",
      "page": 4
    },
    {
      "caption": "Figure 3: b, we can clearly see that the model",
      "page": 4
    },
    {
      "caption": "Figure 3: Analysis of the influence of hyperparameters p and L",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 2: Performance comparison of the proposed MSTR",
      "data": [
        {
          "Method": "Guo (2021)[1]\nWang (2021)[27]\nChen (2022)[24]\nGudmalwar (2022)[28]\nFan (2020)[29]\nZou (2022)[23]\nBaseline\nMSTR(ours)",
          "Params": "-\n-\n16.72M\n-\n-\n-\n27.0M\n27.0M",
          "FLOPs": "-\n-\n2.28G\n-\n-\n-\n892.2M\n33.5M",
          "WA": "61.32\n66.50\n62.90\n-\n70.40\n69.80\n68.90\n70.60",
          "UA": "60.43\n65.70\n64.50\n67.42\n65.00\n71.05\n70.02\n71.60"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: Performance comparison of the proposed MSTR",
      "data": [
        {
          "Method": "Liang (2020)[30]\nLian (2021)[31]\nChen (2022)[24]\nHu (2022)[32]\nBaseline\nMSTR(ours)",
          "Params": "-\n-\n33.2M\n-\n25.3M\n25.3M",
          "FLOPs": "-\n-\n1.64G\n-\n432.7M\n29.8M",
          "WF1": "40.20\n38.20\n41.90\n42.72\n45.20\n46.15"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Representation learning with spectro-temporal-channel attention for speech emotion recognition",
      "authors": [
        "L Guo",
        "L Wang",
        "C Xu",
        "J Dang",
        "E Chng",
        "H Li"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "3",
      "title": "Attentive to individual: A multimodal emotion recognition network with personalized attention profile",
      "authors": [
        "J Li",
        "C Lee"
      ],
      "year": "2019",
      "venue": "Interspeech"
    },
    {
      "citation_id": "4",
      "title": "Speech emotion recognition with dual-sequence lstm architecture",
      "authors": [
        "J Wang",
        "M Xue",
        "R Culhane",
        "E Diao",
        "V Tarokh"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "5",
      "title": "Towards the explainability of multimodal speech emotion recognition",
      "authors": [
        "P Kumar",
        "V Kaushik",
        "B Raman"
      ],
      "year": "2021",
      "venue": "Interspeech"
    },
    {
      "citation_id": "6",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "7",
      "title": "Bert: Pretraining of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pretraining of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "8",
      "title": "Transformer-xl: Attentive language models beyond a fixed-length context",
      "authors": [
        "Z Dai",
        "Z Yang",
        "Y Yang",
        "J Carbonell",
        "Q Le",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Transformer-xl: Attentive language models beyond a fixed-length context",
      "arxiv": "arXiv:1901.02860"
    },
    {
      "citation_id": "9",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "10",
      "title": "Transformerbased streaming asr with cumulative attention",
      "authors": [
        "M Li",
        "S Zhang",
        "C Zorilȃ",
        "R Doddipatla"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "DEFORMER: Coupling Deformed Localized Patterns with Global Context for Robust End-to-end Speech Recognition",
      "authors": [
        "J Xie",
        "J Hansen"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "12",
      "title": "Analysis of Self-Attention Head Diversity for Conformer-based Automatic Speech Recognition",
      "authors": [
        "K Audhkhasi",
        "Y Huang",
        "B Ramabhadran",
        "P Moreno"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "13",
      "title": "Dpt-fsnet: Dual-path transformer based full-band and sub-band fusion network for speech enhancement",
      "authors": [
        "F Dang",
        "H Chen",
        "P Zhang"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Efficient Transformerbased Speech Enhancement Using Long Frames and STFT Magnitudes",
      "authors": [
        "D Oliveira",
        "T Peer",
        "T Gerkmann"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "15",
      "title": "SE-Conformer: Time-Domain Speech Enhancement Using Conformer",
      "authors": [
        "E Kim",
        "H Seo"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech 2021"
    },
    {
      "citation_id": "16",
      "title": "Advances in neural information processing systems",
      "authors": [
        "M Zaheer",
        "G Guruganesh",
        "K Dubey",
        "J Ainslie",
        "C Alberti",
        "S Ontanon",
        "P Pham",
        "A Ravula",
        "Q Wang",
        "L Yang"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "17",
      "title": "Image transformer",
      "authors": [
        "N Parmar",
        "A Vaswani",
        "J Uszkoreit",
        "L Kaiser",
        "N Shazeer",
        "A Ku",
        "D Tran"
      ],
      "year": "2018",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "18",
      "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
      "authors": [
        "Z Liu",
        "Y Lin",
        "Y Cao",
        "H Hu",
        "Y Wei",
        "Z Zhang",
        "S Lin",
        "B Guo"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "19",
      "title": "Key-sparse transformer for multimodal speech emotion recognition",
      "authors": [
        "W Chen",
        "X Xing",
        "X Xu",
        "J Yang",
        "J Pang"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "20",
      "title": "Speech emotion recognition with global-aware fusion on multi-scale feature representation",
      "authors": [
        "W Zhu",
        "X Li"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "21",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "E Mower",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "22",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "23",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "24",
      "title": "Speech emotion recognition with co-attention based multi-level acoustic information",
      "authors": [
        "H Zou",
        "Y Si",
        "C Chen",
        "D Rajan",
        "E Chng"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "25",
      "title": "Speechformer: A hierarchical efficient framework incorporating the characteristics of speech",
      "authors": [
        "W Chen",
        "X Xing",
        "X Xu",
        "J Pang",
        "L Du"
      ],
      "year": "2022",
      "venue": "Speechformer: A hierarchical efficient framework incorporating the characteristics of speech",
      "arxiv": "arXiv:2203.03812"
    },
    {
      "citation_id": "26",
      "title": "Hubert: How much can a bad teacher benefit asr pre-training",
      "authors": [
        "W Hsu",
        "Y Tsai",
        "B Bolte",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "27",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "28",
      "title": "Learning mutual correlation in multimodal transformer for speech emotion recognition",
      "authors": [
        "Y Wang",
        "G Shen",
        "Y Xu",
        "J Li",
        "Z Zhao"
      ],
      "year": "2021",
      "venue": "Interspeech"
    },
    {
      "citation_id": "29",
      "title": "The magnitude and phase based speech representation learning using autoencoder for classifying speech emotions using deep canonical correlation analysis",
      "authors": [
        "A Gudmalwar",
        "B Basel",
        "A Dutta",
        "C Rao"
      ],
      "year": "2022",
      "venue": "Interspeech"
    },
    {
      "citation_id": "30",
      "title": "Isnet: Individual standardization network for speech emotion recognition",
      "authors": [
        "W Fan",
        "X Xu",
        "B Cai",
        "X Xing"
      ],
      "year": "2022",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "31",
      "title": "Semi-supervised multi-modal emotion recognition with cross-modal distribution matching",
      "authors": [
        "J Liang",
        "R Li",
        "Q Jin"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "32",
      "title": "Ctnet: Conversational transformer network for emotion recognition",
      "authors": [
        "Z Lian",
        "B Liu",
        "J Tao"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "33",
      "title": "Mm-dfn: Multimodal dynamic fusion network for emotion recognition in conversations",
      "authors": [
        "D Hu",
        "X Hou",
        "L Wei",
        "L Jiang",
        "Y Mo"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    }
  ]
}