{
  "paper_id": "2111.10202v1",
  "title": "Multimodal Emotion Recognition With High-Level Speech And Text Features",
  "published": "2021-09-29T07:08:40Z",
  "authors": [
    "Mariana Rodrigues Makiuchi",
    "Kuniaki Uto",
    "Koichi Shinoda"
  ],
  "keywords": [
    "Emotion recognition",
    "disentanglement representation learning",
    "deep learning",
    "multimodality",
    "wav2vec 2.0"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Automatic emotion recognition is one of the central concerns of the Human-Computer Interaction field as it can bridge the gap between humans and machines. Current works train deep learning models on low-level data representations to solve the emotion recognition task. Since emotion datasets often have a limited amount of data, these approaches may suffer from overfitting, and they may learn based on superficial cues. To address these issues, we propose a novel cross-representation speech model, inspired by disentanglement representation learning, to perform emotion recognition on wav2vec 2.0 speech features. We also train a CNN-based model to recognize emotions from text features extracted with Transformer-based models. We further combine the speech-based and text-based results with a score fusion approach. Our method is evaluated on the IEMOCAP dataset in a 4-class classification problem, and it surpasses current works on speech-only, text-only, and multimodal emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Works",
      "text": "2.1. Wav2vec 2.0 Wav2vec 2.0  [12]  is a framework to obtain speech representations via self-supervision. The wav2vec model is trained on large amounts of unlabelled speech data, and then it is fine-tuned on labelled data for Automatic Speech Recognition (ASR). Wav2vec is composed of a feature encoder and a context network. The encoder takes a raw waveform as input, and outputs a sequence of features with stride of 20 ms and receptive field of 25 ms. These features encode the speech's local information, and they have a size of 768 and 1024 for the \"base\" and \"large\" versions of wav2vec, respectively. The feature sequence is then inputted to the Transformer-based context network, which outputs a contextualized representation of speech. In the \"base\" and \"large\" wav2vec, there are 12 and 24 Transformer blocks, respectively. Although the wav2vec 2.0 learned representations were originally applied to ASR, other tasks, such as speech emotion recognition, can also benefit from these representations  [11, 13, 14] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Speech Emotion Recognition",
      "text": "During its early stage, most works on Speech Emotion Recognition (SER) proposed solutions based on Hidden Markov Models (HMM)  [15] , Support Vector Machines (SVM)  [4, 16] , or Gaussian Mixture Models (GMM)  [5] . However, given the superior performance of deep learning on many speech-related tasks  [17, 18] , deep learning approaches for SER became predominant.\n\nA problem characteristic to SER is the definition of appropriate features to represent emotion from speech  [19] . Previous studies have attempted to extract emotion information from Mel-frequency cepstral coefficients (MFCC), pitch and energy  [5, 6, 7] . However, recent studies showed that employing a weighted sum with learnable weights to combine the local and contextualized outputs of a pre-trained wav2vec 2.0 model yields better speech emotion recognition results  [11] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Text Emotion Recognition",
      "text": "Current Text Emotion Recognition (TER) works use either features from an ASR model trained from scratch  [9] , or Word2Vec or GloVe  [3]  features. Such works yield good results, but, given the outstanding performance of Transformerbased models in various NLP tasks  [20, 21, 22] , it is natural to question commonly used representations for the TER task.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Disentanglement Representation Learning",
      "text": "Disentanglement Representation Learning aims to separate the underlying factors of variation in the data  [23] . The idea is that, by disentangling these factors, we can discard the factors that are uninformative to the task that we would like to solve, while keeping the relevant factors. Disentanglement has been applied to image  [24] , video  [25]  and speech  [26]  applications. In speech-related works, it has been applied mainly to speech conversion and prosody transfer tasks  [27, 28] .\n\nAutoVC  [27]  is an autoencoder that extracts a speakerindependent representation of speech content for speech conversion. A mel-spectrogram is inputted to the model's encoder, and the decoder reconstructs the spectrogram from the encoder's output and a speaker identity embedding. By controlling the encoder's bottleneck size, speaker identity information is eliminated at the encoder's bottleneck. Speech-Flow  [26]  builds upon AutoVC to disentangle speech into pitch, rhythm and content features.\n\nEven though these works show impressive results in speech conversion, only few works attempt to disentangle speech for SER.  [29]  proposed an autoencoder to disentangle speech style and speaker identity from i-vectors and x-vectors, and they used the speech style embedding for SER.  [30]  used adversarial training to disentangle speech features into speaker identity and emotion features. These methods hold similarities with the SER method proposed in this paper, but our approach differs from previous works in that we explicitly eliminate speaker identity information from speech to obtain emotion features, and we perform experiments on the disentanglement property of these features.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "We propose a model to perform SER and a model to perform TER. The SER model takes as input wav2vec features, a mel spectrogram, speaker identity embeddings, and a phone sequence. All these features are extracted from the same speech segment, and the model outputs the probabilities for each emotion class, for the speech segment. The TER model takes as input text features extracted from an utterance's transcript, and outputs the probabilities for each emotion class, for the utterance. The SER and TER results are combined via score fusion to obtain the multimodal emotion class probabilities. Our proposed method, including the SER model, the TER model and the fusion approach, is depicted in Figure  1 .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Ter Model",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Multimodal Emotion",
      "text": "Class Probabilities Score Fusion Fig.  1 . Proposed method depicting the SER and TER models and the fusion approach. 1D ConvNorm layers are defined as a 1D CNN layer followed by batch normalization, and BLSTM layers are bidirectional LSTMs. The number of layers is shown as each block name's prefix. The number of filters F and kernels K in each ConvNorm and CNN layers are shown as the block name's suffix F xK. The number of neurons in LSTM, BLSTM, and Linear layers is shown as the blocks names' suffix.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Speech Emotion Recognition",
      "text": "We propose an encoder-decoder model that takes wav2vec 2.0 features as input, and reconstructs the corresponding melfrequency spectrogram. Our model has four main components: an encoder, a decoder, a phone encoder, and a classifier. The model is trained over speech segments of 96 frames, which is about 2 seconds long. These segments are randomly cropped from the speech utterances during training.\n\nOur SER model is similar to AutoVC  [27] . However, our model differs in three aspects. First, wav2vec features are the acoustic input to our encoder. Second, we include an emotion classifier and an emotion loss to our method. Third, we define a phone encoder, whose output is inputted to the decoder.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Wav2Vec 2.0 Feature Extraction",
      "text": "We extract the wav2vec features from a \"large\" wav2vec 2.0 model pre-trained on 60k hours of unlabelled speech data from the LibriVox dataset 1 . We take the features from the feature encoder's output and from the output of all the 24 Transformer layers in the context network. Thus, for each speech frame, there are 25 1024-dimensional wav2vec features. 1 https://huggingface.co/facebook/wav2vec2-large-lv60",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Speaker Identity Feature Extraction",
      "text": "We extract speaker identity features with Resemblyzer [31] 2  , which is pre-trained on LibriSpeech  [32] , VoxCeleb1  [33]  and VoxCeleb2  [34] . For each utterance, a 256-dimensional embedding is obtained to represent the speaker identity. For each speaker, we extract speaker identity features from 100 randomly selected utterances, and we take their average as the final identity embedding to represent the speaker.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Encoder",
      "text": "A weighted average h avg , with trainable weights α, over the 25 wav2vec 2.0 features h i , is computed as described in  [11] :\n\nh avg is then concatenated with the 256-dimensional speaker identity embedding frame by frame.\n\nThe BLSTM layers in the encoder have d neurons, and their output have a size of [2d, 96] since we concatenate the layers' output in both forward and backward directions. d determines the size of the bottleneck, as it reduces the size of the features in the channel dimension. The downsampler operation  [27]  takes as input an array of size 2d for each speech frame, and returns the arrays taken at every f frames. Thus, this operation reduces the temporal dimension of the feature array, by the downsampling factor f . The encoder outputs a feature array of size [2d, 96/f ], in which d and f control the bottleneck dimension. By controlling the size of the bottleneck, we would like to obtain a disentangled speech representation, that contains emotion information, but that does not contain speaker identity or phonetic information.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Decoder",
      "text": "At the decoder, the encoder's features are upsampled so that their size is the same as before the downsampling operation, by repeating each feature in the temporal dimension f times. Since the encoder's features contain only emotion information, the decoder takes as input not only the encoder's output, but also speaker identity embeddings and phone sequence embeddings to be able to reconstruct the spectrogram.\n\nThe output from the decoder's linear layer is a feature array of size 80 for each speech frame, which represents a mel-spectrogram of the speech segment. These features are compared with the ground-truth mel-spectrogram by means of a reconstruction loss L r1 , which is used to update all the model's parameters. We also compute the reconstruction loss L r2 between the decoder's output and the same ground-truth mel-spectrogram. L r1 and L r2 are computed as\n\nin which M is the training batch size, x k is the k-th feature element in the batch outputted by the model, and y k is the corresponding ground-truth for x k .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Phone Encoder",
      "text": "The phone encoder takes as input a sequence of phone embeddings, and outputs a representation for the whole phone sequence. We follow two steps to obtain these phone embeddings. First, for each utterance, we extract the phone alignment information from the speech signal and its corresponding text transcript, by using the Gentle aligner 3 . Second, we obtain the phone sequence from the phone alignment information, by determining the longest phone for each frame. We define an id number to each phone, and we also assign ids to silence, not-identified phones, and to each special token in the dataset (e.g. \"[LAUGHTER]\"), totalling 128 distinct phone ids represented as one-hot embeddings.\n\n3 https://github.com/lowerquality/gentle",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Classifier",
      "text": "The classifier encourages the encoder's output to contain emotion information. We compute the cross-entropy loss L e between the emotion label c and the softmax of the logits z outputted by the classifier as\n\n.\n\n(3)",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Training And Inference",
      "text": "The objective function to be minimized during training is given as the sum between L r1 , L r2 and L e . During inference, the softmax function is applied to the model's outputted emotion classes logits array to obtain the emotion class probabilities. The class with the highest probability is selected as the final classification result. The model takes as input features from a speech segment of 96 frames. Thus, to obtain an utterance-level prediction, we first compute the emotion class probabilities every 96 consecutive frames in an utterance (without overlap), zero-padding as necessary, and we take the average of these segment-level probabilities as the final utterance-level probability.  [35]  shows that the TER task can benefit from processing embeddings of all text tokens before performing the emotion classification. Inspired by these results, we propose a CNNbased model to process all the token's embeddings in an utterance, extracted with Transformer-based models.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Text Emotion Recognition",
      "text": "We extract a text representation of shape [N, L] for each utterance with pre-trained Transformer-based models, in which N is the number of tokens in the utterance excluding special tokens, and L is the size of each token's feature. We zero-pad the text representation so that the input to the TER model have size [N , L], in which N is the maximum number of tokens found in an utterance of the dataset. These text features are processed with the TER model illustrated in Figure  1 , which is trained on the cross-entropy loss defined in Equation (  3 ). Similar to our SER model, during inference, the softmax function is applied to the TER model's logits array to obtain the emotion class probabilities. However, differently from the SER model, the output from the TER model represents the utterance-level emotion classification result.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Multimodal Emotion Recognition",
      "text": "The speech-based utterance-level probabilities p s and the probabilities outputted from the text model p t for the same utterance are combined as\n\nin which p f is the fused probability, and w 1 and w 2 are fixed weights assigned to the speech and text modalities, respectively. The weights determine the degree of contribution of each data modality to the fused probability, and the emotion classification result for an utterance corresponds to the emotion class with the highest fused probability.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Dataset",
      "text": "We utilize the Interactive Emotional Dyadic Motion Capture (IEMOCAP)  [36]  dataset to evaluate our method. Given the amount of data and the phonetic and semantic diversity of its utterances, the IEMOCAP dataset is considered well-suited for speech-based and text-based emotion recognition. There are 10 actors in this dataset, whose interactions are organized in 5 dyadic sessions, each with an unique pair of male and female actors. The dataset contains approximately 12 hours of audiovisual data, which is segmented into speech turns (or utterances). Each utterance is labelled by three annotators.\n\nFollowing previous works  [3, 8, 9] , we consider only the utterances which are given the same label by at least two annotators, and we merge the utterances labelled as \"Happy\" and \"Excited\" into the \"Happy\" category. We further select only the utterances with the labels \"Angry\", \"Neutral\", \"Sad\" and \"Happy\", resulting in 5,531 utterances, which is approximately 7 hours of data. We utilize only the speech data, the transcripts, and the labels.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Training Configuration",
      "text": "We perform a leave-one-session-out cross-validation in all our experiments. We report our results in terms of Weighted Accuracy (WA) and Unweighted Accuracy (UA). WA is equivalent to the average recall over all the emotion classes and UA is the fraction of samples correctly classified.\n\nAll models are implemented in PyTorch, and, in every training experiment, we use the Adam optimizer with learning rate 10 -4 , and with the default exponential decay rate of the moment estimates. The SER models are trained with a batch size of 2 for 1 million iterations. The TER models are trained with a batch size of 4 for 412,800 iterations.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Speech Emotion Recognition",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Emotion Recognition Experiments",
      "text": "We perform SER with two bottleneck configurations for the encoder, \"Small\" and \"Large\", which have respective bottleneck dimension d equal to 8 and 128, and respective downsampling factor f set as 48 and 2. The utterance-level SER results are presented in Table  1 .\n\nTable  1  indicates that the \"Small\" configuration performs better in the SER task when compared to the \"Large\" model.   [37]  68.3 66.9 Self-Attn+LSTM  [3]  55.6 -BLSTM+Self-Attn  [9]  57.0 55.7 Transformer  [38]  -64.9 CNN+Feat-Attn  [39]  66.7 -wav2vec+CNN  [11]  -67.9 Ours (Small) 70.1 70.7\n\nWe compare the results obtained with the \"Small\" model with the current state-of-the-art in Table  2 .\n\nWe further evaluate whether inputting the wav2vec embeddings is advantageous to SER. We train our model with the same parameters as the \"Small\" configuration, but with a mel-spectrogram as input instead of the wav2vec features. Our results achieved an UA of 50.4% on the 5-fold crossvalidation, which is 19.7% worse than of the model with wav2vec features as input, in terms of absolute accuracy. Therefore, we can conclude that the learned weighted average of the wav2vec embeddings is a better representation of speech for SER on the IEMOCAP dataset when compared to the traditional mel-frequency spectrograms.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Disentanglement Experiments",
      "text": "We train 4-linear-layer (with, from the input to the output, 2048, 1024, 1024, and 8 neurons) classifiers on the obtained speech representations to solve the speaker identification task. Our goal is to see if the obtained emotion features contain speaker identity information. Ideally, we would like our features to be speaker-independent, and to hold a generic emotion representation that could be used across speakers.\n\nWe train the classifiers on a 5-fold cross-validation, but we define the folds differently for this experiment. We randomly separate 80% of each speaker's data for training, and the remaining 20% for test. The folds have speaker dependent train and test sets, and each of them contains the data of only 4 sessions. We train the classifiers on a cross-entropy loss. Table  3  summarizes the speaker identity recognition results.\n\nTable  3  suggests that the features extracted with the \"Large\" model contain more information about speaker identity than the ones from the \"Small\" model. Overall, from the results in Tables  3  and 1 , we can see that the features extracted Table  3 . UA (%) results for the speaker identification task on features extracted with the \"Small\" and \"Large\" models. with the \"Small\" model can achieve a better SER accuracy and a worse speaker identity accuracy when compared to the features extracted with the \"Large\" model. This result suggests that the bottleneck size can lead to a disentanglement of factors in speech, which makes the SER task easier.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Model",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Discussion",
      "text": "We believe that our SER model achieves better results when compared to previous methods due to three factors. First, we use high-level speech representations as the input to our model, which, apart from our work, is only done by  [11]  and  [38] . Second, we are careful in analyzing the type of information encoded in the features obtained by our model, which makes the features have a certain level of disentanglement as shown in Section 6.2. Third, our model can leverage both high-level and low-level features since it is trained to reconstruct spectrograms from wav2vec features.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Text Emotion Recognition",
      "text": "We train the TER model with input features extracted from different Transformer-based models  4  . We use the \"large\" version of all these models, which output a 1024-dimensional feature for each token. The TER results are shown in Table  4  for different feature extractors, and we compare our best results with the current state-of-the-art in Table  5 .\n\nFrom Table  5 , we can see that our method achieves better results than previous works, except for  [40] , which uses context information (i.e., features from succeeding and preceding utterances). Our model differs from previous works in that we Table  5 . Comparison of our TER results with current works in terms of UA(%) and WA (%).\n\nModel UA WA BERT+Attn+Context  [40]  71.9 71.2 BERT+Attn  [40]  64.8 62.9 BLSTM+Self-Attn  [9]  63.6 63.7 Self-Attn+LSTM  [3]  65.9 -Ours (BERT uncased) 66.1 67.0 73.0 73.5\n\ndo not use recurrent neural networks or self-attention, and we benefit from the text representations learned by Transformerbased models trained on large text corpora. We believe our model achieved good results due to BERT's deep features, and to the ability of our model's 1D CNN layers to extract temporal information from the sequence of token's features.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Multimodal Emotion Recognition",
      "text": "We combine the results from our best speech and text models by experimenting with different weight values w 1 and w 2 .\n\nOur best multimodal results are acquired when w 1 = 0.6 and w 2 = 1, and they are reported in Table  6 .\n\nThis result shows that, when combining the speech and the text results, it is better to give less importance to the speech model's result, even though its accuracy is higher than the text model's. We believe this may be related to the confidence in which the TER and the SER models obtain their scores, but further investigation is required. By comparing our results in Tables  1, 4 , and 6, we can conclude that the solution to the emotion recognition task benefits from combining different types of data, since our multimodal result is better than our speech-only and text-only results. Our multimodal approach gives better results than current works except for  [40] , which uses the context information. We attribute the reason of our good results to the fact that our unimodal models outperform other unimodal models, and not to our choice of fusion method. We believe we could achieve better results with a more sophisticated fusion approach or by jointly training the speech and text modalities.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "We proposed a cross-representation encoder-decoder model inspired in disentanglement representation learning to perform SER. Our model leverages both high-level wav2vec features and low-level mel-frequency spectrograms, and it achieves an accuracy of 70.1% on the IEMOCAP dataset. We also used a CNN-based model that processes token's embeddings extracted with pre-trained Transformer-based models to perform TER, achieving an accuracy of 66.1% on the same dataset. We further combined the speech-based and the text-based results via score fusion, achieving an accuracy of 73.0%. Our speech-only, text-only and multimodal results surpassed current works', showing that emotion recognition can benefit from disentanglement representation learning, high-level data representations, and multimodalities.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Proposed method depicting the SER and TER models and the fusion approach. 1D ConvNorm layers are deﬁned as a",
      "page": 3
    },
    {
      "caption": "Figure 1: , which is trained on the cross-entropy loss deﬁned",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 3: suggests that the features extracted with the",
      "data": [
        {
          "Model": "GRU+Context [37]",
          "UA": "68.3",
          "WA": "66.9"
        },
        {
          "Model": "Self-Attn+LSTM [3]",
          "UA": "55.6",
          "WA": "-"
        },
        {
          "Model": "BLSTM+Self-Attn [9]",
          "UA": "57.0",
          "WA": "55.7"
        },
        {
          "Model": "Transformer [38]",
          "UA": "-",
          "WA": "64.9"
        },
        {
          "Model": "CNN+Feat-Attn [39]",
          "UA": "66.7",
          "WA": "-"
        },
        {
          "Model": "wav2vec+CNN [11]",
          "UA": "-",
          "WA": "67.9"
        },
        {
          "Model": "Ours (Small)",
          "UA": "70.1",
          "WA": "70.7"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 4: 5-fold cross-validation UA (%) results for TER on",
      "data": [
        {
          "Model": "BERT+Attn+Context [40]",
          "UA": "71.9",
          "WA": "71.2"
        },
        {
          "Model": "BERT+Attn [40]",
          "UA": "64.8",
          "WA": "62.9"
        },
        {
          "Model": "BLSTM+Self-Attn [9]",
          "UA": "63.6",
          "WA": "63.7"
        },
        {
          "Model": "Self-Attn+LSTM [3]",
          "UA": "65.9",
          "WA": "-"
        },
        {
          "Model": "Ours (BERT uncased)",
          "UA": "66.1",
          "WA": "67.0"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 4: 5-fold cross-validation UA (%) results for TER on",
      "data": [
        {
          "Model": "BERT+Attn+Context [40]",
          "UA": "76.1",
          "WA": "77.4"
        },
        {
          "Model": "LAS-ASR [8]",
          "UA": "66.0",
          "WA": "64.0"
        },
        {
          "Model": "ASR-SER [9]",
          "UA": "69.7",
          "WA": "68.6"
        },
        {
          "Model": "CMA+Raw waveform [3]",
          "UA": "72.8",
          "WA": "-"
        },
        {
          "Model": "Ours (w1 = 0.6, w2 = 1)",
          "UA": "73.0",
          "WA": "73.5"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 4: 5-fold cross-validation UA (%) results for TER on",
      "data": [
        {
          "Model": "ALBERT",
          "Avg ± std": "62.3 ± 2.3"
        },
        {
          "Model": "BERTc",
          "Avg ± std": "65.5 ± 3.3"
        },
        {
          "Model": "BERTu",
          "Avg ± std": "66.1 ± 2.1"
        },
        {
          "Model": "BERTuwm",
          "Avg ± std": "65.8 ± 2.6"
        },
        {
          "Model": "ELECTRA",
          "Avg ± std": "56.6 ± 3.8"
        },
        {
          "Model": "RoBERTa",
          "Avg ± std": "64.1 ± 3.5"
        },
        {
          "Model": "XLNetc",
          "Avg ± std": "58.1 ± 3.6"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Continuously variable duration hidden markov models for automatic speech recognition",
      "authors": [
        "Stephen E Levinson"
      ],
      "year": "1986",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "2",
      "title": "Sentiment analysis by capsules",
      "authors": [
        "Yequan Wang",
        "Aixin Sun",
        "Jialong Han",
        "Ying Liu",
        "Xiaoyan Zhu"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 world wide web conference"
    },
    {
      "citation_id": "3",
      "title": "Multimodal emotion recognition using cross-modal attention and 1d convolutional neural networks",
      "authors": [
        "Krishna Dn",
        "Ankita Patil"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech 2020"
    },
    {
      "citation_id": "4",
      "title": "Speech emotion recognition using support vector machine",
      "authors": [
        "Yixiong Pan",
        "Peipei Shen",
        "Liping Shen"
      ],
      "year": "2012",
      "venue": "International Journal of Smart Home"
    },
    {
      "citation_id": "5",
      "title": "Emotion recognition from assamese speeches using mfcc features and gmm classifier",
      "authors": [
        "Aditya Bihar Kandali",
        "Aurobinda Routray",
        "Tapan Basu"
      ],
      "year": "2008",
      "venue": "TENCON 2008-2008 IEEE region 10 conference"
    },
    {
      "citation_id": "6",
      "title": "Speech emotion recognition using deep neural network and extreme learning machine",
      "authors": [
        "Kun Han",
        "Dong Yu",
        "Ivan Tashev"
      ],
      "year": "2014",
      "venue": "Speech emotion recognition using deep neural network and extreme learning machine"
    },
    {
      "citation_id": "7",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin Wöllmer",
        "Björn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "8",
      "title": "Speech representation learning for emotion recognition using end-to-end asr with factorized adaptation",
      "authors": [
        "Sung-Lin Yeh",
        "Yun-Shao Lin",
        "Chi-Chun Lee"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech 2020"
    },
    {
      "citation_id": "9",
      "title": "Endto-end speech emotion recognition combined with acoustic-to-word asr model",
      "authors": [
        "Han Feng",
        "Sei Ueno",
        "Tatsuya Kawahara"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech 2020"
    },
    {
      "citation_id": "10",
      "title": "Explaining and harnessing adversarial examples",
      "authors": [
        "Ian Goodfellow",
        "Jonathon Shlens",
        "Christian Szegedy"
      ],
      "year": "2014",
      "venue": "Explaining and harnessing adversarial examples",
      "arxiv": "arXiv:1412.6572"
    },
    {
      "citation_id": "11",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "Leonardo Pepino",
        "Pablo Riera",
        "Luciana Ferrer"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech 2021"
    },
    {
      "citation_id": "12",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "13",
      "title": "Jointly Fine-Tuning \"BERT-Like\" Self Supervised Models to Improve Multimodal Speech Emotion Recognition",
      "authors": [
        "Shamane Siriwardhana",
        "Andrew Reis"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "14",
      "title": "On the use of self-supervised pretrained acoustic and linguistic features for continuous speech emotion recognition",
      "authors": [
        "Manon Macary",
        "Marie Tahon",
        "Yannick Estève",
        "Anthony Rousseau"
      ],
      "year": "2021",
      "venue": "2021 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "15",
      "title": "Speech emotion recognition using hidden markov models",
      "authors": [
        "Tin Lay Nwe",
        "Say Foo",
        "Liyanage C De Silva"
      ],
      "year": "2003",
      "venue": "Speech communication"
    },
    {
      "citation_id": "16",
      "title": "in 2005 international conference on machine learning and cybernetics",
      "authors": [
        "Yi-Lin Lin",
        "Gang Wei"
      ],
      "year": "2005",
      "venue": "in 2005 international conference on machine learning and cybernetics"
    },
    {
      "citation_id": "17",
      "title": "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition",
      "authors": [
        "William Chan",
        "Navdeep Jaitly",
        "Quoc Le",
        "Oriol Vinyals"
      ],
      "year": "2016",
      "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "18",
      "title": "Wavenet: A generative model for raw audio",
      "authors": [
        "Aäron Van Den Oord",
        "Sander Dieleman",
        "Heiga Zen",
        "Karen Simonyan",
        "Oriol Vinyals",
        "Alex Graves",
        "Nal Kalchbrenner",
        "Andrew Senior",
        "Koray Kavukcuoglu"
      ],
      "year": "2016",
      "venue": "9th ISCA Speech Synthesis Workshop"
    },
    {
      "citation_id": "19",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "Moataz Ayadi",
        "Mohamed Kamel",
        "Fakhri Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "20",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "21",
      "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
      "authors": [
        "Zhilin Yang",
        "Zihang Dai",
        "Yiming Yang",
        "Jaime Carbonell",
        "Russ Salakhutdinov",
        "Quoc V Le"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "22",
      "title": "Electra: Pre-training text encoders as discriminators rather than generators",
      "authors": [
        "Kevin Clark",
        "Minh-Thang Luong",
        "Quoc Le",
        "Christopher Manning"
      ],
      "year": "2020",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "23",
      "title": "",
      "authors": [
        "Ian Goodfellow",
        "Yoshua Bengio",
        "Aaron Courville",
        "Deep Learning"
      ],
      "year": "2016",
      "venue": ""
    },
    {
      "citation_id": "24",
      "title": "Exploring disentangled feature representation beyond face identification",
      "authors": [
        "Yu Liu",
        "Fangyin Wei",
        "Jing Shao",
        "Lu Sheng",
        "Junjie Yan",
        "Xiaogang Wang"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "25",
      "title": "Unsupervised learning of disentangled representations from video",
      "authors": [
        "Emily L Denton"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "26",
      "title": "Unsupervised speech decomposition via triple information bottleneck",
      "authors": [
        "Kaizhi Qian",
        "Yang Zhang",
        "Shiyu Chang",
        "Mark Hasegawa-Johnson",
        "David Cox"
      ],
      "year": "2020",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "27",
      "title": "Autovc: Zero-shot voice style transfer with only autoencoder loss",
      "authors": [
        "Kaizhi Qian",
        "Yang Zhang",
        "Shiyu Chang",
        "Xuesong Yang",
        "Mark Hasegawa-Johnson"
      ],
      "year": "2019",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "28",
      "title": "Towards end-to-end prosody transfer for expressive speech synthesis with tacotron",
      "authors": [
        "Eric Skerry-Ryan",
        "Ying Battenberg",
        "Yuxuan Xiao",
        "Daisy Wang",
        "Joel Stanton",
        "Ron Shor",
        "Rob Weiss",
        "Rif Clark",
        "Saurous"
      ],
      "year": "2018",
      "venue": "Proeedings of ICML"
    },
    {
      "citation_id": "29",
      "title": "Disentangling style factors from speaker representations",
      "authors": [
        "Jennifer Williams",
        "Simon King"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "30",
      "title": "Speaker-invariant affective representation learning via adversarial training",
      "authors": [
        "Haoqi Li",
        "Ming Tu",
        "Jing Huang",
        "Shrikanth Narayanan",
        "Panayiotis Georgiou"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "31",
      "title": "Generalized end-to-end loss for speaker verification",
      "authors": [
        "Li Wan",
        "Quan Wang",
        "Alan Papir",
        "Ignacio Moreno"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "32",
      "title": "Librispeech: an asr corpus based on public domain audio books",
      "authors": [
        "Vassil Panayotov",
        "Guoguo Chen",
        "Daniel Povey",
        "Sanjeev Khudanpur"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "33",
      "title": "Voxceleb: a large-scale speaker identification dataset",
      "authors": [
        "A Nagrani",
        "J Chung",
        "A Zisserman"
      ],
      "year": "2017",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "34",
      "title": "Voxceleb2: Deep speaker recognition",
      "authors": [
        "J Chung",
        "A Nagrani",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "35",
      "title": "Fusion approaches for emotion recognition from speech using acoustic and text-based features",
      "authors": [
        "Leonardo Pepino",
        "Pablo Riera",
        "Luciana Ferrer",
        "Agustín Gravano"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "36",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "37",
      "title": "A novel attention-based gated recurrent unit and its efficacy in speech emotion recognition",
      "authors": [
        "Srividya Tirunellai Rajamani",
        "Adria Kumar T Rajamani",
        "Shuo Mallol-Ragolta",
        "Björn Liu",
        "Schuller"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "38",
      "title": "Transformer based unsupervised pre-training for acoustic representation learning",
      "authors": [
        "Ruixiong Zhang",
        "Haiwei Wu",
        "Wubo Li",
        "Dongwei Jiang",
        "Wei Zou",
        "Xiangang Li"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "39",
      "title": "Advancing multiple instance learning with attention modeling for categorical speech emotion recognition",
      "authors": [
        "Shuiyang Mao",
        "C-C Jay Ching",
        "Tan Kuo",
        "Lee"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech 2020"
    },
    {
      "citation_id": "40",
      "title": "Emotion recognition by fusing time synchronous and time asynchronous representations",
      "authors": [
        "Wen Wu",
        "Chao Zhang",
        "Philip Woodland"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    }
  ]
}