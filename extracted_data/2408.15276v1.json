{
  "paper_id": "2408.15276v1",
  "title": "A Survey Of Deep Learning For Group-Level Emotion Recognition",
  "published": "2024-08-13T11:54:09Z",
  "authors": [
    "Xiaohua Huang",
    "Jinke Xu",
    "Wenming Zheng",
    "Qirong Mao",
    "Abhinav Dhall"
  ],
  "keywords": [
    "Group-level Emotion Recognition",
    "Deep Learning",
    "Feature representation learning",
    "Attention mechanism",
    "Fusion scheme"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "With the advancement of artificial intelligence (AI) technology, group-level emotion recognition (GER) has emerged as an important area in analyzing human behavior. Early GER methods are primarily relied on handcrafted features. However, with the proliferation of Deep Learning (DL) techniques and their remarkable success in diverse tasks, neural networks have garnered increasing interest in GER. Unlike individual's emotion, group emotions exhibit diversity and dynamics. Presently, several DL approaches have been proposed to effectively leverage the rich information inherent in group-level image and enhance GER performance significantly. In this survey, we present a comprehensive review of DL techniques applied to GER, proposing a new taxonomy for the field cover all aspects of GER based on DL. The survey overviews datasets, the deep GER pipeline, and performance comparisons of the state-of-the-art methods past decade. Moreover, it summarizes and discuss the fundamental approaches and advanced developments for each aspect. Furthermore, we identify outstanding challenges and suggest potential avenues for the design of robust GER systems. To the best of our knowledge, thus survey represents the first comprehensive review of deep GER methods, serving as a pivotal references for future GER research endeavors.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "E MOTIO exhibits a profound influence on human percep- tion, attention, memory, and decision-making, directly impacting both physical and mental well-being  [1] . Consequently, the comprehension and perception of emotions not only enhance interpersonal communication but also implicitly contribute to the regulation of human physical health. With the evolution of big data technology, the broadening of application scenarios, and the progress in artificial intelligence technology, group-level emotion recognition (GER) has become a focal point for researchers  [2, 3] . Unlike individuallevel emotion recognition, which analyzes the emotions of individuals through facial expressions, speech, and postures, GER focuses on identifying the collective emotions of a group of people. Existing GER technology integrates diverse information, encompassing facial expressions, postures, social interactions, etc., to predict the emotions of a group. Taking facial expression data as an example, GER typically involves three key steps: first, given an image, detecting and extracting information such as faces and postures from images or videos; secondly, employing handcrafted descriptor or neural networks to extract facial features and other relevant information; finally, inputting these features as sequential data into another model like recurrent neural network to classify the group-level emotion. However, GER presents three key challenges. Firstly, the groups and contexts involved in GER may exhibit diversity and complexity. Secondly, the labeling of group-level emotion demands more nuanced considerations for the emotions expressed by the primary group, introducing additional complexities compared to individual-level emotion labeling. Lastly, due to the involvement of multiple individuals and diverse contexts, the recognition process becomes more complex than individual-level emotion recognition. Despite these challenges, GER remains an essential and formidable research area. It would be utilized to analyze emotion changes of a group of people, detecting abnormal behaviors and potential dangers in a timely manner in surveillance videos, or understanding students' learning status in collaborative learning  [4] .\n\nThis article provides a comprehensive survey of deep learning approaches for GER. Different from existing review  [5] , this paper elaborates on the methods of GER with a unique focus on deep learning architecture, providing insights into the current state and technical challenges associated with deep learning method in GER. In the beginning,we commence by providing an in-depth analysis of groups and emotions from a social perspective, offering a concise concept for GER. Subsequently, we present a thorough description of the image-based and video-based group-level emotion databases currently available for GER. Moreover, we explore recently developed deep learning methods for GER and scrutinize their technical challenges. In conclusion, we discuss the development trends of GER, offering valuable insights and guidance for future research and applications.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Group-Level Emotion",
      "text": "Emotions, traditionally viewed as individual-level phenomena in common parlance, have increasingly received attention in the field of social psychology as being potentially grouplevel  [6] . Niedenthal and Brauer offer a broad definition of group-level emotions, characterizing them as emotions experienced by individuals on behalf of a group with which they identify  [7] . This perspective suggests that group-level emotions are inherently more complex than their individual-level counterparts. Adopting both sociological and psychological perspectives, we will proceed by elucidating the concepts of groups and emotions before providing a precise definition of group emotions.\n\nThe fundamental disparity between group-level emotion and individual-level emotion arises from the distinction between the concepts of a group and an individual. While an individual pertains to an singular, independent entity, a group transcends mere aggregation, constituting a social phenomenon that amalgamates individuals into a collective entity. Various perspectives and definitions of the concept of a group have been proposed in scholarly literature  [8] [9] [10] [11] . Shaw et al.  [8]  defines a group as comprising two or more individuals engaged in mutual interaction and influence, emphasizing the significance of interaction among group members. Szilagi and Wallance  [9]  characterize a group as a collection of two or more individuals who interact and depend on each other to achieve a common goal. Additionally, Barron et al.  [10]  further elaborate on this definition by conceptualizing a group as individuals connected by some kind of bond, exhibiting varying degrees of cohesion. Moreover, Dasgupta et al.  [11]  describe a group as a group of people closely interconnected in some manner. According to these studies, a group consists of multiple individuals with a shared objective and direct or indirect interactions between them during a certain period of time. Thus, only when these conditions are met can a combination of multiple individuals be deemed a group.\n\nNumerous scholars have investigated the concept of emotion within the sociological and psychological field. Schachter et al.  [12]  proposed that emotion encompasses both a physiological arousal state and a cognitive state adapted to this physiological state. Kleinginna et al.  [13]  conducted a comprehensive review categorizing 92 different definitions of emotion from various literature sources. Ekman in  [14]  suggested that \"emotions evolve from the adaptive value of human beings in handling basic life tasks\", indicating that emotions arise as adaptive responses to different situations encountered during social activities. Averill regards emotion as a complex of impulsive motivation in higher cognition, involving various psychological processes and physiological responses  [15] . Cabanac defines emotion as any psychological experience with high intensity and high pleasure content, emphasizing the relationship between emotion and psychological experience  [16] . Barrett posits that emotions are constructs of the brain's interpretation of external and internal stimuli  [17] . Scherer views emotion as a biopsychological phenomenon resulting from the interaction of specific neural systems and physiological responses, cognitive assessments, and social-cultural factors  [18] . Therefore, emotions can be understood as physiological and psychological responses to stimuli in our environment.\n\nBased on the foregoing elucidation of group and emotion, group-level emotion denotes the physiological and psychological responses elicited by multiple interacting individuals over a defined period. Scholars have proffered diverse definitions of group-level emotion in their studies. In the beginning, Hatfield et al.  [19]  characterize group-level emotion as the process whereby group members reciprocally influence each other through emotional contagion and empathy, culminating in emotional synchronization and consistency. Essentially, group-level emotion entails the reciprocal transmission and influence of emotions among individuals, leading to emotional consistency. Furthermore, group emotion is perceived as the emotional state propagated among members of a social group, cultivated and diffused through social interaction and shared experiences  [20] . Bars채de and Gibson stressed the importance for researchers in the social science community to approach group-level emotions from both a \"top-down approach\" and a \"bottom-up approach\"  [21] . A \"top-down approach\" suggests that emotion exhibited by group is represented at the group level and is felt by individual members, while the \"bottomup approach\" highlights the unique compositional effects of individual-level group member emotions. According to the framework  [21] , Kelly and Bars채de further proposed that group-level emotion comprises affective compositional effects and affective context  [22] . In essence, group-level emotion emerges from a combination of individual-level affective factors posed by group members and group-level factors that shape the emotional experience of the group. Additionally, Bars채de and Gibson further explore group-level emotion as the emotional state disseminated and shared among members of an organization  [23] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Group-Level Emotion Dataset",
      "text": "The rise of social media platforms has led to a surge in the volume of uploaded photos and videos, driving advancements in big data technologies and affective computing domains, especially GER. In recent years, numerous group-level emotion datasets are established, attracting attention from researchers in the domains of affective computing and computer vision. However, the quality of annotation on images and videos plays an critical role in determining the efficacy of GER models. Thus, this section aims to provide an exhaustive examination of existing group-level emotion datasets, classifying them into two main types: image-based and video-based types.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Image-Based Datasets",
      "text": "In contrast to individual-level emotion datasets, group-level emotion datasets necessitate annotation for a group. Imagebased datasets began to emerge since 2013, with notable representations including MultiEmoVA  [24] , Happiness Image database (HAPPEI)  [25] , Group-level Affect database (GAF)  [2, 26, 27] , Group cohesion dataset  [28] , GroupE-moW  [29] , and SiteGroEmo  [3] . Among these database, HAPPEI, GAF, and the Group cohesion dataset, which were utilized in the EmotiW sub-challenge, have received significant attention and adoption by researchers in the fields of computer vision and affective computing.\n\nMou et al.  [24]  introduced a multi-dimensional group emotion image dataset, namely MultiEmoVA. This dataset was primarily constructed by scouring various social media platforms for real-life photos. Initially, 400 color images were collected, and after manually filtering images with ambiguous emotional expressions, 250 images meeting the criteria were remained. Furthermore, these images were categorized into positive, neutral, and negative. Additionally, Mou et al. also incorporated arousal-level annotation into the dataset, providing more explicit representations of the intensity for each emotion category.\n\nThe HAPPEI database contains images captured from social media platform and categorizes group-level emotions into neutral, small smile, big smile, smile, big laugh, and thrilled, totaling 2,638 images. In contrast, the GAF datbase delineates three emotion categories along the valence dimension: positive, neutral, and negative. The data collection process involved web search using keywords corresponding to various scenarios, such as weddings, birthday parties, and sports events, etc., to obtain images depicting group-level emotions in those contexts. Subsequently, these images were annotated by 2 to 3 experts in affective computing domain. The GAF database has undergone three iterations, namely GAF  [26] , GAF2.0  [2] , and GAF3.0  [27] . The initial version, GAF includes 504 images  [26] , which is relatively small in scale and has been limited usage by researcher for evaluating the performance of deep GER models. However, the subsequent iterations, GAF2.0 and GAF3.0, witnessed a substantial increase in data size, featuring 6,467  [2]  and 17,172 images  [27] , respectively.\n\nIn addition to emotion category, the cohesiveness of a group serves as a crucial indicator of the emotional state, structure and success of a group of individuals. Group Cohesion database was derived from GAF3.0 by adding cohesion labels  [28] . Each image in this database was annotated with a cohesion score ranging from 0 to 3 by five annotators, where 0 represents no cohesion and 3 means strong cohesion. This database was utilized in the Emotiw2019 challenge  [30] , with 9,300 images allocated for training, 4,244 images for validation and 2,899 images for testing purposes.\n\nBesides the above-mentioned databases, another databases have recently proposed by Guo et al.  [29]  and Wang et al.  [3] , that is GroupEmoW and SiteGroEmo. GroupEmoW was created with a stringent criterion mandating that each image contains 2 to 9 individuals engaged in a specific activity, thereby forming distinct groups. According to this criterion, they collected 15,894 images from the internet, creating a diverse dataset with varying image resolutions and in-the-wild. Subsequently, these images were categorized into positive, neutral, and negative. On the other hand, the SiteGroEmo dataset diverges from existing database by capturing image across tourism scenes worldwide. This dataset not only contains rich geographic information and scene variations but also randomly captures the facial and body movements of individuals at a specific moment. Comprising a total of 10,034 images, the dataset is labeled with valence to denote emotions, specifically categorized as negative, neutral, and positive.\n\nIt is important to note that the aforementioned image-based datasets predominantly rely on internet keywords searches for data collection. While this method may expedite the establishment of satisfactory datasets, the existing datasets, with the exception of the MultiEmoVA database, solely employ valence-level annotations for images, lacking arousallevel annotations and more specific emotion categories such as anger and surprise. This limitation may arise from the diverse expression exhibited by participants in a group, making it challenging to annotate group-level images with a comprehensive emotion taxonomy. Additionally, manual annotation may introduce discrepancies in labeling due to cultural differences. Moreover, many images in these datasets may suffer from quality issues such as poor lighting, incomplete capture of facial expressions, or obstructions obscuring certain individuals. Finally, given their static nature, these images lack information about emotional dynamics. In the domain of emotion recognition research, scholars have emphasized that dynamic changes in facial expressions can offer crucial clues for both humans and computers to discern emotions or emotional processes  [31] . Therefore, these uncontrollable circumstances and the absence of dynamic information may have a discernible impact on the accuracy of GER based on image.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Video-Based Datasets",
      "text": "In contrast to image-based datasets, video-based datasets not only capture the temporal dynamics of emotional expressions but also provide additional contextual information, facilitating a more comprehensive and accurate portrayal of changes in emotional states. However, due to the demanding collection and annotation process, only two video-based datasets have emerged recently since 2019: the VGAF dataset introduced by Sharma et al.  [32]  and the GECV dataset by Quach et al.  [33] .\n\nThe VAGF dataset comprises videos sourced from the YouTube platform, each featuring a varying number of individuals forming groups of different sizes. The dataset is partitioned into training, validation, and test sets, encompassing 2,661, 766, and 756 samples, respectively. Alongside valence annotation, the dataset includes cohesion metrics among individuals in the group. The corresponding dataset has also been used in EmotiW2023  [34] . Conversely, the GECV dataset contains videos captured in leisure and crowded scenes, amounting to a total of 627 videos. This dataset is further subdivided into three subsets: GEVC-SingleImg, GEVC-GroupImg, and GEVC-GroupVid. The latter two subsets are tailored to capture group emotions more effectively by showcasing various various group behaviors across different scenarios.\n\nWhile video-based databases offer richer semantic and contextual information compared to image-based databases, facilitating more discriminant criteria for data annotation, they are sourced from media platforms featuring complex scenes that often depict real-life scenarios. Nonetheless, they present limitations such as the absence of physiological signals akin to multi-modal emotion recognition, thereby posing challenges in data collection.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Dataset Summary",
      "text": "The specific comparisons of the existing group emotion datasets are presented in Table  I . Despite all datasets collecting images or videos from diverse real-world scenarios, their sample size is comparably small when compare with comprehensive datasets like AffectNet  [35] . This limited data size impedes the robust learning of group-level features. Recently, in the domain of micro-expression recognition, a composite dataset consolidating various micro-expression databases has become popular  [36] . This approach corroborates the generalization capacity of the method across datasets with disparate characteristics, mitigating the issue of data scarcity. Such a strategy holds promise for GER by potentially augmenting data volumes, particularly for video-based datasets, and enhancing the generalization ability of GER methods.\n\nState-of-the-art approaches, particularly those showcased in the EmotiW challenge, primarily undergo evaluation using image-based databases. All databases adopt an annotation strategy categorizing images into three valence-level categories, as certain nuanced emotions such as fear and contempt pose challenges in data collection, resulting in limited samples for these categories, which are insufficient for robust learning. As seen from Table  I , except SiteGroEmo, each database has a balanced distribution of data across each class. Furthermore, in practical experiments, apart from the HAPPEI and Multi-EmoVA databases, other databases follow an official protocol where the train, validation, and test sets remain strictly fixed without random validation. Such rigid dataset partitioning may not be facilitate accurate assessments of GER method performance.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Iv. Input Modality",
      "text": "Recognizing group-level emotions poses significant challenges due to the diversity in group dynamic, individual emotion expressions, and limited data availability, deep learning (DL) methods, while promising, exhibit varied performance depending on the various modalities utilized. In this section, we describe the diverse information cues based on static image or video sequence utilized for GER and outline their respectively strengths and limitations.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Static Image",
      "text": "Due to the abundant availability of facial images online such as AffectNet database  [35] , numerous existing studies in FER are conducted on static images. Leveraging the efficiency demonstrated by FER with static images, numerous researchers have extended their efforts to GER, incorporating various additional cues such as face, scene, pose, even objects. Convolutional Neural Networks (CNNs), notably VGG, ResNet and their variants, are commonly employed to analyze static images in GER research.\n\nCue aggregation as input. Given the flexible nature of group sizes, GER faces the challenge of effectively aggregating features from multiple individuals within a group to derive an overall emotional state. Existing methods are broadly categorized into two approaches. The first category involves averaging or weighted sum all individuals' emotion scores  [39] [40] [41] [42] , which are typically outputted by a classifier such as Support Vector Machine (SVM). For example, Rassadin et al.  [39]  normalized detected faces and fed them into VGGFace and ImageNet. They constructed an ensemble of four Random Forest classifier trained on the features outputted by VGGFace, ImageNet, and landmark features. Finally, they employed a weighted sum approach to fuse the score outputted by the random forest classifiers. The second approach employs some machine learning algorithms such as bag-of-words and clustering to aggregate all individuals' features into a single feature vector. Balaji et al.  [43] , for example, utilized CNNs to extract face information. Subsequently, Fisher vector and VLAD encoding techniques were applied to compress all individual features in the image, yielding bottom-up features capable of representing group emotions. This method effectively compresses features, reduces computational complexity.\n\nMultimodality as input. Group are often considered as \"emotional entities and a rich source of varied manifestations of affect\". Earlier discussions by Bars채de and Gibson in  [23]  emphasized the necessity for GER researchers to adopt both a \"top-down approach\" and a \"bottom-up approach\". Consequently, GER research has concentrated on integrating both bottom-up and top-down components. Typically, bottom-up refers to individual emotions, while top-down encompasses contextual factors such as background information in an image. Recognizing the benefits of incorporating both bottom-up and top-down components, several studies  [24, [43] [44] [45] [46] [47] [48] [49] [50]  have explored to fuse features from various cues in group-level images. For example, in Garg's work  [46] , a deep convolutional neural network is utilized to identify facial expressions within an image, while a Bayesian network leverages scene descriptors to extract visual features of the image content, thereby inferring the overall emotion of the image. In addition to these modalities, body information  [24, 51]  and object  [49, 50]  have also been incorporated into certain studies. In summary, GER research has commonly explored one or more cues extracted from face, pose/skeleton information, object, and scene context. Cues combination offers the advantage of enabling successful recognition of group-level emotions in the challenging environment when one cue is lacking. However, even employing multiple cues, a key concern arises regarding how to effectively establish connection among these cues to enhance the robustness of GER in real-world scenarios.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "B. Dynamic Image Sequence",
      "text": "As emotions are temporal in nature, such automatic tools in environments like factories, companies, and offices can help identifying interventions to maintain a healthy work culture. Facial expressions, being dynamic cues, evolve and change, revealing their effective signals over time. The visual information captured in videos plays a pivotal role in discerning the emotions depicted within them  [52] . The temporal variations across video frames provides additional information to be exploited, albeit encoding these variations introduces complexity to emotion recognition. Training a network to comprehend the overall affect of a group of people shown across frames poses challenges. In this subsection, we delineate various dynamic inputs.\n\nTemporal information as input. Temporal information encapsulate the dynamics of an entire video sequence in a single instance. The temporal information, modeled by using algorithms like LSTM, has been successfully employed in GER to model scene dynamics and appearance in a video  [32] . Similarly, active image encapsulated spatial and temporal information from video sequences into a single instance by estimating and accumulating changes in each pixel component. Sun et al.  [53]  utilized temporal segment networks to extract RGB information, optical flow frame, and warped optical flow frame for each video, incorporating a temporal shift module to model dynamic scene information. Quach et al.  [33]  introduced a fusion mechanism called Non Volume Preserving Fusion (NVPF) to better model spatial relationships between facial emotions in each frame, effectively addressing emotional ambiguity caused by insufficient facial resolution or undetectable emotions.\n\nFrame aggregation as input. Dynamic image sequence collected in the wild often include complex scene background. Petrova et al.  [54]  designed a method based on VGG-19 framework for each frame, capturing global emotions, followed by using score averaging and accumulation across all frames. Li et al.  [55]  proposed leveraging multi-task learning theory to aggregate frame features, while Liu et al.  [56]  employed four aggregation methods including maximum, minimum, average, and standard deviation to consolidate all individual's face feature in a group image.\n\nMultimodality as input. In analyzing group affect, audio features play a crucial role alongside facial image, as relying solely on facial expressions may lead to inaccuracies in estimating overall group affect. Pitch, speech rate, and duration, etc. have been found relevant to affect analysis. In group settings, these features are vital for distinguishing between situations like arguments and discussions, where the visual model may falter. Visual-audio fusion models have been proposed to enhance visual-based models  [32, [55] [56] [57] [58] . For example, Wang et al.  [57]  introduced a network called Kinjection audiovisual network, which employs a multi-head cross-attention mechanism to jointly model audio and video data, integrating previous emotion knowledge to improve the model's generalization ability. Recent study indicate that human gesture can convery emotion  [59] . Several researchers have incorporated human gesture features fused with scene and face cues for video-level GER  [53, 56] . For example, Sun et al.  [53]  utilized CenterNet for human detection and pose estimation, followed by ResNetSt for extracting body feature. For prediction, the average probability of each frame's prediction was calculated, yielding the video's class prediction.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Discussion",
      "text": "The primary challenge faced by these methods lies in establishing relationships between modalities and effectively fuse them. As evident from the discussed methods, GER has increasingly emphasized mining the dynamic information present in videos. This trend is expected to drive further advancements in Recurrent Neural Networks (RNN) and its derivatives in group-level emotion recognition. Additionally, these studies have ventured beyong single-modal information, paving the way for research to better comprehend and leverage multimodal information. This broader perspective holds promise for enrich the understanding and utilization of diverse sources of information in group-level emotion recognition tasks.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "V. Deep Networks For Ger",
      "text": "Since the inception of artificial neurons in the 1940s, deep learning has been undergone extensive exploration and implementation. Evolving from single-layer perceptrons to multilayer neural networks, Convolutional neural networks (CNNs), Recurrent neural networks (RNNs), Cascade networks, Graph convolutional networks (GCNs), attention mechanisms, and beyond, deep learning has witnessed rapid evolution. So far, various deep learning approaches have emerged to discern the collective emotions of a group of people, leveraging diverse information such as facial expression, gestures, social interactions, and more. Figure  1  illustrates literature spanning database, emotion competition, method, and survey categories of academic papers employing deep learning based methods for GER over the past decade. It is noteworthy that the publication trend has exhibited a noticeable increase, especially attributed to the EmotiW competition. In this section, we delve into the approaches from the perspectives of specialized blocks, network architecture, fusion stage and scheme, and loss function.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Basic Network Block",
      "text": "Before describing the network architecture, Figure  2  first introduce basic network block widely used in GER, including CNN, RNN, GCN.\n\nCNN. Due to the limitations of traditional machine learning in addressing complex environments, many researchers have explored more in-depth research methods. LeCun et al.  [60]  first proposed a convolutional neural network model based on the backpropagation algorithm. However, due to hardware limitations at the time, the research progress of CNN was relatively slow. It was not until 2012 when Krizhevsky et al.  [61]  proposed the AlexNet CNN model in the ImageNet Large Scale Visual Recognition Challenge, which significantly surpassed traditional machine learning methods in accuracy and promoted the development of deep learning in the field of computer vision. Since then, various models based on CNN have been proposed, such as VGGNet, GoogLeNet, ResNet, DenseNet, and MobileNet. Meanwhile, CNN models have also shown their superiority in GER.\n\nRNN. In Convolutional Neural Networks (CNNs), each input and output are independent of each other, but this ignores the relationship between them. Although CNNs can extract good features when processing image datasets, they are not ideal for datasets with time series data, such as speech, audio, and video. Rumelhart et al.  [62]  proposed a method of Recurrent Neural Networks (RNN) that learns and trains through the backpropagation algorithm, and applied it to process data with time series. RNN has the characteristic of introducing a recurrent structure, allowing the network to remember and utilize previous information, thereby extracting better time series features. Various improved network architectures based on RNN have also been widely used, such as Simple Recurrent Neural Network (SRNN), Bidirectional Recurrent Neural Network (BRNN), Long Short-Term Memory Network (LSTM), and Gated Recurrent Unit (GRU), which have achieved good results based on RNN. Normally, RNN is always combined with CNN, leading to the cascade network for GER, as shown in Figure  2c .\n\nGCN. Graph data has a wide presence in the real world, such as social networks, biological networks, recommendation networks, and chemical molecules. However, previous deep learning models based on CNN and RNN mainly deal with vector and matrix data, which neglects the topological structure of graphs and the relationships between nodes, leading to possible information loss and performance degradation. In order to solve this problem, Scarselli et al.  [63]  first proposed a new neural network model, namely, the graph neural network model, which extends existing neural network methods to handle data represented in the graph domain. Recent studies demonstrate that the effectiveness of graph convolutional networks (GCNs) in modeling semantic relationships, making them valuable for facial expression recognition (FER) tasks  [64] , as depicted in Figure  2d . With the success of GCNs in FER,  [65]  introduced GCNs for GER, aiming to enhance performance by capturing inter-individual relationships.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "B. Network Architecture",
      "text": "The efficacy of FER neural units depends on how multiple networks are integrated. GER methods typically adopt one of five network architectures: single-stream, multi-stream, cascade, graph convolutional network, and attention mechanism. In this section, we delve into the specifics of each architecture.\n\n1) Single-stream networks: Typical deep GER methods adopt single CNN with individual input. In single-stream 2D CNNs, the primary input is facial images, while single-stream 3D CNNs directly extract spatial and temporal features from video sequences. Many studies  [39, 40, 66]  employ transfer learning strategy on deep networks pretrained on large-scale face datasets to mitigate the overfitting issues. For example, Rassadin et al.  [39]  employ a pre-trained VGGFace model on detected faces, followed by a weighted sum to obtain the final result using a random forest classifier. Similarly, Lu et al.  [40]  utilize a VGG model pretrained on the VGGFace dataset to extract facial features for GER.\n\nIn addition to transfer learning methods, several works design cascade network  [67, 68]  or kernel methods  [69]  on single-stream shallow CNNs. Sun et al.  [67]  explore various handcrafted feature (LBP), AlexNet, Reduced AlexNet and ResNet, followed by group-expression model or LSTM for group-level happiness intensity estimation. Building on this, Wei et al.  [68]  extend the group-level intensity estimation with VGGFace pretrained VGG-Face dataset. Furthermore, GER with ResNet18, ResNet34, MobileNet, DenseNet, Resnet50, Inception, GoogleNet, and VGG19 pretrained on Imagenet for scene-level information is explored in  [41, 54] . The results demonstrate that VGG surpasses other architectures in GER and excels at distinguishing the complex hidden information in data.\n\nWhile the aforementioned works are based on 2D CNN with image input, several works employ 3D CNN variants  [53, 53, 58]  or cascade network  [70]  to directly extract spatial and temporal features from video sequences. Inflated ResNet-3D  [58] , Temporal shift module (TSM)  [53] , and Temporal Binding Network (TBN)  [53]  are introduced in GER. Additionally, a end-to-end cross-attention cascade network  [70]  combined the idea of ClipBERT  [71]  modules with the temporal sequence to enhance the representation in spatial and temporal dimensions.\n\n2) Multi-stream Networks: Single-stream model represent a basic structure in GER, extracting features solely from a single viewpoint such as the face or scene within group-level image. However, since group-level images encompass diverse and rich information, a single view may not provide sufficient insight. As we discussed in Section IV, employing various inputs from different perspectives can effectively explore spatial and temporal information. Hence, multi-stream networks have been adopted in GER to extract features through multiple inputs. Generally, multi-stream networks can be categorized into networks with two inputs, more than two inputs, and handcrafted features.\n\nMulti-stream networks with two inputs. According to  [72] , the face plays a crucial role in expressing the emotion. Therefore, among multi-stream networks with local and global inputs, the face remains a primary input. Several studies incorporated the face as local information and the scene as global information. They combined a convolution neural network for face while another neural network on scene descriptors for GER  [42, 44, 45, 66, 73] . Experiment results of these studies demonstrate that models based on face outperforms those on other methods and other modality is still competitive and useful to GER. Moreover, the multi-stream networks have shown improvement in GER. Furthermore, Zhang et al.  [74]  presented a semi-supervised group-level emotion recognition (SSGER) framework based on contrastive learning, learning efficient features from both labeled and unlabeled images, where face images and scene images are utilized. In addition to visual features, audio feature are also being considered in dynamic GER. Augusma et al.  [75]  proposed branches for both video and audio, with cross-attention between modalities for GER. In their work, the video branch is based on a finetuned ViT architecture, while the audio branch extracts Melspectrograms and feeds them through CNN blocks into a transformer encoder.\n\nMulti-stream networks with more than two inputs. As mentioned in the previous section, bottom-up components involve individual emotions, while top-down components consider contextual factors such as the background of an image  [23] . Therefore, to enhance group feature representation, some works  [48, 51, 53, 56, 65, [76] [77] [78] [79]  have investigated combinations of more than one top-down component and bottom-up components. Guo et al.  [51]  designed a hybrid network that incorporates scene features, skeleton features, and local facial features with deep convolutional neural networks. Additionally, they  [76]  further used visual attention attention mechanism to fuse face, scene, skeletons, and salient regions. Different from the aforementioned studies, Fujii et al.  [48, 78]  proposed a two-stage architecture for GER. The first stage performs binary classification based on facial expression to distinguish \"Positive\" labels, including discriminative facial expressions from others. For second stage, they considered exploiting object-wise semantic information and scene background for the second classification. Recently, several researchers  [53, 56]  investigated multi-stream network for dynamic GER. Spatio-temporal features and static features were exploited by Sun et al.  [53] . The fusion of several spatiotemporal modality adopted RGB, RGB difference, optical flow, warped optical flow, and audio, while image-level CNNs were designed based on face and body images. Moreover, a hybrid network fusing audio stream, facial emotion stream, environmental object statistics stream (EOS), and video stream are designed with temporal shift module and SVM for GER  [56] .\n\nMulti-stream networks with handcafted features. According to the analysis, the facial emotion or movements of grouplevel emotion are highly related to face textures, while scene information contains more abundant information for GER, the handcrafted features for low-level representation also plays an important role in GER. Multiple works  [43, 68]  combined deep features for face-level and handcrafted features for scene-level to leverage the low-level and high-level information for robust GER.\n\n3) Cascade network: For GER, dealing with varying numbers of face between group-level images poses a significant challenge. As discussed in Section IV-A, current cascade networks can be categorized to two types. The first category utilizes variants of CNN such as ResNet, VGG, followed by decision-level score fusion. In contrast, the second category combines various CNN and RNNs to address the inconsistency in the number of faces between two group-level image. In this section, we will discuss the details of the second cate-gory  [67, 70, 76, 80, 81] .\n\nIn GER, a critical issue is how to effectively model the variability in the number of faces. LSTM is a primary method. Sun et al.  [67]  were the first to investigate the combination of CNNs and LSTM. They explored the use of AlexNet, Reduced AlexNet, and ResNet to extract the individual faces in a group. Subsequently, a weighted LSTM was used to assign different weights to each facial feature based on factors such as the size of the face and the distance between them. Furthermore, visual attention mechanisms were incorporated in LSTM for GER  [70, 76] . In  [70] , a cascade network containing CNN and LSTM was employed to extract image-level and audiolevel features, respectively. An attention mechanism was then introduced to compute important features at each time step. Additionally, Li et al.  [82]  employed a similar architecture at the face-level for GER. However, in contrast to  [67] , skeletons and scene features were directly fused at the end. Moreover, skeletons information extracted by OpenPose toolkit was considered in cascade network proposed by Slogrove et al.  [80] . In their approach, the coordinates and confidence information of all individual key points were fed into an LSTM as a sequence for modeling, resulting in a group-level emotion classification. Additionally, speech signals were investigated for GER in  [81] . They proposed a method incorporating a cascade network with multi-task learning for GER, using deep spectral features on speech signal. The cascade network based on CNN and RNN, was designed to extract discriminative deep spectral features, while multi-task learning combines emotion recognition and speaker identification tasks during the model training process.\n\n4) GCN based network: Individuals within a group often exhibit diverse social relationships with others. To highlight this social aspect, several works  [3, 29]  have utilized graph convolutional networks (GCNs) to model both visual features and social context within group-level images. In these approaches, the emotional states of individuals are treated as node features, while the interactions between individuals are represented as graph edges, thus forming a graph structure. Guo et al.  [29]  introduced a group-level emotion recognition method based on four cues, where faces, bodies, objects, and the entire image are transformed into a graph structure. This graph represents the relationships within the group based on these four cues, facilitating group-level emotion recognition. Additionally, Wang et al.  [3]  proposed a context-consistent cross-graph neural network to mitigate emotional biases resulting from different cues in multi-cue emotion recognition.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "5) Attention Based Network:",
      "text": "In order to prioritize important character or object features that play a pivotal role in group emotions, five studies have incorporated attention mechanisms. Gupta et al.  [83]  detect local facial emotions by employing an attention mechanism to concentrate on more pertinent local information on the face, generating probability attention weights through the Softmax function. The weighted sum of facial features is then calculated based on these attention weights to produce a single facial feature vector representation. Additionall, Guo et al.  [76]  and Khan et al.  [79]  also integrated attention mechanisms. They introduced a novel region attention network (RAN) to detect and extract crucial features in facial regions. The region attention mechanism comprises an attention generator and an attention applier, where the former generates adaptive region attention weights to emphasize important facial features in different regions, while the latter applies these generated region attention weights to the output of the feature extractor to enhance the distinction of facial features. Furthermore, Wang et al.  [84]  proposed a cascaded attention network, which leverages the importance of each face in the image to generate a global representation based on all faces, effectively focusing on the feature information of the most important face.\n\nMoreover, Transformer-based architectures have found extensive applications in NLP  [85]  and computer vision  [86]  tasks. Inspired by the significant success of Transformer architecture in various tasks, two recent studies  [75, 87]  have explored the application of transformers to the GER task. Augusma et al.  [75]  initially utilized the visual Transformer mechanism and the BERT framework to extract features from global images and speech, respectively. They employed a cross-attention mechanism to learn the weights of the two modes and subsequently performed weight fusion. Moreover, a dual-branch cross-patch attention Transformer (DCAT) was proposed to incorporate the psychological concept of the Most Important Person (MIP) and the global image.\n\nIn summary, GER network architectures can be broadly categorized into single-stream, multi-stream, cascade networks, GNN-based networks, and attention-based networks. While single-stream serves as the fundamental model, it only considers a single view of the group-level image. To leverage more information, multi-stream networks learn features from multiple perspectives for robust GER. Additionally, as the group size fluctuates, cascade networks sequentially incorporate various modules like RNNs and LSTMs to construct an end-to-end GER network. GNN effectively models interactions between individuals based on social relationships. Conversely, the attention mechanism, inspired by the psychological concept of the most important person, focuses on extracting key features from all individuals. In the future, combining more effective modules in multi-stream, cascade, and attentionbased approaches could further enhance GER performance.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "C. Fusion Stage And Scheme",
      "text": "In GER, the fusion stage plays a crucial role in integrating information from multiple cues to improve the accuracy of emotion recognition. It encompass various methods for combining features extracted from different modalities, such as facial expressions, scene, skeleton etc. One common fusion approach is score-level, as used in studies like  [47, 49, 76] . In their approaches, the output scores from individual modalities are combined using techniques like averaging or mean voting.\n\nAlternatively, feature-level fusion used in studies like those by  [33, 48, 78] , integrates raw feature representations extracted from each modality before feeding them into a classifier. This approach allows the model to learn more complex relationships between different modalities but may be more computationally intensive. Besides score-level and feature-level fusion, kernelbased  [69, 88]  and loss function-based  [50, 74]  fusion are two alternative ways to fuse multi-modality. For example,  [74]  proposed a weight cross-entropy loss function on Scene-Face network by combining face and scene information.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "D. Loss Function",
      "text": "Different from classical methods, where the feature extraction and classification are independent, deep networks can perform end-to-end classification through loss functions by penalizing the deviation between predicted and true labels during training. Most GER works directly apply the commonly used softmax cross-entropy loss  [39] . The softmax loss is typically effective at correctly classifying known categories. However, in practical classification tasks, the classification of unknown samples is also essential. Therefore, to achieve better generalization ability, it is crucial to further enhance interclass difference and reduce intra-class variation, especailly for data scarcity. Metric learning techniques, such as contrastive loss  [89] , have been developed to ensure intra-class compactness and inter-class separability by measuring the relative distances between inputs. Wang et al.  [90]  proposed a contrastive learning-based self-attentive network. In this approach, different features are embedded into a vector space, and the similarity and difference of features are learned by enhancing the similarity between samples of the same class and reducing the similarity between samples of different classes. Then, adaptive weight calculation and weighted average fusion are performed to adaptively fuse features at different levels. Although the above two methods have achieved good performance, they are still limited to static images. Additionally, metric learning loss often requires effective sample mining strategies for robust recognition performance. Metric learning alone may not suffice for learning a discriminative metric space for GER. To address these challenges, Zhang et al.  [74]  proposed a semi-supervised group-level emotion recognition framework based on contrastive learning to learn efficient features from both labeled and unlabeled images. To alleviate the uncertainty of given pseudo-labels, they introduce Weight Cross-Entropy Loss (WCE-Loss) to suppress the influence of samples with unreliable pseudo-labels in the training process.\n\nIn summary, although most current GER approaches utilize the standard softmax cross-entropy loss, only a limited number of studies have explored alternative loss functions such as contrastive learning loss or introduced novel loss functions to enhance inter-class separability, intra-class compactness, and achieve well-balanced learning. Looking ahead, investigating more effective loss functions targeting discriminative representations for group-level emotion features holds significant promise as a direction for future research in GER.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Vi. Experiments A. Performance Metric",
      "text": "The standard evaluation metric for GER typically involves using accuracy for group-level emotion recognition and mean square error for group happiness estimation. Accuracy assesses the proportion of correct predictions relative to the total number of evaluated samples, providing a measure of the model's overall performance in correctly identifying grouplevel emotion. Conversely, mean square error quantifies the average squared difference between the predicted and true happiness values, offering a measure of the model's accuracy in estimating the happiness levels of a group.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "B. Model Evaluation Protocols",
      "text": "Cross-validation stands as a widely utilized protocol for evaluating GER performance. This protocol involves dividing the dataset into train, validation, and test sets, ensuring fair verification of deep learning architectures on group emotion datasets. Cross-validation in the GER contains fixed partition validation and K-fold cross validation. The first kind of crossvalidation is commonly used as the official evaluation method in the competitions such as the Emotion Recognition in the wild challenge (EmotiW)  [27] . Here, the train, validation, and test sets are pre-determined and remain fixed throughout the competition, eliminating randomization. Participants receive the train and validation sets at the competition's outset for model development, while the test set is revealed later for final performance assessment and ranking. On the other hand, K-fold cross-validation protocol is also prevalent in GER research. This protocol involves randomly dividing the dataset into k equally sized parts, with each part serving as a test set in turn while the remaining portions constitute the training data. The process repeats k times, with each partition serving as the test set once. The choice for k, typically 4 or 5 in GER, can significantly impact evaluating time while ensuring robust performance assessment.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "C. Performance Analysis",
      "text": "Table  II  reports the performance of various deep learning models for GER as reported in EmotiW since 2016. With the exceptions of  [54, 56] , most methods proposed in these competition aim to fuse more than two cues such as face, scene, and skeleton, among others. Notably, established neural network architectures such as VGG have been commonly employed for extracting facial expression feature. Additionally, to accommodate the variable number of faces/objects, variations of RNN have been prevalent in many algorithms. Furthermore, since 2019, attention modules, facilitated by the successful application of transformer architectures, have gained widespread adoption in Emotion Recognition in the Wild competitions. Regarding performance metrics, in the HAPPEI dataset, the lowest reported RMSE is 0.822. In the GAF2.0 competition, the highest reported accuracy is 80.9%, while in GAF3.0, it is 68.08%. This suggests that GAF3.0 introduced more competitive samples and increased the challenge level. Additionally, a new track called Group Cohesion, derived from GAF3.0, was introduced to evaluate group cohesion. Despite improvements in team performance between EmotionW2016 and EmotiW2019, there is still significant room for enhancing overall performance. Since 2020, the utilization of video-based datasets has become prevalent, indicating a growing consideration for temporal information in algorithm development. Moreover, traditional feature extraction techniques have been gradually supplanted by deep learning methods in recent years.\n\nBeyond competition, many researchers have also explored GER using group-level emotion databases like HAPPEI and GAF. Tables III and IV offer detailed comparison of method or model accuracies proposed by researchers over the last decade, excluding those participating in EmotionW competition. Notably, except for  [33, 69, 88, 91] , all methods fused more than two cues. Among these, face, pose/skeleton, and object are regarded as local component, while scene information serves as global information. This approach aligns with the concept of bottom-up and top-down components mentioned in group emotion theory. Furthermore, widely recognized network blocks such as VGG, Xception, ResNet and AlexNet have found extensive use in GER due to their promising performance in image and face recognition tasks. Additionally, the fusion of various networks enables better exploitation of complementary information between them. Common fusion schemes include Average and Feature concatenation, which provide straightforward solutions to the fusion problem. However, some researchers have proposed novel approaches to fuse multi-modality features. For example, Guo et al.  [29]  introduced a graph convolutional network to facilitate information exchange among features extracted from different models. Zhu et al.  [50]  proposed a uncertain-aware learning to extract more robust representation from face, object, and scene modalities for GER. Moreover, unlike competitions, recently proposed GER methods have been evaluated across various databases, such as GroupEmoW, GAF, and GECV-GroupImg to access their generalization ability. With the introduction of databases like VGAF and GECV-GroupVid, researchers have started exploring the spatiotemporal GER.\n\nIn general, modality fusion can yield promising results across all datasets. Different modalities contribute diverse information, allowing for more comprehensive exploration of limited GER samples. Since the combined inputs offer robust GER solutions, multi-stream networks are recommended to effectively learn representations from available modalities. In contrast, single-modality approaches perform worse due to limited information and redundancy.\n\nFrom Tables  III  and IV , it is clear that fusion of scores and features is a common approach in integrating multiple modalities. Additionally, there is a growing trend towards using loss functions for multi-modality fusion. Fusion schemes like cross-attention, GCN, ECL, and NVPF have demonstrated state-of-the-art results across all databases. This is likely due to the challenges posed bythat the limited number of GER samples and group sizes, making leveraging additional data sources as reasonable and effective solution.\n\nPresently, scene information based on LSTM and averaging features across all faces are commonly employed in videobased GER studies. This is likely because flexible group sizes pose the main challenge for video-based GER. Recently, Quach et al.  [33]  proposed a Non-Volume Preserving Fusion (NVPF) mechanism with LSTM to model spatial representation between groups of multiple faces and temporal relationships between multiple video frames. However, the small-sample GE dataset limits the ability to model group-level features for video-based GER. The combination of transfer learning and graph-based methods is anticipated to be a promising direction for future GER studies.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Vii. Challenges And Future Direction",
      "text": "Group-level emotion recognition in unconstrained environments has become significant attention within the computer vision community, offering substantial implications for social public security and education. This article delves into the intricate concepts of group dynamics and emotion, along with methodologies for recognizing group-level emotion, and pertient datasets, aiming to provide a comprehensive analysis of the current landscape and future trajectories of GER. This endeavor furnishes a robust theoretical foundation for potential applications of GER in domains such as social psychology, human-computer interaction, and smart cities. In this section, we summarize and discuss the future prospects of GER from three pivotal dimensions: database-level, technique-level, multimodality, and evaluation metric.\n\nGER encounters three primary technical challenges. Firstly, the accurate discernment of emotions inherently presents complex, compounded by the potential bias introduced by human subjective labeling in annotating datasets. Secondly, the fluctuating number of individuals and diverse scenes within a group necessitates enhanced generalization and robustness of features. Furthermore, different extracted features may convey inconsistent emotions. Lastly, the cross-fusion of diverse features in a multimodal model and achieving end-to-end feature learning pose significant challenges. Despite these obstacles hindering GER advancement, its potential applications span a wide spectrum.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "A. Ger Database",
      "text": "While currently available group emotion databases primarily collect images and videos from websites and media platforms, their sample size remain relatively small. On the other hand, Smith et al.  [97]  emphasize the significance of the changes in group emotions along time, noting that individuals may react differently to external group members based on their prevailing emotion states. Pantic et al.  [31]  found that dynamic videos provide more discriminative information to extract temporal changes in emotions. Therefore, the collection of richer and more realistic video data holds promise for furnishing contextual semantic insights into group interactions and dynamic processes, facilitating a more nuanced observation of emotional dynamics within team.\n\nPresently, database annotations are typically derived from independent observers rating the images, lacking subjective evaluation like self-reporting measurements. This limitation may lead to the challenges in acquiring such measurements from online media platforms. However, the absence of subjective evaluation may potentially yield considerable performance in group-level emotion recognition. By integrating subjective evaluation and other auxiliary information, computer scientists can devise advanced methodologies for analyzing data, bridging the gap between computational methods and social science research. Moreover, existing databases primarily reply on methods such as multi-observer cross-calibration to categorize images or videos, which may introduce calibration biases due to cultural disparities and overlook the fundamental tenets of group emotion posited by social psychologists  [23] . Therefore, it is crucial to expanding the repertoire of basic emotion categories to encompass more generalized emotion states in affective computing is imperative. Additionally, involving social psychologists in the data collection and annotation process can furnish more rational and valuable calibration information.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "B. Ger Technique",
      "text": "The methods ranging from Convolutional Neural Networks (CNN) to Recurrent Neural Networks (RNN), Hybrid Networks combining CNN and RNN, and Graph Convolutional Networks (GCN), have already demonstrated remarkable success in GER. However, the challenge of limited sample sizes necessitates the exploration of unsupervised and selfsupervised learning techniques. Unsupervised learning technique like Generative adversarial networks  [98]  and generative AI  [99]  can be valuable for learning meaningful representations from unannotated data, enabling the development of robust deep GER models. These techniques help in extracting rich features from data, even when labeled examples are scarce. Similarly, self-supervised learning paradigms, such as contrastive learning  [100] , offer a way to learn representations from auxiliary tasks, enhancing the generalization capabilities of GER models across diverse group contexts.\n\nAs GER systems move towards real-world deployment, the demand for continual learning and adaptive capabilities become increasingly critical. Deep learning architectures will need to evolve to accommodate dynamic changes in group compositions, social contexts, and environmental conditions over time. Incremental learning strategies, lifelong learning approaches, and adaptive neural networks will play crucial roles in enabling models to adapt and refine their representations based on incoming data streams. Moreover, techniques for mitigating catastrophic forgetting and domain adaptation will be essential for ensuring the long-term stability and effectiveness of GER systems. These methods help models retain previously learned knowledge while adapting to new information, thereby enhancing their robustness in real-world scenarios.\n\nBy embracing unsupervised and self-supervised learning techniques, as well as continual learning and adaptive methodologies, future deep learning systems will be better equipped to capture the nuanced dynamics of group-level emotions across diverse scenarios and domains. This holistic approach holds the potential to significantly advance the field of Group Emotion Recognition and its applications in various domains.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "C. Multi-Modal Architecture For Ger",
      "text": "In the research process of GER, we have observed a continuous evolution towards richer and more diverse in feature representations. Initially, the focus was predominantly on a single facial feature. However, as research progressed, there was a shift towards utilizing multiple features concurrently, including facial features, local object features, and scene features. This enrichment of feature diversity has contributed to an enhanced accuracy of group-level emotion recognition to a certain extent. Moreover, researcher have begun to explore the integration of different types of data sources. For example, studies by Sharma et al.  [32] , Wang et al.  [57] , Liu et al.  [101] , and Pinto et al.  [58]  have incorporated both video frame features and audio features, while Liu et al.  [56]  and Sun et al.  [53]  have combined both facial and audio features. These approaches, known as multimodal methods, offer advantages such as robustness against interference, high interpretability, and broad applicability compared to single-modal methods.\n\nTraditionally, GER relied primarily on single-modal information. However, as the field has progressed and dataset forms have diversified, researchers have increasingly delved into multi-modal methods. These methods leverage various data forms including images, videos, and sounds. The adoption of multimodal fusion techniques in group-level emotion recognition not only facilitates a more comprehensive understanding of emotions but also enhances the accuracy and robustness of the recognition process.\n\nNevertheless, the application of multi-modal fusion in GER poses several challenges. Obtaining and annotating datasets for group-level emotion recognition, particularly those encompassing different modalities, can be arduous. Furthermore, variations in feature extraction and fusion methods across modalities present additional complexities, making it challenging to harmonize features between different modalities.\n\nOverall, while multi-modal fusion encounters challenges in GER, its potential and advantages are undeniable. With advancements in technology and ongoing research efforts, it is anticipated that these challenges will be gradually addressed, and multimodal methods will emerge as the primary research direction in the field of GER. Future research endeavors will likely focus on effectively integrating information from diverse modalities and constructing larger and more diverse datasets for GER, thereby presenting both challenges and opportunities in this dynamic field.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "D. Evaluation Metric Of Ger",
      "text": "It is worth noting that while accuracy is a common metric in GER, it can be influenced by biased data. To address this issue, F1-score offers a more comprehensive evaluation by considering True Positives (TP), False Positives (FP), and False Negatives (FN). This metric provides a balanced assessment of the true classification performance.\n\nAlthough the data may not exhibit severe imbalance, as indicated in Table  I , it may be more suitable to employ metrics such as Unweighted F1-score (UF1) and Unweighted Average Recall (UAR) to assess method performance. UF1, also known as macro-averaged F1-score, calculates the average F1-score across all classes, offering equal weighting to each class in multi-class scenarios. Conversely, UAR computes the average accuracy per class, normalized by the total number of classes. UAR helps mitigate bias arising from class imbalance existing in some databases,e.g., SiteGroEmo and is often referred to as balanced accuracy.",
      "page_start": 13,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: illustrates literature spanning",
      "page": 5
    },
    {
      "caption": "Figure 1: The overview of deep learning based technical papers and survey papers for group-level emotion recognition. Viewed",
      "page": 6
    },
    {
      "caption": "Figure 2: d. With the success of GCNs",
      "page": 6
    },
    {
      "caption": "Figure 2: Basic blocks for GER: (1) Convolution block; (b) Recurrent neural network (RNN); (c) Cascade network; (d) Graph",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "HAPPEI\n[26]*",
          "Type": "Image",
          "Sample size": "2,638",
          "Category": "Neural\n(92), Small smile (147),\nLarge smile (774), Small\nlaugh (1256),\nLarge laugh (331), Thrilled (38)",
          "Task": "Regression",
          "Protocol": "4-fold cross validation\ntest\ntrain (1500), val\n(1138),\n(496)"
        },
        {
          "Dataset": "MultiEmoVA [24]",
          "Type": "Image",
          "Sample size": "250",
          "Category": "High-pos (46), Medium-pos (64),\nHigh-neg (31), medium-neg (27),\nlow-neg (10), neu (72)",
          "Task": "Classification",
          "Protocol": "5-fold cross validation"
        },
        {
          "Dataset": "GAF2.0 [2]",
          "Type": "Image",
          "Sample size": "6,467",
          "Category": "pos (2,356), neu (2,092), neg (2,019)",
          "Task": "Classification",
          "Protocol": "test\ntrain (3,630), val\n(2,068),\n(772)"
        },
        {
          "Dataset": "GAF3.0 [27]",
          "Type": "Image",
          "Sample size": "17,172",
          "Category": "pos (6,553), neu (5,364), neg (5,256)",
          "Task": "Classification",
          "Protocol": "test\ntrain (9,836), val\n(4346),\n(3011)"
        },
        {
          "Dataset": "Group Cohesion [28, 30, 37]",
          "Type": "Image",
          "Sample size": "16,433",
          "Category": "[0, 3]",
          "Task": "Regression",
          "Protocol": "test\ntrain (9,300), val\n(4,244),\n(2,899)"
        },
        {
          "Dataset": "SiteGroEmo [3]",
          "Type": "Image",
          "Sample size": "10,034",
          "Category": "pos (4,660), neu (4,355), neg (1,019)",
          "Task": "Classification",
          "Protocol": "test\ntrain (6,096), val\n(1,972),\n(1,966)"
        },
        {
          "Dataset": "GroupEmoW [29]",
          "Type": "Image",
          "Sample size": "15,894",
          "Category": "pos (6,636), neu (4,947), neg (4,311)",
          "Task": "Classification",
          "Protocol": "test\ntrain (11,127), val\n(3,178),\n(1,589)"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset\n(Year)": "",
          "Cate.": "",
          "Ref.": "",
          "Modality": "F\nS\nP\nA\nT",
          "Network\narchitecture": "",
          "Fusion\nscheme": "",
          "Fusion\nstage": "",
          "Pre-train": "",
          "Prot": "",
          "Perf.": ""
        },
        {
          "Dataset\n(Year)": "HAPPEI*\n(2016)",
          "Cate.": "6",
          "Ref.": "[42]",
          "Modality": "\n",
          "Network\narchitecture": "ResNet\nfor F\nCENTRIST+PCA for S",
          "Fusion\nscheme": "LSTM",
          "Fusion\nstage": "Feature",
          "Pre-train": "FER2013",
          "Prot": "val",
          "Perf.": "0.494"
        },
        {
          "Dataset\n(Year)": "",
          "Cate.": "",
          "Ref.": "",
          "Modality": "",
          "Network\narchitecture": "",
          "Fusion\nscheme": "",
          "Fusion\nstage": "",
          "Pre-train": "",
          "Prot": "test",
          "Perf.": "0.822"
        },
        {
          "Dataset\n(Year)": "",
          "Cate.": "",
          "Ref.": "[67]",
          "Modality": "",
          "Network\narchitecture": "AlexNet",
          "Fusion\nscheme": "LSTM",
          "Fusion\nstage": "Feature",
          "Pre-train": "FER2013",
          "Prot": "val",
          "Perf.": "0.4942"
        },
        {
          "Dataset\n(Year)": "",
          "Cate.": "",
          "Ref.": "",
          "Modality": "",
          "Network\narchitecture": "",
          "Fusion\nscheme": "",
          "Fusion\nstage": "",
          "Pre-train": "",
          "Prot": "test",
          "Perf.": "0.836"
        },
        {
          "Dataset\n(Year)": "",
          "Cate.": "",
          "Ref.": "[93]",
          "Modality": "\n",
          "Network\narchitecture": "ResNet+LSTM for F\nCENTRIST/VGG for S",
          "Fusion\nscheme": "Concat",
          "Fusion\nstage": "Feature",
          "Pre-train": "-",
          "Prot": "val",
          "Perf.": "0.55"
        },
        {
          "Dataset\n(Year)": "",
          "Cate.": "",
          "Ref.": "",
          "Modality": "",
          "Network\narchitecture": "",
          "Fusion\nscheme": "",
          "Fusion\nstage": "",
          "Pre-train": "",
          "Prot": "test",
          "Perf.": "0.865"
        },
        {
          "Dataset\n(Year)": "GAF2.0\n(2017)",
          "Cate.": "3",
          "Ref.": "[39]",
          "Modality": "\n",
          "Network\narchitecture": "VGG for F\nImageNet\nfor S",
          "Fusion\nscheme": "Soft aggregation\n&weighting",
          "Fusion\nstage": "Score",
          "Pre-train": "VGG face\nImageNet",
          "Prot": "val",
          "Perf.": "75.39%"
        },
        {
          "Dataset\n(Year)": "",
          "Cate.": "",
          "Ref.": "",
          "Modality": "",
          "Network\narchitecture": "",
          "Fusion\nscheme": "",
          "Fusion\nstage": "",
          "Pre-train": "",
          "Prot": "test",
          "Perf.": "78.53%"
        },
        {
          "Dataset\n(Year)": "",
          "Cate.": "",
          "Ref.": "[43]",
          "Modality": "\n",
          "Network\narchitecture": "HOG+FV for S\nVGG+VLAD for F",
          "Fusion\nscheme": "Cont",
          "Fusion\nstage": "Feature",
          "Pre-train": "-",
          "Prot": "val",
          "Perf.": "65%"
        },
        {
          "Dataset\n(Year)": "",
          "Cate.": "",
          "Ref.": "",
          "Modality": "",
          "Network\narchitecture": "",
          "Fusion\nscheme": "",
          "Fusion\nstage": "",
          "Pre-train": "",
          "Prot": "test",
          "Perf.": "75.10%"
        },
        {
          "Dataset\n(Year)": "",
          "Cate.": "",
          "Ref.": "[44]",
          "Modality": "\n",
          "Network\narchitecture": "AlexNet\nfor F\nContext\nfor S",
          "Fusion\nscheme": "Bayesian Network",
          "Fusion\nstage": "Score",
          "Pre-train": "-",
          "Prot": "val",
          "Perf.": "67.75%"
        },
        {
          "Dataset\n(Year)": "",
          "Cate.": "",
          "Ref.": "",
          "Modality": "",
          "Network\narchitecture": "",
          "Fusion\nscheme": "",
          "Fusion\nstage": "",
          "Pre-train": "",
          "Prot": "test",
          "Perf.": "64.68%"
        },
        {
          "Dataset\n(Year)": "",
          "Cate.": "",
          "Ref.": "[51]",
          "Modality": "\n\n",
          "Network\narchitecture": "VGG for F\nInception&ResNet\nfor P\nInception&VGG for S",
          "Fusion\nscheme": "SVM",
          "Fusion\nstage": "Score",
          "Pre-train": "FER2013\nGENKI-4K",
          "Prot": "val",
          "Perf.": "80.05%"
        },
        {
          "Dataset\n(Year)": "",
          "Cate.": "",
          "Ref.": "",
          "Modality": "",
          "Network\narchitecture": "",
          "Fusion\nscheme": "",
          "Fusion\nstage": "",
          "Pre-train": "",
          "Prot": "test",
          "Perf.": "80.61%"
        },
        {
          "Dataset\n(Year)": "",
          "Cate.": "",
          "Ref.": "[66]",
          "Modality": "\n",
          "Network\narchitecture": "Xception for F\nVGG for S",
          "Fusion\nscheme": "Cont",
          "Fusion\nstage": "Feature",
          "Pre-train": "FER2013",
          "Prot": "val",
          "Perf.": "72.38%"
        },
        {
          "Dataset\n(Year)": "",
          "Cate.": "",
          "Ref.": "",
          "Modality": "",
          "Network\narchitecture": "",
          "Fusion\nscheme": "",
          "Fusion\nstage": "",
          "Pre-train": "",
          "Prot": "test",
          "Perf.": "63.43%"
        },
        {
          "Dataset\n(Year)": "",
          "Cate.": "",
          "Ref.": "[68]",
          "Modality": "\n",
          "Network\narchitecture": "VGG+DCNN for F\nCENTRIST+VGG for S",
          "Fusion\nscheme": "LSTM/SVM",
          "Fusion\nstage": "Feature",
          "Pre-train": "-",
          "Prot": "val",
          "Perf.": "-"
        },
        {
          "Dataset\n(Year)": "",
          "Cate.": "",
          "Ref.": "",
          "Modality": "",
          "Network\narchitecture": "",
          "Fusion\nscheme": "",
          "Fusion\nstage": "",
          "Pre-train": "",
          "Prot": "test",
          "Perf.": "79.78%"
        },
        {
          "Dataset\n(Year)": "",
          "Cate.": "",
          "Ref.": "[73]",
          "Modality": "\n",
          "Network\narchitecture": "4-layer CNN for F\nResNet\nfor S",
          "Fusion\nscheme": "Average",
          "Fusion\nstage": "Score",
          "Pre-train": "FERPlus\nPlaces",
          "Prot": "val",
          "Perf.": "83.7%"
        },
        {
          "Dataset\n(Year)": "",
          "Cate.": "",
          "Ref.": "",
          "Modality": "",
          "Network\narchitecture": "",
          "Fusion\nscheme": "",
          "Fusion\nstage": "",
          "Pre-train": "",
          "Prot": "test",
          "Perf.": "80.9%"
        },
        {
          "Dataset\n(Year)": "GAF3.0\n(2018)",
          "Cate.": "3",
          "Ref.": "[76]",
          "Modality": "\n\n",
          "Network\narchitecture": "VGG for F\nInception/SE-ResNet\nfor S\nResNet\nfor P",
          "Fusion\nscheme": "Weight\nAverage",
          "Fusion\nstage": "Score",
          "Pre-train": "FER2013\nGENKI-4K",
          "Prot": "val",
          "Perf.": "78.98%"
        },
        {
          "Dataset\n(Year)": "",
          "Cate.": "",
          "Ref.": "",
          "Modality": "",
          "Network\narchitecture": "",
          "Fusion\nscheme": "",
          "Fusion\nstage": "",
          "Pre-train": "",
          "Prot": "test",
          "Perf.": "68.08%"
        },
        {
          "Dataset\n(Year)": "",
          "Cate.": "",
          "Ref.": "[77]",
          "Modality": "\n",
          "Network\narchitecture": "ResNet\nfor F\nVGG for S",
          "Fusion\nscheme": "Weight\naverage",
          "Fusion\nstage": "Score",
          "Pre-train": "FER2013\nRAF-DB",
          "Prot": "val",
          "Perf.": "78.39%"
        },
        {
          "Dataset\n(Year)": "",
          "Cate.": "",
          "Ref.": "",
          "Modality": "",
          "Network\narchitecture": "",
          "Fusion\nscheme": "",
          "Fusion\nstage": "",
          "Pre-train": "",
          "Prot": "test",
          "Perf.": "65.59%"
        },
        {
          "Dataset\n(Year)": "",
          "Cate.": "",
          "Ref.": "[83]",
          "Modality": "\n\n",
          "Network\narchitecture": "DenseNet\nfor S\nSphereFace for F",
          "Fusion\nscheme": "Cont",
          "Fusion\nstage": "Feature",
          "Pre-train": "ImageNet\nCASIA-Webface",
          "Prot": "val",
          "Perf.": "80.98%"
        },
        {
          "Dataset\n(Year)": "",
          "Cate.": "",
          "Ref.": "",
          "Modality": "",
          "Network\narchitecture": "",
          "Fusion\nscheme": "",
          "Fusion\nstage": "",
          "Pre-train": "",
          "Prot": "test",
          "Perf.": "64.83%"
        },
        {
          "Dataset\n(Year)": "",
          "Cate.": "",
          "Ref.": "[84]",
          "Modality": "\n",
          "Network\narchitecture": "CAN for F, ResNet\nfor\nS, SE-net\nfor P",
          "Fusion\nscheme": "Average",
          "Fusion\nstage": "Score",
          "Pre-train": "FERPlus",
          "Prot": "val",
          "Perf.": "86.7%"
        },
        {
          "Dataset\n(Year)": "",
          "Cate.": "",
          "Ref.": "",
          "Modality": "",
          "Network\narchitecture": "",
          "Fusion\nscheme": "",
          "Fusion\nstage": "",
          "Pre-train": "",
          "Prot": "test",
          "Perf.": "67.48%"
        },
        {
          "Dataset\n(Year)": "Group\nCohesion*\n(2019)",
          "Cate.": "4",
          "Ref.": "[94]",
          "Modality": "\n\n",
          "Network\narchitecture": "CAN for F\nSE-Net\nfor S/P",
          "Fusion\nscheme": "Average",
          "Fusion\nstage": "Score",
          "Pre-train": "FERPlus\nImageNet",
          "Prot": "val",
          "Perf.": "0.5588"
        },
        {
          "Dataset\n(Year)": "",
          "Cate.": "",
          "Ref.": "",
          "Modality": "",
          "Network\narchitecture": "",
          "Fusion\nscheme": "",
          "Fusion\nstage": "",
          "Pre-train": "",
          "Prot": "test",
          "Perf.": "0.4382"
        },
        {
          "Dataset\n(Year)": "",
          "Cate.": "",
          "Ref.": "[95]",
          "Modality": "\n\n",
          "Network\narchitecture": "DensePose for S\nResNet/Inception/NasNet\nfor S\nResNet\nfor F",
          "Fusion\nscheme": "Average",
          "Fusion\nstage": "Score",
          "Pre-train": "VGG Face2\nRAF-DB",
          "Prot": "val",
          "Perf.": "0.517"
        },
        {
          "Dataset\n(Year)": "",
          "Cate.": "",
          "Ref.": "",
          "Modality": "",
          "Network\narchitecture": "",
          "Fusion\nscheme": "",
          "Fusion\nstage": "",
          "Pre-train": "",
          "Prot": "test",
          "Perf.": "0.416"
        },
        {
          "Dataset\n(Year)": "",
          "Cate.": "",
          "Ref.": "[96]",
          "Modality": "\n\n",
          "Network\narchitecture": "VGG+SVR for F\nEfficient+SVR for P\nDensenet+SVR for S",
          "Fusion\nscheme": "Grid\nSearch",
          "Fusion\nstage": "Score",
          "Pre-train": "FER2013\nEmotic",
          "Prot": "val",
          "Perf.": "0.672"
        },
        {
          "Dataset\n(Year)": "",
          "Cate.": "",
          "Ref.": "",
          "Modality": "",
          "Network\narchitecture": "",
          "Fusion\nscheme": "",
          "Fusion\nstage": "",
          "Pre-train": "",
          "Prot": "test",
          "Perf.": "0.444"
        },
        {
          "Dataset\n(Year)": "VGAF\n(2020)",
          "Cate.": "3",
          "Ref.": "[54]",
          "Modality": "",
          "Network\narchitecture": "VGG+ML",
          "Fusion\nscheme": "-",
          "Fusion\nstage": "-",
          "Pre-train": "ImageNet",
          "Prot": "val",
          "Perf.": "57.18%"
        },
        {
          "Dataset\n(Year)": "",
          "Cate.": "",
          "Ref.": "",
          "Modality": "",
          "Network\narchitecture": "",
          "Fusion\nscheme": "",
          "Fusion\nstage": "",
          "Pre-train": "",
          "Prot": "test",
          "Perf.": "59.13%"
        },
        {
          "Dataset\n(Year)": "",
          "Cate.": "",
          "Ref.": "[81]",
          "Modality": "",
          "Network\narchitecture": "DeepSpectrum+AlexNet\n+VGG+DenseNet",
          "Fusion\nscheme": "Mean",
          "Fusion\nstage": "Score",
          "Pre-train": "-",
          "Prot": "val",
          "Perf.": "58.09%"
        },
        {
          "Dataset\n(Year)": "",
          "Cate.": "",
          "Ref.": "",
          "Modality": "",
          "Network\narchitecture": "",
          "Fusion\nscheme": "",
          "Fusion\nstage": "",
          "Pre-train": "",
          "Prot": "test",
          "Perf.": "62.70%"
        },
        {
          "Dataset\n(Year)": "",
          "Cate.": "",
          "Ref.": "[56]",
          "Modality": "\n\n\n\n",
          "Network\narchitecture": "TSM for T\nDense for F, OpenSmile for A",
          "Fusion\nscheme": "Average",
          "Fusion\nstage": "Score",
          "Pre-train": "FER2013",
          "Prot": "val",
          "Perf.": "74.28%"
        },
        {
          "Dataset\n(Year)": "",
          "Cate.": "",
          "Ref.": "",
          "Modality": "",
          "Network\narchitecture": "",
          "Fusion\nscheme": "",
          "Fusion\nstage": "",
          "Pre-train": "",
          "Prot": "test",
          "Perf.": "76.85%"
        },
        {
          "Dataset\n(Year)": "",
          "Cate.": "",
          "Ref.": "[57]",
          "Modality": "\n\n",
          "Network\narchitecture": "K-injection",
          "Fusion\nscheme": "Cont",
          "Fusion\nstage": "Feature",
          "Pre-train": "-",
          "Prot": "val",
          "Perf.": "66.19%"
        },
        {
          "Dataset\n(Year)": "",
          "Cate.": "",
          "Ref.": "",
          "Modality": "",
          "Network\narchitecture": "",
          "Fusion\nscheme": "",
          "Fusion\nstage": "",
          "Pre-train": "",
          "Prot": "test",
          "Perf.": "66.40%"
        },
        {
          "Dataset\n(Year)": "",
          "Cate.": "",
          "Ref.": "[53]",
          "Modality": "\n\n\n",
          "Network\narchitecture": "TSM\nTBN",
          "Fusion\nscheme": "Weight\nsum",
          "Fusion\nstage": "Score",
          "Pre-train": "ImageNet",
          "Prot": "val",
          "Perf.": "71.93%"
        },
        {
          "Dataset\n(Year)": "",
          "Cate.": "",
          "Ref.": "",
          "Modality": "",
          "Network\narchitecture": "",
          "Fusion\nscheme": "",
          "Fusion\nstage": "",
          "Pre-train": "",
          "Prot": "test",
          "Perf.": "70.77%"
        },
        {
          "Dataset\n(Year)": "VGAF\n(2023)",
          "Cate.": "3",
          "Ref.": "[55]",
          "Modality": "\n\n\n",
          "Network\narchitecture": "ResNet\nfor F, SeNet\nfor S\nHubert\nlarge for A",
          "Fusion\nscheme": "Cont",
          "Fusion\nstage": "Feature",
          "Pre-train": "FER2013\nImageNet",
          "Prot": "val",
          "Perf.": "68.41%"
        },
        {
          "Dataset\n(Year)": "",
          "Cate.": "",
          "Ref.": "",
          "Modality": "",
          "Network\narchitecture": "",
          "Fusion\nscheme": "",
          "Fusion\nstage": "",
          "Pre-train": "",
          "Prot": "test",
          "Perf.": "72%"
        },
        {
          "Dataset\n(Year)": "",
          "Cate.": "",
          "Ref.": "[75]",
          "Modality": "\n\n",
          "Network\narchitecture": "ViT-large for V\nCNN+Transformer\nfor A",
          "Fusion\nscheme": "Average",
          "Fusion\nstage": "Feature",
          "Pre-train": "ImageNet",
          "Prot": "val",
          "Perf.": "78.72%"
        },
        {
          "Dataset\n(Year)": "",
          "Cate.": "",
          "Ref.": "",
          "Modality": "",
          "Network\narchitecture": "",
          "Fusion\nscheme": "",
          "Fusion\nstage": "",
          "Pre-train": "",
          "Prot": "test",
          "Perf.": "75.13%"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "",
          "Database": "",
          "Modality": "F\nS\nP\nO",
          "Network": "",
          "Fusion\nScheme": "",
          "Fusion\nStage": "",
          "Prot.": "",
          "Cate.": "",
          "Perf.": ""
        },
        {
          "Method": "[91]\n[45]\n[69]\n[49]",
          "Database": "GAF2.0",
          "Modality": "",
          "Network": "AlexNet",
          "Fusion\nScheme": "HeatMap",
          "Fusion\nStage": "Feature",
          "Prot.": "val",
          "Cate.": "3",
          "Perf.": "55.23%"
        },
        {
          "Method": "",
          "Database": "",
          "Modality": "\n",
          "Network": "Xception for\nface\nVGG for scene",
          "Fusion\nScheme": "Concat",
          "Fusion\nStage": "Feature",
          "Prot.": "val",
          "Cate.": "3",
          "Perf.": "71.83%"
        },
        {
          "Method": "",
          "Database": "",
          "Modality": "",
          "Network": "CNN, RVLBP",
          "Fusion\nScheme": "DMKL",
          "Fusion\nStage": "Kernel",
          "Prot.": "val",
          "Cate.": "3",
          "Perf.": "79.49%"
        },
        {
          "Method": "",
          "Database": "",
          "Modality": "\n",
          "Network": "MobileNet\nfor scene\nLSTM for\nface",
          "Fusion\nScheme": "Average",
          "Fusion\nStage": "Score",
          "Prot.": "1-fold",
          "Cate.": "3",
          "Perf.": "78%"
        },
        {
          "Method": "[47]\n[82]",
          "Database": "GAF3.0",
          "Modality": "",
          "Network": "Inception/VGG for scene\nVGG for\nface",
          "Fusion\nScheme": "SVM",
          "Fusion\nStage": "Score",
          "Prot.": "val",
          "Cate.": "3",
          "Perf.": "70.1%"
        },
        {
          "Method": "",
          "Database": "",
          "Modality": "\n\n",
          "Network": "VGG+LSTM for\nface\nDense for Skeleton\nAttention for Scene",
          "Fusion\nScheme": "Concat",
          "Fusion\nStage": "Feature",
          "Prot.": "val",
          "Cate.": "3",
          "Perf.": "62.90%"
        },
        {
          "Method": "[88]",
          "Database": "MultiEmoVA\nHAPPEI*\nGAF2.0",
          "Modality": "",
          "Network": "RVLBP, VGG",
          "Fusion\nScheme": "SVM-CGAK",
          "Fusion\nStage": "Kernel",
          "Prot.": "5-fold\n4-fold\nval",
          "Cate.": "5 6 3",
          "Perf.": "54.40%\n0.4920\n72.17%"
        },
        {
          "Method": "[48]",
          "Database": "GAF2.0\nGAF3.0",
          "Modality": "\n\n",
          "Network": "VGG, attention for\nface\nVGG, attention for object\nVGG for scene",
          "Fusion\nScheme": "Hierarchical",
          "Fusion\nStage": "Feature",
          "Prot.": "val",
          "Cate.": "3",
          "Perf.": "80.41%\n76.61%"
        },
        {
          "Method": "[50]",
          "Database": "MultiEmoVA\nGAF2.0\nGAF3.0",
          "Modality": "\n\n",
          "Network": "ResNet\nfor\nface\nVGG for object\nVGG for scene",
          "Fusion\nScheme": "UAL",
          "Fusion\nStage": "Loss",
          "Prot.": "5-fold\nval\nval",
          "Cate.": "5 3 3",
          "Perf.": "61.22%\n79.19%\n77.10%"
        },
        {
          "Method": "[29]",
          "Database": "GroupEmoW\nGAF2.0\nSocEID",
          "Modality": "\n\n\n",
          "Network": "VGG for\nface\nSE-ResNet\nfor skeleton\nSENet\nfor object\nInception for scene",
          "Fusion\nScheme": "GNN",
          "Fusion\nStage": "Score",
          "Prot.": "test\nval\ntest",
          "Cate.": "3 3 8",
          "Perf.": "89.14%\n78.16%\n91.61%"
        },
        {
          "Method": "[79]",
          "Database": "GAF2.0\nGroupEmoW",
          "Modality": "\n\n",
          "Network": "Resnet\nfor all modalities\nAttention module",
          "Fusion\nScheme": "CARAN",
          "Fusion\nStage": "Loss",
          "Prot.": "val\ntest",
          "Cate.": "3",
          "Perf.": "67.61%\n90.18%"
        },
        {
          "Method": "[74]",
          "Database": "GroupEmoW\nGAF2.0\nGAF3.0",
          "Modality": "\n",
          "Network": "ResNet\nfor all modalities",
          "Fusion\nScheme": "FusionNet",
          "Fusion\nStage": "Loss",
          "Prot.": "test\nval\nval",
          "Cate.": "3",
          "Perf.": "88.67%\n78.51%\n77.01%"
        },
        {
          "Method": "[3]",
          "Database": "GroupEmoW\nGAF2.0\nGAF3.0",
          "Modality": "\n\n",
          "Network": "ResNet+LSTM+GNN for\nface\nSE-ResNet+GNN for object\nSE-ResNet+GNN for scene",
          "Fusion\nScheme": "ECL",
          "Fusion\nStage": "Loss",
          "Prot.": "test\nval\nval",
          "Cate.": "3",
          "Perf.": "90.06%\n79.45%\n79.95%"
        },
        {
          "Method": "[87]",
          "Database": "GAF3.0\nGroupEmoW",
          "Modality": "\n",
          "Network": "Multi-scale Transformer",
          "Fusion\nScheme": "DCAT",
          "Fusion\nStage": "Feature",
          "Prot.": "val\ntest",
          "Cate.": "3",
          "Perf.": "79.20%\n90.47%"
        },
        {
          "Method": "[33]",
          "Database": "GECV-GroupImg\nGAF3.0",
          "Modality": "",
          "Network": "EmoNet",
          "Fusion\nScheme": "NVPF",
          "Fusion\nStage": "Feature",
          "Prot.": "val",
          "Cate.": "3",
          "Perf.": "77.02%\n76.12%"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "",
          "Dataset": "",
          "Modality": "F\nS\nP\nA",
          "Network": "",
          "Fusion\nScheme": "",
          "Fusion\nStage": "",
          "Prot.": "",
          "Cate.": "",
          "Accuracy": ""
        },
        {
          "Method": "Sharma+[32]",
          "Dataset": "VGAF",
          "Modality": "\n",
          "Network": "LSTM for scene\nOpenSMILE for audio",
          "Fusion\nScheme": "Concat",
          "Fusion\nStage": "Feature",
          "Prot.": "val",
          "Cate.": "3",
          "Accuracy": "47.50%"
        },
        {
          "Method": "Pinto+[58]",
          "Dataset": "",
          "Modality": "\n",
          "Network": "ResNet\nfor scene\nBi-LSTM for audio",
          "Fusion\nScheme": "SVM",
          "Fusion\nStage": "Score",
          "Prot.": "val",
          "Cate.": "3",
          "Accuracy": "65.74%"
        },
        {
          "Method": "Evtodienko+[70]",
          "Dataset": "",
          "Modality": "\n",
          "Network": "Hubert+Attention for audio\nResNet+Attention for scene",
          "Fusion\nScheme": "Concat",
          "Fusion\nStage": "Feature",
          "Prot.": "val",
          "Cate.": "3",
          "Accuracy": "60.37%"
        },
        {
          "Method": "Quach+[33]",
          "Dataset": "GECV\n-GroupVid",
          "Modality": "",
          "Network": "EmoNet",
          "Fusion\nScheme": "TNVPF",
          "Fusion\nStage": "Feature",
          "Prot.": "test",
          "Cate.": "3",
          "Accuracy": "70.97%"
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The impact of emotion on perception, attention, memory, and decision-making",
      "authors": [
        "T Brosch",
        "K Scherer",
        "D Grandjean",
        "D Sander"
      ],
      "year": "1920",
      "venue": "Swiss medical weekly"
    },
    {
      "citation_id": "2",
      "title": "From individual to group-level emotion recognition: Emotiw 5.0",
      "authors": [
        "A Dhall",
        "R Goecke",
        "S Ghosh",
        "J Joshi",
        "J Hoey",
        "T Gedeon"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "3",
      "title": "Congnn: Context-consistent cross-graph neural network for group emotion recognition in the wild",
      "authors": [
        "Y Wang",
        "S Zhou",
        "Y Liu",
        "K Wang",
        "F Fang",
        "H Qian"
      ],
      "year": "2022",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "4",
      "title": "Leaders and followers identified by emotional mimicry during collaborative learning: A facial expression recognition study on emotional valence",
      "authors": [
        "M Dindar",
        "S J채rvel채",
        "S Ahola",
        "X Huang",
        "G Zhao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "5",
      "title": "Automatic emotion recognition for groups: a review",
      "authors": [
        "E Veltmeijer",
        "C Gerritsen",
        "K Hindriks"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "Group-level emotions",
      "authors": [
        "E Smith",
        "D Mackie"
      ],
      "year": "2016",
      "venue": "Current Opinion in Psychology"
    },
    {
      "citation_id": "7",
      "title": "Social functionality of human emotion",
      "authors": [
        "P Niedenthal",
        "M Brauer"
      ],
      "year": "2012",
      "venue": "Annual review of psychology"
    },
    {
      "citation_id": "8",
      "title": "Group dynamics",
      "authors": [
        "M Shaw"
      ],
      "year": "1961",
      "venue": "Annual Review of Psychology"
    },
    {
      "citation_id": "9",
      "title": "Organizational Behavior and Performance",
      "authors": [
        "A Szilagyi",
        "M Wallace"
      ],
      "year": "1983",
      "venue": "Organizational Behavior and Performance"
    },
    {
      "citation_id": "10",
      "title": "A time to grow and a time to die: Growth and mortality of credit unions in new york city, 1914-1990",
      "authors": [
        "D Barron",
        "E West",
        "M Hannan"
      ],
      "year": "1994",
      "venue": "American Journal of Sociology"
    },
    {
      "citation_id": "11",
      "title": "Group entitativity and group perception: Associations between physical features and psychological judgment",
      "authors": [
        "N Dasgupta",
        "M Banaji",
        "R Abelson"
      ],
      "year": "1999",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "12",
      "title": "Cognitive, social, and physiological determinants of emotional state",
      "authors": [
        "S Schachter",
        "J Singer"
      ],
      "year": "1962",
      "venue": "Psychological review"
    },
    {
      "citation_id": "13",
      "title": "A categorized list of emotion definitions, with suggestions for a consensual definition",
      "authors": [
        "P Kleinginna",
        "A Kleinginna"
      ],
      "year": "1981",
      "venue": "Motivation and emotion"
    },
    {
      "citation_id": "14",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "15",
      "title": "What are emotions, really?",
      "authors": [
        "J Averill"
      ],
      "year": "1998",
      "venue": "Cognition & Emotion"
    },
    {
      "citation_id": "16",
      "title": "What is emotion?",
      "authors": [
        "M Cabanac"
      ],
      "year": "2002",
      "venue": "Behavioural processes"
    },
    {
      "citation_id": "17",
      "title": "Are emotions natural kinds?",
      "authors": [
        "L Barrett"
      ],
      "year": "2006",
      "venue": "Perspectives on psychological science"
    },
    {
      "citation_id": "18",
      "title": "Cross-corpus acoustic emotion recognition: Variances and strategies",
      "authors": [
        "B Schuller",
        "B Vlasenko",
        "F Eyben",
        "M W철llmer",
        "A Stuhlsatz",
        "A Wendemuth",
        "G Rigoll"
      ],
      "year": "2010",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "19",
      "title": "Emotional contagion",
      "authors": [
        "E Hatfield",
        "J Cacioppo",
        "R Rapson"
      ],
      "year": "1993",
      "venue": "Current directions in psychological science"
    },
    {
      "citation_id": "20",
      "title": "Antecedents and purchase consequences of customer participation in small group brand communities",
      "authors": [
        "R Bagozzi",
        "U Dholakia"
      ],
      "year": "2006",
      "venue": "International Journal of research in Marketing"
    },
    {
      "citation_id": "21",
      "title": "Group emotion: A view from top and bottom",
      "authors": [
        "S Barsade",
        "D Gibson"
      ],
      "year": "1998",
      "venue": "Res. Manag. Group Teamss"
    },
    {
      "citation_id": "22",
      "title": "Mood and emotions in small groups and work teams",
      "authors": [
        "J Kelly",
        "S Barsade"
      ],
      "year": "2001",
      "venue": "Organizational behavior and human decision processes"
    },
    {
      "citation_id": "23",
      "title": "Why does affect matter in organizations?",
      "authors": [
        "S Barsade",
        "D Gibson"
      ],
      "year": "2007",
      "venue": "Academy of management perspectives"
    },
    {
      "citation_id": "24",
      "title": "Group-level arousal and valence recognition in static images: Face, body and context",
      "authors": [
        "W Mou",
        "O Celiktutan",
        "H Gunes"
      ],
      "year": "2015",
      "venue": "2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "25",
      "title": "Finding happiest moments in a social context",
      "authors": [
        "A Dhall",
        "J Joshi",
        "I Radwan",
        "R Goecke"
      ],
      "year": "2012",
      "venue": "Computer Vision-ACCV 2012: 11th Asian Conference on Computer Vision"
    },
    {
      "citation_id": "26",
      "title": "The more the merrier: Analysing the affect of a group of people in images",
      "authors": [
        "A Dhall",
        "J Joshi",
        "K Sikka",
        "R Goecke",
        "N Sebe"
      ],
      "year": "2015",
      "venue": "2015 11th IEEE international conference and workshops on automatic face and gesture recognition (FG)"
    },
    {
      "citation_id": "27",
      "title": "Emotiw 2018: Audio-video, student engagement and group-level affect prediction",
      "authors": [
        "A Dhall",
        "A Kaur",
        "R Goecke",
        "T Gedeon"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "28",
      "title": "Predicting group cohesiveness in images",
      "authors": [
        "S Ghosh",
        "A Dhall",
        "N Sebe",
        "T Gedeon"
      ],
      "year": "2019",
      "venue": "2019 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "29",
      "title": "Graph neural networks for image understanding based on multiple cues: Group emotion recognition and event recognition as use cases",
      "authors": [
        "X Guo",
        "L Polania",
        "B Zhu",
        "C Boncelet",
        "K Barner"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "30",
      "title": "Automatic emotion, engagement and cohesion prediction tasks",
      "authors": [
        "A Dhall"
      ],
      "year": "2019",
      "venue": "2019 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "31",
      "title": "Dynamics of facial expression: recognition of facial actions and their temporal segments from face profile image sequences",
      "authors": [
        "M Pantic",
        "I Patras"
      ],
      "year": "2006",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)"
    },
    {
      "citation_id": "32",
      "title": "Automatic group level affect and cohesion prediction in videos",
      "authors": [
        "G Sharma",
        "S Ghosh",
        "A Dhall"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos"
    },
    {
      "citation_id": "33",
      "title": "Nonvolume preserving-based fusion to group-level emotion recognition on crowd videos",
      "authors": [
        "K Quach",
        "N Le",
        "C Duong",
        "I Jalata",
        "K Roy",
        "K Luu"
      ],
      "year": "2022",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "34",
      "title": "Emotiw 2023: Emotion recognition in the wild challenge",
      "authors": [
        "A Dhall",
        "M Singh",
        "R Goecke",
        "T Gedeon",
        "D Zeng",
        "Y Wang",
        "K Ikeda"
      ],
      "year": "2023",
      "venue": "Proceedings of the 25th International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "35",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "36",
      "title": "Megc 2019-the second facial micro-expressions grand challenge",
      "authors": [
        "J See",
        "M Yap",
        "J Li",
        "X Hong",
        "S.-J Wang"
      ],
      "year": "2019",
      "venue": "Gesture Recognition"
    },
    {
      "citation_id": "37",
      "title": "Automatic prediction of group cohesiveness in images",
      "authors": [
        "S Ghosh",
        "A Dhall",
        "N Sebe",
        "T Gedeon"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "38",
      "title": "Emotiw 2020: Driver gaze, group emotion, student engagement and physiological signal based challenges",
      "authors": [
        "A Dhall",
        "G Sharma",
        "R Goecke",
        "T Gedeon"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "39",
      "title": "Group-level emotion recognition using transfer learning from face identification",
      "authors": [
        "A Rassadin",
        "A Gruzdev",
        "A Savchenko"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "40",
      "title": "Happiness intensity estimation for a group of people in images using convolutional neural networks",
      "authors": [
        "G Lu",
        "W Zhang"
      ],
      "year": "2019",
      "venue": "2019 3rd international conference on electronic information technology and computer engineering (EITCE)"
    },
    {
      "citation_id": "41",
      "title": "Group emotion recognition based on multilayer hybrid network",
      "authors": [
        "C Pan",
        "D Yu",
        "L Sijiang",
        "G Zhen",
        "Y Lei"
      ],
      "year": "2018",
      "venue": "2018 IEEE 3rd International Conference on Image, Vision and Computing (ICIVC)"
    },
    {
      "citation_id": "42",
      "title": "Happiness level prediction with sequential inputs via multiple regressions",
      "authors": [
        "J Li",
        "S Roy",
        "J Feng",
        "T Sim"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "43",
      "title": "Multi-level feature fusion for group-level emotion recognition",
      "authors": [
        "B Balaji",
        "V Oruganti"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "44",
      "title": "Emotion recognition in the wild using deep neural networks and bayesian classifiers",
      "authors": [
        "L Surace",
        "M Patacchiola",
        "E Battini",
        "W S철nmez",
        "A Spataro",
        "Cangelosi"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "45",
      "title": "Enhancing feature correlation for bi-modal group emotion recognition",
      "authors": [
        "N Liu",
        "Y Fang",
        "Y Guo"
      ],
      "year": "2018",
      "venue": "Advances in Multimedia Information Processing-PCM 2018: 19th Pacific-Rim Conference on Multimedia"
    },
    {
      "citation_id": "46",
      "title": "Group emotion recognition using machine learning",
      "authors": [
        "S Garg"
      ],
      "year": "2019",
      "venue": "Group emotion recognition using machine learning",
      "arxiv": "arXiv:1905.01118"
    },
    {
      "citation_id": "47",
      "title": "Group emotion recognition in adverse face detection",
      "authors": [
        "B Nagarajan",
        "V Oruganti"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "48",
      "title": "Hierarchical group-level emotion recognition",
      "authors": [
        "K Fujii",
        "D Sugimura",
        "T Hamamoto"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "49",
      "title": "Group emotion recognition based on global and local features",
      "authors": [
        "D Yu",
        "L Xingyu",
        "D Shuzhan",
        "Y Lei"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "50",
      "title": "Towards a robust group-level emotion recognition via uncertainty-aware learning",
      "authors": [
        "Q Zhu",
        "Q Mao",
        "J Zhang",
        "X Huang",
        "W Zheng"
      ],
      "year": "2023",
      "venue": "Towards a robust group-level emotion recognition via uncertainty-aware learning",
      "arxiv": "arXiv:2310.04306"
    },
    {
      "citation_id": "51",
      "title": "Group-level emotion recognition using deep models on image scene, faces, and skeletons",
      "authors": [
        "X Guo",
        "L Polan챠a",
        "K Barner"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "52",
      "title": "The role of facial movements in emotion recognition",
      "authors": [
        "E Krumhuber",
        "L Skora",
        "H Hill",
        "K Lander"
      ],
      "year": "2023",
      "venue": "Nature Reviews Psychology"
    },
    {
      "citation_id": "53",
      "title": "Multi-modal fusion using spatio-temporal and static features for group emotion recognition",
      "authors": [
        "M Sun",
        "J Li",
        "H Feng",
        "W Gou",
        "H Shen",
        "J Tang",
        "Y Yang",
        "J Ye"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "54",
      "title": "Group-level emotion recognition using a unimodal privacy-safe non-individual approach",
      "authors": [
        "A Petrova",
        "D Vaufreydaz",
        "P Dessus"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "55",
      "title": "Audio-visual group-based emotion recognition using local and global feature aggregation based multi-task learning",
      "authors": [
        "S Li",
        "H Lian",
        "C Lu",
        "Y Zhao",
        "C Tang",
        "Y Zong",
        "W Zheng"
      ],
      "year": "2023",
      "venue": "Proceedings of the 25th International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "56",
      "title": "Group level audio-video emotion recognition using hybrid networks",
      "authors": [
        "C Liu",
        "W Jiang",
        "M Wang",
        "T Tang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "57",
      "title": "Implicit knowledge injectable cross attention audiovisual model for group emotion recognition",
      "authors": [
        "Y Wang",
        "J Wu",
        "P Heracleous",
        "S Wada",
        "R Kimura",
        "S Kurihara"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 international conference on multimodal interaction"
    },
    {
      "citation_id": "58",
      "title": "Audiovisual classification of group emotion valence using activity recognition networks",
      "authors": [
        "J Pinto",
        "T Gonc 쨍alves",
        "C Pinto",
        "L Sanhudo",
        "J Fonseca",
        "F Gonc 쨍alves",
        "P Carvalho",
        "J Cardoso"
      ],
      "year": "2020",
      "venue": "2020 IEEE 4th International Conference on Image Processing"
    },
    {
      "citation_id": "59",
      "title": "Survey on emotional body gesture recognition",
      "authors": [
        "F Noroozi",
        "C Corneanu",
        "D Kamiska",
        "T Sapiski",
        "S Escalera",
        "G Anbarjafari"
      ],
      "year": "2018",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "60",
      "title": "Backpropagation applied to handwritten zip code recognition",
      "authors": [
        "Y Lecun",
        "B Boser",
        "J Denker",
        "D Henderson",
        "R Howard",
        "W Hubbard",
        "L Jackel"
      ],
      "year": "1989",
      "venue": "Neural computation"
    },
    {
      "citation_id": "61",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2012",
      "venue": "Proceedings of NeurIPS"
    },
    {
      "citation_id": "62",
      "title": "Learning representations by back-propagating errors",
      "authors": [
        "D Rumelhart",
        "G Hinton",
        "R Williams"
      ],
      "year": "1986",
      "venue": "nature"
    },
    {
      "citation_id": "63",
      "title": "The graph neural network model",
      "authors": [
        "F Scarselli",
        "M Gori",
        "A Tsoi",
        "M Hagenbuchner",
        "G Monfardini"
      ],
      "year": "2008",
      "venue": "The graph neural network model"
    },
    {
      "citation_id": "64",
      "title": "Relation modeling with graph convolutional networks for facial action unit detection",
      "authors": [
        "Z Liu",
        "J Dong",
        "C Zhang",
        "L Wang",
        "J Dang"
      ],
      "year": "2020",
      "venue": "Proceedings of the International Conference MultiMedia Modeling"
    },
    {
      "citation_id": "65",
      "title": "Implementing the affective mechanism for group emotion recognition with a new graph convolutional network architecture",
      "authors": [
        "X Wang",
        "D Zhang",
        "D.-J Lee"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "66",
      "title": "Group emotion recognition in the wild by combining deep neural networks for facial expression classification and scene-context analysis",
      "authors": [
        "A Abbas",
        "S Chalup"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "67",
      "title": "Lstm for dynamic emotion and group emotion recognition in the wild",
      "authors": [
        "B Sun",
        "Q Wei",
        "L Li",
        "Q Xu",
        "J He",
        "L Yu"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "68",
      "title": "A new deeplearning framework for group emotion recognition",
      "authors": [
        "Q Wei",
        "Y Zhao",
        "Q Xu",
        "L Li",
        "J He",
        "L Yu",
        "B Sun"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "69",
      "title": "Group-level human affect recognition with multiple graph kernel fusion",
      "authors": [
        "X Huang"
      ],
      "year": "2022",
      "venue": "INFORMS International Conference on Service Science"
    },
    {
      "citation_id": "70",
      "title": "Multimodal end-to-end group emotion recognition using cross-modal attention",
      "authors": [
        "L Evtodienko"
      ],
      "year": "2021",
      "venue": "Multimodal end-to-end group emotion recognition using cross-modal attention",
      "arxiv": "arXiv:2111.05890"
    },
    {
      "citation_id": "71",
      "title": "Less is more: Clipbert for video-and-language learning via sparse sampling",
      "authors": [
        "J Lei",
        "L Li",
        "L Zhou",
        "Z Gan",
        "T Berg",
        "M Bansal",
        "J Liu"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "72",
      "title": "Emotion recognition: The role of featural and configural face information",
      "authors": [
        "D Bombari",
        "P Schmid",
        "M Mast",
        "S Birri",
        "F Mast",
        "J Lobmaier"
      ],
      "year": "2013",
      "venue": "Quarterly Journal of Experimental Psychology"
    },
    {
      "citation_id": "73",
      "title": "Group emotion recognition with individual facial emotion cnns and global image based cnns",
      "authors": [
        "L Tan",
        "K Zhang",
        "K Wang",
        "X Zeng",
        "X Peng",
        "Y Qiao"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "74",
      "title": "Semi-supervised group emotion recognition based on contrastive learning",
      "authors": [
        "J Zhang",
        "X Wang",
        "D Zhang",
        "D.-J Lee"
      ],
      "year": "2022",
      "venue": "Electronics"
    },
    {
      "citation_id": "75",
      "title": "Multimodal group emotion recognition in-the-wild using privacy-compliant features",
      "authors": [
        "A Augusma",
        "D Vaufreydaz",
        "F Letu챕"
      ],
      "year": "2023",
      "venue": "Proceedings of the 25th International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "76",
      "title": "Grouplevel emotion recognition using hybrid deep models based on faces, scenes, skeletons and visual attentions",
      "authors": [
        "X Guo",
        "B Zhu",
        "L Polan챠a",
        "C Boncelet",
        "K Barner"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "77",
      "title": "Grouplevel emotion recognition using deep models with a four-stream hybrid network",
      "authors": [
        "A.-S Khan",
        "Z Li",
        "J Cai",
        "Z Meng",
        "J O'reilly",
        "Y Tong"
      ],
      "year": "2018",
      "venue": "ICMI"
    },
    {
      "citation_id": "78",
      "title": "Hierarchical group-level emotion recognition in the wild",
      "authors": [
        "K Fujii",
        "D Sugimura",
        "T Hamamoto"
      ],
      "year": "2019",
      "venue": "2019 14th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "79",
      "title": "Regional attention networks with context-aware fusion for group emotion recognition",
      "authors": [
        "A Khan",
        "Z Li",
        "J Cai",
        "Y Tong"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "80",
      "title": "Group emotion recognition in the wild using pose estimation and lstm neural networks",
      "authors": [
        "K Slogrove",
        "D Van Der Haar"
      ],
      "year": "2022",
      "venue": "2022 International Conference on Artificial Intelligence, Big Data, Computing and Data Communication Systems (icABCD)"
    },
    {
      "citation_id": "81",
      "title": "Grouplevel speech emotion recognition utilising deep spectrum features",
      "authors": [
        "S Ottl",
        "S Amiriparian",
        "M Gerczuk",
        "V Karas",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "82",
      "title": "Group-level emotion recognition based on faces, scenes, skeletons features",
      "authors": [
        "D Li",
        "R Luo",
        "S Sun"
      ],
      "year": "2019",
      "venue": "Eleventh International Conference on Graphics and Image Processing"
    },
    {
      "citation_id": "83",
      "title": "An attention model for group-level emotion recognition",
      "authors": [
        "A Gupta",
        "D Agrawal",
        "H Chauhan",
        "J Dolz",
        "M Pedersoli"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "84",
      "title": "Cascade attention networks for group emotion recognition with face, body and image cues",
      "authors": [
        "K Wang",
        "X Zeng",
        "J Yang",
        "D Meng",
        "K Zhang",
        "X Peng",
        "Y Qiao"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "85",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        " Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "86",
      "title": "A survey on vision transformer",
      "authors": [
        "K Han",
        "Y Wang",
        "H Chen",
        "X Chen",
        "J Guo",
        "Z Liu",
        "Y Tang",
        "A Xiao",
        "C Xu",
        "Y Xu"
      ],
      "year": "2022",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "87",
      "title": "Most important person-guided dual-branch crosspatch attention for group affect recognition",
      "authors": [
        "H Xie",
        "M.-X Lee",
        "T.-J Chen",
        "H.-J Chen",
        "H.-I Liu",
        "H.-H Shuai",
        "W.-H Cheng"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "88",
      "title": "Analyzing group-level emotion with global alignment kernel based approach",
      "authors": [
        "X Huang",
        "A Dhall",
        "R Goecke",
        "M Pietik채inen",
        "G Zhao"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "89",
      "title": "Understanding the behaviour of contrastive loss",
      "authors": [
        "F Wang",
        "H Liu"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "90",
      "title": "A self-fusion network based on contrastive learning for group emotion recognition",
      "authors": [
        "X Wang",
        "D Zhang",
        "H.-Z Tan",
        "D.-J Lee"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "91",
      "title": "Group affect prediction using multimodal distributions",
      "authors": [
        "S Shamsi",
        "B Singh",
        "M Wadhwa"
      ],
      "year": "2018",
      "venue": "2018 IEEE Winter Applications of Computer Vision Workshops"
    },
    {
      "citation_id": "92",
      "title": "Emotiw 2016: Video and group-level emotion recognition challenges",
      "authors": [
        "A Dhall",
        "R Goecke",
        "J Joshi",
        "J Hoey",
        "T Gedeon"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "93",
      "title": "A deep look into group happiness prediction from images",
      "authors": [
        "A Cerekovic"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "94",
      "title": "Exploring regularizations with face, body and image cues for group cohesion prediction",
      "authors": [
        "D Guo",
        "K Wang",
        "J Yang",
        "K Zhang",
        "X Peng",
        "Y Qiao"
      ],
      "year": "2019",
      "venue": "2019 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "95",
      "title": "Group-level cohesion prediction using deep learning models with a multi-stream hybrid network",
      "authors": [
        "T Dang",
        "S.-H Kim",
        "H.-J Yang",
        "G.-S Lee",
        "T.-H Vo"
      ],
      "year": "2019",
      "venue": "2019 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "96",
      "title": "Automatic group cohesiveness detection with multi-modal features",
      "authors": [
        "B Zhu",
        "X Guo",
        "K Barner",
        "C Boncelet"
      ],
      "year": "2019",
      "venue": "2019 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "97",
      "title": "Dynamics of group-based emotions: Insights from intergroup emotions theory",
      "authors": [
        "E Smith",
        "D Mackie"
      ],
      "year": "2015",
      "venue": "Emotion Review"
    },
    {
      "citation_id": "98",
      "title": "Local and global perception generative adversarial network for facial expression synthesis",
      "authors": [
        "Y Xia",
        "W Zheng",
        "Y Wang",
        "H Yu",
        "J Dong",
        "F.-Y Wang"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "99",
      "title": "Generative emotional ai for speech emotion recognition: The case for synthetic emotional speech augmentation",
      "authors": [
        "S Latif",
        "A Shahid",
        "J Qadir"
      ],
      "year": "2023",
      "venue": "Applied Acoustics"
    },
    {
      "citation_id": "100",
      "title": "Emotion-aware multi-view contrastive learning for facial emotion recognition",
      "authors": [
        "D Kim",
        "B Song"
      ],
      "year": "2022",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "101",
      "title": "Multimodal emotion recognition with capsule graph convolutional based representation fusion",
      "authors": [
        "J Liu",
        "S Chen",
        "L Wang",
        "Z Liu",
        "Y Fu",
        "L Guo",
        "J Dang"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    }
  ]
}