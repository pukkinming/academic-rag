{
  "paper_id": "2208.00877v2",
  "title": "Self-Supervised Group Meiosis Contrastive Learning For Eeg-Based Emotion Recognition",
  "published": "2022-07-12T21:20:12Z",
  "authors": [
    "Haoning Kan",
    "Jiale Yu",
    "Jiajin Huang",
    "Zihe Liu",
    "Haiyan Zhou"
  ],
  "keywords": [
    "EEG-based emotion recognition",
    "group-level representation",
    "contrastive-learning",
    "self-supervised learning",
    "data augmentation",
    "meiosis"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The progress of EEG-based emotion recognition has received widespread attention from the fields of human-machine interactions and cognitive science in recent years. However, how to recognize emotions with limited labels has become a new research and application bottleneck. To address the issue, this paper proposes a Self-supervised Group Meiosis Contrastive learning framework (SGMC) based on the stimuli consistent EEG signals in human being. In the SGMC, a novel geneticsinspired data augmentation method, named Meiosis, is developed. It takes advantage of the alignment of stimuli among the EEG samples in a group for generating augmented groups by pairing, cross exchanging, and separating. And the model adopts a group projector to extract group-level feature representations from group EEG samples triggered by the same emotion video stimuli. Then contrastive learning is employed to maximize the similarity of group-level representations of augmented groups with the same stimuli. The SGMC achieves the state-of-the-art emotion recognition results on the publicly available DEAP dataset with an accuracy of 94.72% and 95.68% in valence and arousal dimensions, and also reaches competitive performance on the public SEED dataset with an accuracy of 94.04%. It is worthy of noting that the SGMC shows significant performance even when using limited labels. Moreover, the results of feature visualization suggest that the model might have learned video-level emotionrelated feature representations to improve emotion recognition. And the effects of group size are further evaluated in the hyper parametric analysis. Finally, a control experiment and ablation study are carried out to examine the rationality of architecture. The code is provided publicly online 1 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotion plays a crucial role in human cognition and involves many application fields. For example, in the field of human-machine interaction  [1] , emotion recognition enables *Corresponding author 1 https://github.com/kanhaoning/Self-supervised-group-meiosis-contrastivelearning-for-EEG-based-emotion-recognition the machine to provide more humanized interaction. In consumer neuroscience, emotion analysis is a common tool to obtain the user experience for product design  [2] . Recently, the method of emotion recognition based on Electroencephalography (EEG) signal has shown its advantages. Compared to conscious behavior signals such as facial expression and body language, the EEG has the advantage of being difficult to hide or disguise. Compared with other physiological signals such as the fMRI (functional magnetic resonance imaging), and ECG (Electrocardiogram), the EEG is more convenient for sampling and has a higher time resolution.\n\nThere is great progress in the field of EEG-based emotion recognition. With traditional machine learning techniques, the handcrafted features are calculated and selected carefully, which is quite critical during emotion recognition. While these approaches relie too much on the researcher's experiences on EEG signals and cognitive related knowledge. In recent years, the development of deep learning methods achieves competitive accuracy, which could not pay attention to the handcrafted features. With the guidance of a large number of data with labels, the deep learning models would learn high-level emotion-related feature representation for precise affective computing  [3] -  [7] .\n\nGenerally, artificial labels are crucial for training deep learning models based on the common supervised methods. But there is some condition requiring higher accurate and real-time recognition, obtaining qualified labels is expensive. For example, in neuroscience, EEG is frequently used to explore the process of emotion, such as in the tasks of empathy and reading comprehension. Participants are usually required to answer questions, so that the researchers could get their emotion situations. However, the emotion labels obtained through this way are time-consuming and laborious. And it is easy to generate subjective bias, which might decrease the reliability of labels  [8] ,  [9] . Similarly, in the application of consumer neuroscience, the EEG signals are recorded to evaluate the participants' emotional states while they are playing games, listening to music, and watching movies, and advertisements, which aims to provide instructive references to the content creator,  [10] -  [13] . In these conditions, the precise and time-intensive labels are also required. Therefore, the lack of qualified labels hinders the application of machine learningbased models in many precise fields.\n\nPrevious studies have explored to reduce the dependence on artificial labels  [14] ,  [15] . Several neuroscience studies have shown the exploitable consistency of stimuli in emotion EEG signals. They have discovered that the EEG signals among a group of subjects who watched the same emotional video clips share similar group-level stimuli-related features  [16] ,  [17] . Such features correlated with preference, arousal, valence, etc, are potential to make up for the lack of artificial labels. Existing methods mainly adopt the self-supervised learning (SSL) method to exploit such stimuli consistency. SSL can generate labels according to the attributes of data for learning. For example, shen et al. proposed a novel contrastive learning framework  [18]  to learn representation by making the model maximizing the similarity between representations of EEG signals corresponding to the same stimuli. However, there exist random effects in the emotion-related EEG signals. For example, whether the subjects were distracted during the emotional tasks and their fatigue situations would increase the noise of the signals. And also the responses of participants could not be totally the same, which increases the difficulty in maximizing the similarity across subjects in contrastive learning.\n\nTo further improve the EEG-based emotion recognition under the SSL framework, we proposed a Self-supervised Group Meiosis Contrastive learning (SGMC) framework for EEG-based emotion recognition.\n\nFirst, since larger samples could be better to represent the characteristics of signals from the view of statistics, we design a group projector in SGMC to collect a group of EEG samples to extract group-level representation for contrastive learning.\n\nSecond, we proposed a novel method of data augmentation to provides augmented group samples for contrastive learning. Applying data augmentation to enhance contrastive learning is a basic paradigm, however, there are few studies on augmenting group samples. Inspired by the meiosis mechanism in genetics  [19] , we augment data without changing stimuli features by pairing, cross exchanging, and separating. In this way, data augmentation enables contrastive learning further take the advantage of the alignment of stimuli in the EEG signal group. And then the SGMC enables the model learn critical representations and achieve competitive emotion recognition performance with a significant improvement. Here we summarize the contributions of this paper as follows:\n\n• To reduce the dependence on emotion labels, we introduce a self-supervised contrastive learning framework to further exploit the consistency of stimuli for EEG-based emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work A. Eeg-Based Emotion Recognition",
      "text": "In earlier studies, emotional features of EEG signals were usually extracted to recognize by some traditional machine learning strategies. Such as the support vector machine (SVM)  [20] , Gaussian Naive Bayes classification  [21] , and k-nearest neighbor (k-NN)  [22]  are widely used classify emotion of the EEG signal.\n\nCompared with the traditional machine learning method, the deep learning model has more advantages in extracting high-level emotional features. In recent years, more and more deep learning neural networks based on emotion recognition models achieved good performance on EEG-based emotion recognition tasks  [3] -  [7] .\n\nRecently popular methods focus on recurrent neural networks (RNNs/LSTMs), and convolutional neural networks (CNNs). In 2017, Alhagry et al  [23]  adopted a two-layer long-short term memory (LSTM) to reach satisfactory emotion classification with the input of the raw EEG signals.\n\nIn 2020, Li et al  [24]  constructed model BiHDM adopted four RNN modules to capture the input of each hemispheric EEG electrode's data from horizontal and vertical streams and achieved the SOTA. CNN is also widely used for extracting spatial features of the EEG signal. In 2016, Li et al  [25]  proposed a hybrid network structure based on CNN and RNN for emotion recognition based on multi-channel EEG signals, which shown the effectiveness of a hybrid network in the trial-level emotion recognition tasks. In 2017, Alhagry et al.  [26]  explored a convolutional neural network and a simple deep neural network. This CNN model shown more significant performance and achieved the SOTA. In 2018 Shawky et al.  [27]  proposed a 3D CNNs model, which divides raw signals into 6-s segments to input. In the same year, Yang et al.  [28]  proposed a hybrid model combining CNN and RNN networks to learn spatial-temporal representation for emotion recognition. It utilized a sparse matrix as input to reflect the relative position of the electrodes. Compared with complex input of RNNs and 2D/3D CNNs, Cheah et al.  [29]  proposed a 1D-CNN based ResNet18, which adopted simple input(channel × time) to train the deeper neural network. It is more suitable to perform pre-training with simple data processing and a faster training process.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Self-Supervised Learning",
      "text": "Self-supervised learning aims to learn representation without relying on artificial labels. The latest research in the field of machine learning and deep learning shown the potential of the SSL method in learning generalized and robust representations  [30] -  [35] . SSL has been widely used in many fields. For example, in computer vision (CV), Gidaris et al.  [30]  based on spatial properties designed an SSL task to rotate the original image and require the model to predict the rotation angle. Based on the temporal properties of the video, an SSL task  [31]  was designed to require the model to predict whether the two video frames are close in time. In natural language processing (NLP), word2vec  [32]  designed SSL tasks such as predicting headword and adjacency words, etc. BERT  [33]  designed two SSL tasks masked language prediction and next sentence prediction, and achieved SOTA on 11 NLP tasks. In EEG signal processing, Zhang et al.  [36]  applied Generative Adversarial Network to design the SSL method. It makes the generator augment masked original signals to get simulated signals and requires the model to distinguish real and simulated signals, which alleviates the problem of EEG data scarcity and achieves SOTA.\n\nRecently contrastive-learning-based SSL has made progress in EEG signal processing. Contrastive learning defines any two samples with internal relations as the positive pair, otherwise, it is the negative pair, whose loss function aims to maximize the similarity of representations between positive pairs minimums the similarity of representation between negative pairs. Shen et al.  [18]  proposed a self-supervised contrastive learning framework CLISA to improve intersubjects prediction, which requires the model to predict whether two EEG signals are recorded when two subjects watch the same video clip. In this way, the model learned well inter-subject representation ability and achieved SOTA in inter-subject prediction after fine-tuning. In  [14]  several self-supervised contrastive learning methods were proposed to improve performance on limited label sample tasks. Among them, Relative Positioning (RP) requires the model to predict whether the two EEG signals are recorded in close time, and Contrastive Predictive Coding (CPC) requires the model to predict the representation of adjacent EEG signals via the anchor signal. They confirmed that models learned physiologically and clinically meaningful feature representations by SSL pre-training without label guidance. Further, they fine-tuned the pre-trained model to significantly outperformed the fully-supervised baseline on less labeled sample learning tasks. In  [15]  an augment-based SSL method is proposed, which requires the model to predict whether two augmented EEG signals come from the same original signal. It applies classical data augmentation such as time warping permutation and crop&resize and so on. The generalization ability of the model has significantly improved and exceeded fully-supervised learning in both the full and the limited labeled sample learning on sleep staging. Contrastive learning shows its excellence in improving inter-subject prediction, learning physiological feature representation without labels, and so on in EEG signal processing.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. Proposed Method",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Overall Framework",
      "text": "This paper designs a Self-supervised Group Meiosis Contrastive learning (SGMC) framework for EEG-based emotion recognition. As illustrated in Fig.  1  the proposed framework consists of a contrastive learning pre-training process and an emotion recognition fine-tuning process. In the pre-training process, SGMC contains five components: a group sampler, the Meiosis data augmentation, a base encoder, a group projector, and a contrastive loss function. Firstly, the group sampler generates a minibatch containing several groups of EEG signals for augmenting. Secondly, the Meiosis augments each EEG group to generate two groups for constructing the positive and negative pairs. Nextly the base encoder extracts individual-level stimuli-related representations from each EEG signal. Then the group projector aggregates each group of representations to extract group-level stimuli-related representations and map them into another latent space for computing the similarity. Together, the parameters of the base encoder and group projector are optimized by minimizing the contrastive loss. In the fine-tuning process, the model that consisted of the pre-trained base encoder and initialized classifier performs the emotion classification training.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Group Sampler",
      "text": "Generally, it is difficult to contrastive learning through extracting stimuli-related features from individual EEG samples. So we take the strategy of extracting from group EEG samples, to achieve it we construct the sampler to provide input for the minibatch.\n\nIn the processed dataset, video clips and subjects correspond to two axes of the dataset tensor. Among it, each EEG sample was defined as X s v ∈ R M×C , corresponding to a 1second signal recorded when subject s watched a 1-second video clip v, where M is the number of times samples and C is the dimension of signals (e.g.,channels). To obtain a minibatch, illustrated in Fig.  2\n\nvi , X s2 vi , ..., X s 2Q vi } corresponding to the video clip v i . Among G i , each individual sample shared the similar -related features. In this way, sampler would provide the minibatch with P group samples {G 1 , G 2 , ..., G P } corresponding to P different stimuli for pre-training.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Meiosis Data Augmentation",
      "text": "Meiosis aims to augment one group sample to generate two groups that preserve the same stimuli-related features by utilizing the alignment of stimuli in the EEG group for constructing the positive pair.\n\nTo increase the meaningful difficulty of the model decoding EEG samples, we hope to mix signals of different subjects. Moreover, to preserve the original stimuli-related features for extraction by SGMC, we select the signals corresponding to\n\nFurthermore, to take the advantage of the diversity of group combinations, we can randomly pair for crossover and separating. As illustrated in Fig.  3  the overall Meiosis data augmentation can be designed as follows: 1) Individual pairing: For one original EEG signals group\n\n2) Crossover : Meiosis receives a randomly given split position c to perform transformation (1) for each pairs to obtain {{ X\n\n.., 2Q} can be obtained that sharing the similar group-level stimuli-related features.\n\nSuch data augmentation for group sample we represent it as follows function expression:\n\nWhen Meiosis is built, for one minibatch of P group samples G, 2P group samples G can be obtained as follows:\n\n, form negative pairs with any other 2(P -1) group samples .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Base Encoder",
      "text": "To extract group-level stimuli-related features for contrastive learning, we fisrt design a base encoder to extract individuallevel stimuli-related features from each individual EEG sample. We introduce the base encoder f : R M×C → R D which map individual EEG sample X to its representation h on a 512-dimensional feature space. Based on the existing model ResNet18-1D  [29] , the base encoder is designed as follows:\n\nAs illustrated in Fig.  4 . It mainly contains 17 convolutional layers (Conv) with a 1D kernel. The kernels of the first convolutional layer parallel the time axis of the EEG signal tensor with a length of 9. Each residual block contains two convolutional layers with the same number and length of the kernels. In each residual block, kernels of the first layer parallel the time axis of the input EEG tensor, and the second layer parallels the channel axis. For the eight residual blocks, the length of the kernels is 15, 15, 11, 11, 7, 7, 3, and 3 in descending order. Max pooling with the 1D kernel (Maxpool), Avg pooling with the 1D kernel (Avgpool), Batch Normalization (BN), and Rectified Linear Unit (RELU) layers are shown in the corresponding positions in the figure . \nThrough the base encoder, for a augmented group sample G t i , its individual-level stimuli-related representation set {h 1 , h 2 , ..., h Q } can be obtained as by:\n\nThe set is used for further extracting group-level features.\n\nThe individual representations can also be used for extracting emotional features for emotion classification.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "E. Group Projector",
      "text": "The group projector aims to accurately project stimulirelated representations into latent space from just 1-second EEG signals for calculating the similarity of video clip stimuli. To alleviate the hinders in extracting stimuli-related features from individual samples ( fatigue, distraction, etc), the group projector is designed to extract group-level features from multiple samples.\n\nA group of samples is an unordered set of matrixes that lacks a special extraction method. Most models focus on regular input representations. Such as the input of multichannel images, there is a fixed order between different channels, as well as video, there is a fixed sequence between different frames. In the problem of unordered point cloud classification,  [37]  proposed PointNet adopting the symmetric function to build a network realized the features extraction of the unordered point cloud. Inspired by it, we adopted a symmetric function to design a model suitable for extracting features from group EEG signals.\n\nAs illustrated in Fig.  4  we designed the group projector consisting of a base projector and symmetric function MaxPool1D.\n\nTo mitigate individual feature loss, the dimension of individual representation can be upgraded for extraction. We introduce the base projector l : R D → R H that adopt a multilayer perceptron (MLP) to project each individual representation h on a 4096-dimensional feature space. The base projector contains three fully-connected layers with 1024, 2048, and 4096 hidden units in ascending order and adopt ReLU as the activation function of the first two layers. Batch Normalization and Dropout with 0.5 are shown in the corresponding positions in the figure . \nTo ensure an invariant output to represent the group sample with any input permutations, 1-dimension max-pooling (Max-Pool1D) is adopted to aggregate the information from each dimension-upgraded representation. As illustrated in Fig.  4 , the 1D kernel of MaxPool1D is perpendicular to the dimensionupgraded representation vector. The scanning direction of the kernel is parallel to upgraded representation vector with a stride of 1, and the padding is 0. Such MaxPool can extract the maximum values on 4096 feature dimensions from Q dimension-upgraded representations to obtain the group-level feature representation in latent space.\n\nWe note group projector as g : R Q×D → R H . Extracted group represetation in latent space can be obtained through g as follows:\n\nIn the emotion classification fine-tuing task, we use the classifier to extract emotional features and predict emotion labels from the representations extracted by the base encoder. As illustrated in Fig.  4  the classifier mainly contains three fully-connected layers with 512, 256, and 128 hidden units in descending order. Batch Normalization ReLU and Dropout with 0.5 are shown in the corresponding positions in the figure.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "G. The Contrastive Loss",
      "text": "To measure the similarity of group-level stimuli-related features between two group samples, we can calculate the cosine similarity of their group representation vectors.\n\nThe contrastive loss is designed to maximize the similarity of two group-level representations of groups sharing the same stimuli label in a positive pair. Similar to the SimCLR framework  [38] , we adopt the normalized temperature-scaled cross-entropy to define loss function as follows:\n\nwhere 1 [j =i] ∈ {0, 1} is an indicator function equaling to 1 if j = i. τ is the temperature parameter of softmax. The smaller the loss function is, the larger similarity between z A i and z B i , and the smaller the similarity between z A i and other group representations come from the same minibatch.\n\nFinally, the total loss for an iteration is the average of all contrastive losses for backpropagation as follows:",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "H. Pre-Training Process",
      "text": "Based on the constructed group sampler, data augmentation, base encoder, group projector, and loss function the SGMC pre-training can be performed.\n\nIn a pre-training, we first set a number of epochs T 1 , and then iterate the epoch. In Nextly for the Meiosis data augmentation, to avoid the model cheating by recognizing the split position, we randomly generate a fixed split position c, sent it to each time of Meiosis in this iteration (1 < c < M -1). 2Q augmented group samples G = { G t i |i = 1, 2, ..., P ; t ∈ {A, B}}} can be obtained by  (3) . Further we extract group-level features and project them to latent space to obtain group representations by (  4 ) and  (5) . Furthermore, we calculate loss L by (  6 )-  (8) . Finally, we abate loss L by backpropagation to calculate the gradient for optimizer updating parameters of f and g. Detailed procedures are summarized in Algorithm 1.   Randomly generate a split position c.  Obtain Z = {z t i |i = 1, 2, ..., P ; t =∈ {A, B}} from G through f and g by (  4 ) and (5).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "9:",
      "text": "Calculate loss L by (  6 )-(  8 ).",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "10:",
      "text": "Abate loss L through optimizer updating parameters of f and g.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "11:",
      "text": "until all video clips are enumerated. 12: end for Output: base encoder f , throw away group projector g.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "I. Fine-Tuning Process",
      "text": "To achieve excellent emotional classification performance, based on learned feature representations we further fine-tune the model with labeled samples. As illustrated in Fig.  1  emotion classification supervised training is performed on the model consisting of an initialized classifier and the SGMC pre-trained base encoder.\n\nWe denote the training data as X and their labels as y. We denote the classifier as k(•) The label y is a categorical variable. For example, if there are four emotional categories, y can take four values: 0, 1, 2 or 3. We need to predict the emotion category y for each sample X ∈ R M×C . The pretrained base encoder f extracts the representation from original EEG signal X for classifier k(•) extract predictive features to obtain prediction category y pre = k(f (X)). We apply the cross entropy function to define the loss function for the emotion classification task and apply an optimizer to minimize the loss function to optimize the parameters of the model. Finally, when the loss function converges, a predictive EEGbased emotion recognition model is obtained.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Iv. Experiments",
      "text": "In this section, we introduce the implementation detail on the DEAP and SEED dataset and our experiment evaluation. In our experiment, we verify the effectiveness by comparing the SGMC with other competitive methods of emotion recognition and evaluating its performance on limited labeled sample learning. Further, we explore the reason for the effectiveness by visualizing the feature representation learned by the SGMC. Moreover, we explore the meaningful law of the framework by evaluating the different combinations of hyper parameters. Furthermore, we verify the rationality of architecture design by conducting control and ablation experiments.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Implementation Detail",
      "text": "In this section, we elaborate on our implementation detail of the dataset, data processing, and basic hyper parameters utilized in the experiments.\n\n(1) Dataset DEAP: The widely-used DEAP dataset  [21]  includes 32channel EEG signals and 8-channel peripheral physiological signals recorded by 32 subjects when watched 40 pieces of a one-minute music video. Each trial data was recorded under 3-seconds of resting state and 60-seconds of stimuli. The recorded EEG signals are down-sampled to a 128 Hz sampling rate and processed with a bandpass frequency filter from 4-45 Hz by the provider. After watching each video, subjects were asked to rate their emotional levels of arousal, valence, liking, and dominance from 1 to 9 for each video. We adopt the EEG signals and rating values of arousal and valence to perform emotion recognition. We set the threshold value of the rating value of arousal and valence at 5. When the rating value is more than 5.0, the corresponding EEG signals are labeled as high arousal or valence. Otherwise, it is labeled as low arousal or valence. Each EEG signal corresponds to valence and arousal two labels, which can be used to construct two or four classification tasks. SEED: The SEED dataset is widely used in emotion recognition algorithms  [39] . The dataset recorded the EEG signals from 15 subjects when watching 15 videos selected from movies in three categories of emotions, including positive, neutral, and negative. Each video is about 4 minutes long. Each subject repeated the experiments for three sessions, with an interval of more than one week. The EEG signals were recorded via 62 electrodes at a sampling rate of 1000Hz and have been downsampled to 200 Hz and filtered from 0 to 75 Hz by the provider.\n\n(2) Data Process\n\nOn the DEAP, we use a 1-second-long sliding window to separate the 63s signal of each trial into 63 non-overlapping EEG signal segments. To improve accuracy, following existing work  [28]  we reduce the 3s resting state EEG signals from the 60s emotional stimuli EEG signal. In detail, in each trial, we average the 3s baseline EEG signal segments to get a 1s average baseline EEG signal segment. The remaining 60 segments each subtract the average baseline segment to become input samples. All samples correspond to a total of 2400 (40 videos with 60-seconds-long) repeated 1-secondlong video clips. 1680, 320, and 320 1-second video clips are randomly divided into three sets from 2400 video clips in the ratio of 70:15:15. These three sets of video clips that were watched by 32 subjects correspond to 53760, 11520, and 11520 (70:15:15) EEG segments which are used as the training set, testing set, and validation set respectively.\n\nOn SEED, we first perform an L2 normalization for each trial of EEG signal in each channel. Similar to the DEAP dataset we divide movie videos into 1-second windows. Because the length between the trial videos is different, we segment adjacent windows from front to back according to the time axis until the coverage of windows exceeds the video range. 3394 video clips are obtained from 15 movie videos and randomly divided into 2734, 510, and 510 clips , which three sets of video clips are in the ratio of 70:15:15. These three sets of video clips that were watched by  15",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "(3) Basic Configuration",
      "text": "To accurately evaluate the performance of emotion recognition for a pre-training framework, there are two steps we adopted for evaluating the results. We first save pre-trained models with the different epochs. Next, we select the model with the highest average accuracy on emotion recognition obtained by five times of fine-tuning. Such average accuracy is evaluated as the result.\n\nTo speed up sampling, in the pre-training process we set the five axes of dataset tensor to correspond to video clip, subject, 1, channel, sampling point respectivey. In the finetuning process, the first two axes video clip and subject of the dataset are reshaped into a sample axis. Each axis of reshaped dataset corresponds to sample, 1, channel, sampling point in turn. In the pre-training task, each epoch traverses every video clip of the dataset, a fine pre-training task generally needs to train more than 2000 epochs. To reduce the workload, we use the validation dataset to adjust the hyper parameters of the SGMC framework and use the test dataset to evaluate the model. The tensor shape of the training set, testing set, and validation set are represented as Shape tr , Shape te , and Shape val and are listed in Table  I .\n\nIn this paper, we use PyTorch  [40]  to implement our experiments based on the NVIDIA RTX3060 GPU. The Adam optimizer  [41]  is used to minimize the loss functions for both the pre-training and fine-tuning process. We represent lr as the learning rate of the optimizer. In the pre-training process and fine-tuning process, the number of epochs, batch size, the temperature parameter τ , learning rate lr, number of video clips per iteration P , number of samples per group Q, and size of the tensor of the dataset have applied different values, as shown in Table  I , we list all hyper parameters utilized in two processes on DEAP and SEED dataset.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "B. Emotion Classification Performance (1) Performance On Deap",
      "text": "As illustrated in Table  II , On the DEAP dataset, We first compare the SGMC with four state-of-the-art methods in the two emotion dimensions of valence and arousal: one residual long short-term memory network utilizing multi-modal data MMResLSTM  [43] , a channel-fused dense convolutional network CDCN  [42] , and a hybrid network of convolutional neural networks and recurrent networks with a channel-wise attention mechanism ACRNN  [44] . From Table  II , it can be found that the accuracy of the proposed SGMC is 1% higher than the second in the valence dimension and 2.3% higher than in the arousal dimension. The comparison results demonstrate the effectiveness of the SGMC on EEG-based emotion recognition.\n\nTo verify the effectiveness of the proposed framework in the data augmentation and self-supervised learning fields, we further compare the SGMC with a GAN-based data augmentation method MCLFS-GAN  [45]  and a self-supervised GAN-based data augmentation framework GANSER  [36] . Especially, according to the experimental setting of MCLFS-GAN  [45]  and GANSER  [36] , we further performe a comparison on a fourcategory classification problem: distinguishing EEG signals of four categories: high valence and high arousal, high valence and low arousal, low valence and high arousal, and low valence and high arousal. In Table  II , it can be found that the proposed method outperforms the existing data augmentation and selfsupervised learning method over 11.33% and 2.09% on four-",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Method",
      "text": "Valence Arousal Four CNN-LSTM (2020)  [28]  90.82 86.13 -CDCN (2020)  [42]  92.24 92.92 -MMResLSTM (2019)  [43]  92.87 92.30 -ARCNN (2019)  [44]  93    Furthermore, we first compare the proposed SGMC with our own fully-supervised baseline using the same network model without pre-training. In valence, arousal, and four-category dimensions, the accuracy of the SGMC exceedes the fullysupervised baseline over 3.49% 3.32% and 4.97%, which shows the significant effect of improving emotion recognition.\n\n(2) Performance on SEED As illustrated in Table  III , Similar to the DEAP, we first compare our proposed SGMC with four fully-supervised stateof-the-art studies : GRSLR  [46]  adopting a graph regularized sparse linear regression model, BiHDM  [24]  utilizing two independent recurrent networks for the left and right hemispheres of the brain, DGCNN  [47]  adopting a dynamic graph convolutional neural network, and a 1D CNN-based residual neural network ResNet18  [29] . Results use accuracy in the    III  the proposed SGMC outperforms the four state-of-the-art studies, reflecting its good emotion recognition performance on the SEED. Further, we compare the SGMC with our fully-supervised baseline using the same model. Especially, the SEED dataset has nearly five times the data volume of the DEAP dataset. Therefore, it can better reflect the performance of self-supervised learning by utilizing a large number of unlabeled samples to make up for scarce artificial accurate labels. We report results obtained from fine-tuning with four various percentages of the total training set labeled samples (based on pre-training on the full training set). From 1% to 50% percentage of labeled samples, the SGMC exceeds our fully-supervised baseline over 44.84%, 33.52%, and 8.24%. Such results show the proposed SGMC can take advantage of consistency of stimuli to significantly make up for artificial accurate labels. Using the full training set labeled samples to fine-tune, the SGMC significantly exceed our fully-supervised baseline over 4.27% as well. This shows the SGMC contributes a significant improvement by utilizing large unlabeled data.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "C. Performance On Limited Labeled Sample Learning",
      "text": "Based on the above results on SEED, it can be found that fewer labeled samples can also lead to good results. To evaluate the performance on limited labeled sample learning, we further evaluate the results on DEAP and SEED when the number of labeled samples per category increasing. We adopte a model based on SGMC pre-trained with the full training set and an initialized model to compare their performance on fine-tuning/fully-supervised learning with the same limited labeled sample. On the DEAP the results adopt a four-category  we can find that in any amount of labeled samples regimes, the accuracy of the SGMC fine-tuning is significantly superior to the fully-supervised baseline, and it is more significant in the lower labeled samples regime. On the DEAP dataset, when the number of labeled samples per category is over 10, the performance of the SGMC significantly outperforms the supervised. When fine-tuned with 5000 labeled samples per category (37.2% of the full training set), the SGMC reaches a good accuracy of 87.51% which is nearly by 87.68% of fully-supervised accuracy training with the full training set. On the SEED dataset, when fine-tuned with even only one labeled sample per category (0.00278% of the training set), the SGMC reached an accuracy of 59.42%. When fine-tuned with 50 samples per category (0.14% of the training set), the accuracy of 91.01% outperforms the fully-supervised baseline with 100% labeled data. Further, we observe that when the number of category is over 500, the curve has converged. This shows the SGMC enables a significant decline in the demand for artificial labels and reflects the consistency of stimuli have been well exploited to make up for artificial labels.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "D. Representation Visualization",
      "text": "To explore how SGMC contributes to superior performance on emotion recognition, we visualize the learned feature representations of the SGMC fine-tuned model and the only fully-supervised model.\n\nAs illustrated in Fig.  8 , the 512-dimension feature representations extracted by the base encoder from the samples of the full SEED testing set are projected to two dimensions through t-SNE  [48] . In the figure above, 15 colors represent samples corresponding to the 15 trial video clips (about 4-minutes). It can be found that in the visualization of the SGMC finetuned (right), the feature representation of the same video clip  tends to gather together to form 15 distinguishable groups. On the contrary, in the visualization of fully-supervised (left), the representations corresponding to the different video clips cannot be distinguished significantly. Visualization reveals that the SGMC not only learns stimuli-related feature representations but also enables the model to distinguish whether different stimuli come from a continuous video. Further, we mark the corresponding emotion labels with three colors in the figure below. There are more indistinguishable representations with different emotion labels mixed together in fully-supervised visualization (left). In the SGMC fine-tuned visualization (right), there are fewer feature representations with the different emotion labels mixed together and shows better emotional discrimination. It reflects that the SGMC enables the model to learn the video-level stimuli-related representation to improve emotion recognition performance.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "E. Effect Of Hyper Parameters",
      "text": "To explore the effect of the number of samples per group (Q) and the number of selected video clips per iteration (P) on contrastive learning, we evaluate various combinations of hyper parameters. In our experiment strategy, each given Q, we evaluate various P including 2, 4, 8, 16, 32, 64, and select the one that achieves the best result on emotion recognition as the appropriate P for given Q. The results of the different Q on emotion recognition are illustrated in Fig.  11 . The appropriate P and number of epochs of pre-training, and corresponding pre-training accuracy of the different Q are reported in Table  IV  .\n\nOn the DEAP, When Q = 2 and P = 4, the SGMC achieves the best performance. On the SEED, when Q = 2 and P = 16, the SGMC achieves the best performance.  Further, it can be observed that an opposite law exists in the DEAP and SEED datasets. When given a larger Q, the appropriate P on the DEAP tends to be smaller, and on the SEED tends to be larger. The possible reason is the difference in labeling between the two datasets. On the SEED, the emotional labels are labeled by the experiment designer, which is determined by the emotional attribute of the video stimuli. On the DEAP dataset, emotional labels are labeled by the rating of the subjects. Such labeling is more related to the personalized differences of the subject than to the SEED. And because the larger P , the more difficult the contrastive learning is. At the time the model is more encouraged to focus on extracting stimuli-related features and ignore the personalized features that are irrelevant stimuli. So the larger P lead to better results on the SEED and hinders better results on the DEAP. This indicates that a smaller P should be considered first to use when the data was labeled by the subject, and a larger P should be considered first to use when the data was labeled by the emotional attributes of the stimuli.\n\nFurthermore, it can be found that generally the greater the Q (when P are constant), the greater the accuracy of pre-",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "F. Archtechture Design Analysis",
      "text": "In this section, we validate our designed choices by control and ablation experiments. We first verify the rationality of the symmetric function we choose. Furthermore, we evaluate the rationality of the strategy of constructing the group sample, utilizing Meiosis augmentation, and constructing the positivenegative pairs.\n\n(",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "1) Comparison With Various Symmetric Function",
      "text": "The SGMC selects the symmetric function MaxPool1D to construct the group projector. To verify its rationality, we compare MaxPool1D with a common similar AvgPool1D and an additional opposite MinPool1D which is implemented by taking the minimum value in each dimension of upgraded representations. Illustrate in Fig.  4 . and Table  IV  MaxPool1D is significantly better than others. The possible reason is that MaxPool1D is more beneficial for model selecting emotionrelated features to extract from upgraded feature representations. Although MinPool1D also has a selection ability, the features it selects are more detrimental to improving learning emotion-related representation. This verifies the rationality of using MaxPool1D to aggregating group features for contrastive learning.\n\n(",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "2) Ablation Study",
      "text": "To investigate the rationality of some novel designs of the architecture, we conduct an ablation study for these three components: group sample, Meiosis data augmentation, and stimuli consistency. We can get the new version by removing one or two components, and the evaluation strategy is consistent with the basic configuration. When the group sample is ablated, we use individual samples for contrastive learning (just let Q = 1). When Meiosis data augmentation  is ablated, for augmenting the group/individual samples we skip the crossover process and go directly into the separation process after completing individual pairing. After removing the stimuli consistency, we change the way of constructing the positive pair with samples sharing the same stimuli. Instead, the sampler is required randomly sample EEG signals with any stimuli to form the sample group for augmenting and constructing pairs. The results of the four-category classification on DEAP and three-category classification on SEED are reported in Fig.  11 . The detail of ablation and the number of epochs of pre-training are reported in Table VI.\n\nTo verify the effectiveness of group sample on the SGMC, we design a version Non-group by removing the group sample. It can be observed that the emotion recognition performance significantly declined on DEAP and SEED by more than  The proposed framework achieves state-of-the-art emotion recognition results on the DEAP, and also reaches competitive performance on the SEED datasets. Compared to the fullysupervised baseline, the SGMC improves emotion recognition significantly, especially when there are limited labels. In addition, the results of feature visualization suggest that the model might have learned the video-level feature representations, and improves the performance of the model. The hyper parametric analysis further demonstrates the role of group samples during emotion recognition. Finally, the rationality of the framework design including the selection of symmetric functions, the construction of the positive-negative pairs, and Meiosis data augmentation are verified.\n\nIn the future, we will continue to develop such kinds of group-sample-based SSL frameworks while with low calculation costs.",
      "page_start": 11,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Illustration of the proposed SGMC. During the process of pre-training,",
      "page": 3
    },
    {
      "caption": "Figure 2: The illustration of sampling for a minibatch. Sampler ﬁrst samples",
      "page": 4
    },
    {
      "caption": "Figure 3: The illustration of the Meiosis data augmentation. A group of EEG",
      "page": 4
    },
    {
      "caption": "Figure 4: Details of the architecture of the base encoder, group projector,",
      "page": 5
    },
    {
      "caption": "Figure 5: The confusion matrix of classiﬁcation on DEAP",
      "page": 8
    },
    {
      "caption": "Figure 6: The confusion matrix of classiﬁcation on SEED",
      "page": 9
    },
    {
      "caption": "Figure 7: the results of the different number of labeled samples per",
      "page": 9
    },
    {
      "caption": "Figure 8: , the 512-dimension feature represen-",
      "page": 9
    },
    {
      "caption": "Figure 7: Learning effect of the labeled sample size for the emotion recognition. the left is for the result on DEAP, and the right is for SEED. The red line",
      "page": 10
    },
    {
      "caption": "Figure 8: t-SNE visualization for feature representations demonstrated on SEED",
      "page": 10
    },
    {
      "caption": "Figure 9: The group size effect on accuracy of DEAP (left) and SEED (right).",
      "page": 11
    },
    {
      "caption": "Figure 10: Emotion classiﬁcation accuracy based on fully-supervised model",
      "page": 12
    },
    {
      "caption": "Figure 11: Emotion classiﬁcation accuracy based on the ﬁve new versions fully-",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "…",
          "Column_3": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "91.70": "1.67",
          "2.83": "3.40",
          "2.64": "3.16"
        },
        {
          "91.70": "1.36",
          "2.83": "3.12",
          "2.64": "92.62"
        },
        {
          "91.70": "1.24",
          "2.83": "94.87",
          "2.64": "2.00"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "92.88": "4.54",
          "4.43": "93.58",
          "2.69": "1.88"
        },
        {
          "92.88": "2.26",
          "4.43": "2.26",
          "2.69": "95.49"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "F\nS",
          "Column_12": "ully-\nGM",
          "Column_13": "supe\nC",
          "Column_14": "rvise",
          "Column_15": "d"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "F\nS",
          "Column_12": "ully-\nGM",
          "Column_13": "supe\nC",
          "Column_14": "rvise",
          "Column_15": "d"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "DEAP Dataset SEED Dataset\n90\n80 )%( )%(\n80\nycaruccA ycaruccA\n70\n60\n60\n40 50\n1 10 50 100 1 10 50 100\nPercentage of labels (%) Percentage of labels (%)": "SGMC Mixup-augment Non-group Fully-supervised\nNon-consistent Non-augment Consistent-only"
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "U Cowie",
        "E Douglas-Cowie",
        "N Tsapatsoulis",
        "G Votsis",
        "S Kollias",
        "W Fellenz",
        "J Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal processing magazine"
    },
    {
      "citation_id": "2",
      "title": "Neuromarketing: the hope and hype of neuroimaging in business",
      "authors": [
        "I Ariely",
        "G Berns"
      ],
      "year": "2010",
      "venue": "Nature reviews neuroscience"
    },
    {
      "citation_id": "3",
      "title": "EEG emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "D Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2020",
      "venue": "Trans. Affective Computing"
    },
    {
      "citation_id": "4",
      "title": "An efficient LSTM network for emotion recognition from multichannel EEG signals",
      "authors": [
        "E Du",
        "C Ma",
        "G Zhang",
        "J Li",
        "Y.-K Lai",
        "G Zhao",
        "X Deng",
        "Y.-J Liu",
        "H Wang"
      ],
      "year": "2020",
      "venue": "Trans. Affective Computing"
    },
    {
      "citation_id": "5",
      "title": "EEGbased emotion recognition via channel-wise attention and self attention",
      "authors": [
        "Y Tao",
        "C Li",
        "R Song",
        "J Cheng",
        "Y Liu",
        "F Wan",
        "X Chen"
      ],
      "year": "2020",
      "venue": "Trans. Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "Emotion recognition based on high-resolution EEG recordings and reconstructed brain sources",
      "authors": [
        "F Becker",
        "J Fleureau",
        "P Guillotel",
        "F Wendling",
        "I Merlet",
        "L Al Bera"
      ],
      "year": "2020",
      "venue": "Trans. Affective Computing"
    },
    {
      "citation_id": "7",
      "title": "SparseDGCNN: Recognizing emotion from multichannel EEG signals",
      "authors": [
        "H Zhang",
        "M Yu",
        "Y.-J Liu",
        "G Zhao",
        "D Zhang",
        "W Zheng"
      ],
      "year": "2021",
      "venue": "SparseDGCNN: Recognizing emotion from multichannel EEG signals"
    },
    {
      "citation_id": "8",
      "title": "Understanding Human Reading Comprehension with brain signals",
      "authors": [
        "I Ye",
        "Z Xie",
        "Y Liu"
      ],
      "year": "2021",
      "venue": "Understanding Human Reading Comprehension with brain signals",
      "arxiv": "arXiv:2108.01360"
    },
    {
      "citation_id": "9",
      "title": "Assessment of Empathy in an Affective VR Environment using EEG Signals",
      "authors": [
        "F Alimardani",
        "A Hermans",
        "M Tinga"
      ],
      "year": "2020",
      "venue": "Assessment of Empathy in an Affective VR Environment using EEG Signals",
      "arxiv": "arXiv:2003.10886"
    },
    {
      "citation_id": "10",
      "title": "Automatic Recognition of Boredom in Video Games Using Novel Biosignal Moment-Based Features",
      "authors": [
        "B Giakoumis",
        "D Tzovaras",
        "D Moustakas"
      ],
      "year": "2011",
      "venue": "Trans. Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "Musical NeuroPicks: a consumer-grade BCI for on-demand music streaming services",
      "authors": [
        "L Kalaganis F P , Adamos",
        "D Laskaris"
      ],
      "year": "2017",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "12",
      "title": "Understanding Consumer Preferences for Movie Trailers from EEG using Machine Learning",
      "authors": [
        "H Pandey",
        "P Swarnkar",
        "R Kakaria"
      ],
      "year": "2020",
      "venue": "Annual Conference of Cognitive Science"
    },
    {
      "citation_id": "13",
      "title": "Engagement Estimation in Advertisement Videos with EEG",
      "authors": [
        "P Balasubramanian",
        "S Gullapuram",
        "S Shukla"
      ],
      "year": "2018",
      "venue": "Engagement Estimation in Advertisement Videos with EEG",
      "arxiv": "arXiv:1812.03364"
    },
    {
      "citation_id": "14",
      "title": "Uncovering the structure of clinical EEG signals with self-supervised learning",
      "authors": [
        "J Banville",
        "H Chehab",
        "O Hyvrinen"
      ],
      "year": "2021",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "15",
      "title": "Self-supervised Contrastive Learning for EEG-based Sleep Staging",
      "authors": [
        "I Jiang",
        "X Zhao",
        "J Du"
      ],
      "year": "2021",
      "venue": "International Joint Conference on Neural Networks"
    },
    {
      "citation_id": "16",
      "title": "Audience preferences are predicted by temporal reliability of neural processing",
      "authors": [
        "J Dmochowski",
        "M Bezdek",
        "B Abelson",
        "J Johnson",
        "E Schumacher",
        "L Parra"
      ],
      "year": "2014",
      "venue": "Nature communications"
    },
    {
      "citation_id": "17",
      "title": "Correlated Components of Ongoing EEG Point to Emotionally Laden Attention-A Possible Marker of Engagement?",
      "authors": [
        "J Dmochowski",
        "S Paul",
        "D Joao",
        "L Parra"
      ],
      "year": "2012",
      "venue": "Frontiers in Human Neuroscience"
    },
    {
      "citation_id": "18",
      "title": "Contrastive Learning of Subject-Invariant EEG Representations for Cross-Subject Emotion Recognition",
      "authors": [
        "H Shen",
        "X Liu",
        "X Hu"
      ],
      "year": "2022",
      "venue": "Trans. Affective Computing"
    },
    {
      "citation_id": "19",
      "title": "Disjunction of Homologous Chromosomes in Meiosis I Depends on Proteolytic Cleavage of the Meiotic Cohesin Rec8 by Separin",
      "authors": [
        "C Sara",
        "B Buonomo"
      ],
      "year": "2000",
      "venue": "Cell"
    },
    {
      "citation_id": "20",
      "title": "Eeg-based emotion recognition using recurrence plot analysis and k nearest neighbor classifier",
      "authors": [
        "D Bahari",
        "Amin Fatemeh",
        "Janghorbani"
      ],
      "year": "2013",
      "venue": "20th Iranian Conference on Biomedical Engineering (ICBME)"
    },
    {
      "citation_id": "21",
      "title": "DEAP: A database for emotion analysis using physiological signals",
      "authors": [
        "K Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2011",
      "venue": "Trans. Affective Computing"
    },
    {
      "citation_id": "22",
      "title": "EEG-based emotion recognition using frequency domain features and support vector machines",
      "authors": [
        "G Wang",
        "Xiao Wei",
        "Dan Nie",
        "Bao-Liang Lu"
      ],
      "year": "2011",
      "venue": "International conference on neural information processing"
    },
    {
      "citation_id": "23",
      "title": "Emotion recognition based on EEG using LSTM recurrent neural network",
      "authors": [
        "G Alhagry",
        "Aly Salma",
        "Reda Aly Fahmy",
        "El-Khoribi"
      ],
      "year": "2017",
      "venue": "Emotion"
    },
    {
      "citation_id": "24",
      "title": "A novel bi-hemispheric discrepancy model for eeg emotion recognition",
      "authors": [
        "B Li",
        "W Zheng",
        "L Wang",
        "Y Zong",
        "L Qi",
        "Z Cui",
        "T Zhang",
        "T Song"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "25",
      "title": "Emotion recognition from multi-channel EEG data through convolutional recurrent neural network",
      "authors": [
        "Y Li",
        "D Song",
        "P Zhang",
        "G Yu",
        "Y Hou",
        "B Hu"
      ],
      "year": "2016",
      "venue": "International Conference on Bioinformatics and Biomedicine"
    },
    {
      "citation_id": "26",
      "title": "Using deep and convolutional neural networks for accurate emotion classification on DEAP dataset",
      "authors": [
        "T Tripathi",
        "S Acharya",
        "S Sharma",
        "R Mittal",
        "S Bhattacharya"
      ],
      "year": "2017",
      "venue": "AAAI"
    },
    {
      "citation_id": "27",
      "title": "EEG-Based Emotion Recognition using 3D Convolutional Neural Networks",
      "authors": [
        "X Shawky",
        "R El-Khoribi",
        "M Shoman"
      ],
      "year": "2018",
      "venue": "International Journal of Advanced Computer Science and Applications"
    },
    {
      "citation_id": "28",
      "title": "Emotion recognition from multichannel EEG through parallel convolutional recurrent neural network",
      "authors": [
        "G Yang",
        "Q Wu",
        "M Qiu"
      ],
      "year": "2018",
      "venue": "International Joint Conference on Neural Networks"
    },
    {
      "citation_id": "29",
      "title": "Optimizing Residual Networks and VGG for Classification of EEG Signals: Identifying Ideal Channels for Emotion Recognition",
      "authors": [
        "K Cheah K H , Nisar",
        "H Yap"
      ],
      "year": "2021",
      "venue": "Journal of Healthcare Engineering"
    },
    {
      "citation_id": "30",
      "title": "Unsupervised Representation Learning by Predicting Image Rotations",
      "authors": [
        "I Gidaris",
        "S Singh",
        "P Komodakis"
      ],
      "year": "2018",
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "31",
      "title": "Time-Contrastive Networks: Self-Supervised Learning from Video",
      "authors": [
        "H Sermanet",
        "P Lynch",
        "C Hsu"
      ],
      "year": "2017",
      "venue": "International Conference on Robotics and Automation"
    },
    {
      "citation_id": "32",
      "title": "Distributed representations of words and phrases and their compositionality",
      "authors": [
        "E Mikolov",
        "I Sutskever",
        "K Chen",
        "G Corrado",
        "J Dean"
      ],
      "year": "2013",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "33",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "C Devlin",
        "J Chang",
        "M Lee"
      ],
      "year": "2018",
      "venue": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
    },
    {
      "citation_id": "34",
      "title": "Federated selfsupervised learning of multisensor representations for embedded intelligence",
      "authors": [
        "J Saeed",
        "F Salim",
        "T Ozcelebi",
        "J Lukkien"
      ],
      "year": "2020",
      "venue": "IEEE Internet of Things Journal"
    },
    {
      "citation_id": "35",
      "title": "Self-supervised learning for ecg-based emotion recognition",
      "authors": [
        "O Sarkar",
        "A Etemad"
      ],
      "year": "2020",
      "venue": "International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "36",
      "title": "GANSER: A Self-supervised Data Augmentation Framework for EEG-based Emotion Recognition",
      "authors": [
        "G Zhang",
        "Z Zhong",
        "S H Liu"
      ],
      "year": "2022",
      "venue": "Trans. Affective Computing"
    },
    {
      "citation_id": "37",
      "title": "PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation",
      "authors": [
        "S Qi C R",
        "H Su",
        "K Mo"
      ],
      "year": "2017",
      "venue": "Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "38",
      "title": "A simple framework for contrastive learning of visual representations",
      "authors": [
        "M Chen",
        "S Kornblith",
        "M Norouzi",
        "G Hinton"
      ],
      "year": "2020",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "39",
      "title": "Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks",
      "authors": [
        "W Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    },
    {
      "citation_id": "40",
      "title": "PyTorch: An imperative style, high-performance deep learning library",
      "authors": [
        "T Paszke",
        "S Gross",
        "F Massa",
        "A Lerer",
        "J Bradbury",
        "G Chanan",
        "T Killeen",
        "Z Lin",
        "N Gimelshein",
        "L Antiga"
      ],
      "year": "2019",
      "venue": "Conference and Workshop on Neural Information Processing Systems"
    },
    {
      "citation_id": "41",
      "title": "Adam: A method for tochastic optimization",
      "authors": [
        "R Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for tochastic optimization"
    },
    {
      "citation_id": "42",
      "title": "A channel fused dense convolutional network for EEG-based emotion recognition",
      "authors": [
        "U Gao",
        "X Wang",
        "Y Yang",
        "Y Li",
        "K Ma",
        "G Chen"
      ],
      "year": "2020",
      "venue": "Trans. Cognitive and Developmental Systems"
    },
    {
      "citation_id": "43",
      "title": "Emotion recognition using multimodal residual lstm network",
      "authors": [
        "L Ma",
        "H Tang",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2019",
      "venue": "ACM International Conference on Multimedia"
    },
    {
      "citation_id": "44",
      "title": "EEGbased emotion recognition via channel-wise attention and self attention",
      "authors": [
        "T Tao",
        "C Li",
        "R Song",
        "J Cheng",
        "Y Liu",
        "F Wan",
        "X Chen"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "45",
      "title": "Multi-reservoirs eeg signal feature sensing and recognition method based on generative adversarial networks",
      "authors": [
        "F Dong",
        "F Ren"
      ],
      "year": "2020",
      "venue": "Computer Communications"
    },
    {
      "citation_id": "46",
      "title": "Eeg emotion recognition based on graph regularized sparse linear regression",
      "authors": [
        "W Li",
        "W Zheng",
        "Z Cui",
        "Y Zong",
        "S Ge"
      ],
      "year": "2018",
      "venue": "Neural Processing Letters"
    },
    {
      "citation_id": "47",
      "title": "Eeg emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "R Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "48",
      "title": "Visualizing data using t-sne",
      "authors": [
        "S Van Der Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "49",
      "title": "mixup: Beyond Empirical Risk Minimization",
      "authors": [
        "H Zhang",
        "H Cisse",
        "M Dauphin"
      ],
      "year": "2018",
      "venue": "International Conference on Learning Representations"
    }
  ]
}