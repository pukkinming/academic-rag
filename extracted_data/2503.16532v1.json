{
  "paper_id": "2503.16532v1",
  "title": "Modelling Emotions In Face-To-Face Setting: The Interplay Of Eye-Tracking, Personality, And Temporal Dynamics",
  "published": "2025-03-18T13:15:32Z",
  "authors": [
    "Meisam Jamshidi Seikavandi",
    "Jostein Fimland",
    "Maria Barrett",
    "Paolo Burelli"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Accurate emotion recognition is pivotal for nuanced and engaging human-computer interactions, yet remains difficult to achieve, especially in dynamic, conversation-like settings. In this study, we showcase how integrating eye-tracking data, temporal dynamics, and personality traits can substantially enhance the detection of both perceived and felt emotions. Seventy-three participants viewed short, speech-containing videos from the CREMA-D dataset, while being recorded for eye-tracking signals (pupil size, fixation patterns), Big Five personality assessments, and self-reported emotional states. Our neural network models combined these diverse inputs-including stimulus emotion labels for contextual cues-and yielded marked performance gains compared to the stateof-the-art. Specifically, perceived valence predictions reached a macro F1-score of 0.76, and models incorporating personality traits and stimulus information demonstrated significant improvements in felt emotion accuracy. These results highlight the benefit of unifying physiological, individual and contextual factors to address the subjectivity and complexity of emotional expression. Beyond validating the role of user-specific data in capturing subtle internal states, our findings inform the design of future affective computing and human-agent systems, paving the way for more adaptive and cross-individual emotional intelligence in real-world interactions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition is a core topic in affective computing, enabling systems to interpret and respond to human emotions in a more personalized and context-sensitive manner. Traditional approaches often rely on analyzing static images of facial expressions or simplified emotion labels. However, genuine human interactions-even when one is merely observing a speaker-are dynamic and influenced by subtle cues and individual differences in personality  [1] [2] [3] .\n\nIn many real-world scenarios, one observes a talking face and makes inferences about what that speaker is feeling or expressing. This listener's perspective is critical, for instance, in teleconferencing or virtual agent setups, where users view speechcontaining video feeds and react based on perceived emotional cues. Although such contexts could be described as \"dialogue-like\", they are not always fully interactive with turn-taking. Our work focuses on approximating this listener scenario, in which participants watch short, speech-containing video clips from the CREMA-D dataset  [4] , whose stimuli were recorded by professional actors. By doing so, we align with real-life conditions where an observer is trying to interpret a speaker's emotions dynamically.\n\nDespite recent progress, current emotion recognition models face key challenges:\n\n• Temporal complexity. Many methods overlook the continuous, time-varying nature of facial cues in speech-based videos.\n\n• Influence of personality. Few studies ac-count for individual differences such as personality traits, which modulate how a person perceives emotional stimuli and how they feel in response  [5, 6] .\n\n• Distinguishing perceived vs. felt emotions.\n\nUsers may perceive certain emotional cues in the speaker, yet feel a different or more nuanced emotional response themselves.\n\nTo systematically address these points, we build on the crucial distinction among Expressed Emotions (E e ), Perceived Emotions (E p ), and Felt Emotions (E f ). As illustrated in Fig.  1 , when a human observer encounters emotional stimuli, they recognize the expressed emotion (E e ) and form an internal perceived label (E p ). In turn, they can experience a corresponding or even conflicting felt state (E f ). Our research targets both perceived and felt emotions, acknowledging that these two dimensions can diverge in real situations. In this paper, we address these gaps by integrating eye-tracking data, temporal modelling, and personality traits to predict E p and E f in a dynamic, speech-based setting. We conducted a comprehensive experiment with 73 non-actor participants, each watching clips from CREMA-D  [4]  (acted stimuli). We collected (1) detailed eye movements (e.g., fixations and pupil size), (2) selfreported personality profiles  [7, 8] , and (3) triallevel perceived and felt emotional responses. Using these modalities, we trained neural networks to capture the temporal progression of gaze patterns, the stable influence of personality, and the difference between perceived and felt emotion labels.\n\nOur contributions are threefold:\n\n1. Integration of Multimodal Data: We demonstrate how merging eye-tracking signals, personality traits, and temporal dynamics significantly enhances emotion recognition for talking-face scenarios.\n\n2. Insights into Personality's Role: We provide empirical evidence showing that personality traits modulate both how participants perceive others' emotions and how they feel in response, which can reduce model uncertainty and improve predictions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Advancement In Emotion Recognition:",
      "text": "We propose a multimodal neural approach that yields high predictive performance for both perceived and felt emotion categories, supporting future applications in humancomputer interaction, virtual agents, and broader affective computing contexts.\n\nBy incorporating the complexities of dynamic facial cues, user-specific personality factors, and the distinction between perceived and felt emotions, our work moves beyond static or purely acted setups. While this study does not encompass a fully interactive dialogue with turn-taking, it focuses on a critical component of many interactions-the observer's side-making it relevant to real-world systems where the user is frequently in a listening or monitoring role. We believe these findings pave the way for more adaptive and user-centred emotion recognition approaches that respect individual differences and capture emotional subtleties more effectively.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Background",
      "text": "Emotion recognition is a pivotal area in affective computing, focusing on enabling machines to detect and interpret human emotions under varied conditions. A wide range of studies have explored different modalities and methodologies, often obtaining impressive accuracy levels. However, many of these works tend to simplify emotion detection by focusing on specific arousal and valence scores or by employing highly controlled datasets. In practice, real-world emotion perception is affected by multiple, often interdependent factors such as personality traits, gaze patterns, and contextual cues. Consequently, several important challenges remain insufficiently addressed, especially regarding the use of non-actor participants, the role of individual differences, and the handling of temporal dynamics.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Emotion Models",
      "text": "Two primary frameworks characterize how emotions are commonly modeled. Discrete models categorize emotions into fundamental groups such as the \"Big Six\": anger, disgust, fear, joy, sadness, and surprise  [9] . These discrete labels are intuitive but may struggle to capture subtle or mixed emotional states in realistic settings  [10] . On the other hand, dimensional models represent emotions along low-dimensional axes like arousal and valence  [11] , and occasionally dominance. Although dimensional models offer a more continuous representation, many studies reduce them to a few broad labels (e.g., \"low,\" \"medium,\" or \"high\" arousal), potentially overlooking fine-grained emotional shifts.\n\nIn short, both discrete and dimensional models have trade-offs: discrete categories can oversimplify nuanced or spontaneous expressions, while dimensional approaches can be too broad or still require artificial binning of values. Balancing these strengths and limitations is an ongoing challenge in emotion recognition.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multimodal Emotion Recognition Approaches",
      "text": "Leveraging multiple modalities, such as facial expressions, vocal prosody, and physiological signals, has demonstrated robust gains in emotion recognition accuracy. For example, Kollias et al.  [12]  used a multi-task learning approach on audio-visual data to jointly detect valence, arousal, expressions, and action units, obtaining high macro F1 scores. Similarly, the reviews by Li et al.  [13]  and Zhang et al.  [14]  emphasize that systems that combine facial, speech, and physiological cues can exceed F1 scores of 0.85 in controlled settings. However, many of these studies primarily rely on actors portraying clear-cut emotional expressions or simplifying the outputs to valence/arousal alone, which might reduce ecological validity when transferred to spontaneous or subtle real-world scenarios. Furthermore, while these multimodal setups can be highly accurate, they also introduce practical trade-offs. Recording physiological signals such as EEG or electrodermal activity often requires specialized equipment and controlled laboratory conditions. In realistic applications, these constraints can limit scalability. Thus, understanding how to integrate more accessible signals, such as gaze, or basic speech patterns-while still capturing emotional complexity remains a pressing issue.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Eye-Tracking-Based Emotion Recognition",
      "text": "Among visual modalities, eye-tracking stands out for its capacity to capture both attentional focus (fixations) and arousal levels (pupil dilation)  [15] . Eye movements have been shown to correlate with emotional states  [16]  and can reveal the facial features a person deems most informative  [17] . For instance, Lu et al.  [18]  combined eye-tracking and EEG to achieve high accuracy in emotion recognition, underscoring the synergy between physiological and gaze-based measures. However, this approach also has limitations. Specialized eyetracking hardware may be expensive or intrusive, and factors such as lighting or corrective lenses can compromise data quality.\n\nAdditionally, although some studies apply eyetracking in VR environments to classify emotions  [19] , it remains underutilized in humanagent interaction (HAI) contexts, where nuanced emotional understanding is crucial  [20] . In many real-world scenarios, even modest improvements in gaze-based monitoring could enhance an agent's ability to adapt to a user's emotional state, provided the system can handle unstructured, realtime data effectively.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Gaze Strategies",
      "text": "Eye gaze strategies-i.e., how individuals distribute their attention across a face-offer valuable insights into emotion-specific cues. Research shows that gaze patterns vary with age  [21]  and gender  [22] , while different facial regions (eyes, mouth, etc.) provide varying degrees of diagnosticity for certain emotions  [23] . However, these variations also introduce complexity: if emotion recognition systems focus solely on the eyes, they might underestimate signals from the mouth, and vice versa. Moreover, gaze direction (direct vs. averted) can modulate emotion perception in ways that are not fully understood or integrated into computational models  [24] .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Stimuli And Participant Considerations",
      "text": "A recurring critique of many emotion recognition datasets is their reliance on actors who display prototypical emotions  [25, 26] . While this controlled approach can yield high accuracy in laboratory settings, it may fail to generalize to the subtle and often overlapping emotions seen \"in the wild.\" By contrast, utilizing non-actor participants and more naturalistic stimuli could better reflect everyday experiences, though it also introduces variability and noise. Some works attempt to add realism by including evocative videos or music, but these too can be limited by cultural context or subjective interpretation. Our approach, involving \"talking faces\" and non-actor participants, aims to capture more spontaneous responses-although it still faces the challenge of bridging the gap between controlled experiments and authentic real-world interactions.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Temporal Dynamics In Emotion Recognition",
      "text": "The majority of existing studies concentrate on static frames or brief segments, often overlooking how emotional expressions evolve over time. Although valence and arousal can fluctuate rapidly, few works incorporate temporal dynamics in a comprehensive way. Wang et al.  [27]  stress that capturing real-time changes in emotional states presents challenges, including the need for standardized metrics and more sophisticated sequence modelling. Without a temporal perspective, systems risk mislabeling emotions that unfold gradually or detecting only fleeting cues-further underscoring the need for dynamic modelling of gaze, facial movements, and other signals.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Personality And Emotion Recognition",
      "text": "Another dimension often neglected in emotion recognition systems is the user's personality profile.\n\nThe Big Five personality traitsneuroticism, extraversion, openness, agreeableness, and conscientiousness-can significantly shape an individual's emotional responses  [28, 29] . For instance, those high in neuroticism tend to dwell on negative stimuli, while extraverted individuals focus more readily on positive cues  [30] . Despite these established relationships, many emotion recognition models still treat users as a homogeneous group, risking reduced accuracy and personalization.\n\nFurthermore, recent research has demonstrated that eye-tracking data can reveal personality traits themselves  [3, 31, 32] , opening the door to adaptive systems that leverage real-time gaze analysis to refine emotion predictions. However, incorporating personality raises new complexities, such as how to measure or infer traits reliably and how to handle ethical concerns around privacy and user profiling.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Personality-Inspired Eye-Tracking-Based Emotion Recognition",
      "text": "By integrating personality assessments into eyetracking-based emotion recognition, researchers can better account for individual gaze patterns and emotional biases. For example, those high in neuroticism may fixate longer on negative facial features  [33] , while extraverts frequently scan positive features  [34] . Although these insights have shown promise in controlled experiments, the impact of personality in more real-world or conversation-like scenarios remains less explored. Practical issues, such as how to maintain calibration for different users and how to fuse personality traits with other modalities, still require further work.\n\nNevertheless, personality-inspired approaches represent a significant step toward more robust, user-centric emotion recognition. They hint at a future in which systems adapt not only to immediate emotional cues but also to deeper, trait-level aspects of users, making interactions more personalized and empathetic.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Summary",
      "text": "In summary, while numerous methods have advanced emotion recognition in terms of accuracy and modality integration, key challenges persist:\n\n• Over-Reliance on Emotion-Evoking Stimuli: Many studies rely on artificially evocative materials (e.g., dramatic videos or staged scenes) to elicit strong emotional reactions. However, such stimuli may not generalize to everyday faceto-face communication, where expressions tend to be more subtle and context-dependent.\n\n• Neglect of Individual Differences: Personality traits, cultural backgrounds, and personal experiences heavily shape emotional responses, but remain underrepresented in many models.\n\n• Limited Attention to Temporal Dynamics: Emotions evolve over time, yet temporal changes are often treated as static or are underutilized in modeling.\n\nOur work seeks to address these gaps using more naturalistic \"talking face\" stimuli, incorporating personality traits, and explicitly modelling temporal dynamics through eye-tracking data. In doing so, we aim to deepen both the theoretical understanding of how personality shapes emotional perception and the practical capability of affective computing systems to operate effectively in realworld contexts.  The experiment simulated the listening part of a conversational context where participants engaged with dynamic, emotionally expressive stimuli. Participants completed 88 trials: 4 practice and 84 main trials, presented in random order. Stimuli were 84 video clips selected from the CREMA-D dataset  [4] , featuring 91 actors (48 male, 43 female) aged 20-74 portraying six basic emotions (Anger, Disgust, Fear, Happy, Neutral, Sad) at varying intensity levels. The selected clips balanced emotions and actor demographics to enhance the range of expressions and improve generalisability.\n\nThe experiment was designed to approximate a face-to-face conversational scenario in which participants observe short, speech-containing videos. Each participant completed 88 trials (4 practice, 84 main), presented in random order. The videos were sourced from the CREMA-D dataset  [4] , which features 91 actors (48 male, 43 female) aged 20-74, each portraying one of six basic emotions (Anger, Disgust, Fear, Happy, Neutral, Sad) at varying intensity levels. We selected 84 clips that balanced different emotions and actor demographics, supporting a broader range of expressions and potentially improving generalisability. To simulate faceto-face dialogue more closely, a short written scenario was displayed before each video, prompting participants to imagine they were conversing with the individual in the video. This contextual priming was intended to increase engagement and emotional alignment, even though no real turn-taking occurred.\n\nDuring the experiment, eye-tracking data were recorded using a GP3 HD eye tracker at a sampling rate of 150 Hz. The eye tracker was calibrated for each participant using a standard 9-point calibration procedure. The data collection was synchronized with the presentation of stimuli using the Lab Streaming Layer (LSL) framework to ensure precise alignment between eye-tracking data and stimulus events.\n\nFigure  2  illustrates the experimental setup, showing a participant seated in front of the monitor with sensors attached.\n\nBefore beginning the trials, participants completed the BFI-44 questionnaire  [8]  to assess their personality traits (openness, conscientiousness, extraversion, agreeableness, neuroticism). After each video, participants rated their perceived and felt emotions on 9-point Likert scales for valence (1 = very negative, 9 = very positive) and arousal (1 = very calm, 9 = very excited). We chose a 9-point scale for its higher resolution, which can capture subtle affective variations  [35, 36]  more effectively than smaller-scale alternatives. These self-reported ratings formed the ground truth labels for our emotion recognition models.\n\nAn example frame from the video stimuli used in the experiment is shown in Figure  3 .\n\nThe 9-point scales used for rating emotional arousal and valence are depicted in Figure  4 .",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Data Preprocessing And Feature Extraction",
      "text": "Following data collection, we performed several preprocessing steps to ensure high-quality eyetracking signals:\n\n1. Quality Filtering: We discarded data points flagged by the eye-tracker's algorithms (e.g., blinks, tracking loss).  3. Pupil Baseline Correction: We used the mean pupil size from each participant's neutralvideo periods as an individual baseline. Subsequent pupil measurements were subtracted from this baseline to reduce cross-participant physiological differences.\n\nFor each trial, we extracted the following features:\n\n• Fixation Metrics: Mean, median, and variance of fixation duration/dispersion.\n\n• Pupil Metrics: Corrected mean, min, max, and variance of pupil size.\n\n• Saccadic Metrics: Amplitude, duration, peak velocity, and acceleration, reflecting scanning behavior.\n\nFigure  5 : Facial landmarks (via OpenFace  [37] ) partitioned into multiple ROIs.\n\n• Gaze Regions: Using OpenFace  [37]  and convex hull segmentation  [38] , we labeled each gaze point by facial region (eyes, eyebrows, nose, mouth, or outside). The proportion of fixations per region provides insight into attention distribution.\n\n• Environmental Variables: Ambient light (in lux), room temperature, and stimulus brightness levels, controlling for potential external influences on pupil dilation.\n\n• Personality Traits: BFI-44 scores scaled to [0,1].",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Features For Modeling",
      "text": "Since participants' fixations and saccades vary in number and timing, we standardized the temporal dimension by interpolation into 15 equally spaced time steps per trial. This creates a uniform representation across 2-4 s videos, facilitating sequence modelling with architectures such as Long Short-Term Memory (LSTM) networks  [39] . Each time step includes gaze-region allocations, pupil size, and saccadic measures, capturing how visual attention and arousal evolve. By combining these time-series features with static context variables (environmental data and personality scores), our models can exploit both dynamic and trait-level information to improve emotion recognition accuracy.\n\nIn sum, this design aims to balance ecological validity (via \"talking face\" videos, textual context, and participant diversity) with reproducible data processing steps (calibration, baseline correction, and interpolation), thereby offering a robust foundation for exploring how temporal eye-tracking patterns and personality traits influence perceived and felt emotions.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Statistical Analysis",
      "text": "We examined the relationships between personality traits, eye-tracking metrics, and emotion labels (perceived/felt valence and arousal) through (1) correlation analysis at the participant level and (2) linear mixed-effects (LME) modeling at the trial level. Correlation analysis provided an exploratory overview of aggregated associations, while LME models accounted for repeated measures and individual differences.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Correlation Analysis",
      "text": "Because each participant contributed multiple trials, we averaged eye-tracking metrics (e.g., pupil variance), emotion labels, and personality traits to ensure independence. We then computed Pearson's correlation coefficients. Table  2  shows that conscientiousness and agreeableness correlate positively with felt valence, whereas neuroticism shows a negative correlation. Stimulus-specific effects (Table  3 ) further suggest that personality modulates emotional responses differently depending on the displayed emotion. Eyetracking metrics also correlate with emotion (Table 4), where higher pupil variance corresponds to lower perceived and felt valence.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Linear Mixed-Effects Modeling",
      "text": "To leverage trial-level data and account for withinsubject variability, we fit an LME model for each\n\nwhere i indexes the trial and j indexes the participant. The predictor (e.g., pupil size, fixation time, or a personality trait) was tested separately to mitigate multicollinearity; u 0j is a random intercept for each participant. These models revealed significant effects of pupil metrics on emotion (Table  5 ); e.g., increased mean/max pupil size was associated with higher arousal and lower valence. Bonferroni corrections were applied to all multiple comparisons.\n\nAnalyses also examined facial fixations (Tables 6-7): mouth fixation correlated with higher perceived arousal, whereas eye fixation related to felt valence. Personality, particularly neuroticism, emerged as a strong negative predictor for valence.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Summary And Key Insights",
      "text": "Overall, LME models clarified how personality, pupil metrics, and fixation patterns jointly shape emotional perception. Higher pupil dilation indicates elevated arousal but lower valence, and neuroticism consistently aligns with negative valence. Future work could add random slopes or interaction terms (e.g., Neuroticism×PupilMean) to capture further individual-level variability. Nonetheless, these findings already highlight the impor-   5 Machine Learning Modeling",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Emotion Labeling And Data Preparation",
      "text": "We aimed to predict four emotion labels: felt valence, perceived valence, felt arousal, and perceived arousal. Due to the imprecision in self-reported emotions, each label was grouped into three classes-low/negative (1-3), medium/neutral (4-6), and high/positive (7-9) by the ranges specified. The dataset was split into training (64%), validation (16%), and testing (20%) sets using stratified splitting to maintain class distributions. Following prior works  [40] [41] [42] , binning continuous valence and arousal ratings into three categories simplifies classification tasks and mitigates subjective variability in self-reported emotions. This approach enables clearer distinctions between emotional states while reducing model complexity. However, we acknowledge that this may result in the loss of finer-grained information.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Feature Engineering And Preprocessing",
      "text": "Normalization and Scaling.\n\nTo ensure equal contribution of features and reduce the risk of bias, we applied consistent preprocessing techniques. Personality trait scores (ranging from 0 to 50) were scaled by dividing by 50  [43]  to fit the [0, 1] interval. Skewed features, such as saccade amplitude and duration, were transformed with MinMaxScaler, which rescales the data based on the theoretical min/max values derived from the training set. Other continuous features (e.g., corrected pupil sizes, environmental variables) were standardized using StandardScaler, subtracting the mean and dividing by the standard deviation computed from the training split only. This approach avoids data leakage by ensuring that vali- One-Hot Encoding.\n\nStimulus emotion (happy, sad, neutral, angry, disgust, fear) was encoded as a 6-dimensional one-hot vector. This encoding enables the model to treat each distinct emotion category independently.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Neural Network Architecture",
      "text": "Our neural network (NN) model integrates multiple input streams (Figure  6 ). Each stream is preprocessed separately to capture its specific characteristics before feature fusion. This architecture ensures that temporal features (e.g., eye-tracking data) and static features (e.g., personality traits, stimuli emotion) are modeled appropriately for their unique roles in predicting emotional states.\n\nEye-Tracking Data: Processed through LSTM layers to capture temporal dependencies. Personality Traits: Processed through fully connected layers. Stimuli Emotion: One-hot encoded and processed through fully connected layers. Environmental Variables: Processed through fully connected layers.\n\nTo prevent the model from overfitting on personality or environmental variables, small Gaussian noise was added to these inputs during training, serving as a form of data augmentation. This noise regularization helps ensure that the model generalizes well across participants and experimental conditions.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Classification Approach",
      "text": "Emotion prediction was framed as a three-class classification task. Outputs were passed through a softmax activation function to obtain class probabilities. We used categorical cross-entropy loss, with class weights inversely proportional to class frequencies to address class imbalance.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Model Training And Evaluation",
      "text": "For neural network models, we conducted manual and grid search hyperparameter tuning. The search space included learning rates in {10 -3 , 10 -4 , 10 -5 }, dropout rates in {0.2, 0.3, 0.5}, and weight decay parameters. The best configurations were selected based on macro F1 scores on the validation set. Early stopping was used during training, halting if no validation improvement was observed within 10 epochs.\n\nHyperparameters such as learning rates, dropout rates, and weight decay were tuned based on validation performance. We evaluated models using the F1-score, suitable for imbalanced datasets as it considers both precision and recall.\n\nWe compared our NN models against support vector machines (SVMs) as baselines. The SVMs used stimuli emotion, and stimuli emotion combined with personality data as input features. The SVM baselines were chosen for their simplicity and effectiveness with non-temporal data. Unlike the NN, which can leverage sequential dependencies in eye-tracking data, the SVMs were limited to static features (e.g., stimuli emotion, personality). This comparison highlights the added value of temporal modeling and feature integration in the NN.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Results",
      "text": "Table  8  presents the F1-scores for different models and emotion labels. The best results for each emotion label are bolded.\n\nThe results indicate that integrating personality traits alongside temporal eye-tracking data and stimuli emotion significantly enhances model performance, particularly for felt emotions. This supports the notion that felt emotions, being more subjective, benefit most from incorporating highlevel personal trait information. The SVM baselines performed well on perceived emotions, likely due to the direct influence of stimuli on perception, but lacked the capability to model sequential dependencies, underscoring the importance of our temporal NN design.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Discussion",
      "text": "We integrated eye-tracking data, personality traits, and stimulus-emotion labels to improve emotion recognition in short, speech-containing clips. Although not fully interactive dialogues, focusing on the listener's perspective offered controlled condi-tions to isolate critical predictors of perceived and felt emotions.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Complexity And Agreement",
      "text": "Emotions are influenced by stimuli, individual traits, and context, making them challenging to model. Table  9  shows user agreement varied widely: perceived valence had the highest agreement (77.7%), while felt arousal was lowest (56.0%). Personalized calibration could better address such subjective variability.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Model Performance And Stimulus Emotion",
      "text": "Higher agreement (e.g., perceived valence) led to stronger performance. Our best model reached a macro F1 of 0.77 for perceived valence (Table  8 ).\n\nStimulus emotion notably boosted perceived-emotion prediction; an SVM using only stimulus emotion already achieved good accuracy. However, the NN outperformed it on felt emotions by incorporating eye-tracking data, reflecting the added benefit of physiological cues for internal states.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Multimodality And Individual Differences",
      "text": "Combining stimulus emotion, personality, and eyetracking yielded the highest macro F1-scores (0.77 for perceived valence, 0.58 for felt valence). Personality accounted for individual differences, while eye-tracking provided real-time physiological insights  [44] , and stimulus context clarified perception.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Comparison And Future Directions",
      "text": "Compared",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusion",
      "text": "This study shows that temporal eye-tracking, personality traits, and stimulus emotion can jointly enhance emotion recognition in short, speech-based clips. Although not full dialogues, modeling the listener's perspective proved effective: personality traits improved felt-emotion accuracy (e.g., raising felt arousal F1 from 0.36 to 0.52), and stimulus emotion elevated perceived-emotion performance (perceived valence from 0.34 to 0.77).",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Implications",
      "text": "Our multimodal approach underscores the necessity of merging physiological signals, personal traits, and context in affective computing. It can guide the development of user-centric applications (e.g., mental health tools, virtual agents) that adapt to individual differences more precisely.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Limitations And Future Work",
      "text": "Relying on actor-driven stimuli and a relatively homogeneous participant sample restricts generalizability to spontaneous, diverse real-world conversations. Future work should explore interactive dialogues, address class imbalance for low-agreement labels, and expand modalities (e.g., vocal prosody, micro-expressions). Personalized calibration remains crucial for handling highly subjective emotions.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Outlook",
      "text": "In essence, combining user-specific traits, physiological data, and external cues is essential for capturing both perceived and felt emotions. As research progresses toward fully interactive, real-time systems, these insights can foster more ethical, privacy-conscious, and adaptive affective computing solutions-ultimately mirroring the complex and subjective nature of human emotions.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Ethical Impact Statement",
      "text": "This research investigates emotion detection in dialogues by integrating eye-tracking data, temporal dynamics, and personality traits, providing insights that could enhance affective computing applications. Given that the study involves human participants, it was conducted with oversight from an ethical review board. Informed consent was obtained from all participants, covering data collection, usage, and analysis. To safeguard participant privacy, all data was anonymized, and participants were informed of their right to withdraw at any time without consequence. Potential risks stem from privacy concerns related to emotion recognition, especially when using eye-tracking data like pupil size, which participants cannot directly control or consciously moderate. Unlike facial expressions or vocal cues, there is limited cultural awareness around eye metrics such as pupil dilation, which may involuntarily reveal emotional states. This can potentially limit individuals' ability to hide emotions, raising privacy issues as it could lead to unintentional exposure of affective information. Recognizing this, we implemented strict protocols to anonymize data, restrict access to authorized research personnel, and provide participants with clear explanations of the data collected.\n\nWhile these risks are present, we believe they are outweighed by the benefits of this research, which include advancements in human-computer interaction, socially assistive technologies, and applications in mental health and education. The anonymization of data, clear communication of its use, and emphasis on ethical handling protocols are key risk mitigation strategies. Our findings could enable more sensitive and context-aware affective",
      "page_start": 12,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Illustration of three key emotion types:",
      "page": 2
    },
    {
      "caption": "Figure 2: The experimental setup of a participant",
      "page": 5
    },
    {
      "caption": "Figure 2: illustrates the experimental setup, show-",
      "page": 6
    },
    {
      "caption": "Figure 3: The 9-point scales used for rating emotional",
      "page": 6
    },
    {
      "caption": "Figure 3: A frame from the video stimuli used in",
      "page": 6
    },
    {
      "caption": "Figure 4: The 9-point scales used for describing the",
      "page": 6
    },
    {
      "caption": "Figure 5: Facial landmarks (via OpenFace [37])",
      "page": 7
    },
    {
      "caption": "Figure 6: Neural network architecture integrating",
      "page": 8
    },
    {
      "caption": "Figure 6: ). Each stream is prepro-",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Modelling Emotions in Face-to-Face Setting: The": "Interplay of Eye-Tracking, Personality, and Temporal"
        },
        {
          "Modelling Emotions in Face-to-Face Setting: The": "Dynamics"
        },
        {
          "Modelling Emotions in Face-to-Face Setting: The": "Meisam J. Seikavandi1, Jostein Fimland1, Maria Barrett2, and Paolo Burelli1"
        },
        {
          "Modelling Emotions in Face-to-Face Setting: The": "1 brAIn lab, IT University of Copenhagen, Denmark"
        },
        {
          "Modelling Emotions in Face-to-Face Setting: The": "2"
        },
        {
          "Modelling Emotions in Face-to-Face Setting: The": "IT University of Copenhagen, Denmark"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": "computing,\nenabling systems\nto interpret and re-"
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": "spond\nto\nhuman\nemotions\nin\na more\npersonal-"
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": "ized and context-sensitive manner. Traditional ap-"
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": ""
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": "proaches often rely on analyzing static images of fa-"
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": ""
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": "cial expressions or simplified emotion labels. How-"
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": ""
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": "ever, genuine human interactions—even when one"
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": ""
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": "is merely observing a speaker—are dynamic and in-"
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": ""
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": "fluenced by subtle cues and individual differences in"
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": ""
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": "personality [1–3]."
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": ""
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": "In many real-world scenarios, one observes a talk-"
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": ""
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": "ing\nface\nand makes\ninferences\nabout what\nthat"
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": ""
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": "speaker is feeling or expressing. This listener’s per-"
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": ""
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": "spective is critical,\nfor instance,\nin teleconferencing"
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": ""
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": "or virtual agent\nsetups, where users view speech-"
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": ""
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": "containing video feeds and react based on perceived"
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": ""
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": "emotional cues. Although such contexts could be"
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": ""
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": "described as “dialogue-like”,\nthey are not always"
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": ""
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": "fully interactive with turn-taking.\nOur work fo-"
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": ""
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": "cuses on approximating this\nlistener\nscenario,\nin"
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": ""
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": "which participants watch short,\nspeech-containing"
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": ""
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": "video clips from the CREMA-D dataset [4], whose"
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": ""
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": "stimuli were\nrecorded by professional actors.\nBy"
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": ""
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": "doing so, we align with real-life conditions where"
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": ""
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": "an observer is trying to interpret a speaker’s emo-"
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": ""
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": "tions dynamically."
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": ""
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": "Despite recent progress, current emotion recog-"
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": ""
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": "nition models face key challenges:"
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": ""
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": ""
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": "• Temporal\ncomplexity. Many methods over-"
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": ""
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": "look the continuous, time-varying nature of facial"
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": ""
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": "cues in speech-based videos."
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": ""
        },
        {
          "Emotion\nrecognition\nis\na\ncore\ntopic\nin\naffective": "•\nInfluence\nof\npersonality.\nFew studies\nac-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "count\nfor\nindividual differences\nsuch as person-": "ality traits, which modulate how a person per-",
          "the stable influence of personality, and the differ-": "ence between perceived and felt emotion labels."
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "ceives emotional stimuli and how they feel\nin re-",
          "the stable influence of personality, and the differ-": "Our contributions are threefold:"
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "sponse [5, 6].",
          "the stable influence of personality, and the differ-": ""
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "",
          "the stable influence of personality, and the differ-": "1.\nIntegration\nof Multimodal Data:\nWe"
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "• Distinguishing perceived vs.\nfelt emotions.",
          "the stable influence of personality, and the differ-": "demonstrate how merging eye-tracking signals,"
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "Users may perceive certain emotional cues in the",
          "the stable influence of personality, and the differ-": "personality traits, and temporal dynamics sig-"
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "speaker, yet feel a different or more nuanced emo-",
          "the stable influence of personality, and the differ-": "nificantly\nenhances\nemotion\nrecognition\nfor"
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "tional response themselves.",
          "the stable influence of personality, and the differ-": "talking-face scenarios."
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "",
          "the stable influence of personality, and the differ-": "2.\nInsights into Personality’s Role: We pro-"
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "To systematically address these points, we build",
          "the stable influence of personality, and the differ-": ""
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "",
          "the stable influence of personality, and the differ-": "vide empirical evidence showing that personal-"
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "on the crucial distinction among Expressed Emo-",
          "the stable influence of personality, and the differ-": ""
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "",
          "the stable influence of personality, and the differ-": "ity traits modulate both how participants per-"
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "tions\nPerceived Emotions\nand Felt\n(Ee),\n(Ep),",
          "the stable influence of personality, and the differ-": ""
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "",
          "the stable influence of personality, and the differ-": "ceive\nothers’\nemotions and how they feel\nin"
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "Emotions\nAs\nillustrated in Fig. 1, when a\n(Ef ).",
          "the stable influence of personality, and the differ-": ""
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "",
          "the stable influence of personality, and the differ-": "response, which can reduce model uncertainty"
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "human observer encounters emotional stimuli, they",
          "the stable influence of personality, and the differ-": ""
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "",
          "the stable influence of personality, and the differ-": "and improve predictions."
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "recognize the expressed emotion (Ee) and form an",
          "the stable influence of personality, and the differ-": ""
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "internal perceived\nlabel\nIn turn,\nthey can\n(Ep).",
          "the stable influence of personality, and the differ-": ""
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "",
          "the stable influence of personality, and the differ-": "3. Advancement\nin Emotion Recognition:"
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "experience a corresponding or even conflicting felt",
          "the stable influence of personality, and the differ-": ""
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "",
          "the stable influence of personality, and the differ-": "We\npropose\na multimodal\nneural\napproach"
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "state\nOur\nresearch targets both perceived\n(Ef ).",
          "the stable influence of personality, and the differ-": ""
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "",
          "the stable influence of personality, and the differ-": "that\nyields\nhigh\npredictive\nperformance\nfor"
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "and felt\nemotions,\nacknowledging that\nthese\ntwo",
          "the stable influence of personality, and the differ-": ""
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "",
          "the stable influence of personality, and the differ-": "both\nperceived\nand\nfelt\nemotion\ncategories,"
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "dimensions can diverge in real situations.",
          "the stable influence of personality, and the differ-": ""
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "",
          "the stable influence of personality, and the differ-": "supporting\nfuture\napplications\nin\nhuman-"
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "",
          "the stable influence of personality, and the differ-": "computer\ninteraction,\nvirtual\nagents,\nand"
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "",
          "the stable influence of personality, and the differ-": "broader affective computing contexts."
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "",
          "the stable influence of personality, and the differ-": "By incorporating the complexities of dynamic fa-"
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "",
          "the stable influence of personality, and the differ-": "cial cues, user-specific personality factors, and the"
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "",
          "the stable influence of personality, and the differ-": "distinction between perceived and felt\nemotions,"
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "",
          "the stable influence of personality, and the differ-": "our work moves beyond static or purely acted se-"
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "",
          "the stable influence of personality, and the differ-": "tups. While this study does not encompass a fully"
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "",
          "the stable influence of personality, and the differ-": "interactive dialogue with turn-taking,\nit focuses on"
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "",
          "the stable influence of personality, and the differ-": "a critical component of many interactions—the ob-"
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "",
          "the stable influence of personality, and the differ-": "server’s side—making it relevant to real-world sys-"
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "",
          "the stable influence of personality, and the differ-": "tems where the user is frequently in a listening or"
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "",
          "the stable influence of personality, and the differ-": "monitoring role. We believe these findings pave the"
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "",
          "the stable influence of personality, and the differ-": "way for more adaptive and user-centred emotion"
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "Figure 1:\nIllustration of\nthree key emotion types:",
          "the stable influence of personality, and the differ-": ""
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "",
          "the stable influence of personality, and the differ-": "recognition approaches that respect individual dif-"
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "Expressed (Ee), Perceived (Ep), and Felt (Ef ).",
          "the stable influence of personality, and the differ-": ""
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "",
          "the stable influence of personality, and the differ-": "ferences and capture emotional subtleties more ef-"
        },
        {
          "count\nfor\nindividual differences\nsuch as person-": "",
          "the stable influence of personality, and the differ-": "fectively."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "employing highly controlled datasets.\nIn practice,": "real-world emotion perception is affected by multi-",
          "outputs to valence/arousal alone, which might re-": "duce ecological validity when transferred to spon-"
        },
        {
          "employing highly controlled datasets.\nIn practice,": "ple, often interdependent factors such as personal-",
          "outputs to valence/arousal alone, which might re-": "taneous or subtle real-world scenarios."
        },
        {
          "employing highly controlled datasets.\nIn practice,": "ity traits, gaze patterns, and contextual cues. Con-",
          "outputs to valence/arousal alone, which might re-": "Furthermore, while these multimodal setups can"
        },
        {
          "employing highly controlled datasets.\nIn practice,": "sequently,\nseveral\nimportant challenges remain in-",
          "outputs to valence/arousal alone, which might re-": "be highly accurate,\nthey also introduce practical"
        },
        {
          "employing highly controlled datasets.\nIn practice,": "sufficiently addressed, especially regarding the use",
          "outputs to valence/arousal alone, which might re-": "trade-offs. Recording physiological signals such as"
        },
        {
          "employing highly controlled datasets.\nIn practice,": "of non-actor participants, the role of individual dif-",
          "outputs to valence/arousal alone, which might re-": "EEG or electrodermal activity often requires\nspe-"
        },
        {
          "employing highly controlled datasets.\nIn practice,": "ferences, and the handling of temporal dynamics.",
          "outputs to valence/arousal alone, which might re-": "cialized equipment and controlled laboratory con-"
        },
        {
          "employing highly controlled datasets.\nIn practice,": "",
          "outputs to valence/arousal alone, which might re-": "ditions.\nIn realistic applications, these constraints"
        },
        {
          "employing highly controlled datasets.\nIn practice,": "",
          "outputs to valence/arousal alone, which might re-": "can limit scalability. Thus, understanding how to"
        },
        {
          "employing highly controlled datasets.\nIn practice,": "2.1\nEmotion Models",
          "outputs to valence/arousal alone, which might re-": ""
        },
        {
          "employing highly controlled datasets.\nIn practice,": "",
          "outputs to valence/arousal alone, which might re-": "integrate more accessible signals,\nsuch as gaze, or"
        },
        {
          "employing highly controlled datasets.\nIn practice,": "Two primary\nframeworks\ncharacterize how emo-",
          "outputs to valence/arousal alone, which might re-": "basic\nspeech patterns—while\nstill\ncapturing emo-"
        },
        {
          "employing highly controlled datasets.\nIn practice,": "tions are commonly modeled. Discrete models",
          "outputs to valence/arousal alone, which might re-": "tional complexity remains a pressing issue."
        },
        {
          "employing highly controlled datasets.\nIn practice,": "categorize emotions into fundamental groups such",
          "outputs to valence/arousal alone, which might re-": ""
        },
        {
          "employing highly controlled datasets.\nIn practice,": "as\nthe\n“Big Six”:\nanger, disgust,\nfear,\njoy,\nsad-",
          "outputs to valence/arousal alone, which might re-": ""
        },
        {
          "employing highly controlled datasets.\nIn practice,": "ness,\nand surprise\n[9].\nThese discrete\nlabels are",
          "outputs to valence/arousal alone, which might re-": "2.3\nEye-Tracking-Based\nEmotion"
        },
        {
          "employing highly controlled datasets.\nIn practice,": "intuitive\nbut may\nstruggle\nto\ncapture\nsubtle\nor",
          "outputs to valence/arousal alone, which might re-": "Recognition"
        },
        {
          "employing highly controlled datasets.\nIn practice,": "mixed emotional states in realistic settings [10]. On",
          "outputs to valence/arousal alone, which might re-": ""
        },
        {
          "employing highly controlled datasets.\nIn practice,": "",
          "outputs to valence/arousal alone, which might re-": "Among visual modalities, eye-tracking stands out"
        },
        {
          "employing highly controlled datasets.\nIn practice,": "the other hand, dimensional models\nrepresent",
          "outputs to valence/arousal alone, which might re-": ""
        },
        {
          "employing highly controlled datasets.\nIn practice,": "",
          "outputs to valence/arousal alone, which might re-": "for\nits capacity to capture both attentional\nfocus"
        },
        {
          "employing highly controlled datasets.\nIn practice,": "emotions along low-dimensional axes\nlike arousal",
          "outputs to valence/arousal alone, which might re-": ""
        },
        {
          "employing highly controlled datasets.\nIn practice,": "",
          "outputs to valence/arousal alone, which might re-": "(fixations) and arousal\nlevels\n(pupil dilation)\n[15]."
        },
        {
          "employing highly controlled datasets.\nIn practice,": "and valence [11], and occasionally dominance. Al-",
          "outputs to valence/arousal alone, which might re-": ""
        },
        {
          "employing highly controlled datasets.\nIn practice,": "",
          "outputs to valence/arousal alone, which might re-": "Eye movements have been shown to correlate with"
        },
        {
          "employing highly controlled datasets.\nIn practice,": "though dimensional models offer a more\ncontinu-",
          "outputs to valence/arousal alone, which might re-": ""
        },
        {
          "employing highly controlled datasets.\nIn practice,": "",
          "outputs to valence/arousal alone, which might re-": "emotional states [16] and can reveal the facial\nfea-"
        },
        {
          "employing highly controlled datasets.\nIn practice,": "ous representation, many studies reduce them to a",
          "outputs to valence/arousal alone, which might re-": ""
        },
        {
          "employing highly controlled datasets.\nIn practice,": "",
          "outputs to valence/arousal alone, which might re-": "tures a person deems most\ninformative\n[17].\nFor"
        },
        {
          "employing highly controlled datasets.\nIn practice,": "few broad labels (e.g., “low,” “medium,” or “high”",
          "outputs to valence/arousal alone, which might re-": ""
        },
        {
          "employing highly controlled datasets.\nIn practice,": "",
          "outputs to valence/arousal alone, which might re-": "instance, Lu et al.\n[18] combined eye-tracking and"
        },
        {
          "employing highly controlled datasets.\nIn practice,": "arousal), potentially overlooking fine-grained emo-",
          "outputs to valence/arousal alone, which might re-": ""
        },
        {
          "employing highly controlled datasets.\nIn practice,": "",
          "outputs to valence/arousal alone, which might re-": "EEG to achieve high accuracy in emotion recog-"
        },
        {
          "employing highly controlled datasets.\nIn practice,": "tional shifts.",
          "outputs to valence/arousal alone, which might re-": ""
        },
        {
          "employing highly controlled datasets.\nIn practice,": "",
          "outputs to valence/arousal alone, which might re-": "nition, underscoring the\nsynergy between physio-"
        },
        {
          "employing highly controlled datasets.\nIn practice,": "In short,\nboth discrete\nand dimensional mod-",
          "outputs to valence/arousal alone, which might re-": ""
        },
        {
          "employing highly controlled datasets.\nIn practice,": "",
          "outputs to valence/arousal alone, which might re-": "logical\nand gaze-based measures.\nHowever,\nthis"
        },
        {
          "employing highly controlled datasets.\nIn practice,": "els have\ntrade-offs:\ndiscrete\ncategories\ncan over-",
          "outputs to valence/arousal alone, which might re-": ""
        },
        {
          "employing highly controlled datasets.\nIn practice,": "",
          "outputs to valence/arousal alone, which might re-": "approach\nalso\nhas\nlimitations.\nSpecialized\neye-"
        },
        {
          "employing highly controlled datasets.\nIn practice,": "simplify nuanced or spontaneous expressions, while",
          "outputs to valence/arousal alone, which might re-": ""
        },
        {
          "employing highly controlled datasets.\nIn practice,": "",
          "outputs to valence/arousal alone, which might re-": "tracking hardware may be expensive or\nintrusive,"
        },
        {
          "employing highly controlled datasets.\nIn practice,": "dimensional approaches can be too broad or still re-",
          "outputs to valence/arousal alone, which might re-": ""
        },
        {
          "employing highly controlled datasets.\nIn practice,": "",
          "outputs to valence/arousal alone, which might re-": "and factors such as lighting or corrective lenses can"
        },
        {
          "employing highly controlled datasets.\nIn practice,": "quire artificial binning of values. Balancing these",
          "outputs to valence/arousal alone, which might re-": ""
        },
        {
          "employing highly controlled datasets.\nIn practice,": "",
          "outputs to valence/arousal alone, which might re-": "compromise data quality."
        },
        {
          "employing highly controlled datasets.\nIn practice,": "strengths and limitations is an ongoing challenge in",
          "outputs to valence/arousal alone, which might re-": ""
        },
        {
          "employing highly controlled datasets.\nIn practice,": "",
          "outputs to valence/arousal alone, which might re-": "Additionally, although some\nstudies apply eye-"
        },
        {
          "employing highly controlled datasets.\nIn practice,": "emotion recognition.",
          "outputs to valence/arousal alone, which might re-": ""
        },
        {
          "employing highly controlled datasets.\nIn practice,": "",
          "outputs to valence/arousal alone, which might re-": "tracking\nin VR environments\nto\nclassify\nemo-"
        },
        {
          "employing highly controlled datasets.\nIn practice,": "",
          "outputs to valence/arousal alone, which might re-": "tions\n[19],\nit\nremains\nunderutilized\nin human-"
        },
        {
          "employing highly controlled datasets.\nIn practice,": "2.2\nMultimodal\nEmotion Recogni-",
          "outputs to valence/arousal alone, which might re-": ""
        },
        {
          "employing highly controlled datasets.\nIn practice,": "",
          "outputs to valence/arousal alone, which might re-": "agent\ninteraction (HAI)\ncontexts, where nu-"
        },
        {
          "employing highly controlled datasets.\nIn practice,": "tion Approaches",
          "outputs to valence/arousal alone, which might re-": "anced emotional understanding is crucial\n[20].\nIn"
        },
        {
          "employing highly controlled datasets.\nIn practice,": "",
          "outputs to valence/arousal alone, which might re-": "many real-world scenarios,\neven modest\nimprove-"
        },
        {
          "employing highly controlled datasets.\nIn practice,": "Leveraging multiple modalities,\nsuch as\nfacial ex-",
          "outputs to valence/arousal alone, which might re-": ""
        },
        {
          "employing highly controlled datasets.\nIn practice,": "",
          "outputs to valence/arousal alone, which might re-": "ments in gaze-based monitoring could enhance an"
        },
        {
          "employing highly controlled datasets.\nIn practice,": "pressions, vocal prosody, and physiological signals,",
          "outputs to valence/arousal alone, which might re-": ""
        },
        {
          "employing highly controlled datasets.\nIn practice,": "",
          "outputs to valence/arousal alone, which might re-": "agent’s ability to adapt to a user’s emotional state,"
        },
        {
          "employing highly controlled datasets.\nIn practice,": "has demonstrated robust gains in emotion recogni-",
          "outputs to valence/arousal alone, which might re-": ""
        },
        {
          "employing highly controlled datasets.\nIn practice,": "",
          "outputs to valence/arousal alone, which might re-": "provided the system can handle unstructured, real-"
        },
        {
          "employing highly controlled datasets.\nIn practice,": "tion accuracy. For example, Kollias et al.\n[12] used",
          "outputs to valence/arousal alone, which might re-": ""
        },
        {
          "employing highly controlled datasets.\nIn practice,": "",
          "outputs to valence/arousal alone, which might re-": "time data effectively."
        },
        {
          "employing highly controlled datasets.\nIn practice,": "a multi-task learning approach on audio-visual data",
          "outputs to valence/arousal alone, which might re-": ""
        },
        {
          "employing highly controlled datasets.\nIn practice,": "to jointly detect valence, arousal, expressions, and",
          "outputs to valence/arousal alone, which might re-": ""
        },
        {
          "employing highly controlled datasets.\nIn practice,": "action units, obtaining high macro F1 scores. Sim-",
          "outputs to valence/arousal alone, which might re-": ""
        },
        {
          "employing highly controlled datasets.\nIn practice,": "",
          "outputs to valence/arousal alone, which might re-": "2.4\nGaze Strategies"
        },
        {
          "employing highly controlled datasets.\nIn practice,": "ilarly,\nthe\nreviews by Li\net al.\n[13] and Zhang et",
          "outputs to valence/arousal alone, which might re-": ""
        },
        {
          "employing highly controlled datasets.\nIn practice,": "al.\n[14]\nemphasize\nthat\nsystems\nthat\ncombine\nfa-",
          "outputs to valence/arousal alone, which might re-": "Eye gaze strategies—i.e., how individuals distribute"
        },
        {
          "employing highly controlled datasets.\nIn practice,": "cial, speech, and physiological cues can exceed F1",
          "outputs to valence/arousal alone, which might re-": "their attention across a face—offer valuable insights"
        },
        {
          "employing highly controlled datasets.\nIn practice,": "scores of 0.85 in controlled settings. However, many",
          "outputs to valence/arousal alone, which might re-": "into\nemotion-specific\ncues.\nResearch shows\nthat"
        },
        {
          "employing highly controlled datasets.\nIn practice,": "of these studies primarily rely on actors portraying",
          "outputs to valence/arousal alone, which might re-": "gaze patterns vary with age [21] and gender\n[22],"
        },
        {
          "employing highly controlled datasets.\nIn practice,": "clear-cut emotional expressions or\nsimplifying the",
          "outputs to valence/arousal alone, which might re-": "while different\nfacial\nregions\n(eyes, mouth,\netc.)"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "tain emotions [23]. However,\nthese variations also",
          "2.7\nPersonality and Emotion Recog-": ""
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "",
          "2.7\nPersonality and Emotion Recog-": "nition"
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "introduce complexity:\nif emotion recognition sys-",
          "2.7\nPersonality and Emotion Recog-": ""
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "tems\nfocus\nsolely on the\neyes,\nthey might under-",
          "2.7\nPersonality and Emotion Recog-": "Another\ndimension\noften\nneglected\nin\nemotion"
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "estimate\nsignals\nfrom the mouth, and vice versa.",
          "2.7\nPersonality and Emotion Recog-": "recognition\nsystems\nis\nthe\nuser’s\npersonality"
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "Moreover, gaze direction (direct vs.\naverted) can",
          "2.7\nPersonality and Emotion Recog-": "profile.\nThe\nBig\nFive\npersonality\ntraits—"
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "modulate emotion perception in ways that are not",
          "2.7\nPersonality and Emotion Recog-": "neuroticism, extraversion, openness, agreeableness,"
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "fully understood or\nintegrated into computational",
          "2.7\nPersonality and Emotion Recog-": "and conscientiousness—can significantly shape an"
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "models [24].",
          "2.7\nPersonality and Emotion Recog-": "individual’s\nemotional\nresponses\n[28, 29].\nFor\nin-"
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "",
          "2.7\nPersonality and Emotion Recog-": "stance,\nthose high in neuroticism tend to\ndwell"
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "",
          "2.7\nPersonality and Emotion Recog-": "on\nnegative\nstimuli, while\nextraverted\nindividu-"
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "2.5\nStimuli and Participant Consid-",
          "2.7\nPersonality and Emotion Recog-": ""
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "",
          "2.7\nPersonality and Emotion Recog-": "als\nfocus more\nreadily on positive\ncues\n[30].\nDe-"
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "erations",
          "2.7\nPersonality and Emotion Recog-": "spite these established relationships, many emotion"
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "",
          "2.7\nPersonality and Emotion Recog-": "recognition models\nstill\ntreat users as a homoge-"
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "A recurring critique of many emotion recognition",
          "2.7\nPersonality and Emotion Recog-": ""
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "",
          "2.7\nPersonality and Emotion Recog-": "neous group, risking reduced accuracy and person-"
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "datasets is their reliance on actors who display pro-",
          "2.7\nPersonality and Emotion Recog-": ""
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "",
          "2.7\nPersonality and Emotion Recog-": "alization."
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "totypical\nemotions\n[25, 26]. While this\ncontrolled",
          "2.7\nPersonality and Emotion Recog-": ""
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "",
          "2.7\nPersonality and Emotion Recog-": "Furthermore,\nrecent\nresearch has demonstrated"
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "approach can yield high accuracy in laboratory set-",
          "2.7\nPersonality and Emotion Recog-": ""
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "",
          "2.7\nPersonality and Emotion Recog-": "that eye-tracking data can reveal personality traits"
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "tings,\nit may fail to generalize to the subtle and of-",
          "2.7\nPersonality and Emotion Recog-": ""
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "",
          "2.7\nPersonality and Emotion Recog-": "themselves [3,31,32], opening the door to adaptive"
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "ten overlapping emotions\nseen “in the wild.” By",
          "2.7\nPersonality and Emotion Recog-": ""
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "",
          "2.7\nPersonality and Emotion Recog-": "systems\nthat\nleverage\nreal-time gaze analysis\nto"
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "contrast,\nutilizing non-actor participants\nand",
          "2.7\nPersonality and Emotion Recog-": ""
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "",
          "2.7\nPersonality and Emotion Recog-": "refine emotion predictions. However,\nincorporating"
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "more naturalistic stimuli could better reflect every-",
          "2.7\nPersonality and Emotion Recog-": ""
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "",
          "2.7\nPersonality and Emotion Recog-": "personality raises new complexities, such as how to"
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "day experiences, though it also introduces variabil-",
          "2.7\nPersonality and Emotion Recog-": ""
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "",
          "2.7\nPersonality and Emotion Recog-": "measure or infer traits reliably and how to handle"
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "ity and noise. Some works attempt to add realism",
          "2.7\nPersonality and Emotion Recog-": ""
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "",
          "2.7\nPersonality and Emotion Recog-": "ethical concerns around privacy and user profiling."
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "by including evocative videos or music, but\nthese",
          "2.7\nPersonality and Emotion Recog-": ""
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "too can be limited by cultural context or\nsubjec-",
          "2.7\nPersonality and Emotion Recog-": ""
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "tive interpretation. Our approach,\ninvolving “talk-",
          "2.7\nPersonality and Emotion Recog-": ""
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "",
          "2.7\nPersonality and Emotion Recog-": "2.8\nPersonality-Inspired\nEye-"
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "ing faces” and non-actor participants, aims to cap-",
          "2.7\nPersonality and Emotion Recog-": ""
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "",
          "2.7\nPersonality and Emotion Recog-": "Tracking-Based Emotion Recog-"
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "ture more spontaneous responses—although it still",
          "2.7\nPersonality and Emotion Recog-": ""
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "",
          "2.7\nPersonality and Emotion Recog-": "nition"
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "faces the challenge of bridging the gap between con-",
          "2.7\nPersonality and Emotion Recog-": ""
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "trolled experiments and authentic real-world inter-",
          "2.7\nPersonality and Emotion Recog-": ""
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "",
          "2.7\nPersonality and Emotion Recog-": "By\nintegrating personality\nassessments\ninto\neye-"
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "actions.",
          "2.7\nPersonality and Emotion Recog-": ""
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "",
          "2.7\nPersonality and Emotion Recog-": "tracking-based\nemotion\nrecognition,\nresearchers"
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "",
          "2.7\nPersonality and Emotion Recog-": "can better account for individual gaze patterns and"
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "",
          "2.7\nPersonality and Emotion Recog-": "emotional biases. For example, those high in neu-"
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "2.6\nTemporal Dynamics in Emotion",
          "2.7\nPersonality and Emotion Recog-": ""
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "",
          "2.7\nPersonality and Emotion Recog-": "roticism may fixate\nlonger on negative\nfacial\nfea-"
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "Recognition",
          "2.7\nPersonality and Emotion Recog-": ""
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "",
          "2.7\nPersonality and Emotion Recog-": "tures [33], while extraverts frequently scan positive"
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "The majority\nof\nexisting\nstudies\nconcentrate\non",
          "2.7\nPersonality and Emotion Recog-": "features [34]. Although these insights have shown"
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "static frames or brief\nsegments, often overlooking",
          "2.7\nPersonality and Emotion Recog-": "promise\nin controlled experiments,\nthe\nimpact of"
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "how emotional\nexpressions\nevolve over\ntime.\nAl-",
          "2.7\nPersonality and Emotion Recog-": "personality in more real-world or conversation-like"
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "though valence and arousal can fluctuate rapidly,",
          "2.7\nPersonality and Emotion Recog-": "scenarios\nremains\nless\nexplored.\nPractical\nissues,"
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "few works\nincorporate temporal dynamics in a",
          "2.7\nPersonality and Emotion Recog-": "such as how to maintain calibration for different"
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "comprehensive way. Wang et al.\n[27]\nstress\nthat",
          "2.7\nPersonality and Emotion Recog-": "users and how to fuse personality traits with other"
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "capturing\nreal-time\nchanges\nin\nemotional\nstates",
          "2.7\nPersonality and Emotion Recog-": "modalities, still require further work."
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "presents\nchallenges,\nincluding\nthe need for\nstan-",
          "2.7\nPersonality and Emotion Recog-": "Nevertheless,\npersonality-inspired\nap-"
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "dardized metrics and more sophisticated sequence",
          "2.7\nPersonality and Emotion Recog-": "proaches represent a significant step toward more"
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "modelling. Without a temporal perspective,\nsys-",
          "2.7\nPersonality and Emotion Recog-": "robust,\nuser-centric\nemotion\nrecognition.\nThey"
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "tems\nrisk mislabeling emotions\nthat unfold grad-",
          "2.7\nPersonality and Emotion Recog-": "hint at a future in which systems adapt not only"
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "ually or detecting only fleeting cues—further un-",
          "2.7\nPersonality and Emotion Recog-": "to immediate\nemotional\ncues but also to deeper,"
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "derscoring the need for dynamic modelling of gaze,",
          "2.7\nPersonality and Emotion Recog-": "trait-level\naspects\nof\nusers, making\ninteractions"
        },
        {
          "provide\nvarying degrees\nof diagnosticity\nfor\ncer-": "facial movements, and other signals.",
          "2.7\nPersonality and Emotion Recog-": "more personalized and empathetic."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: Participant Demographics",
      "data": [
        {
          "2.9\nSummary": ""
        },
        {
          "2.9\nSummary": "In\nsummary, while\nnumerous methods\nhave\nad-"
        },
        {
          "2.9\nSummary": "vanced emotion recognition in terms of accuracy"
        },
        {
          "2.9\nSummary": "and modality integration, key challenges persist:"
        },
        {
          "2.9\nSummary": ""
        },
        {
          "2.9\nSummary": "• Over-Reliance on Emotion-Evoking Stim-"
        },
        {
          "2.9\nSummary": "uli: Many studies\nrely on artificially evocative"
        },
        {
          "2.9\nSummary": "materials (e.g., dramatic videos or staged scenes)"
        },
        {
          "2.9\nSummary": "to\nelicit\nstrong\nemotional\nreactions.\nHowever,"
        },
        {
          "2.9\nSummary": "such stimuli may not generalize to everyday face-"
        },
        {
          "2.9\nSummary": "to-face\ncommunication, where\nexpressions\ntend"
        },
        {
          "2.9\nSummary": "to be more subtle and context-dependent."
        },
        {
          "2.9\nSummary": ""
        },
        {
          "2.9\nSummary": "• Neglect of\nIndividual Differences: Person-"
        },
        {
          "2.9\nSummary": ""
        },
        {
          "2.9\nSummary": "ality traits,\ncultural backgrounds, and personal"
        },
        {
          "2.9\nSummary": "experiences heavily shape\nemotional\nresponses,"
        },
        {
          "2.9\nSummary": "but remain underrepresented in many models."
        },
        {
          "2.9\nSummary": "• Limited Attention to Temporal Dynamics:"
        },
        {
          "2.9\nSummary": "Emotions evolve over time, yet temporal changes"
        },
        {
          "2.9\nSummary": "are often treated as static or are underutilized in"
        },
        {
          "2.9\nSummary": "modeling."
        },
        {
          "2.9\nSummary": "Our work seeks to address these gaps using more"
        },
        {
          "2.9\nSummary": "naturalistic\n“talking\nface”\nstimuli,\nincorporating"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "different\nemotions\nand actor demographics,\nsup-": "porting a broader range of expressions and poten-"
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "tially improving generalisability. To simulate face-"
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "to-face dialogue more closely, a short written sce-"
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "nario was displayed before each video, prompting"
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "participants to imagine they were conversing with"
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "the individual\nin the video. This contextual prim-"
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "ing was intended to increase engagement and emo-"
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "tional alignment, even though no real\nturn-taking"
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "occurred."
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "During the\nexperiment,\neye-tracking data were"
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": ""
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "recorded using a GP3 HD eye tracker at a sampling"
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": ""
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "rate of 150 Hz. The eye tracker was calibrated for"
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "each participant using a standard 9-point calibra-"
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "tion procedure. The data collection was\nsynchro-"
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "nized with the presentation of stimuli using the Lab"
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "Streaming Layer (LSL) framework to ensure precise"
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "alignment between eye-tracking data and stimulus"
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "events."
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "Figure 2 illustrates the experimental setup, show-"
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "ing a participant seated in front of the monitor with"
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "sensors attached."
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "Before\nbeginning\nthe\ntrials,\nparticipants\ncom-"
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "pleted the BFI-44 questionnaire [8] to assess their"
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "personality traits (openness, conscientiousness, ex-"
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": ""
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "traversion, agreeableness, neuroticism). After each"
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": ""
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "video, participants\nrated their perceived\nand felt"
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "emotions on 9-point Likert scales for valence (1 ="
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "very negative, 9 = very positive) and arousal (1 ="
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "very calm, 9 = very excited). We chose a 9-point"
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "scale\nfor\nits higher\nresolution, which can capture"
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "subtle affective variations\n[35, 36] more effectively"
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": ""
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "than smaller-scale alternatives. These self-reported"
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": ""
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "ratings formed the ground truth labels for our emo-"
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": ""
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "tion recognition models."
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": ""
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "An example frame from the video stimuli used in"
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": ""
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "the experiment is shown in Figure 3."
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": ""
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "The\n9-point\nscales\nused\nfor\nrating\nemotional"
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "arousal and valence are depicted in Figure 4."
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": ""
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": ""
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "3.3\nData Preprocessing and Feature"
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": ""
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "Extraction"
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": ""
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "Following\ndata\ncollection, we\nperformed\nseveral"
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "preprocessing\nsteps\nto\nensure\nhigh-quality\neye-"
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "tracking signals:"
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "1. Quality Filtering: We discarded data points"
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "flagged\nby\nthe\neye-tracker’s\nalgorithms\n(e.g.,"
        },
        {
          "different\nemotions\nand actor demographics,\nsup-": "blinks, tracking loss)."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 2: Correlations between personality traits",
      "data": [
        {
          "processing steps\n(calibration, baseline\ncorrection,": "and interpolation), thereby offering a robust foun-"
        },
        {
          "processing steps\n(calibration, baseline\ncorrection,": "dation for exploring how temporal eye-tracking pat-"
        },
        {
          "processing steps\n(calibration, baseline\ncorrection,": "terns and personality traits influence perceived and"
        },
        {
          "processing steps\n(calibration, baseline\ncorrection,": "felt emotions."
        },
        {
          "processing steps\n(calibration, baseline\ncorrection,": ""
        },
        {
          "processing steps\n(calibration, baseline\ncorrection,": "4\nStatistical Analysis"
        },
        {
          "processing steps\n(calibration, baseline\ncorrection,": ""
        },
        {
          "processing steps\n(calibration, baseline\ncorrection,": ""
        },
        {
          "processing steps\n(calibration, baseline\ncorrection,": "We examined the relationships between personal-"
        },
        {
          "processing steps\n(calibration, baseline\ncorrection,": ""
        },
        {
          "processing steps\n(calibration, baseline\ncorrection,": "ity traits, eye-tracking metrics, and emotion labels"
        },
        {
          "processing steps\n(calibration, baseline\ncorrection,": ""
        },
        {
          "processing steps\n(calibration, baseline\ncorrection,": "(perceived/felt valence\nand arousal)\nthrough (1)"
        },
        {
          "processing steps\n(calibration, baseline\ncorrection,": ""
        },
        {
          "processing steps\n(calibration, baseline\ncorrection,": "correlation analysis at the participant level and (2)"
        },
        {
          "processing steps\n(calibration, baseline\ncorrection,": "linear mixed-effects\n(LME) modeling at\nthe\ntrial"
        },
        {
          "processing steps\n(calibration, baseline\ncorrection,": ""
        },
        {
          "processing steps\n(calibration, baseline\ncorrection,": "level. Correlation analysis provided an exploratory"
        },
        {
          "processing steps\n(calibration, baseline\ncorrection,": ""
        },
        {
          "processing steps\n(calibration, baseline\ncorrection,": "overview of\naggregated\nassociations, while LME"
        },
        {
          "processing steps\n(calibration, baseline\ncorrection,": ""
        },
        {
          "processing steps\n(calibration, baseline\ncorrection,": "models accounted for repeated measures and indi-"
        },
        {
          "processing steps\n(calibration, baseline\ncorrection,": ""
        },
        {
          "processing steps\n(calibration, baseline\ncorrection,": "vidual differences."
        },
        {
          "processing steps\n(calibration, baseline\ncorrection,": ""
        },
        {
          "processing steps\n(calibration, baseline\ncorrection,": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 3: Personality-emotion correlations for spe- Table5: LMEresultsforpupilmetrics(Bonferroni-",
      "data": [
        {
          "Table 3: Personality-emotion correlations": "cific stimuli",
          "for": "",
          "spe-": "",
          "Table 5: LME results for pupil metrics (Bonferroni-": ""
        },
        {
          "Table 3: Personality-emotion correlations": "Stimulus",
          "for": "Corr.",
          "spe-": "",
          "Table 5: LME results for pupil metrics (Bonferroni-": ""
        },
        {
          "Table 3: Personality-emotion correlations": "",
          "for": "",
          "spe-": "",
          "Table 5: LME results for pupil metrics (Bonferroni-": "Predictor"
        },
        {
          "Table 3: Personality-emotion correlations": "",
          "for": "(p)",
          "spe-": "",
          "Table 5: LME results for pupil metrics (Bonferroni-": ""
        },
        {
          "Table 3: Personality-emotion correlations": "",
          "for": "",
          "spe-": "",
          "Table 5: LME results for pupil metrics (Bonferroni-": "Pupil Mean"
        },
        {
          "Table 3: Personality-emotion correlations": "Happy",
          "for": "0.37",
          "spe-": "",
          "Table 5: LME results for pupil metrics (Bonferroni-": ""
        },
        {
          "Table 3: Personality-emotion correlations": "",
          "for": "",
          "spe-": "",
          "Table 5: LME results for pupil metrics (Bonferroni-": "Pupil Mean"
        },
        {
          "Table 3: Personality-emotion correlations": "",
          "for": "(0.001)",
          "spe-": "",
          "Table 5: LME results for pupil metrics (Bonferroni-": ""
        },
        {
          "Table 3: Personality-emotion correlations": "",
          "for": "",
          "spe-": "",
          "Table 5: LME results for pupil metrics (Bonferroni-": "Pupil Mean"
        },
        {
          "Table 3: Personality-emotion correlations": "Neutral",
          "for": "-0.32",
          "spe-": "",
          "Table 5: LME results for pupil metrics (Bonferroni-": ""
        },
        {
          "Table 3: Personality-emotion correlations": "",
          "for": "",
          "spe-": "",
          "Table 5: LME results for pupil metrics (Bonferroni-": "Pupil Max"
        },
        {
          "Table 3: Personality-emotion correlations": "",
          "for": "(0.006)",
          "spe-": "",
          "Table 5: LME results for pupil metrics (Bonferroni-": ""
        },
        {
          "Table 3: Personality-emotion correlations": "",
          "for": "",
          "spe-": "",
          "Table 5: LME results for pupil metrics (Bonferroni-": "Pupil Max"
        },
        {
          "Table 3: Personality-emotion correlations": "Fear",
          "for": "-0.27",
          "spe-": "",
          "Table 5: LME results for pupil metrics (Bonferroni-": ""
        },
        {
          "Table 3: Personality-emotion correlations": "",
          "for": "",
          "spe-": "",
          "Table 5: LME results for pupil metrics (Bonferroni-": "Pupil Max"
        },
        {
          "Table 3: Personality-emotion correlations": "",
          "for": "(0.022)",
          "spe-": "",
          "Table 5: LME results for pupil metrics (Bonferroni-": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5\nMachine Learning Modeling": "",
          "dation and test data are transformed using param-": "eters derived from training."
        },
        {
          "5\nMachine Learning Modeling": "5.1\nEmotion\nLabeling\nand\nData",
          "dation and test data are transformed using param-": ""
        },
        {
          "5\nMachine Learning Modeling": "Preparation",
          "dation and test data are transformed using param-": "One-Hot Encoding."
        },
        {
          "5\nMachine Learning Modeling": "We\naimed\nto\npredict\nfour\nemotion\nlabels:\nfelt",
          "dation and test data are transformed using param-": "Stimulus emotion (happy, sad, neutral, angry, dis-"
        },
        {
          "5\nMachine Learning Modeling": "valence, perceived valence,\nfelt arousal, and",
          "dation and test data are transformed using param-": "gust,\nfear) was encoded as a 6-dimensional one-hot"
        },
        {
          "5\nMachine Learning Modeling": "perceived arousal.\nDue\nto\nthe\nimprecision in",
          "dation and test data are transformed using param-": "vector. This encoding enables\nthe model\nto treat"
        },
        {
          "5\nMachine Learning Modeling": "self-reported emotions, each label was grouped into",
          "dation and test data are transformed using param-": "each distinct emotion category independently."
        },
        {
          "5\nMachine Learning Modeling": "three classes—low/negative (1–3), medium/neutral",
          "dation and test data are transformed using param-": ""
        },
        {
          "5\nMachine Learning Modeling": "(4–6), and high/positive (7–9) by the ranges spec-",
          "dation and test data are transformed using param-": ""
        },
        {
          "5\nMachine Learning Modeling": "",
          "dation and test data are transformed using param-": "5.3\nNeural Network Architecture"
        },
        {
          "5\nMachine Learning Modeling": "ified.\nThe dataset was\nsplit\ninto training (64%),",
          "dation and test data are transformed using param-": ""
        },
        {
          "5\nMachine Learning Modeling": "validation\n(16%),\nand\ntesting\n(20%)\nsets\nusing",
          "dation and test data are transformed using param-": ""
        },
        {
          "5\nMachine Learning Modeling": "",
          "dation and test data are transformed using param-": "Our neural network (NN) model integrates multiple"
        },
        {
          "5\nMachine Learning Modeling": "stratified splitting to maintain class distributions.",
          "dation and test data are transformed using param-": ""
        },
        {
          "5\nMachine Learning Modeling": "",
          "dation and test data are transformed using param-": "input\nstreams\n(Figure 6). Each stream is prepro-"
        },
        {
          "5\nMachine Learning Modeling": "Following prior works\n[40–42], binning continuous",
          "dation and test data are transformed using param-": ""
        },
        {
          "5\nMachine Learning Modeling": "",
          "dation and test data are transformed using param-": "cessed separately to capture its specific characteris-"
        },
        {
          "5\nMachine Learning Modeling": "valence\nand arousal\nratings\ninto\nthree\ncategories",
          "dation and test data are transformed using param-": ""
        },
        {
          "5\nMachine Learning Modeling": "",
          "dation and test data are transformed using param-": "tics before feature fusion. This architecture ensures"
        },
        {
          "5\nMachine Learning Modeling": "simplifies classification tasks and mitigates subjec-",
          "dation and test data are transformed using param-": ""
        },
        {
          "5\nMachine Learning Modeling": "",
          "dation and test data are transformed using param-": "that temporal features (e.g., eye-tracking data) and"
        },
        {
          "5\nMachine Learning Modeling": "tive variability in self-reported emotions. This ap-",
          "dation and test data are transformed using param-": ""
        },
        {
          "5\nMachine Learning Modeling": "",
          "dation and test data are transformed using param-": "static features (e.g., personality traits, stimuli emo-"
        },
        {
          "5\nMachine Learning Modeling": "proach enables\nclearer distinctions between emo-",
          "dation and test data are transformed using param-": ""
        },
        {
          "5\nMachine Learning Modeling": "",
          "dation and test data are transformed using param-": "tion) are modeled appropriately for\ntheir unique"
        },
        {
          "5\nMachine Learning Modeling": "tional\nstates while\nreducing model\ncomplexity.",
          "dation and test data are transformed using param-": ""
        },
        {
          "5\nMachine Learning Modeling": "",
          "dation and test data are transformed using param-": "roles in predicting emotional states."
        },
        {
          "5\nMachine Learning Modeling": "However, we acknowledge that\nthis may result\nin",
          "dation and test data are transformed using param-": ""
        },
        {
          "5\nMachine Learning Modeling": "",
          "dation and test data are transformed using param-": "Eye-Tracking Data: Processed through LSTM"
        },
        {
          "5\nMachine Learning Modeling": "the loss of finer-grained information.",
          "dation and test data are transformed using param-": ""
        },
        {
          "5\nMachine Learning Modeling": "",
          "dation and test data are transformed using param-": "layers to capture temporal dependencies."
        },
        {
          "5\nMachine Learning Modeling": "",
          "dation and test data are transformed using param-": "Personality\nTraits:\nProcessed\nthrough\nfully"
        },
        {
          "5\nMachine Learning Modeling": "5.2\nFeature\nEngineering\nand\nPre-",
          "dation and test data are transformed using param-": "connected layers."
        },
        {
          "5\nMachine Learning Modeling": "",
          "dation and test data are transformed using param-": "Stimuli Emotion:\nOne-hot\nencoded\nand\npro-"
        },
        {
          "5\nMachine Learning Modeling": "processing",
          "dation and test data are transformed using param-": ""
        },
        {
          "5\nMachine Learning Modeling": "",
          "dation and test data are transformed using param-": "cessed through fully connected layers."
        },
        {
          "5\nMachine Learning Modeling": "Normalization and Scaling.",
          "dation and test data are transformed using param-": ""
        },
        {
          "5\nMachine Learning Modeling": "",
          "dation and test data are transformed using param-": "Environmental Variables:\nProcessed through"
        },
        {
          "5\nMachine Learning Modeling": "",
          "dation and test data are transformed using param-": "fully connected layers."
        },
        {
          "5\nMachine Learning Modeling": "To ensure equal contribution of features and reduce",
          "dation and test data are transformed using param-": ""
        },
        {
          "5\nMachine Learning Modeling": "the risk of bias, we applied consistent preprocess-",
          "dation and test data are transformed using param-": ""
        },
        {
          "5\nMachine Learning Modeling": "",
          "dation and test data are transformed using param-": "To prevent\nthe model\nfrom overfitting on per-"
        },
        {
          "5\nMachine Learning Modeling": "ing techniques.\nPersonality trait\nscores\n(ranging",
          "dation and test data are transformed using param-": ""
        },
        {
          "5\nMachine Learning Modeling": "",
          "dation and test data are transformed using param-": "sonality or environmental variables, small Gaussian"
        },
        {
          "5\nMachine Learning Modeling": "from 0 to 50) were\nscaled by dividing by 50 [43]",
          "dation and test data are transformed using param-": ""
        },
        {
          "5\nMachine Learning Modeling": "",
          "dation and test data are transformed using param-": "noise was added to these\ninputs during training,"
        },
        {
          "5\nMachine Learning Modeling": "to fit\nthe [0, 1]\ninterval.\nSkewed features,\nsuch as",
          "dation and test data are transformed using param-": ""
        },
        {
          "5\nMachine Learning Modeling": "",
          "dation and test data are transformed using param-": "serving as a form of data augmentation. This noise"
        },
        {
          "5\nMachine Learning Modeling": "saccade amplitude and duration, were transformed",
          "dation and test data are transformed using param-": ""
        },
        {
          "5\nMachine Learning Modeling": "",
          "dation and test data are transformed using param-": "regularization helps ensure that the model general-"
        },
        {
          "5\nMachine Learning Modeling": "with MinMaxScaler, which rescales the data based",
          "dation and test data are transformed using param-": ""
        },
        {
          "5\nMachine Learning Modeling": "",
          "dation and test data are transformed using param-": "izes well across participants and experimental con-"
        },
        {
          "5\nMachine Learning Modeling": "on the theoretical min/max values derived from the",
          "dation and test data are transformed using param-": ""
        },
        {
          "5\nMachine Learning Modeling": "",
          "dation and test data are transformed using param-": "ditions."
        },
        {
          "5\nMachine Learning Modeling": "training set. Other continuous\nfeatures\n(e.g., cor-",
          "dation and test data are transformed using param-": ""
        },
        {
          "5\nMachine Learning Modeling": "rected pupil\nsizes,\nenvironmental variables) were",
          "dation and test data are transformed using param-": ""
        },
        {
          "5\nMachine Learning Modeling": "standardized using\nsubtracting\nStandardScaler,",
          "dation and test data are transformed using param-": "5.4\nClassification Approach"
        },
        {
          "5\nMachine Learning Modeling": "the mean and dividing by the standard deviation",
          "dation and test data are transformed using param-": ""
        },
        {
          "5\nMachine Learning Modeling": "",
          "dation and test data are transformed using param-": "Emotion\nprediction was\nframed\nas\na\nthree-class"
        },
        {
          "5\nMachine Learning Modeling": "computed from the\ntraining split only.\nThis ap-",
          "dation and test data are transformed using param-": ""
        },
        {
          "5\nMachine Learning Modeling": "",
          "dation and test data are transformed using param-": "classification task.\nOutputs were passed through"
        },
        {
          "5\nMachine Learning Modeling": "proach avoids data leakage by ensuring that vali-",
          "dation and test data are transformed using param-": ""
        },
        {
          "5\nMachine Learning Modeling": "",
          "dation and test data are transformed using param-": "a softmax activation function to obtain class prob-"
        },
        {
          "5\nMachine Learning Modeling": "",
          "dation and test data are transformed using param-": "abilities. We used categorical\ncross-entropy loss,"
        },
        {
          "5\nMachine Learning Modeling": "",
          "dation and test data are transformed using param-": "with class weights\ninversely proportional\nto class"
        },
        {
          "5\nMachine Learning Modeling": "Table 7: Mixed linear model results for emotion la-",
          "dation and test data are transformed using param-": ""
        },
        {
          "5\nMachine Learning Modeling": "",
          "dation and test data are transformed using param-": "frequencies to address class imbalance."
        },
        {
          "5\nMachine Learning Modeling": "bels and facial regions (with Bonferroni correction)",
          "dation and test data are transformed using param-": ""
        },
        {
          "5\nMachine Learning Modeling": "Emotion Label\nPredictor\nCoefficient (p-value)",
          "dation and test data are transformed using param-": ""
        },
        {
          "5\nMachine Learning Modeling": "Perceived Arousal\nEyebrow\n-0.770 (p = 0.001)",
          "dation and test data are transformed using param-": "5.5\nModel Training and Evaluation"
        },
        {
          "5\nMachine Learning Modeling": "Felt Valence\nEye\n-0.723 (p < 0.001)",
          "dation and test data are transformed using param-": ""
        },
        {
          "5\nMachine Learning Modeling": "",
          "dation and test data are transformed using param-": "For neural network models, we conducted manual"
        },
        {
          "5\nMachine Learning Modeling": "Felt Valence\nMouth\n1.115 (p < 0.001)",
          "dation and test data are transformed using param-": ""
        },
        {
          "5\nMachine Learning Modeling": "",
          "dation and test data are transformed using param-": "and grid search hyperparameter tuning. The search"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 9: shows user agreement var-",
      "data": [
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "dropout\nrates\nin {0.2, 0.3, 0.5}, and weight decay",
          "tions to isolate critical predictors of perceived and": "felt emotions."
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "parameters. The best configurations were selected",
          "tions to isolate critical predictors of perceived and": ""
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "based on macro F1 scores on the validation set.",
          "tions to isolate critical predictors of perceived and": ""
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "",
          "tions to isolate critical predictors of perceived and": "6.1\nComplexity and Agreement"
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "Early stopping was used during training, halting if",
          "tions to isolate critical predictors of perceived and": ""
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "no validation improvement was observed within 10",
          "tions to isolate critical predictors of perceived and": "Emotions\nare\ninfluenced\nby\nstimuli,\nindividual"
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "epochs.",
          "tions to isolate critical predictors of perceived and": "traits,\nand\ncontext,\nmaking\nthem challenging"
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "Hyperparameters such as learning rates, dropout",
          "tions to isolate critical predictors of perceived and": "to model.\nTable\n9\nshows\nuser\nagreement\nvar-"
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "rates, and weight decay were tuned based on val-",
          "tions to isolate critical predictors of perceived and": "ied widely:\nperceived\nvalence\nhad\nthe\nhighest"
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "idation performance. We evaluated models using",
          "tions to isolate critical predictors of perceived and": "agreement\n(77.7%), while\nfelt arousal was\nlowest"
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "the F1-score, suitable for imbalanced datasets as it",
          "tions to isolate critical predictors of perceived and": "(56.0%). Personalized calibration could better ad-"
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "considers both precision and recall.",
          "tions to isolate critical predictors of perceived and": "dress such subjective variability."
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "We\ncompared our NN models\nagainst\nsupport",
          "tions to isolate critical predictors of perceived and": ""
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "vector machines\n(SVMs) as baselines. The SVMs",
          "tions to isolate critical predictors of perceived and": ""
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "",
          "tions to isolate critical predictors of perceived and": "6.2\nModel Performance and Stimu-"
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "used stimuli\nemotion,\nand stimuli\nemotion com-",
          "tions to isolate critical predictors of perceived and": ""
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "",
          "tions to isolate critical predictors of perceived and": "lus Emotion"
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "bined with personality data as input features. The",
          "tions to isolate critical predictors of perceived and": ""
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "SVM baselines were chosen for their simplicity and",
          "tions to isolate critical predictors of perceived and": ""
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "",
          "tions to isolate critical predictors of perceived and": "Higher agreement\n(e.g., perceived valence)\nled to"
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "effectiveness with non-temporal data.\nUnlike\nthe",
          "tions to isolate critical predictors of perceived and": ""
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "",
          "tions to isolate critical predictors of perceived and": "stronger\nperformance.\nOur\nbest model\nreached"
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "NN, which can leverage sequential dependencies in",
          "tions to isolate critical predictors of perceived and": ""
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "",
          "tions to isolate critical predictors of perceived and": "a macro F1\nof\n0.77 for\nperceived\nvalence\n(Ta-"
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "eye-tracking data, the SVMs were limited to static",
          "tions to isolate critical predictors of perceived and": ""
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "",
          "tions to isolate critical predictors of perceived and": "ble\n8).\nStimulus\nemotion\nnotably\nboosted"
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "features\n(e.g.,\nstimuli emotion, personality). This",
          "tions to isolate critical predictors of perceived and": ""
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "",
          "tions to isolate critical predictors of perceived and": "perceived-emotion prediction; an SVM using only"
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "comparison highlights the added value of temporal",
          "tions to isolate critical predictors of perceived and": ""
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "",
          "tions to isolate critical predictors of perceived and": "stimulus emotion already achieved good accuracy."
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "modeling and feature integration in the NN.",
          "tions to isolate critical predictors of perceived and": ""
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "",
          "tions to isolate critical predictors of perceived and": "However,\nthe NN outperformed\nit\non\nfelt\nemo-"
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "",
          "tions to isolate critical predictors of perceived and": "tions by incorporating eye-tracking data, reflecting"
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "",
          "tions to isolate critical predictors of perceived and": "the added benefit of physiological cues for internal"
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "5.6\nResults",
          "tions to isolate critical predictors of perceived and": ""
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "",
          "tions to isolate critical predictors of perceived and": "states."
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "Table 8 presents\nthe F1-scores\nfor different mod-",
          "tions to isolate critical predictors of perceived and": ""
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "els and emotion labels. The best\nresults\nfor each",
          "tions to isolate critical predictors of perceived and": ""
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "",
          "tions to isolate critical predictors of perceived and": "6.3\nMultimodality\nand\nIndividual"
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "emotion label are bolded.",
          "tions to isolate critical predictors of perceived and": ""
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "The\nresults\nindicate\nthat\nintegrating personal-",
          "tions to isolate critical predictors of perceived and": "Differences"
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "ity traits alongside temporal eye-tracking data and",
          "tions to isolate critical predictors of perceived and": ""
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "",
          "tions to isolate critical predictors of perceived and": "Combining stimulus emotion, personality, and eye-"
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "stimuli emotion significantly enhances model per-",
          "tions to isolate critical predictors of perceived and": ""
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "",
          "tions to isolate critical predictors of perceived and": "tracking yielded the highest macro F1-scores (0.77"
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "formance,\nparticularly for\nfelt\nemotions.\nThis",
          "tions to isolate critical predictors of perceived and": ""
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "",
          "tions to isolate critical predictors of perceived and": "for perceived valence, 0.58 for\nfelt valence).\nPer-"
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "supports the notion that felt emotions, being more",
          "tions to isolate critical predictors of perceived and": ""
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "",
          "tions to isolate critical predictors of perceived and": "sonality accounted for individual differences, while"
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "subjective, benefit most\nfrom incorporating high-",
          "tions to isolate critical predictors of perceived and": ""
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "",
          "tions to isolate critical predictors of perceived and": "eye-tracking\nprovided\nreal-time\nphysiological\nin-"
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "level personal\ntrait\ninformation.\nThe SVM base-",
          "tions to isolate critical predictors of perceived and": ""
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "",
          "tions to isolate critical predictors of perceived and": "sights\n[44], and stimulus context clarified percep-"
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "lines performed well on perceived emotions,\nlikely",
          "tions to isolate critical predictors of perceived and": ""
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "",
          "tions to isolate critical predictors of perceived and": "tion."
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "due\nto the direct\ninfluence of\nstimuli on percep-",
          "tions to isolate critical predictors of perceived and": ""
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "tion, but lacked the capability to model sequential",
          "tions to isolate critical predictors of perceived and": ""
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "dependencies, underscoring the importance of our",
          "tions to isolate critical predictors of perceived and": "6.4\nComparison and Future Direc-"
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "temporal NN design.",
          "tions to isolate critical predictors of perceived and": ""
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "",
          "tions to isolate critical predictors of perceived and": "tions"
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "",
          "tions to isolate critical predictors of perceived and": "Compared to\nthe SVM baseline,\nour NN better"
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "6\nDiscussion",
          "tions to isolate critical predictors of perceived and": "captured felt emotions—demonstrating that multi-"
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "",
          "tions to isolate critical predictors of perceived and": "modal data support more nuanced prediction. Re-"
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "We integrated eye-tracking data, personality traits,",
          "tions to isolate critical predictors of perceived and": "maining challenges\ninclude sample representative-"
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "and stimulus-emotion labels\nto\nimprove\nemotion",
          "tions to isolate critical predictors of perceived and": "ness,\ngenuine\ntwo-way\ninteractions,\nclass\nimbal-"
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "recognition in short,\nspeech-containing clips.\nAl-",
          "tions to isolate critical predictors of perceived and": "ance,\nand the need for personalized calibration."
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "though not fully interactive dialogues,\nfocusing on",
          "tions to isolate critical predictors of perceived and": "Future\nexpansions\ncould integrate\nspeech signals"
        },
        {
          "space included learning rates in {10−3, 10−4, 10−5},": "the listener’s perspective offered controlled condi-",
          "tions to isolate critical predictors of perceived and": "or micro-expressions\nand address\ninterpretability"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 8: Model performances in terms of F1-scores and their corresponding hyperparameters (Learning",
      "data": [
        {
          "Low": "",
          "Medium": "",
          "High": "",
          "Macro F1": "",
          "Learning Rate": "",
          "Dropout": ""
        },
        {
          "Low": "0.32",
          "Medium": "0.54",
          "High": "0.17",
          "Macro F1": "0.34",
          "Learning Rate": "0.0002",
          "Dropout": "0.3"
        },
        {
          "Low": "0.35",
          "Medium": "0.22",
          "High": "0.28",
          "Macro F1": "0.28",
          "Learning Rate": "0.0002",
          "Dropout": "0.3"
        },
        {
          "Low": "0.45",
          "Medium": "0.37",
          "High": "0.07",
          "Macro F1": "0.30",
          "Learning Rate": "0.0003",
          "Dropout": "0.2"
        },
        {
          "Low": "0.29",
          "Medium": "0.49",
          "High": "0.24",
          "Macro F1": "0.34",
          "Learning Rate": "0.0002",
          "Dropout": "0.3"
        },
        {
          "Low": "",
          "Medium": "",
          "High": "",
          "Macro F1": "",
          "Learning Rate": "",
          "Dropout": ""
        },
        {
          "Low": "0.18",
          "Medium": "0.57",
          "High": "0.24",
          "Macro F1": "0.33",
          "Learning Rate": "0.00035",
          "Dropout": "0.3"
        },
        {
          "Low": "0.58",
          "Medium": "0.17",
          "High": "0.29",
          "Macro F1": "0.34",
          "Learning Rate": "0.00035",
          "Dropout": "0.3"
        },
        {
          "Low": "0.45",
          "Medium": "0.41",
          "High": "0.23",
          "Macro F1": "0.36",
          "Learning Rate": "0.0003",
          "Dropout": "0.2"
        },
        {
          "Low": "0.32",
          "Medium": "0.46",
          "High": "0.25",
          "Macro F1": "0.34",
          "Learning Rate": "0.0003",
          "Dropout": "0.2"
        },
        {
          "Low": "",
          "Medium": "",
          "High": "",
          "Macro F1": "",
          "Learning Rate": "",
          "Dropout": ""
        },
        {
          "Low": "",
          "Medium": "",
          "High": "",
          "Macro F1": "",
          "Learning Rate": "",
          "Dropout": ""
        },
        {
          "Low": "0.46",
          "Medium": "0.49",
          "High": "0.40",
          "Macro F1": "0.45",
          "Learning Rate": "0.0003",
          "Dropout": "0.2"
        },
        {
          "Low": "0.57",
          "Medium": "0.33",
          "High": "0.29",
          "Macro F1": "0.40",
          "Learning Rate": "0.0002",
          "Dropout": "0.2"
        },
        {
          "Low": "0.58",
          "Medium": "0.58",
          "High": "0.40",
          "Macro F1": "0.52",
          "Learning Rate": "0.0002",
          "Dropout": "0.2"
        },
        {
          "Low": "0.38",
          "Medium": "0.57",
          "High": "0.28",
          "Macro F1": "0.41",
          "Learning Rate": "0.0002",
          "Dropout": "0.2"
        },
        {
          "Low": "",
          "Medium": "",
          "High": "",
          "Macro F1": "",
          "Learning Rate": "",
          "Dropout": ""
        },
        {
          "Low": "",
          "Medium": "",
          "High": "",
          "Macro F1": "",
          "Learning Rate": "",
          "Dropout": ""
        },
        {
          "Low": "0.56",
          "Medium": "0.33",
          "High": "0.58",
          "Macro F1": "0.49",
          "Learning Rate": "0.0002",
          "Dropout": "0.3"
        },
        {
          "Low": "0.77",
          "Medium": "0.56",
          "High": "0.91",
          "Macro F1": "0.75",
          "Learning Rate": "0.0002",
          "Dropout": "0.3"
        },
        {
          "Low": "0.47",
          "Medium": "0.45",
          "High": "0.29",
          "Macro F1": "0.40",
          "Learning Rate": "0.0002",
          "Dropout": "0.2"
        },
        {
          "Low": "0.50",
          "Medium": "0.51",
          "High": "0.54",
          "Macro F1": "0.52",
          "Learning Rate": "0.0003",
          "Dropout": "0.3"
        },
        {
          "Low": "",
          "Medium": "",
          "High": "",
          "Macro F1": "",
          "Learning Rate": "",
          "Dropout": ""
        },
        {
          "Low": "",
          "Medium": "",
          "High": "",
          "Macro F1": "",
          "Learning Rate": "",
          "Dropout": ""
        },
        {
          "Low": "0.63",
          "Medium": "0.48",
          "High": "0.65",
          "Macro F1": "0.59",
          "Learning Rate": "0.0007",
          "Dropout": "0.3"
        },
        {
          "Low": "0.77",
          "Medium": "0.63",
          "High": "0.90",
          "Macro F1": "0.77",
          "Learning Rate": "0.0007",
          "Dropout": "0.3"
        },
        {
          "Low": "0.61",
          "Medium": "0.53",
          "High": "0.48",
          "Macro F1": "0.54",
          "Learning Rate": "0.0004",
          "Dropout": "0.3"
        },
        {
          "Low": "0.53",
          "Medium": "0.62",
          "High": "0.60",
          "Macro F1": "0.58",
          "Learning Rate": "0.0007",
          "Dropout": "0.3"
        },
        {
          "Low": "",
          "Medium": "",
          "High": "",
          "Macro F1": "",
          "Learning Rate": "",
          "Dropout": ""
        },
        {
          "Low": "0.57",
          "Medium": "0.26",
          "High": "0.61",
          "Macro F1": "0.48",
          "Learning Rate": "N/A",
          "Dropout": "N/A"
        },
        {
          "Low": "0.76",
          "Medium": "0.52",
          "High": "0.92",
          "Macro F1": "0.73",
          "Learning Rate": "N/A",
          "Dropout": "N/A"
        },
        {
          "Low": "0.48",
          "Medium": "0.28",
          "High": "0.32",
          "Macro F1": "0.36",
          "Learning Rate": "N/A",
          "Dropout": "N/A"
        },
        {
          "Low": "0.50",
          "Medium": "0.36",
          "High": "0.59",
          "Macro F1": "0.48",
          "Learning Rate": "N/A",
          "Dropout": "N/A"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 9: User Agreement on Emotion Labels (%)",
      "data": [
        {
          "through attention-weight analyses or feature abla-": "tion.",
          "Outlook": ""
        },
        {
          "through attention-weight analyses or feature abla-": "",
          "Outlook": "In essence,\ncombining user-specific\ntraits,"
        },
        {
          "through attention-weight analyses or feature abla-": "or adaptive tutoring, must also consider ethical as-",
          "Outlook": ""
        },
        {
          "through attention-weight analyses or feature abla-": "",
          "Outlook": "logical data, and external cues is essential"
        },
        {
          "through attention-weight analyses or feature abla-": "pects like data privacy and informed consent.",
          "Outlook": ""
        },
        {
          "through attention-weight analyses or feature abla-": "",
          "Outlook": "turing both perceived and felt\nemotions."
        },
        {
          "through attention-weight analyses or feature abla-": "",
          "Outlook": ""
        },
        {
          "through attention-weight analyses or feature abla-": "",
          "Outlook": "systems,\nthese\ninsights\ncan\nfoster more"
        },
        {
          "through attention-weight analyses or feature abla-": "7",
          "Outlook": ""
        },
        {
          "through attention-weight analyses or feature abla-": "",
          "Outlook": "ing\nsolutions—ultimately mirroring\nthe"
        },
        {
          "through attention-weight analyses or feature abla-": "",
          "Outlook": "and subjective nature of human emotions."
        },
        {
          "through attention-weight analyses or feature abla-": "This",
          "Outlook": ""
        },
        {
          "through attention-weight analyses or feature abla-": "sonality traits, and stimulus emotion can jointly en-",
          "Outlook": ""
        }
      ],
      "page": 12
    },
    {
      "caption": "Table 9: User Agreement on Emotion Labels (%)",
      "data": [
        {
          "While\nthese\nrisks are present, we believe\nthey": "are\noutweighed by\nthe benefits\nof\nthis\nresearch,"
        },
        {
          "While\nthese\nrisks are present, we believe\nthey": "which include\nadvancements\nin human-computer"
        },
        {
          "While\nthese\nrisks are present, we believe\nthey": ""
        },
        {
          "While\nthese\nrisks are present, we believe\nthey": "interaction, socially assistive technologies, and ap-"
        },
        {
          "While\nthese\nrisks are present, we believe\nthey": ""
        },
        {
          "While\nthese\nrisks are present, we believe\nthey": "plications\nin mental health and education.\nThe"
        },
        {
          "While\nthese\nrisks are present, we believe\nthey": ""
        },
        {
          "While\nthese\nrisks are present, we believe\nthey": "anonymization of data, clear communication of\nits"
        },
        {
          "While\nthese\nrisks are present, we believe\nthey": ""
        },
        {
          "While\nthese\nrisks are present, we believe\nthey": "use, and emphasis on ethical handling protocols are"
        },
        {
          "While\nthese\nrisks are present, we believe\nthey": ""
        },
        {
          "While\nthese\nrisks are present, we believe\nthey": "key risk mitigation strategies. Our findings could"
        },
        {
          "While\nthese\nrisks are present, we believe\nthey": ""
        },
        {
          "While\nthese\nrisks are present, we believe\nthey": "enable more\nsensitive and context-aware affective"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "computing applications\nthat\nrespect user privacy": "while advancing the field of emotion recognition in",
          "[9] P. Ekman, “An argument for basic emotions,”": "Nature: The nature of human nature, vol. 2,"
        },
        {
          "computing applications\nthat\nrespect user privacy": "a safe and ethically responsible manner.",
          "[9] P. Ekman, “An argument for basic emotions,”": "p. 294, 2005."
        },
        {
          "computing applications\nthat\nrespect user privacy": "",
          "[9] P. Ekman, “An argument for basic emotions,”": "[10]\nI. Siegert, R. B¨ock, B. Vlasenko, D. Philippou-"
        },
        {
          "computing applications\nthat\nrespect user privacy": "References",
          "[9] P. Ekman, “An argument for basic emotions,”": "H¨ubner,\nand A. Wendemuth,\n“Appropriate"
        },
        {
          "computing applications\nthat\nrespect user privacy": "",
          "[9] P. Ekman, “An argument for basic emotions,”": "emotional\nlabelling of non-acted speech using"
        },
        {
          "computing applications\nthat\nrespect user privacy": "[1] D. J. Hughes,\nI. K. Kratsiotis, K. Niven, and",
          "[9] P. Ekman, “An argument for basic emotions,”": "basic emotions, geneva emotion wheel and self"
        },
        {
          "computing applications\nthat\nrespect user privacy": "D. Holman,\n“Personality traits and emotion",
          "[9] P. Ekman, “An argument for basic emotions,”": "assessment manikins,”\nin 2011\nIEEE Inter-"
        },
        {
          "computing applications\nthat\nrespect user privacy": "regulation: A targeted review and recommen-",
          "[9] P. Ekman, “An argument for basic emotions,”": "national Conference on Multimedia and Expo,"
        },
        {
          "computing applications\nthat\nrespect user privacy": "dations.,” Emotion, vol. 20, no. 1, p. 63, 2020.",
          "[9] P. Ekman, “An argument for basic emotions,”": "pp. 1–6, IEEE, 2011."
        },
        {
          "computing applications\nthat\nrespect user privacy": "[2] K. Kaspar and P. K¨onig, “Emotions and per-",
          "[9] P. Ekman, “An argument for basic emotions,”": "[11] A. Mehrabian,\n“Pleasure-arousal-dominance:"
        },
        {
          "computing applications\nthat\nrespect user privacy": "sonality traits as high-level\nfactors\nin visual",
          "[9] P. Ekman, “An argument for basic emotions,”": "A general\nframework for describing and mea-"
        },
        {
          "computing applications\nthat\nrespect user privacy": "attention: a review,” Frontiers in human neu-",
          "[9] P. Ekman, “An argument for basic emotions,”": "suring individual differences in temperament,”"
        },
        {
          "computing applications\nthat\nrespect user privacy": "roscience, vol. 6, p. 321, 2012.",
          "[9] P. Ekman, “An argument for basic emotions,”": "Current\nPsychology,\nvol.\n14,\npp.\n261–292,"
        },
        {
          "computing applications\nthat\nrespect user privacy": "",
          "[9] P. Ekman, “An argument for basic emotions,”": "1996."
        },
        {
          "computing applications\nthat\nrespect user privacy": "[3] J. F. Rauthmann, C. T. Seubert, P. Sachse,",
          "[9] P. Ekman, “An argument for basic emotions,”": ""
        },
        {
          "computing applications\nthat\nrespect user privacy": "and M. R. Furtner,\n“Eyes\nas windows\nto",
          "[9] P. Ekman, “An argument for basic emotions,”": "[12] D. Kollias,\nP.\nTzirakis, M.\nA.\nNicolaou,"
        },
        {
          "computing applications\nthat\nrespect user privacy": "the\nsoul: Gazing behavior\nis\nrelated to per-",
          "[9] P. Ekman, “An argument for basic emotions,”": "A. Papaioannou, G. Zhao, B. Schuller,\nand"
        },
        {
          "computing applications\nthat\nrespect user privacy": "sonality,” Journal of Research in Personality,",
          "[9] P. Ekman, “An argument for basic emotions,”": "S.\nZafeiriou,\n“Abaw:\nValence-arousal\nesti-"
        },
        {
          "computing applications\nthat\nrespect user privacy": "vol. 46, no. 2, pp. 147–156, 2012.",
          "[9] P. Ekman, “An argument for basic emotions,”": "mation,\nexpression\nrecognition,\naction\nunit"
        },
        {
          "computing applications\nthat\nrespect user privacy": "",
          "[9] P. Ekman, “An argument for basic emotions,”": "detection & multi-task\nlearning\nchallenges,”"
        },
        {
          "computing applications\nthat\nrespect user privacy": "[4] H. Cao, D. G. Cooper, M. K. Keutmann, R. C.",
          "[9] P. Ekman, “An argument for basic emotions,”": "in Proceedings of\nthe IEEE/CVF Conference"
        },
        {
          "computing applications\nthat\nrespect user privacy": "Gur, A. Nenkova, and R. Verma, “Crema-d:",
          "[9] P. Ekman, “An argument for basic emotions,”": "on Computer Vision and Pattern Recognition"
        },
        {
          "computing applications\nthat\nrespect user privacy": "Crowd-sourced emotional multimodal\nactors",
          "[9] P. Ekman, “An argument for basic emotions,”": "Workshops (CVPRW), pp. 10790–10800, 2022."
        },
        {
          "computing applications\nthat\nrespect user privacy": "dataset,” IEEE transactions on affective com-",
          "[9] P. Ekman, “An argument for basic emotions,”": ""
        },
        {
          "computing applications\nthat\nrespect user privacy": "puting, vol. 5, no. 4, pp. 377–390, 2014.",
          "[9] P. Ekman, “An argument for basic emotions,”": "[13] X. Li and Y. Chen, “Emotion recognition us-"
        },
        {
          "computing applications\nthat\nrespect user privacy": "",
          "[9] P. Ekman, “An argument for basic emotions,”": "ing different\nsensors,\nemotion models, meth-"
        },
        {
          "computing applications\nthat\nrespect user privacy": "[5] R. Fang, R. Zhang, E. Hosseini, C. Fang,",
          "[9] P. Ekman, “An argument for basic emotions,”": "ods,\nand\ndatasets:\nA\ncomprehensive\nre-"
        },
        {
          "computing applications\nthat\nrespect user privacy": "M. Eslaminehr, S. Rafatirad, and H. Homay-",
          "[9] P. Ekman, “An argument for basic emotions,”": "view,” Frontiers\nin Neuroergonomics, vol. 5,"
        },
        {
          "computing applications\nthat\nrespect user privacy": "oun,\n“Apex:\nAttention\non\npersonality",
          "[9] P. Ekman, “An argument for basic emotions,”": "no. 1338243, 2024."
        },
        {
          "computing applications\nthat\nrespect user privacy": "based emotion rexgnition framework,” arXiv",
          "[9] P. Ekman, “An argument for basic emotions,”": ""
        },
        {
          "computing applications\nthat\nrespect user privacy": "preprint arXiv:2409.06118, 2024.",
          "[9] P. Ekman, “An argument for basic emotions,”": "[14] L. Zhang and M. Wang, “Survey of deep emo-"
        },
        {
          "computing applications\nthat\nrespect user privacy": "",
          "[9] P. Ekman, “An argument for basic emotions,”": "tion recognition in dynamic data using facial,"
        },
        {
          "computing applications\nthat\nrespect user privacy": "[6] M. Mostafa,\nT.\nCrick,\nA.\nC.\nCalderon,",
          "[9] P. Ekman, “An argument for basic emotions,”": "speech, and textual\ncues,” Frontiers\nin Psy-"
        },
        {
          "computing applications\nthat\nrespect user privacy": "and G. Oatley,\n“Incorporating\nemotion and",
          "[9] P. Ekman, “An argument for basic emotions,”": "chology, vol. 14, p. 10978716, 2023."
        },
        {
          "computing applications\nthat\nrespect user privacy": "personality-based\nanalysis\nin\nuser-centered",
          "[9] P. Ekman, “An argument for basic emotions,”": ""
        },
        {
          "computing applications\nthat\nrespect user privacy": "modelling,” in Research and Development\nin",
          "[9] P. Ekman, “An argument for basic emotions,”": "[15] G. Mohammadi and P. Vuilleumier, “A multi-"
        },
        {
          "computing applications\nthat\nrespect user privacy": "Intelligent Systems XXXIII: Incorporating Ap-",
          "[9] P. Ekman, “An argument for basic emotions,”": "componential\napproach\nto\nemotion\nrecogni-"
        },
        {
          "computing applications\nthat\nrespect user privacy": "plications and Innovations\nin Intelligent Sys-",
          "[9] P. Ekman, “An argument for basic emotions,”": "tion\nand\nthe\neffect\nof\npersonality,”\nIEEE"
        },
        {
          "computing applications\nthat\nrespect user privacy": "tems XXIV 33, pp. 383–389, Springer, 2016.",
          "[9] P. Ekman, “An argument for basic emotions,”": "Transactions on Affective Computing, vol. 13,"
        },
        {
          "computing applications\nthat\nrespect user privacy": "",
          "[9] P. Ekman, “An argument for basic emotions,”": "p. 1127–1139, July 2022."
        },
        {
          "computing applications\nthat\nrespect user privacy": "[7] O. P. John, E. M. Donahue, and R. L. Ken-",
          "[9] P. Ekman, “An argument for basic emotions,”": ""
        },
        {
          "computing applications\nthat\nrespect user privacy": "tle, “Big five inventory,” Journal of personality",
          "[9] P. Ekman, “An argument for basic emotions,”": "[16] M. J. Seikavandi, M. J. Barrett, and P. Bu-"
        },
        {
          "computing applications\nthat\nrespect user privacy": "and social psychology, 1991.",
          "[9] P. Ekman, “An argument for basic emotions,”": "relli, “Modeling face emotion perception from"
        },
        {
          "computing applications\nthat\nrespect user privacy": "",
          "[9] P. Ekman, “An argument for basic emotions,”": "naturalistic\nface viewing:\nInsights\nfrom fixa-"
        },
        {
          "computing applications\nthat\nrespect user privacy": "[8] A. Fossati,\nS. Borroni, D. Marchione,\nand",
          "[9] P. Ekman, “An argument for basic emotions,”": "tional\nevents and gaze\nstrategies,” in Recent"
        },
        {
          "computing applications\nthat\nrespect user privacy": "C. Maffei, “The big five inventory (bfi),” Eu-",
          "[9] P. Ekman, “An argument for basic emotions,”": "Advances in Deep Learning Applications: New"
        },
        {
          "computing applications\nthat\nrespect user privacy": "ropean Journal\nof Psychological Assessment,",
          "[9] P. Ekman, “An argument for basic emotions,”": "Techniques and Practical Examples, Taylor &"
        },
        {
          "computing applications\nthat\nrespect user privacy": "2011.",
          "[9] P. Ekman, “An argument for basic emotions,”": "Francis, 2025."
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "takis, D. Manousos, I. Karatzanis, N. Tachos,",
          "of the North American Chapter of the Associa-": "tion for Computational Linguistics (NAACL),"
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "E. Tripoliti, K. Marias, D.\nI. Fotiadis,\nand",
          "of the North American Chapter of the Associa-": "2024."
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "M. Tsiknakis, “Review of Eye Tracking Met-",
          "of the North American Chapter of the Associa-": ""
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "",
          "of the North American Chapter of the Associa-": "[26] R. Chen and Q. Liu, “esee-d: Emotional state"
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "rics Involved in Emotional and Cognitive Pro-",
          "of the North American Chapter of the Associa-": ""
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "",
          "of the North American Chapter of the Associa-": "estimation\nbased\non\neye-tracking\ndataset,”"
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "cesses,” Ieee Reviews in Biomedical Engineer-",
          "of the North American Chapter of the Associa-": ""
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "",
          "of the North American Chapter of the Associa-": "ArXiv Preprint, vol. arXiv:2403.11590, 2024."
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "ing, vol. 16, pp. 260–277, 2023.",
          "of the North American Chapter of the Associa-": ""
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "",
          "of the North American Chapter of the Associa-": "[27] Y. Wang and S. Gao,\n“Emotion recognition"
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "[18] Y. Lu, W.-L. Zheng, B. Li,\nand B.-L. Lu,",
          "of the North American Chapter of the Associa-": ""
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "",
          "of the North American Chapter of the Associa-": "in adaptive virtual reality settings: Challenges"
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "“Combining\neye movements\nand\neeg\nto\nen-",
          "of the North American Chapter of the Associa-": ""
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "",
          "of the North American Chapter of the Associa-": "and opportunities,” IEEE Transactions on Af-"
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "hance emotion recognition.,” in IJCAI, vol. 15,",
          "of the North American Chapter of the Associa-": ""
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "",
          "of the North American Chapter of the Associa-": "fective Computing, 2023."
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "pp. 1170–1176, Buenos Aires, 2015.",
          "of the North American Chapter of the Associa-": ""
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "",
          "of the North American Chapter of the Associa-": "[28] E. G. Kehoe, J. M. Toomey, J. H. Balsters, and"
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "[19] L. J. Zheng, J. Mountstephens, and J. T. T.",
          "of the North American Chapter of the Associa-": ""
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "",
          "of the North American Chapter of the Associa-": "A. L. Bokde, “Personality modulates\nthe\nef-"
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "Wi,\n“Multiclass\nemotion classification using",
          "of the North American Chapter of the Associa-": ""
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "",
          "of the North American Chapter of the Associa-": "fects of emotional arousal and valence on brain"
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "pupil\nsize\nin vr: Tuning support vector ma-",
          "of the North American Chapter of the Associa-": ""
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "",
          "of the North American Chapter of the Associa-": "activation,” Social cognitive and affective neu-"
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "chines\nto\nimprove\nperformance,”\nin\nJour-",
          "of the North American Chapter of the Associa-": ""
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "",
          "of the North American Chapter of the Associa-": "roscience, vol. 7, no. 7, pp. 858–870, 2012."
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "nal of Physics: Conference Series, vol. 1529,",
          "of the North American Chapter of the Associa-": ""
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "p. 052062, IOP Publishing, 2020.",
          "of the North American Chapter of the Associa-": "[29] A. J. Zautra, G. G. Affleck, H. Tennen, J. W."
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "",
          "of the North American Chapter of the Associa-": "Reich, and M. C. Davis, “Dynamic approaches"
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "[20] S. Wu, Z. Du, W. Li, D. Huang, and Y. Wang,",
          "of the North American Chapter of the Associa-": ""
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "",
          "of the North American Chapter of the Associa-": "to emotions and stress\nin everyday life: Bol-"
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "“Continuous emotion recognition in videos by",
          "of the North American Chapter of the Associa-": ""
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "",
          "of the North American Chapter of the Associa-": "ger and zuckerman reloaded with positive as"
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "fusing\nfacial\nexpression,\nhead\npose\nand\neye",
          "of the North American Chapter of the Associa-": ""
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "",
          "of the North American Chapter of the Associa-": "well as negative affects,” Journal of personal-"
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "gaze,”\nin\n2019\nInternational Conference\non",
          "of the North American Chapter of the Associa-": ""
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "",
          "of the North American Chapter of the Associa-": "ity, vol. 73, no. 6, pp. 1511–1538, 2005."
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "Multimodal Interaction, pp. 40–48, 2019.",
          "of the North American Chapter of the Associa-": ""
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "",
          "of the North American Chapter of the Associa-": "[30] P. T. Costa\nand R. R. McCrae,\n“Influence"
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "[21] L. Chaby, I. Hupont, M. Avril, V. Luherne-du",
          "of the North American Chapter of the Associa-": ""
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "",
          "of the North American Chapter of the Associa-": "of\nextraversion\nand\nneuroticism on\nsubjec-"
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "Boullay,\nand M. Chetouani,\n“Gaze behavior",
          "of the North American Chapter of the Associa-": ""
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "",
          "of the North American Chapter of the Associa-": "tive well-being: happy and unhappy people.,”"
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "consistency among older and younger adults",
          "of the North American Chapter of the Associa-": ""
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "",
          "of the North American Chapter of the Associa-": "Journal of personality and social psychology,"
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "when looking at emotional faces,” Frontiers in",
          "of the North American Chapter of the Associa-": ""
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "",
          "of the North American Chapter of the Associa-": "vol. 38, no. 4, p. 668, 1980."
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "Psychology, vol. 8, p. 548, 2017.",
          "of the North American Chapter of the Associa-": ""
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "",
          "of the North American Chapter of the Associa-": "[31] T. Ait Challal and O. Grynszpan, “What gaze"
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "[22] A.\nCoutrot,\nN.\nBinetti,\nC.\nHarrison,",
          "of the North American Chapter of the Associa-": ""
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "",
          "of the North American Chapter of the Associa-": "tells us about personality,” in Proceedings of"
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "I. Mareschal,\nand A.\nJohnston,\n“Face\nex-",
          "of the North American Chapter of the Associa-": ""
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "",
          "of the North American Chapter of the Associa-": "the 6th International Conference on Human-"
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "ploration\ndynamics\ndifferentiate\nmen\nand",
          "of the North American Chapter of the Associa-": ""
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "",
          "of the North American Chapter of the Associa-": "Agent Interaction, pp. 129–137, 2018."
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "women,”\nJournal\nof\nvision,\nvol.\n16,\nno.\n14,",
          "of the North American Chapter of the Associa-": ""
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "pp. 16–16, 2016.",
          "of the North American Chapter of the Associa-": "[32] L. Chen, W. Cai, D. Yan, and S. Berkovsky,"
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "",
          "of the North American Chapter of the Associa-": "“Eye-tracking-based\npersonality\nprediction"
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "[23] M.\nSchurgin,\nJ. Nelson,\nS.\nIida, H. Ohira,",
          "of the North American Chapter of the Associa-": "with recommendation interfaces,” User Mod-"
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "J. Chiao, and S. Franconeri, “Eye movements",
          "of the North American Chapter of the Associa-": "eling\nand User-Adapted\nInteraction,\nvol.\n33,"
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "during emotion recognition in faces,” Journal",
          "of the North American Chapter of the Associa-": "pp. 121–157, Mar. 2023."
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "of vision, vol. 14, no. 13, pp. 14–14, 2014.",
          "of the North American Chapter of the Associa-": ""
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "",
          "of the North American Chapter of the Associa-": "[33] L.\nChen,\nX.\nLiu,\nX. Weng, M.\nHuang,"
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "[24] J. Liang, Y.-Q. Zou, S.-Y. Liang, Y.-W. Wu,",
          "of the North American Chapter of the Associa-": "Y. Weng, H. Zeng, Y. Li, D. Zheng,\nand"
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "and W.-J. Yan, “Emotional gaze: The effects",
          "of the North American Chapter of the Associa-": "C. Chen, “The emotion regulation mechanism"
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "of gaze direction on the perception of\nfacial",
          "of the North American Chapter of the Associa-": "in neurotic individuals: The potential\nrole of"
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "emotions,” Frontiers\nin psychology,\nvol.\n12,",
          "of the North American Chapter of the Associa-": "mindfulness and cognitive bias,” International"
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "p. 684357, 2021.",
          "of the North American Chapter of the Associa-": "Journal of Environmental Research and Public"
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "",
          "of the North American Chapter of the Associa-": "Health, vol. 20, no. 2, p. 896, 2023."
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "[25] S. Park and J. Lee, “Modality effects on emo-",
          "of the North American Chapter of the Associa-": ""
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "tion perception in english by chinese l2 english",
          "of the North American Chapter of the Associa-": "[34] B. W. Haas, R. T. Constable, and T. Canli,"
        },
        {
          "[17] V.\nSkaramagkas, G. Giannakakis,\nE. Ktis-": "users: An eye-tracking study,” in Proceedings",
          "of the North American Chapter of the Associa-": "“Stop the sadness: Neuroticism is associated"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "sponse to emotional\nfacial expressions,” Neu-",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": "“Scikit-learn: Machine\nlearning\nin python,”"
        },
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "roimage, vol. 42, no. 1, pp. 385–392, 2008.",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": "Journal of machine learning research, vol. 12,"
        },
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": "no. Oct, pp. 2825–2830, 2011."
        },
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "[35] P.\nJ.\nLang, M. M.\nBradley,\nand\nB.\nN.",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": ""
        },
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "Cuthbert, “Affective norms\nfor english words",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": "[44] P. Tarnowski, M. Ko(cid:32)lodziej, A. Majkowski,"
        },
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "(anew):\nAffective\nratings\nof words\nand\nin-",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": "and R. J. Rak, “Eye-tracking analysis for emo-"
        },
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "structions\nfor use,” Behavior Research Meth-",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": "tion recognition,” Computational\nintelligence"
        },
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "ods, vol. 51, no. 4, pp. 1246–1265, 2019.",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": "and neuroscience, vol. 2020, 2020."
        },
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "[36] C.\nF. Benitez-Quiroz,\nR. B. Wilbur,\nand",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": ""
        },
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "A. M. Martinez, “Improving the measurement",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": ""
        },
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "of emotional responses with fine-grained likert",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": ""
        },
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "scales,” Emotion Review, vol. 14, no. 1, pp. 26–",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": ""
        },
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "36, 2022.",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": ""
        },
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "[37] B. Amos, B. Ludwiczuk, M. Satyanarayanan,",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": ""
        },
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "et\nal.,\n“Openface:\nA general-purpose\nface",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": ""
        },
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "recognition library with mobile applications,”",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": ""
        },
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "CMU School\nof Computer\nScience,\nvol.\n6,",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": ""
        },
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "no. 2, p. 20, 2016.",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": ""
        },
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "[38] R.\nAdipranata,\nC.\nG.\nBallangan,\nR.\nP.",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": ""
        },
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "Ongkodjojo, et al., “Fast method for multiple",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": ""
        },
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "human face segmentation in color image,” In-",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": ""
        },
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "ternational Journal of Advanced Science and",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": ""
        },
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "Technology, vol. 3, pp. 19–32, 2009.",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": ""
        },
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "[39] A. Graves,\nS. Fern´andez,\nand J. Schmidhu-",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": ""
        },
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "ber, “Bidirectional lstm networks for improved",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": ""
        },
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "phoneme classification and recognition,” in In-",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": ""
        },
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "ternational conference on artificial neural net-",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": ""
        },
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "works, pp. 799–804, Springer, 2005.",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": ""
        },
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "[40] L. Fiorini, F. Bossi, and F. Di Gruttola, “Eeg-",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": ""
        },
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "based emotional valence and emotion regula-",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": ""
        },
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "tion classification: a data-centric and explain-",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": ""
        },
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "able\napproach,”\nScientific Reports,\nvol.\n14,",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": ""
        },
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "no. 1, p. 24046, 2024.",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": ""
        },
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "[41] J. Heffner and O. FeldmanHall, “A probabilis-",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": ""
        },
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "tic map of emotional experiences during com-",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": ""
        },
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "petitive social\ninteractions,” Nature communi-",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": ""
        },
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "cations, vol. 13, no. 1, p. 1718, 2022.",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": ""
        },
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "[42] N. Garg, R. Garg, A. Anand, and V. Baths,",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": ""
        },
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "“Decoding the neural signatures of valence and",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": ""
        },
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "arousal\nfrom portable eeg headset,” Frontiers",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": ""
        },
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "in Human Neuroscience, vol. 16, p. 1051463,",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": ""
        },
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "2022.",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": ""
        },
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "[43] F. Pedregosa, G. Varoquaux, A. Gramfort,",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": ""
        },
        {
          "with\nsustained medial\nprefrontal\ncortex\nre-": "V. Michel, B. Thirion, O. Grisel, M. Blondel,",
          "P. Prettenhofer, R. Weiss, V. Dubourg, et al.,": ""
        }
      ],
      "page": 15
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Personality traits and emotion regulation: A targeted review and recommendations",
      "authors": [
        "D Hughes",
        "I Kratsiotis",
        "K Niven",
        "D Holman"
      ],
      "year": "2020",
      "venue": "Emotion"
    },
    {
      "citation_id": "2",
      "title": "Emotions and personality traits as high-level factors in visual attention: a review",
      "authors": [
        "K Kaspar",
        "P König"
      ],
      "year": "2012",
      "venue": "Frontiers in human neuroscience"
    },
    {
      "citation_id": "3",
      "title": "Eyes as windows to the soul: Gazing behavior is related to personality",
      "authors": [
        "J Rauthmann",
        "C Seubert",
        "P Sachse",
        "M Furtner"
      ],
      "year": "2012",
      "venue": "Journal of Research in Personality"
    },
    {
      "citation_id": "4",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "5",
      "title": "Apex: Attention on personality based emotion rexgnition framework",
      "authors": [
        "R Fang",
        "R Zhang",
        "E Hosseini",
        "C Fang",
        "M Eslaminehr",
        "S Rafatirad",
        "H Homayoun"
      ],
      "year": "2024",
      "venue": "Apex: Attention on personality based emotion rexgnition framework",
      "arxiv": "arXiv:2409.06118"
    },
    {
      "citation_id": "6",
      "title": "Incorporating emotion and personality-based analysis in user-centered modelling",
      "authors": [
        "M Mostafa",
        "T Crick",
        "A Calderon",
        "G Oatley"
      ],
      "year": "2016",
      "venue": "Research and Development in Intelligent Systems XXXIII: Incorporating Applications and Innovations in Intelligent Systems XXIV"
    },
    {
      "citation_id": "7",
      "title": "Big five inventory",
      "authors": [
        "O John",
        "E Donahue",
        "R Kentle"
      ],
      "year": "1991",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "8",
      "title": "The big five inventory (bfi)",
      "authors": [
        "A Fossati",
        "S Borroni",
        "D Marchione",
        "C Maffei"
      ],
      "year": "2011",
      "venue": "The big five inventory (bfi)"
    },
    {
      "citation_id": "9",
      "title": "Nature: The nature of human nature",
      "authors": [
        "P Ekman"
      ],
      "year": "2005",
      "venue": "Nature: The nature of human nature"
    },
    {
      "citation_id": "10",
      "title": "Appropriate emotional labelling of non-acted speech using basic emotions, geneva emotion wheel and self assessment manikins",
      "authors": [
        "I Siegert",
        "R Böck",
        "B Vlasenko",
        "D Philippou-Hübner",
        "A Wendemuth"
      ],
      "year": "2011",
      "venue": "2011 IEEE International Conference on Multimedia and Expo"
    },
    {
      "citation_id": "11",
      "title": "Pleasure-arousal-dominance: A general framework for describing and measuring individual differences in temperament",
      "authors": [
        "A Mehrabian"
      ],
      "year": "1996",
      "venue": "Current Psychology"
    },
    {
      "citation_id": "12",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & multi-task learning challenges",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "13",
      "title": "Emotion recognition using different sensors, emotion models, methods, and datasets: A comprehensive review",
      "authors": [
        "X Li",
        "Y Chen"
      ],
      "year": "2024",
      "venue": "Frontiers in Neuroergonomics"
    },
    {
      "citation_id": "14",
      "title": "Survey of deep emotion recognition in dynamic data using facial, speech, and textual cues",
      "authors": [
        "L Zhang",
        "M Wang"
      ],
      "year": "2023",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "15",
      "title": "A multicomponential approach to emotion recognition and the effect of personality",
      "authors": [
        "G Mohammadi",
        "P Vuilleumier"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "Modeling face emotion perception from naturalistic face viewing: Insights from fixational events and gaze strategies",
      "authors": [
        "M Seikavandi",
        "M Barrett",
        "P Burelli"
      ],
      "year": "2025",
      "venue": "Recent Advances in Deep Learning Applications: New Techniques and Practical Examples"
    },
    {
      "citation_id": "17",
      "title": "Review of Eye Tracking Metrics Involved in Emotional and Cognitive Processes",
      "authors": [
        "V Skaramagkas",
        "G Giannakakis",
        "E Ktistakis",
        "D Manousos",
        "I Karatzanis",
        "N Tachos",
        "E Tripoliti",
        "K Marias",
        "D Fotiadis",
        "M Tsiknakis"
      ],
      "year": "2023",
      "venue": "Ieee Reviews in Biomedical Engineering"
    },
    {
      "citation_id": "18",
      "title": "Combining eye movements and eeg to enhance emotion recognition",
      "authors": [
        "Y Lu",
        "W.-L Zheng",
        "B Li",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IJCAI"
    },
    {
      "citation_id": "19",
      "title": "Multiclass emotion classification using pupil size in vr: Tuning support vector machines to improve performance",
      "authors": [
        "L Zheng",
        "J Mountstephens",
        "J Wi"
      ],
      "year": "2020",
      "venue": "Journal of Physics: Conference Series"
    },
    {
      "citation_id": "20",
      "title": "Continuous emotion recognition in videos by fusing facial expression, head pose and eye gaze",
      "authors": [
        "S Wu",
        "Z Du",
        "W Li",
        "D Huang",
        "Y Wang"
      ],
      "year": "2019",
      "venue": "2019 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "21",
      "title": "Gaze behavior consistency among older and younger adults when looking at emotional faces",
      "authors": [
        "L Chaby",
        "I Hupont",
        "M Avril",
        "V Luherne-Du Boullay",
        "M Chetouani"
      ],
      "year": "2017",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "22",
      "title": "Face exploration dynamics differentiate men and women",
      "authors": [
        "A Coutrot",
        "N Binetti",
        "C Harrison",
        "I Mareschal",
        "A Johnston"
      ],
      "year": "2016",
      "venue": "Journal of vision"
    },
    {
      "citation_id": "23",
      "title": "Eye movements during emotion recognition in faces",
      "authors": [
        "M Schurgin",
        "J Nelson",
        "S Iida",
        "H Ohira",
        "J Chiao",
        "S Franconeri"
      ],
      "year": "2014",
      "venue": "Journal of vision"
    },
    {
      "citation_id": "24",
      "title": "Emotional gaze: The effects of gaze direction on the perception of facial emotions",
      "authors": [
        "J Liang",
        "Y.-Q Zou",
        "S.-Y Liang",
        "Y.-W Wu",
        "W.-J Yan"
      ],
      "year": "2021",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "25",
      "title": "Modality effects on emotion perception in english by chinese l2 english users: An eye-tracking study",
      "authors": [
        "S Park",
        "J Lee"
      ],
      "venue": "Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL)"
    },
    {
      "citation_id": "26",
      "title": "esee-d: Emotional state estimation based on eye-tracking dataset",
      "authors": [
        "R Chen",
        "Q Liu"
      ],
      "year": "2024",
      "venue": "ArXiv Preprint",
      "arxiv": "arXiv:2403.11590"
    },
    {
      "citation_id": "27",
      "title": "Emotion recognition in adaptive virtual reality settings: Challenges and opportunities",
      "authors": [
        "Y Wang",
        "S Gao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "28",
      "title": "Personality modulates the effects of emotional arousal and valence on brain activation",
      "authors": [
        "E Kehoe",
        "J Toomey",
        "J Balsters",
        "A Bokde"
      ],
      "year": "2012",
      "venue": "Social cognitive and affective neuroscience"
    },
    {
      "citation_id": "29",
      "title": "Dynamic approaches to emotions and stress in everyday life: Bolger and zuckerman reloaded with positive as well as negative affects",
      "authors": [
        "A Zautra",
        "G Affleck",
        "H Tennen",
        "J Reich",
        "M Davis"
      ],
      "year": "2005",
      "venue": "Journal of personality"
    },
    {
      "citation_id": "30",
      "title": "Influence of extraversion and neuroticism on subjective well-being: happy and unhappy people",
      "authors": [
        "P Costa",
        "R Mccrae"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "31",
      "title": "What gaze tells us about personality",
      "authors": [
        "T Challal",
        "O Grynszpan"
      ],
      "year": "2018",
      "venue": "Proceedings of the 6th International Conference on Human-Agent Interaction"
    },
    {
      "citation_id": "32",
      "title": "Eye-tracking-based personality prediction with recommendation interfaces",
      "authors": [
        "L Chen",
        "W Cai",
        "D Yan",
        "S Berkovsky"
      ],
      "year": "2023",
      "venue": "User Modeling and User-Adapted Interaction"
    },
    {
      "citation_id": "33",
      "title": "The emotion regulation mechanism in neurotic individuals: The potential role of mindfulness and cognitive bias",
      "authors": [
        "L Chen",
        "X Liu",
        "X Weng",
        "M Huang",
        "Y Weng",
        "H Zeng",
        "Y Li",
        "D Zheng",
        "C Chen"
      ],
      "year": "2023",
      "venue": "International Journal of Environmental Research and Public Health"
    },
    {
      "citation_id": "34",
      "title": "Stop the sadness: Neuroticism is associated with sustained medial prefrontal cortex response to emotional facial expressions",
      "authors": [
        "B Haas",
        "R Constable",
        "T Canli"
      ],
      "year": "2008",
      "venue": "Neuroimage"
    },
    {
      "citation_id": "35",
      "title": "Affective norms for english words (anew): Affective ratings of words and instructions for use",
      "authors": [
        "P Lang",
        "M Bradley",
        "B Cuthbert"
      ],
      "year": "2019",
      "venue": "Behavior Research Methods"
    },
    {
      "citation_id": "36",
      "title": "Improving the measurement of emotional responses with fine-grained likert scales",
      "authors": [
        "C Benitez-Quiroz",
        "R Wilbur",
        "A Martinez"
      ],
      "year": "2022",
      "venue": "Emotion Review"
    },
    {
      "citation_id": "37",
      "title": "Openface: A general-purpose face recognition library with mobile applications",
      "authors": [
        "B Amos",
        "B Ludwiczuk",
        "M Satyanarayanan"
      ],
      "year": "2016",
      "venue": "CMU School of Computer Science"
    },
    {
      "citation_id": "38",
      "title": "Fast method for multiple human face segmentation in color image",
      "authors": [
        "R Adipranata",
        "C Ballangan",
        "R Ongkodjojo"
      ],
      "year": "2009",
      "venue": "International Journal of Advanced Science and Technology"
    },
    {
      "citation_id": "39",
      "title": "Bidirectional lstm networks for improved phoneme classification and recognition",
      "authors": [
        "A Graves",
        "S Fernández",
        "J Schmidhuber"
      ],
      "year": "2005",
      "venue": "Bidirectional lstm networks for improved phoneme classification and recognition"
    },
    {
      "citation_id": "40",
      "title": "Eegbased emotional valence and emotion regulation classification: a data-centric and explainable approach",
      "authors": [
        "L Fiorini",
        "F Bossi",
        "F Di Gruttola"
      ],
      "year": "2024",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "41",
      "title": "A probabilistic map of emotional experiences during competitive social interactions",
      "authors": [
        "J Heffner",
        "O Feldmanhall"
      ],
      "year": "2022",
      "venue": "Nature communications"
    },
    {
      "citation_id": "42",
      "title": "Decoding the neural signatures of valence and arousal from portable eeg headset",
      "authors": [
        "N Garg",
        "R Garg",
        "A Anand",
        "V Baths"
      ],
      "year": "2022",
      "venue": "Frontiers in Human Neuroscience"
    },
    {
      "citation_id": "43",
      "title": "Scikit-learn: Machine learning in python",
      "authors": [
        "F Pedregosa",
        "G Varoquaux",
        "A Gramfort",
        "V Michel",
        "B Thirion",
        "O Grisel",
        "M Blondel",
        "P Prettenhofer",
        "R Weiss",
        "V Dubourg"
      ],
      "year": "2011",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "44",
      "title": "Eye-tracking analysis for emotion recognition",
      "authors": [
        "P Tarnowski",
        "M Ko Lodziej",
        "A Majkowski",
        "R Rak"
      ],
      "year": "2020",
      "venue": "Computational intelligence and neuroscience"
    }
  ]
}