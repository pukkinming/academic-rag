{
  "paper_id": "2509.22729v1",
  "title": "Multi-Modal Sentiment Analysis With Dynamic Attention Fusion",
  "published": "2025-09-25T09:54:04Z",
  "authors": [
    "Sadia Abdulhalim",
    "Muaz Albaghdadi",
    "Moshiur Farazi"
  ],
  "keywords": [
    "Multimodal Sentiment Analysis",
    "Attention Mechanism",
    "BERT",
    "COVAREP",
    "Fusion Models",
    "Affective Computing"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Traditional sentiment analysis has long been a unimodal task, relying solely on text. This approach overlooks nonverbal cues such as vocal tone and prosody that are essential for capturing true emotional intent. We introduce Dynamic Attention Fusion (DAF), a lightweight framework that combines frozen text embeddings from a pretrained language model with acoustic features from a speech encoder, using an adaptive attention mechanism to weight each modality per utterance. Without any fine-tuning of the underlying encoders, our proposed DAF model consistently outperforms both static fusion and unimodal baselines on a large multimodal benchmark. We report notable gains in F1-score and reductions in prediction error and perform a variety of ablation studies that support our hypothesis that the dynamic weighting strategy is crucial for modeling emotionally complex inputs. By effectively integrating verbal and non-verbal information, our approach offers a more robust foundation for sentiment prediction and carries broader impact for affective computing applications-from emotion recognition and mental health assessment to more natural human-computer interaction.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Sentiment analysis is a multimodal AI task that focuses on identifying and interpreting human emotions, opinions, and attitudes from various types of input modalities of data. The objective is to determine the polarity of a given expression into positive, negative and neutral and with more advanced formulations it can also capture more nuanced emotional states such as joy, anger, or sadness. Sentiment analysis has become indispensable for understanding public opinion, consumer preferences, and social behavior as it extracts subjective information from digital interactions -at scale  [1] . A typical sentiment analysis process involves analyzing patterns, context, and cues that reflect subjective human feelings, which are often complex and influenced by personal, cultural, and situational factors  [2] , making it even more important. This complexity makes sentiment analysis a profound challenge for artificial intelligence. For instance, the same sentence can convey entirely different emotions depending on the context, tone of voice, or accompanying facial expressions. Sarcasm, irony, and culturally specific idioms are just a few of the linguistic nuances that can easily be misinterpreted by an algorithm. Therefore, developing robust sentiment analysis models requires not just sophisticated algorithms, but also a deep, context-aware understanding of human expression, making it a critical frontier in the pursuit of more emotionally intelligent AI  [3] .\n\nHistorically, sentiment analysis has been predominantly focused on analyzing textual data. This early focus centered on classifying opinions from written sources such as product reviews, survey responses, and social media posts to determine sentiment polarity. Initial research and subsequent techniques developed in this domain primarily relied on Natural Language Processing (NLP) and machine learning models designed specifically for text, such as Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks  [4] . These unimodal models were trained exclusively on large corpora of textual data, such as the IMDB and Amazon review datasets. As a result, traditional sentiment analysis methods were inherently limited in their ability to fully capture the depth and variation of human expression  [5] , such as sarcasm, irony, or emotional tone shifts, leading to higher misclassification rates in many real-world settings  [1] . Furthermore, text data omits prosodic information such as pitch, volume, and rhythm, which often signal emotional emphasis. There are some techniques to encode such non-verbal cues by manually appending by special characters into the input text, but such techniques still fall short of improving model performance  [1] .\n\nRecognizing these shortcomings, researchers have increasingly advocated for multimodal sentiment analysis (MSA) to integrate heterogeneous data modalities like text, audio, and visual cues-to achieve a more holistic representation of emotion. Unlike traditional sentiment analysis, which typically relies on textual data alone, MSA aims for a more comprehensive and accurate understanding of sentiment by integrating information from various modalities available in the problem setting, such as facial expressions, speech, and physiological signals. For example, textual features capture lexical sentiment, audio features provide paralinguistic signals (tone, prosody), and visual features supply facial expressions and body language  [6] . Early multimodal approaches employed feature-level fusion, where embeddings from distinct modalities were concatenated and fed into downstream classifiers  [1] . These were enhanced by enabling decision-level fusion, combining modality-specific predictions via weighted voting or majority logic  [7] .\n\nWhile promising, multimodal learning presents its own set of difficulties. Non-text modalities, such as audio and video, lack a consistent, structured representation like language does. Audio signals convey valuable cues, such as pitch, tone, and prosody, but are also influenced by variations in pronunciation, noise, and temporal dynamics. Video data contributes facial expressions and gestures, but is sensitive to alignment and framing. Despite these challenges, audio is particularly suited for complementing textual input, as it offers crucial paralinguistic cues such as emphasis and emotional tone, making it an ideal candidate for fusion with language.  [6] . Moreover, integrating modalities with fundamentally different distributions introduces further complexity. Direct concatenation or static fusion mechanisms can lead to information redundancy or even degradation in performance due to misaligned or noisy signals  [8] .\n\nIn this paper, we address these limitations by proposing a Dynamic Attention Fusion (DAF) mechanism that adaptively assigns weights to each modality based on its relative informativeness for a given input sample. Rather than treating all modalities equally, our proposed DAF model learns to focus more on the modality providing clearer emotional cues-be it speech or text-on a case-by-case basis. To evaluate the effectiveness of our approach, we conduct experiments on the CMU-MOSEI dataset  [9] , leveraging pretrained BERT  [10]  for text and COVAREP  [11]  for audio to extract rich embeddings without fine-tuning. We benchmark DAF against text-only and static fusion baselines, showing that our dynamic strategy consistently improves sentiment prediction metrics. Our findings demonstrate the value of adaptive fusion in capturing emotional nuance and validate our hypothesis that contextsensitive attention yields more robust sentiment classification. The contributions of our paper are as follows:\n\n• We introduce a Dynamic Attention Fusion framework that adaptively weights textual and acoustic embeddings per input, overcoming the limitations of static concatenation and misaligned signals.\n\n• We leverage frozen pretrained encoders (BERT for text and COVAREP for audio) to enable zero-fine-tuning deployment, minimizing training overhead and facilitating rapid adaptation to new domains. • We empirically validate our approach on the CMU-MOSEI benchmark, conducting extensive ablation studies to demonstrate the necessity of dynamic weighting for capturing nuanced emotional cues.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Unimodal Sentiment Analysis. Early sentiment analysis focused exclusively on textual data, employing lexicon-based methods and machine learning models such as RNNs and LSTMs to classify polarity in product reviews and social media posts  [4] . Transformer-based architectures (e.g., BERT) further improved contextual understanding by leveraging pre-trained embeddings, enabling better handling of subtle phenomena like sarcasm and polysemy  [5] ,  [12] . However, these unimodal approaches remain blind to prosodic and visual cues-such as tone, pitch, and facial expression-that are essential for disambiguating emotional intent in real-world app reviews. Our work addresses this gap by integrating non-textual modalities directly into the sentiment analysis pipeline.\n\nPromise of Multimodal Sentiment Analysis. Multimodal sentiment analysis (MSA) seeks to enrich polarity detection by fusing textual, acoustic, and visual signals  [6] . Early fusion schemes concatenated modality-specific features before classification, while late fusion combined independent predictions via voting or weighted averaging  [1] ,  [7] . These methods demonstrated that audio and video data can complement text, capturing prosody and facial expressions that reveal user frustration or satisfaction beyond words alone. Nevertheless, static fusion strategies often suffer from modality imbalance and information redundancy, motivating dynamic fusion mechanisms in our proposed framework.\n\nKey Multimodal Techniques and Their Gaps. Recent MSA architectures include deep CNNs for audio-visual analysis, multitask learning frameworks aligning sentiment prediction with modality coherence, and cross-modal transformers (e.g., MulT) that learn inter-modal attention without explicit alignment  [8] ,  [13] . Gated fusion models such as CMAGF dynamically weight modalities based on learned importance, while region-text alignment networks like ITIN improve image-text correspondence  [13] . Despite these advances, existing models still struggle with noisy or missing modalities, lack fine-grained dynamic weighting, and offer limited interpretability of fusion decisions. Our Dynamic Attention Fusion (DAF) mechanism specifically tackles these shortcomings by adaptively weighting each modality per instance and providing transparent attention scores.\n\nOngoing MSA Research and Open Challenges. The MSA field continues to evolve, with surveys highlighting predominant late-fusion practices, emerging interest in aspectlevel multimodal sentiment (MABSA), and the need for robust evaluation on benchmarks like CMU-MOSI and CMU-MOSEI  [8] ,  [14] ,  [15] . Key challenges include handling asynchronous modality streams, mitigating text-driven biases, and improving cross-modal generalization under domain shifts. Moreover, the computational overhead of large LLMs raises practical deployment concerns. Our work contributes to this landscape by demonstrating scalable, context-aware fusion with minimal fine-tuning and by evaluating on diverse real-world app review datasets to validate generalizability and efficiency.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Methodology",
      "text": "We proposes a dynamic multi-modal sentiment analysis framework that integrates linguistic, acoustic, and visual cues to capture a more nuanced understanding of spoken language. The methodology is structured around four key components: (1) modality-specific feature extraction, (2) cross-modal attention-dynamic fusion, and (3) sentiment prediction.\n\nGiven that text often conveys the dominant sentiment cues, a text-driven cross-modal attention mechanism is applied to guide focus over the time-aligned audio and video representations. Formally, the transformed text embedding t ′ is computed by projecting the original text vector t via a learned weight matrix W t . The audio and video sequences, encoded as a ′ and v ′ respectively, are attended to by computing similarity scores between t ′ and the respective modality features. Attention weights are calculated by\n\nand the attended context vectors are obtained by\n\nThis mechanism allows the textual information to modulate the contribution of audio and video features dynamically.\n\nRather than employing static fusion such as simple concatenation, the proposed Dynamic Attention Fusion (DAF) module adaptively weighs the importance of each modality on a per-sample basis. The gating network, implemented as a small multilayer perceptron (MLP), takes the concatenated intermediate vectors [t ′ ∥ ã ∥ ṽ] and outputs scalar weights\n\nwhich are then normalized via softmax to produce attention weights\n\nThe final fused representation z is computed as the weighted sum\n\nFor baseline comparison, a static fusion method is also evaluated in which the modality features are concatenated directly without adaptive weighting:\n\nThe fused vector z is passed through a nonlinear transformation followed by a fully connected output layer to yield the sentiment prediction. Specifically, a ReLU activation is applied to a hidden layer,\n\nfollowed by a linear output layer,\n\nSince the sentiment labels are continuous in the range [-3, 3], a tanh activation constrains the output accordingly.\n\nThe model's performance is evaluated from both regression and classification perspectives. Regression metrics include Mean Absolute Error (MAE), which measures the average absolute difference between predicted and true sentiment values, and the Pearson Correlation Coefficient (CC), which assesses linear correlation. For classification evaluation, sentiment predictions are discretized into seven classes reflecting the original Likert scale, enabling 7-class sentiment classification accuracy. In addition, binary classification (positive vs. negative) is performed by excluding neutral samples, with evaluation metrics such as Accuracy, F1-Score (including F1 without neutral), and the Receiver Operating Characteristic Area Under Curve (ROC-AUC) to assess discrimination capability.\n\nExperiments are conducted across different modality combinations to isolate the contribution of each source. These include text-only (BERT embeddings), text with audio (BERT + COVAREP), text with video (BERT + FACET), and the full multimodal model with dynamic attention fusion integrating all three modalities. Comparisons are also made between static early fusion baselines and the proposed dynamic fusion approach to demonstrate the benefits of adaptive modality weighting.\n\nThough this methodology is developed primarily for multimodal sentiment regression, it generalizes readily to related affective computing tasks such as emotion recognition, mental health monitoring, and broader human-centered AI applications. Using pre-trained frozen encoders and a flexible dynamic attention fusion mechanism, the approach balances robustness, interpretability, and adaptability.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iv. Experiments And Results",
      "text": "We utilize the CMU-MOSEI dataset  [9] , a richly annotated benchmark comprising over 23, 000 sentence-level opinion segments drawn from more than 1,000 YouTube videos. Each segment is aligned with textual transcripts, audio recordings, and video frames. Sentiment annotations are provided on a 7-point Likert scale from -3 (strongly negative) to +3 (strongly positive), enabling fine-grained sentiment modeling. The dataset's scale, diversity of speakers, and modality richness make it well-suited for multimodal sentiment learning.\n\nEach modality is independently encoded into fixeddimensional latent vectors using pretrained models. The textual modality is represented by sentence-level BERT  [10]  embeddings, characterized by a 768-dimensional vector. For audio, we utilized COVAREP  [11]  acoustic features, provided at a frame-level granularity with 74 dimensions. The video modality consists of FACET  [16]  facial expression features, also captured at a frame-level and comprising 35 dimensions. The dataset was systematically partitioned into standard splits for training, validation, and testing. All three modalities were pre-extracted and stored as .pkl files, ensuring accurate alignment through the original timestamps provided by the CMU-MultimodalSDK  [9] . To maintain consistency and ensure complete input for our models, any sentences found to be missing data from any of the three modalities were systematically discarded.\n\nTo prepare the raw data for model input, several crucial preprocessing steps were applied. Optional L2 normalization was performed on both audio and video features to standardize their respective scales, a common practice to mitigate issues arising from differing feature ranges. Furthermore, all instances of NaN (Not a Number) or inf (infinity) values identified within the audio and video sequences were diligently replaced with zeros to prevent computational errors and ensure data integrity. For attention-based models, sequences underwent appropriate padding to achieve uniform length, with corresponding masks applied to prevent the attention mechanism from attending to these introduced padding tokens. The text embeddings were specifically extracted from the BERT  [10]  [CLS] tokens and subsequently organized into sequential training, validation, and test sets.\n\nWe evaluated the fusion strategy implemented within our framework to assess its efficacy in multimodal sentiment analysis. These foundational architectures included the Attention-Based Model employed text-guided attention mechanisms to derive contextual representations of both audio and video features, leveraging separate Luong-style attention mechanisms for each.\n\nA consistent set of hyperparameters was maintained across the experiments. The Learning Rate was set to 0.00005, and a Batch Size of 32 was used for training. Models were trained for up to 200 Epochs, with Early Stopping implemented using a patience of 10 to prevent overfitting. The Adam optimizer was employed for training, and Mean Squared Error (MSE) served as the Loss Function for the regression task. To stabilize training, Gradient Clipping was applied with a maximum norm of 4.0. Bidirectional encoders were utilized, indicated by Bidirectional Encoders set to True, and an Input Dropout rate of 0.2 was applied. The Hidden Size for the fusion layer was 32, and the Attention Projection Size was also set to 32. All model training was significantly accelerated by leveraging available GPU resources.\n\nTo further augment the performance and robustness of our base attention model, we introduced a novel Dynamic Attention Fusion strategy. This mechanism was specifically engineered to adaptively weigh the contribution of each modality-audio and video-based on its perceived informativeness for a given input instance.\n\nWhile conventional fusion strategies, such as early and late fusion, typically assign equal importance to all modalities  [14] , the reality of multimodal data often reveals significant variability in modality relevance and signal-to-noise ratio on a per-instance basis. For example, within an opinionated video segment, the textual content might overwhelmingly dominate the sentiment expression, whereas in another, salient facial expressions within the video stream could convey more critical information. The dynamic attention mechanism directly addresses this limitation by learning per-instance modality weights, thereby enabling the model to dynamically prioritize and focus on the most informative modalities.\n\nWithin our modified architecture, the dynamic attention mechanism operates through a precise sequence of steps. Initially, standard Luong-style attention mechanisms are employed to compute text-guided contextual representations for both the audio and video modalities. This process can be formally expressed as:\n\nwhere T denotes the text embeddings, A represents the audio features, and V corresponds to the video features. Following this, a sophisticated learnable gating mechanism processes these attended audio (h audio ) and video (h video ) representations to derive dynamic modality weights, w a and w v . This reweighting can be formalized as:\n\nwhere σ signifies the sigmoid activation function, W g is a learnable weight matrix, and b g is a learnable bias vector; the concatenation [h audio ; h video ] combines the two attended representations. Finally, these dynamically learned weights (w a , w v ) are utilized to combine the attended audio and video vectors, resulting in a fused representation:\n\nThis h fused vector is then passed through a projection layer to generate the ultimate sentiment prediction. This intricate mechanism empowers the model to adapt its focus dynamically, based on the relative clarity or inherent noise present in each modality for any given input.\n\nTo evaluate the effectiveness of our proposed Dynamic Attention Fusion (DAF) model, we conducted comprehensive experiments on the CMU-MOSEI dataset  [9]  under multiple modality configurations. The performance was assessed using key metrics including Accuracy, F1-Score, Mean Absolute Error (MAE), and Correlation Coefficient (CC), across both the full 7-class and binary (positive/negative) sentiment classification settings. Below, we detail our findings for each configuration and analyze the implications of modality contributions. We summarize the results in Tab. I Text-Only: The text-only model using BERT embeddings achieved 82.2% binary accuracy and an F1-score of 0.870, with a mean absolute error (MAE) of 0.541 and 7-class accuracy of 53.30% (Table  I ). This strong unimodal performance underscores the richness of contextual language features for sentiment classification, capturing nuances such as sarcasm and emphasis solely from text. Such high unimodal accuracy is expected, as textual data provides the most structured and direct representation of sentiment cues, setting a high bar on CMU-MOSEI. Text + Audio: Incorporating COVAREP audio embeddings produced no change in overall accuracy (82.2%), a slight dip in F1 to 0.869, a marginal MAE increase to 0.543, and a small drop in 7-class accuracy to 53.24%. These results suggest that paralinguistic cues-tone, pitch, prosody-offer complementary information primarily in ambiguous cases, but may introduce noise when the text signal is already strong. The Dynamic Attention Fusion module therefore must learn to attend to audio only when it clarifies uncertain textual sentiment. Text + Video: Adding FACET video features yielded a modest accuracy rise to 82.3% and maintained F1 at 0.870, while MAE remained at 0.543 and 7-class accuracy increased slightly to 53.43%. This demonstrates that facial expressions and visual gestures can enhance sentiment detection, especially for emotionally expressive utterances, although their impact is tempered by potential misalignment and feature variability. Text + Audio + Video: The full tri-modal model attained the highest accuracy (82.7%) and F1-score (0.874), reduced MAE to 0.539, and achieved a 7-class accuracy of 53.41%. These gains-+0.5% accuracy and +0.4% F1 over textonly-confirm that integrating semantic, vocal, and facial information yields cumulative benefits. The Dynamic Attention Fusion module effectively weights each modality per sample, leading to more nuanced, context-aware sentiment predictions.",
      "page_start": 3,
      "page_end": 5
    },
    {
      "section_name": "A. Comparison To Static Fusion Methods",
      "text": "We next compare our Dynamic Attention Fusion (DAF) against static fusion baselines, including early concatenation of modalities and fixed weighted fusion. As shown in Figure  2 , DAF yields consistently lower MAE, higher correlation coefficient (CC), and improved accuracy relative to static fusion-even when the absolute gains are modest. In particular, early concatenation treats all modalities equally, which can Several broader trends emerge from our experiments. First, fine-grained 7-class sentiment classification remains challenging across all modality configurations, with only marginal gains from fusion. This likely reflects the inherent difficulty of distinguishing subtle sentiment shifts and the subjectivity of human annotations. Second, despite relying solely on pretrained encoders without task-specific fine-tuning, our architecture design alone yields measurable improvements-underscoring the critical role of fusion strategy over model size or additional training data. Third, while the textonly BERT baseline performs remarkably well (indicating the strength of contextual embeddings), the addition of audio and video modalities via DAF proves most beneficial in emotionally rich or sarcastic utterances where nonverbal cues disambiguate sentiment.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Binary Classification And Roc Analysis",
      "text": "To further evaluate discrimination between positive and negative sentiment, we computed the Receiver Operating Characteristic (ROC) curve for each fusion strategy (Figure  3 ). The area under the curve (AUC) for DAF exceeds that of static fusion by a noticeable margin, demonstrating superior trade-offs between true and false positive rates. This analysis confirms that adaptive fusion not only improves regression metrics but also enhances binary decision reliability, which is critical for downstream tasks such as sentiment-driven content moderation and real-time feedback systems.\n\nOverall, these results validate Dynamic Attention Fusion as a flexible, interpretable, and performance-effective framework for multimodal sentiment analysis, paving the way for extensions in emotion recognition, human-computer interaction, and affective computing. Lightweight dynamic fusion mechanisms, such as DAF, are promising for real-time applications, including empathetic dialogue systems, mental health monitoring, and social media sentiment analysis. Methodologically, DAF is encoder-agnostic and can operate with features from any pretrained modality encoder, while conceptually being architecture-agnostic and integrable into larger multimodal transformers or additional modalities such as vision or physiological signals.\n\nHowever, several limitations remain. For instance, performance gains over strong text baselines are modest, and the video modality contributes little, which we attribute to the coarse nature of the FACET features. Future work will therefore integrate stronger encoders, such as pre-trained video transformers or CLIP-based representations. As our evaluation is currently limited to the CMU-MOSEI dataset, further testing on datasets such as CMU-MOSI and MELD is necessary to confirm generalizability. Finally, while encoders were frozen to maintain efficiency, DAF readily supports fine-tuning, and we will explore robustness analyses with modality dropout or adversarial noise injection to clarify its resilience in noisy or sarcastic contexts.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "V. Conclusion",
      "text": "This paper has presented Dynamic Attention Fusion (DAF), a novel framework for multimodal sentiment analysis that adaptively weights textual, acoustic, and visual modalities based on their contextual informativeness. By leveraging pretrained BERT and COVAREP embeddings without taskspecific fine-tuning, DAF consistently outperforms static fusion strategies and unimodal baselines on the CMU-MOSEI benchmark, achieving an F1-score of 87.38% and an MAE of 0.539. Our analysis demonstrates that while text alone captures much of the sentiment signal, the inclusion of audio and video cues via dynamic fusion yields meaningful gains, particularly in emotionally nuanced or ambiguous utterances. Moreover, DAF's per-instance attention weights offer interpretable insights into which modalities drive each prediction.\n\nLooking ahead, several avenues exist to enhance this work. Fine-tuning the underlying encoders on sentiment-specific corpora may further refine feature representations, while more sophisticated temporal modeling of visual data -such as transformer-based video encoders could improve alignment and leverage facial and gesture dynamics. Finally, evaluating DAF on additional multimodal datasets (e.g., MELD, CMU-MOSI, IEMOCAP, SEMAINE) will test its generalizability and robustness in real-world affective computing applications. Overall, Dynamic Attention Fusion represents a promising step toward more intelligent, emotionally aware AI systems that fully exploit the richness of human communication across modalities.",
      "page_start": 6,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of the proposed dynamic multimodal sentiment analysis framework. (1) Modality-specific encoders extract contextual representations from",
      "page": 3
    },
    {
      "caption": "Figure 2: Performance comparison between static and dynamic attention",
      "page": 5
    },
    {
      "caption": "Figure 3: ROC Curve for the dynamic attention fusion model, achieving an",
      "page": 5
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "Evolving techniques in sentiment analysis: a comprehensive review",
      "authors": [
        "M Kumar",
        "L Khan",
        "H.-T Chang"
      ],
      "year": "2025",
      "venue": "PeerJ Computer Science"
    },
    {
      "citation_id": "2",
      "title": "Exploring multimodal sentiment analysis models: A comprehensive survey",
      "authors": [
        "P Dao",
        "T Nguyen-Tat",
        "M Roantree",
        "V Ngo"
      ],
      "year": "2024",
      "venue": "2024 International Conference on Multimedia Analysis and Pattern Recognition (MAPR)"
    },
    {
      "citation_id": "3",
      "title": "New avenues in opinion mining and sentiment analysis",
      "authors": [
        "E Cambria",
        "B Schuller",
        "Y Xia",
        "C Havasi"
      ],
      "year": "2013",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "4",
      "title": "Deep learning-based sentiment classification: A comparative survey",
      "authors": [
        "A Mabrouk",
        "R Redondo",
        "M Kayed"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "5",
      "title": "Text based sentiment analysis using lstm",
      "authors": [
        "G Murthy",
        "S Allu",
        "B Andhavarapu",
        "M Bagadi",
        "M Belusonti"
      ],
      "year": "2020",
      "venue": "Int. J. Eng. Res. Tech. Res"
    },
    {
      "citation_id": "6",
      "title": "Complementary fusion of multifeatures and multi-modalities in sentiment analysis",
      "authors": [
        "F Chen",
        "Z Luo",
        "Y Xu",
        "D Ke"
      ],
      "year": "2019",
      "venue": "Complementary fusion of multifeatures and multi-modalities in sentiment analysis",
      "arxiv": "arXiv:1904.08138"
    },
    {
      "citation_id": "7",
      "title": "Dyncim: Dynamic curriculum for imbalanced multimodal learning",
      "authors": [
        "C Qian",
        "K Han",
        "J Wang",
        "Z Yuan",
        "C Lyu",
        "J Chen",
        "Z Liu"
      ],
      "year": "2025",
      "venue": "Dyncim: Dynamic curriculum for imbalanced multimodal learning"
    },
    {
      "citation_id": "8",
      "title": "A comprehensive survey on multimodal sentiment analysis: Techniques, models, and applications",
      "authors": [
        "H Zhang"
      ],
      "year": "2024",
      "venue": "Advances in Engineering Innovation"
    },
    {
      "citation_id": "9",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations"
    },
    {
      "citation_id": "10",
      "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2018",
      "venue": "CoRR"
    },
    {
      "citation_id": "11",
      "title": "Covarep-a collaborative voice analysis repository for speech technologies",
      "authors": [
        "G Degottex",
        "J Kane",
        "T Drugman",
        "T Raitio",
        "S Scherer"
      ],
      "year": "2014",
      "venue": "2014 ieee international conference on acoustics, speech and signal processing (icassp)"
    },
    {
      "citation_id": "12",
      "title": "Hybrid deep neural network with domain knowledge for text sentiment analysis",
      "authors": [
        "J Khan",
        "N Ahmad",
        "Y Lee",
        "S Khalid",
        "D Hussain"
      ],
      "year": "2025",
      "venue": "Mathematics"
    },
    {
      "citation_id": "13",
      "title": "Multimodal sentiment analysis: A survey",
      "authors": [
        "L Songning",
        "X Hu",
        "H Xu",
        "Z Ren",
        "Z Liu"
      ],
      "venue": "Displays"
    },
    {
      "citation_id": "14",
      "title": "Exploring multimodal sentiment analysis models: A comprehensive survey",
      "authors": [
        "P Dao",
        "T Nguyen-Tat",
        "M Roantree",
        "V Ngo"
      ],
      "year": "2024",
      "venue": "2024 International Conference on Multimedia Analysis and Pattern Recognition (MAPR)"
    },
    {
      "citation_id": "15",
      "title": "A survey on multimodal aspectbased sentiment analysis",
      "authors": [
        "H Zhao",
        "M Yang",
        "X Bai",
        "H Liu"
      ],
      "year": "2024",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "16",
      "title": "Facet: Fairness in computer vision evaluation benchmark",
      "authors": [
        "L Gustafson",
        "C Rolland",
        "N Ravi",
        "Q Duval",
        "A Adcock",
        "C.-Y Fu",
        "M Hall",
        "C Ross"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    }
  ]
}