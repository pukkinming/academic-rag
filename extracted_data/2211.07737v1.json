{
  "paper_id": "2211.07737v1",
  "title": "Describing Emotions With Acoustic Property Prompts For Speech Emotion Recognition",
  "published": "2022-11-14T20:29:37Z",
  "authors": [
    "Hira Dhamyal",
    "Benjamin Elizalde",
    "Soham Deshmukh",
    "Huaming Wang",
    "Bhiksha Raj",
    "Rita Singh"
  ],
  "keywords": [
    "emotion recognition",
    "contrastive languageaudio pretraining",
    "acoustic properties",
    "prompt generation",
    "prompt augmentation"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotions lie on a broad continuum and treating emotions as a discrete number of classes limits the ability of a model to capture the nuances in the continuum. The challenge is how to describe the nuances of emotions and how to enable a model to learn the descriptions. In this work, we devise a method to automatically create a description (or prompt) for a given audio by computing acoustic properties, such as pitch, loudness, speech rate, and articulation rate. We pair a prompt with its corresponding audio using 5 different emotion datasets. We trained a neural network model using these audio-text pairs. Then, we evaluate the model using one more dataset. We investigate how the model can learn to associate the audio with the descriptions, resulting in performance improvement of Speech Emotion Recognition and Speech Audio Retrieval. We expect our findings to motivate research describing the broad continuum of emotion.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition is the task of detecting emotion from a given audio. Emotion detection is playing an increasingly important role in the digital world today however there is still need for improvement. Humans express emotions on a very broad continuum. Models like the plutchik wheel of emotion  [1]  or the Ekman's model of emotion  [2]  capture all emotions as a combination of 6 or 8 basic ones. Although these frameworks are extremely popular and provide ease of modelling, they limit the ability of machine learning models to capture the nuances in the spectrum of human emotions.\n\nThe continuum of emotions instead of being categorized by handful of predefined classes, can be thought of in terms of some high dimensional continuous space, where any emotion can lie. This is important to model emotions since each expression of emotion is diverse and unique. It is dependent on speaker, culture, context among other factors. Labelling two instances of emotion with the same label of 'anger', ignores the intricacy of the expression. Therefore, we explore modelling the continuity of emotions.\n\nThis work was done when the first author was an intern at Microsoft This continuity of emotions can be captured by the flexibility that natural language descriptions provide. These descriptions can use affective language, that are often casually used to describe an emotion. Such affective language has acoustic correlates, for example: an angry man 'shouting loudly' is describing the emotion by directly referring to the loudness or intensity of the speech.\n\nThe choice of natural language description effects the high dimensional representation learned from the text, hence it is very important to choose the right description for the emotion. This leads to the question:\n\nHow do we describe an emotion using natural language and how can a model learn it?\n\nIn this work, we propose a method to describe the continuum of emotion by using the audio themselves to guide the descriptions. Previous research shows that there are numerous acoustic correlates of emotion  [3, 4, 5] . These acoustic correlates refer to the low level information like the average pitch, intensity, speech rate and articulation rate. We extract these correlates from each audio and use them to form the description in an automatic and scalable way. We call descriptions generated in this manner 'acoustic prompts'.\n\nThus we need a model that learns to associate the audio and their descriptions. To do this, we build on top of the Contrastive Language-Audio Pretraining (CLAP) model  [6] . The model has separate audio and text encoders. It brings the audio and text representations to the same multimodal space. This architecture yields state of the art performance in learning audio concepts with natural language description. CLAP enables to evaluate the learned model on zero-shot and supervised classification and audio retrieval  [7] .\n\nFrom our experiments, we find that acoustic prompts improve the model's performance in emotion classification settings. Specifically, when the training dataset are relatively smaller, classification performance improves 3.8% in Ravdess (Sec. 5.1). In a finetuning setup, we observe 3.7% improvement (Sec. 5.2.2) in Ravdess. The model also learns associations between the audio and their acoustic properties, which is observed in audio retrieval experiment (Sec. 5.3). Precision@K improves significantly when the model is trained using the acoustic prompts.\n\nwhere Xa ∈ R N ×V are the audio representations of dimensionality V , and Xt ∈ R N ×U are the text representations of dimensionality U . We brought audio and text representations into a joint multimodal space of dimension d by using a projection layer:\n\nwhere E a ∈ R N ×d , E t ∈ R N ×d , L a and L t are the linear projections for audio and text respectively. Now that the audio and text embeddings (E a , E t ) are comparable, we can measure similarity:\n\nwhere τ is a temperature parameter to scale the range of logits. The similarity matrix C ∈ R N ×N has N correct pairs in the diagonal and N 2 -N incorrect pairs in the off-diagonal.\n\nThe loss can be calculated as:\n\nwhere k = 1 N N i=0 log diag(sof tmax(C)) along text and audio axis respectively. We used this symmetric crossentropy loss (L) over the similarity matrix to jointly train the audio and text encoders along with their linear projections.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Datasets And Clap Architecture",
      "text": "We use 6 Emotion Datasets (ED) for training and testing, see Table  1 . The literature using these many datasets is rare. The original CLAP model is trained with audio-text pairs sourced from three audio captioning datasets: ClothoV2  [8] , AudioCaps  [9] , MACS  [10] , and one sound event dataset: FSD50K  [11] . Altogether are referred as 4D henceforth.\n\nThe architecture is based on the CLAP model in  [6] . We chose this architecture because it yields SoTA performance in learning audio concepts with natural language description. We use log Mel spectrograms from the audios, sampled at 44K Hz, as input to the audio encoder -CNN14  [17] , which is pretrained on 2M audio clips from AudioSet. The text encoder is BERT uncased. The audio encodings are of 1024 dimensional whereas text encodings are 768 dimensional. Both encodings are then projected into a joint multimodal space of dimension 1024. Both audio and text encoders are frozen in our experiments, but the projection layers are learnable. We use PyTorch to implement the model architecture. The model is trained with 0.0001 learning rate, batch size of 128, for 30 epochs using Adam optimizer.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Prompt Generation",
      "text": "Emotion datasets do not have associated descriptions for each audio. Therefore, we devise a scalable and automatic prompting method that is based on the acoustic properties of the speech audios. There are numerous acoustic properties that describe emotions, as discussed in Section 1. Hence, we hypothesize that including that information in the prompts would benefit emotion recognition. We calculate the pitch and intensity using Librosa  [18]  and we calculate speech rate and articulation rate using Praat  [19] . We construct the prompts in the manner described below:",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Class Label (Prompt)",
      "text": "The simplest description for each audio can be the class label, i.e. 'anger'. Thus, we use this as the baseline prompt to compare against the proposed prompts.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Pitch Prompt",
      "text": "Pitch is known to be affected by emotion, lower pitch is related with negative emotions like fear and high pitch is related with positive emotions like happiness or surprise  [4] . Since pitch is naturally sex specific, we bin the average pitch into both based on sex and without sex. Including sex information, we bin the average pitch into four classes, low-male pitch (< 132.5 Hz), high-male pitch (> 132.5 Hz, < 180 Hz), low-female pitch (> 180 Hz, < 210 Hz) and high-female pitch (> 210 Hz). For the sex agnostic case, we bin into two categories based on cutoff of 170 Hz. The cutoffs are obtained from the average numbers for vocal pitch reported in literature. The prompt is set as 'bin-class emotion-class', an example of which is 'low pitch anger' (without sex information) or 'low male pitch anger' (otherwise).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Intensity Prompt",
      "text": "Intensity is known to be affected by emotion, low intensity is linked with negative emotion like sadness or melancholy and high intensity is linked with joy or excitement  [4] . We bin the average intensity over the audio clip in two bins, low and high intensity at 60 dB. The same rule as pitch prompt is followed to form the intensity prompt, an example of which is 'high intensity anger'.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Speech-Rate Prompt",
      "text": "It has been observed that faster spoken speech is linked with highly potent emotions such as anger and happiness whilst slower speech is linked with sadness, disgust and boredom  [3] . Speech rate is calculated by extracting the number of syllables spoken divided by the total duration of the audio clip. We use 3.12 syllables/sec as the cutoff to bin the speech rate into two bins, low and high speech rate. An example of speech-rate prompt is 'high speech rate anger'.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Articulation-Rate Prompt",
      "text": "Similarly to speech rate, fast articulation rate is linked with emotions of interest, fear or happiness; whereas slow articulation rate is indicative of sadness and disgust  [3] . Articulation rate is calculated as total number of syllables divided by the total phonation time. We bin the audio into low and high articulation rate at the cutoff of 4 syllables/sec. An example of articulation-rate prompt is 'high articulation rate anger'. Even though speech and articulation rate are similar concepts, speech rate captures speaker specific information in the form of number of pauses and hesitation whereas articulation rate would ignore such information.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Prompt Augmentation",
      "text": "To combine all 5 prompts, we pair an audio clip independently with each acoustic prompt. Thus, one audio clip will result in 5 pairs used for training our model.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments And Results",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Prompt Analysis",
      "text": "To evaluate which of the proposed acoustic prompts are better and to access if any of them are better than the class label, we apply the trained model on emotion classification. The model is trained 6 different times, where each time the description associated with emotion audios are varied. Among the 6, 1 is using the class label alone, 4 are using the acoustic prompts as described in Section 4, and 1 is using the prompt augmentation -which combines all the acoustic prompts. We train the model on 4 audio captioning datasets and 1 emotion dataset. The left pat of Figure  2  shows the performance achieved when the model is trained on the training set (including 4D and Ravdess) and tested on the testing set of Ravdess. When done similarly for crema-d, the performance achieved is shown on the right side of Figure  2 . We observe that among the 4 acoustic prompts, pitch prompt gives the best performance. Next best performance is achieved by the intensity prompt. This can be observed in both Ravdess and Crema-d. On Crema-d, articulation rate prompt performs better than speech rate prompt but the reverse is observed in Ravdess. Secondly, we observe that overall prompt augmentation is giving the best performance in both datasets. This validates our original hypothesis that using acoustic prompts would help the emotion classification.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion Classification",
      "text": "To evaluate how the acoustic prompts would help in emotion classification, we perform the following two experiments. The first is a zero-shot like setup where we leave one dataset out, which is used during the testing stage. The second is a finetuning setup where the model from the first setup is fine tuned on the left out dataset.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Leave One Out",
      "text": "This setup evaluates how well a model trained on a predefined set of classes generalizes to a new dataset, which might have same or different sets of classes. Out of the 6 emotion datasets, we leave one out for testing and train the model on the other 5 emotion datasets. Therefore the training and testing datasets are completely different. In case where Ravdess is the testing dataset, 'calm' class is not represented in any of the other training datasets and is a zero shot classification result. In case of Crema-d, all the emotion classes are represented in the training classes from other datasets.\n\nWe perform 4 experiments, shown in columns 2,3 of Table 2. There are two main takeaways from this experiment. Firstly adding Emotion datasets in the training stage is helping the performance on the left out emotion dataset. This can be observed in both Ravdess and Crema-d. For crema-d the performance improves from 17.85% to 35.22% just by changing from 4D to 5ED in the training sets. In ravdess, the performance improves from 15.99% to 22.88%. Secondly, prompt augmentation's results are similar to the results obtained when trained using only the class label. We believe this is happening because of the distribution shift in the training and testing datasets. This also effects distributions of the acoustics -which directly effects the acoustic prompts in the training and testing datasets. For example, 'high intensity anger' might not be occurring in the training datasets, but is present in the testing dataset. This is harming the benefit learnt from the acoustic prompts to be transferable to a completely new dataset at testing time. However we do observe improvement using acoustic prompts when we perform the finetuning experiment in the next section. Note that the SoTA performance for this evaluation setup is not found in literature because the general evaluation setup is when the dataset is present in both training and testing sets.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Finetune",
      "text": "In this experiment, we want to analyse how using the acoustic prompts at pre-training stage would benefit the classification. We take the model from the previous stage and perform fine tuning on the dataset that had been left out.\n\nThe results for supervised classification are shown in columns 4,5 of Table  2 . The main takeaway from this experiment is that with finetuning, prompt augmentation shows improvement over the other models. In Ravdess, we see improvement in performance by absolute 3.77%, from 68.69% to 72.46%. In Crema-d the performance is about the same as when the prompt augmentation is not used -72.86% vs 72.56%, however not significantly different.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Emotion Audio Retrieval",
      "text": "With the increasing sizes of the audio databases, to be able to search such databases for specific types of audios is important. Therefore we evaluate our models specifically for the audio retrieval task. This would access whether the trained model learns the associations between the acoustic prompts and the respective acoustic properties. We make queries similarly to the prompts as described in Section 4. For a given query, the model outputs top K audios whose audio embeddings have highest cosine similarity to the text embedding of the query. For the top K audios, since we know the true acoustic prompts, we can evaluate how good the model's outputs is w.r.t the query.\n\nFigure  3  shows the results of audio retrieval. We calculate precision@K for each acoustic prompt shown on the xaxis. From the results we observe that the model trained on all datasets, and using prompt augmentation performs the best in all cases (all types of queries). The takeaway here is that our model is able to retrieve audio significantly better when trained using prompt augmentation. The precision@K numbers are comparable to numbers observed in audio retrieval tasks  [22] . The results are encouraging since this suggests that we can introduce even more elaborate descriptions for each audio and the model will learn associations and be able to retrieve audios with those descriptions. Fig.  3 : Precision@K achieved when the trained model under different settings is used for the audio retrieval task.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "This work performs emotion recognition using the audios and their natural language descriptions. We use the acoustics of emotions to prompt the audios, in fact there can be more complicated descriptions, invoking the semantics, environment, context among other factors. We envision that as methods of describing emotions become more complicated, our ability to model more nuanced emotions will become better. The acoustic properties we extract include pitch, intensity, speech rate and articulation rate from the audios. We find that among the acoustic prompts, pitch prompt is the best performing. Overall, when we do prompt augmentation it achieves the highest accuracy and improves the performance in Ravdess by 3.8%. We also perform emotion audio retrieval and find that when using model trained on prompt augmentation, we get the significantly better retrieval performance.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The left part of the image shows the model training. Given a batch",
      "page": 2
    },
    {
      "caption": "Figure 1: (left) shows the Contrastive Language-Audio Pretrain-",
      "page": 2
    },
    {
      "caption": "Figure 2: Accuracy achieved using different prompts on Crema-d (left)",
      "page": 3
    },
    {
      "caption": "Figure 2: shows the per-",
      "page": 3
    },
    {
      "caption": "Figure 3: shows the results of audio retrieval. We calcu-",
      "page": 4
    },
    {
      "caption": "Figure 3: Precision@K achieved when the trained model under different set-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": "ABSTRACT"
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": ""
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": "Emotions lie on a broad continuum and treating emotions"
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": ""
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": "as a discrete number of classes limits the ability of a model to"
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": ""
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": "capture the nuances in the continuum. The challenge is how to"
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": ""
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": "describe the nuances of emotions and how to enable a model"
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": ""
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": "to learn the descriptions. In this work, we devise a method to"
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": ""
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": "automatically create a description (or prompt) for a given au-"
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": "dio by computing acoustic properties, such as pitch, loudness,"
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": ""
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": "speech rate, and articulation rate. We pair a prompt with its"
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": ""
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": "corresponding audio using 5 different emotion datasets. We"
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": ""
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": "trained a neural network model using these audio-text pairs."
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": ""
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": "Then, we evaluate the model using one more dataset. We"
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": "investigate how the model can learn to associate the audio"
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": "with the descriptions, resulting in performance improvement"
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": "of Speech Emotion Recognition and Speech Audio Retrieval."
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": ""
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": "We expect our ﬁndings to motivate research describing the"
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": ""
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": "broad continuum of emotion."
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": ""
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": "Index Terms— emotion recognition, contrastive language-"
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": "audio\npretraining,\nacoustic\nproperties,\nprompt\ngeneration,"
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": "prompt augmentation"
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": ""
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": ""
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": "1.\nINTRODUCTION"
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": ""
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": "Speech emotion recognition is the task of detecting emotion"
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": "from a given audio. Emotion detection is playing an increas-"
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": "ingly important role in the digital world today however there"
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": "is still need for improvement. Humans express emotions on"
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": "a very broad continuum. Models like the plutchik wheel of"
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": "emotion [1] or the Ekman’s model of emotion [2] capture all"
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": "emotions as a combination of 6 or 8 basic ones. Although"
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": "these frameworks are extremely popular and provide ease of"
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": "modelling,\nthey limit\nthe ability of machine learning models"
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": "to capture the nuances in the spectrum of human emotions."
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": "The continuum of emotions instead of being categorized"
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": ""
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": "by handful of predeﬁned classes, can be thought of in terms"
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": ""
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": "of some high dimensional continuous space, where any emo-"
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": ""
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": "tion can lie. This is important\nto model emotions since each"
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": ""
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": "expression of emotion is diverse and unique.\nIt\nis dependent"
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": ""
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": "on speaker, culture, context among other\nfactors. Labelling"
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": ""
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": "two instances of emotion with the same label of ‘anger’,\nig-"
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": ""
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": "nores the intricacy of the expression. Therefore, we explore"
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": ""
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": "modelling the continuity of emotions."
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": ""
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": "This work was done when the ﬁrst author was an intern at Microsoft"
        },
        {
          "{hyd, bhiksha, rsingh}@cs.cmu.edu, {benjaminm, sdeshmukh, huawang}@microsoft.com": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "Dataset\nFiles\nClass\nEmotions"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "CMU-\nang, exc, fear, sad, frus, neu"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "23K\n9"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "MOSEI[12]\nsurprise, hap, disappoint"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "hap, fear, sad, surprise, exc,"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "IEMOCAP[13]\n10K\n9"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "ang, neu, disappoint, frus"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "neu, surprise, fear, sad,"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "MELD[14]\n10K\n7"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "joy, disgust, ang"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "CREMA-\nang, disappoint, fear, hap,"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "7K\n6"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "D[15]\nneu, sad"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": ""
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "neu, calm, hap, sad,"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": ""
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "RAVDESS[16]\n2.5K\n8"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "ang, fear, disgust, surprise"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": ""
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "CMU MOSI[12]\n2.2K\n3\nneu, positive, negative"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": ""
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": ""
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": ""
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "The original CLAP model\nis\ntrained with audio-text pairs"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "sourced from three audio captioning datasets: ClothoV2 [8],"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": ""
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "AudioCaps\n[9], MACS [10],\nand one sound event dataset:"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": ""
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "FSD50K [11]. Altogether are referred as 4D henceforth."
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": ""
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "The architecture is based on the CLAP model\nin [6]. We"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": ""
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "chose this architecture because it yields SoTA performance"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": ""
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "in learning audio concepts with natural language description."
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": ""
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "We use log Mel spectrograms from the audios,\nsampled at"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": ""
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "44K Hz, as input\nto the audio encoder - CNN14 [17], which"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "is pretrained on 2M audio clips from AudioSet. The text en-"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": ""
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "coder is BERT uncased. The audio encodings are of 1024 di-"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "mensional whereas text encodings are 768 dimensional. Both"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": ""
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "encodings are then projected into a joint multimodal space of"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": ""
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "dimension 1024. Both audio and text encoders are frozen in"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": ""
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "our experiments, but\nthe projection layers are learnable. We"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": ""
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "use PyTorch to implement the model architecture. The model"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": ""
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "is trained with 0.0001 learning rate, batch size of 128, for 30"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "epochs using Adam optimizer."
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": ""
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "4. PROMPT GENERATION"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": ""
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "Emotion\ndatasets\ndo\nnot\nhave\nassociated\ndescriptions\nfor"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": ""
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "each audio.\nTherefore, we devise a scalable and automatic"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": ""
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "prompting method that\nis based on the acoustic properties of"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": ""
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "the speech audios.\nThere are numerous acoustic properties"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "that describe emotions, as discussed in Section 1. Hence, we"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": ""
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "hypothesize that\nincluding that\ninformation in the prompts"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "would beneﬁt emotion recognition. We calculate the pitch"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "and intensity using Librosa\n[18]\nand we\ncalculate\nspeech"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "rate and articulation rate using Praat [19]. We construct\nthe"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "prompts in the manner described below:"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "4.1. Class label (Prompt)"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "The simplest description for each audio can be the class la-"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": ""
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": ""
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "bel,\ni.e.\n‘anger’. Thus, we use this as the baseline prompt\nto"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "compare against the proposed prompts."
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": ""
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "4.2. Pitch Prompt"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": ""
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "Pitch is known to be affected by emotion,\nlower pitch is re-"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "lated with negative emotions like fear and high pitch is re-"
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "lated with positive emotions like happiness or surprise [4]."
        },
        {
          "Table 1: Details of the 6 emotion datasets used in this paper.": "Since pitch is naturally sex speciﬁc, we bin the average pitch"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "mation, we bin the average pitch into four classes,\nlow-male",
          "as described in Section 4, and 1 is using the prompt augmen-": "tation - which combines all the acoustic prompts."
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "pitch (< 132.5 Hz), high-male pitch (> 132.5 Hz, < 180 Hz),",
          "as described in Section 4, and 1 is using the prompt augmen-": ""
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "low-female pitch (> 180 Hz, < 210 Hz) and high-female",
          "as described in Section 4, and 1 is using the prompt augmen-": ""
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "pitch (> 210 Hz). For the sex agnostic case, we bin into two",
          "as described in Section 4, and 1 is using the prompt augmen-": ""
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "categories based on cutoff of 170 Hz.\nThe cutoffs are ob-",
          "as described in Section 4, and 1 is using the prompt augmen-": ""
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "tained from the average numbers for vocal pitch reported in",
          "as described in Section 4, and 1 is using the prompt augmen-": ""
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "literature. The prompt\nis set as ‘bin-class emotion-class’, an",
          "as described in Section 4, and 1 is using the prompt augmen-": ""
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "example of which is ‘low pitch anger’ (without sex informa-",
          "as described in Section 4, and 1 is using the prompt augmen-": "Fig.\n2:\nAccuracy\nachieved\nusing\ndifferent\nprompts\non Crema-d\n(left)"
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "tion) or ‘low male pitch anger’ (otherwise).",
          "as described in Section 4, and 1 is using the prompt augmen-": "and Ravdess\n(right).\nC=Class\nlabel, P=Pitch prompt,\nI=Intensity prompt,"
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "",
          "as described in Section 4, and 1 is using the prompt augmen-": "SR=Speech-Rate prompt, AR=Articulation-Rate prompt, PA=Prompt Aug-"
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "4.3.\nIntensity Prompt",
          "as described in Section 4, and 1 is using the prompt augmen-": ""
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "",
          "as described in Section 4, and 1 is using the prompt augmen-": "mentation."
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "Intensity is known to be affected by emotion, low intensity is",
          "as described in Section 4, and 1 is using the prompt augmen-": ""
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "",
          "as described in Section 4, and 1 is using the prompt augmen-": "We train the model on 4 audio captioning datasets and"
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "linked with negative emotion like sadness or melancholy and",
          "as described in Section 4, and 1 is using the prompt augmen-": ""
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "",
          "as described in Section 4, and 1 is using the prompt augmen-": "1 emotion dataset.\nThe left pat of Figure 2 shows the per-"
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "high intensity is linked with joy or excitement [4]. We bin the",
          "as described in Section 4, and 1 is using the prompt augmen-": ""
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "",
          "as described in Section 4, and 1 is using the prompt augmen-": "formance achieved when the model\nis trained on the training"
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "average intensity over the audio clip in two bins, low and high",
          "as described in Section 4, and 1 is using the prompt augmen-": ""
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "",
          "as described in Section 4, and 1 is using the prompt augmen-": "set (including 4D and Ravdess) and tested on the testing set"
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "intensity at 60 dB. The same rule as pitch prompt is followed",
          "as described in Section 4, and 1 is using the prompt augmen-": ""
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "",
          "as described in Section 4, and 1 is using the prompt augmen-": "of Ravdess. When done similarly for crema-d,\nthe perfor-"
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "to form the intensity prompt, an example of which is ‘high",
          "as described in Section 4, and 1 is using the prompt augmen-": ""
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "",
          "as described in Section 4, and 1 is using the prompt augmen-": "mance achieved is shown on the right side of Figure 2. We ob-"
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "intensity anger’.",
          "as described in Section 4, and 1 is using the prompt augmen-": ""
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "",
          "as described in Section 4, and 1 is using the prompt augmen-": "serve that among the 4 acoustic prompts, pitch prompt gives"
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "4.4.\nSpeech-rate Prompt",
          "as described in Section 4, and 1 is using the prompt augmen-": ""
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "",
          "as described in Section 4, and 1 is using the prompt augmen-": "the best performance. Next best performance is achieved by"
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "It has been observed that faster spoken speech is linked with",
          "as described in Section 4, and 1 is using the prompt augmen-": "the intensity prompt. This can be observed in both Ravdess"
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "highly potent emotions such as anger and happiness whilst",
          "as described in Section 4, and 1 is using the prompt augmen-": "and Crema-d. On Crema-d, articulation rate prompt performs"
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "slower speech is linked with sadness, disgust and boredom",
          "as described in Section 4, and 1 is using the prompt augmen-": "better than speech rate prompt but\nthe reverse is observed in"
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "[3].\nSpeech rate is calculated by extracting the number of",
          "as described in Section 4, and 1 is using the prompt augmen-": "Ravdess. Secondly, we observe that overall prompt augmen-"
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "syllables\nspoken divided by the total duration of\nthe audio",
          "as described in Section 4, and 1 is using the prompt augmen-": "tation is giving the best performance in both datasets. This"
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "clip. We use 3.12 syllables/sec as the cutoff to bin the speech",
          "as described in Section 4, and 1 is using the prompt augmen-": "validates our original hypothesis that using acoustic prompts"
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "rate into two bins,\nlow and high speech rate. An example of",
          "as described in Section 4, and 1 is using the prompt augmen-": "would help the emotion classiﬁcation."
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "speech-rate prompt is ‘high speech rate anger’.",
          "as described in Section 4, and 1 is using the prompt augmen-": ""
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "",
          "as described in Section 4, and 1 is using the prompt augmen-": "5.2. Emotion Classiﬁcation"
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "4.5. Articulation-rate Prompt",
          "as described in Section 4, and 1 is using the prompt augmen-": ""
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "",
          "as described in Section 4, and 1 is using the prompt augmen-": "To evaluate how the acoustic prompts would help in emo-"
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "Similarly to speech rate,\nfast articulation rate is linked with",
          "as described in Section 4, and 1 is using the prompt augmen-": ""
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "",
          "as described in Section 4, and 1 is using the prompt augmen-": "tion classiﬁcation, we perform the following two experiments."
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "emotions of interest, fear or happiness; whereas slow articu-",
          "as described in Section 4, and 1 is using the prompt augmen-": ""
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "",
          "as described in Section 4, and 1 is using the prompt augmen-": "The ﬁrst is a zero-shot like setup where we leave one dataset"
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "lation rate is indicative of sadness and disgust [3]. Articula-",
          "as described in Section 4, and 1 is using the prompt augmen-": ""
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "",
          "as described in Section 4, and 1 is using the prompt augmen-": "out, which is used during the testing stage. The second is a"
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "tion rate is calculated as total number of syllables divided by",
          "as described in Section 4, and 1 is using the prompt augmen-": ""
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "",
          "as described in Section 4, and 1 is using the prompt augmen-": "ﬁnetuning setup where the model from the ﬁrst setup is ﬁne"
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "the total phonation time. We bin the audio into low and high",
          "as described in Section 4, and 1 is using the prompt augmen-": ""
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "",
          "as described in Section 4, and 1 is using the prompt augmen-": "tuned on the left out dataset."
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "articulation rate at\nthe cutoff of 4 syllables/sec. An example",
          "as described in Section 4, and 1 is using the prompt augmen-": ""
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "of articulation-rate prompt is ‘high articulation rate anger’.",
          "as described in Section 4, and 1 is using the prompt augmen-": "5.2.1.\nLeave one out"
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "Even though speech and articulation rate are similar concepts,",
          "as described in Section 4, and 1 is using the prompt augmen-": "This\nsetup evaluates how well\na model\ntrained on a pre-"
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "speech rate captures speaker speciﬁc information in the form",
          "as described in Section 4, and 1 is using the prompt augmen-": "deﬁned set of classes generalizes\nto a new dataset, which"
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "of number of pauses and hesitation whereas articulation rate",
          "as described in Section 4, and 1 is using the prompt augmen-": "might have same or different sets of classes. Out of\nthe 6"
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "would ignore such information.",
          "as described in Section 4, and 1 is using the prompt augmen-": "emotion datasets, we leave one out\nfor\ntesting and train the"
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "4.6. Prompt Augmentation",
          "as described in Section 4, and 1 is using the prompt augmen-": "model on the other 5 emotion datasets. Therefore the training"
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "",
          "as described in Section 4, and 1 is using the prompt augmen-": "and testing datasets are completely different.\nIn case where"
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "To combine all 5 prompts, we pair an audio clip independently",
          "as described in Section 4, and 1 is using the prompt augmen-": ""
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "",
          "as described in Section 4, and 1 is using the prompt augmen-": "Ravdess is the testing dataset, ‘calm’ class is not represented"
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "with each acoustic prompt. Thus, one audio clip will result in",
          "as described in Section 4, and 1 is using the prompt augmen-": ""
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "",
          "as described in Section 4, and 1 is using the prompt augmen-": "in any of the other training datasets and is a zero shot classiﬁ-"
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "5 pairs used for training our model.",
          "as described in Section 4, and 1 is using the prompt augmen-": ""
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "",
          "as described in Section 4, and 1 is using the prompt augmen-": "cation result.\nIn case of Crema-d, all\nthe emotion classes are"
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "5. EXPERIMENTS AND RESULTS",
          "as described in Section 4, and 1 is using the prompt augmen-": "represented in the training classes from other datasets."
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "",
          "as described in Section 4, and 1 is using the prompt augmen-": "We perform 4 experiments, shown in columns 2,3 of Ta-"
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "5.1. Prompt Analysis",
          "as described in Section 4, and 1 is using the prompt augmen-": ""
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "",
          "as described in Section 4, and 1 is using the prompt augmen-": "ble 2. There are two main takeaways from this experiment."
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "To evaluate which of the proposed acoustic prompts are better",
          "as described in Section 4, and 1 is using the prompt augmen-": "Firstly adding Emotion datasets in the training stage is help-"
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "and to access if any of them are better than the class label, we",
          "as described in Section 4, and 1 is using the prompt augmen-": "ing the performance on the left out emotion dataset. This can"
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "apply the trained model on emotion classiﬁcation. The model",
          "as described in Section 4, and 1 is using the prompt augmen-": "be observed in both Ravdess and Crema-d. For crema-d the"
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "is trained 6 different\ntimes, where each time the description",
          "as described in Section 4, and 1 is using the prompt augmen-": "performance improves from 17.85% to 35.22% just by chang-"
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "associated with emotion audios are varied. Among the 6, 1 is",
          "as described in Section 4, and 1 is using the prompt augmen-": "ing from 4D to 5ED in the training sets.\nIn ravdess,\nthe per-"
        },
        {
          "into both based on sex and without sex.\nIncluding sex infor-": "using the class label alone, 4 are using the acoustic prompts",
          "as described in Section 4, and 1 is using the prompt augmen-": "formance improves from 15.99% to 22.88%."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: The main takeaway from this ex-",
      "data": [
        {
          "Leave one out dataset": "Ravdess",
          "Finetune": "Ravdess"
        },
        {
          "Leave one out dataset": "12.50",
          "Finetune": "12.50"
        },
        {
          "Leave one out dataset": "15.99",
          "Finetune": "68.50"
        },
        {
          "Leave one out dataset": "22.88",
          "Finetune": "68.50"
        },
        {
          "Leave one out dataset": "38.46",
          "Finetune": "68.69"
        },
        {
          "Leave one out dataset": "27.88",
          "Finetune": "72.46"
        },
        {
          "Leave one out dataset": "-",
          "Finetune": "81.82 [21]"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: The main takeaway from this ex-",
      "data": [
        {
          "5 ED - class label": "4D + [5 ED - class label]",
          "35.22": "34.68",
          "22.88\n68.54\n68.50": "72.86\n38.46\n68.69"
        },
        {
          "5 ED - class label": "4D + [5 ED - prompt augmentation]",
          "35.22": "33.00",
          "22.88\n68.54\n68.50": "72.46\n27.88\n72.56"
        },
        {
          "5 ED - class label": "SoTA",
          "35.22": "-",
          "22.88\n68.54\n68.50": "-\n74.70 [20]\n81.82 [21]"
        },
        {
          "5 ED - class label": "Secondly, prompt augmentation’s results are similar to the",
          "35.22": "",
          "22.88\n68.54\n68.50": "Figure 3 shows the results of audio retrieval. We calcu-"
        },
        {
          "5 ED - class label": "results obtained when trained using only the class label. We",
          "35.22": "",
          "22.88\n68.54\n68.50": "late precision@K for each acoustic prompt shown on the x-"
        },
        {
          "5 ED - class label": "believe this is happening because of the distribution shift\nin",
          "35.22": "",
          "22.88\n68.54\n68.50": "axis. From the results we observe that\nthe model\ntrained on"
        },
        {
          "5 ED - class label": "the training and testing datasets.\nThis also effects distribu-",
          "35.22": "",
          "22.88\n68.54\n68.50": "all datasets, and using prompt augmentation performs the best"
        },
        {
          "5 ED - class label": "tions of\nthe acoustics\n- which directly effects\nthe acoustic",
          "35.22": "",
          "22.88\n68.54\n68.50": "in all cases (all\ntypes of queries). The takeaway here is that"
        },
        {
          "5 ED - class label": "prompts\nin the training and testing datasets.\nFor example,",
          "35.22": "",
          "22.88\n68.54\n68.50": "our model\nis able to retrieve audio signiﬁcantly better when"
        },
        {
          "5 ED - class label": "‘high intensity anger’ might not be occurring in the training",
          "35.22": "",
          "22.88\n68.54\n68.50": "trained using prompt augmentation. The precision@K num-"
        },
        {
          "5 ED - class label": "datasets, but is present in the testing dataset. This is harming",
          "35.22": "",
          "22.88\n68.54\n68.50": "bers are comparable to numbers observed in audio retrieval"
        },
        {
          "5 ED - class label": "the beneﬁt learnt from the acoustic prompts to be transferable",
          "35.22": "",
          "22.88\n68.54\n68.50": "tasks [22].\nThe results are encouraging since this suggests"
        },
        {
          "5 ED - class label": "to a completely new dataset at\ntesting time. However we do",
          "35.22": "",
          "22.88\n68.54\n68.50": "that we can introduce even more elaborate descriptions for"
        },
        {
          "5 ED - class label": "observe improvement using acoustic prompts when we per-",
          "35.22": "",
          "22.88\n68.54\n68.50": "each audio and the model will\nlearn associations and be able"
        },
        {
          "5 ED - class label": "form the ﬁnetuning experiment in the next section. Note that",
          "35.22": "",
          "22.88\n68.54\n68.50": "to retrieve audios with those descriptions."
        },
        {
          "5 ED - class label": "the SoTA performance for this evaluation setup is not found",
          "35.22": "",
          "22.88\n68.54\n68.50": ""
        },
        {
          "5 ED - class label": "in literature because the general evaluation setup is when the",
          "35.22": "",
          "22.88\n68.54\n68.50": ""
        },
        {
          "5 ED - class label": "dataset is present in both training and testing sets.",
          "35.22": "",
          "22.88\n68.54\n68.50": ""
        },
        {
          "5 ED - class label": "5.2.2. Finetune",
          "35.22": "",
          "22.88\n68.54\n68.50": ""
        },
        {
          "5 ED - class label": "In this experiment, we want to analyse how using the acoustic",
          "35.22": "",
          "22.88\n68.54\n68.50": ""
        },
        {
          "5 ED - class label": "prompts at pre-training stage would beneﬁt the classiﬁcation.",
          "35.22": "",
          "22.88\n68.54\n68.50": ""
        },
        {
          "5 ED - class label": "We take the model from the previous stage and perform ﬁne",
          "35.22": "",
          "22.88\n68.54\n68.50": ""
        },
        {
          "5 ED - class label": "tuning on the dataset that had been left out.",
          "35.22": "",
          "22.88\n68.54\n68.50": ""
        },
        {
          "5 ED - class label": "The\nresults\nfor\nsupervised\nclassiﬁcation\nare\nshown\nin",
          "35.22": "",
          "22.88\n68.54\n68.50": ""
        },
        {
          "5 ED - class label": "columns 4,5 of Table 2.\nThe main takeaway from this ex-",
          "35.22": "",
          "22.88\n68.54\n68.50": ""
        },
        {
          "5 ED - class label": "periment is that with ﬁnetuning, prompt augmentation shows",
          "35.22": "",
          "22.88\n68.54\n68.50": ""
        },
        {
          "5 ED - class label": "",
          "35.22": "",
          "22.88\n68.54\n68.50": "Fig. 3: Precision@K achieved when the trained model under different set-"
        },
        {
          "5 ED - class label": "improvement over the other models.\nIn Ravdess, we see im-",
          "35.22": "",
          "22.88\n68.54\n68.50": ""
        },
        {
          "5 ED - class label": "",
          "35.22": "",
          "22.88\n68.54\n68.50": "tings is used for the audio retrieval task."
        },
        {
          "5 ED - class label": "provement\nin performance by absolute 3.77%, from 68.69%",
          "35.22": "",
          "22.88\n68.54\n68.50": ""
        },
        {
          "5 ED - class label": "to 72.46%.\nIn Crema-d the performance is about\nthe same",
          "35.22": "",
          "22.88\n68.54\n68.50": "6. CONCLUSION"
        },
        {
          "5 ED - class label": "as when the prompt augmentation is not used - 72.86% vs",
          "35.22": "",
          "22.88\n68.54\n68.50": ""
        },
        {
          "5 ED - class label": "",
          "35.22": "",
          "22.88\n68.54\n68.50": "This work performs emotion recognition using the audios and"
        },
        {
          "5 ED - class label": "72.56%, however not signiﬁcantly different.",
          "35.22": "",
          "22.88\n68.54\n68.50": ""
        },
        {
          "5 ED - class label": "",
          "35.22": "",
          "22.88\n68.54\n68.50": "their natural\nlanguage descriptions. We use the acoustics of"
        },
        {
          "5 ED - class label": "",
          "35.22": "",
          "22.88\n68.54\n68.50": "emotions to prompt the audios, in fact there can be more com-"
        },
        {
          "5 ED - class label": "5.3. Emotion Audio Retrieval",
          "35.22": "",
          "22.88\n68.54\n68.50": ""
        },
        {
          "5 ED - class label": "",
          "35.22": "",
          "22.88\n68.54\n68.50": "plicated descriptions,\ninvoking the semantics, environment,"
        },
        {
          "5 ED - class label": "With the increasing sizes of the audio databases, to be able to",
          "35.22": "",
          "22.88\n68.54\n68.50": ""
        },
        {
          "5 ED - class label": "",
          "35.22": "",
          "22.88\n68.54\n68.50": "context among other\nfactors. We envision that as methods"
        },
        {
          "5 ED - class label": "search such databases for speciﬁc types of audios is impor-",
          "35.22": "",
          "22.88\n68.54\n68.50": ""
        },
        {
          "5 ED - class label": "",
          "35.22": "",
          "22.88\n68.54\n68.50": "of describing emotions become more complicated, our abil-"
        },
        {
          "5 ED - class label": "tant.\nTherefore we evaluate our models speciﬁcally for\nthe",
          "35.22": "",
          "22.88\n68.54\n68.50": ""
        },
        {
          "5 ED - class label": "",
          "35.22": "",
          "22.88\n68.54\n68.50": "ity to model more nuanced emotions will become better. The"
        },
        {
          "5 ED - class label": "audio retrieval\ntask.\nThis would access whether\nthe trained",
          "35.22": "",
          "22.88\n68.54\n68.50": ""
        },
        {
          "5 ED - class label": "",
          "35.22": "",
          "22.88\n68.54\n68.50": "acoustic properties we extract include pitch, intensity, speech"
        },
        {
          "5 ED - class label": "model\nlearns the associations between the acoustic prompts",
          "35.22": "",
          "22.88\n68.54\n68.50": ""
        },
        {
          "5 ED - class label": "",
          "35.22": "",
          "22.88\n68.54\n68.50": "rate and articulation rate from the audios. We ﬁnd that among"
        },
        {
          "5 ED - class label": "and the respective acoustic properties. We make queries sim-",
          "35.22": "",
          "22.88\n68.54\n68.50": ""
        },
        {
          "5 ED - class label": "",
          "35.22": "",
          "22.88\n68.54\n68.50": "the acoustic prompts, pitch prompt\nis\nthe best performing."
        },
        {
          "5 ED - class label": "ilarly to the prompts as described in Section 4.\nFor a given",
          "35.22": "",
          "22.88\n68.54\n68.50": ""
        },
        {
          "5 ED - class label": "",
          "35.22": "",
          "22.88\n68.54\n68.50": "Overall, when we do prompt augmentation it achieves\nthe"
        },
        {
          "5 ED - class label": "query,\nthe model outputs top K audios whose audio embed-",
          "35.22": "",
          "22.88\n68.54\n68.50": ""
        },
        {
          "5 ED - class label": "",
          "35.22": "",
          "22.88\n68.54\n68.50": "highest accuracy and improves the performance in Ravdess"
        },
        {
          "5 ED - class label": "dings have highest cosine similarity to the text embedding",
          "35.22": "",
          "22.88\n68.54\n68.50": ""
        },
        {
          "5 ED - class label": "",
          "35.22": "",
          "22.88\n68.54\n68.50": "by 3.8%. We also perform emotion audio retrieval and ﬁnd"
        },
        {
          "5 ED - class label": "of the query. For the top K audios, since we know the true",
          "35.22": "",
          "22.88\n68.54\n68.50": ""
        },
        {
          "5 ED - class label": "",
          "35.22": "",
          "22.88\n68.54\n68.50": "that when using model\ntrained on prompt augmentation, we"
        },
        {
          "5 ED - class label": "acoustic prompts, we can evaluate how good the model’s out-",
          "35.22": "",
          "22.88\n68.54\n68.50": ""
        },
        {
          "5 ED - class label": "",
          "35.22": "",
          "22.88\n68.54\n68.50": "get the signiﬁcantly better retrieval performance."
        },
        {
          "5 ED - class label": "puts is w.r.t the query.",
          "35.22": "",
          "22.88\n68.54\n68.50": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "7. REFERENCES": "",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "database,” Language resources and evaluation, vol. 42,"
        },
        {
          "7. REFERENCES": "[1] Robert Plutchik,\nThe emotions,\nUniversity Press of",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "no. 4, pp. 335–359, 2008."
        },
        {
          "7. REFERENCES": "America, 1991.",
          "“Iemocap:\nInteractive emotional dyadic motion capture": ""
        },
        {
          "7. REFERENCES": "",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "[14] Soujanya Poria, Devamanyu Hazarika, Navonil Ma-"
        },
        {
          "7. REFERENCES": "[2] Paul Ekman, Are there basic emotions?, American Psy-",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "jumder, Gautam Naik, Erik Cambria,\nand Rada Mi-"
        },
        {
          "7. REFERENCES": "chological Association, 1992.",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "halcea,\n“Meld: A multimodal multi-party dataset\nfor"
        },
        {
          "7. REFERENCES": "",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "arXiv preprint\nemotion recognition in conversations,”"
        },
        {
          "7. REFERENCES": "[3] Robert W Frick, “Communicating emotion: The role of",
          "“Iemocap:\nInteractive emotional dyadic motion capture": ""
        },
        {
          "7. REFERENCES": "",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "arXiv:1810.02508, 2018."
        },
        {
          "7. REFERENCES": "prosodic features.,” Psychological bulletin, vol. 97, no.",
          "“Iemocap:\nInteractive emotional dyadic motion capture": ""
        },
        {
          "7. REFERENCES": "3, pp. 412, 1985.",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "[15] Houwei Cao,\nDavid G Cooper, Michael K Keut-"
        },
        {
          "7. REFERENCES": "",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "mann, Ruben C Gur, Ani Nenkova, and Ragini Verma,"
        },
        {
          "7. REFERENCES": "[4] Klaus R Scherer,\n“Acoustic concomitants of emotional",
          "“Iemocap:\nInteractive emotional dyadic motion capture": ""
        },
        {
          "7. REFERENCES": "",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "“Crema-d: Crowd-sourced emotional multimodal actors"
        },
        {
          "7. REFERENCES": "dimensions:\nJudging affect\nfrom synthesized tone se-",
          "“Iemocap:\nInteractive emotional dyadic motion capture": ""
        },
        {
          "7. REFERENCES": "",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "dataset,” IEEE transactions on affective computing, vol."
        },
        {
          "7. REFERENCES": "quences.,” 1972.",
          "“Iemocap:\nInteractive emotional dyadic motion capture": ""
        },
        {
          "7. REFERENCES": "",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "5, no. 4, pp. 377–390, 2014."
        },
        {
          "7. REFERENCES": "[5] Aneta Pavlenko, Emotions and multilingualism., Cam-",
          "“Iemocap:\nInteractive emotional dyadic motion capture": ""
        },
        {
          "7. REFERENCES": "",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "[16] Steven R Livingstone and Frank A Russo,\n“The ryer-"
        },
        {
          "7. REFERENCES": "bridge University Press, 2005.",
          "“Iemocap:\nInteractive emotional dyadic motion capture": ""
        },
        {
          "7. REFERENCES": "",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "son audio-visual database of emotional speech and song"
        },
        {
          "7. REFERENCES": "",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "(ravdess): A dynamic, multimodal set of facial and vo-"
        },
        {
          "7. REFERENCES": "[6] Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Is-",
          "“Iemocap:\nInteractive emotional dyadic motion capture": ""
        },
        {
          "7. REFERENCES": "",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "cal expressions in north american english,”\nPloS one,"
        },
        {
          "7. REFERENCES": "mail, and Huaming Wang,\n“Clap: Learning audio con-",
          "“Iemocap:\nInteractive emotional dyadic motion capture": ""
        },
        {
          "7. REFERENCES": "",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "vol. 13, no. 5, pp. e0196391, 2018."
        },
        {
          "7. REFERENCES": "cepts from natural language supervision,” arXiv preprint",
          "“Iemocap:\nInteractive emotional dyadic motion capture": ""
        },
        {
          "7. REFERENCES": "arXiv:2206.04769, 2022.",
          "“Iemocap:\nInteractive emotional dyadic motion capture": ""
        },
        {
          "7. REFERENCES": "",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "[17] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,"
        },
        {
          "7. REFERENCES": "",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "Wenwu Wang, and Mark D Plumbley,\n“Panns: Large-"
        },
        {
          "7. REFERENCES": "[7] Soham Deshmukh, Benjamin Elizalde,\nand Huaming",
          "“Iemocap:\nInteractive emotional dyadic motion capture": ""
        },
        {
          "7. REFERENCES": "",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "scale pretrained audio neural networks\nfor audio pat-"
        },
        {
          "7. REFERENCES": "Wang,\n“Audio retrieval with wavtext5k and clap train-",
          "“Iemocap:\nInteractive emotional dyadic motion capture": ""
        },
        {
          "7. REFERENCES": "",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "IEEE/ACM Transactions on Audio,\ntern recognition,”"
        },
        {
          "7. REFERENCES": "ing,” arXiv preprint arXiv:2209.14275, 2022.",
          "“Iemocap:\nInteractive emotional dyadic motion capture": ""
        },
        {
          "7. REFERENCES": "",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "Speech, and Language Processing, vol. 28, pp. 2880–"
        },
        {
          "7. REFERENCES": "",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "2894, 2020."
        },
        {
          "7. REFERENCES": "[8] Konstantinos Drossos, Samuel Lipping,\nand Tuomas",
          "“Iemocap:\nInteractive emotional dyadic motion capture": ""
        },
        {
          "7. REFERENCES": "Virtanen,\n“Clotho:\nan audio captioning dataset,”\nin",
          "“Iemocap:\nInteractive emotional dyadic motion capture": ""
        },
        {
          "7. REFERENCES": "",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "[18] Brian McFee, Colin Raffel, Dawen Liang, Daniel P El-"
        },
        {
          "7. REFERENCES": "IEEE International Conference on Acoustics,\nSpeech",
          "“Iemocap:\nInteractive emotional dyadic motion capture": ""
        },
        {
          "7. REFERENCES": "",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "lis, Matt McVicar, Eric Battenberg, and Oriol Nieto, “li-"
        },
        {
          "7. REFERENCES": "and Signal Processing (ICASSP), 2020.",
          "“Iemocap:\nInteractive emotional dyadic motion capture": ""
        },
        {
          "7. REFERENCES": "",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "brosa: Audio and music signal analysis in python,”\nin"
        },
        {
          "7. REFERENCES": "",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "Proceedings of\nthe 14th python in science conference,"
        },
        {
          "7. REFERENCES": "[9] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,",
          "“Iemocap:\nInteractive emotional dyadic motion capture": ""
        },
        {
          "7. REFERENCES": "",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "2015, vol. 8, pp. 18–25."
        },
        {
          "7. REFERENCES": "and Gunhee Kim,\n“AudioCaps: Generating Captions",
          "“Iemocap:\nInteractive emotional dyadic motion capture": ""
        },
        {
          "7. REFERENCES": "for Audios in The Wild,” in NAACL-HLT, 2019.",
          "“Iemocap:\nInteractive emotional dyadic motion capture": ""
        },
        {
          "7. REFERENCES": "",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "https://www.fon.hum.uva.nl/\n[19]\n“Praat.,”"
        },
        {
          "7. REFERENCES": "",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "praat,\n[Online]."
        },
        {
          "7. REFERENCES": "[10]\nIrene Mart´ın-Morat´o and Annamaria Mesaros, “What is",
          "“Iemocap:\nInteractive emotional dyadic motion capture": ""
        },
        {
          "7. REFERENCES": "the ground truth? reliability of multi-annotator data for",
          "“Iemocap:\nInteractive emotional dyadic motion capture": ""
        },
        {
          "7. REFERENCES": "",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "[20] Tung-Yu Wu, Chen-An Li, Tzu-Han Lin, Tsu-Yuan Hsu,"
        },
        {
          "7. REFERENCES": "audio tagging,” in 2021 29th European Signal Process-",
          "“Iemocap:\nInteractive emotional dyadic motion capture": ""
        },
        {
          "7. REFERENCES": "",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "and Hung-Yi Lee, “The ability of self-supervised speech"
        },
        {
          "7. REFERENCES": "ing Conference (EUSIPCO). IEEE, 2021, pp. 76–80.",
          "“Iemocap:\nInteractive emotional dyadic motion capture": ""
        },
        {
          "7. REFERENCES": "",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "arXiv\npreprint\nmodels\nfor\naudio\nrepresentations,”"
        },
        {
          "7. REFERENCES": "",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "arXiv:2209.12900, 2022."
        },
        {
          "7. REFERENCES": "[11] Eduardo Fonseca, Xavier Favory, Jordi Pons, Frederic",
          "“Iemocap:\nInteractive emotional dyadic motion capture": ""
        },
        {
          "7. REFERENCES": "Font, and Xavier Serra,\n“Fsd50k: An open dataset of",
          "“Iemocap:\nInteractive emotional dyadic motion capture": ""
        },
        {
          "7. REFERENCES": "",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "[21] Cristina Luna-Jim´enez, Ricardo Kleinlein, David Griol,"
        },
        {
          "7. REFERENCES": "human-labeled sound events,” IEEE/ACM Transactions",
          "“Iemocap:\nInteractive emotional dyadic motion capture": ""
        },
        {
          "7. REFERENCES": "",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "Zoraida Callejas,\nJuan M Montero,\nand\nFernando"
        },
        {
          "7. REFERENCES": "on Audio, Speech, and Language Processing, 2022.",
          "“Iemocap:\nInteractive emotional dyadic motion capture": ""
        },
        {
          "7. REFERENCES": "",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "Fern´andez-Mart´ınez,\n“A proposal for multimodal emo-"
        },
        {
          "7. REFERENCES": "",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "tion\nrecognition\nusing\naural\ntransformers\nand\naction"
        },
        {
          "7. REFERENCES": "[12] AmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Po-",
          "“Iemocap:\nInteractive emotional dyadic motion capture": ""
        },
        {
          "7. REFERENCES": "",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "units on ravdess dataset,” Applied Sciences, vol. 12, no."
        },
        {
          "7. REFERENCES": "ria, Erik Cambria, and Louis-Philippe Morency, “Multi-",
          "“Iemocap:\nInteractive emotional dyadic motion capture": ""
        },
        {
          "7. REFERENCES": "",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "1, pp. 327, 2021."
        },
        {
          "7. REFERENCES": "modal language analysis in the wild: Cmu-mosei dataset",
          "“Iemocap:\nInteractive emotional dyadic motion capture": ""
        },
        {
          "7. REFERENCES": "and interpretable dynamic fusion graph,” in Proceedings",
          "“Iemocap:\nInteractive emotional dyadic motion capture": ""
        },
        {
          "7. REFERENCES": "",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "[22] Bongjun Kim and Bryan Pardo,\n“Improving content-"
        },
        {
          "7. REFERENCES": "of the 56th Annual Meeting of the Association for Com-",
          "“Iemocap:\nInteractive emotional dyadic motion capture": ""
        },
        {
          "7. REFERENCES": "",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "based audio retrieval by vocal\nimitation feedback,”\nin"
        },
        {
          "7. REFERENCES": "putational Linguistics (Volume 1: Long Papers), 2018,",
          "“Iemocap:\nInteractive emotional dyadic motion capture": ""
        },
        {
          "7. REFERENCES": "",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "ICASSP 2019-2019 IEEE International Conference on"
        },
        {
          "7. REFERENCES": "pp. 2236–2246.",
          "“Iemocap:\nInteractive emotional dyadic motion capture": ""
        },
        {
          "7. REFERENCES": "",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "Acoustics,\nSpeech\nand\nSignal Processing\n(ICASSP)."
        },
        {
          "7. REFERENCES": "",
          "“Iemocap:\nInteractive emotional dyadic motion capture": "IEEE, 2019, pp. 4100–4104."
        },
        {
          "7. REFERENCES": "[13] Carlos Busso, Murtaza Bulut,\nChi-Chun Lee, Abe",
          "“Iemocap:\nInteractive emotional dyadic motion capture": ""
        },
        {
          "7. REFERENCES": "Kazemzadeh, Emily Mower, Samuel Kim, Jeannette N",
          "“Iemocap:\nInteractive emotional dyadic motion capture": ""
        },
        {
          "7. REFERENCES": "Chang,\nSungbok Lee,\nand\nShrikanth\nS Narayanan,",
          "“Iemocap:\nInteractive emotional dyadic motion capture": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "The emotions",
      "authors": [
        "Robert Plutchik"
      ],
      "year": "1991",
      "venue": "The emotions"
    },
    {
      "citation_id": "3",
      "title": "Are there basic emotions?",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1992",
      "venue": "Are there basic emotions?"
    },
    {
      "citation_id": "4",
      "title": "Communicating emotion: The role of prosodic features",
      "authors": [
        "Robert Frick"
      ],
      "year": "1985",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "5",
      "title": "Acoustic concomitants of emotional dimensions: Judging affect from synthesized tone sequences",
      "authors": [
        "Klaus Scherer"
      ],
      "year": "1972",
      "venue": "Acoustic concomitants of emotional dimensions: Judging affect from synthesized tone sequences"
    },
    {
      "citation_id": "6",
      "title": "Emotions and multilingualism",
      "authors": [
        "Aneta Pavlenko"
      ],
      "year": "2005",
      "venue": "Emotions and multilingualism"
    },
    {
      "citation_id": "7",
      "title": "Clap: Learning audio concepts from natural language supervision",
      "authors": [
        "Benjamin Elizalde",
        "Soham Deshmukh"
      ],
      "year": "2022",
      "venue": "Clap: Learning audio concepts from natural language supervision",
      "arxiv": "arXiv:2206.04769"
    },
    {
      "citation_id": "8",
      "title": "Audio retrieval with wavtext5k and clap training",
      "authors": [
        "Soham Deshmukh",
        "Benjamin Elizalde",
        "Huaming Wang"
      ],
      "year": "2022",
      "venue": "Audio retrieval with wavtext5k and clap training",
      "arxiv": "arXiv:2209.14275"
    },
    {
      "citation_id": "9",
      "title": "Clotho: an audio captioning dataset",
      "authors": [
        "Konstantinos Drossos",
        "Samuel Lipping",
        "Tuomas Virtanen"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "10",
      "title": "AudioCaps: Generating Captions for Audios in The Wild",
      "authors": [
        "Chris Dongjoo Kim",
        "Byeongchang Kim",
        "Hyunmin Lee",
        "Gunhee Kim"
      ],
      "year": "2019",
      "venue": "NAACL-HLT"
    },
    {
      "citation_id": "11",
      "title": "What is the ground truth? reliability of multi-annotator data for audio tagging",
      "authors": [
        "Irene Martín",
        "Annamaria Mesaros"
      ],
      "year": "2021",
      "venue": "2021 29th European Signal Processing Conference"
    },
    {
      "citation_id": "12",
      "title": "Fsd50k: An open dataset of human-labeled sound events",
      "authors": [
        "Eduardo Fonseca",
        "Xavier Favory",
        "Jordi Pons",
        "Frederic Font",
        "Xavier Serra"
      ],
      "year": "2022",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "13",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "14",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "15",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Gautam Naik",
        "Erik Cambria",
        "Rada Mihalcea"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "16",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "Houwei Cao",
        "David Cooper",
        "Ruben Michael K Keutmann",
        "Ani Gur",
        "Ragini Nenkova",
        "Verma"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "17",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "R Steven",
        "Frank Livingstone",
        "Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "18",
      "title": "Panns: Largescale pretrained audio neural networks for audio pattern recognition",
      "authors": [
        "Qiuqiang Kong",
        "Yin Cao",
        "Turab Iqbal",
        "Yuxuan Wang",
        "Wenwu Wang",
        "Mark Plumbley"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "19",
      "title": "librosa: Audio and music signal analysis in python",
      "authors": [
        "Brian Mcfee",
        "Colin Raffel",
        "Dawen Liang",
        "P Daniel",
        "Matt Ellis",
        "Eric Mcvicar",
        "Oriol Battenberg",
        "Nieto"
      ],
      "year": "2015",
      "venue": "Proceedings of the 14th python in science conference"
    },
    {
      "citation_id": "20",
      "title": "",
      "authors": [
        "Praat"
      ],
      "venue": ""
    },
    {
      "citation_id": "21",
      "title": "The ability of self-supervised speech models for audio representations",
      "authors": [
        "Tung-Yu Wu",
        "Chen-An Li",
        "Tzu-Han Lin",
        "Tsu-Yuan Hsu",
        "Hung-Yi Lee"
      ],
      "year": "2022",
      "venue": "The ability of self-supervised speech models for audio representations",
      "arxiv": "arXiv:2209.12900"
    },
    {
      "citation_id": "22",
      "title": "A proposal for multimodal emotion recognition using aural transformers and action units on ravdess dataset",
      "authors": [
        "Cristina Luna-Jiménez",
        "Ricardo Kleinlein",
        "David Griol",
        "Zoraida Callejas",
        "Juan Montero",
        "Fernando Fernández-Martínez"
      ],
      "year": "2021",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "23",
      "title": "Improving contentbased audio retrieval by vocal imitation feedback",
      "authors": [
        "Bongjun Kim",
        "Bryan Pardo"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    }
  ]
}