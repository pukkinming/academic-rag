{
  "paper_id": "2201.07876v1",
  "title": "Unsupervised Personalization Of An Emotion Recognition System: The Unique Properties Of The Externalization Of Valence In Speech",
  "published": "2022-01-19T22:14:49Z",
  "authors": [
    "Kusha Sridhar",
    "Carlos Busso"
  ],
  "keywords": [
    "Speech emotion recognition",
    "adaptation",
    "transfer learning",
    "emotional dimensions",
    "valence"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The prediction of valence from speech is an important, but challenging problem. The externalization of valence in speech has speaker-dependent cues, which contribute to performances that are often significantly lower than the prediction of other emotional attributes such as arousal and dominance. A practical approach to improve valence prediction from speech is to adapt the models to the target speakers in the test set. Adapting a speech emotion recognition (SER) system to a particular speaker is a hard problem, especially with deep neural networks (DNNs), since it requires optimizing millions of parameters. This study proposes an unsupervised approach to address this problem by searching for speakers in the train set with similar acoustic patterns as the speaker in the test set. Speech samples from the selected speakers are used to create the adaptation set. This approach leverages transfer learning using pre-trained models, which are adapted with these speech samples. We propose three alternative adaptation strategies: unique speaker, oversampling and weighting approaches. These methods differ on the use of the adaptation set in the personalization of the valence models. The results demonstrate that a valence prediction model can be efficiently personalized with these unsupervised approaches, leading to relative improvements as high as 13.52%.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "T HE area of speech emotion recognition (SER) is an important research problem due to its key potential in fields such as human-computer interactions (HCIs), healthcare  [1] ,  [2]  and behavioral studies  [3] ,  [4] . Despite remarkable advances in emotion recognition, detecting emotions from speech is still a challenging task. The usual formulation to describe emotions is with categorical descriptors such as happiness, sadness, anger and neutral. However, this approach may not capture the intra and inter class variability across distinct emotional classes. (i.e., variability across sentences with the same emotional class labels and variability across sentences with different emotional class labels). An alternative representation is the use of emotional attributes, as suggested by the core affect theory  [5] . The most common attributes are arousal (calm versus active), valence (unpleasant versus pleasant) and dominance (weak versus strong). Because of their direct application in many areas, it is very important to build accurate models, which can reliably predict these emotional attributes. The estimation of emotional attributes is often posed as • K. Sridhar and C. Busso are with the Erik Jonsson School of Engineering and Computer Science, The University of Texas at Dallas, ( E-mail: Kusha.Sridhar@utdallas.edu, busso@utdallas.edu)\n\na regression problem, where the goal is to predict the scores associated with these attributes. In particular, the emotional attribute valence is key to understand many behavioral disorders  [6] ,  [7]  such as post-traumatic stress disorder (PTSD), depression, schizophrenia and anxiety. Although different approaches have been proposed to improve SER systems, the prediction of valence using acoustic features is often less accurate than other emotional attributes such as arousal or dominance. The gap in performance is significant even with different methods specially implemented to correct this problem, such as using features from other modalities  [8] ,  [9] ,  [10] ,  [11] , modeling contextual information  [12]  or regularizing deep neural network (DNNs) under a multitask learning (MTL) framework  [13] ,  [14] . It is important to explore why predicting valence from speech is so difficult, and use these findings and insights to improve SER systems.\n\nIn our previous work, we studied the prediction of valence from speech  [15] , focusing the analysis on the role of regularization in DNNs. In particular, we explored the role of dropout as a form of regularization and analyzed its effect on the prediction of valence. Our analysis showed that a higher dropout rate (i.e., higher regularization) led to improvements in valence predictions. The optimal dropout rate for valence was higher than the optimal dropout rates for arousal and dominance across different configurations of the DNNs. A hypothesis from this study was that a heavily regularized network learns features that are more consistent across speakers, placing less emphasis on speaker-dependent emotional cues. We also conducted controlled speaker-dependent experiments to evaluate this hypothesis, where data from the same speakers were included in the train and test partitions. For valence, we observed relative gains in concordance correlation coefficient (CCC) up to 30% between speaker-dependent and speakerindependent experiments. The corresponding relative improvements observed for arousal and dominance were less than 4%. These results showed that valence emotional cues include more speakerdependent traits, explaining why heavily regularizing a DNN helps to learn more general emotional cues across speakers  [15] . Building on these results, we propose an unsupervised personalization approach that is extremely useful in the prediction of valence.\n\nThis paper explores the speaker-dependent nature of emotional cues in the externalization of valence. We hypothesize that a regression model trained to detect valence from speech can be adapted to a target speaker. The goal is to leverage the information from the emotional cues of speakers in the train set to fine-tune a regression model already trained to perform well on the prediction of valence. Our approach identifies speakers in the train set that are closer in the acoustic space to the speakers in the test set. Data from these selected speakers are used to create an adaptation set to personalize the SER models toward the test speakers. We achieve the adaptation by using three alternative methods: unique speaker, oversampling and weighting approaches. The unique speaker approach randomly selects samples from the data obtained from the selected speakers in the train set without replacement, regardless of how many times these speakers are selected (i.e., a speaker in the train set may be found to be closer to more than one speaker in the test set). The oversampling approach draws data from the selected speakers as a function of the number of times that a given speaker is selected. For instance, if a speaker in the train set is found to be closer to two speakers in the test set, the selected sentences from that training speaker is counted twice. This approach repeats the data from this speaker during the adaptation phase, so the model sees the same speech samples in multiple batches in a single epoch. The weighting approach uses weights, where samples from the selected speakers in the train set are weighted more. This approach adds weights on the cost function during the training process, building the models from scratch. We demonstrate the idea of personalization under two scenarios: 1) separate SER models, where each of them is personalized to a single test speaker (i.e., individual adaptation models), and 2) a single SER model personalized to a pool of 50 target speakers (i.e., global adaptation model). We evaluate the approaches by monitoring the loss function on either a separate development set or the adaptation set.\n\nUsing the proposed model adaptation strategies leads to relative improvements in CCC as high as 13.52% in the prediction of valence. While the adaptation experiments prove to be very effective for valence, the improvements achieved for arousal and dominance are less than 1.9% (on the MSP-Podcast corpus). This result indicates the need for a personalization method to improve the prediction of valence, highlighting the benefits of our proposed approach. The contributions of our study are:\n\n• We leverage the finding that the externalization of valence in acoustic features is more speakerdependent than arousal and dominance, raising awareness on the need for special considerations in its detection.\n\n• We successfully personalize a SER system using unsupervised adaptation strategies by exploiting the speaker-dependent traits.\n\n• We propose three alternative adaptation strategies to personalize a SER system, obtaining important relative performance improvements in the prediction of valence.\n\nOne of the key strengths of this study is that we find similar speaker in the emotional feature space alone. By exploiting similarities in the emotional feature space, we have suppressed the speaker trait or text dependencies. Our approach provides a much more powerful way of comparing emotional similarities than traditional methods used for speaker identification. Likewise, our personalization approach avoids or minimizes \"concept drift.\" SER is a challenging problem, where the prediction models can become more volatile with the addition of more data over time. If the distribution of the newly acquired data starts to diverge or tend to fill up the sparse regions of the old data's distribution, the prediction results may see a drop in performance. Therefore, models built for analyzing such data quickly become obsolete. This phenomenon is referred to as concept drift. With our personalization study, we can minimize the impact of concept drift by developing personalized SER models that are tailored to target speakers. We can periodically re-fit or update the models to target speakers or even weight the data based on their historical significance to develop better personalized models.\n\nThe paper is organized as follows. Section 2 discusses relevant studies on the prediction of valence from speech. It also describes the adaptation and personalization approach proposed for improving SER systems. Section 3 presents the database used in this study. Section 4 describes the analysis on the role of regularization in the prediction of valence from speech, summarizing the study presented in our preliminary work  [15] . Section 5 presents the proposed formulation to personalize a SER system, building on the insights learned from the analysis in Section 4. Section 6 presents the results obtained by using our proposed approaches to personalize a SER system. We primarily present the results on the MSP-Podcast corpus, but we evaluate the generalization of our proposed approach with two other databases. The paper concludes with Section 7, which summarizes our key findings, providing future directions for this study.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Improving The Prediction Of Valence",
      "text": "While valence is a key dimension to understand complex human behaviors, its predictions using speech features are often lower than the predictions of other emotional attributes such as arousal or dominance  [16] . Therefore, several speech studies have focused on understanding and improving valence prediction. Busso and Rahman  [17]  studied acoustic properties of emotional cues that describe valence. They built separate support vector regression (SVR) models trained with different groups of acoustic features: energy, fundamental frequency, voice quality, spectral, Mel-frequency cepstral coefficients (MFCCs) and RASTA features. They also built binary classifiers to distinguish between two groups of sentences characterized by similar arousal but different valence. The study showed that spectral and fundamental frequency features are the most discriminative for valence. Koolagudi and Rao  [18]  claimed that MFCCs were effective to classify emotion along the valence dimension (i.e., spectral features). Cook et al.  [19] ,  [20]  explored the structure of the fundamental frequency (F0), extracting dominant pitches in the detection of valence from speech.\n\nDespande et al.  [21]  proposed a reduced feature set consisting of the autocorrelation of pitch contour, root mean square (RMS) energy and a 10-dimensional time domain difference (TDD) vector. The TDD vector corresponds to successive differences in the speech signal. The feature set collectively led to better results than MFCCs or OpenSmile features  [22] . Tursunov et al.  [23]  used acoustic descriptors associated with timbre perception to classify discrete emotions, and emotions along the valence dimension. Tahon et al.  [24]  showed that voice quality features were also useful in the detection of valence.\n\nOther studies have explored modeling strategies to improve the prediction of valence. Lee et al.  [25]  used dynamic Bayesian networks to capture time dependencies and mutual influence of interlocutors during dyadic interactions. Contextual information was found to be particularly useful in the prediction of valence, leading to relative improvements higher than the one observed for arousal. Another alternative approach to improve valence was by regularizing a DNN. For example, Parthasarathy and Busso  [13]  showed that jointly predicting valence, arousal and dominance under a multitask learning (MTL) framework helps to improve its prediction. The MTL framework acts as regularization in DNNs. Other approaches using MTL have shown similar findings. Since the performance of lexical models often outperforms acoustic models in predicting valence  [9] , Lakomkin et al.  [26]  suggested the use of the output of an automatic speech recognition (ASR) as the input of a character-based DNN.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Model Adaptation In Ser Tasks",
      "text": "Unlike other speech tasks such as automatic speech recognition (ASR) that rely on abundant data, databases used in SER are often small. Therefore, many researchers have explored the use of model adaptation techniques to generalize the models beyond the training conditions. Most of the adaptation techniques aim to attenuate sources of variability including channel, language and speaker mismatches. Early studies demonstrated the effectiveness of these techniques with algorithms based on support vector machine (SVM)  [27] . Abdelwahab and Busso  [28]  demonstrated the importance of data selection strategy for domain adaptation. They illustrated that, incrementally adapting emotion classification models using active learning to select samples from the target domain can improve their performance. They used a conservative approach where only the correctly classified samples were used to adapt the model, leaving out the incorrect ones in order to avoid large changes in the hyperplane between the classes.\n\nRecent efforts in model adaptation have mainly focused on DNNs, where important advances have been made in the area of transferring knowledge between domains  [29] . DNNs with their deep architectures can learn useful representations by compactly representing functions. Deng et al.  [30]  used sparse autoencoders to learn feature representations in the source domain that are more consistent with the target domain. This goal was achieved by simultaneously minimizing the reconstruction error in both domains. Deng et al.  [31]  proposed the use of unlabeled data under a deep autoencoder framework to reduce the mismatch between train and test conditions. They also simultaneously learned common traits from both labeled and unlabeled data.\n\nInstead of the traditional method of pre-training and fine-tuning for model adaptation, Gideon et al.  [32]  used progressive networks to enhance a SER system. They trained the model on new tasks by freezing the layers related to previously learned tasks and used their intermediate representations as inputs to new parallel layers. This study also used paralinguistic information from gender and speaker identity to achieve improvements. Similarly, other variants of adaptation techniques use kernel mean matching (KMM)  [33] , Nonnegative matrix factorization  [34] , domain adaptive least-squares regression  [35] , and PCANet  [36] . These methods lead to improvements on emotion recognition tasks, by using hybrid frameworks involving unsupervised followed by supervised learning. Our proposed approach is different from these studies, since we aim to explicitly exploit similarities between speakers in the train and test sets, as measured in the feature space. This approach leads to powerful adaptation methods that are particularly useful to predict valence.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Speech Emotion Personalization",
      "text": "This study focuses on adapting or personalizing a SER system to a target set of speakers. Busso and Rahman  [37]  demonstrated the idea of personalization using an unsupervised feature normalization scheme. They used the iterative feature normalization (IFN) method  [38]  to reduce speaker variability, while preserving the discriminative information of the features across emotional classes. The IFN algorithm has two steps. First, it detects neutral sentences which are used to estimate the normalization parameters. Then, the data is normalized with these parameters. Since the detection of neutral speech is not perfect, this process is iteratively repeated leading to important improvements. Busso and Rahman  [37]  implemented the IFN scheme as a front end of a SER system designed to recognize emotion from a target speaker, observing huge improvements in accuracy. Our study exploits the speakerdependencies in the externalization of valence to personalize a SER system towards target speakers.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Resources",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Emotional Corpora",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "The Msp-Podcast Corpus",
      "text": "The study relies on the MSP-Podcast corpus  [39] , which provides a diverse collection of spontaneous speech segments that are rich in emotional content. The speech segments are obtained from podcasts taken from various audio-sharing websites, using the retrieval-based approach proposed by Mariooryad et al.  [40] . The content of the podcasts is diverse, including discussions on sport, politics, entertainment, games, social problems and healthcare. The podcasts are segmented into speaking turns between 2.75s and 11s duration. These segments are automatically processed to discard segments with music, overlapped speech, and noisy recordings. Since most of the segments are expected to be neutral, we retrieve candidate segments to be included in the database by leveraging a diversified set of SER algorithms to detect emotions. The selected speech segments are annotated on Amazon Mechanical Turk (AMT) using a crowdsourcing protocol similar to the one introduced by Burmania et al.  [41] . This crowdsourcing protocol stops the annotators in real-time if their performance is evaluated as poor. The raters annotate each speaking turn for its arousal, valence and dominance content using self-assessment manikins (SAMs) on a seven Likert-type scale. The ground truth labels for each speaking turn is the average across the scores provided by the annotators. Although we do not use categorical annotations in this study, the corpus also includes annotations of primary and secondary emotions. The primary emotion corresponds to the dominant emotional class. The secondary emotion corresponds to all the emotional classes that can be perceived in the speech segments.\n\nThe collection of the MSP-Podcast corpus is an ongoing effort. This study uses version 1.6 of the MSP-Podcast corpus, which consists of 50,362 speech segments (83h29m) annotated with emotional classes. From this set, 42,567 segments have been manually assigned to 1,078 speakers. The speaker identity for the rest of the corpus has not been assigned yet. Figure  1  illustrates the partition of the dataset used in this study. The test set has 10,124 speech segments from 50 speakers, and the development set has 5,958 speech segments from 40 speakers. Each speaker in the test and development sets has a minimum of five minutes of data. The rest of the corpus is included in the train set, which consists of a total of 34,280 speech segments. The data partition aims to create speaker-independent partitions between sets. Lotfian and Busso  [39]  provide more details on this corpus.\n\nAs shown in Figure  1 , we further split the test set into two partitions for this study: test-A and test-B sets. The test-A set includes 200s of recording for each of the 50 speakers in the test set. The test-B set includes the rest of the recordings in the test set. Each test speaker has at least 300s (5 mins) of data. After removing 200s from each speaker to form the test-A set, the test-B set is left with at least 100s of data for each speaker. The average duration per speaker in the test-B set is 1005.96s.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "The Iemocap And Msp-Improv Corpora",
      "text": "Besides the MSP-Podcast corpus, we use two other databases for our experimental evaluations. The first database is the USC-IEMOCAP corpus  [42] , which is an audiovisual corpus and contains dyadic interactions from 10 actors in improvised scenarios. This study only uses the audio. The database contains 10,039 speaking turns, which are annotated with emotional labels for arousal, valence and dominance by at least two raters using a five-Likert scale. We also use the MSP-IMPROV corpus  [43] , which is a multimodal emotional database that contains interactions between pairs of actors engaged in improvised scenarios. In addition to the conversations during improvised scenarios, the dataset also contains the interactions between the actors during the breaks, resulting in more naturalistic data. The corpus uses a novel elicitation scheme, where two actors in an improvised scenario leads one of them to utter target sentences. For each of the target sentences, four emotional scenarios were created to contextualize the sentence to elicit happy, angry, sad and neutral reactions, respectively. This corpus consists of 8,438 turns of emotional sentences recorded from 12 actors (over 9 hours). The sessions were manually segmented into speaking turns, which were annotated with emotional labels using perceptual evaluations. Each turn was annotated for arousal, valence and dominance by five or more raters using a five-Likert scale. In both",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Msp-Podcast Corpus Test Set",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Test-A",
      "text": "Test-B databases, the consensus emotional attribute label assigned to each utterance is the average across the scores provided by the annotators, which is linearly mapped between -3 and 3.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Acoustic Features",
      "text": "This study uses the feature set proposed for the computational paralinguistics challenge (ComParE) in Interspeech 2013  [44] . The features are extracted by estimating several low-level descriptors (LLDs) such as energy, fundamental frequency and MFCCs. For each speech segment, statistics such as mean, standard deviation, range and regression coefficients are estimated for each LLD, creating high-level descriptors (HLDs). With this approach, the feature vector is fixed regardless of the duration of the sentence. The ComParE set creates a 6,373 dimensional feature vector for each sentence.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Role Of Regularization",
      "text": "Our proposed personalization method for valence builds upon the findings reported in our preliminary study  [15] . This section summarizes the main findings on the role of dropout rate as a form of regularization in DNNs and its impact on SER. The study in Sridhar et al.  [15]  was conducted on an early version of this corpus. We update the analysis with the release of the corpus used for this study (release 1.6 of the MSP-Podcast corpus).",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Optimal Dropout Rate For Best Performance",
      "text": "Our previous study focused on the role of dropout as a form of regularization in improving the prediction of valence  [15] . When dropout is used in DNNs, random portions of the network are shutdown at every iteration, training a smaller network on each epoch. This approach helps in learning feature weights in random conjunction of neurons, preventing developing co-dependencies with neighboring nodes. This regularization approach leads to better generalization. We explore the role of regularization in the prediction of valence by changing the dropout rate p. The goal of this analysis is to understand the optimal value p that leads to the best performance for different network configurations (i.e., different number of layers, different number of nodes per layer). We train the models for 1,000 epochs, with an early stopping criterion based on the development loss. The loss function is based on CCC, which has led to better performance than mean squared error (MSE)  [16] . We train separate regression models by changing the dropout rate p ∈ {0.0, 0.1, • • • , 0.9}, recording the optimal dropout rate leading to the best performance on the development set. We evaluate two networks with three and seven layers, implemented with 256, 512 and 1,024 nodes per layer. Figure  2  illustrates the results, showing the optimal dropout rate observed in the development set. The optimal dropout rates that give the best performance are higher for valence than arousal and dominance. While the optimal dropout rate decreases as we increase the number of layers or number of nodes per layer in a DNN, Figure  2  shows that the gap between the optimal dropout rates for valence and arousal/dominance stays consistent. Interestingly, the optimal dropout rates for arousal and dominance are exactly the same across different DNN configurations, whereas it is different for valence. The results show that the need for higher regularization for valence is consistent across variations in the architectures of the DNNs.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Speaker-Dependent Versus Speaker-Independent Models",
      "text": "Section 4.1 demonstrated that a DNN needs to be heavily regularized to give good predictions for valence. We hypothesize that this finding can be explained by the speaker-dependent nature of speech cues in the externalization of valence (i.e., we use different acoustic cues to express valence).\n\nA DNN with higher regularization, learns more generic trends present across speakers, leading to better generalization. To validate our hypothesis, we conduct a controlled emotion detection evaluation, where we train DNNs with either speakerdependent or speaker-independent partitions. SER should be performed with speaker-independent partitions, where the data in the train and test partitions are from disjoint set of speakers. A model trained with data from speakers in the test set has an unfair advantage over a system evaluated with data from new speakers, resulting in overestimated performance. Our goal is to quantify the benefits of using speaker-dependent partitions. We build DNNs with four layers, implemented with 256, 512 or 1,024 nodes. The speakerdependent model is built by adding the test-A set in the train set (Fig.  1 ). This approach creates a train set with partial knowledge about the speakers in the test set. In contrast, the speaker-independent model uses only the train set. To have a fair comparison, both models are evaluated on the test-B set with speech samples that are not used to either train or optimize the parameters of the systems. Table  1  shows the CCC values of the models for speaker-independent and speaker-dependent conditions. The last column calculates the relative improvements achieved under the speakerdependent condition. We observe a performance gain up to 19.03% for valence. The performances for the arousal and dominance models also increase, but the relative improvements are less than 3%. The fact that the performances increase using speaker dependent sets is expected. What is unexpected is that the relative gain is significantly higher for valence than for arousal and dominance. These results clearly show that learning emotional traits from the target speakers in the test set has clear benefits for valence, validating our hypothesis that the externalization of valence in speech has speakerdependent traits.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Proposed Personalization Method",
      "text": "",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Motivation",
      "text": "The findings in Section 4 suggest that leveraging data from speakers in the train set that are closer to our target speakers in the test set should benefit our SER models. This is the premise of our proposed approach. We aim to improve the prediction of valence, bridging the gap in performance between speaker-dependent and speaker-independent conditions reported in Table  1 . Data sampled from the selected speakers' recordings are used to create an adaptation set, as illustrated in Figure  3(a) . Once the closest speakers are identified, we can either adapt the models or assign more weights to samples from this adapatation set. This section describes our unsupervised personalization approach to improve the prediction of valence. Unlike the speakerdependent settings used in Section 4.1 (and Sec. 6.7), the analysis and experiments in this study operate with speaker-independent partitions for train, development and test sets. The assumption in our formulation is that we have the speaker identity associated with each sentence in the test set.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Estimation Of Similarity Between Speakers",
      "text": "A key step in our approach is to identify speakers in the train set that are closer to the speakers in the test set. Ideally, we would like to identify speakers who externalize valence cues in speech in a similar way. This aim is difficult with no clear solutions. We simplify our formulation by searching for similarities between speakers in the space of emotional speech features. By exploiting similarities on the emotional feature space, we expect to focus more on emotional patterns than on speaker traits, which would be the focus of speaker embeddings created with methods such as i-vector  [45]  or the x-vector  [46] . Our approach relies on principal component analysis (PCA) to reduce the dimension of the space, followed by fitting a Gaussian mixture model (GMM) to the resulting reduced feature space.\n\nWe aim to quantify the similarity in the feature space between the speaker i in the train set, and the speaker j in the test set, d(i, j). The first step is to reduce the feature space, since we consider a high dimensional feature vector (6,373D -Sec. 3.2). Reducing the feature space creates a more compact feature representation, where the similarity between speakers can be more efficiently computed. We implement this step with PCA, which is a popular unsupervised dimensionality reduction technique. First, we estimate the zero-mean vector y s = f s -f , where f s is the feature vector of sentence s, and f is the mean feature vector. Then, we concatenate these M vectors, creating matrix F (Eq. 1). From this matrix, we estimate the sample covariance matrix Q using Equation  2 . Then, we compute the eigenvectors of Q, selecting the ones with the highest eigenvalues, which are considered as the principal components (PCs).\n\nThe PCA-based feature reduction is implemented for each speaker in the test set, creating speaker-dependent transformations. We use the 10 most important dimensions, which explain in average 57.9% of the variance in the feature space. The speech sentences from speaker i (train set) are projected into the PCA space associated with speaker j (test set). The speech sentences from speaker j are also projected in that space. After the PCA projections, we fit two separate GMMs on the reduced feature space, one for the sentences of speakers i (p train i ), and another for the sentences of speaker j (q test j ). The GMMs have 10 mixtures, matching the reduced dimension of the PCA projections. Finally, we estimate the similarity between the GMMs using the Kulback Liebler Divergence (KLD).\n\nw n(j) N (x j , µ n(j) , Σ n(j) ) (4)\n\nFor a given speaker j in the test set, we estimate d(i, j) for all the speakers in the train set, sorting their scores in increasing order. The closest speakers in the train set are the top speakers in this ranked list. This approach is repeated for each of the 50 speakers in the test set (i.e., we have 50 different PCA projections). While this step can be implemented using all the data from the test set, we use the test-A set to have the same amount of data for each speaker (i.e., 200s -Fig.  1 ). Figure  3(a   speaker. Notice that the adaptation set is a subset of the train set, for which we have labels.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Personalization Approach",
      "text": "After selecting the speakers in the train set that are closest to each of the test speakers, our next step is to leverage data from these train set speakers to either personalize or adapt the emotion prediction models for valence. We propose and evaluate three alternative methods, referred to as unique speaker, oversampling, and weighting approaches. The first two approaches rely on adapting a model. We built a regression model using the train set to predict valence. The weights of the pre-trained models are frozen with the exception of the last layer, which are fine-tuned using the adaptation data. The last approach requires training the network from scratch. Unique speaker approach: Each speaker in the test set generates a list with its N closest speakers in the train set. Since some speakers in the train set may be close to more than one speaker, the total number of selected speakers after combining the lists from the 50 speakers in the test set is less or equal to N × 50. The unique speaker approach considers all the data from these speakers in the train set. We create the adaptation set by sampling from the data from these speakers without replacement. Therefore, each speech segment can be considered only once in the adaptation set. Figure  3 (b) illustrates the process for the case when we have only 2 speakers in the test set. In the example, speakers 2, 7 and 120 in the train set are found to be close to both test speakers, hence, we consider them only once when forming the adaptation set. We implement a balance sampling criterion that aims to select approximately the same amount of data for each speakers. For example, for an adaptation set of 200s, if we have 7 unique speakers selected as the closest speakers from the test speakers, as in the example, we would randomly select approximately 28.6s for each of these speakers. We adopt this approach in an attempt to diversify and balance the speech samples selected from all the speakers in the unique speakers set. This approach uses the pre-trained models trained with all the train set, personalizing the models with the adaptation set. Oversampling approach: A speaker in the train set may be in the list of the closest speakers for more than one speaker in the test set. The oversampling approach assumes that these samples are more relevant during the adaptation process. If a speaker is selected C times (i.e., the speaker is one of the closest speakers for C speakers in the test set), the oversampling method will create C copies of his/her sentences before randomly drawing the samples. This process is illustrated in Figure  3(b)  were speakers 2, 7 and 120 in the train set are copied twice. Therefore, more samples from these speakers will appear in the adaptation set. We form the adaptation set with a balance approach, choosing approximately the same amount of data from the selected speakers. In the example from Figure  3 (b), we would select 20 second for each of the 10 sets for an adaptation set of 200s. Sentences can even be repeated on the adaptation set. This approach also fine-tunes the pre-trained model using speech samples from the oversampled adaptation set. Weighting approach: The third approach to personalize a model is by increasing the weights in the loss function during the training process for speech samples in the adaptation set (i.e., same set used in the unique speaker approach). Unlike the previous two approaches, which adapt a pre-trained system, this approach trains the regression model from scratch. As described in Equation  6 , we use L = (1 -CCC) as the loss function to train our models. For the weighting approach, this cost is assigned to a sample in the train set, but not on the adaptation set. For samples in the adaptation set, we multiply the cost L by a factor λ > 1. Therefore, an error of a sample from the adaptation set is λ times more costly than an error made on other samples from the train set not included in the adaptation set. We experiment with weighting ratios of 1:2, 1:3, 1:4 and 1:5, where higher weights are assigned to samples in the adaptation set. This approach uses all the train set, increasing the importance of correctly predicting samples in the adaptation set.\n\nThe proposed unsupervised adaptation schemes can be jointly applied to all the speakers in the test set, creating a single model. We refer to this approach as global adaptation (GA) model. Alternatively, the approaches can be individually implemented for each speaker, creating as many models as speakers in the test set. This implementation only works for the unique speaker and weighting approaches. The oversampling approach does not apply in this case, since each speech segment in the adaptation set is drawn only once (i.e., we consider one test speaker at a time). We refer to this approach as the individual adaptation (IA) model. We evaluate both implementations in Section 6 using adaptation sets of different sizes.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Experimental Results",
      "text": "The prediction of valence is formulated as a regression problem implemented with DNNs with four dense layers and 512 nodes per layer. This setting achieved the best performance for valence on Table  1 . We use rectified linear unit (ReLU) at the hidden layers and linear activations for the output layer. The DNNs are trained with batch normalization for the hidden layers. We use a dropout rate of p = 0.7 at the hidden layers. The selection of this rate follows the findings in Section 4.1, which demonstrates that a higher value of dropout is important for improving the detection of valence. We pre-train the models for 200 epochs with an early stopping criterion based on the performance on the development set. The best model is used to evaluate the results. The DNNs are trained with stochastic gradient descent (SGD) with momentum of 0.9, and a learning rate of r = 0.001. For the unique speaker and oversampling approaches, the learning rate is reduced to r adap = 0.0001 while adapting the regression model. We adapt the models with these approaches for 100 extra epochs with an early stopping criterion based on the performance on the development set. For the weighting approach, we train the models from scratch for 200 epochs with early stopping criterion, maximizing the performance on the development set. The loss function (Eq. 6) relies on the concordance correlation coefficient (CCC), which is defined in Equation  7 .\n\nThe parameters µ x and µ y , and σ x and σ y are the means and standard deviations of the true labels (x) and the predicted labels (y), and ρ is the Pearson's correlation coefficient between them. CCC takes into account not only the correlation between the true emotional labels and their estimates, but also the difference in their means. This metric takes care of the bias-variance tradeoff when comparing the true and predicted labels. CCC is also the evaluation metric in all our experimental evaluation.\n\nThe input to the DNNs is the 6,373D acoustic feature vector (Sec. 3.2). The features are normalized to have zero mean and unit standard deviation. This normalization is done using the mean and standard deviation values estimated over the training samples. After this normalization, we expect the features to be within a reasonable range. We remove outliers by clipping the features with values that deviate more than three standard deviations from their means (i.e.,\n\nThe output of the DNNs is the prediction score for the emotional attribute.\n\nWe use the speaker-independent and speakerdependent models described in Section 4.2 as baselines, where the results are listed in Table  1 . The speaker-independent model does not rely on any adaptation scheme to personalize the models to perform better on the test set. As described in Section 4.2, the speaker-dependent model is built by adding the test-A set to the train set, using partial information from the speakers. While this setting is not representative of the performance expected for the regression model when evaluated on speech from unknown speakers, it provides an upper bound performance to contextualize the improvements observed with our proposed personalization methods. For analysis purposes, we report the performance of the speaker-dependent models obtained with the addition of 50s, 100s, 150s, and 200s per speaker in the test set. These extra samples are obtained from the test-A set. We consistently evaluate all the models using the test-B set. This experimental setting creates fair comparisons, since these samples are not used to train any of the models.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Global Adaptation Model",
      "text": "We evaluate the performance of the system with the global adaptation model, where a single regression model is built. The three adaptation schemes are implemented by considering the 50 speakers in the test set. The adaptation set is obtained by identifying the closest speakers in the train set to each speaker in the test set (Sec. 5.2). We implement this approach by identifying the five closest speakers in the train set (N = 5). Section 6.3 shows the results with different number of speakers. We incrementally add more samples by randomly selecting 50s, 100s, 150s, 200s, and 300s from the selected speakers associated with a given speaker in the test set (Sec. 5.3). This process is repeated for each of the speakers in the test set to observe the performance trend as a function of the size of the adaptation data. First, we evaluate the unique speaker and oversampling approaches, which rely on model adaptation. Figure  4  shows the results. The two solid horizontal lines are the speaker-dependent (green) and speaker-independent (red) baselines. The green line (triangle) corresponds to the speaker-dependent model as we increase the amount of data from the test-A set. The performance for the unique speaker approach is shown in blue (square). We clearly observe an improvement over the speakerindependent baseline, which demonstrate that the adaptation scheme is effective even with a very small adaptation set (e.g., 50s). The pink (asterisk) line in Figure  4  shows the performance of the oversampling approach, which leads to better performance than the unique speaker approach. Both approaches use the same amount of adaptation data, but rely on different criteria to select the adaptation samples. Adding samples in multiple minibatches according to the oversampling strategy is beneficial to improve the prediction of valence. For both models, we observe consistent improvement in CCC as more data is added into the adaptation set from 50s to 200s. After this point, the performance seems to saturate, observing fluctuations. Interestingly, adaptation with 200s of data using the oversampling approach leads to better performance than the speaker-dependent baseline implemented with 50s and 100s of data from the test-A set. Second, we evaluate the performance of the weighting method, which trains the models from scratch, weighting more the samples in the adaptation set (Sec. 5.3). We evaluate the amount of data included in this selected set, including 50s, 100s, 150s, 200s per speaker in the test set. We also consider using all the data from the selected speakers. Only samples in this set are weighed more, implementing this approach with different ratios (1:2, 1:3, 1:4 and 1:5). Figure  5  shows the results. For weighting ratios 1:2 and 1:3, the performance gradually increases by adding more data in the selected set, peaking at 200s per speaker. However, the opposite trend is observed when the weighting ratios are either 1:4 or 1:5. Increasing the weights of speech samples in the adaptation set diminishes the information provided in the rest of the train data, leading to worse performance. The best performance is obtained with a weighting ratio of 1:3, when the selected set includes 200s per speaker. Figures  4  and 5  show that this setting achieves similar performance than the best setting for the oversampling approach.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Individual Adaptation Model",
      "text": "This section presents the results of our approach implemented using the individual adaptation model. This approach builds one model for each of the speakers in the test set, creating the adaptation set with the samples from the speakers in the train set that are closer to this speaker (i.e., 50 separate models). For each model, we attempt to select equal duration of speech samples from each of the closest train speakers in the adaptation set to balance the amount of data used from each speaker. After adapting the models, the results are reported by concatenating the predicted vectors for each speaker in the test set. We estimate the CCC values for the entire test-B set. The approach is implemented with the five closest speakers to each speaker in the test set. The performance of the approaches are reported by increasing the adaptation set. As explained in Section 5.3, we only evaluate the unique speaker and weighting approaches, since the oversampling approach cannot be implemented with a single speaker in the test set. Figure  6  shows the CCC scores obtained for different sizes of the adaptation set. The results show improvements over the speaker-independent baseline performance. The performance gains are consistently higher when all the data from the closest speakers are used. The weighting approach also leads to better performance than the unique speaker approach. However, the results are worse than the CCC values of approaches implemented with the global adaptation model (Figs.  4  and 5 ). The decrease in performance in the IA model can be associated with the adaptation procedure. In the IA models, we adapt separate models for each target speaker. This procedure involves adapting 50 different models where their parameters and hyperplanes change based on a small adaptation set. This approach may be too aggressive, resulting in lower performance than adapting a single model, considering all the target speakers. We have seen similar observations in the area of active learning in SER tasks, where a more conservative adaptation strategy led to better results  [28] . In the GA models, we are adapting a single model, where the shape and direction of the change of the hyperplane are smoother than in the case of the IA models achieving a better and stable performance.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Number Of Closest Speakers",
      "text": "This section evaluates the number of closest speakers (N ) from the train set selected for each speaker in the test set. If this number is too small, the adaptation set will not have enough variability. If this number is too high, we will select speakers that are not very close to the target speakers. We implement the weighting approach with the ratio 1:3. Figure  7  shows the results on the test-B set for the models implemented with the global and individual adaptation models using the proposed adaptation schemes (N ∈ {3, 5, 10}). The results clearly show that N = 5 is a good balance, obtaining higher performance across conditions. We set N = 5 for the rest of the experimental evaluation.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Minimizing Loss On Adaptation Set",
      "text": "The results of the model adaptation presented in Sections 6.  the test set. Given the assumption that the selected speakers in the train set are similar to the test set, we can optimize the system by minimizing the loss function on the adaptation set when fine-tuning the model (i.e., samples from the selected speakers used for adaptation). Since we want to personalize the system, it is theoretically correct to maximize the performance of the model on data that is found to be closer to the target speakers. This section evaluates this idea. We record the best performance of the model by using an early stopping criterion on the adaptation loss. The only special case is the weighting approach which trains the models from scratch. Since monitoring the loss exclusively on the adaptation set will ignore other samples in the train set, we decide to monitor the loss on the full train set. The differences in the weights increase the emphasis of samples from the adaptation set, achieving essentially the same goal. We use the adaptation set using the 200s condition. The weighting approach is implemented with the 1:3 ratio, which gave the     and 6 ).\n\nFigure  8  shows the performance improvements achieved by minimizing the loss function on the adaptation set. The darker bars indicate the results obtained while monitoring the training loss, and the lighter bars indicate the results obtained while monitoring the development loss. We include the relative improvements over the speaker-independent baseline with numbers on top of the bars. The relative improvements are constantly higher when maximizing the performance on the train set, tailoring even more the models to the test speakers. We conducted a one-tailed student t-test over ten trials to assert if the differences in performance between minimizing the loss function on the development and train sets are statistically significant. We assert significance when p-values<0.05. The statistical test indicates that the differences are statistically significant for all the adaptation approaches implemented with either the global or individual adaptation models. This approach leads to performances that are closer to the speaker-dependent baseline.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Performance On Other Emotional Attributes",
      "text": "The premise of this study is that valence is externalized in speech with more speaker-dependent traits than arousal and dominance. Therefore, personalization approaches to bring the models closer to the test speakers should have a higher impact on valence. This section implements the proposed adaptation schemes on arousal and dominance, comparing the relative improvements over the speakerindependent baseline with the results for valence.\n\nTable  2  reports the performance and relative improvements over the speaker-independent baseline for valence, arousal, and dominance when using the proposed methods. The relative improvements for arousal and dominance are less than 1.9%, mirroring the results observed in Table  1  that shows relative improvements less than 3% for arousal and dominance when labeled data from the target speakers is available (i.e., speaker-dependent condition). Therefore, it is not surprising that the method does not lead to big improvements for arousal and dominance where there is little room for improvements. We argue that the approach is successful even for arousal or dominance, since the relative improvements for these emotional attributes are similar to the values reported in Table  1  under speaker dependent conditions. In contrast, the relative improvements for valence are as high as 13.52%. These results validate our hypothesis that exploiting speaker-dependent characteristics between train and test speakers helps to personalize a SER system in the prediction of valence.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Comparison With Other Baselines",
      "text": "We compare the results from our proposed personalization approach with other state-of-the art approaches. We consider multi-task learning (MTL)  [13] , and ladder network  [14]  for SER. These are some of the most successful approaches used in SER. The MTL approach jointly predicts arousal, valence and dominance, where the loss function is a weighted sum of the individual attribute losses. We used L = (1 -CCC) as the loss for each attribute (L aro , L val , L dom ). Equation  8 shows the overall loss function, where (α, β) ∈ [0, 1] and α + β ≤ 1.\n\nThe hyperparameters α and β are tuned on the development set.\n\nThe ladder network approach follows the implementation presented by Parthasarathy and Busso  [14] . This method uses the reconstruction of feature representations at various layers in a DNN as auxiliary tasks. In addition, we consider the  speaker-independent baseline discussed in previous sections, referred here to as single task learning (STL). Table  3  describes the results, which clearly show significant improvements over alternative methods in CCC for valence using our proposed approach, reinforcing our claim about the benefits of personalization in the estimation of valence. Other approaches are effective for arousal and dominance.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Performance On Other Corpora",
      "text": "We validate the effectiveness of the proposed personalization approaches with other emotional databases. We use a K-fold cross-validation strategy to train DNN models using the USC-IEMOCAP and MSP-IMPROV databases. In each fold, we consider 2 speakers as the test speakers and the rest of the speakers as the train speakers. With this crossvalidation approach, all the speakers are at some point considered as the test speakers. The final results are averaged across the K folds. Similar to Figure  1 , we split the test set of both IEMOCAP and MSP-IMPROV databases into two partitions for this study: test-A and test-B sets. The test-A set includes 200s of recording for each of the speakers in the test set. This set is reserved for finding the closest training speakers to the target speakers. The test-B set includes the rest of the recordings in the test set. We implement the global and individual adaptation models with all the different adaptation approaches by minimizing the loss in the development set.\n\nTable  4  shows the CCC results for speakerindependent and speaker-dependent conditions using the USC-IEMOCAP and the MSP-IMPROV corpora. The results validate the findings in Table  1 , showing that the speaker-dependent condition leads to higher relative gains for valence than for arousal or dominance.\n\nTable  5  shows the results obtained with different adaptation schemes on the USC-IEMOCAP and MSP-IMPROV databases. The results are consistent with the findings observed with the MSP-Podcast corpus. We observe that the relative improvements over the speaker-independent baseline are much higher for valence than the relative improvements for arousal and dominance. With the USC-IEMOCAP corpus, we achieve relative gains in performance up to 10.41% for valence whereas less than 1.03% for arousal and dominance. Similarly, with the MSP-IMPROV corpus, we achieve relative gains in performance up to 17.36% for valence whereas less than 4.22% for arousal and dominance. These results show the effectiveness of our proposed approach applied to other emotional corpora, reinforcing our finding about the speakerdependent nature of valence emotional cues.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Conclusions",
      "text": "This paper demonstrated that a valence prediction system can be personalized to target speakers by exploiting speaker-dependent traits. The study proposed to create an adaptation set by identifying speakers in the train set that are closer to the speakers in the test set. Since we evaluate the similarity between speakers by comparing the acoustic feature spaces associated with each speaker without using emotional labels, the adaptation approaches are fully unsupervised. We proposed three methods to create this adaptation set: unique speaker, oversampling and weighting approaches. The adaptation sets from the selected closer speakers are used to personalize the DNNs to the speakers in the test set. The experimental results showed that the global adaptation models achieved better performance than the individual adaptation models. Further improvements are observed when the loss function is minimized by monitoring the loss in the adaptation set. The proposed adaptation schemes lead to relative improvements up to 13.52% over a speaker-independent baseline. We observed significant improvements in performance, even when only a few seconds of adaptation data (belonging to the train set) for each of the speakers in the test set was used for adaptation. However, increasing the amount of adaptation data did not contribute to further improvements of the model. The maximum performance gains were observed for 200s of adaptation data for each of the speakers in the test set. We also demonstrated the effectiveness of the proposed personalization approaches with the USC-IEMOCAP and MSP-IMPROV databases, showing consistent findings.\n\nThere are many interesting and important applications where our personalization models can be very helpful. In healthcare applications where medical data cannot be frequently obtained, a personalized system can keep track of a patient's expressive behaviors and his/her medical record for better and efficient treatment. Another example is on personal assistant systems on mobile devices, which are often used by a limited number of users. Over time, the system can collect enough data from the target users to improve their emotion recognition systems. For cloud-based applications, the training data needs to be stored in the cloud, instead of the edge device. With a simple modification on the approach to obtain the PCA projections (i.e., finding a common PCA space across testing speakers), the training data can be pre-estimated and stored. Therefore, this approach does not require storage or computational resources on the edge devices and can be efficiently implemented during inferences.\n\nThis study demonstrated the importance of exploiting the speaker-dependent traits observed in the externalization of valence from speech, which led to clear improvements. It also showed that we can personalize SER models by just finding speakers in the train set that are similar to the target speakers. The proposed formulation is flexible, requiring only to know the block of data associated with each speaker in the test set. This assumption is reasonable since it is straightforward to group data per speaker in many practical applications (e.g., assigning all the speech collected during a call center session to a single user). As a future work, we will evaluate more sophisticated methods to as-sess the similarity of speakers by considering more than acoustic similarities. Also, we will explore the use of the proposed adaptation schemes in other deep learning frameworks such as autoencoders, generative adversarial networks (GANs) or long shortterm memory (LSTM). Another open question is to investigate adaptation schemes that are effective for arousal and dominance.",
      "page_start": 16,
      "page_end": 16
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: illustrates the partition",
      "page": 5
    },
    {
      "caption": "Figure 1: , we further split the test",
      "page": 5
    },
    {
      "caption": "Figure 1: Partitions of the MSP-Podcast corpus used in this study",
      "page": 5
    },
    {
      "caption": "Figure 2: Optimal dropout rate observed in the development set as",
      "page": 6
    },
    {
      "caption": "Figure 2: illustrates the results, showing the",
      "page": 6
    },
    {
      "caption": "Figure 2: shows that the gap between the optimal dropout",
      "page": 6
    },
    {
      "caption": "Figure 1: ). This approach creates a train",
      "page": 6
    },
    {
      "caption": "Figure 1: ). Figure 3(a)",
      "page": 7
    },
    {
      "caption": "Figure 3: Illustration of proposed personalization approach. We",
      "page": 8
    },
    {
      "caption": "Figure 3: (b) illustrates the",
      "page": 8
    },
    {
      "caption": "Figure 4: Results on the test-B set for the global adaptation model",
      "page": 10
    },
    {
      "caption": "Figure 4: shows the results. The two solid hor-",
      "page": 10
    },
    {
      "caption": "Figure 4: shows the performance of",
      "page": 10
    },
    {
      "caption": "Figure 5: Results on the test-B set for the global adaptation model",
      "page": 10
    },
    {
      "caption": "Figure 5: shows the results.",
      "page": 10
    },
    {
      "caption": "Figure 6: Results on the test-B set for the individual adaptation",
      "page": 11
    },
    {
      "caption": "Figure 6: shows the CCC scores obtained for",
      "page": 11
    },
    {
      "caption": "Figure 7: Evaluation of the optimal number of closer speakers",
      "page": 11
    },
    {
      "caption": "Figure 7: shows the results on the test-B set",
      "page": 11
    },
    {
      "caption": "Figure 8: Improvement in performance achieved by monitoring the",
      "page": 12
    },
    {
      "caption": "Figure 8: shows the performance improvements",
      "page": 12
    },
    {
      "caption": "Figure 1: , we split the test set of both IEMOCAP and",
      "page": 14
    }
  ],
  "tables": [
    {
      "caption": "Table 1: shows the CCC values of the mod-",
      "data": [
        {
          "Valence": "Arousal",
          "256\n512\n1,024": "256\n512\n1,024",
          "0.3076\n0.3083\n0.2997": "0.7153\n0.7164\n0.7104",
          "0.3373\n0.3670\n0.3538": "0.7216\n0.7331\n0.7258",
          "9.65\n19.03\n18.05": "0.88\n2.33\n2.16"
        },
        {
          "Valence": "Dominance",
          "256\n512\n1,024": "256\n512\n1,024",
          "0.3076\n0.3083\n0.2997": "0.6300\n0.6374\n0.6253",
          "0.3373\n0.3670\n0.3538": "0.6379\n0.6565\n0.6352",
          "9.65\n19.03\n18.05": "1.25\n2.99\n1.58"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3 Speakers\n5 Speakers": "10 Speakers"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Speaker Dependent": "13.52%"
        },
        {
          "Speaker Dependent": "11.80%\n10.67%"
        },
        {
          "Speaker Dependent": "9.56%\n7.68%\n6.87%"
        },
        {
          "Speaker Dependent": "Speaker\nIndependent"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Valence": "Arousal",
          "Unique speaker (GA)\nOversampling (GA)\nWeighting (GA)\nUnique speaker (IA)\nWeighting (IA)": "Unique speaker (GA)\nOversampling (GA)\nWeighting (GA)\nUnique speaker (IA)\nWeighting (IA)",
          "0.3295\n6.87\n0.3378\n9.56\n0.3412\n10.67\n0.3332\n8.07\n0.3319\n7.65": "0.7196\n0.44\n0.7209\n0.62\n0.7222\n0.80\n0.7185\n0.29\n0.7202\n0.53",
          "0.3320\n7.68\n0.3447\n11.80\n0.3500\n13.52\n0.3366\n9.17\n0.3385\n9.79": "0.7221\n0.79\n0.7258\n1.31\n0.7296\n1.84\n0.7267\n1.43\n0.7271\n1.49"
        },
        {
          "Valence": "Dominance",
          "Unique speaker (GA)\nOversampling (GA)\nWeighting (GA)\nUnique speaker (IA)\nWeighting (IA)": "Unique speaker (GA)\nOversampling (GA)\nWeighting (GA)\nUnique speaker (IA)\nWeighting (IA)",
          "0.3295\n6.87\n0.3378\n9.56\n0.3412\n10.67\n0.3332\n8.07\n0.3319\n7.65": "0.6410\n0.56\n0.6428\n0.84\n0.6433\n0.92\n0.6399\n0.39\n0.6417\n0.67",
          "0.3320\n7.68\n0.3447\n11.80\n0.3500\n13.52\n0.3366\n9.17\n0.3385\n9.79": "0.6415\n0.64\n0.6430\n0.87\n0.6451\n1.20\n0.6419\n0.70\n0.6422\n0.75"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Speaker Dependent": "8.07%"
        },
        {
          "Speaker Dependent": "Speaker\nIndependent"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table 1: that shows",
      "data": [
        {
          "Valence": "Arousal",
          "IEM\nIMP": "IEM\nIMP",
          "0.4428\n0.3420": "0.6953\n0.5958",
          "0.5072\n0.4164": "0.7255\n0.6218",
          "14.54\n21.75": "4.34\n4.36"
        },
        {
          "Valence": "Dominance",
          "IEM\nIMP": "IEM\nIMP",
          "0.4428\n0.3420": "0.5444\n0.5625",
          "0.5072\n0.4164": "0.5678\n0.4911",
          "14.54\n21.75": "4.29\n6.18"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table 1: that shows",
      "data": [
        {
          "Valence": "Arousal",
          "Unique speaker (GA)\nOversampling (GA)\nWeighting (GA)\nUnique speaker (IA)\nWeighting (IA)": "Unique speaker (GA)\nOversampling (GA)\nWeighting (GA)\nUnique speaker (IA)\nWeighting (IA)",
          "0.4761\n7.52\n0.4790\n8.17\n0.4889\n10.41\n0.4725\n6.70\n0.4759\n7.47": "0.6988\n0.50\n0.7001\n0.69\n0.7002\n0.70\n0.6966\n0.18\n0.7000\n0.68",
          "0.3866\n13.04\n0.4014\n17.36\n0.3915\n15.52\n0.3855\n12.71\n0.3873\n13.24": "0.6057\n1.66\n0.6086\n2.14\n0.6191\n3.91\n0.6087\n2.16\n0.6095\n2.29"
        },
        {
          "Valence": "Dominance",
          "Unique speaker (GA)\nOversampling (GA)\nWeighting (GA)\nUnique speaker (IA)\nWeighting (IA)": "Unique speaker (GA)\nOversampling (GA)\nWeighting (GA)\nUnique speaker (IA)\nWeighting (IA)",
          "0.4761\n7.52\n0.4790\n8.17\n0.4889\n10.41\n0.4725\n6.70\n0.4759\n7.47": "0.5491\n0.86\n0.5500\n1.02\n0.5495\n0.93\n0.5451\n0.12\n0.5496\n0.95",
          "0.3866\n13.04\n0.4014\n17.36\n0.3915\n15.52\n0.3855\n12.71\n0.3873\n13.24": "0.4718\n2.01\n0.4731\n2.29\n0.4820\n4.21\n0.4710\n1.83\n0.4713\n1.90"
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Identifying mood episodes using dialogue features from clinical interviews",
      "authors": [
        "Z Aldeneh",
        "M Jaiswal",
        "M Picheny",
        "M Mcinnis",
        "E Provost"
      ],
      "year": "2019",
      "venue": "Identifying mood episodes using dialogue features from clinical interviews"
    },
    {
      "citation_id": "2",
      "title": "Emotion recognition from natural phone conversations in individuals with and without recent suicidal ideation",
      "authors": [
        "J Gideon",
        "H Schatten",
        "M Mcinnis",
        "E Provost"
      ],
      "year": "2019",
      "venue": "Emotion recognition from natural phone conversations in individuals with and without recent suicidal ideation"
    },
    {
      "citation_id": "3",
      "title": "Behavioral signal processing: Deriving human behavioral informatics from speech and language",
      "authors": [
        "S Narayanan",
        "P Georgiou"
      ],
      "year": "2013",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "4",
      "title": "Behavioral signal processing for understanding (distressed) dyadic interactions: some recent developments",
      "authors": [
        "P Georgiou",
        "M Black",
        "S Narayanan"
      ],
      "year": "2011",
      "venue": "joint ACM workshop on Human gesture and behavior understanding (J-HGBU 2011)"
    },
    {
      "citation_id": "5",
      "title": "Core affect and the psychological construction of emotion",
      "authors": [
        "J Russell"
      ],
      "year": "2003",
      "venue": "Psychological review"
    },
    {
      "citation_id": "6",
      "title": "Emotional valence modulates brain functional abnormalities in depression: evidence from a meta-analysis of fmri studies",
      "authors": [
        "N Groenewold",
        "E Opmeer",
        "P De Jonge",
        "A Aleman",
        "S Costafreda"
      ],
      "year": "2013",
      "venue": "Neuroscience & Biobehavioral Reviews"
    },
    {
      "citation_id": "7",
      "title": "Reduced sensitivity to emotional facial expressions in borderline personality disorder: effects of emotional valence and intensity",
      "authors": [
        "M Hagenhoff",
        "N Franzen",
        "L Gerstner",
        "G Koppe",
        "G Sammer",
        "P Netter",
        "B Gallhofer",
        "S Lis"
      ],
      "year": "2013",
      "venue": "Journal of personality disorders"
    },
    {
      "citation_id": "8",
      "title": "Continuous prediction of spontaneous affect from multiple cues and modalities in valence-arousal space",
      "authors": [
        "M Nicolaou",
        "H Gunes",
        "M Pantic"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "9",
      "title": "Pooling acoustic and lexical features for the prediction of valence",
      "authors": [
        "Z Aldeneh",
        "S Khorram",
        "D Dimitriadis",
        "E Provost"
      ],
      "year": "2017",
      "venue": "ACM International Conference on Multimodal Interaction (ICMI 2017)"
    },
    {
      "citation_id": "10",
      "title": "Exploiting acoustic and lexical properties of phonemes to recognize valence from speech",
      "authors": [
        "B Zhang",
        "S Khorram",
        "E Provost"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "Valence of emotional memories: A study of lexical and acoustic features in older adult affective speech",
      "authors": [
        "E Tournier"
      ],
      "year": "2019",
      "venue": "Valence of emotional memories: A study of lexical and acoustic features in older adult affective speech"
    },
    {
      "citation_id": "12",
      "title": "Exploring cross-modality affective reactions for audiovisual emotion recognition",
      "authors": [
        "S Mariooryad",
        "C Busso"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "Jointly predicting arousal, valence and dominance with multi-task learning",
      "authors": [
        "S Parthasarathy",
        "C Busso"
      ],
      "year": "2017",
      "venue": "Jointly predicting arousal, valence and dominance with multi-task learning"
    },
    {
      "citation_id": "14",
      "title": "Ladder networks for emotion recognition: Using unsupervised auxiliary tasks to improve predictions of emotional attributes",
      "year": "2018",
      "venue": "Interspeech 2018, Hyderabad, India"
    },
    {
      "citation_id": "15",
      "title": "Role of regularization in the prediction of valence from speech",
      "authors": [
        "K Sridhar",
        "S Parthasarathy",
        "C Busso"
      ],
      "year": "2018",
      "venue": "Role of regularization in the prediction of valence from speech"
    },
    {
      "citation_id": "16",
      "title": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis",
        "F Ringeval",
        "R Brueckner",
        "E Marchi",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2016",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "Unveiling the acoustic properties that describe the valence dimension",
      "authors": [
        "C Busso",
        "T Rahman"
      ],
      "year": "2012",
      "venue": "Interspeech 2012"
    },
    {
      "citation_id": "18",
      "title": "Exploring speech features for classifying emotions along valence dimension",
      "authors": [
        "S Koolagudi",
        "K Rao",
        "; Chaudhury",
        "S Mitra",
        "C Murthy"
      ],
      "year": "2009",
      "venue": "International Conference on Pattern Recognition and Machine Intelligence"
    },
    {
      "citation_id": "19",
      "title": "Evaluation of the affective valence of speech using pitch substructure",
      "authors": [
        "N Cook",
        "T Fujisawa",
        "K Takami"
      ],
      "year": "2005",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "20",
      "title": "The use of multi-pitch patterns for evaluating the positive and negative valence of emotional speech",
      "authors": [
        "N Cook",
        "T Fujisawa"
      ],
      "year": "2006",
      "venue": "Speech Prosody (SP 2006)"
    },
    {
      "citation_id": "21",
      "title": "Detecting emotional valence using time-domain analysis of speech signals",
      "authors": [
        "G Deshpande",
        "V Viraraghavan",
        "M Duggirala",
        "S Patel"
      ],
      "year": "2019",
      "venue": "IEEE International Engineering in Medicine and Biology Conference"
    },
    {
      "citation_id": "22",
      "title": "A successive difference feature for detecting emotional valence from speech",
      "authors": [
        "G Deshpande",
        "V Viraraghavan",
        "R Gavas"
      ],
      "year": "2019",
      "venue": "Workshop on Speech, Music and Mind (SMM 2019)"
    },
    {
      "citation_id": "23",
      "title": "Discriminating emotions in the valence dimension from speech using timbre features",
      "authors": [
        "A Tursunov",
        "S Kwon",
        "H Pang"
      ],
      "year": "2019",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "24",
      "title": "Usual voice quality features and glottal features for emotional valence detection",
      "authors": [
        "M Tahon",
        "G Degottex",
        "L Devillers"
      ],
      "year": "2012",
      "venue": "Speech Prosody (SP 2012)"
    },
    {
      "citation_id": "25",
      "title": "Modeling mutual influence of interlocutor emotion states in dyadic spoken interactions",
      "authors": [
        "C.-C Lee",
        "C Busso",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2009",
      "venue": "Modeling mutual influence of interlocutor emotion states in dyadic spoken interactions"
    },
    {
      "citation_id": "26",
      "title": "Incorporating end-to-end speech recognition models for sentiment analysis",
      "authors": [
        "E Lakomkin",
        "M Zamani",
        "C Weber",
        "S Magg",
        "S Wermter"
      ],
      "year": "2019",
      "venue": "International Conference on Robotics and Automation (ICRA 2019)"
    },
    {
      "citation_id": "27",
      "title": "Supervised domain adaptation for emotion recognition from speech",
      "authors": [
        "M Abdelwahab",
        "C Busso"
      ],
      "year": "2015",
      "venue": "International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "28",
      "title": "Incremental adaptation using active learning for acoustic emotion recognition",
      "year": "2017",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "29",
      "title": "Deep learning of representations for unsupervised and transfer learning",
      "authors": [
        "Y Bengio"
      ],
      "year": "2012",
      "venue": "ICML Workshop on Unsupervised and Transfer Learning"
    },
    {
      "citation_id": "30",
      "title": "Sparse autoencoder-based feature transfer learning for speech emotion recognition",
      "authors": [
        "J Deng",
        "Z Zhang",
        "E Marchi",
        "B Schuller"
      ],
      "year": "2013",
      "venue": "2013 Humaine Association Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "31",
      "title": "Introducing shared-hidden-layer autoencoders for transfer learning and their application in acoustic emotion recognition",
      "authors": [
        "J Deng",
        "R Xia",
        "Z Zhang",
        "Y Liu",
        "B Schuller"
      ],
      "year": "2014",
      "venue": "Proc. International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "32",
      "title": "Progressive neural networks for transfer learning in emotion recognition",
      "authors": [
        "J Gideon",
        "S Khorram",
        "Z Aldeneh",
        "D Dimitriadis",
        "E Provost"
      ],
      "year": "2017",
      "venue": "Progressive neural networks for transfer learning in emotion recognition",
      "arxiv": "arXiv:1706.03256"
    },
    {
      "citation_id": "33",
      "title": "On acoustic emotion recognition: compensating for covariate shift",
      "authors": [
        "A Hassan",
        "R Damper",
        "M Niranjan"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "34",
      "title": "Speech emotion recognition using transfer non-negative matrix factorization",
      "authors": [
        "P Song",
        "S Ou",
        "W Zheng",
        "Y Jin",
        "L Zhao"
      ],
      "year": "2016",
      "venue": "Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "35",
      "title": "Crosscorpus speech emotion recognition based on domainadaptive least-squares regression",
      "authors": [
        "Y Zong",
        "W Zheng",
        "T Zhang",
        "X Huang"
      ],
      "year": "2016",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "36",
      "title": "Unsupervised domain adaptation for speech emotion recognition using pcanet",
      "authors": [
        "Z Huang",
        "W Xue",
        "Q Mao",
        "Y Zhan"
      ],
      "year": "2017",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "37",
      "title": "A personalized emotion recognition system using an unsupervised feature adaptation scheme",
      "authors": [
        "T Rahman",
        "C Busso"
      ],
      "year": "2012",
      "venue": "International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "38",
      "title": "Iterative feature normalization scheme for automatic emotion detection from speech",
      "authors": [
        "C Busso",
        "S Mariooryad",
        "A Metallinou",
        "S Narayanan"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "39",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "40",
      "title": "Building a naturalistic emotional speech corpus by retrieving expressive behaviors from existing speech corpora",
      "authors": [
        "S Mariooryad",
        "R Lotfian",
        "C Busso"
      ],
      "year": "2014",
      "venue": "Building a naturalistic emotional speech corpus by retrieving expressive behaviors from existing speech corpora"
    },
    {
      "citation_id": "41",
      "title": "Increasing the reliability of crowdsourcing evaluations using online quality assessment",
      "authors": [
        "A Burmania",
        "S Parthasarathy",
        "C Busso"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "42",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Journal of Language Resources and Evaluation"
    },
    {
      "citation_id": "43",
      "title": "MSP-IMPROV: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "C Busso",
        "S Parthasarathy",
        "A Burmania",
        "M Abdelwahab",
        "N Sadoughi",
        "E Provost"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "44",
      "title": "The INTERSPEECH 2013 computational paralinguistics challenge: Social signals, conflict, emotion, autism",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner",
        "A Vinciarelli",
        "K Scherer",
        "F Ringeval",
        "M Chetouani",
        "F Weninger",
        "F Eyben",
        "E Marchi",
        "M Mortillaro",
        "H Salamin",
        "A Polychroniou",
        "F Valente",
        "S Kim"
      ],
      "year": "2013",
      "venue": "The INTERSPEECH 2013 computational paralinguistics challenge: Social signals, conflict, emotion, autism"
    },
    {
      "citation_id": "45",
      "title": "Language recognition via i-vectors and dimensionality reduction",
      "authors": [
        "N Dehak",
        "P Torres-Carrasquillo",
        "D Reynolds",
        "R Dehak"
      ],
      "year": "2011",
      "venue": "Twelfth annual conference of the international speech communication association"
    },
    {
      "citation_id": "46",
      "title": "X-vectors: Robust dnn embeddings for speaker recognition",
      "authors": [
        "D Snyder",
        "D Garcia-Romero",
        "G Sell",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    }
  ]
}