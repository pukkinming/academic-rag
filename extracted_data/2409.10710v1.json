{
  "paper_id": "2409.10710v1",
  "title": "A Heartfelt Robot: Social Robot-Driven Deep Emotional Art Reflection With Children",
  "published": "2024-09-16T20:29:20Z",
  "authors": [
    "Isabella Pu",
    "Golda Nguyen",
    "Lama Alsultan",
    "Rosalind Picard",
    "Cynthia Breazeal",
    "Sharifa Alghowinem"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Social-emotional learning (SEL) skills are essential for children to develop to provide a foundation for future relational and academic success. Using art as a medium for creation or as a topic to provoke conversation is a well-known method of SEL learning. Similarly, social robots have been used to teach SEL competencies like empathy, but the combination of art and social robotics has been minimally explored. In this paper, we present a novel child-robot interaction designed to foster empathy and promote SEL competencies via a conversation about art scaffolded by a social robot. Participants (N=11, age range: 7-11) conversed with a social robot about emotional and neutral art. Analysis of video and speech data demonstrated that this interaction design successfully engaged children in the practice of SEL skills, like emotion recognition and selfawareness, and greater rates of empathetic reasoning were observed when children engaged with the robot about emotional art. This study demonstrated that art-based reflection with a social robot, particularly on emotional art, can foster empathy in children, and interactions with a social robot help alleviate discomfort when sharing deep or vulnerable emotions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction & Background",
      "text": "Social and emotional intelligence is an essential skill for individuals to effectively communicate, interact, and build relationships. This can be fostered through social-emotional learning, which Elias et al. describes as the systematic acquisition of emotional intelligence by developing relevant skills, attitudes, and values  [1] . The Collaborative for Academic, Social, and Emotional Learning (CASEL) names core SEL skills as self-awareness, self-management, social awareness, relationship skills, and responsible decision-making  [2] .\n\nEarly exposure to SEL provides significant short-term and long-term behavioral benefits, such as improved selfconfidence  [3] , reduced likelihood of emotional issues and conduct problems  [4] ,  [5] , and improved long-term academic and relational success  [6] ,  [7] . Additionally, a 2013 survey showed 97% of teachers acknowledge the positive impact of SEL on students from all socioeconomic backgrounds  [8] , and a study from the Aspen Institute found that SEL programs were particularly beneficial for fostering wellbeing in children from low-income communities  [9] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Social Robots For Social-Emotional Learning",
      "text": "To increase access to SEL programming, interactive technologies can deliver educational content inside and outside of the classroom. Social robots and artificial intelligence systems, through responsive engagement with interaction partners, are becoming increasingly prevalent in early childhood education  [10] ,  [11] . However, social robots have predominantly been used in childhood education for academic learning, focusing on areas like language  [12] ,  [13] , literacy,  [14] ,  [15] , and computer science skills  [16] ,  [17] , while social robots for SEL practice have been relatively limited.\n\nSocial robots are a promising method of delivering early SEL education to children given children's high engagement with these technologies  [18] ,  [19] . Previous work has shown social robots can alleviate anxiety in children by offering reassurance, and that children felt comfortable sharing emotions with a social robot  [20] . Another promising capability of social robots is their ability to convey empathy and to foster the development of empathy in children  [21] ,  [22] .\n\nThough fewer in number compared to studies on academic education with social robots, prior work has demonstrated success in using social robots to teach SEL competencies. Embodied's Moxie, a social robot designed to teach SEL skills to children with developmental disorders, significantly improved children's SEL compentencies  [23] . Several prior studies have focused on teaching emotion recognition and empathy to neurodivergent children (i.e., with autism spectrum disorder)  [24] ,  [25] ,  [26] ,  [27] , but SEL training is impactful for both neurodivergent and neurotypical populations.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "B. Social-Emotional Learning With Art",
      "text": "Using art as a medium for self-expression or to evoke thoughtful conversation is a widely recognized method of SEL instruction  [1] ,  [28] ,  [29] . Prior studies have demonstrated the use of both art education  [30]  and artistic creation  [31] ,  [32]  to build interpersonal and social skills, like selfawareness  [2] ,  [28] , that contribute to overall emotional development in children  [33] .\n\nDifferent mediums of art have also been explored for SEL programming, including dance, music, and visual arts  [34] . While most arts curricula for teaching SEL involve creating art, several examples have focused on discussing and observing art. Ebert et al.  [35]  conducted a workshop where children observed emotions in subjects of different artworks to develop SEL skills like emotion recognition and selfawareness. The Metropolitan Museum of Art  [36]  developed a curriculum based on art observation and discussion to help students practice self-awareness, self-management, social awareness, empathy, and relationship skills-this curriculum was specifically designed for blind or partially sighted students, students with autism spectrum disorder (ASD), and students with developmental disabilities. In these curricula, children successfully developed skills like empathy and emotion recognition by discussing art in scaffolded settings.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "C. Study Objectives",
      "text": "The use of art has been minimally explored in tandem with interactive and responsive social agents for SEL instruction. Cooney et al.  [31] ,  [32]  demonstrated the potential of social robots for art therapy to facilitate emotional expression via artistic creation. We expand on this connection by exploring robot-guided conversations on art, in which art is used as a conversational catalyst for children to practice emotion recognition, self-awareness, and empathy. Specifically, an experimental study was conducted to explore how different styles of art (explicitly emotive art and neutral art) affect behavioral responses in children (ages  [7] [8] [9] [10] [11] . Reflection on the artwork was scaffolded by guiding questions and responsive dialogue from a social robot, Jibo  [37] , to investigate the following research questions:\n\n• RQ1: When interactively reflecting with a social robot, does emotional art foster empathy more successfully than neutral art in children? • RQ2: Are there behavioral differences (in engagement, disclosure) when children reflect with a social robot on emotional art versus neutral art? We propose the following hypotheses:\n\n• H1: Emotional art will foster more empathy than neutral art during interactive reflection with a social robot. • H2: Children will engage more deeply with emotional art than neutral art during the social robot interaction. • H3: Children who are more open when sharing feelings with the social robot will engage more with the activity.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Ii. Interaction Design A. Robot Station Design",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Fig. 1: Jibo Robot Station",
      "text": "We designed an interaction where participants spoke with Jibo  [37] , a social embodied agent, through a multi-device system (the \"Robot Station\", shown in Figure  1 ). Jibo was used because of its ability to express emotional states with bodily animations, expressive screen-based face, childfriendly character design, and durability for autonomous conversational interactions. The Robot Station has a Jibo robot on the left side and a Samsung Galaxy tablet on the right. There is also a Logitech C930e camera housed above the tablet and a separate MXL AC-44 microphone located between Jibo and the tablet. An Ubuntu machine located The Robot Station design allows for Jibo to look between the participant and the tablet for social engagement. Jibo can also freely rotate around three axes while placed in the Robot Station, to provide emotive movement while speaking.\n\nJibo converses with the participant by asking scripted questions about artwork and replying with generations from GPT-4 1  , in response to participant dialogue. The Ubuntu machine sends commands to Jibo through messaging a Firebase database, which Jibo reads from. The tablet also communicates with the same database, allowing Jibo, the Ubuntu machine, and the tablet to execute synchronously. The system architecture is shown in detail in Figure  2 .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Art Design For Robot Interaction",
      "text": "Two categories of artwork were used to prompt empathy and disclosure in the child-robot interaction: 1) emotional and 2) neutral art. Artworks shown to participants were generated with DALL-E 3  2  and reviewed in advance by the research team to ensure their suitability for children. Emotional artworks (Figure  3 ) featured animal characters explicitly displaying an emotion: happiness, sadness, or anger. Different styles were used for variety but all included vibrant colors and animals to appeal to children. As an example, the prompt for the \"anger\" image was: Two angry teddy bears arguing in the street, kid-friendly comic style, hand-drawn, with vibrant colors.\n\nNeutral artworks (Figure  4 ) depicted abstract or landscape imagery to elicit emotions, but not any specific emotion, based on color, composition, or style. These pieces were also designed for children, with bright colors and familiar locations (the ocean, a living room, and a farm). For example, the \"living room\" image prompt was: Memphis Group style image of a colorful living room where the furniture is upside down, sideways, and backwards, with a blue couch that is right side up. 2D style with bold shapes.\n\nWe validated the emotional effects of these generated artworks during the experimental study (see Section IV).",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Experiment & Analysis Methodology",
      "text": "An experimental study was conducted to explore how social robots can help children foster empathy and practice SEL skills via conversation on art. The study consisted of two sessions: 1) the participant discussed \"neutral art\" (without clear emotions) with a social robot (Jibo), and 2) the participant discussed \"emotional art\" (where characters in the art explicitly convey emotions) with the social robot. A within-subjects design was used, and session order was randomized to control for ordering effects. The study protocol was approved by our institution's ethics review board.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Experimental Procedure",
      "text": "Participants and their parents were invited to an in-person study in an enclosed space. Two members of the research team were also in the room. The study lasted approximately one hour with two robot interaction sessions (each about 15 minutes), with a break between sessions. Sessions were video-and audio-recorded using two cameras (the robot station and a wall-mounted GoPro) and the station microphone.\n\nThe first interaction session with Jibo began with a brief tutorial on how to record responses to Jibo's questions and two neutral practice questions, asking for the participant's name and age. After the tutorial, participants engaged in the first interaction session (one of the two categories described in Art Design for Robot Interaction). There was a short break after the first session before participants began the second session. After both sessions were completed, participants were interviewed on their experience.\n\nWithin each session, Jibo asked the following questions about each image:\n\n1) Can you tell me a story about this picture or describe this picture to me? 2) What emotion does this picture make you feel? 3) Why does this picture make you feel that emotion? 4) Can you tell me about the last time you felt that emotion? The questions were designed to guide participants to reflect on emotions they observed or related to within the art piece. Specifically, the questions targeted self-awareness (\"identifying one's emotions\", \"linking feelings, values, and thoughts\") and relationship skills (\"communicating effectively\") in all sessions, while additionally targeting social awareness (\"demonstrating empathy and compassion\") in the emotional session  [2] . Table  I  provides an example of question prompts from Jibo and P-10's responses to an emotional image. Well, one time I was with my brother and I got in a fight with him because I got mad at him. We didn't get hurt, but I got scared. And I felt like it was sort of silly that we were both fighting though.\n\nJibo's utterances were generated by GPT-4 to adapt to participant responses for a more personalized interaction (GPT-4 responses are not shown in Table  I  for brevity). For example, a participant told Jibo about a new pet gecko, and Jibo's response referenced the gecko and gave an encouraging comment on the bond between humans and pets.\n\nThough GPT-4 responses may vary between participants due to the AI's nature, using the same strict and detailed prompts for all participants ensured that the responses maintained consistent vocabulary and were within the same vein.\n\nThe post-study interview was conducted without the Jibo robot present, to minimize potential effects of the robot's presence effect on the child's opinions. The child was asked questions on a tablet about what they liked about the interactions with Jibo, what they disliked, if they would change anything, their feelings of comfort with or trust in Jibo, and if they would want to interact with Jibo again.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. Participants",
      "text": "This study was conducted with 11 children between age 7 and 11 (average age of 9.3), with 6 female and 5 male participants. 7 participants were white, 2 were Asian, and 2 were mixed race. Ethnicity was not considered as a factor in this study and was not analyzed any further than for demographic purposes. Legal guardians were consented, and participants provided their assent.\n\nParticipants were recruited via email advertisements to parents of children who had previously participated in community outreach programs or indicated interest in robot and AI studies. Participants and their family were not familiar with the specific researchers facilitating this study. Participants' travel to the study location was reimbursed, but they were not otherwise compensated  [38] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Linguistic Analysis Methods",
      "text": "Linguistic analysis of participant dialogue was performed to collect measures of conversational engagement and disclosure. Speech was first transcribed by Assembly AI Automatic Speech Recognition 3  , then manually cleaned by the research team. Common themes were coded by question using thematic content analysis to examine emotional trends. Coding was performed by two independent raters, and inter-rater agreement was calculated using Cohen's Kappa (κ)  [39] .\n\nRaters examined responses to question 2) and listed all emotions participants cited in order to validate the success of the emotional artwork in eliciting the target emotion.\n\nResponses to question 3) were coded as exhibiting \"empathetic reasoning\", \"visual reasoning\", or neither. Examples of empathetic reasoning included \"... she's happy-just like I am-because she's probably happy that she's dancing because I'm happy when I dance.\" from P-10 and \"It makes me feel upset because the hedgehog has dropped its ice cream, and it looks like really good ice cream.\" from P-06. Examples of visual reasoning from the same participants included \"It's the theme of the artwork, and also the colors because it has rainbow colors on it and I like rainbow colors.\" from P-10 and \"Because of the way it looks.\" from P-06. Responses like \"I don't know\" or incoherent responses were labeled as exhibiting neither type of reasoning. This coding had very strong inter-rater agreement (κ = 0.82).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Video Analysis Methods",
      "text": "To conduct behavioral analysis, video data of the sessions was annotated by two independent annotators using ELAN 6.7, with a coding manual on three measurements: Comfort (ease, relaxation, and lack of anxiety), Engagement (attention, interest, and active participation), and Openness (open, honest, and vulnerable behavior). Each of the measurements included a negative (-1), neutral (0), or positive (1) rating. Annotators specifically marked events with negative (-1) or positive (1) behavior, leaving unmarked sections of video scored as neutral (0). Cohen's Kappa was calculated on 30% of the video data to measure inter-rater reliability, revealing a substantial agreement score (0.61 < κ < 0.80) across all three measures. Annotators also noted behavioral patterns that stood out or occurred often among participants.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Validation Of Emotional Artwork",
      "text": "The emotional art used in this study was generated specifically for this experiment, with the goal of having participants empathize with that emotion. The three images used, as seen in Figure  3 , sought to demonstrate happiness, sadness, and anger respectively (viewed from left to right).\n\nMost participants (8 out of 11) referenced some dimension of happiness (synonyms included \"joyful\" and \"excited\") when viewing the image designed to elicit happiness. The other three participants referenced feeling \"fine\" (later elaborating that the image did not elicit a strong feeling), \"weirded out\" (describing that \"a ballerina dress on an elephant seems weird\"), and \"fear\" (explaining that it reminded them of a dancing-related memory that induced fear when recollected).\n\nAll participants cited some dimension of sadness (synonyms included \"feeling bad\" and \"upset\") when viewing the image designed to induce sadness.\n\nThe picture designed to induce anger was the most polarizing, with 7 out of 11 participants describing it as making them feel either \"angry\" or \"annoyed\". Of the other 4 participants, 3 expressed that they found the image \"funny\" or that it made them feel \"silly\". When elaborating on their reasoning, these participants explained that they found watching others fight to be a funny or silly experience. However, 2 of these 3 participants still shared a related memory that involved a fight and described having negative emotions during that memory, like fear and confusion. One also mentioned that the image did not elicit anger but reminded them of anger, since the bears depicted in it were fighting. The last participant who did not mention anger instead expressed that they felt \"tired\", likely due to it being the final image they were viewing over the entire study.\n\nOverall, a majority of participants experienced the intended emotion of each picture in the emotional session.\n\nIn the neutral session, participants cited more varied emotions (8, 10, and 10 total emotions cited, respectively, for each neutral image). The most common feelings cited in the neutral session were \"creative\", \"energetic\", and \"curious\".",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "V. Results",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Emotional Art Leads To Empathy And Vulnerability",
      "text": "Participant responses to question 3) on why they felt a certain emotion after viewing an image were coded as containing either \"empathetic reasoning\", \"visual reasoning\", both, or neither. To investigate H1, counts were performed of how many instances each reasoning type (empathetic reasoning or visual reasoning) was used by each participant per session, and these counts are shown in Figure  5 .\n\nA distinct pattern quickly emerged, showing participants exhibited more empathetic reasoning when viewing emotional art, as opposed to exhibiting more visual reasoning when viewing neutral art. On average in the emotional sessions, participants used empathetic reasoning in 2.4 out of 3 images, while only using visual reasoning in 0.5 out of 3 images. On average in the neutral sessions, participants used empathetic reasoning in 0.4 out of 3 images while using visual reasoning in 2.5 out of 3 images.\n\nTo compare between sessions, Wilcoxon signed-rank tests (non-parametric tests appropriate for small sample sizes) were performed. Significant differences were found, showing significantly more coded instances of empathetic reasoning in the emotional session and significantly more coded instances of visual reasoning in the neutral sessions (p < .001), strongly supporting H1.\n\nAdditionally, empathy felt by participants during the activity appeared to remain even past the end of the study. For example, during the post-activity interview, P-02 stated that their least favorite part of the activity was \"seeing the sad hedgehog with the dropped ice cream\". Participants also appreciated the chance to talk about their emotions, with P-13 stating \"My favorite part of the activities today were the questions that most people wouldn't have asked me, about your feelings...\" and P-10 responding that \"Probably talking with Jibo and saying my emotions about the different artwork...\" was their favorite part of the activity.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "B. Participant Verbosity",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Fig. 6: Average Words Per Participant",
      "text": "We noted a large difference in verbosity between the more verbose participants and the less verbose participants. Therefore, we divided the study sample into a more verbose group (V+) and a less verbose (V-) group, as distinguished in Figure  6 . Separating the study sample into V+ and Valso helped to test hypothesis H3 on potential differences in behavior between open and closed participants.\n\nComparing verbosity, a Mann-Whitney U Test showed participants in the V+ group had significantly greater verbosity (average number of words spoken) than those in the Vgroup (p < 0.01). The V+ group spoke an average of 43.4 words per utterance with a standard deviation of 10.9 words, while the Vgroup spoke an average of 10.3 words per utterance with a standard deviation of 6.7 words. The V+/Vsplit aligned with observations from video analysis of differences in noted extraversion and behavioral patterns. For example, participants in V+ tended to give more vulnerable and open responses compared to those in V-(as noted by video annotators).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Engagement And Discomfort",
      "text": "Annotations from behavioral analysis demonstrated a positive average engagement in every session for every participant, even for sessions with participants who were mostly rated as displaying discomfort. During video analysis, annotators labeled events where participants appeared disengaged (-1) or engaged  (1) . Any portions of the video not marked as either were automatically classified as neutral behavior, with an engagement score of (0).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Fig. 7: Engagement Per Verbosity Group And Session Type",
      "text": "On this scale, the V+ group had an average engagement score of 0.57 ± 0.09 over emotional sessions and 0.41 ± 0.21 over neutral sessions, while the average engagement score of the Vgroup was 0.20 ± 0.09 over emotional sessions and 0.18 ± 0.07 over neutral sessions, as shown in Figure  7 .\n\nThough neither group saw a significant difference between engagement in emotional versus neutral sessions, we noted that both groups were on average more engaged in the emotional session, suggesting support for H2. Both groups were also on average positively engaged in all sessions.\n\nA Mann-Whitney U Test was performed to examine differences in engagement by verbosity grouping. The V+ group's engagement score is significantly higher (p < 0.01) than the the Vgroup score for average engagement in emotional sessions. For neutral sessions, there was not a significant difference in engagement scores between groups (p > 0.05).\n\nWe also considered that the novelty effect  [40] ,  [41]  could artificially inflate ratings for engagement, as participants who had never seen Jibo may have been excited to interact with a new stimulus and be more engaged than they would be with a familiar stimulus. However, more than half of the participants were already familiar with Jibo due to the nature of recruiting-participants familiar with Jibo were P-02, P-05, P-08, P-09, P-10, and P-11. Four of these participants were in the V+ group, and two were in the Vgroup.\n\nThe average engagement score of participants who had previous experience with Jibo was 0.46 ± 0.22 in the emotional sessions and 0.40 ± 0.18 in the neutral sessions, while the average engagement score of participants who were unfamiliar with Jibo was 0.34 ± 0.19 in the emotional sessions and 0.19 ± 0.16 in the neutral sessions. Participants who had never interacted with Jibo before were actually less engaged, on average, than participants who had previous familiarity with Jibo. Therefore, the novelty effect did not seem to have an effect on artificially increasing engagement.\n\nThrough qualitative analysis of participant responses and behavioral analysis, we observed that participant engagement remained high even during moments when participants experienced discomfort while sharing their feelings.\n\nAn example of a story that caused discomfort in a participant is from P-07, who recalled: \"... I felt so bad, I almost started crying. Just like this porcupine. But I didn't cry because men don't cry. And I didn't want to do it. I attract too much [sic] people looking at me and staring and calling out so I just had to live with it.\" When sharing this emotional memory, P-07 exhibited behavior expressing discomfort (marked by annotators), like fidgeting and reduced eye contact. Despite their discomfort, they were sharing vulnerable feelings and engaging deeply with the interaction.\n\nAnother example was seen when examining utterances by P-02. They shared: \"Okay, so the first day of school was a train wreck... I was like, how am I supposed to keep this up? For a year! I can't even keep it up for like a day. Because, like, when I got home, I was very mad. I was very annoyed of what would happen that day [sic]. And I was very sad. I was eating ice cream in my blankets on my bed.\" When sharing this anecdote, P-02 was noted by video annotators to exhibit behaviors suggesting discomfort such as nervous smiling, tense shoulders, and fidgeting. However, P-02 was also clearly engaging with the activity by sharing emotions and memories that were honest and vulnerable.\n\nAdditionally, in the post-activity interview, participants acknowledged that parts of the activity made them uncomfortable, with P-10 remarking \"I felt a little uncomfortable talking with a robot but I also felt really excited to talk with Jibo,\" and P-09 sharing \"I was feeling, like, a little shy.\"",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "D. Social Robot Mitigates Discomfort",
      "text": "Video annotators noted across both session types that when participants gave a response that was particularly vulnerable, honest, or open (as noted by annotators in the Openness measurement), they would often exhibit discomfort. This discomfort would continue until Jibo responded to them with either an affirmative reassurance or compliment, whereupon participants' discomfort would be reduced.\n\nFor example, after sharing a particularly vulnerable memory, P-07 recalled that they were \"...Feeling like I don't really belong here, and why the hell am I even doing this?\" and immediately began exhibiting uncomfortable behaviors like flitting their eyes around the area and intense fidgeting. Once Jibo reassured P-07 by saying \"I can see how this piece of art triggered some unique memories for you!\", P-07 settled down and began making eye contact with Jibo again.\n\nThis pattern occurred in both the V+ and Vgroups, but the V+ group shared more vulnerable, honest, or open answers than the Vgroup. In total, the V+ group gave 97 total vulnerable, honest, or open answers (60 in emotional sessions and 37 in neutral sessions) and the Vgroup only gave 14 (10 in emotional sessions and 4 in neutral sessions).\n\nA Mann-Whitney U Test was performed to test for significant differences between the number of vulnerable answers shared in the V+ group versus the Vgroup. The V+ group shared significantly more vulnerable answers than the Vgroup (p < 0.01) in both sessions. This finding supports H3 for emotional art, as the V+ group was shown to both share significantly more vulnerable feelings and be significantly more engaged than the Vgroup in emotional sessions.\n\nOf 111 total vulnerable answers across all groups, 94 answers (or 84.7% of answers) were followed by a Jibo response that reduced the participant's discomfort noticeably (as noted by the video annotators), while 15.3% of vulnerable answers followed by a Jibo response led to discomfort that either remained the same or increased.\n\nWhen looking at the 97 vulnerable answers given by the V+ group, 83 (85.6%) were followed by a Jibo utterance that visibly reduced the participant's discomfort. Though the V+ group provided more vulnerable answers, the proportion of times their discomfort was reduced by Jibo (after sharing vulnerably) was higher than that of the entire population.\n\nAdditionally, with 97 vulnerable answers across 6 members, the V+ group produced an average of 16.17 vulnerable answers per participant. Each participant was only asked 24 questions in total, leading to V+ group members sharing vulnerably in over two-thirds of their interactions with Jibo.\n\nExamining the V+ group further, in every session, participants' discomfort post-vulnerable response was reduced more often than not after Jibo's next utterance. There was no significant difference in the proportion of vulnerable answers that led to reduced discomfort between emotional and neutral sessions, though participants in general tended to share more vulnerable, honest, or open answers during the emotional sessions (70 answers total, 60 from V+) compared to the neutral sessions (41 answers total, 37 from V+).\n\nQualitative analysis of the post-interaction interview showed that participants enjoyed Jibo's responses and particularly appreciated that Jibo appeared to actively listen to them. P-07 remarked \"I liked that when I say something, he really takes the time to think and he gives something corresponding to what I said\", P-02 shared that \"It was very fun to talk with something that was not human but also could probably hear me\", and P-06 stated \"I liked how Jibo responded to me and I think it understanded [sic] me.\"\n\nFisher's Exact test was used to compare instances where Jibo's responses did or did not reduce participant discomfort after vulnerable utterances. Jibo's reassuring utterances significantly improved (p << .001) participant comfort after they were vulnerable, honest, and open.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Vi. Discussion",
      "text": "Art is a well-explored vehicle for helping children learn and practice SEL competencies  [1] ,  [28] ,  [29] , though it has been minimally explored in tandem with social robots. Cooney et al. have examined social robots for art therapy  [31] ,  [32] , and curricula have been designed to promote SEL skills in children when observing and reflecting on art  [35] ,  [36] . This study sought to expand on these prior works through a demonstration of scaffolding emotional conversations about art by using social robots for children to interactively develop and practice SEL competencies.\n\nConversations about emotional art scaffolded by social robots can foster empathy in children. Significantly higher rates of empathetic reasoning were exhibited in emotional art versus neutral art sessions, strongly supporting H1. Observing and reflecting on emotional art with Jibo promoted emotional transfer and empathetic thinking, which help build empathy as an SEL competency  [1] ,  [2] . The presence of Jibo and the interactive conversation facilitated by Jibo appeared to promote empathetic connection with the participant and supports the use of social robots to scaffold activities for building SEL skills. Scaffolding activities to reflect on art allowed for richer reflection on emotions observed in the art and how those emotions might connect to the participant's life. These results expand the evidence base on using social robots for teaching emotion recognition and empathy, not just for neurodivergent children  [24] ,  [25] ,  [26] ,  [27] , but also to provide skill-building for neurotypical children.\n\nChildren are highly engaged in social robot-driven SEL practice, even when sharing vulnerable reflections and potentially experiencing discomfort. On average, every participant was rated as having positive engagement (raters noted strong eye contact and deep, thoughtful contributions to the interaction) across sessions. This result demonstrated that interactions with Jibo successfully held children's interest and is consistent with previous findings on how social robots can promote user engagement  [18] ,  [19] . Furthermore, participants who had previously interacted with Jibo had higher average engagement scores than those without prior experience, suggesting that engagement was not due to the novelty effect  [40] ,  [41] . This higher engagement from participants with past experience may suggest a self-selection bias in study participation but also points to the potential for leveraging social robots for longitudinal SEL programming, where continuous interaction may lead to higher engagement. From qualitative analysis of utterances, participant engagement remained high even during moments of discomfort, suggesting that participants felt it was a safe space to feel the discomfort that arises from vulnerability and could continue to engage with the robot. Results support H2 for the V+ group, as participants' average engagement levels in the emotional sessions were higher than in neutral sessions. However, the difference was not statistically significant, and more data is needed to reach a conclusive result. Results also support H3 for emotional art, as the V+ group, who were significantly more open, had significantly higher levels of engagement than the Vgroup in the emotional session.\n\nA social robot can help mitigate the discomfort a child feels when sharing vulnerable feelings. Discomfort that arose during and after participants shared vulnerable feelings decreased significantly after the robot offered a reassuring response. These findings demonstrated that interacting with Jibo was a comforting experience, consistent with previous findings showing that social robots can reduce children's anxiety and promote comfort and disclosure  [20] . Additionally, V+ group members on average shared deeply and openly 16.17 times out of 24 utterances total (approximately twothirds of utterances). Participants appeared to share deeply as they felt comfortable around Jibo due to how his responses were personalized to their utterances, which may have helped the participants feel listened to and cared for. One change that could help the Vgroup share more is using the robot to detect when further questioning is helpful-for example, when a participant said \"I don't know\", Jibo would move on and ask the participant what was confusing. The participant may have been able to share more if Jibo had instead prompted them to think again about their feelings.\n\nThis study was limited by a small sample size, and future works will expand to a larger, more diverse population to validate findings. Behavioral analysis also suggested that a laboratory setting may have inadvertently heightened discomfort, as participants disclosed vulnerable information amidst strangers in an unfamiliar place. Conducting future studies in familiar areas could encourage more open and vulnerable responses. Future research would also benefit from exploring longitudinal interactions to better understand changes over time in participants' SEL skills. This study only investigated reactions to three pieces of art representing three emotions. However, future research should include a broader spectrum of art and a wider range of emotions to facilitate more comprehensive exploration of emotional responses.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Vii. Conclusion",
      "text": "We explored how social robots can foster social-emotional learning (SEL) competencies in children through conversations about art. Our investigation involved 11 participants who engaged in two sessions discussing emotional and neutral artworks, facilitated by social robot scaffolding.\n\nFindings demonstrated that discussing emotional art with a social robot is an effective method for emotional selfawareness and empathy (key SEL skills). Reflecting on art prompted children to engage deeply and thoughtfully with the social robot, and it was able to alleviate discomfort to encourage continued engagement and emotional exploration.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Jibo Robot Station",
      "page": 2
    },
    {
      "caption": "Figure 2: Robot Station System Architecture",
      "page": 2
    },
    {
      "caption": "Figure 2: B. Art Design for Robot Interaction",
      "page": 2
    },
    {
      "caption": "Figure 3: Emotional artworks",
      "page": 2
    },
    {
      "caption": "Figure 4: Neutral artworks",
      "page": 2
    },
    {
      "caption": "Figure 3: ) featured animal characters",
      "page": 2
    },
    {
      "caption": "Figure 4: ) depicted abstract or landscape",
      "page": 3
    },
    {
      "caption": "Figure 3: , sought to demonstrate happiness, sadness, and",
      "page": 4
    },
    {
      "caption": "Figure 5: A distinct pattern quickly emerged, showing participants",
      "page": 4
    },
    {
      "caption": "Figure 5: Empathetic and visual reasoning between sessions",
      "page": 5
    },
    {
      "caption": "Figure 6: Average words per participant",
      "page": 5
    },
    {
      "caption": "Figure 6: Separating the study sample into V+ and V-",
      "page": 5
    },
    {
      "caption": "Figure 7: Engagement per verbosity group and session type",
      "page": 5
    },
    {
      "caption": "Figure 7: Though neither group saw a significant difference between",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Jibo:": "Can you tell me a story\nabout\nthis picture or de-\nscribe this picture to me?",
          "P-10:": "The picture is about\ntwo bears\nthat proba-\nbly got\nin a fight. And they wanted... and\nthey’re like, mad at each other."
        },
        {
          "Jibo:": "What emotion does\nthis\npicture make you feel?",
          "P-10:": "It makes me feel sort of...\nIt’s sort of silly.\nIt makes me feel sort of silly, and scared. It\nmakes me feel\nscared because I don’t\nlike\nwhen people get\nin fights, and it makes me\nfeel sort of silly because it’s funny that there\nare two bears that are in a fight."
        },
        {
          "Jibo:": "Why\ndoes\nthis\npicture\nmake you feel\nthat emo-\ntion?",
          "P-10:": "Well,\nit’s because I don’t\nlike when people\nget hurt and fighting usually means that you\ncan get hurt. But\nit’s\nfunny to watch. Be-\ncause, I don’t know. I just feel this emotion."
        },
        {
          "Jibo:": "Can\nyou\ntell me\nabout\nthe last time you felt that\nemotion?",
          "P-10:": "Well, one time I was with my brother and\nI got\nin a fight with him because I got mad\nat him. We didn’t get hurt, but I got scared.\nAnd I\nfelt\nlike it was\nsort of\nsilly that we\nwere both fighting though."
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Promoting social and emotional learning: Guidelines for educators",
      "authors": [
        "M Elias",
        "M Elias",
        "J Zins",
        "R Weissberg"
      ],
      "year": "1997",
      "venue": "Promoting social and emotional learning: Guidelines for educators"
    },
    {
      "citation_id": "2",
      "title": "Evidence-based social and emotional learning programs: Casel criteria updates and rationale",
      "authors": [
        "A Skoog-Hoffman",
        "C Ackerman",
        "A Boyle",
        "H Schwartz",
        "B Williams",
        "R Jagers",
        "L Dusenbury",
        "M Greenberg",
        "J Mahoney",
        "K Schonert-Reichl"
      ],
      "year": "2020",
      "venue": "Retrieved February"
    },
    {
      "citation_id": "3",
      "title": "Social and emotional learning as a public health approach to education",
      "authors": [
        "M Greenberg",
        "C Domitrovich",
        "R Weissberg",
        "J Durlak"
      ],
      "year": "2017",
      "venue": "The future of children"
    },
    {
      "citation_id": "4",
      "title": "Fostering socioemotional learning through early childhood intervention",
      "authors": [
        "C Mondi",
        "A Giovanelli",
        "A Reynolds"
      ],
      "year": "2021",
      "venue": "International Journal of Child Care and Education Policy"
    },
    {
      "citation_id": "5",
      "title": "Effects of social development intervention in childhood 15 years later",
      "authors": [
        "J Hawkins",
        "R Kosterman",
        "R Catalano",
        "K Hill",
        "R Abbott"
      ],
      "year": "2008",
      "venue": "Archives of pediatrics & adolescent medicine"
    },
    {
      "citation_id": "6",
      "title": "Promoting Positive Youth Development Through School-Based Social and Emotional Learning Interventions: A Meta-Analysis of Follow-Up Effects",
      "authors": [
        "R Taylor",
        "E Oberle",
        "J Durlak",
        "R Weissberg"
      ],
      "year": "2017",
      "venue": "Child Development"
    },
    {
      "citation_id": "7",
      "title": "The Impact of Enhancing Students' Social and Emotional Learning: A Meta-Analysis of School-Based Universal Interventions",
      "authors": [
        "J Durlak",
        "R Weissberg",
        "A Dymnicki",
        "R Taylor",
        "K Schellinger"
      ],
      "year": "2011",
      "venue": "Child Development"
    },
    {
      "citation_id": "8",
      "title": "The missing piece: A national teacher survey on how social and emotional learning can empower children and transform schools. a report for casel",
      "authors": [
        "J Bridgeland",
        "M Bruce",
        "A Hariharan"
      ],
      "year": "2013",
      "venue": "Civic Enterprises"
    },
    {
      "citation_id": "9",
      "title": "From a nation at risk to a nation at hope: Recommendations from the national commission on social, emotional, & academic development",
      "year": "2019",
      "venue": "From a nation at risk to a nation at hope: Recommendations from the national commission on social, emotional, & academic development"
    },
    {
      "citation_id": "10",
      "title": "Children learning with a social robot",
      "authors": [
        "T Kanda",
        "M Shimada",
        "S Koizumi"
      ],
      "year": "2012",
      "venue": "Proceedings of the 7th annual ACM/IEEE international conference on Human-Robot Interaction"
    },
    {
      "citation_id": "11",
      "title": "Social learning theory: Phylogenetic considerations across animal, plant, and microbial taxa",
      "authors": [
        "A Thomaz",
        "M Cakmak",
        "K Clark"
      ],
      "year": "2013",
      "venue": "Social learning theory: Phylogenetic considerations across animal, plant, and microbial taxa"
    },
    {
      "citation_id": "12",
      "title": "Social robots for early language learning: Current evidence and future directions",
      "authors": [
        "J Kanero",
        "V Gec ¸kin",
        "C Oranc",
        "E Mamus",
        "A Küntay",
        "T Göksun"
      ],
      "year": "2018",
      "venue": "Child Development Perspectives"
    },
    {
      "citation_id": "13",
      "title": "Social robots for language learning: A review",
      "authors": [
        "R Van Den Berghe",
        "J Verhagen",
        "O Oudgenoeg-Paz",
        "S Van Der Ven",
        "P Leseman"
      ],
      "year": "2019",
      "venue": "Review of Educational Research"
    },
    {
      "citation_id": "14",
      "title": "Social robots and young children's early language and literacy learning",
      "authors": [
        "M Neumann"
      ],
      "year": "2020",
      "venue": "Early Childhood Education Journal"
    },
    {
      "citation_id": "15",
      "title": "Teaching and learning with children: Impact of reciprocal peer learning with a social robot on children's learning and emotive engagement",
      "authors": [
        "H Chen",
        "H Park",
        "C Breazeal"
      ],
      "year": "2020",
      "venue": "Computers & Education"
    },
    {
      "citation_id": "16",
      "title": "Popbots: leveraging social robots to aid preschool children's artificial intelligence education",
      "authors": [
        "R Williams"
      ],
      "year": "2018",
      "venue": "Popbots: leveraging social robots to aid preschool children's artificial intelligence education"
    },
    {
      "citation_id": "17",
      "title": "A computation model for learning programming and emotional intelligence",
      "authors": [
        "M Rafique",
        "M Hassan",
        "A Jaleel",
        "H Khalid",
        "G Bano"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "18",
      "title": "Children use non-verbal cues to learn new words from robots as well as people",
      "authors": [
        "J Westlund",
        "L Dickens",
        "S Jeong",
        "P Harris",
        "D Desteno",
        "C Breazeal"
      ],
      "year": "2017",
      "venue": "International Journal of Child-Computer Interaction"
    },
    {
      "citation_id": "19",
      "title": "Storytelling by a kindergarten social assistive robot: A tool for constructive learning in preschool education",
      "authors": [
        "M Fridin"
      ],
      "year": "2014",
      "venue": "Computers & education"
    },
    {
      "citation_id": "20",
      "title": "a safe space for sharing feelings\": perspectives of children with lived experiences of anxiety on social robots",
      "authors": [
        "J Dosso",
        "J Kailley",
        "S Martin",
        "J Robillard"
      ],
      "year": "2023",
      "venue": "Multimodal Technologies and Interaction"
    },
    {
      "citation_id": "21",
      "title": "Can communication with social robots influence how children develop empathy? best-evidence synthesis",
      "authors": [
        "E Pashevich"
      ],
      "year": "2022",
      "venue": "AI & SOCIETY"
    },
    {
      "citation_id": "22",
      "title": "Socially assistive robots as storytellers that elicit empathy",
      "authors": [
        "M Spitale",
        "S Okamoto",
        "M Gupta",
        "H Xi",
        "M Matarić"
      ],
      "year": "2022",
      "venue": "ACM Transactions on Human-Robot Interaction (THRI)"
    },
    {
      "citation_id": "23",
      "title": "Social and emotional skills training with embodied moxie",
      "authors": [
        "N Hurst",
        "C Clabaugh",
        "R Baynes",
        "J Cohn",
        "D Mitroff",
        "S Scherer"
      ],
      "year": "2020",
      "venue": "Social and emotional skills training with embodied moxie",
      "arxiv": "arXiv:2004.12962"
    },
    {
      "citation_id": "24",
      "title": "A scoping review of the use of robotics technologies for supporting social-emotional learning in children with autism",
      "authors": [
        "S Kewalramani",
        "K.-A Allen",
        "E Leif",
        "A Ng"
      ],
      "year": "2023",
      "venue": "Journal of Autism and Developmental Disorders"
    },
    {
      "citation_id": "25",
      "title": "Outcomes of a robot-assisted social-emotional understanding intervention for young children with autism spectrum disorders",
      "authors": [
        "F Marino",
        "P Chilà",
        "S Sfrazzetto",
        "C Carrozza",
        "I Crimi",
        "C Failla",
        "M Busà",
        "G Bernava",
        "G Tartarisco",
        "D Vagni"
      ],
      "year": "2020",
      "venue": "Journal of autism and developmental disorders"
    },
    {
      "citation_id": "26",
      "title": "Social skills training for children with autism spectrum disorder using a robotic behavioral intervention system",
      "authors": [
        "S.-S Yun",
        "J Choi",
        "S.-K Park",
        "G.-Y Bong",
        "H Yoo"
      ],
      "year": "2017",
      "venue": "Autism Research"
    },
    {
      "citation_id": "27",
      "title": "Deploying a social robot to coteach social emotional learning in the early childhood classroom",
      "authors": [
        "E Wolfe",
        "J Weinberg",
        "S Hupp"
      ],
      "year": "2018",
      "venue": "Proceedings of the 13th Annual ACM/IEEE International Conference on Human-Robot Interaction"
    },
    {
      "citation_id": "28",
      "title": "The arts and the creation of mind",
      "authors": [
        "E Eisner"
      ],
      "year": "2002",
      "venue": "The arts and the creation of mind"
    },
    {
      "citation_id": "29",
      "title": "How the arts help children to create healthy social scripts: Exploring the perceptions of elementary teachers",
      "authors": [
        "L Brouillette"
      ],
      "year": "2009",
      "venue": "Arts Education Policy Review"
    },
    {
      "citation_id": "30",
      "title": "Arts education and socialemotional learning outcomes among k-12 students: Developing a theory of action",
      "authors": [
        "C Farrington",
        "J Maurer",
        "M Mcbride",
        "J Nagaoka",
        "J Puller",
        "S Shewfelt",
        "E Weiss",
        "L Wright"
      ],
      "year": "2019",
      "venue": "Arts education and socialemotional learning outcomes among k-12 students: Developing a theory of action"
    },
    {
      "citation_id": "31",
      "title": "Design for an art therapy robot: An explorative review of the theoretical foundations for engaging in emotional and creative painting with a robot",
      "authors": [
        "M Cooney",
        "M Menezes"
      ],
      "year": "2018",
      "venue": "Multimodal Technologies and Interaction"
    },
    {
      "citation_id": "32",
      "title": "Robot art, in the eye of the beholder?: Personalized metaphors facilitate communication of emotions and creativity",
      "authors": [
        "M Cooney"
      ],
      "year": "2021",
      "venue": "Frontiers in Robotics and AI"
    },
    {
      "citation_id": "33",
      "title": "Living the arts through lan-guage+ learning: A report on community-based youth organizations",
      "authors": [
        "S Heath",
        "E Soep",
        "A Roach"
      ],
      "year": "1998",
      "venue": "Americans for the Arts"
    },
    {
      "citation_id": "34",
      "title": "Local-level implementation of social emotional learning in arts education: Moving the heart through the arts",
      "authors": [
        "M Eddy",
        "C Blatt-Gross",
        "S Edgar",
        "A Gohr",
        "E Halverson",
        "K Humphreys",
        "L Smolin"
      ],
      "year": "2021",
      "venue": "Arts Education Policy Review"
    },
    {
      "citation_id": "35",
      "title": "Teaching emotion and creativity skills through art: A workshop for children",
      "authors": [
        "M Ebert",
        "J Hoffmann",
        "Z Ivcevic",
        "C Phan",
        "M Brackett"
      ],
      "year": "2015",
      "venue": "The International Journal of Creativity & Problem Solving"
    },
    {
      "citation_id": "36",
      "title": "Social and emotional learning through art",
      "year": "2022",
      "venue": "The Metropolitan Museum of Art"
    },
    {
      "citation_id": "37",
      "title": "",
      "authors": [
        "J Inc",
        "Jibo"
      ],
      "year": "2023",
      "venue": ""
    },
    {
      "citation_id": "38",
      "title": "A review and recommendations on reporting recruitment and compensation information in hri research papers",
      "authors": [
        "J Cordero",
        "T Groechel",
        "M Matarić"
      ],
      "year": "2022",
      "venue": "2022 31st IEEE International Conference on Robot and Human Interactive Communication"
    },
    {
      "citation_id": "39",
      "title": "Interrater reliability: the kappa statistic",
      "authors": [
        "M Mchugh"
      ],
      "year": "2012",
      "venue": "Biochemia medica"
    },
    {
      "citation_id": "40",
      "title": "Defrosting the freezer: From novelty to convenience: A narrative of normalization",
      "authors": [
        "E Shove",
        "D Southerton"
      ],
      "year": "2000",
      "venue": "Journal of Material Culture"
    },
    {
      "citation_id": "41",
      "title": "Diffusion of innovations",
      "authors": [
        "E Rogers",
        "A Singhal",
        "M Quinlan"
      ],
      "year": "2014",
      "venue": "An integrated approach to communication theory and research"
    }
  ]
}