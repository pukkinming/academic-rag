{
  "paper_id": "2111.07234v1",
  "title": "Speech Emotion Recognition System By Quaternion Nonlinear Echo State Network",
  "published": "2021-11-14T03:45:43Z",
  "authors": [
    "Fatemeh Daneshfar",
    "Seyed Jahanshah Kabudian"
  ],
  "keywords": [
    "Echo state network",
    "Quaternion algebra",
    "Bilinear filter",
    "Speech emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Echo state network (ESN) is a powerful and efficient tool for displaying dynamic data. However, many existing ESNs have limitations for properly modeling high-dimensional data. The most important limitation of these networks is the high memory consumption due to their reservoir structure, which has prevented the increase of reservoir units and the maximum use of special capabilities of this type of networks. One way to solve this problem is to use quaternion algebra. Because quaternions have four different dimensions, high-dimensional data are easily represented and, using Hamilton's multiplication, with fewer parameters than real numbers, make external relations between the multidimensional features easier. In addition to the memory problem in the ESN network, the linear output of the ESN network poses an indescribable limit to its processing capacity, as it cannot effectively utilize higher-order statistics of features provided by the nonlinear dynamics of reservoir neurons. In this research, a new structure based on ESN is presented, in which quaternion algebra is used to compress the network data with the simple split function, and the output linear combiner is replaced by a multidimensional bilinear filter. This filter will be used for nonlinear calculations of the output layer of the ESN. In addition, the two-dimensional principal component analysis (2dPCA) technique is used to reduce the number of data transferred to the bilinear filter. In this study, the coefficients and the weights of the quaternion nonlinear ESN (QNESN) are optimized using genetic algorithm (GA). In order to prove the effectiveness of the proposed model compared to the previous methods, experiments for speech emotion recognition (SER) have been performed on EMODB, SAVEE and IEMOCAP speech emotional datasets. Comparisons show that the proposed QNESN network performs better than the ESN and most currently SER systems.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Reservoir computing (RC),  [1]  is one of the most widely used and effective methods in training recursive neural networks (RNNs). These networks have a dynamic repository that can be easily used to process complex data. An RC network usually consists of three components: an input layer, a large RNN layer (called a reservoir), and a linear output layer in which the weights of the input layer are hidden and the weights of the hidden layer (return weights) are randomly assigned. They become constant throughout the learning period. Repository computing networks can avoid the complex RNN training process, which is based on gradient descent. While learning the dynamic state generated from the recursive hidden layer, performing well in identifying nonlinear systems, signal processing systems and time series prediction systems  [3, 4] .\n\nESNs are the most popular RC networks due to their relative simplicity and strong theoretical reasoning  [6] [7] [8] [9]  and have had many successful applications so far  [10] . The reservoir in the echo state network is initialized with sparse connections and limitations in the spectral radius, which ensures long-term and rich echo sate dynamics  [9, 1, 11] . This repository can be viewed as a nonlinear time core that can map the sequence of inputs to a high-dimensional space. In this learning space, it will be a simple linear regression operation from the repository to the outputs. Hence, ESNs are a powerful and efficient tool for dynamic display. However, many existing ESNs have limitations to the proper modeling of large data sets. The most important limitation of these networks is their high memory consumption due to their reservoir structure, which prevents the increase of reservoir units and maximum use of capabilities of this type of network. One way to solve this problem is to use quaternion algebra and Hamilton's multiplication for network data. Since quaternions have four different dimensions, using the Hamilton's multiplication property, they can easily display compact data with high dimensions and discover the relationships between features with fewer parameters (less memory consumption). Recently, limited work has been done on the use of quaternion algebra for speech recognition  [12] [13] [14] , and according to the author's knowledge, this is the first time that this type of complex numbers has been used in the field of speech emotion recognition. Although quaternion algebra has already been used in echo state network design  [15] , the model presented is based on the use of a nonlinear quaternion activation function with local analytical properties in which the nonlinear gradient descent algorithm is used to find the weights of the output layer. Since in this current paper the output weights are estimated based on a non-gradient way, there is no need for a complex and time-consuming process to produce gradients and to derive derivative conditions of the quaternion activity function. Therefore, in this current research, a simpler concept called split function  [16]  will be used as an activity function, which is actually a function with real value that is applied as a fraction to a quaternion number.\n\nIn addition to the memory consumption problem in the echo state network, in all ESN models presented so far, the linear output of the network creates an indescribable limitation to the ESN processing capability, as it cannot effectively use the higher order statistics of the signals provided by the repository. Therefore, one of the effective methods is to study and propose different schemes for combining the signals provided by the reservoir in order to approximate the desired input behavior and produce output more carefully. In this regard, using a nonlinear structure in the output layer can be a good choice. In this research, a multidimensional bilinear filter will be used in ESN network for nonlinear output layer calculations based on quaternion algebra. In addition, in this study, before transferring the network reservoir state to a multidimensional bilinear filter, using the two-dimensional principal component analysis (2dPCA) technique, we prevent the number of bilinear filter coefficients from increasing too much and its complexity from increasing.\n\nThe method proposed in this paper has been investigated in a SER system. In this system, after extracting short-term features, statistical data are used to find related frame features, taking into account that the extracted features are at the frame level, while the evaluation of SER systems is usually based on emotional speech. In this paper, the long-term features of mean, standard deviation, skewness, and kurtosis (which well-describe four different perspectives of a signal) are considered as different dimensions of quaternion numbers to discover both the internal relations between these statistical data and the external relations between different time frames by Hamilton's multiplication. In addition, the bilinear filter coefficients as well as the weights of the QNESN network are optimized using the training set features and the genetic algorithm. Then this model is used to emotionally classify the features of the test set. Finally, in order to prove the effectiveness of the proposed QNESN model compared to the previous methods, experiments have been performed on three well-known sets of emotional speech. Comparisons show that QNESN generally performs better than the simple ESN as well as most recently SER systems.\n\nConsidering the above considerations, the contributions made in this paper can be summarized as follows:\n\n1) Using the features of Gabor filter bank (GBFB)  [17] , single frequency cepstral coefficient (SFCC)  [18]  along with glottal waveform, which have not been used in recognizing speech emotions. These features are suitable for analyzing speech features, improving the results of speech / non-speech classification, providing spectral-temporal contrast at all times, reducing redundancy, better recognizing speaker-related emotions, better recognizing similar emotions, and distinguishing distinct emotions. 2) Introducing a new model of quaternion nonlinear echo state network (based on split function as activity function) for the first time in this paper, which uses quaternion algebra, will have a more compact representation of data and reduce the problem of high storage memory consumption. In addition, using its simple activity function, it will provide a simpler model for discovering relationships between input data with a smaller number of parameters. 3) Introducing a new quaternion of speech emotional features as the input of the echo sate network, which incorporates four different perspectives of speech time frames and is used for the first time for emotion recognition applications. 4) Introduce a new multidimensional bilinear filter design for the first time in this paper and use it in the output layer of the ESN network to maintain the nonlinear nature of the reservoir and present it to the output, which will increase network performance with fewer neurons, reduce complexity and simplify the mathematical model. 5) Using evolutionary algorithms to optimize all parameters of the quaternion nonlinear echo state network (since the usual methods of inverse matrix, etc. cannot be applied). 6) Reduce the dimensions of quaternion features by using two-dimensional principle component analysis that will reduce computations and increase system accuracy.\n\nThis article is organized into nine sections. The second and third sections provide an overview of current and background research. Section 4 gives a general description of the designed system. Section 5 describes how to classify using a simple echo state network, a nonlinear ESN, and a quaternion nonlinear ESN. In the sixth and seventh sections, we have evaluated and analyzed the proposed model, and finally in the eighth and ninth sections, we have discussed and concluded in this regard.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "2. Related Works",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "2.1. Quaternion Networks",
      "text": "Due to the ability of quaternion algebra to reduce network parameters and discover more connections between input features using Hamilton's multiplication, many recent studies have been done using it in the design of neural networks, deep networks, recurrent networks, etc.  [12, 14, [19] [20] [21] [22] [23] [24] .  Arena et al. (1994)  first introduced the quaternion neural networks (QNN) with a special algorithm for learning it effectively in the same way as real neural networks. Following this proposal, many researches have examined the main features of QNNs and focused on each of its main components such as activation functions  [25] , error functions  [26] , initialization of parameters  [21]  and their architectural development for better performance in the field of quaternion multidimensional data is suggested. Because quaternion numbers are overly complex numbers that contain a real component and three separate imaginary components, they can be suitable for displaying three-and four-dimensional feature vectors such as channels (R, G, B) in image processing. For this reason, in the field of image processing, many articles have been done using quaternion algebra  [19, 21, 27, 28]  in which the dimensions of quaternion numbers are actually properties related to the color part of the image.\n\nIn the field of speech processing, the use of quaternion algebra is very new and so far few studies have used it. In  [14]  a long-term quaternion-based recursive neural network for speech recognition has been proposed in which the MFCC property, along with its derivatives, form the various dimensions of a quaternion number. In  [20]  a quaternion neural network is designed to detect speech across multiple channels, in which each microphone is represented by one of the dimensions of the quaternion number.\n\nIn addition, a deep quaternion-based neural network has been designed to understand spoken language, and in  [12]  a quaternion evolutionary neural network has been used for automatic speech recognition.\n\nOne of the challenges of recent research in the field of SER is to increase the complexity of the proposed models due to the large dimensions of the feature vector. The use of quaternion algebra in neural network architecture can well reduce the complexity of models by reducing the number of parameters (reducing the size of matrices by a quarter) and ultimately have better performance. Quaternion algebra has already been used in echo state network design  [15] . The model presented in this paper is based on the use of nonlinear quaternion activation function with local analytical properties in which the nonlinear gradient descent algorithm is used to find the weights of the output layer. In this model, due to the use of gradient-based methods, there will be a need for a complex and time-consuming process of gradient production and establishment of derivative conditions of the quaternion activity function.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Speech Emotion Recognition Systems",
      "text": "Recently, many studies have been published in the field of speech emotion recognition systems. In the study presented by  [29] , two models with Gaussian radial basis function kernel (GRBF) and a linear kernel with binary tree and a combined smooth maximum regression model for emotion classification are proposed. In  [30]  a semi-regulatory feature selection method is used to reduce the dimension of emotional features. The long-term properties of acoustic features have been investigated by  [31]  using the modulation filtering method and the excitation source component related to the speech stimulus component. The model presented by  [32]  uses both vocal and spectral features of speech, along with a simple Bayesian classifier. In  [33] [34] , the residual sinusoidal amplitude and normalized cepstral coefficients are also used as emotional features. Researchers have also used speech and sound quality features as emotional features  [35]  and in the model proposed by  [36] , a randomized deep belief network (RDBN) as a deep neural network (DNN) to achieve high-level features form low-level features have been used, and finally  [37]  has proposed new emotional features based on the energy content of the wavelet-based time frequency distribution. In  [38]  provides a semi supervised generative adversarial network (SSGAN) for extracting and recognizing emotion from labeled and unlabeled data, and  [39]  uses an adversarial network with an auto-encoder as a classifier of emotional features. In addition,  [40]  has provided an adversarial auto-encoder to solve the problem of lack of emotional data and lack of proper labeling.\n\nMeanwhile, some studies have used wavelet transform to better detect speech emotions. For example, in the model presented by  [41]  using wavelet packet analysis, new features of speech signal have been extracted to detect speech emotions. In  [42] , the features extracted using the triangular filter bank and the equivalent extracted coefficient are used to detect the emotion. Research  [43]  has used a weighting method to find features related to different emotions and inspired by feature selection methods such as mRMR and ReliefF, and has also used deep learning to categorize features. In the architecture proposed by  [44] , the vector with high dimensions of emotional features including fundamental frequency, zero cross rate, MFCC, energy and harmonic noise ratio is divided into different subsections using C-means fuzzy (FCM) clustering algorithm. They are categorized using multiple random forest algorithm. In addition, research  [45]  has used the active feature selection method to select effective emotional features and has shown that the selection of effective features will have a good effect on the accuracy of emotion recognition systems. In the model presented by  [46] , the performance of two different types of classifiers in SER systems are compared with each other. In model  [47] , an ensemble learning model random forest algorithm is used to find the importance of different features. In this method, the weighted binary cuckoo algorithm for speech feature selection is used to select the features and also uses the decision tree classifiers, linear differential classification, random forest and SVM. In the research presented by  [48] , the proposed method of discriminative non-negative matrix factorization (DSNMF) has been used to reduce the dimensions of input features. In the model proposed by  [49] , the Hilbert-Huang-Hurst coefficient vector (HHHC) is used as one of the nonlinear features of the audio source, due to their effect on the speech production mechanism, to better display emotional states. GMM, HMM, DNN, CNN and CRNN have also been used as categories.\n\nMost of the mentioned methods have used deep learning and generative adversarial networks in their proposed models. The main advantage of deep learning methods is the increased accuracy of the system in recognizing emotions. But the lack of emotional speech data will overfitting deep networks. Also, the large number of parameters for setting and initialization, as well as the high learning time, are other limitations of the methods. Although generative adversarial networks are highly usable and useful for low-data set problems using additive data properties, their limitations include vanishing gradient, high training time, and the problem of non-convergence and instability in all structures. Table  1  summarizes the recent methods performed in the SER domain. In addition, Figure  1  shows the recognition rate of the recent methods according to their learning system on the EMODB dataset. Use the semi-supervised generative adversarial network as classifier Zhao 2020,  [38]  83.31 84.49 User the generative adversarial network and auto-encoder as classifier Yi 2020,  [97]  66.70 N/A Use the adversarial auto-encoder for feature discriminative recognition Latif 2020  [40]  79.20 N/A Use wavelet analysis to extract features Wang 2020,  [41]  N/A 77.08 Feature extraction using a triangular filter bank Sugan 2020,  [42]  N/A 81.80 Use Hilbert-Huang-Hurst coefficients to extract features Vieira 2020,  [49]  76.90 N/A Use the active feature selection method to select the feature Haider 2020,  [45]  N/A 72.19 Use the feature weighting method based on emotional groups to select the features Li 2021,  [67]  N/A 85.61 Use the C-means fuzzy clustering algorithm to select feature Chen 2020,  [44]  86.19 N/A Use of robust differential sparse regression to select differential features",
      "page_start": 3,
      "page_end": 5
    },
    {
      "section_name": "Background",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Quaternion Algebra",
      "text": "In mathematics, quaternions are a numbering device for expanding complex numbers. They were first introduced by the Irish mathematician William Hamilton in 1843 and applied to mechanics in threedimensional space. The set of quaternions H is equivalent to R 4 , a four-dimensional vector space on real numbers. Thus, each element H can be uniquely written as a linear combination of these basic elements;\n\nWhere a, b, c and d are real numbers. The basic element 1 is the identical element H. In fact, each quaternion number, unlike real numbers, has four different dimensions, so it is easy to package multidimensional input features. This will reduce the quaternion grid parameters by a quarter. In this network, each unit will contain four times the unit of information in real networks. H has three addition operations, scalar multiplication and quaternion multiplication. The sum of two elements H is defined as the sum of them as elements R 4 . Similarly, the product of an element H in a real number, like the product of a scalar, is defined in R 4 .\n\nThe quaternion multiplication (Hamilton multiplication) is indicated by the symbol ⨂ and for the two quaternions Q1 and Q2 will be as follows:\n\nOne of the characteristics of quaternions is that the multiplication of two quaternions has no displacement properties. But it has the property of participation and distributability. The Hamilton multiplication, due to its distributive nature, makes it possible to discover the hidden internal relationships between the components of a quaternion. In addition, it helps to encode the interdependence between input features with fewer parameters, so the use of quaternion algebra in highdimensional space will be very cost-effective.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Cost Function In Quaternion Algebra",
      "text": "In  [16]  there is a concept called split function, which is actually a function with real value that will be applied to a quaternion number separately ,\n\nArena, 1993 proved that the theorem of global approximation when using the split function in quaternion space is as true as the real number space  [16] . Also in  [26]  a function called QMSE or mean square error in quaternion space is presented as follows and based on the split function introduced in  [16] .\n\nWhere Y is the output vector of the quaternion and t is the target quaternion class of the destination. Most quaternion space classification articles  [22, 26, 50]  have used the QMSE function to find the best Y quaternion vector class. In this research, the proposed method in  [50]  has been used to find the output class .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Echo State Network",
      "text": "The echo state network is a recursive neural network with three layers: an input layer, a large recursive layer (reservoir) with fixed sparse hidden-to-hidden connections and an outer layer  [7] . The overall structure of the ESN network is shown in Figure  2 . In this study, we name the number of input dimensions with , we also assume that the repository has nerve cell (return unit). In addition, ⃗ ( ) represents the input at time t (according to the following relation),\n\n(1) ( )\n\nWhere T is the number of input samples. Also ⃗ ( ) is the state of the reservoir at time t. According to the following relation ,\n\n(1) ( )\n\nTherefore, for a simple ESN, the relations would be as follows  [51]  ، ( ) ( ( ), (  1 ) In relation  (8) , a∈ [0,1] leaking rate and f are the activation function for reservoir units (typically, the non-linear tanh function is used). In the standard RC framework, the reservoir parameters, i.e. the input weights and the reservoir weights W, are obtained according to relations (9) and  (10) . These coefficients are left without training after initialization and under the stable conditions presented in  [51]  by analyzing the eco state specifications for different reservoir .\n\n,\n\n(1) ( )\n\n(1) ( )\n\nIn relation to the output calculation, at each time step t, the state of the reservoir feeds the output layer (Figure  2 ). If we denote the size of the output space by , the output in the time step t is calculated by the following linear function :\n\nAs is a readout weight matrix that is adjusted on the training set, and by methods such as inverse matrix or linear regression .",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Nonlinear Echo State Network",
      "text": "Due to the limited efficiency of ESN network with linear output, one of the effective methods is to study and propose different schemes to combine the signals provided by the reservoir, in order to approximate the input behavior and produce the desired signal more accurately for the output layer. In this context, the use of a nonlinear structure in the output layer can be a good choice (Figure  3 ). This has been used by  [52] [53]  in which, the Volterra filter, is used as a nonlinear filter in the output layer. As shown in relation  (12) , the Volterra series is a suitable method for approximating any function using N of its previous example and nonlinearly. The higher the value of N, the closer this approximation will be to reality. N actually represents the amount of memory or delay of the filter, and the order of the filter, which is , is the number of sentences that have been multiplied consecutively, and h is an dimensional function, which is the Volterra filter kernel .\n\nThe most important feature of this filter is that the mapping of a Euclidean space to a scalar space, as well as the output at any time, is dependent on all previous inputs and makes it possible to use higher statistics. But one of the most important problems is that finding its kernels is complicated and time consuming and requires a lot of calculations. In addition, to estimate the nonlinear function of the Volterra filter requires the estimation of more kernels, which doubles its complexity. In 2005, to reduce the computational complexity of the Volterra filter, an adaptive bilinear filter was introduced by  [53]  (relation 13), in which output y(t) using the previous N-1 output, and previous input x(t), with the product of both will be achieved.\n\nThis filter, unlike Volterra, which uses only the feedforward coefficients, will use both the feedforward coefficients (matrix a) and the feedback coefficients (matrix b). In  [53]  it is proved that all nonlinear systems can be estimated with a good approximation using a bilinear filter and with a lower order than the Volterra filter. Figure  (4)  shows a one-dimensional bilinear filter in which, according to the relation shown, the N is order of the filter and its amount of memory, and the coefficients a, b and c are usually obtained using the least squares error method. The use of bilinear filter in the output layer of the ESN network will not only avoid the problems of designing the Volterra filter, but will also be able to effectively use the higher order statistics of the features provided by the reservoir. To the best of our knowledge, this has never been suggested by anyone. So far, various bilinear filters with different applications have been proposed. Many of these models are suitable for one-input-one-output dimensions  [54, 55] , while the SER domain is a multi-dimensional input-output space. Previously, different models based on the definition of recurrent neural network with bilinear filter in a multi-input-multi-output (MIMO) system have been proposed and used in many systems  [56] [57] . In this paper, for the first time, the design of a bilinear filter with multidimensional input-output has been done.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Overall System Description",
      "text": "As shown in Figure (  5 ), the SER system proposed using the proposed QNESN model consists of two main sections: the preprocessing and feature extraction section, and the feature classification section.\n\nIn the preprocessing phase, silent intervals are detected and eliminated. Then, the Cepstral coefficient of perceptual linear prediction (PLPC), the SFCC  [18] , and their first-and second-order derivatives, along with the GBFB  [17] , are extracted from both speech signals and glottal waveforms. They are also suitable for analyzing speech characteristics, improving the results of speech / non-speech classification, providing Spectro-temporal contrast at all times, reducing redundancy, better recognizing speakerrelated emotions, better recognizing similar emotions, and recognizing distinct emotions  [17] . Because glottal waveform features are significantly affected by a person's emotional state and speech style  [58] , in this study, glottal waveform features were used to achieve a more accurate emotional classification.\n\nThen, using the training set features and genetic algorithm, the coefficients of the proposed QNESN network are obtained, and finally, this network estimates the emotional class of the experimental speech signal. In the following, the details of each of these sections and how the QNESN model is designed will be described.\n\nFigure  5 . Overall description of the speech emotion recognition system using QNESN",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Emotional Features Classification",
      "text": "This section explains step-by-step how to design a QNESN classifier. Initially, for better ESN network performance, we created a new connection from input to output (Figure  6 ). Therefore, relation  (11)  will change as follows ,\n\nAnd the network output and output weight matrices are also defined as follows ,\n\n(1) ( )\n\n]\n\n(1) ( ) Table  2  shows a comparison between network performance with the previous simple structure and the new structure in the SAVEE dataset emotion recognition rate with the features extracted in the previous section, along with a reservoir layer of 100 units. According to the results, the ESN network with the new structure has performed better than the previous structure.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Emotional Data Preparation",
      "text": "Because the echo state network is highly efficient as a time series, the features extracted at the frame level will be used in this experiment. The difference is that to reduce the complexity caused by the high number of frames associated with each utterance, another framing (with a larger window length) will be performed to reduce the number of frames associated with each speech utterance .\n\nIf we display the short-term features extracted with the 10ms shift window in the feature extraction section, with a matrix V,\n\n(1) ( )\n\nThen, by applying the statistical functions (medium-term features) of the mean, standard deviation, skewness and kurtosis with 2W frame window length and K frame shift window, on the V vector, the final input vector will be obtained according to the following relation :",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Echo State Network Hyperparameters",
      "text": "Important hyper-parameters used to initialize an ESN network are:\n\n Input Scaling (IS) -The input scale is used to generate the initial random values of the input matrix, so that the random values of its elements must have a uniform distribution between -IS and IS.  Spectral Radius (SR) -The spectral radius is used to generate the initial random values of the reservoir weight matrix (W) using the following relation:\n\nIn this relation, the matrix W has random elements generated in the interval [-1 1], where ( ) represents the largest eigenvalue of that matrix. In addition, to ensure that the echo state is maintained, the spectral radius takes values less than one.\n\n Leaking Rate (LR) -Determines the velocity of the reservoir in response to the input.  Regularization coefficient -(C) According to the theory  [59]  of the classification problem in relation  (11) , using the regularization coefficient C in the regularization formula introduced by  [59]  will improve the generalization efficiency of the ESN network.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Nonlinear Echo State Network Using Multi-Dimensional Bilinear Filter",
      "text": "Previously, bilinear filter relationships in one-dimensional input-output environments have been introduced in relation (  13 ). Since the input-output of SER systems is multidimensional, to use a twoline filter, its relationships must be defined in multidimensional. The model presented by the simple ESN network using nonlinear output will be in Figure  (7) . In the proposed model, in the output layer, the reservoir state will be given to a multidimensional bilinear filter to obtain the final output using its coefficients. In this way, the nonlinear properties of the reservoir will be preserved and used to produce the best possible output. In this case, all the relations of the echo network will be the same as before (relations 6 to 11), except that the output will be based on relation  (20)  instead of relation  (11) . In this multidimensional model, each of the output dimensions y(t) will be obtained from the sum of N-1 of the previous output sample at that dimension, along with the reservoir state signal x(t), and the product of the two .\n\n: filterorder\n\nThis relation will be vectorized as follows ,\n\nThe filter input matrix is obtained from relation  (7) , where ( ) is the i input (echo state) of sample t. Also, the output of the filter according to relation (  15 ) is defined as vector as follows,\n\n... (\n\nWhere T is the number of input samples and is the number of output dimensions (y dimensions). Finally, the coefficients of the bilinear filter will be as follows (feedforward coefficient for all output dimensions and reservoir units) ,\n\n...\n\nCross product coefficients (for output dimensions of p, filter order N and number of reservoir units ),\n\nFeedback coefficient ,\n\n...\n\nThus relation (  21 ) can be written as follows ,\n\nWhich,\n\nThen relation  (26)  will change as follows , ( ) ( )\n\nSo far, various algorithms have been used to solve relation  (30)  and obtain filter coefficients ( matrix).\n\nAn example of a simple multidimensional bilinear is shown in Figure  (8) . In this bilinear filter, the input is two-dimensional ( = 2) and the output is two-dimensional (p = 2) and the order of the filter is two-dimensional (N = 2). So, the output will be as follows ,\n\nAnd the coefficients are as,\n\n. The structure of a multidimensional bilinear filter",
      "page_start": 11,
      "page_end": 14
    },
    {
      "section_name": "Quaternion Nonlinear Echo State Network",
      "text": "One of the major problems in the design of SER systems is their high memory consumption due to the extraction of features with high emotional dimensions. Since the ESN is recursive, the weight matrix of the reservoir layer will increase in size as the reservoir units increase. The larger the number of reservoir units, the more memory is required. In addition, the larger the input size of the ESN, the larger the number of reservoir units must be to better model it. One of the available solutions to this problem is the use of quaternion algebra or high-dimensional algebra. In this study, the mean-term features of mean, standard deviation, skewness and kurtosis will be considered as different dimensions of quaternion numbers. If quaternion algebra is used in the design of a nonlinear eco-state network to detect emotion, in this architecture all input, output, coefficients and weights matrices will be quaternion (Figure (  9 )) . In this case, the matrix of input features is as follows, each dimension of which will include one of the mid-term properties.\n\n) 31 (  1 1\n\n(1) ( )\n\nIn addition, the reservoir state matrix will be defined as follows in the quaternion state.\n\n(1) ( )\n\nThe output matrix of the model in quaternion mode will be as follows,\n\n,\n\nAlso, the matrix of input weights and the state network reservoir in the quaternion space are defined as follows :\n\n,\n\nIn addition, the matrix of bilinear filter coefficients in the quaternion space will be as follows:\n\n1,1\n\n[ ,..., ]\n\nof the echo state network will change in the form of the following relation in the quaternion state, so that the reservoir state matrix is obtained in this way ,\n\nIn this regard, the tanh function is a split function and, as used in  [14, 20, 21, 27, 50] , will be applied to all dimensions of the quaternion number separately. In addition, the output of the Quaternion nonlinear echo state network after passing through the bilinear filter will be calculated using the following relation:\n\nThe previous real multiplications are also replaced by the Hamilton multiplication in the following order :\n\n)",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "Quaternion Nonlinear Echo State Network By Two-Dimensional Principle Component Analysis",
      "text": "One of the limitations of the bilinear filter is the high dimensions of matrices A and B, which are completely dependent on the number of reservoir units. To reduce this complexity, as in the method proposed in  [52, 53] , the dimensions of the input signal ([ ( ); ( )] X t U t ) to the bilinear filter can be reduced by a method such as principal component analysis, which ultimately reduces the dimensions of matrices A and B. Principal component analysis is one of the methods that has been successful in processing information and reducing dimensions; But to apply this algorithm to quaternion matrices, they must be formed, which in turn eliminates the spatial correlation of adjacent data. This problem already existed when applying principal component analysis to image matrices. To solve this problem in the field of image, two-dimensional principal component analysis  [60]  has been proposed that does not require the process of converting an image or matrix to a vector. Since quaternion data are also three-dimensional matrices and require a similar image-to-vector conversion process to reduce the dimension by principal component analysis, two-dimensional principal component analysis can also be used to reduce the dimension in these data. The corresponding pseudocode is given in algorithm (1). In addition, in this algorithm, the implementation of quaternion nonlinear echo state network by two-dimensional principal component analysis method, the coefficients of which are obtained using genetic algorithm, is also shown. In this section, due to the stated limitations, the dimensions of the bilinear filter input matrix will be reduced by using the dimensional reduction method. In this case, the ESN will be as shown in Figure  (10) . In addition, the previous relations from relation (32) to (36) will remain unchanged, except that relation (37), (  38 ) and (  40 ) will change as follows:\n\n, :filter order 7: for = 1 ∶ do // for all samples belonging to X' 8: for = 1 ∶ do // for all output dimension. 9:\n\n) In the proposed QNESN model, each quaternion property vector (both train and test data) will have dimensions of 554. Each dimension of a quaternion property is equivalent to one of the mid-term properties (mean, standard deviation, skewness, and kurtosis). These attributes apply to the extracted short-term features in the feature extraction section. In addition, in this algorithm, all parameters of the QNESN network, including the input weight matrix (relation 35), the reservoir weight (relation 36) and the matrix of linear filter coefficients (relations 45, 46 and 39) in the quaternion space will be defined as θ vector .",
      "page_start": 17,
      "page_end": 18
    },
    {
      "section_name": ", , , ,",
      "text": "This vector is optimized by the genetic algorithm. The initial population of the GA includes the random values in the range [-1 1]. This value is such that the echo state network retains its echo property, so the values given to the reservoir weight matrix (W) must be true in condition  (14) . In genetic algorithm, a QNESN network is designed with this population member to find the suitability of each member ( ) of the generation (including ESN weights and bilinear filter coefficients) and its output is evaluated according to the training data compared to the desired output. For this evaluation, for each of the speech formats belonging to the training set, the average of all the frames related to that speech at the output of the network is obtained as follows, and then this average is scale to [0 1].\n\n2 mean( ( : ))\n\nWhere Y is the output of the QNESN network and k1 to k2 are the speech frames of , so the matrix is a matrix with dimensions × 4 which is passed to the interval [0 1]. Then, using the QMSE algorithm presented in  [16]  the emotional class of that speech format, is obtained using the following relation ,\n\n[ ,..., ] QMSE( ,..., , )\n\n1 arg max( )\n\nWhere the set [ × , … , × ] contains labels for each output class defined as a quaternion matrix. For example, in the first-order quaternion matrix, only the first row has values of one, and the other objects are zero.\n\nThen all the labels found by the network are compared with the desired output and the correct detection rate is measured. After completing the implementation of the genetic algorithm, and estimating the optimal values of weights and coefficients ( ), using the best value obtained for these coefficients, a quaternion nonlinear echo state network is designed whose performance is tested by the data and pointed in the same way. The final performance will actually be the recognition rate of network on the experimental data set.",
      "page_start": 20,
      "page_end": 21
    },
    {
      "section_name": "Experiments",
      "text": "All SER systems require an emotional database to evaluate their performance. Following the emotional databases properties used, will be described.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Emotional Datasets",
      "text": "Many emotional databases have been designed to test the performance of SER systems. In this study, three common databases including Berlin Database of Emotional Speech (EMODB)  [61] , Surrey Audio-Visual Expressed Emotion (SAVEE)  [62] , and the Interactive Emotional Dyadic Motion Capture (IEMOCAP)  [63]  were employed to evaluate the effectiveness of the proposed system, whose specifications are listed in Table  3 .",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "1-",
      "text": "",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Emotional Data Preparation",
      "text": "The results obtained using the new frame size (with 2W window length and K-frame shift window) and the application of mid-term properties are shown in Figures  (11 and 12)  for two different emotional databases. According to the presented results, the best value for window length was 40 frames and shift value was 10 frames for SAVEE database and window length was 40 frames and shift value was 30 frames for EMODB database. The feature vector extracted in this case has 2216 dimensions .",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Network Hyper-Parameter Regularization",
      "text": "With increasing depth and more complex structures of reservoir calculation models, optimization of the hyperparameters of these networks becomes more important. In this paper, various experiments have been performed to adjust the different values of C, SR, IS and LR hyperparameters that have a significant effect on the designed network performance. For convenience and speed of experiments, a simplified model of Figure  (6)  with a reservoir layer consisting of 100 neurons is provided .\n\nIn this experiment, different values of C are selected from the range  [10 … 10 ]  and the values related to each of the three parameters SR, IS and LR are selected from the range [0… 1]. All tests are performed on SAVEE database. According to the experiments performed, the best values obtained for these parameters are shown in Table  4 . Table  4 . Optimal values of ESN hyperparameters = . = . = . =",
      "page_start": 22,
      "page_end": 22
    },
    {
      "section_name": "Qnesn Model Experiments",
      "text": "To evaluate and validate the performance of the proposed QNESN model, several experiments have been performed to detect speech emotion in the introduced emotion databases. In this study, the results of these experiments are presented in the leave one speaker out strategy (LOSO) through which many recent studies have evaluated their systems. According to this strategy, the emotional database is divided into two different parts, in which the training and experimental datasets are completely different, and the experimental database consists of only one speaker (i.e., a gender) who is not present in the training database. Therefore, network training and optimization is performed automatically on the sentences of the speakers of the training set and the recognition rate and network performance are tested on the speaker of the experimental set. After repeating the process for the total number of speakers, finally, the network performance for all speakers will be equal to the average recognition rate of different experiments .\n\nIn addition, this paper uses the weighted average recall (WAR) and non-weighted average recall (UAR) criteria to measure the recognition rate of experiments. The criterion for calling the weighted average or the micro average, as the recognition rate of the weighted average classes with the previous class probability, and the non-weighted average or the macro average calling, is the average recognition rate of all classes without weighting. The relationship between these two criteria is as follows,\n\nWhere M is the number of emotions, and , denoting true positives and false negatives for emotions i and , respectively, represents the total number of samples of all emotions. To evaluate the performance of the proposed nonlinear model, the best value for the linear filter order (N) -which will have a significant effect on system performance -was first obtained during experiments on the SAVEE emotional database in the form of LOSO. In this experiment, a nonlinear echo state network (NESN) with 100 reservoir units and a bilinear filter with different levels has been used, the relationship of which has already been shown in  (20) . Bilinear filter coefficients (relations 23, 24 and 25) are obtained using genetic algorithm (the range of all coefficient values are considered in [-1,1]). The final results can be seen in Table  (5) . According to the obtained results, the 3-biline filter has a higher efficiency than other filters. Experiments have also been performed to better and more accurately investigate the effect of the bilinear filter used in the output layer and the quaternion algebra in the ESN network. In this experiment, four different networks ESN, NESN, QESN and QNESN with filter order 3 and a number of different reservoir units are designed in which the values of weights and coefficients are optimized using genetic algorithm. In this optimization, which is done as LOSO, for each fold of the SAVEE dataset, a genetic algorithm is implemented based on the training data of that fold, and finally, using the optimal values of the obtained coefficients, the performance of the designed model is measured by those coefficients. The final efficiency will be equal to the average efficiency of different folds. Figure  (13)  shows the average and best value of the training cost functions of each generation of the genetic algorithm in successive iterations for one of the folds of the SAVEE database. In all experiments, each generation of the genetic algorithm has 500 members. Other characteristics of are shown in Table  (6) .  Table  7  shows the performance of five different networks for four different modes. In each part of the tests, the number of reservoir neurons, the size of the reservoir matrix, the size of the input signal at the output layer ( + ), the dimension reduction number, vector Ɵ and its size, recognition rates (UAR and WAR) and the average execution time for an audio file are shown. These experiments were performed in four different groups with different reservoirs. In all groups, the reservoir output will be given to the output layer after applying the reduction algorithm (PCA in real mode and 2dPCA in quaternion mode). To make the correct comparison in each group, the networks were tested under the same conditions. However, due to the fact that the network was a real or quaternion type, the dimensions of the input data will be different in the final layer. In addition, except for the first network, in which the weight of the output layer is calculated using the usual inverse matrix method, other networks use genetic algorithms to calculate weights and coefficients. Therefore, according to the design of each network, the vector θ and its size are different. The first network is the simple ESN in which the input weight matrix ( ) according to relation  (9)  and the reservoir weight matrix (W) according to relation  (10)  are fixed and the output weight matrix ( ) according to relation  (11)  is calculated using the inverse matrix method. In the second case, all weights of the input matrix, reservoir matrix, and output matrix of the ESN network are optimized using genetic algorithms. In this case the vector θ is obtained from the following relation ,\n\nIn the third network, which is NESN, only a bilinear filter is placed in the output layer, so not only the weights of the input matrix and reservoir matrix, but also the matrix of filter coefficients will be optimized using the genetic algorithm. Therefore, the vector length θ in this case will be longer than the normal ESN network and also due to the use of a bilinear filter in the output layer, it will have a higher execution time than the linear ESN network and its recognition rate is higher than the previous mode.\n\nThe size of vector θ in this case is also obtained from the following relation:\n\n) ( (  1 ))]\n\nIn the fourth case, the QESN quaternion network is tested. This network has a linear output layer, but all network weight matrices are defined as quaternions. Therefore, the length of θ will be longer than the previous three cases. The size of vector θ in this network is obtained from the following relation:\n\nIn the last case where the QNESN network is tested, a nonlinear ESN network with quaternion outputs is used, so the values of all matrix filter coefficients and network weights are optimized by the genetic algorithm, and the vector θ will be longer than in other cases.\n\n] According to the descriptions, due to time-consuming Hamiltonian multiplication operation, in each group, the highest execution time is usually related to quaternion networks and the lowest execution time is related to the simple ESN network. In each group, due to the properties of the bilinear filter, the use of nonlinear network has produced better results compared to the linear network, and this filter has done the classification of features at the output more accurately. The use of quaternion numbers has also produced more accurate results than a real-valued network. In this table, in each group, the maximum values related to the length θ, the maximum execution time, and the maximum values of UAR and WAR are specified. Thus, the highest amount of WAR obtained using QESN and QNESN networks with 100 and 200 reservoirs is 65.56% and 66.67%, respectively, and the highest recognition rates are obtained for ESN and NESN networks with 100 and 200 reservoirs, which are 62.27% and 63.31%, respectively. According to the results, QNESN network is more efficient than other networks, due to the nonlinear structure used in the output, the use of Hamiltonian multiplication to discover relationships between different data and optimization of network coefficients using GA.\n\nIn a more detailed comparison, shown in Table  8 , the real and quaternion networks with equivalent reservoir sizes are in the same group. In the second category, the real networks ESN and NESN with 200-reservoir with their equivalent quaternion networks (QESN and QNESN) with 50-quaternion reservoir (equivalent to 200 real reservoirs), in terms of memory consumption (θ vector length) and recognition rate (WAR) have been compared. In this group, QNESN and QESN quaternion networks with less memory consumption produce better results compared to real networks. In the third group, with 400 real reservoir units and 100 quaternion reservoir units, QESN and QNESN quaternion networks have been able to produce better results compared to real networks with less memory consumption (θ length). In addition, in the last group (800 real units and 200 quaternion units), quaternion networks have been able to have this advantage over real networks in both higher accuracy and lower memory consumption. In fact, in the comparison made in this table, the efficiency of using quaternion algebra and bilinear filters in increasing the accuracy and precision of the SER system with less memory consumption compared to its real equivalent networks can be seen. Of course, this increase in accuracy has been further improved by increasing the size of the reservoir compared to real networks, and at lower dimensions (group 1 in Table  8 ), real networks have performed better than quaternion networks (see Figures  14  and 15  for further comparison). In addition, in the last row of the table, the effect of using dimensional reduction using the two-dimensional principal component analysis algorithm is clearly visible. It can be concluded that the use of 2DPCA reduction algorithm has a positive effect on the overall results of the SER system .\n\nTable  9  shows the amount of memory consumed in the process of optimizing the parameters of ESN, NESN, QESN and NQESN networks. This table shows the length of each chromosome (according to the size of the θ vector) as well as the amount of memory consumed by the genetic algorithm per population of 500 (one generation). In addition, the specifications of the system on which the tests were performed are also visible. The relationship between the amount of memory consumed during optimization and the final accuracy of the generated SER system is well illustrated in this table. As can be seen, quaternion networks with less memory consumption during the optimization process have ultimately produced higher accuracy (see Figure  16  for further comparison).\n\nTo further evaluate the performance of the proposed algorithm, all previous experiments on the EMODB database have been performed (Table  10 ). Also, the best results obtained for the EMODB, SAVEE and IEMOCAP databases are shown in comparison with other work done so far and in the same conditions in Tables  11 to 13 . According to the results presented in these two tables, the proposed architecture in this section has a higher efficiency compared to many recent works done on SAVEE, EMODB and IEMOCAP databases .",
      "page_start": 22,
      "page_end": 22
    },
    {
      "section_name": "Ga Memory Consumption (Gb) Ga Memory Consumption War",
      "text": "Tables 14 to  16  show the confusion matrices for the best results obtained so far on the EMODB, SAVEE, and IEMOCAP emotion databases. According to the EMODB database confusion matrices, angry emotion with 91.11% accuracy has a higher recognition rate than other emotions. In addition, sad emotion had the highest similarity to bore emotion (compared to other emotions) due to similar vocal characteristics and glottal waveform (90.95% to 6.79%). Also, angry emotion has the highest similarity to happy emotion among other emotions (91.11% to 4.15%). This similarity is also due to the similarity of the prosodic features (both happy and angry emotions have more energy than other emotions) and the glottal waveform for these two emotions. In addition, in the case of the SAVEE and IEMOCAP databases confusion matrices, the most similar emotion to the happy emotion is angry (60.36% to 10.66%) and (61.29% to 15.74%).\n\nCompared to other proposed architectures, the method  [41]  using features based on wavelet packet analysis, could not distinguish seven different emotions from each other in the EMODB database (low recognition rate of fear emotion, 55.07% and neutral 56.96%, compared to the recognition rate of 79.09% and 90.01% in the QNESN method). This also true for the SAVEE and IEMOCAP emotional databases. In addition, in the method presented by  [44] , due to the use of membership algorithm to select appropriate features, the recognition rate of some emotions such as neutral (96.20% vs. 90.01%) and fear (84% vs. 0.09) 79%) is higher than the QNESN method and the recognition rate of some emotions such as disgust (63.20% versus 87.01%), bore (80.6% versus 81.24%) and anger (74% versus 11 / 91%) is less than QNESN method. These results indicate that the effect of glottal wave signal, linear perceptual coefficient and Gabor filter-based features on emotions such as anger, bore and disgust are more than the emotion of neutral and fear. Also, the results of the proposed QNESN network in recognizing the emotions of the SAVEE emotion database and also in recognizing the emotions of the IEMOCAP emotion database have been remarkable compared to many of the results obtained so far and in the same experimental conditions. However, in the architecture presented in this paper, due to the use of Hamiltonian multiplication, the response time of the algorithm is longer than the similar real network. The maximum response time obtained for the EMODB database according to Table  (10)  in this algorithm is 5.745 seconds, which is smaller than the execution time reported by  [125]  on the same database (8.23 seconds). The maximum execution time on this emotional base in the architecture presented using real networks is even less than this value (0.802 seconds). The main reason for the high speed of the proposed emotion recognition system compared to other architectures is the use of fast classifier based on echo state network.  In following, table  (17)  shows the recognition rate of the QNESN network for each of the ten folds of the EMODB database. According to the obtained values, the recognition rate in these experiments has increased well for the emotions expressed by each speaker and with each gender. In fact, the presented architecture has well recognized both the emotions expressed by the female speaker (86.7%) and the emotions expressed by the male speaker (84.08%). In these experiments, the use of SFCC features is very helpful in recognizing the same emotions expressed by different speakers and categorizing them as the same emotion, and finally increasing the average and final recognition rate by increasing the recognition rate in each fold.  Figure  17  compares the method presented in this paper in terms of the execution time (a) and the audio file response time (for one second of audio file) (b) with recent references that have reported these parameters. Meanwhile, in the model presented by  [97] , the execution time of the proposed learning algorithm for the generative adversarial network is 0.07 seconds in each iteration using the graphical processing unit (GPU). For the general implementation of this algorithm, 800 different iteration are required. Finally, the total execution time required for this algorithm is 56 seconds. In the model provided by Air  [126] , the best result provided using the DenseNet201 network requires 364 seconds of training time on the EMODB dataset using the GPU. In the architecture presented by  [127] , the total execution time of the proposed algorithm on the EMODB dataset is 5396 seconds using GPU. In a study done by  [128]  using the GPU, the total execution time of the learning algorithm was 1850 seconds and the response time to one second of the audio file (emotion recognition only) in the experimental data set was 0.4 seconds. In the model presented by  [46] , the best classifier execution time on the processor with specifications (Corei5-8GB RAM) is reported to be 2299 seconds. In the architecture presented by  [44] , the response time for the best results obtained using the multiple random forest classifier was 0.07 seconds (unfortunately CPU specifications were not reported). Ali Bakhshi  [129]  performed all his tests on a processor with specifications (Intel Corei7-4GB RAM) and reported the total execution time on 170 speech utterances from the EMODB database, 5 minutes (the average length of each speech utterance was 2.5 seconds). Finally, the response time for 1 second of the audio file was approximately 0.71 seconds. In the research presented by  [112] , the response time to 1 second of the speech utterance of the EMODB dataset was reported to be 0.4 seconds using a processor with specifications (Corei7).\n\nIn the research presented by  [125] , the execution time of the proposed algorithm on the EMODB data set using a processor (Core E5-48GB RAM) is reported to be 8.23 seconds. Among these, the shortest execution time among all recent models has been obtained by  [97]  and also the shortest response time to 1 second of the input file, among the recent models presented by  [44] . Execution time using the pQPSO dimension reduction algorithm and the GMM statistical classifier (previously published by the authors  [79] ) for each fold of the EMODB database, on a processor with Intel Core i3-8 GB RAM specifications, was 0.75 hours, which is finally, it lasted 7.5 hours on 10 different folds (Figure  17 , part A). The use of GMM statistical classifier with high components (128 components) and the use of the objective function, based on the development dataset recognition rate using GMM, have been the most important reasons for the high execution time of this algorithm. Despite the high execution time, this algorithm has a short response time (Figure  17 , part b). This time was 0.6 seconds for 1 second of each speech utterance. The execution time was 7.64 hours using the pQPSO dimension reduction algorithm and the GEBF statistical classifier (previously published by the same authors  [78] ) on the same processor (Figure  17 , Part A). The use of GMM statistical classifier with high components (1024 components) has been the most important reason for the high execution time of this algorithm. Despite the high execution time, this algorithm has a better response time (Figure  17 , part b). This time was 1.1 seconds for each second of each speech utterance. However, the time required for the QNESN classifier has reached 5 hours on the entire dataset and on the same processor due to parameter optimization using GA and Hamiltonian multiplication, and the response time is 1.52 seconds. The reason for increasing the response time of QNESN compared to the methods  [78]  and  [79]  is the use of Hamiltonian multiplication in MATLAB environment, which is time consuming. Compared to other references, since the implementation and execution environment of the programs are not the same and also experiments have been performed with different processors and in many references, GPUs have been used, different execution and response times have been obtained.",
      "page_start": 23,
      "page_end": 33
    },
    {
      "section_name": "Statistical Analysis Of Recognition Rates",
      "text": "Figure  18  compares the method presented in this paper with other methods presented by the authors of this paper (  [78] [79] ). As it turns out, the highest mean value among the three different emotional databases is related to EMODB and the lowest value is related to SAVEE database. Due to the small number of samples of the training set compared to the samples of the experimental set in the SAVEE dataset (compared to the EMODB and IEMOCAP datasets), a significant decrease in the recognition rate is evident in this database. These results indicate that the mid-term features used in the QNESN method perform better on artificial speech formats (EMODB and SAVEE datasets). However, these features on IEMOCAP speech formats, which include both artificial conversations and improvised scenarios, have not yielded the desired results. In addition, the short-term features used in  [79]  are suitable for longer speech utterances (the length of the speech utterances for the part of IEMOCAP used in these experiments is 4.73 ± 0.18 seconds), while the mid-term features used in the QNESN method are suitable for utterances with shorter average length (EMODB 2 to 3 seconds and SAVEE 3.85 ± 0.33 seconds). Therefore, it can be concluded that the mid-term features, and the QNESN method will produce higher recognition rates in datasets with shorter lengths and fewer samples, but detect and differentiate more emotions (7 different emotions). However, short-term features and  [79]  in the dataset with longer length and more samples but for recognizing fewer emotions (4 emotions) will be better.",
      "page_start": 34,
      "page_end": 35
    },
    {
      "section_name": "Statistical Analysis Of Different Emotion Recognition Rates",
      "text": "Figure  (19)  shows the recognition rates of different emotions for the proposed method and for different emotion sets. According to section a, in the EMODB, angry, sad, and neutral emotions are identified at a higher rate than other emotions due to the large number of samples. This is also evident in section c for the neutral emotion in IEMOCAP. In addition, sad and angry emotions with fewer samples in IEMOCAP had lower recognition rates. According to section (d), the emotions of bore, happiness, sadness and fear with mid-term features and QNESN method had higher recognition rates and neutral, angry, disgust and surprised emotions with short-term features and using method  [79]  had better results. Emotions with instantaneous changes (such as angry and surprised emotions) appear to have higher recognition rates using short-term features and the GMM statistical classifier. Also, the emotions of bore, happiness, sadness and fear are better modeled as time series and mid-term features and its dynamics are recognized by QNESN classifier with higher recognition rate. However, the method presented in this paper does not recognize the emotion of disgust better than other emotions. Because disgust speech has fewer vocal features than other emotions and therefore has few examples in the emotional data collections, then it is more complex than other emotions and requires better classifiers and the use of more relevant features to recognition.\n\nIn general, it seems that the emotional features proposed in this paper have improved the average rate of recognition of different emotions and have well differentiated different emotions from each other. In other words, the proposed method, on average, has been able to distinguish different emotions (in the case of the same speakers and different speakers) from each other, and also to consider well the same emotions expressed by the same speakers and different speakers.",
      "page_start": 36,
      "page_end": 36
    },
    {
      "section_name": "Discussion",
      "text": "In this paper, a new method for designing SER systems based on the QNESN network as a classifier is proposed, which makes the final architecture more accurate than many recently published SER systems. This study uses the features of GBFB, SFCC, PLPC and glottal waveform of speech input signal, which has been effective in reducing redundancy and differentiation between different emotions. In this research, an efficient set of long-term statistics (mean, standard deviation, skewness and kurtosis), are considered as different dimensions of quaternion numbers, each of which evaluates the speech signal from a specific perspective. The nonlinear classifier presented in this paper is a dynamic scalable model that has a high memory capacity for various features. According to experiments, features that pass through a nonlinear output produce better results than features that pass through a linear output structure. In fact, by using the nonlinear filter in the output layer, ESN can classify the input data with less error. This progress has been achieved without increasing the complexity of the filter coefficient training process using the 2dPCA technique. Such nonlinear structures are better suited to time series problems as well as large nonlinear problems such as emotion recognition than simple structures such as ESN and ELM networks. In addition, the results show that the proposed model has a strong dynamic behavior and therefore will be useful for modeling high-dimensional data at different times and in different hierarchies. Another reason for the high recognition rate of this model for emotional databases is the ability to detect external relationships between the multidimensional features of a sequence as well as their internal latent structural dependencies using Hamiltonian multiplication and lower parameters. The use of quaternion algebra causes the high-dimensional feature to be encapsulated and the size of the reservoir matrix to be one-fourth (relative to real numbers). Because in the case of real numbers, if is the number of reservoir units, the size of the reservoir matrix will be × , but using quaternion algebra this size will be reduced to × × 4. Although quaternion algebra and quaternion multiplication are powerful tools in reducing the amount of memory consumed by the state network, the execution time and optimization of the network coefficients due to the Hamiltonian multiplication structure will be longer than the real state and this is the biggest limitation of using these numbers. Therefore, the coefficient optimization phase can be performed on parallel computers or GPUs to increase the speed of the QNESN coefficient optimization process. Although the proposed architecture has a higher execution time compared to ESN and ELM in real data mode, it ultimately runs much faster than other deeper models, and will run less than 60 minutes for samples with less than 1000 dimensions, which is a small amount of time. In the future, the performance of this structure can be investigated in the multi-corpus state and also for other emotional databases. In addition, the results reported in this study are based on emotional databases in noise-free environments, while real databases are instantaneous and noisy, in which the proposed frameworks need to be further developed, which will be another research path for the future work of the authors.",
      "page_start": 35,
      "page_end": 36
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, a nonlinear model of a quaternion echo state network is proposed to speech emotion recognition. The use of a multidimensional bilinear filter in the output layer of this network has enabled the system to have a good ability to transfer nonlinear features from the reservoir layer to the output and to classify the input data with less error. Also, with the help of this filter, the proposed model has made good use of the higher order statistics of reservoir signals. In this model, the use of the 2dPCA technique before the bilinear filter has caused that only a part of the main components of the reservoir state has been used by this filter, and this has prevented it from increasing its complexity. In addition, in the proposed model, quaternion algebra is used to reduce the amount of memory consumed by the reservoir. These numbers provide a more compact representation of multidimensional speech features, resulting in a higher capacity to use high-dimensional data. Also, by using Hamilton multiplications, the external relations between the features and their internal latent structural dependencies are extracted with less parameters. The proposed architecture for recognizing speech emotion has been evaluated on three different emotional databases with different features. According to the results presented in different stages, the proposed nonlinear quaternion structure has better results than the simple structure of the echo state network. The results of this study not only indicate an increase in QNESN network performance, but also describe and introduce it as a new signal processing tool.",
      "page_start": 37,
      "page_end": 37
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows the recognition rate of",
      "page": 4
    },
    {
      "caption": "Figure 1: Recognition rates of recent methods on EMODB dataset",
      "page": 5
    },
    {
      "caption": "Figure 2: In this study, we name the number of input",
      "page": 6
    },
    {
      "caption": "Figure 2: Structure of a simple echo state network",
      "page": 7
    },
    {
      "caption": "Figure 2: ). If we denote the size of the output space by \u0002\u0011, the output in the time step t is calculated by",
      "page": 7
    },
    {
      "caption": "Figure 3: Nonlinear echo state network",
      "page": 8
    },
    {
      "caption": "Figure 4: One-dimensional bilinear filter",
      "page": 9
    },
    {
      "caption": "Figure 5: Overall description of the speech emotion recognition system using QNESN",
      "page": 9
    },
    {
      "caption": "Figure 6: ). Therefore, relation (11) will",
      "page": 10
    },
    {
      "caption": "Figure 6: ESN network with new relations",
      "page": 10
    },
    {
      "caption": "Figure 7: Nonlinear echo state network with multi-dimensional bilinear filter",
      "page": 12
    },
    {
      "caption": "Figure 8: The structure of a multidimensional bilinear filter",
      "page": 14
    },
    {
      "caption": "Figure 9: Quaternion nonlinear ESN",
      "page": 15
    },
    {
      "caption": "Figure 10: Quaternion nonlinear ESN with two-dimensianal principle component analysis",
      "page": 18
    },
    {
      "caption": "Figure 11: Recognition rate for windowing with different frames using echo state network (SAVEE)",
      "page": 21
    },
    {
      "caption": "Figure 12: Recognition rate for windowing with different frames using echo state network (EMODB)",
      "page": 22
    },
    {
      "caption": "Figure 13: Mean and best value of training cost function in different iterations of QNESN parameter",
      "page": 24
    },
    {
      "caption": "Figure 16: for further comparison).",
      "page": 26
    },
    {
      "caption": "Figure 14: Comparison of different ESN types from Table (8) in terms of performance (WAR) and θ length",
      "page": 27
    },
    {
      "caption": "Figure 15: Comparison of different ESN types from Table (8) in terms of performance (WAR) and duration time",
      "page": 28
    },
    {
      "caption": "Figure 16: Comparison of different ESN networks from Table (9) in terms of WAR and memory consumption",
      "page": 29
    },
    {
      "caption": "Figure 17: compares the method presented in this paper in terms of the execution time (a) and the audio",
      "page": 33
    },
    {
      "caption": "Figure 17: , part b). This time was 0.6 seconds for 1 second of each",
      "page": 33
    },
    {
      "caption": "Figure 17: , Part A). The use of GMM statistical classifier with high components (1024",
      "page": 33
    },
    {
      "caption": "Figure 17: , part b). This time was 1.1",
      "page": 33
    },
    {
      "caption": "Figure 17: Comparison of (a) the execution time and (b) the response time of the propoed method with other",
      "page": 34
    },
    {
      "caption": "Figure 18: compares the method presented in this paper with other methods presented by the authors of",
      "page": 34
    },
    {
      "caption": "Figure 18: Comparison of recognition rate (WAR) of the proposed method using measure of central tendency in",
      "page": 34
    },
    {
      "caption": "Figure 19: Comparison of recogniton rate (WAR) of different emotions a) EMODB b) SAVEE c) IEMOCAP",
      "page": 36
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Recent methods and their recognition rates on EMODB dataset",
      "data": [
        {
          "Ref": "Zhao 2020, [38]",
          "Proposed method": "Use the semi-supervised generative adversarial network as classifier",
          "WAR": "65.20",
          "UAR": "68.00"
        },
        {
          "Ref": "Yi 2020, [97]",
          "Proposed method": "User the generative adversarial network and auto-encoder as classifier",
          "WAR": "84.49",
          "UAR": "83.31"
        },
        {
          "Ref": "Latif 2020 [40]",
          "Proposed method": "Use the adversarial auto-encoder for feature discriminative recognition",
          "WAR": "N/A",
          "UAR": "66.70"
        },
        {
          "Ref": "Wang 2020, [41]",
          "Proposed method": "Use wavelet analysis to extract features",
          "WAR": "N/A",
          "UAR": "79.20"
        },
        {
          "Ref": "Sugan 2020, [42]",
          "Proposed method": "Feature extraction using a triangular filter bank",
          "WAR": "77.08",
          "UAR": "N/A"
        },
        {
          "Ref": "Vieira 2020, [49]",
          "Proposed method": "Use Hilbert-Huang-Hurst coefficients to extract features",
          "WAR": "81.80",
          "UAR": "N/A"
        },
        {
          "Ref": "Haider 2020, [45]",
          "Proposed method": "Use the active feature selection method to select the feature",
          "WAR": "N/A",
          "UAR": "76.90"
        },
        {
          "Ref": "Li 2021, [67]",
          "Proposed method": "Use the feature weighting method based on emotional groups to select the features",
          "WAR": "72.19",
          "UAR": "N/A"
        },
        {
          "Ref": "Chen 2020, [44]",
          "Proposed method": "Use the C-means fuzzy clustering algorithm to select feature",
          "WAR": "85.61",
          "UAR": "N/A"
        },
        {
          "Ref": "Song 2020 [125]",
          "Proposed method": "Use of robust differential sparse regression to select differential features",
          "WAR": "N/A",
          "UAR": "86.19"
        },
        {
          "Ref": "Zhang 2021 [47]",
          "Proposed method": "Using ensemble learning model random forest algorithm and weighted binary cuckoo algorithm \nto select superior features",
          "WAR": "83.70",
          "UAR": "N/A"
        },
        {
          "Ref": "Hou 2020 [48]",
          "Proposed method": "Use of discriminative non-negative matrix factorization for feature dimension reduction",
          "WAR": "82.80",
          "UAR": "83.30"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 2: shows a comparison between network performance with the previous simple structure and the",
      "data": [
        {
          "ESN Structure": "Structure of Figure 2",
          "WAR": "44.37%"
        },
        {
          "ESN Structure": "Structure of Figure 6",
          "WAR": "57.88%"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 3: Specifications of emotional datasets",
      "data": [
        {
          "Dataset": "",
          "Language": "",
          "Number of \nSpeakers": "",
          "Emotions": "Anger \n(A)"
        },
        {
          "Dataset": "EMODB",
          "Language": "German",
          "Number of \nSpeakers": "10",
          "Emotions": "127"
        },
        {
          "Dataset": "SAVEE",
          "Language": "English",
          "Number of \nSpeakers": "4",
          "Emotions": "60"
        },
        {
          "Dataset": "IEMOCAP",
          "Language": "English",
          "Number of \nSpeakers": "10",
          "Emotions": "1103"
        }
      ],
      "page": 21
    },
    {
      "caption": "Table 6: The GA Specifications for optimizing ESN, NESN, QESN and QNESN network parameters",
      "data": [
        {
          "Population": "Population type \nDouble vector"
        },
        {
          "Population": "Population size \n500"
        },
        {
          "Population": "Creation Function \nUniform"
        },
        {
          "Population": "Initial Population \n[]"
        },
        {
          "Population": "Selection"
        },
        {
          "Population": "Selection Function \nStochastic Uniform"
        },
        {
          "Population": "Reproduction"
        },
        {
          "Population": "Elite Count \n.05*Population size"
        },
        {
          "Population": "Crossover Fraction \n0.8"
        },
        {
          "Population": "Mutation"
        },
        {
          "Population": "Mutation Function \nUniform"
        },
        {
          "Population": "Rate \n0.01"
        }
      ],
      "page": 24
    },
    {
      "caption": "Table 6: The GA Specifications for optimizing ESN, NESN, QESN and QNESN network parameters",
      "data": [
        {
          "Crossover": "Crossover function \nScattered"
        },
        {
          "Crossover": "Migration"
        },
        {
          "Crossover": "Direction \nForward"
        },
        {
          "Crossover": "Fraction \n0.2"
        },
        {
          "Crossover": "Interval \n20"
        },
        {
          "Crossover": "Constraint parameters"
        },
        {
          "Crossover": "Nonlinear constrain \nAugmented Lagrangian \nalgorithm"
        },
        {
          "Crossover": "Initial Penalty \n10"
        },
        {
          "Crossover": "Penalty Factor \n100"
        },
        {
          "Crossover": "Stopping Criteria"
        },
        {
          "Crossover": "100*Number of \nGenerations \nVariables"
        },
        {
          "Crossover": "Function Tolerance \n1e-6"
        },
        {
          "Crossover": "Constraint Tolerance \n1e-3"
        }
      ],
      "page": 24
    },
    {
      "caption": "Table 7: Performance comparison (1) of ESN, NESN, QESN and QNESN networks with different",
      "data": [
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "ESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "25",
          "W \nmatrix \nsize": "25×25",
          "\u0000\u0000\u0000 + \u0000\u0000": "25+2216+1=2242",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "200",
          "\u0000": "--",
          "\u0000\u0000\u0000(\u0000)": "--",
          "WAR": "57.29%",
          "UAR": "53.10%",
          "Duration \nTime (s)": "0.3350"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "ESN \n(GA)",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "25",
          "W \nmatrix \nsize": "25×25",
          "\u0000\u0000\u0000 + \u0000\u0000": "25+2216+1=2242",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "200",
          "\u0000": "[\u0000\u0000\u0000, \u0000, \u0000\u0000\u0000\u0000]",
          "\u0000\u0000\u0000(\u0000)": "57450",
          "WAR": "58.00%",
          "UAR": "54.76%",
          "Duration \nTime (s)": "0.3229"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "NESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "25",
          "W \nmatrix \nsize": "25×25",
          "\u0000\u0000\u0000 + \u0000\u0000": "25+2216+1=2242",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "200",
          "\u0000": "[\u0000, \u0000, \u0000, \u0000\u0000\u0000, \u0000]",
          "\u0000\u0000\u0000(\u0000)": "60264",
          "WAR": "59.03%",
          "UAR": "55.70%",
          "Duration \nTime (s)": "0.3203"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "QESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "25",
          "W \nmatrix \nsize": "25×25",
          "\u0000\u0000\u0000 + \u0000\u0000": "25+554+1=580",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "200",
          "\u0000": "[\u0000\u0000\u0000, \u0000, \u0000\u0000\u0000\u0000]",
          "\u0000\u0000\u0000(\u0000)": "63600",
          "WAR": "61.80%",
          "UAR": "60.18%",
          "Duration \nTime (s)": "0.2923"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "QNESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "25",
          "W \nmatrix \nsize": "25×25",
          "\u0000\u0000\u0000 + \u0000\u0000": "25+554+1=580",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "200",
          "\u0000": "[\u0000, \u0000, \u0000, \u0000\u0000\u0000, \u0000]",
          "\u0000\u0000\u0000(\u0000)": "74856",
          "WAR": "62.22%",
          "UAR": "60.60%",
          "Duration \nTime (s)": "0.2661"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "ESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "50",
          "W \nmatrix \nsize": "50×50",
          "\u0000\u0000\u0000 + \u0000\u0000": "50+2216+1=2266",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "400",
          "\u0000": "--",
          "\u0000\u0000\u0000(\u0000)": "--",
          "WAR": "61.46%",
          "UAR": "58.10%",
          "Duration \nTime (s)": "0.3195"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "ESN \n(GA)",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "50",
          "W \nmatrix \nsize": "50×50",
          "\u0000\u0000\u0000 + \u0000\u0000": "50+2216+1=2266",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "400",
          "\u0000": "[\u0000\u0000\u0000, \u0000, \u0000\u0000\u0000\u0000]",
          "\u0000\u0000\u0000(\u0000)": "116150",
          "WAR": "62.10%",
          "UAR": "59.71%",
          "Duration \nTime (s)": "0.3239"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "NESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "50",
          "W \nmatrix \nsize": "50×50",
          "\u0000\u0000\u0000 + \u0000\u0000": "50+2216+1=2266",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "400",
          "\u0000": "[\u0000, \u0000, \u0000, \u0000\u0000\u0000, \u0000]",
          "\u0000\u0000\u0000(\u0000)": "121764",
          "WAR": "62.46%",
          "UAR": "60.05%",
          "Duration \nTime (s)": "0.3254"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "QESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "50",
          "W \nmatrix \nsize": "50×50",
          "\u0000\u0000\u0000 + \u0000\u0000": "50+554+1=605",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "400",
          "\u0000": "[\u0000\u0000\u0000, \u0000, \u0000\u0000\u0000\u0000]",
          "\u0000\u0000\u0000(\u0000)": "132200",
          "WAR": "64.95%",
          "UAR": "63.05%",
          "Duration \nTime (s)": "0.5241"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "QNESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "50",
          "W \nmatrix \nsize": "50×50",
          "\u0000\u0000\u0000 + \u0000\u0000": "50+554+1=605",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "400",
          "\u0000": "[\u0000, \u0000, \u0000, \u0000\u0000\u0000, \u0000]",
          "\u0000\u0000\u0000(\u0000)": "154656",
          "WAR": "65.49%",
          "UAR": "63.60%",
          "Duration \nTime (s)": "0.4832"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "ESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "100",
          "W \nmatrix \nsize": "100×100",
          "\u0000\u0000\u0000 + \u0000\u0000": "100+2216+1=2317",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "400",
          "\u0000": "--",
          "\u0000\u0000\u0000(\u0000)": "--",
          "WAR": "61.46%",
          "UAR": "58.10%",
          "Duration \nTime (s)": "0.3366"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "ESN \n(GA)",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "100",
          "W \nmatrix \nsize": "100×100",
          "\u0000\u0000\u0000 + \u0000\u0000": "100+2216+1=2317",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "400",
          "\u0000": "[\u0000\u0000\u0000, \u0000, \u0000\u0000\u0000\u0000]",
          "\u0000\u0000\u0000(\u0000)": "234500",
          "WAR": "62.27%",
          "UAR": "59.88%",
          "Duration \nTime (s)": "0.3390"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "NESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "100",
          "W \nmatrix \nsize": "100×100",
          "\u0000\u0000\u0000 + \u0000\u0000": "100+2216+1=2317",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "400",
          "\u0000": "[\u0000, \u0000, \u0000, \u0000\u0000\u0000, \u0000]",
          "\u0000\u0000\u0000(\u0000)": "240114",
          "WAR": "62.86%",
          "UAR": "60.43%",
          "Duration \nTime (s)": "0.3583"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "QESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "100",
          "W \nmatrix \nsize": "100×100",
          "\u0000\u0000\u0000 + \u0000\u0000": "100+554+1=655",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "400",
          "\u0000": "[\u0000\u0000\u0000, \u0000, \u0000\u0000\u0000\u0000]",
          "\u0000\u0000\u0000(\u0000)": "273200",
          "WAR": "65.56%",
          "UAR": "63.65%",
          "Duration \nTime (s)": "1.0175"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "QNESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "100",
          "W \nmatrix \nsize": "100×100",
          "\u0000\u0000\u0000 + \u0000\u0000": "100+554+1=655",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "400",
          "\u0000": "[\u0000, \u0000, \u0000, \u0000\u0000\u0000, \u0000]",
          "\u0000\u0000\u0000(\u0000)": "295656",
          "WAR": "66.48%",
          "UAR": "64.57%",
          "Duration \nTime (s)": "1.0003"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "ESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "200",
          "W \nmatrix \nsize": "200×200",
          "\u0000\u0000\u0000 + \u0000\u0000": "200+2216+1=2417",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "500",
          "\u0000": "--",
          "\u0000\u0000\u0000(\u0000)": "--",
          "WAR": "61.46%",
          "UAR": "57.98%",
          "Duration \nTime (s)": "0.3973"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "ESN \n(GA)",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "200",
          "W \nmatrix \nsize": "200×200",
          "\u0000\u0000\u0000 + \u0000\u0000": "200+2216+1=2417",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "500",
          "\u0000": "[\u0000\u0000\u0000, \u0000, \u0000\u0000\u0000\u0000]",
          "\u0000\u0000\u0000(\u0000)": "486900",
          "WAR": "62.21%",
          "UAR": "59.69%",
          "Duration \nTime (s)": "0.3950"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "NESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "200",
          "W \nmatrix \nsize": "200×200",
          "\u0000\u0000\u0000 + \u0000\u0000": "200+2216+1=2417",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "500",
          "\u0000": "[\u0000, \u0000, \u0000, \u0000\u0000\u0000, \u0000]",
          "\u0000\u0000\u0000(\u0000)": "493914",
          "WAR": "63.31%",
          "UAR": "60.73%",
          "Duration \nTime (s)": "0.4110"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "QESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "200",
          "W \nmatrix \nsize": "200×200",
          "\u0000\u0000\u0000 + \u0000\u0000": "200+554+1=755",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "500",
          "\u0000": "[\u0000\u0000\u0000, \u0000, \u0000\u0000\u0000\u0000]",
          "\u0000\u0000\u0000(\u0000)": "618000",
          "WAR": "65.52%",
          "UAR": "63.64%",
          "Duration \nTime (s)": "2.2917"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "QNESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "200",
          "W \nmatrix \nsize": "200×200",
          "\u0000\u0000\u0000 + \u0000\u0000": "200+554+1=755",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "500",
          "\u0000": "[\u0000, \u0000, \u0000, \u0000\u0000\u0000, \u0000]",
          "\u0000\u0000\u0000(\u0000)": "646056",
          "WAR": "66.67%",
          "UAR": "65.77%",
          "Duration \nTime (s)": "2.2383"
        }
      ],
      "page": 25
    },
    {
      "caption": "Table 8: Comparison of the performance (2) of ESN, NESN, QESN and QNESN networks with different",
      "data": [
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "ESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "100",
          "W \nmatrix \nsize": "100×100",
          "\u0000\u0000\u0000 + \u0000\u0000": "100+2216+1=2317",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "400",
          "\u0000": "--",
          "\u0000\u0000\u0000(\u0000)": "--",
          "WAR": "61.46%",
          "UAR": "58.10%",
          "Duration \nTime (s)": "0.3366"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "ESN \n(GA)",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "100",
          "W \nmatrix \nsize": "100×100",
          "\u0000\u0000\u0000 + \u0000\u0000": "100+2216+1=2317",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "400",
          "\u0000": "[\u0000\u0000\u0000, \u0000, \u0000\u0000\u0000\u0000]",
          "\u0000\u0000\u0000(\u0000)": "234500",
          "WAR": "62.27%",
          "UAR": "59.88%",
          "Duration \nTime (s)": "0.3390"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "NESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "100",
          "W \nmatrix \nsize": "100×100",
          "\u0000\u0000\u0000 + \u0000\u0000": "100+2216+1=2317",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "400",
          "\u0000": "[\u0000, \u0000, \u0000, \u0000\u0000\u0000, \u0000]",
          "\u0000\u0000\u0000(\u0000)": "240114",
          "WAR": "62.86%",
          "UAR": "60.43%",
          "Duration \nTime (s)": "0.3583"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "QESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "25",
          "W \nmatrix \nsize": "25×25",
          "\u0000\u0000\u0000 + \u0000\u0000": "25+554+1=580",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "200",
          "\u0000": "[\u0000\u0000\u0000, \u0000, \u0000\u0000\u0000\u0000]",
          "\u0000\u0000\u0000(\u0000)": "63600",
          "WAR": "61.80%",
          "UAR": "60.18%",
          "Duration \nTime (s)": "0.2923"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "QNESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "25",
          "W \nmatrix \nsize": "25×25",
          "\u0000\u0000\u0000 + \u0000\u0000": "25+554+1=580",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "200",
          "\u0000": "[\u0000, \u0000, \u0000, \u0000\u0000\u0000, \u0000]",
          "\u0000\u0000\u0000(\u0000)": "74856",
          "WAR": "62.22%",
          "UAR": "60.60%",
          "Duration \nTime (s)": "0.2661"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "ESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "200",
          "W \nmatrix \nsize": "200×200",
          "\u0000\u0000\u0000 + \u0000\u0000": "200+2216+1=2417",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "500",
          "\u0000": "--",
          "\u0000\u0000\u0000(\u0000)": "--",
          "WAR": "61.46%",
          "UAR": "57.98%",
          "Duration \nTime (s)": "0.3973"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "ESN \n(GA)",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "200",
          "W \nmatrix \nsize": "200×200",
          "\u0000\u0000\u0000 + \u0000\u0000": "200+2216+1=2417",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "500",
          "\u0000": "[\u0000\u0000\u0000, \u0000, \u0000\u0000\u0000\u0000]",
          "\u0000\u0000\u0000(\u0000)": "486900",
          "WAR": "62.21%",
          "UAR": "59.69%",
          "Duration \nTime (s)": "0.3950"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "NESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "200",
          "W \nmatrix \nsize": "200×200",
          "\u0000\u0000\u0000 + \u0000\u0000": "200+2216+1=2417",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "500",
          "\u0000": "[\u0000, \u0000, \u0000, \u0000\u0000\u0000, \u0000]",
          "\u0000\u0000\u0000(\u0000)": "493914",
          "WAR": "63.31%",
          "UAR": "60.73%",
          "Duration \nTime (s)": "0.4110"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "QESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "50",
          "W \nmatrix \nsize": "50×50",
          "\u0000\u0000\u0000 + \u0000\u0000": "50+554+1=605",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "400",
          "\u0000": "[\u0000\u0000\u0000, \u0000, \u0000\u0000\u0000\u0000]",
          "\u0000\u0000\u0000(\u0000)": "132200",
          "WAR": "64.95%",
          "UAR": "63.05%",
          "Duration \nTime (s)": "0.5241"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "QNESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "50",
          "W \nmatrix \nsize": "50×50",
          "\u0000\u0000\u0000 + \u0000\u0000": "50+554+1=605",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "400",
          "\u0000": "[\u0000, \u0000, \u0000, \u0000\u0000\u0000, \u0000]",
          "\u0000\u0000\u0000(\u0000)": "154656",
          "WAR": "65.49%",
          "UAR": "63.60%",
          "Duration \nTime (s)": "0.4832"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "ESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "400",
          "W \nmatrix \nsize": "400×400",
          "\u0000\u0000\u0000 + \u0000\u0000": "400+2216+1=2617",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "500",
          "\u0000": "--",
          "\u0000\u0000\u0000(\u0000)": "--",
          "WAR": "61.46%",
          "UAR": "57.98%",
          "Duration \nTime (s)": "0.4845"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "ESN \n(GA)",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "400",
          "W \nmatrix \nsize": "400×400",
          "\u0000\u0000\u0000 + \u0000\u0000": "400+2216+1=2617",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "500",
          "\u0000": "[\u0000\u0000\u0000, \u0000, \u0000\u0000\u0000\u0000]",
          "\u0000\u0000\u0000(\u0000)": "1210300",
          "WAR": "62.67%",
          "UAR": "60.12%",
          "Duration \nTime (s)": "0.4946"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "NESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "400",
          "W \nmatrix \nsize": "400×400",
          "\u0000\u0000\u0000 + \u0000\u0000": "400+2216+1=2617",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "500",
          "\u0000": "[\u0000, \u0000, \u0000, \u0000\u0000\u0000, \u0000]",
          "\u0000\u0000\u0000(\u0000)": "1217314",
          "WAR": "63.89%",
          "UAR": "61.25%",
          "Duration \nTime (s)": "0.4901"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "QESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "100",
          "W \nmatrix \nsize": "100×100",
          "\u0000\u0000\u0000 + \u0000\u0000": "100+554+1=655",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "400",
          "\u0000": "[\u0000\u0000\u0000, \u0000, \u0000\u0000\u0000\u0000]",
          "\u0000\u0000\u0000(\u0000)": "273200",
          "WAR": "65.56%",
          "UAR": "63.65%",
          "Duration \nTime (s)": "1.0175"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "QNESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "100",
          "W \nmatrix \nsize": "100×100",
          "\u0000\u0000\u0000 + \u0000\u0000": "100+554+1=655",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "400",
          "\u0000": "[\u0000, \u0000, \u0000, \u0000\u0000\u0000, \u0000]",
          "\u0000\u0000\u0000(\u0000)": "295656",
          "WAR": "66.48%",
          "UAR": "64.57%",
          "Duration \nTime (s)": "1.0003"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "ESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "800",
          "W \nmatrix \nsize": "800×800",
          "\u0000\u0000\u0000 + \u0000\u0000": "800+2216+1=3017",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "600",
          "\u0000": "--",
          "\u0000\u0000\u0000(\u0000)": "--",
          "WAR": "62.08%",
          "UAR": "58.57%",
          "Duration \nTime (s)": "0.7514"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "ESN \n(GA)",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "800",
          "W \nmatrix \nsize": "800×800",
          "\u0000\u0000\u0000 + \u0000\u0000": "800+2216+1=3017",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "600",
          "\u0000": "[\u0000\u0000\u0000, \u0000, \u0000\u0000\u0000\u0000]",
          "\u0000\u0000\u0000(\u0000)": "3057800",
          "WAR": "63.12%",
          "UAR": "60.54%",
          "Duration \nTime (s)": "0.7400"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "NESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "800",
          "W \nmatrix \nsize": "800×800",
          "\u0000\u0000\u0000 + \u0000\u0000": "800+2216+1=3017",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "600",
          "\u0000": "[\u0000, \u0000, \u0000, \u0000\u0000\u0000, \u0000]",
          "\u0000\u0000\u0000(\u0000)": "3066214",
          "WAR": "64.55%",
          "UAR": "61.90%",
          "Duration \nTime (s)": "0.6857"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "QESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "200",
          "W \nmatrix \nsize": "200×200",
          "\u0000\u0000\u0000 + \u0000\u0000": "200+554+1=755",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "500",
          "\u0000": "[\u0000\u0000\u0000, \u0000, \u0000\u0000\u0000\u0000]",
          "\u0000\u0000\u0000(\u0000)": "618000",
          "WAR": "65.52%",
          "UAR": "63.64%",
          "Duration \nTime (s)": "2.2917"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "QNESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "200",
          "W \nmatrix \nsize": "200×200",
          "\u0000\u0000\u0000 + \u0000\u0000": "200+554+1=755",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "500",
          "\u0000": "[\u0000, \u0000, \u0000, \u0000\u0000\u0000, \u0000]",
          "\u0000\u0000\u0000(\u0000)": "646056",
          "WAR": "66.67%",
          "UAR": "65.77%",
          "Duration \nTime (s)": "2.2383"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "QNESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "200",
          "W \nmatrix \nsize": "200×200",
          "\u0000\u0000\u0000 + \u0000\u0000": "200+554+1=755",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "--",
          "\u0000": "[\u0000, \u0000, \u0000, \u0000\u0000\u0000, \u0000]",
          "\u0000\u0000\u0000(\u0000)": "667476",
          "WAR": "65.16%",
          "UAR": "64.28%",
          "Duration \nTime (s)": "1.0124"
        }
      ],
      "page": 27
    },
    {
      "caption": "Table 9: Comparison of memory consumption of genetic algorithm in ESN, NESN, QESN and QNESN network",
      "data": [
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "ESN",
          "Reservoir Unit No \n(\u0000\u0000\u0000)": "100",
          "\u0000\u0000\u0000(\u0000) \n(double \nvector)": "--",
          "Chromosome \nLength": "--",
          "GA Memory \nConsumption \n(One Generation)": "--",
          "CPU Specification": "Core™ i3-4160-3.60GHz \n8.00 GB",
          "WAR": "61.46%"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "ESN (GA)",
          "Reservoir Unit No \n(\u0000\u0000\u0000)": "100",
          "\u0000\u0000\u0000(\u0000) \n(double \nvector)": "234500",
          "Chromosome \nLength": "1.87 MB",
          "GA Memory \nConsumption \n(One Generation)": "938 MB",
          "CPU Specification": "Xeon(R) E7-2699-2.30GHz \n10.00 GB",
          "WAR": "62.27%"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "NESN",
          "Reservoir Unit No \n(\u0000\u0000\u0000)": "100",
          "\u0000\u0000\u0000(\u0000) \n(double \nvector)": "240114",
          "Chromosome \nLength": "1.92 MB",
          "GA Memory \nConsumption \n(One Generation)": "960.45 MB",
          "CPU Specification": "Xeon(R) E7-2699-2.30GHz \n10.00 GB",
          "WAR": "62.86%"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "QESN",
          "Reservoir Unit No \n(\u0000\u0000\u0000)": "25",
          "\u0000\u0000\u0000(\u0000) \n(double \nvector)": "63600",
          "Chromosome \nLength": "0.5088 MB",
          "GA Memory \nConsumption \n(One Generation)": "254.4 MB",
          "CPU Specification": "Xeon(R) E7-2699-2.30GHz \n10.00 GB",
          "WAR": "61.80%"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "QNESN",
          "Reservoir Unit No \n(\u0000\u0000\u0000)": "25",
          "\u0000\u0000\u0000(\u0000) \n(double \nvector)": "74856",
          "Chromosome \nLength": "0.5988 MB",
          "GA Memory \nConsumption \n(One Generation)": "299.42 MB",
          "CPU Specification": "Xeon(R) E7-2699-2.30GHz \n10.00 GB",
          "WAR": "62.22%"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "ESN",
          "Reservoir Unit No \n(\u0000\u0000\u0000)": "200",
          "\u0000\u0000\u0000(\u0000) \n(double \nvector)": "--",
          "Chromosome \nLength": "--",
          "GA Memory \nConsumption \n(One Generation)": "--",
          "CPU Specification": "Core™ i3-4160-3.60GHz \n8.00 GB",
          "WAR": "61.46%"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "ESN (GA)",
          "Reservoir Unit No \n(\u0000\u0000\u0000)": "200",
          "\u0000\u0000\u0000(\u0000) \n(double \nvector)": "486900",
          "Chromosome \nLength": "3.89 MB",
          "GA Memory \nConsumption \n(One Generation)": "1.945 GB",
          "CPU Specification": "Core™ i3-4160-3.60GHz \n8.00 GB",
          "WAR": "62.21%"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "NESN",
          "Reservoir Unit No \n(\u0000\u0000\u0000)": "200",
          "\u0000\u0000\u0000(\u0000) \n(double \nvector)": "493914",
          "Chromosome \nLength": "3.95 MB",
          "GA Memory \nConsumption \n(One Generation)": "1.976 GB",
          "CPU Specification": "Core™ i3-4160-3.60GHz \n8.00 GB",
          "WAR": "63.31%"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "QESN",
          "Reservoir Unit No \n(\u0000\u0000\u0000)": "50",
          "\u0000\u0000\u0000(\u0000) \n(double \nvector)": "132200",
          "Chromosome \nLength": "1.05 MB",
          "GA Memory \nConsumption \n(One Generation)": "528.8 MB",
          "CPU Specification": "Core™ i7-M640-2.80GHz \n8.00 GB",
          "WAR": "64.95%"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "QNESN",
          "Reservoir Unit No \n(\u0000\u0000\u0000)": "50",
          "\u0000\u0000\u0000(\u0000) \n(double \nvector)": "154656",
          "Chromosome \nLength": "1.23 MB",
          "GA Memory \nConsumption \n(One Generation)": "618.62 MB",
          "CPU Specification": "Core™ i7-M640-2.80GHz \n8.00 GB",
          "WAR": "65.49%"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "ESN",
          "Reservoir Unit No \n(\u0000\u0000\u0000)": "400",
          "\u0000\u0000\u0000(\u0000) \n(double \nvector)": "--",
          "Chromosome \nLength": "--",
          "GA Memory \nConsumption \n(One Generation)": "--",
          "CPU Specification": "Core™ i3-4160-3.60GHz \n8.00 GB",
          "WAR": "61.46%"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "ESN (GA)",
          "Reservoir Unit No \n(\u0000\u0000\u0000)": "400",
          "\u0000\u0000\u0000(\u0000) \n(double \nvector)": "1210300",
          "Chromosome \nLength": "9.68 MB",
          "GA Memory \nConsumption \n(One Generation)": "4.84 GB",
          "CPU Specification": "Core™ i7-3720QM-2.60GHz \n16.00 GB",
          "WAR": "62.67%"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "NESN",
          "Reservoir Unit No \n(\u0000\u0000\u0000)": "400",
          "\u0000\u0000\u0000(\u0000) \n(double \nvector)": "1217314",
          "Chromosome \nLength": "9.73 MB",
          "GA Memory \nConsumption \n(One Generation)": "4.86 GB",
          "CPU Specification": "Core™ i7-3720QM-2.60GHz \n16.00 GB",
          "WAR": "63.89%"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "QESN",
          "Reservoir Unit No \n(\u0000\u0000\u0000)": "100",
          "\u0000\u0000\u0000(\u0000) \n(double \nvector)": "273200",
          "Chromosome \nLength": "2.18 MB",
          "GA Memory \nConsumption \n(One Generation)": "1.09 GB",
          "CPU Specification": "Core™ i3-4160-3.60GHz \n8.00 GB",
          "WAR": "65.56%"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "QNESN",
          "Reservoir Unit No \n(\u0000\u0000\u0000)": "100",
          "\u0000\u0000\u0000(\u0000) \n(double \nvector)": "295656",
          "Chromosome \nLength": "2.36 MB",
          "GA Memory \nConsumption \n(One Generation)": "1.18 GB",
          "CPU Specification": "Core™ i3-4160-3.60GHz \n8.00 GB",
          "WAR": "66.48%"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "ESN",
          "Reservoir Unit No \n(\u0000\u0000\u0000)": "800",
          "\u0000\u0000\u0000(\u0000) \n(double \nvector)": "--",
          "Chromosome \nLength": "--",
          "GA Memory \nConsumption \n(One Generation)": "--",
          "CPU Specification": "Core™ i3-4160-3.60GHz \n8.00 GB",
          "WAR": "62.08%"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "ESN (GA)",
          "Reservoir Unit No \n(\u0000\u0000\u0000)": "800",
          "\u0000\u0000\u0000(\u0000) \n(double \nvector)": "3057800",
          "Chromosome \nLength": "24.46 MB",
          "GA Memory \nConsumption \n(One Generation)": "12.23 GB",
          "CPU Specification": "Core™ i5-6400-2.70GHz \n32.00 GB",
          "WAR": "63.12%"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "NESN",
          "Reservoir Unit No \n(\u0000\u0000\u0000)": "800",
          "\u0000\u0000\u0000(\u0000) \n(double \nvector)": "3066214",
          "Chromosome \nLength": "24.52 MB",
          "GA Memory \nConsumption \n(One Generation)": "12.26 GB",
          "CPU Specification": "Core™ i5-6400-2.70GHz \n32.00 GB",
          "WAR": "64.55%"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "QESN",
          "Reservoir Unit No \n(\u0000\u0000\u0000)": "200",
          "\u0000\u0000\u0000(\u0000) \n(double \nvector)": "618000",
          "Chromosome \nLength": "4.94 MB",
          "GA Memory \nConsumption \n(One Generation)": "2.47 GB",
          "CPU Specification": "Core™ i3-4160-3.60GHz \n8.00 GB",
          "WAR": "65.52%"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "QNESN",
          "Reservoir Unit No \n(\u0000\u0000\u0000)": "200",
          "\u0000\u0000\u0000(\u0000) \n(double \nvector)": "646056",
          "Chromosome \nLength": "5.16 MB",
          "GA Memory \nConsumption \n(One Generation)": "2.58 GB",
          "CPU Specification": "Core™ i3-4160-3.60GHz \n8.00 GB",
          "WAR": "66.67%"
        }
      ],
      "page": 28
    },
    {
      "caption": "Table 10: Comparison of the performance of ESN, NESN, QESN and QNESN networks with different",
      "data": [
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "ESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "100",
          "W \nmatrix \nsize": "100×100",
          "\u0000\u0000\u0000 + \u0000\u0000": "100+2216+1=2317",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "400",
          "\u0000": "--",
          "\u0000\u0000\u0000(\u0000)": "--",
          "WAR": "78.45%",
          "UAR": "78.19%",
          "Duration \nTime (s)": "0.42"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "ESN \n(GA)",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "100",
          "W \nmatrix \nsize": "100×100",
          "\u0000\u0000\u0000 + \u0000\u0000": "100+2216+1=2317",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "400",
          "\u0000": "[\u0000\u0000\u0000, \u0000, \u0000\u0000\u0000\u0000]",
          "\u0000\u0000\u0000(\u0000)": "234500",
          "WAR": "79.97%",
          "UAR": "79.26%",
          "Duration \nTime (s)": "0.4162"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "NESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "100",
          "W \nmatrix \nsize": "100×100",
          "\u0000\u0000\u0000 + \u0000\u0000": "100+2216+1=2317",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "400",
          "\u0000": "[\u0000, \u0000, \u0000, \u0000\u0000\u0000, \u0000]",
          "\u0000\u0000\u0000(\u0000)": "240114",
          "WAR": "80.69%",
          "UAR": "79.85%",
          "Duration \nTime (s)": "0.4256"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "QESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "25",
          "W \nmatrix \nsize": "25×25",
          "\u0000\u0000\u0000 + \u0000\u0000": "25+554+1=580",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "200",
          "\u0000": "[\u0000\u0000\u0000, \u0000, \u0000\u0000\u0000\u0000]",
          "\u0000\u0000\u0000(\u0000)": "63600",
          "WAR": "80.27%",
          "UAR": "78.79%",
          "Duration \nTime (s)": "0.6387"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "QNESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "25",
          "W \nmatrix \nsize": "25×25",
          "\u0000\u0000\u0000 + \u0000\u0000": "25+554+1=580",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "200",
          "\u0000": "[\u0000, \u0000, \u0000, \u0000\u0000\u0000, \u0000]",
          "\u0000\u0000\u0000(\u0000)": "74856",
          "WAR": "80.52%",
          "UAR": "79.21%",
          "Duration \nTime (s)": "0.6786"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "ESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "200",
          "W \nmatrix \nsize": "200×200",
          "\u0000\u0000\u0000 + \u0000\u0000": "200+2216+1=2417",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "500",
          "\u0000": "--",
          "\u0000\u0000\u0000(\u0000)": "--",
          "WAR": "77.86%",
          "UAR": "77.80%",
          "Duration \nTime (s)": "0.4895"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "ESN \n(GA)",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "200",
          "W \nmatrix \nsize": "200×200",
          "\u0000\u0000\u0000 + \u0000\u0000": "200+2216+1=2417",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "500",
          "\u0000": "[\u0000\u0000\u0000, \u0000, \u0000\u0000\u0000\u0000]",
          "\u0000\u0000\u0000(\u0000)": "486900",
          "WAR": "79.57%",
          "UAR": "78.61%",
          "Duration \nTime (s)": "0.4931"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "NESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "200",
          "W \nmatrix \nsize": "200×200",
          "\u0000\u0000\u0000 + \u0000\u0000": "200+2216+1=2417",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "500",
          "\u0000": "[\u0000, \u0000, \u0000, \u0000\u0000\u0000, \u0000]",
          "\u0000\u0000\u0000(\u0000)": "493914",
          "WAR": "80.61%",
          "UAR": "79.71%",
          "Duration \nTime (s)": "0.4888"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "QESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "50",
          "W \nmatrix \nsize": "50×50",
          "\u0000\u0000\u0000 + \u0000\u0000": "50+554+1=605",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "400",
          "\u0000": "[\u0000\u0000\u0000, \u0000, \u0000\u0000\u0000\u0000]",
          "\u0000\u0000\u0000(\u0000)": "132200",
          "WAR": "82.93%",
          "UAR": "81.35%",
          "Duration \nTime (s)": "1.1551"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "QNESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "50",
          "W \nmatrix \nsize": "50×50",
          "\u0000\u0000\u0000 + \u0000\u0000": "50+554+1=605",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "400",
          "\u0000": "[\u0000, \u0000, \u0000, \u0000\u0000\u0000, \u0000]",
          "\u0000\u0000\u0000(\u0000)": "154656",
          "WAR": "83.48%",
          "UAR": "81.89%",
          "Duration \nTime (s)": "1.2408"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "ESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "400",
          "W \nmatrix \nsize": "400×400",
          "\u0000\u0000\u0000 + \u0000\u0000": "400+2216+1=2617",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "500",
          "\u0000": "--",
          "\u0000\u0000\u0000(\u0000)": "--",
          "WAR": "77.86%",
          "UAR": "77.86%",
          "Duration \nTime (s)": "0.6513"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "ESN \n(GA)",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "400",
          "W \nmatrix \nsize": "400×400",
          "\u0000\u0000\u0000 + \u0000\u0000": "400+2216+1=2617",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "500",
          "\u0000": "[\u0000\u0000\u0000, \u0000, \u0000\u0000\u0000\u0000]",
          "\u0000\u0000\u0000(\u0000)": "1210300",
          "WAR": "80.00%",
          "UAR": "79.07%",
          "Duration \nTime (s)": "0.6404"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "NESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "400",
          "W \nmatrix \nsize": "400×400",
          "\u0000\u0000\u0000 + \u0000\u0000": "400+2216+1=2617",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "500",
          "\u0000": "[\u0000, \u0000, \u0000, \u0000\u0000\u0000, \u0000]",
          "\u0000\u0000\u0000(\u0000)": "1217314",
          "WAR": "81.13%",
          "UAR": "80.29%",
          "Duration \nTime (s)": "0.5864"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "QESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "100",
          "W \nmatrix \nsize": "100×100",
          "\u0000\u0000\u0000 + \u0000\u0000": "100+554+1=655",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "400",
          "\u0000": "[\u0000\u0000\u0000, \u0000, \u0000\u0000\u0000\u0000]",
          "\u0000\u0000\u0000(\u0000)": "273200",
          "WAR": "83.53%",
          "UAR": "81.96%",
          "Duration \nTime (s)": "1.9484"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "QNESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "100",
          "W \nmatrix \nsize": "100×100",
          "\u0000\u0000\u0000 + \u0000\u0000": "100+554+1=655",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "400",
          "\u0000": "[\u0000, \u0000, \u0000, \u0000\u0000\u0000, \u0000]",
          "\u0000\u0000\u0000(\u0000)": "295656",
          "WAR": "84.45%",
          "UAR": "82.88%",
          "Duration \nTime (s)": "2.49"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "ESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "800",
          "W \nmatrix \nsize": "800×800",
          "\u0000\u0000\u0000 + \u0000\u0000": "800+2216+1=3017",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "600",
          "\u0000": "--",
          "\u0000\u0000\u0000(\u0000)": "--",
          "WAR": "79.42%",
          "UAR": "78.99%",
          "Duration \nTime (s)": "0.802"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "ESN \n(GA)",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "800",
          "W \nmatrix \nsize": "800×800",
          "\u0000\u0000\u0000 + \u0000\u0000": "800+2216+1=3017",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "600",
          "\u0000": "[\u0000\u0000\u0000, \u0000, \u0000\u0000\u0000\u0000]",
          "\u0000\u0000\u0000(\u0000)": "3057800",
          "WAR": "80.76%",
          "UAR": "80.32%",
          "Duration \nTime (s)": "0.8186"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "NESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "800",
          "W \nmatrix \nsize": "800×800",
          "\u0000\u0000\u0000 + \u0000\u0000": "800+2216+1=3017",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "600",
          "\u0000": "[\u0000, \u0000, \u0000, \u0000\u0000\u0000, \u0000]",
          "\u0000\u0000\u0000(\u0000)": "3066214",
          "WAR": "82.67%",
          "UAR": "82.22%",
          "Duration \nTime (s)": "0.8585"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "QESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "200",
          "W \nmatrix \nsize": "200×200",
          "\u0000\u0000\u0000 + \u0000\u0000": "200+554+1=755",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "500",
          "\u0000": "[\u0000\u0000\u0000, \u0000, \u0000\u0000\u0000\u0000]",
          "\u0000\u0000\u0000(\u0000)": "618000",
          "WAR": "83.83%",
          "UAR": "83.37%",
          "Duration \nTime (s)": "4.8026"
        },
        {
          "\u0000\u0000\u0000\n− \u0000\u0000\u0000\u0000": "QNESN",
          "Reservoir \nUnit No \n(\u0000\u0000\u0000)": "200",
          "W \nmatrix \nsize": "200×200",
          "\u0000\u0000\u0000 + \u0000\u0000": "200+554+1=755",
          "\u0000\u0000\u0000\u0000\u0000\u0000": "500",
          "\u0000": "[\u0000, \u0000, \u0000, \u0000\u0000\u0000, \u0000]",
          "\u0000\u0000\u0000(\u0000)": "646056",
          "WAR": "85.39%",
          "UAR": "84.92%",
          "Duration \nTime (s)": "5.7045"
        }
      ],
      "page": 29
    },
    {
      "caption": "Table 11: EMODB recognition rates (%) compared to the QNESN model",
      "data": [
        {
          "Ref": "Haider 2020, [45]",
          "WAR": "82.40",
          "UAR \nRef": "N/A \nTzinis 2018 [93]",
          "UAR": "N/A"
        },
        {
          "Ref": "Zhao 2020, [38]",
          "WAR": "82.70",
          "UAR \nRef": "N/A \nKalinli 2016 [95]",
          "UAR": "N/A"
        },
        {
          "Ref": "Chen 2018, [64]",
          "WAR": "82.80",
          "UAR \nRef": "N/A \nHou 2020 [48]",
          "UAR": "83.30"
        },
        {
          "Ref": "Sidorov 2016, [65]",
          "WAR": "82.82",
          "UAR \nRef": "76.81 \nDaneshfar 2020, [79]",
          "UAR": "N/A"
        },
        {
          "Ref": "Aghajani 2020, [66]",
          "WAR": "83.70",
          "UAR \nRef": "N/A \nZhang 2021 [47]",
          "UAR": "N/A"
        },
        {
          "Ref": "Li 2021, [67]",
          "WAR": "83.74",
          "UAR \nRef": "N/A \nYi 2019 [39]",
          "UAR": "N/A"
        },
        {
          "Ref": "Yüncü 2014, [68]",
          "WAR": "83.80",
          "UAR \nRef": "N/A \nDeb 2018 [73]",
          "UAR": "N/A"
        },
        {
          "Ref": "Khan 2017, [32]",
          "WAR": "84.49",
          "UAR \nRef": "N/A \nYi 2020 [97]",
          "UAR": "83.31"
        },
        {
          "Ref": "Sinith 2015, [69]",
          "WAR": "84.50",
          "UAR \nRef": "N/A \nTawari 2010 [98]",
          "UAR": "N/A"
        },
        {
          "Ref": "Deb 2017, [71]",
          "WAR": "84.50",
          "UAR \nRef": "N/A \nÖzseven 2018 [99]",
          "UAR": "N/A"
        },
        {
          "Ref": "Deb 2016, [70]",
          "WAR": "84.62",
          "UAR \nRef": "N/A \nÖzseven 2019 [100]",
          "UAR": "N/A"
        },
        {
          "Ref": "Tao 2016, [74]",
          "WAR": "85.10",
          "UAR \nRef": "N/A \nDeb 2017, [72]",
          "UAR": "N/A"
        },
        {
          "Ref": "Kadiri 2015 [31]",
          "WAR": "85.32",
          "UAR \nRef": "N/A \nMeng 2019 [101]",
          "UAR": "N/A"
        },
        {
          "Ref": "Shirani 2016, [75]",
          "WAR": "85.39",
          "UAR \nRef": "N/A \nProposed",
          "UAR": "84.92"
        },
        {
          "Ref": "Bashirpour 2016, [34]",
          "WAR": "85.57",
          "UAR \nRef": "N/A \nSajjad 2020 [127]",
          "UAR": "N/A"
        },
        {
          "Ref": "Sugan 2020, [42]",
          "WAR": "85.61",
          "UAR \nRef": "N/A \nChen 2020, [44]",
          "UAR": "N/A"
        },
        {
          "Ref": "Luengo 2010, [35]",
          "WAR": "86.36",
          "UAR \nRef": "N/A \nSingh 2020 [46]",
          "UAR": "N/A"
        },
        {
          "Ref": "",
          "WAR": "90.21",
          "UAR \nRef": "N/A \nEr 2020 [126]",
          "UAR": "N/A"
        }
      ],
      "page": 30
    },
    {
      "caption": "Table 11: EMODB recognition rates (%) compared to the QNESN model",
      "data": [
        {
          "Ref": "Haider 2020 [45]",
          "WAR": "N/A",
          "UAR": "42.4"
        },
        {
          "Ref": "Papakostas 2017 [102]",
          "WAR": "44.00",
          "UAR": "N/A"
        },
        {
          "Ref": "Liu 2018 [103]",
          "WAR": "44.18",
          "UAR": "N/A"
        },
        {
          "Ref": "Noroozi and Marjanovic 2017 [104]",
          "WAR": "45.51",
          "UAR": "N/A"
        },
        {
          "Ref": "Vásquez-Correa 2016 [106]",
          "WAR": "47.30",
          "UAR": "N/A"
        },
        {
          "Ref": "Sun 2015 [30]",
          "WAR": "50.00",
          "UAR": "N/A"
        },
        {
          "Ref": "Sun 2017 [29]",
          "WAR": "51.46",
          "UAR": "49.33"
        },
        {
          "Ref": "Wen 2017 [36]",
          "WAR": "53.60",
          "UAR": "N/A"
        },
        {
          "Ref": "Tzinis 2018 [93]",
          "WAR": "54.00",
          "UAR": "53.80"
        },
        {
          "Ref": "Sugan 2020, [42]",
          "WAR": "55.83",
          "UAR": "N/A"
        },
        {
          "Ref": "Noroozi and Sapiński 2017 [105]",
          "WAR": "56.07",
          "UAR": "N/A"
        },
        {
          "Ref": "Sinith 2015 [69]",
          "WAR": "57.50",
          "UAR": "N/A"
        },
        {
          "Ref": "Sun 2015 [85]",
          "WAR": "58.76",
          "UAR": "N/A"
        },
        {
          "Ref": "Daneshfar 2020, [79]",
          "WAR": "59.38",
          "UAR": "55.00"
        },
        {
          "Ref": "Zhang 2021 [47]",
          "WAR": "60.16",
          "UAR": "N/A"
        },
        {
          "Ref": "Daneshfar 2020, [78]",
          "WAR": "60.79",
          "UAR": "N/A"
        },
        {
          "Ref": "Nguyen 2020 [107]",
          "WAR": "62.00",
          "UAR": "N/A"
        },
        {
          "Ref": "Jiang 2019 [108]",
          "WAR": "62.49",
          "UAR": "59.40"
        },
        {
          "Ref": "Wang 2020 [41]",
          "WAR": "66.20",
          "UAR": "81.8"
        },
        {
          "Ref": "Proposed",
          "WAR": "66.67",
          "UAR": "65.77"
        },
        {
          "Ref": "Farooq 2020 [109]",
          "WAR": "66.90",
          "UAR": "N/A"
        }
      ],
      "page": 30
    },
    {
      "caption": "Table 11: EMODB recognition rates (%) compared to the QNESN model",
      "data": [
        {
          "Ref": "Latif 2019 [110]",
          "WAR": "65.20",
          "UAR \nRef": "63.80 \nZhao 2020 [38]",
          "UAR": "68.00"
        },
        {
          "Ref": "Zong 2018 [111]",
          "WAR": "65.71",
          "UAR \nRef": "65.20 \nDaneshfar 2020, [78]",
          "UAR": "65.73"
        },
        {
          "Ref": "Latif 2020 [40]",
          "WAR": "66.32",
          "UAR \nRef": "N/A \nProposed",
          "UAR": "63.11"
        },
        {
          "Ref": "Kwon 2020 [112]",
          "WAR": "66.80",
          "UAR \nRef": "N/A \nDeb 2018, [73]",
          "UAR": "N/A"
        },
        {
          "Ref": "Ghosh 2016 [113]",
          "WAR": "66.80",
          "UAR \nRef": "N/A \nYi 2019 [39]",
          "UAR": "62.83"
        },
        {
          "Ref": "Xie 2019 [114]",
          "WAR": "66.92",
          "UAR \nRef": "N/A \nYi 2020 [97]",
          "UAR": "64.51"
        },
        {
          "Ref": "Li 2020 [43]",
          "WAR": "67.10",
          "UAR \nRef": "N/A \nLiu 2018 [103]",
          "UAR": "66.20"
        },
        {
          "Ref": "Zhao 2018 [118]",
          "WAR": "69.00",
          "UAR \nRef": "N/A \nYeh 2020",
          "UAR": "70.10"
        },
        {
          "Ref": "Huang 2018 [119]",
          "WAR": "71.50",
          "UAR \nRef": "65.70 \nSun 2020 [92]",
          "UAR": "N/A"
        },
        {
          "Ref": "Li 2021 [67]",
          "WAR": "71.75",
          "UAR \nRef": "N/A \nLi 2018 [115]",
          "UAR": "N/A"
        },
        {
          "Ref": "Xia 2015 [15]",
          "WAR": "73.02",
          "UAR \nRef": "61.70 \nFan 2020",
          "UAR": "65.86"
        },
        {
          "Ref": "Zhao 2019 [117]",
          "WAR": "74.80",
          "UAR \nRef": "60.90 \nDaneshfar 2020, [79]",
          "UAR": "N/A"
        },
        {
          "Ref": "Mao 2019 [120]",
          "WAR": "",
          "UAR \nRef": "N/A",
          "UAR": ""
        }
      ],
      "page": 30
    },
    {
      "caption": "Table 15: SAVEE confusion matrix using QNESN",
      "data": [
        {
          "61.29": "13.55",
          "15.74": "64.56",
          "11.98": "11.46",
          "11": "10.44"
        },
        {
          "61.29": "7.95",
          "15.74": "12.26",
          "11.98": "65.02",
          "11": "14.76"
        },
        {
          "61.29": "10.8",
          "15.74": "19",
          "11.98": "8.62",
          "11": "61.58"
        }
      ],
      "page": 32
    },
    {
      "caption": "Table 15: SAVEE confusion matrix using QNESN",
      "data": [
        {
          "61.29": "13.55",
          "15.74": "64.56",
          "11.98": "11.46",
          "11": "10.44"
        },
        {
          "61.29": "7.95",
          "15.74": "12.26",
          "11.98": "65.02",
          "11": "14.76"
        },
        {
          "61.29": "10.8",
          "15.74": "19",
          "11.98": "8.62",
          "11": "61.58"
        }
      ],
      "page": 32
    },
    {
      "caption": "Table 15: SAVEE confusion matrix using QNESN",
      "data": [
        {
          "fold number (gender in the test set)": "1 (m)",
          "WAR": "82.99"
        },
        {
          "fold number (gender in the test set)": "2 (f)",
          "WAR": "86.88"
        },
        {
          "fold number (gender in the test set)": "3 (f)",
          "WAR": "89.95"
        },
        {
          "fold number (gender in the test set)": "4 (m)",
          "WAR": "80.14"
        },
        {
          "fold number (gender in the test set)": "5 (m)",
          "WAR": "80.84"
        },
        {
          "fold number (gender in the test set)": "6 (m)",
          "WAR": "90.79"
        },
        {
          "fold number (gender in the test set)": "7 (f)",
          "WAR": "87.69"
        },
        {
          "fold number (gender in the test set)": "8 (f)",
          "WAR": "80.17"
        },
        {
          "fold number (gender in the test set)": "9 (m)",
          "WAR": "85.66"
        },
        {
          "fold number (gender in the test set)": "10 (f)",
          "WAR": "88.83"
        },
        {
          "fold number (gender in the test set)": "Mean (m)",
          "WAR": "84.08"
        },
        {
          "fold number (gender in the test set)": "Mean (f)",
          "WAR": "86.70"
        }
      ],
      "page": 32
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "79.95": ""
        },
        {
          "79.95": ""
        },
        {
          "79.95": ""
        }
      ],
      "page": 34
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Reservoir computing approaches to recurrent neural network training",
      "authors": [
        "M Lukoševičius",
        "H Jaeger"
      ],
      "year": "2009",
      "venue": "Computer Science Review"
    },
    {
      "citation_id": "2",
      "title": "Learning assistance by demonstration: Smart mobility with shared control and paired haptic controllers",
      "authors": [
        "H Soh",
        "Y Demiris"
      ],
      "year": "2015",
      "venue": "Journal of Human-Robot Interaction"
    },
    {
      "citation_id": "3",
      "title": "Adaptive elastic echo state network for multivariate time series prediction",
      "authors": [
        "M Xu",
        "M Han"
      ],
      "year": "2016",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "4",
      "title": "DeePr-ESN: A deep projection-encoding echo-state network",
      "authors": [
        "Q Ma",
        "L Shen",
        "G Cottrell"
      ],
      "year": "2020",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "5",
      "title": "A novel echo state network for multivariate and nonlinear time series prediction",
      "authors": [
        "L Shen",
        "J Chen",
        "Z Zeng",
        "J Yang",
        "J Jin"
      ],
      "year": "2018",
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "6",
      "title": "Mean-field theory of echo state networks",
      "authors": [
        "M Massar",
        "S Massar"
      ],
      "year": "2013",
      "venue": "Physical Review E"
    },
    {
      "citation_id": "7",
      "title": "Short term memory in echo state networks",
      "authors": [
        "H Jaeger"
      ],
      "year": "2002",
      "venue": "GMD R eport"
    },
    {
      "citation_id": "8",
      "title": "A local Echo State Property through the largest Lyapunov exponent",
      "authors": [
        "G Wainrib",
        "M Galtier"
      ],
      "year": "2016",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "9",
      "title": "Re-visiting the echo state property",
      "authors": [
        "I Yildiz",
        "H Jaeger",
        "S Kiebel"
      ],
      "year": "2012",
      "venue": "Neural networks"
    },
    {
      "citation_id": "10",
      "title": "Reservoir computing trends",
      "authors": [
        "M Lukoševičius",
        "H Jaeger",
        "B Schrauwen"
      ],
      "year": "2012",
      "venue": "KI-Künstliche Intelligenz"
    },
    {
      "citation_id": "11",
      "title": "The \"echo state\" approach to analysing and training recurrent neural networks-with an erratum note",
      "authors": [
        "H Jaeger"
      ],
      "year": "2001",
      "venue": "The \"echo state\" approach to analysing and training recurrent neural networks-with an erratum note"
    },
    {
      "citation_id": "12",
      "title": "Quaternion convolutional neural networks for end-to-end automatic speech recognition",
      "authors": [
        "T Parcollet",
        "Y Zhang",
        "M Morchid",
        "C Trabelsi",
        "G Linarès",
        "R De Mori",
        "Y Bengio"
      ],
      "year": "2018",
      "venue": "Interspeech 2018, 19th Annual conference of the international speech communication association",
      "doi": "10.21437/Interspeech.2018-1898"
    },
    {
      "citation_id": "13",
      "title": "Deep quaternion neural networks for spoken language understanding",
      "authors": [
        "T Parcollet",
        "M Morchid",
        "G Linares"
      ],
      "year": "2017",
      "venue": "IEEE Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "14",
      "title": "Bidirectional quaternion long short-term memory recurrent neural networks for speech recognition",
      "authors": [
        "T Parcollet",
        "M Morchid",
        "G Linarès",
        "R De Mori"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "A multi-task learning framework for emotion recognition using 2D continuous space",
      "authors": [
        "R Xia",
        "Y Liu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on affective computing"
    },
    {
      "citation_id": "16",
      "title": "On the capability of neural networks with complex neurons in complex valued functions approximation",
      "authors": [
        "P Arena",
        "L Fortuna",
        "R Re",
        "M Xibilia"
      ],
      "year": "1993",
      "venue": "1993 IEEE International Symposium on Circuits and Systems"
    },
    {
      "citation_id": "17",
      "title": "Separable spectro-temporal Gabor filter bank features: Reducing the complexity of robust features for automatic speech recognition",
      "authors": [
        "M Schädler",
        "B Kollmeier"
      ],
      "year": "2015",
      "venue": "The Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "18",
      "title": "Single frequency filtering approach for discriminating speech and nonspeech",
      "authors": [
        "G Aneeja",
        "B Yegnanarayana"
      ],
      "year": "2015",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "19",
      "title": "Quaternion convolutional neural networks for heterogeneous image processing",
      "authors": [
        "Titouan Parcollet",
        "Mohamed Morchid",
        "Georges Linarès"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "20",
      "title": "Quaternion Neural Networks for Multichannel Distant Speech Recognition",
      "authors": [
        "X Qiu",
        "T Parcollet",
        "M Ravanelli",
        "N Lane",
        "M Morchid"
      ],
      "year": "2020",
      "venue": "Quaternion Neural Networks for Multichannel Distant Speech Recognition",
      "arxiv": "arXiv:2005.08566"
    },
    {
      "citation_id": "21",
      "title": "Deep quaternion networks",
      "authors": [
        "C Gaudet",
        "A Maida"
      ],
      "year": "2018",
      "venue": "2018 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "22",
      "title": "Remarks on quaternion neural network-based controller trained by feedback error learning",
      "authors": [
        "K Takahashi",
        "A Isaka",
        "T Fudaba",
        "M Hashimoto"
      ],
      "year": "2017",
      "venue": "2017 IEEE/SICE International symposium on system integration (SII)"
    },
    {
      "citation_id": "23",
      "title": "Neural network inversion for multilayer quaternion neural networks",
      "authors": [
        "T Ogawa"
      ],
      "year": "2016",
      "venue": "Comput Technol Appl"
    },
    {
      "citation_id": "24",
      "title": "Geometric techniques for robotics and hmi: Interpolation and haptics in conformal geometric algebra and control using quaternion spike neural networks",
      "authors": [
        "E Bayro-Corrochano",
        "L Lechuga-Gutiérrez",
        "M Garza-Burgos"
      ],
      "year": "2018",
      "venue": "Robot Auton Syst"
    },
    {
      "citation_id": "25",
      "title": "Local hypercomplex analyticity",
      "authors": [
        "De Leo",
        "S Rotelli"
      ],
      "year": "1997",
      "venue": "Local hypercomplex analyticity"
    },
    {
      "citation_id": "26",
      "title": "A quaternary version of the back-propagation algorithm",
      "authors": [
        "T Nitta"
      ],
      "year": "1995",
      "venue": "IEEE International conference on neural networks, 1995. Proceedings"
    },
    {
      "citation_id": "27",
      "title": "Quaternion convolutional neural networks",
      "authors": [
        "X Zhu",
        "Y Xu",
        "H Xu",
        "C Chen"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "28",
      "title": "Neurocomputing",
      "authors": [
        "J Wu",
        "L Xu",
        "F Wu",
        "Y Kong",
        "L Senhadji",
        "H Shu"
      ],
      "year": "2020",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "29",
      "title": "Ensemble softmax regression model for speech emotion recognition",
      "authors": [
        "Y Sun",
        "G Wen"
      ],
      "year": "2017",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "30",
      "title": "Emotion recognition using semi-supervised feature selection with speaker normalization",
      "authors": [
        "Y Sun",
        "G Wen"
      ],
      "year": "2015",
      "venue": "International Journal of Speech Technology"
    },
    {
      "citation_id": "31",
      "title": "Analysis of excitation source features of speech for emotion recognition",
      "authors": [
        "S Kadiri",
        "P Gangamohan",
        "S Gangashetty",
        "B Yegnanarayana"
      ],
      "year": "2015",
      "venue": "Sixteenth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "32",
      "title": "Emotion recognition using prosodie and spectral features of speech and Naïve Bayes Classifier",
      "authors": [
        "A Khan",
        "U Roy"
      ],
      "year": "2017",
      "venue": "2017 international conference on wireless communications, signal processing and networking"
    },
    {
      "citation_id": "33",
      "title": "Exploration of phase information for speech emotion classification",
      "authors": [
        "S Deb",
        "S Dandapat"
      ],
      "year": "2017",
      "venue": "2017 Twenty-third National Conference on Communications (NCC)"
    },
    {
      "citation_id": "34",
      "title": "Speech emotion recognition based on power normalized cepstral coefficients in noisy conditions",
      "authors": [
        "M Bashirpour",
        "M Geravanchizadeh"
      ],
      "year": "2016",
      "venue": "Iranian Journal of Electrical and Electronic Engineering"
    },
    {
      "citation_id": "35",
      "title": "Feature analysis and evaluation for automatic emotion identification in speech",
      "authors": [
        "I Luengo",
        "E Navas",
        "I Hernáez"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "36",
      "title": "Random deep belief networks for recognizing emotions from speech signals",
      "authors": [
        "G Wen",
        "H Li",
        "J Huang",
        "D Li",
        "E Xun"
      ],
      "year": "2017",
      "venue": "Computational intelligence and neuroscience"
    },
    {
      "citation_id": "37",
      "title": "Wavelet-based time-frequency representations for automatic recognition of emotions from speech",
      "authors": [
        "J Vasquez-Correa",
        "T Arias-Vergara",
        "J Orozco-Arroyave",
        "J Vargas-Bonilla",
        "E Noeth"
      ],
      "year": "2016",
      "venue": "Speech Communication; 12. ITG Symposium"
    },
    {
      "citation_id": "38",
      "title": "Robust Semisupervised Generative Adversarial Networks for Speech Emotion Recognition via Distribution Smoothness",
      "authors": [
        "H Zhao",
        "Y Xiao",
        "Z Zhang"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "39",
      "title": "Adversarial data augmentation network for speech emotion recognition",
      "authors": [
        "L Yi",
        "M Mak"
      ],
      "year": "2019",
      "venue": "2019 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "40",
      "title": "Multi-task semi-supervised adversarial autoencoding for speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Epps",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "41",
      "title": "Wavelet packet analysis for speaker-independent emotion recognition",
      "authors": [
        "K Wang",
        "G Su",
        "L Liu",
        "S Wang"
      ],
      "year": "2020",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "42",
      "title": "Speech emotion recognition using cepstral features extracted with novel triangular filter banks based on bark and ERB frequency scales",
      "authors": [
        "N Sugan",
        "N Srinivas",
        "L Kumar",
        "M Nath",
        "A Kanhe"
      ],
      "year": "2020",
      "venue": "Digital Signal Processing"
    },
    {
      "citation_id": "43",
      "title": "Speaker-Invariant Affective Representation Learning via Adversarial Training",
      "authors": [
        "H Li",
        "M Tu",
        "J Huang",
        "S Narayanan",
        "P Georgiou"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "44",
      "title": "Two-layer fuzzy multiple random forest for speech emotion recognition in human-robot interaction",
      "authors": [
        "L Chen",
        "W Su",
        "Y Feng",
        "M Wu",
        "J She",
        "K Hirota"
      ],
      "year": "2020",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "45",
      "title": "Emotion recognition in low-resource settings: An evaluation of automatic feature selection methods",
      "authors": [
        "F Haider",
        "S Pollak",
        "P Albert",
        "S Luz"
      ],
      "year": "2020",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "46",
      "title": "An Efficient Language-Independent Acoustic Emotion Classification System",
      "authors": [
        "R Singh",
        "H Puri",
        "N Aggarwal",
        "V Gupta"
      ],
      "year": "2020",
      "venue": "Arabian Journal for Science and Engineering"
    },
    {
      "citation_id": "47",
      "title": "Speech feature selection and emotion recognition based on weighted binary cuckoo search",
      "authors": [
        "Z Zhang"
      ],
      "year": "2021",
      "venue": "Alexandria Engineering Journal"
    },
    {
      "citation_id": "48",
      "title": "A supervised non-negative matrix factorization model for speech emotion recognition",
      "authors": [
        "M Hou",
        "J Li",
        "G Lu"
      ],
      "year": "2020",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "49",
      "title": "Hilbert-Huang-Hurst-based non-linear acoustic feature vector for emotion classification with stochastic models and learning systems",
      "authors": [
        "V Vieira",
        "R Coelho",
        "F De Assis"
      ],
      "year": "2020",
      "venue": "IET Signal Processing"
    },
    {
      "citation_id": "50",
      "title": "Quaternion neural-network-based PolSAR land classification in Poincaresphere-parameter space",
      "authors": [
        "F Shang",
        "A Hirose"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Geoscience and Remote Sensing"
    },
    {
      "citation_id": "51",
      "title": "Echo state property of deep reservoir computing networks",
      "authors": [
        "C Gallicchio",
        "A Micheli"
      ],
      "year": "2017",
      "venue": "Cognitive Computation"
    },
    {
      "citation_id": "52",
      "title": "An echo state network architecture based on Volterra filtering and PCA with application to the channel equalization problem",
      "authors": [
        "L Boccato",
        "A Lopes",
        "R Attux",
        "F Von Zuben"
      ],
      "year": "2011",
      "venue": "The 2011 International Joint Conference on Neural Networks"
    },
    {
      "citation_id": "53",
      "title": "An extended echo state network using Volterra filtering and principal component analysis",
      "authors": [
        "L Boccato",
        "A Lopes",
        "R Attux",
        "F Von Zuben"
      ],
      "year": "2012",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "54",
      "title": "Nonlinear adaptive bilinear filters for active noise control systems",
      "authors": [
        "S Kuo",
        "H Wu"
      ],
      "year": "2005",
      "venue": "IEEE Transactions on Circuits and Systems I: Regular Papers"
    },
    {
      "citation_id": "55",
      "title": "A novel bilinear functional link neural network filter for nonlinear active noise control",
      "authors": [
        "L Luo",
        "J Sun"
      ],
      "year": "2018",
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "56",
      "title": "A novel adaptive bilinear filter based on pipelined architecture",
      "authors": [
        "J Zhang",
        "H Zhao"
      ],
      "year": "2010",
      "venue": "Digital Signal Processing"
    },
    {
      "citation_id": "57",
      "title": "Diagonal-structure adaptive bilinear filters for multichannel active noise control of nonlinear noise processes",
      "authors": [
        "C Dong",
        "Y Ding",
        "L Tan",
        "S Du",
        "X Guo"
      ],
      "year": "2020",
      "venue": "Mechanical Systems and Signal Processing"
    },
    {
      "citation_id": "58",
      "title": "M-max partial update leaky bilinear filter-error least mean square algorithm for nonlinear active noise control",
      "authors": [
        "D Le",
        "D Li",
        "J Zhang"
      ],
      "year": "2019",
      "venue": "Applied Acoustics"
    },
    {
      "citation_id": "59",
      "title": "Critical analysis of the impact of glottal features in the classification of clinical depression in speech",
      "authors": [
        "I Moore",
        "E Clements",
        "M Peifer",
        "J Weisser"
      ],
      "year": "2007",
      "venue": "IEEE transactions on biomedical engineering"
    },
    {
      "citation_id": "60",
      "title": "The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network",
      "authors": [
        "P Bartlett"
      ],
      "year": "1998",
      "venue": "IEEE transactions on Information Theory"
    },
    {
      "citation_id": "61",
      "title": "From image vector to matrix: A straightforward image projection technique-IMPCA vs",
      "authors": [
        "J Yang",
        "J Yang"
      ],
      "year": "2002",
      "venue": "PCA. pattern Recognition"
    },
    {
      "citation_id": "62",
      "title": "A database of German emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Ninth European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "63",
      "title": "Multimodal emotion recognition",
      "authors": [
        "S Haq",
        "P Jackson"
      ],
      "year": "2011",
      "venue": "Machine audition: principles, algorithms and systems"
    },
    {
      "citation_id": "64",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "65",
      "title": "3-D convolutional recurrent neural networks with attention model for speech emotion recognition",
      "authors": [
        "M Chen",
        "X He",
        "J Yang",
        "H Zhang"
      ],
      "year": "2018",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "66",
      "title": "Speech-based emotion recognition and speaker identification: static vs. dynamic mode of speech representation",
      "authors": [
        "M Sidorov",
        "W Minker",
        "E Semenkin"
      ],
      "year": "2016",
      "venue": "Journal of the Siberian Federal University. The series \"Mathematics and Physics"
    },
    {
      "citation_id": "67",
      "title": "Speech Emotion Recognition Using Scalogram Based Deep Structure",
      "authors": [
        "K Aghajani",
        "I Esmaili Paeen Afrakoti"
      ],
      "year": "2020",
      "venue": "International Journal of Engineering"
    },
    {
      "citation_id": "68",
      "title": "Exploiting the potentialities of features for speech emotion recognition",
      "authors": [
        "D Li",
        "Y Zhou",
        "Z Wang",
        "D Gao"
      ],
      "year": "2021",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "69",
      "title": "Automatic speech emotion recognition using auditory models with binary decision tree and svm",
      "authors": [
        "E Yüncü",
        "H Hacihabiboglu",
        "C Bozsahin"
      ],
      "year": "2014",
      "venue": "2014 22nd International Conference on Pattern Recognition"
    },
    {
      "citation_id": "70",
      "title": "Emotion recognition from audio signals using Support Vector Machine",
      "authors": [
        "M Sinith",
        "E Aswathi",
        "T Deepa",
        "C Shameema",
        "S Rajan"
      ],
      "year": "2015",
      "venue": "2015 IEEE Recent Advances in Intelligent Computational Systems (RAICS)"
    },
    {
      "citation_id": "71",
      "title": "Emotion classification using residual sinusoidal peak amplitude",
      "authors": [
        "Deb",
        "S Dandapat"
      ],
      "year": "2016",
      "venue": "2016 International Conference on Signal Processing and Communications (SPCOM)"
    },
    {
      "citation_id": "72",
      "title": "Exploration of phase information for speech emotion classification",
      "authors": [
        "S Deb",
        "S Dandapat"
      ],
      "year": "2017",
      "venue": "2017 Twenty-third National Conference on Communications (NCC)"
    },
    {
      "citation_id": "73",
      "title": "Emotion classification using segmentation of vowel-like and non-vowellike regions",
      "authors": [
        "S Deb",
        "S Dandapat"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "74",
      "title": "Multiscale amplitude feature and significance of enhanced vocal tract information for emotion classification",
      "authors": [
        "S Deb",
        "S Dandapat"
      ],
      "year": "2018",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "75",
      "title": "Spectral features based on local Hu moments of Gabor spectrograms for speech emotion recognition",
      "authors": [
        "H Tao",
        "R Liang",
        "C Zha",
        "X Zhang",
        "L Zhao"
      ],
      "year": "2016",
      "venue": "IEICE TRANSACTIONS on Information and Systems"
    },
    {
      "citation_id": "76",
      "title": "Speech Emotion Recognition based on SVM as Both Feature Selector and Classifier",
      "authors": [
        "A Shirani",
        "A Nilchi"
      ],
      "year": "2016",
      "venue": "International Journal of Image, Graphics & Signal Processing"
    },
    {
      "citation_id": "77",
      "title": "Speech Emotion Recognition from Raw Audio using Deep Learning",
      "authors": [
        "J Rintala"
      ],
      "year": "2020",
      "venue": "Speech Emotion Recognition from Raw Audio using Deep Learning"
    },
    {
      "citation_id": "78",
      "title": "Classification of emotional speech using 3DEC hierarchical classifier",
      "authors": [
        "A Hassan",
        "R Damper"
      ],
      "year": "2012",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "79",
      "title": "Speech emotion recognition using hybrid spectralprosodic features of speech signal/glottal waveform, metaheuristic-based dimensionality reduction, and Gaussian elliptical basis function network classifier",
      "authors": [
        "F Daneshfar",
        "S Kabudian",
        "A Neekabadi"
      ],
      "year": "2020",
      "venue": "Applied Acoustics",
      "doi": "10.1016/j.apacoust.2020.107360"
    },
    {
      "citation_id": "80",
      "title": "Speech emotion recognition using discriminative dimension reduction by employing a modified quantum-behaved particle swarm optimization algorithm",
      "authors": [
        "F Daneshfar",
        "S Kabudian"
      ],
      "year": "2020",
      "venue": "Multimedia Tools and Applications",
      "doi": "10.1007/s11042-019-08222-8"
    },
    {
      "citation_id": "81",
      "title": "Time-frequency feature and AMS-GMM mask for acoustic emotion classification",
      "authors": [
        "L Zao",
        "D Cavalcante",
        "R Coelho"
      ],
      "year": "2014",
      "venue": "IEEE signal processing letters"
    },
    {
      "citation_id": "82",
      "title": "Improving automatic emotion recognition from speech using rhythm and temporal feature",
      "authors": [
        "M Bhargava",
        "T Polzehl"
      ],
      "year": "2013",
      "venue": "Improving automatic emotion recognition from speech using rhythm and temporal feature",
      "arxiv": "arXiv:1303.1761"
    },
    {
      "citation_id": "83",
      "title": "Deep features-based speech emotion recognition for smart affective services",
      "authors": [
        "A Badshah",
        "N Rahim",
        "N Ullah",
        "J Ahmad",
        "K Muhammad",
        "M Lee",
        "S Kwon",
        "S Baik"
      ],
      "year": "2019",
      "venue": "Deep features-based speech emotion recognition for smart affective services"
    },
    {
      "citation_id": "84",
      "title": "Speech emotion recognition using an enhanced kernel isomap for human-robot interaction",
      "authors": [
        "S Zhang",
        "X Zhao",
        "B Lei"
      ],
      "year": "2013",
      "venue": "International Journal of Advanced Robotic Systems"
    },
    {
      "citation_id": "85",
      "title": "Automatic speech emotion recognition using modulation spectral features",
      "authors": [
        "S Wu",
        "T Falk",
        "W Chan"
      ],
      "year": "2011",
      "venue": "Speech communication"
    },
    {
      "citation_id": "86",
      "title": "Weighted spectral features based on local Hu moments for speech emotion recognition",
      "authors": [
        "Y Sun",
        "G Wen",
        "J Wang"
      ],
      "year": "2015",
      "venue": "Biomedical signal processing and control"
    },
    {
      "citation_id": "87",
      "title": "Dimensionality reduction for speech emotion features by multiscale kernels",
      "authors": [
        "X Xu",
        "J Deng",
        "W Zheng",
        "L Zhao",
        "B Schuller"
      ],
      "year": "2015",
      "venue": "Sixteenth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "88",
      "title": "Feature Selection and Nuisance Attribute Projection for Speech Emotion Recognition",
      "year": "2016",
      "venue": "Lecture Note Series"
    },
    {
      "citation_id": "89",
      "title": "Deep neural networks for acoustic emotion recognition: Raising the benchmarks",
      "authors": [
        "A Stuhlsatz",
        "C Meyer",
        "F Eyben",
        "T Zielke",
        "G Meier",
        "B Schuller"
      ],
      "year": "2011",
      "venue": "2011 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "90",
      "title": "Biologically inspired speech emotion recognition",
      "authors": [
        "R Lotfidereshgi",
        "P Gournay"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "91",
      "title": "Decision tree SVM model with Fisher feature selection for speech emotion recognition",
      "authors": [
        "L Sun",
        "S Fu",
        "F Wang"
      ],
      "year": "2019",
      "venue": "EURASIP Journal on Audio, Speech, and Music Processing"
    },
    {
      "citation_id": "92",
      "title": "Sparse Autoencoder with Attention Mechanism for Speech Emotion Recognition",
      "authors": [
        "T Sun",
        "A Wu"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS)"
    },
    {
      "citation_id": "93",
      "title": "End-to-End speech emotion recognition with gender information",
      "authors": [
        "T Sun"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "94",
      "title": "Integrating Recurrence Dynamics for Speech Emotion Recognition",
      "authors": [
        "E Tzinis",
        "G Paraskevopoulos",
        "C Baziotis",
        "A Potamianos"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "95",
      "title": "Segment-based speech emotion recognition using recurrent neural networks",
      "authors": [
        "E Tzinis",
        "A Potamianos"
      ],
      "year": "2017",
      "venue": "2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "96",
      "title": "Analysis of Multi-Lingual Emotion Recognition Using Auditory Attention Features",
      "authors": [
        "O Kalinli"
      ],
      "year": "2016",
      "venue": "INTERSPEECH"
    },
    {
      "citation_id": "97",
      "title": "Speech emotion recognition using deep convolutional neural network and discriminant temporal pyramid matching",
      "authors": [
        "S Zhang",
        "S Zhang",
        "T Huang",
        "W Gao"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "98",
      "title": "Improving Speech Emotion Recognition With Adversarial Data Augmentation Network",
      "authors": [
        "L Yi",
        "M Mak"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "99",
      "title": "Speech emotion analysis: Exploring the role of context",
      "authors": [
        "A Tawari",
        "M Trivedi"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on multimedia"
    },
    {
      "citation_id": "100",
      "title": "Investigation of the effect of spectrogram images and different texture analysis methods on speech emotion recognition",
      "authors": [
        "T Özseven"
      ],
      "year": "2018",
      "venue": "Applied Acoustics"
    },
    {
      "citation_id": "101",
      "title": "A novel feature selection method for speech emotion recognition",
      "authors": [
        "T Özseven"
      ],
      "year": "2019",
      "venue": "Applied Acoustics"
    },
    {
      "citation_id": "102",
      "title": "Speech emotion recognition from 3D log-mel spectrograms with deep learning network",
      "authors": [
        "H Meng",
        "T Yan",
        "F Yuan",
        "H Wei"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "103",
      "title": "Deep visual attributes vs. hand-crafted audio features on multidomain speech emotion recognition",
      "authors": [
        "M Papakostas",
        "E Spyrou",
        "T Giannakopoulos",
        "G Siantikos",
        "D Sgouropoulos",
        "P Mylonas",
        "F Makedon"
      ],
      "year": "2017",
      "venue": "Computation"
    },
    {
      "citation_id": "104",
      "title": "Speech emotion recognition based on an improved brain emotion learning model",
      "authors": [
        "Z Liu",
        "Q Xie",
        "M Wu",
        "W Cao",
        "Y Mei",
        "J Mao"
      ],
      "year": "2018",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "105",
      "title": "Audio-visual emotion recognition in video clips",
      "authors": [
        "F Noroozi",
        "M Marjanovic",
        "A Njegus",
        "S Escalera",
        "G Anbarjafari"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "106",
      "title": "Vocal-based emotion recognition using random forests and decision tree",
      "authors": [
        "F Noroozi",
        "T Sapiński",
        "D Kamińska",
        "G Anbarjafari"
      ],
      "year": "2017",
      "venue": "International Journal of Speech Technology"
    },
    {
      "citation_id": "107",
      "title": "Wavelet-based time-frequency representations for automatic recognition of emotions from speech",
      "authors": [
        "J Vasquez-Correa",
        "T Arias-Vergara",
        "J Orozco-Arroyave",
        "J Vargas-Bonilla",
        "E Noeth"
      ],
      "year": "2016",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "108",
      "title": "ITG Symposium",
      "venue": "ITG Symposium"
    },
    {
      "citation_id": "109",
      "title": "Joint Deep Cross-Domain Transfer Learning for Emotion Recognition",
      "authors": [
        "D Nguyen",
        "S Sridharan",
        "D Nguyen",
        "S Denman",
        "S Tran",
        "R Zeng",
        "C Fookes"
      ],
      "year": "2020",
      "venue": "Joint Deep Cross-Domain Transfer Learning for Emotion Recognition",
      "arxiv": "arXiv:2003.11136"
    },
    {
      "citation_id": "110",
      "title": "Parallelized Convolutional Recurrent Neural Network With Spectral Features for Speech Emotion Recognition",
      "authors": [
        "P Jiang",
        "H Fu",
        "H Tao",
        "P Lei",
        "L Zhao"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "111",
      "title": "Impact of Feature Selection Algorithm on Speech Emotion Recognition Using Deep Convolutional Neural Network",
      "authors": [
        "M Farooq",
        "F Hussain",
        "N Baloch",
        "F Raja",
        "H Yu",
        "Y Zikria"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "112",
      "title": "Direct Modelling of Speech Emotion from Raw Speech. Proc. Interspeech",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Epps"
      ],
      "year": "2019",
      "venue": "Direct Modelling of Speech Emotion from Raw Speech. Proc. Interspeech"
    },
    {
      "citation_id": "113",
      "title": "Multi-Channel Auto-Encoder for Speech Emotion Recognition",
      "authors": [
        "Z Zong",
        "H Li",
        "Q Wang"
      ],
      "year": "2018",
      "venue": "Multi-Channel Auto-Encoder for Speech Emotion Recognition",
      "arxiv": "arXiv:1810.10662"
    },
    {
      "citation_id": "114",
      "title": "A CNN-Assisted Enhanced Audio Signal Processing for Speech Emotion Recognition",
      "authors": [
        "S Kwon"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "115",
      "title": "Representation Learning for Speech Emotion Recognition",
      "authors": [
        "S Ghosh",
        "E Laksana",
        "L Morency",
        "S Scherer"
      ],
      "year": "2016",
      "venue": "Interspeech"
    },
    {
      "citation_id": "116",
      "title": "Attention-Based Dense LSTM for Speech Emotion Recognition",
      "authors": [
        "Y Xie",
        "R Liang",
        "Z Liang",
        "L Zhao"
      ],
      "year": "2019",
      "venue": "IEICE TRANSACTIONS on Information and Systems"
    },
    {
      "citation_id": "117",
      "title": "An Attention Pooling Based Representation Learning Method for Speech Emotion Recognition",
      "authors": [
        "P Li",
        "Y Song",
        "I Mcloughlin",
        "W Guo",
        "L Dai"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "118",
      "title": "From simulated speech to natural speech, what are the robust features for emotion recognition?",
      "authors": [
        "Y Li",
        "L Chao",
        "Y Liu",
        "W Bao",
        "J Tao"
      ],
      "year": "2015",
      "venue": "2015 International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "119",
      "title": "Compact Convolutional Recurrent Neural Networks via Binarization for Speech Emotion Recognition",
      "authors": [
        "H Zhao",
        "Y Xiao",
        "J Han",
        "Z Zhang"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "120",
      "title": "Exploring spatio-temporal representations by integrating attention-based bidirectional-LSTM-RNNs and FCNs for speech emotion recognition",
      "authors": [
        "Z Zhao",
        "Y Zheng",
        "Z Zhang",
        "H Wang",
        "Y Zhao",
        "C Li"
      ],
      "year": "2018",
      "venue": "Exploring spatio-temporal representations by integrating attention-based bidirectional-LSTM-RNNs and FCNs for speech emotion recognition"
    },
    {
      "citation_id": "121",
      "title": "Speech Emotion Recognition from Variable-Length Inputs with Triplet Loss Function",
      "authors": [
        "J Huang",
        "Y Li",
        "J Tao",
        "Z Lian"
      ],
      "year": "2018",
      "venue": "Interspeech"
    },
    {
      "citation_id": "122",
      "title": "Revisiting hidden Markov models for speech emotion recognition",
      "authors": [
        "S Mao",
        "D Tao",
        "G Zhang",
        "P Ching",
        "T Lee"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "123",
      "title": "Towards Temporal Modelling of Categorical Speech Emotion Recognition",
      "authors": [
        "W Han",
        "H Ruan",
        "X Chen",
        "Z Wang",
        "H Li",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Towards Temporal Modelling of Categorical Speech Emotion Recognition"
    },
    {
      "citation_id": "124",
      "title": "Speech emotion recognition with deep convolutional neural networks",
      "authors": [
        "D Issa",
        "M Demirci",
        "A Yazici"
      ],
      "year": "2020",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "125",
      "title": "CNN+ LSTM Architecture for Speech Emotion Recognition with Data Augmentation",
      "authors": [
        "C Etienne",
        "G Fidanza",
        "A Petrovskii",
        "L Devillers",
        "B Schmauch"
      ],
      "year": "2018",
      "venue": "Proc. Workshop on Speech, Music and Mind"
    },
    {
      "citation_id": "126",
      "title": "Evaluating deep learning architectures for Speech Emotion Recognition",
      "authors": [
        "H Fayek",
        "M Lech",
        "L Cavedon"
      ],
      "year": "2017",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "127",
      "title": "Speech Emotion Recognition Based Robust Discriminative Sparse Regression",
      "authors": [
        "P Song",
        "W Zheng",
        "Y Yu",
        "S Ou"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "128",
      "title": "A Novel Approach for Classification of Speech Emotions Based on Deep and Acoustic Features",
      "authors": [
        "M Er"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "129",
      "title": "Clustering-Based Speech Emotion Recognition by Incorporating Learned Features and Deep BiLSTM",
      "authors": [
        "M Sajjad",
        "S Kwon"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "130",
      "title": "Att-Net: Enhanced emotion recognition system using lightweight self-attention module",
      "authors": [
        "S Kwon"
      ],
      "year": "2021",
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "131",
      "title": "Recognition of emotion from speech using evolutionary cepstral coefficients",
      "authors": [
        "A Bakhshi",
        "S Chalup",
        "A Harimi",
        "S Mirhassani"
      ],
      "year": "2020",
      "venue": "Recognition of emotion from speech using evolutionary cepstral coefficients"
    }
  ]
}