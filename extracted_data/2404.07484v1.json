{
  "paper_id": "2404.07484v1",
  "title": "Multimodal Emotion Recognition By Fusing Video Semantic In Mooc Learning Scenarios",
  "published": "2024-04-11T05:44:27Z",
  "authors": [
    "Yuan Zhang",
    "Xiaomei Tao",
    "Hanxu Ai",
    "Tao Chen",
    "Yanling Gan"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In the Massive Open Online Courses (MOOC) learning scenario, the semantic information of instructional videos has a crucial impact on learners' emotional state. Learners mainly acquire knowledge by watching instructional videos, and the semantic information in the videos directly affects learners' emotional states. However, few studies have paid attention to the potential influence of the semantic information of instructional videos on learners' emotional states. To deeply explore the impact of video semantic information on learners' emotions, this paper innovatively proposes a multimodal emotion recognition method by fusing video semantic information and physiological signals. We generate video descriptions through a pre-trained large language model (LLM) to obtain high-level semantic information about instructional videos. Using the cross-attention mechanism for modal interaction, the semantic information is fused with the eye movement and PhotoPlethysmoGraphy (PPG) signals to obtain the features containing the critical information of the three modes. The accurate recognition of learners' emotional states is realized through the emotion classifier. The experimental results show that our method has significantly improved emotion recognition performance, providing a new perspective and efficient method for emotion recognition research in MOOC learning scenarios. The method proposed in this paper not only contributes to a deeper understanding of the impact of instructional videos on learners' emotional states but also provides a beneficial reference for future research on emotion recognition in MOOC learning scenarios.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In recent years, emotion recognition in MOOC learning has received much attention. Although MOOC learning has advantages such as transcending time and space constraints and abundant learning resources, the problem of high dropout rates remains prominent  [2, 18] , and emotional deficiency is one of the essential reasons  [26] . Emotions play a regulatory and mediating role in cognitive processes  [29]  ; therefore, in the fields of education and cognitive science, understanding and managing emotions is crucial for optimizing learning strategies  [9]  . Research has shown that the presentation of different content in videos can affect the activity of the emotion regulation areas in the human brain, thereby influencing an individual's emotional state  [14, 32]  . Some studies incorporated physical level information such as brightness and saturation from videos into emotion recognition tasks  [37]  , which effectively improve accuracy, but fail to pay attention to the higher-level semantic information related to content carried by videos. At present, most research on affective recognition in MOOC learning often overlooks the potential impact of semantic information in instructional videos on learners' emotions.\n\nEarly research focused on analyzing students' engagement in learning through classroom tests and student evaluations  [12, 20] . More and more studies have been conducted to identify learners' emotional states by acquiring signals such as facial expressions, eye movements, PPG, Electroencephalogram (EEG), Electrodermal Activity (EDA), etc. during the learning process  [3, 23, 36, 37]  . And achieve accurate identification of learners' emotional states by fusing information from multiple modal data at the data level, feature level, and decision level  [3, 4, 21, 28] . In MOOC learning scenarios, learners' emotions are closely related to learners' personal cognition and semantic information in videos, and the presentation of different contents in teaching videos will bring different feelings to learners  [11, 19, 22] . Therefore, we hypothesize that in MOOC learning scenarios, learners' emotions are closely related to the semantic information of instructional videos.\n\nWe propose a multimodal emotion recognition method based on the above analysis by fusing video semantic information and physiological signals. Specifically, we extract semantic information from instructional videos and fuse it with eye movement and PPG signals through the crossattention mechanism to improve the performance of emotion recognition in MOOC learning. To the best of our knowledge, we are the first to apply the semantic information of instructional videos to emotion recognition tasks in MOOC learning, providing a new perspective for emotion recognition research in MOOC learning scenarios. The main contributions of this article can be summarized as follows:\n\n‚Ä¢ We hypothesize there is a close correlation between the semantic information of instructional videos and learners' emotions. To deeply explore the impact of video semantic information on learners' emotions, this study innovatively proposes a multimodal emotion recognition method that integrates video semantic information and physiological signals.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Works",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Emotion Recognition Using Contextual Or Semantic Information",
      "text": "Adding context or semantic information to emotion recognition tasks has gradually become a focus of attention. After adding context information, emotion recognition systems can more accurately infer related emotions. Kosti et al.  [15]  believe that in addition to facial expressions and body postures, scene context also provides important information for us to perceive people's emotions. Scene context information is also a key component in understanding emotional states. Therefore, they created and released the Emotions In Context (EMOTIC) dataset and proposed a baseline CNN model for emotion recognition in scene context. Dashtipour et al.  [7]  proposed a context-aware multi-modal sentiment analysis framework to predict emotional states accurately.\n\nIn addition, some studies use speech transcription to text to obtain additional contextual or semantic information based on audio and visual information. Jiang et al.  [13]  proposed a fuzzy temporal convolutional network based on context selfattention (CSAT-FTCN) to improve the effect of emotion recognition by using speech-transcribed text as a new modality and integrates it with the original audio and visual modalities. Xia et al.  [35]  transcribed speech into the text as semantic information to enhance audio and visual features. Meanwhile, semantic information also serves as a new modality for decision fusion with audio and video modalities for emotion recognition. Tzirakis et al.  [30]  enhanced the performance and effectiveness of emotion recognition by transcribing speech into text as semantic information in speech emotion recognition tasks. The above research shows that incorporating contextual or semantic information into emotion recognition tasks has a positive effect. We believe that further incorporating the high-level semantic information of videos as a global factor into MOOC learning scenarios will also have a positive effect on learners' emotion recognition task.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multimodal Emotion Recognition Based On Attention Mechanism",
      "text": "In the field of emotion recognition, multimodal fusion is a challenging task. Early research typically used traditional data level, feature level, or decision level fusion methods  [16, 33] . However, with the rise of attention mechanisms, research focus has gradually shifted towards cross-modal interaction  [27, 35] . For example, Wang et al.  [34]  utilized an attentionbased fusion emotion transformer fusion (ETF) framework to integrate features from EEG and eye movement signals. Xia et al.  [35]  designed a semantic enhancement module based on the attention mechanism, which enhances audio and visual features through semantic information. At the same time, semantic information is also integrated with audio and video as a new modality to improve emotion recognition performance. Gong et al.  [10]  proposed an intra-and intermodality attention fusion network that effectively learns the critical information between the two modalities and improves the effectiveness of emotion recognition. These studies show that using attention mechanisms can better learn the correlation and complementarity between different modalities, thus achieving more effective multimodal fusion effects.\n\nBased on the semantic information in instructional videos, we propose a multimodal multimodal emotion recognition method by fusing video semantics and physiological signals. By generating video descriptions, we obtain the high-level semantic representation of instructional videos and use crossattention mechanisms to fuse them with eye movement and PPG signals, effectively improving the performance of MOOC learning emotion recognition.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Proposed Method",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Overall Framework Of The Method",
      "text": "The multimodal emotion recognition method by fusing video semantic information and physiological signals consists of three main stages: data processing and feature extraction, Multimodal Emotion Recognition by Fusing Video Semantic in MOOC Learning Scenarios cross-attention fusion, and emotion classification (as shown in Figure  1 ). Firstly, in the data processing and feature extraction stages, we preprocess and extract physiological signals and video semantic information respectively. The physiological signals take eye movement and PPG signals as examples, while video semantic information is derived from the extraction of video stimulus materials in video learning scenarios. Then, the extracted eye movement, PPG, and video semantic features are fed into the fusion module. The fusion module is mainly based on the cross-attention mechanism, which combines the features of three modalities in any pairwise manner and inputs them into multi-head attention to learn the corresponding feature representations. Then, these features are further fused to obtain features that contain important information for three modalities. Finally, the fused features are input into the sentiment classifier for final sentiment prediction.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Video Semantic Information Generation And Feature Extraction",
      "text": "In this study, a crucial task is to obtain semantic information from videos and extract features from video semantic information. To achieve this goal, we first use pretrained LLM to generate video descriptions to obtain semantic information in instructional videos. Then, we used the pretrained BERT model  [8]  to extract the features of video semantic information. The specific process is shown in Figure  2 : Firstly, to ensure the stability of the subsequent running process, we preprocess the original videos and convert them to a unified resolution (1280 x 720), frame rate (25fps), and target bit rate (1000k). Then, we feed the videos into the pretrained model mPLUG-Owl  [38]  (The mPLUG-Owl model is available on the GitHub, HuggingFace, or ModelScope platforms) trained on LLM for generating video descriptions to obtain semantic information. The automatic generation of semantic information in instructional videos has been achieved through the mPLUG-Owl model. As shown in the example in Figure  2 , the generated semantic information includes key content such as scenes, objects, actions, and plots in the video.\n\nFor extracting video semantic features, we first perform text cleaning on the obtained semantic information and then put the preprocessed video semantic information into the Bert model for feature extraction. Semantic information is encoded in tokenized form during this process, and positional encoding is added to each token. Subsequently, after processing by multiple Transformer encoder layers, the model utilizes the self-attention mechanism and feedforward neural network to capture the semantic relationships between tokens. Next, feature representations are extracted from the last Transformer encoder layer to obtain high-quality semantic features. Due to the high dimensionality of the features extracted using the Bert model, we applied the PCA algorithm to reduce the dimensionality of semantic features. The feature dimensions were reduced to 20, 25, 50, 70, and 100, respectively, and experiments showed that the best effect was achieved at 25 dimensions. Therefore, we chose to reduce the semantic features to 25 dimensions and further use LSTM for encoding to obtain the final video semantic features for subsequent experiments.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Cross-Attention Fusion",
      "text": "To effectively learn important information between different modalities, we use Multi-Head Attention (MHA) to model the cross-attention fusion module and use MHA to learn information between any two modalities separately. Each MHA module requires three inputs, namely Query(Q), Key(K), and Value(V). In this article, when learning information from two modalities, we use one modality as the input for Q, while the other modality serves as both K and V. Each input is first projected into a different subspace using a linear layer H times, where H represents the number of heads. The projection of each subspace ‚Ñé ‚àà {0, . . . , ùêª -1} is expressed as:\n\nWhere ùëö 1 , ùëö 2 ‚àà {ùëí, ùëù, ùë†} represents the modality used. In each subspace, scaled dot product attention operations were performed on these projections. For subspace ‚Ñé, the attention operation expression is as follows:\n\nWhere ùê¥ùë°ùë° ‚Ñé (‚ãÖ) refers to attentional operations in subspace h, and ùëë ùëò is the characteristic dimension. All H attention outputs are connected in series and passed through a linear layer to obtain the final output of the multi-head attention (MHA) module.\n\nTo achieve an effective fusion of different modalities, we input the features of any two modalities into MHA and obtained feature weights that contain common information between these two modalities. Subsequently, we perform a mean operation to obtain the final feature weights ùúî ùëö1ùëö2 for these two modalities, as follows:\n\nWhere ùëì ùëö1 , ùëì ùëö2 ‚àà {ùëì ùëí , ùëì ùëù , ùëì ùë† } represents the features of any two modalities. The feature weights of any two modes are calculated to represent: ùúî ùëíùë† , ùúî ùëùùë† , ùúî ùë†ùëí , ùúî ùëùùëí , ùúî ùë†ùëù , ùúî ùëíùëù . Finally, these weights information are stacked to achieve an effective fusion of the three modalities of eye movement, PPG, and video semantic information, obtained feature weights ùúî ùëíùëùùë† containing important information from three modalities:\n\nùúî ùëíùëùùë† = [ùúî ùëíùë† , ùúî ùëùùë† , ùúî ùë†ùëí , ùúî ùëùùëí , ùúî ùë†ùëù , ùúî ùëíùëù ] (6)\n\nFinally, the fused multimodal features are fed into the emotion classifier for emotion prediction. The classifier is composed of two fully connected layers, and the softmax activation function is used in the second fully connected layer. The mathematical expression of emotion prediction is as follows:\n\nWhere ùë¶ ÃÇ represents the final emotion prediction result, ùêπùê∂ ùúÉ 1 and ùêπùê∂ ùúÉ 2 represent fully connected layers with parameters ùúÉ 1 and ùúÉ 2 , respectively, ùúá and ùúé are the average and standard deviation calculated from the output ùúî ùëíùëùùë† of the fusion module, and + represents the concatenation operation.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dataset And Data Processing",
      "text": "To verify the effectiveness of our proposed method, we conducted experiments on the Video Learning Multimodal Emotion Dataset (VLMED)  [3, 37] , which contained the subjects' eye movement, PPG, facial expression, EDA data and the instructional videos watched by the subjects. The data was collected while the subjects watched instructional videos. This dataset simulates MOOC learning scenarios during the collection process, using 5 carefully selected instructional videos to induce different types of emotions: interest, boredom, happiness, confusion, and distraction.\n\nThe experiment collected data from 68 subjects, each of whom watched 5 videos in sequence, including 4 shorter (about 2-3 minutes) and 1 longer (about 10 minutes) instructional video.\n\nIn this study, we mainly used eye movement, PPG data, and instructional videos from this dataset. Extract data with a time window of 1 second, and process and extract features from eye movement and PPG data using the same methods as in papers  [3]  and  [37]  , respectively. We also extracted semantic information from instructional videos to expand the data set. The acquisition method of video semantic information and its feature extraction are introduced in Section 3.2.\n\nDuring the experiment, we observed that the model performed very poorly in recognizing the emotion of Interest category, and the same was encountered in the work of literature 1  [3]  and literature 2  [24]  . We speculate that it may be caused by unbalanced samples in the data set. To solve this problem, we adopted the ADASYN sampling approach  [34]  to enhance the data. ADASYN is a data resampling-based method that synthesizes small sample categories in the feature space to generate high-quality new samples, thereby balancing the distribution of samples in different categories. The number of samples before and after adaptive synthesis sampling is shown in Table  4",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Experimental Setting",
      "text": "In this study, we used NVIDIA GeForce RTX 3070 GPU as the computing platform and constructed and trained the entire model using the TensorFlow framework. We divided the dataset into training and testing sets according to the ratio of 8:2 and trained the model using 5-fold cross-validation on the training set. At the same time, we evaluated the performance of the model using the testing set. During the experiment, we attempted different parameter combinations to determine the optimal parameter configuration as the final parameters of the model. The final network and training parameters of the model are set as follows:\n\nNetwork ParametersÔºöIn the data processing and feature extraction module, we used a Conv1D and a LSTM network to encode eye movement and PPG features. Conv1D includes 16 filters of size 1 and uses ReLU as the activation function; LSTM contains 64 hidden units. Encode semantic features using a LSTM with 64 hidden units. In the cross-attention fusion module, num\\_heads=8, key\\_dim=128, and value\\_dim=64 in multi-head attention. The emotion classifier consists of two fully connected layers, the first consisting of 64 units, while the second consists of 4 units and uses the softmax activation function to achieve the classification of 4 emotions. In addition, we have introduced L2 regularization (l2=0.001) at various levels of the network to reduce model complexity, prevent overfitting, and enhance the model's generalization ability.\n\nTraining ParametersÔºöWhen training, we use sparse categorical cross-entropy as the loss function, Adam as the optimizer, and set the random seed to 7 to ensure the repeatability of the experimental results. Set batch size = 32, epoch = 500, and learning rate= 1e-3. To avoid overfitting, we set the learning rate decay and early stop criteria for model training. If the model does not show improvement for 5 consecutive epochs, the learning rate is attenuated to the original 0.1. When the model does not show better performance for 10 consecutive epochs, we determine that the model is overfitted and terminate the training.\n\nTo evaluate the effectiveness of the model, we comprehensively tested its performance using 5-fold crossvalidation. We calculated the average accuracy (Avg acc ) , average recall (Avg recall ) , and average F1 score (Avg f1 ) as evaluation metrics.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Results And Analysis",
      "text": "Figure  3  shows the confusion matrix for each fold of our model under 5-fold cross-validation. It can be observed that the difference between the results of each fold is not large, which indicates that our proposed model shows effective and stable performance in MOOC learning scenarios. However, we found that the model had relatively low accuracy in identifying the two categories of Interest and Confusion. Specifically, the Interest category is easily misclassified as either Happiness or Confusion, which may be due to both Interest and Happiness representing positive emotions, so they are easy to confuse when classifying emotions. Similarly, Confusion and Boredom are both negative emotions, resulting in some Confusion samples being incorrectly classified as Boredom. In addition, compared with Happiness and Boredom, Interest and Confusion are neutral emotions with low emotional intensity, and their emotion scores are similar. Therefore, some samples of Interest and Confusion have similar feature distributions on the two physiological signals of eye movement and PPG, which is difficult to distinguish effectively. The above analysis is confirmed in the visualization results of feature distribution in Figure  5 .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Effectiveness Of Adaptive Synthetic Sampling.",
      "text": "To overcome the potential impact of imbalanced data distribution, we adopted the Adaptive Synthesis (ADASYN) sampling method for data augmentation, and its effectiveness was verified through experiments. The experimental results are shown in Figure  4 , where (a) and (b) are the ROC curves before and after using ADASYN, respectively. It can be found that without ADASYN processing, the model performs poorly in recognizing the emotion of Interest category. After ADASYN processing, the model has significantly improved its recognition of various emotions. This method effectively alleviates the problem caused by the unbalanced data distribution and improves the model's overall performance. It should be emphasized that the enhanced data were used in other experiments in this paper.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Compare With Other Models.",
      "text": "To better demonstrate the effectiveness of our model, using the data collected in this paper, we reproduce six baseline classifiers and compare them with the methods proposed in this paper, including the traditional machine learning method K nearest neighbor (KNN)  [6]  , deep learning methods of LSTM  [1] , CNN-LSTM  [5]  , as well as Transformer  [31] , CNN-LSTM-MHA-TCN (CLA-TCN)  [37] , and Cascade Multi-Head Attention(CMHA)  [39]  using attention mechanisms. The results are shown in Table  2 . The effect obtained by using the deep learning method is significantly better than that obtained by the machine learning method, indicating that the deep learning method can extract deeper features. When the attention mechanism is used, the effect is further improved, and our method achieves the best performance, indicating that our model can effectively learn important information between different modalities and more efficiently fuse information from different modalities.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Table 2. Results Compared With Other Models",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Model",
      "text": "Acc¬±std(%) Recall(%) F1 KNN  [6]  59.56¬±2.7 59.67 0.58 LSTM  [1]  67.59¬±2.0 67.89 0.67 CNN-LSTM  [5]  73.23¬±1.7 73.14 0.73 Transformer  [31]  78.72¬±1.8 78.96 0.78 CLA-TCN  [37]  82.52¬±2.1 81.03 0.81 CMHA  [39]  83.97¬±1.2 84.01 0.84 Ours 86.69¬±0.7 86.62 0.87",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Comparison With Different Semantic Information.",
      "text": "Unlike most studies that transcribe audio into text as semantic information, we use generated video descriptions as semantic information. To demonstrate the effectiveness of the proposed method, we conducted experiments using caption semantic (Audio transcription into text subtitles as semantic information) and semantic information generated by BiliGPT (https://bibigpt.co, First transcribe the audio into text and then further summarize it as semantic information). Table  3  shows the experimental results. We can see that the emotion recognition effect is improved after using subtitle semantics, but it is not as good as the summary subtitle with reduced redundant information after the summary. When description semantics is used, better results are obtained, because the learners' emotional production in the learning process is affected by the visual content stimulation, and the video description contains this information. Therefore, using video description as semantic information can get better results, which also reflects the innovation of the method in this paper.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Ablation Studies",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Effectiveness Of Multimodal Fusion.",
      "text": "We conducted experiments using unimodal, bimodal, and trimodal, and the results are shown in Table  4 . We observed that emotion recognition improved significantly when multimodal data was used. This result shows that integrating multi-modal data helps to capture learners' emotional states more comprehensively, thus achieving higher performance affective perception. In addition, comparing experiments I and II in Table  4 , we found that the use of eye movement could obtain better results than PPG signals, indicating that there is a strong correlation between learners' emotions and eye movement signals, possibly because in MOOC learning scenarios, learners mainly watch instructional videos through vision. This result also suggests that learners' emotions can be affected by visual stimuli.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Effectiveness Of Video Semantics.",
      "text": "As shown in Table  4 , the experimental effect has been significantly improved after incorporating video semantic information, whether it is bimodal or trimodal. Further comparing experiments IV and V in Table  4 , we found that using eye movement signals and video semantic information for emotion recognition is more effective than using PPG signals and video semantic information. This indicates a stronger correlation between eye movement signals and semantic information. In MOOC learning scenarios, learners acquire knowledge by watching instructional videos, so eye movement signals are naturally directly affected by the instructional videos. This also confirms the importance of integrating video semantic information into MOOC learning emotion recognition tasks.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Effectiveness Of Cross-Attention.",
      "text": "To verify the effectiveness of our proposed cross-attention fusion method, we also conducted experiments by directly concatenating features without using cross-attention fusion. The experimental results show that when cross-attention is used, the accuracy of emotion recognition is significantly improved (See experiments VI and VII in Table  4 ). This indicates that our model can effectively capture the correlation and complementarity between different modalities, thereby improving the performance of emotion recognition. In our method, the data from three modalities is combined pairwise and fed into MHA, so that each modality can learn information related to the other two modalities. Then, the learned features are further fused to obtain features containing important information about the three modalities. Finally, the fused features are used for emotion recognition. By this method, we achieved the effective fusion of multimodal data, which can make full use of the effective information of each modality and improve the accuracy of emotion recognition.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Effects On Public Dataset",
      "text": "To demonstrate the generalization ability of our method, we conducted experiments on the publicly available dataset MAHNOB-HCI  [25] . This dataset is a multimodal database that synchronously records the data of 27 subjects' EEG, eye movements, facial video, audio signals, and peripheral physiological signals while they watched 20 emotional videos. We used the EEG and eye movement signals, along with extracted semantic information from the videos in this dataset, for experimentation. We compared the results in terms of arousal (including Calm, Medium arousal, Excited/Activated) and valence (including Unpleasant, Neutral valence, and Pleasant) dimensions with the baseline. The experimental results are shown in Table  5 . We can see that compared to using single-modal EEG and eye-tracking data, the performance significantly improves when adding video semantic information. The best results are achieved when using all three modalities simultaneously. This experiment further demonstrates the positive impact of incorporating video semantic information on emotion recognition. It also validates the strong generalization capability of our method, making it suitable for emotion recognition tasks induced by stimulus materials.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Visualization",
      "text": "To demonstrate the effectiveness of our method more clearly, we visualized the feature distributions learned by the classifier in the second-to-last layer of our model using t-SNE  [17]  in three different settings. As shown in Figure  5  (a), it is difficult for the model to effectively distinguish different categories of emotions using only eye movement and PPG data. As shown in Figure  5  (b), with the addition of video semantic information, it can be observed that the feature distribution distinguishes different emotion categories becomes more clear, which further verifies that fusing video semantic information has a positive effect on improving emotion recognition performance. When the cross-attention mechanism is further applied, the feature distribution becomes more obvious (as shown in Figure  5 (c )), indicating that the cross-attention mechanism can effectively learn information between different modalities and improve emotion recognition performance. In addition, in Figure  5 , we can also find that it is difficult to distinguish the emotional categories of Interest and Confusion, which explains why the recognition accuracy of Interest and Confusion is low (as shown in Figure  3  and Figure  4 (a) ).  In this work, we propose a multimodal emotion recognition method that integrates video semantic information and physiological signals, aiming at the particularity of the MOOC learning scenario. This is the first attempt to apply semantic information from instructional videos to emotion recognition tasks in MOOC learning. We use a method of generating video descriptions to extract high-level semantic information from educational videos, thereby expanding the dataset.\n\nExperimental results indicate that incorporating video semantic information has a significantly positive impact on emotion recognition. We use cross-attention to capture semantic correlations between different sequences and have designed a multimodal fusion method based on crossattention. This method successfully fuses video semantic information with physiological signals, achieving an accuracy improvement of over 14%. Additionally, we adopted adaptive synthetic sampling for data augmentation, effectively eliminating the impact of data distribution imbalance. To validate the generalization ability of our approach, we further conducted experiments on the publicly available HCI dataset.\n\nThe results indicate that our method can significantly improve the performance of emotion recognition. Overall, through extensive experimentation, we have demonstrated the effectiveness and feasibility of the proposed method, providing new perspectives and effective approaches for emotion recognition studies induced by stimuli materials.\n\nIn the current study, we used the global semantic information of the instructional video for analysis. In future work, we will try to extract more fine-grained video semantic information to conduct experiments. In addition, we will also explore more effective multi-modal fusion strategies to fully utilize information from different modalities to achieve higher emotion recognition performance.",
      "page_start": 7,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ). Firstly, in the data processing and feature extraction",
      "page": 3
    },
    {
      "caption": "Figure 1: It consists of three main stages: data processing and feature extraction, cross-attention fusion, and emotion classification.",
      "page": 3
    },
    {
      "caption": "Figure 2: Firstly, to ensure the stability of the subsequent running",
      "page": 3
    },
    {
      "caption": "Figure 2: , the generated semantic information",
      "page": 3
    },
    {
      "caption": "Figure 2: Schematic diagram of video semantic information generation and semantic feature extraction",
      "page": 4
    },
    {
      "caption": "Figure 3: shows the confusion matrix for each fold of our",
      "page": 5
    },
    {
      "caption": "Figure 3: The confusion matrix of each fold of the model under 5-fold cross-validation",
      "page": 6
    },
    {
      "caption": "Figure 4: , where (a) and (b) are the ROC curves",
      "page": 6
    },
    {
      "caption": "Figure 4: The ROC curves with (a) and without (b) ADASYN",
      "page": 6
    },
    {
      "caption": "Figure 5: (a), it is",
      "page": 7
    },
    {
      "caption": "Figure 5: (b), with the addition of video semantic",
      "page": 7
    },
    {
      "caption": "Figure 5: (c)), indicating",
      "page": 7
    },
    {
      "caption": "Figure 3: and Figure 4 (a)).",
      "page": 7
    },
    {
      "caption": "Figure 5: Visualization results of feature distribution in three",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "‚ö´ We hypothesize there is a close correlation between the": "semantic information of instructional videos and"
        },
        {
          "‚ö´ We hypothesize there is a close correlation between the": "learners' emotions. To deeply explore the impact of video"
        },
        {
          "‚ö´ We hypothesize there is a close correlation between the": "semantic information on learners' emotions, this study"
        },
        {
          "‚ö´ We hypothesize there is a close correlation between the": "innovatively proposes a multimodal emotion recognition"
        },
        {
          "‚ö´ We hypothesize there is a close correlation between the": "method that integrates video semantic information and"
        },
        {
          "‚ö´ We hypothesize there is a close correlation between the": "physiological signals. The instructional video's high-level"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "semantic information is obtained by generating video": "descriptions, which are fused with eye movement and"
        },
        {
          "semantic information is obtained by generating video": "PPG signals to identify the learner's emotional state. This"
        },
        {
          "semantic information is obtained by generating video": "method effectively improves the performance of emotion"
        },
        {
          "semantic information is obtained by generating video": "recognition."
        },
        {
          "semantic information is obtained by generating video": "‚ö´ To effectively capture the correlation and complementar-"
        },
        {
          "semantic information is obtained by generating video": "ity between different modal featuresÔºå we propose a"
        },
        {
          "semantic information is obtained by generating video": "multi-modal emotion recognition module based on cross-"
        },
        {
          "semantic information is obtained by generating video": "attention fusion. By first learning the feature representat-"
        },
        {
          "semantic information is obtained by generating video": "ions of any two modalities separately and then further"
        },
        {
          "semantic information is obtained by generating video": "fusing the learned features to obtain the feature"
        },
        {
          "semantic information is obtained by generating video": "representations of three modalities, we achieved an"
        },
        {
          "semantic information is obtained by generating video": "effective fusion of the three modalities."
        },
        {
          "semantic information is obtained by generating video": "‚ö´ We conducted extensive experiments and analyzed the"
        },
        {
          "semantic information is obtained by generating video": "experimental results in depth. The effectiveness and"
        },
        {
          "semantic information is obtained by generating video": "feasibility of our method have been comprehensively"
        },
        {
          "semantic information is obtained by generating video": "verified, and experimental results show that our method"
        },
        {
          "semantic information is obtained by generating video": "has achieved significant results in practice, with an"
        },
        {
          "semantic information is obtained by generating video": "accuracy improvement of over 14%."
        },
        {
          "semantic information is obtained by generating video": "The rest of this paper is organized as follows: Section 2"
        },
        {
          "semantic information is obtained by generating video": "reviews previous work on emotion recognition. Section 3"
        },
        {
          "semantic information is obtained by generating video": "provides a detailed explanation of the proposed method"
        },
        {
          "semantic information is obtained by generating video": "framework and the extraction of video semantic information."
        },
        {
          "semantic information is obtained by generating video": "Section 4 reports and analyzes the experimental results,"
        },
        {
          "semantic information is obtained by generating video": "extensively verifying the effectiveness of the method"
        },
        {
          "semantic information is obtained by generating video": "proposed in this paper. Section 5 summarizes the"
        },
        {
          "semantic information is obtained by generating video": "experimental results and future work."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Adding context or semantic information to emotion": "recognition tasks has gradually become a focus of attention."
        },
        {
          "Adding context or semantic information to emotion": "After adding context information, emotion recognition"
        },
        {
          "Adding context or semantic information to emotion": "systems can more accurately infer related emotions. Kosti et"
        },
        {
          "Adding context or semantic information to emotion": "al. [15] believe that in addition to facial expressions and body"
        },
        {
          "Adding context or semantic information to emotion": "postures, scene context also provides important information"
        },
        {
          "Adding context or semantic information to emotion": "for us to perceive people's emotions. Scene context"
        },
        {
          "Adding context or semantic information to emotion": "information is also a key component in understanding"
        },
        {
          "Adding context or semantic information to emotion": "emotional states. Therefore, they created and released the"
        },
        {
          "Adding context or semantic information to emotion": "Emotions In Context (EMOTIC) dataset and proposed a"
        },
        {
          "Adding context or semantic information to emotion": "baseline CNN model for emotion recognition in scene context."
        },
        {
          "Adding context or semantic information to emotion": "Dashtipour et al. [7] proposed a context-aware multi-modal"
        },
        {
          "Adding context or semantic information to emotion": "sentiment analysis framework to predict emotional states"
        },
        {
          "Adding context or semantic information to emotion": "accurately."
        }
      ],
      "page": 2
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion Recognition based on EEG using LSTM Recurrent Neural Network",
      "authors": [
        "S Alhagry",
        "A Fahmy",
        "R El-Khoribi"
      ],
      "year": "2017",
      "venue": "International Journal of Advanced Computer Science and Applications (IJACSA)",
      "doi": "10.14569/IJACSA.2017.081046"
    },
    {
      "citation_id": "2",
      "title": "Understanding high dropout rates in MOOCs -a qualitative case study from Pakistan",
      "authors": [
        "K Azhar",
        "N Iqbal",
        "Z Shah",
        "H Ahmed"
      ],
      "year": "2023",
      "venue": "Understanding high dropout rates in MOOCs -a qualitative case study from Pakistan",
      "doi": "10.1080/14703297.2023.2200753"
    },
    {
      "citation_id": "3",
      "title": "An Emotion Recognition Method Based on Eye Movement and Audiovisual Features in MOOC Learning Environment",
      "authors": [
        "J Bao",
        "X Tao",
        "Y Zhou"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Computational Social Systems",
      "doi": "10.1109/TCSS.2022.3221128"
    },
    {
      "citation_id": "4",
      "title": "Feature-level fusion approaches based on multimodal EEG data for depression recognition",
      "authors": [
        "H Cai",
        "Z Qu",
        "Z Li",
        "Y Zhang",
        "X Hu",
        "B Hu"
      ],
      "year": "2020",
      "venue": "Information Fusion",
      "doi": "10.1016/j.inffus.2020.01.008"
    },
    {
      "citation_id": "5",
      "title": "EEGbased emotion recognition using hybrid CNN and LSTM classification",
      "authors": [
        "B Chakravarthi",
        "S.-C Ng",
        "M Ezilarasan",
        "M.-F Leung"
      ],
      "year": "2022",
      "venue": "Frontiers in Computational Neuroscience",
      "doi": "10.3389/fncom.2022.1019776"
    },
    {
      "citation_id": "6",
      "title": "Nearest neighbor pattern classification",
      "authors": [
        "T Cover",
        "P Hart"
      ],
      "year": "1967",
      "venue": "IEEE Transactions on Information Theory",
      "doi": "10.1109/TIT.1967.1053964"
    },
    {
      "citation_id": "7",
      "title": "A novel context-aware multimodal framework for persian sentiment analysis",
      "authors": [
        "K Dashtipour",
        "M Gogate",
        "E Cambria",
        "A Hussain"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "8",
      "title": "",
      "authors": [
        "Doi"
      ],
      "venue": "",
      "doi": "10.1016/j.neucom.2021.02.020"
    },
    {
      "citation_id": "9",
      "title": "BERT: Pretraining of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter"
    },
    {
      "citation_id": "10",
      "title": "Emotion Effects on Online Learning. Intelligent Distributed Computing IX",
      "authors": [
        "A Faria",
        "A Almeida",
        "C Martins",
        "R Gon√ßalves"
      ],
      "year": "2016",
      "venue": "Emotion Effects on Online Learning. Intelligent Distributed Computing IX"
    },
    {
      "citation_id": "11",
      "title": "Emotion recognition from multiple physiological signals using intra-and inter-modality attention fusion network",
      "authors": [
        "L Gong",
        "W Chen",
        "M Li",
        "T Zhang"
      ],
      "year": "2024",
      "venue": "Digital Signal Processing",
      "doi": "10.1016/j.dsp.2023.104278"
    },
    {
      "citation_id": "12",
      "title": "How video production affects student engagement: an empirical study of MOOC videos",
      "authors": [
        "P Guo",
        "J Kim",
        "R Rubin"
      ],
      "year": "2014",
      "venue": "Proceedings of the first ACM conference on Learning @ scale conference"
    },
    {
      "citation_id": "13",
      "title": "Understanding feedback in online learning -A critical review and metaphor analysis",
      "authors": [
        "L Jensen",
        "M Bearman",
        "D Boud"
      ],
      "year": "2021",
      "venue": "Computers & Education",
      "doi": "10.1016/j.compedu.2021.104271"
    },
    {
      "citation_id": "14",
      "title": "CSAT-FTCN: A Fuzzy-Oriented Model with Contextual Self-attention Network for Multimodal Emotion Recognition",
      "authors": [
        "D Jiang",
        "H Liu",
        "R Wei",
        "G Tu"
      ],
      "year": "2023",
      "venue": "Cognitive Computation",
      "doi": "10.1007/s12559-023-10119-6"
    },
    {
      "citation_id": "15",
      "title": "The structural and functional connectivity of the amygdala: From normal emotion to pathological anxiety",
      "authors": [
        "M Kim",
        "R Loucks",
        "A Palmer",
        "A Brown",
        "K Solomon",
        "A Marchante",
        "P Whalen"
      ],
      "year": "2011",
      "venue": "Behavioural Brain Research",
      "doi": "10.1016/j.bbr.2011.04.025"
    },
    {
      "citation_id": "16",
      "title": "Context Based Emotion Recognition Using EMOTIC Dataset",
      "authors": [
        "R Kosti",
        "J Alvarez",
        "A Recasens",
        "A Lapedriza"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": "10.1109/TPAMI.2019.2916866"
    },
    {
      "citation_id": "17",
      "title": "Audio-visual emotion fusion (AVEF): A deep efficient weighted approach",
      "authors": [
        "Y Ma",
        "Y Hao",
        "M Chen",
        "J Chen",
        "P Lu",
        "A Ko≈°ir"
      ],
      "year": "2019",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "18",
      "title": "",
      "authors": [
        "Doi"
      ],
      "venue": "",
      "doi": "10.1016/j.inffus.2018.06.003"
    },
    {
      "citation_id": "19",
      "title": "Visualizing Data using t-SNE",
      "authors": [
        "L Maaten",
        "G Van Der And Hinton"
      ],
      "year": "2008",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "20",
      "title": "Predicting Student Dropout in Massive Open Online Courses Using Deep Learning Models -A Systematic Review",
      "authors": [
        "E Mbunge",
        "J Batani",
        "R Mafumbate",
        "C Gurajena",
        "S Fashoto",
        "T Rugube",
        "B Akinnuwesi",
        "A Metfula"
      ],
      "year": "2022",
      "venue": "Cybernetics Perspectives in Systems"
    },
    {
      "citation_id": "21",
      "title": "The neurobiology of emotion-cognition interactions: fundamental questions and strategies for future research",
      "authors": [
        "H Okon-Singer",
        "T Hendler",
        "L Pessoa",
        "A Shackman"
      ],
      "year": "2015",
      "venue": "Frontiers in Human Neuroscience"
    },
    {
      "citation_id": "22",
      "title": "Are students happier the more they learn? -Research on the influence of course progress on academic emotion in online learning",
      "authors": [
        "X Pan",
        "B Hu",
        "Z Zhou",
        "X Feng"
      ],
      "year": "2023",
      "venue": "Interactive Learning Environments"
    },
    {
      "citation_id": "23",
      "title": "",
      "authors": [
        "Doi"
      ],
      "venue": "",
      "doi": "10.1080/10494820.2022.2052110"
    },
    {
      "citation_id": "24",
      "title": "Predicting Learners' Emotions in Mobile MOOC Learning via a Multimodal Intelligent Tutor. Intelligent Tutoring Systems",
      "authors": [
        "P Pham",
        "J Wang"
      ],
      "year": "2018",
      "venue": "Predicting Learners' Emotions in Mobile MOOC Learning via a Multimodal Intelligent Tutor. Intelligent Tutoring Systems"
    },
    {
      "citation_id": "25",
      "title": "Neural oscillations and learning performance vary with an instructor's gestures and visual materials in video lectures",
      "authors": [
        "Z Pi",
        "Y Zhang",
        "Q Yu",
        "Y Zhang",
        "J Yang",
        "Q Zhao"
      ],
      "year": "2022",
      "venue": "British Journal of Educational Technology",
      "doi": "10.1111/bjet.13154"
    },
    {
      "citation_id": "26",
      "title": "An Improved Empirical Mode Decomposition of Electroencephalogram Signals for Depression Detection",
      "authors": [
        "J Shen",
        "X Zhang",
        "G Wang",
        "Z Ding",
        "B Hu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2019.2934412"
    },
    {
      "citation_id": "27",
      "title": "Research on Emotion Recognition Method Based on Adaptive Window and Fine-Grained Features in MOOC Learning",
      "authors": [
        "X Shen",
        "J Bao",
        "X Tao",
        "Z Li"
      ],
      "year": "2022",
      "venue": "Sensors",
      "doi": "10.3390/s22197321"
    },
    {
      "citation_id": "28",
      "title": "A Multimodal Database for Affect Recognition and Implicit Tagging",
      "authors": [
        "M Soleymani",
        "J Lichtenauer",
        "T Pun",
        "M Pantic"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/T-AFFC.2011.25"
    },
    {
      "citation_id": "29",
      "title": "The Influence of Teacher-Student Interaction on the Effects of Online Learning: Based on a Serial Mediating Model",
      "authors": [
        "H.-L Sun",
        "T Sun",
        "F.-Y Sha",
        "X.-Y Gu",
        "X.-R Hou",
        "F.-Y Zhu",
        "P.-T Fang"
      ],
      "year": "2022",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "30",
      "title": "Attention-Rectified and Texture-Enhanced Cross-Attention Transformer Feature Fusion Network for Facial Expression Recognition",
      "authors": [
        "M Sun",
        "W Cui",
        "Y Zhang",
        "S Yu",
        "X Liao",
        "B Hu",
        "Y Li"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Industrial Informatics",
      "doi": "10.1109/TII.2023.3253188"
    },
    {
      "citation_id": "31",
      "title": "A Multimodal Intelligent Emotion Perception Framework by Data-driven and Knowledge-guided",
      "authors": [
        "X Tao",
        "Y Zhang"
      ],
      "year": "2022",
      "venue": "2nd International Conference on Electronic Information Engineering and Computer Technology (EIECT)"
    },
    {
      "citation_id": "32",
      "title": "The Influences of Emotion on Learning and Memory",
      "authors": [
        "C Tyng",
        "H Amin",
        "M Saad",
        "A Malik"
      ],
      "year": "2017",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "33",
      "title": "Speech Emotion Recognition Using Semantic Information. ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "authors": [
        "P Tzirakis",
        "A Nguyen",
        "S Zafeiriou",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "Speech Emotion Recognition Using Semantic Information. ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "34",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "≈Å Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "35",
      "title": "How brains beware: neural mechanisms of emotional attention",
      "authors": [
        "P Vuilleumier"
      ],
      "year": "2005",
      "venue": "Trends in Cognitive Sciences",
      "doi": "10.1016/j.tics.2005.10.011"
    },
    {
      "citation_id": "36",
      "title": "Multimodal Feature Fusion and Emotion Recognition Based on Variational Autoencoder",
      "authors": [
        "Y Wang",
        "X Guan"
      ],
      "year": "2023",
      "venue": "IEEE 5th International Conference on Civil Aviation Safety and Information Technology (ICCASIT)"
    },
    {
      "citation_id": "37",
      "title": "Emotion Transformer Fusion: Complementary Representation Properties of EEG and Eye Movements on Recognizing Anger and Surprise",
      "authors": [
        "Y Wang",
        "W.-B Jiang",
        "R Li",
        "B.-L Lu"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)"
    },
    {
      "citation_id": "38",
      "title": "Multimodal interaction enhanced representation learning for video emotion recognition",
      "authors": [
        "X Xia",
        "Y Zhao",
        "D Jiang"
      ],
      "year": "2022",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "39",
      "title": "Trial Selection Tensor Canonical Correlation Analysis (TSTCCA) for Depression Recognition with Facial Expression and Pupil Diameter",
      "authors": [
        "M Yang",
        "Y Wu",
        "Y Tao",
        "X Hu",
        "B Hu"
      ],
      "year": "2023",
      "venue": "IEEE Journal of Biomedical and Health Informatics",
      "doi": "10.1109/JBHI.2023.3322271"
    },
    {
      "citation_id": "40",
      "title": "A Method of Multimodal Emotion Recognition in Video Learning Based on Knowledge Enhancement",
      "authors": [
        "H Ye",
        "Y Zhou",
        "X Tao"
      ],
      "year": "2023",
      "venue": "A Method of Multimodal Emotion Recognition in Video Learning Based on Knowledge Enhancement",
      "doi": "10.32604/csse.2023.039186"
    },
    {
      "citation_id": "41",
      "title": "mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality",
      "authors": [
        "Q Ye"
      ],
      "year": "2023",
      "venue": "mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality",
      "doi": "10.48550/ARXIV.2304.14178"
    },
    {
      "citation_id": "42",
      "title": "Multi-Channel Weight-Sharing Autoencoder Based on Cascade Multi-Head Attention for Multimodal Emotion Recognition",
      "authors": [
        "J Zheng",
        "S Zhang",
        "Z Wang",
        "X Wang",
        "Z Zeng"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia",
      "doi": "10.1109/TMM.2022.3144885"
    }
  ]
}