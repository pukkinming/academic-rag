{
  "paper_id": "2403.02167v3",
  "title": "Emovome: A Dataset For Emotion Recognition In Spontaneous Real-Life Speech",
  "published": "2024-03-04T16:13:39Z",
  "authors": [
    "Lucía Gómez-Zaragozá",
    "Rocío del Amor",
    "María José Castro-Bleda",
    "Valery Naranjo",
    "Mariano Alcañiz Raya",
    "Javier Marín-Morales"
  ],
  "keywords": [
    "Speech emotion recognition",
    "spontaneous",
    "valence",
    "arousal",
    "pre-trained model",
    "speaker-independent"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Spontaneous datasets for Speech Emotion Recognition (SER) are scarce and frequently derived from laboratory environments or staged scenarios, such as TV shows, limiting their application in real-world contexts. We developed and publicly released the Emotional Voice Messages (EMOVOME) dataset, including 999 voice messages from real conversations of 100 Spanish speakers on a messaging app, labeled in continuous and discrete emotions by expert and non-expert annotators. We evaluated speaker-independent SER models using acoustic features as baseline and transformer-based models. We compared the results with reference datasets including acted and elicited speech, and analyzed the influence of annotators and gender fairness. The pre-trained UniSpeech-SAT-Large model achieved the highest results, 61.64% and 55.57% Unweighted Accuracy (UA) for 3-class valence and arousal prediction respectively on EMOVOME, a 10% improvement over baseline models. For the emotion categories, 42.58% UA was obtained. EMOVOME performed lower than the acted RAVDESS dataset. The elicited IEMOCAP dataset also outperformed EMOVOME in predicting emotion categories, while similar results were obtained in valence and arousal. EMOVOME outcomes varied with annotator labels, showing better results and fairness when combining expert and non-expert annotations. This study highlights the gap between controlled and real-life scenarios, supporting further advancements in recognizing genuine emotions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "H UMAN communication conveys not only ideas but also emotional states, which play a crucial role in shaping interpersonal interactions. In everyday conversations, individuals rely on both verbal content and emotional cues to interpret meaning and adjust their responses, highlighting the significance of emotion recognition. Speech Emotion Recognition (SER) is an evolving research field aimed at automatically identifying a speaker's emotional state from their voice. With applications ranging from detecting emotional distress in customer service interactions to enhancing naturalness in speech synthesis systems, SER holds significant promise for improving both human-to-human and human-computer interactions. Central to SER are emotional datasets, which are categorized in the literature based on various criteria, including the emotion models used for labeling, the authenticity of emotional expressions, and the recording environment.\n\nThe labeling of emotions in emotional datasets is influenced by the underlying emotion model used, which can be either discrete or continuous. Grounded in basic emotion theories, discrete models consider emotions as universal and distinct categories, including happiness, sadness, and anger among others  [1] . In contrast, dimensional models, based on core affect theory, represent emotions as points within a continuous space defined by affective dimensions such as valence and arousal  [2] . The choice of an emotion model directly impacts the scope and granularity of the dataset, shaping its suitability for different SER applications.\n\nEmotional datasets are also distinguished by the expression authenticity of their samples. In the literature, they are commonly classified as acted, elicited, and spontaneous  [3] -  [5] . Acted (or simulated) datasets comprise speech samples of actors simulating emotions, typically using predetermined texts. Acted emotions tend to be stereotypical and exaggerated, limiting their applicability in real-life scenarios. Elicited (or induced) datasets gather speech samples from artificial situations designed to elicit specific emotional states, such as listening to a story or watching a video. The induction process has limitations and ethical implications, as individuals may react differently to the same stimulus. Spontaneous (or natural) datasets aim to capture genuine emotional expression that occurs without external guidance or deliberate performance, making them the most challenging to collect.\n\nThe recording environment refers to the setting in which emotional data is collected, encompassing the level of control and external influences during the recording process. It can be categorized into staged, laboratory, and real-life settings. Staged settings involve controlled scenarios with carefully designed setups, such as studios for TV shows or YouTube videos, which pose certain limitations. The artificial situation may inadvertently influence speakers, leading to controlled or unnatural emotional expressions  [6] . Additionally, the monologue format of some YouTube videos and podcasts may lack the naturalness typically found in conversational interactions  [7] . It is also worth noting that the individuals present in these contexts are generally communication experts, limiting the extrapolation to the general population. Laboratory settings provide controlled environments with some degree of spontaneity when used to simulate real-life scenarios, although they can also influence the authenticity of expressions  [6] . Exceptions to these limitations exist in datasets from reallife scenarios, such as call center records. These datasets prioritize ecological validity but are typically restricted and come with challenges such as background noise and variability in recording conditions, called in-the-wild conditions.\n\nSER is still an open-ended problem due to its complexity, particularly when addressing spontaneous emotions in reallife environments. Collecting spontaneous emotional data is inherently difficult and relies on subjective external evaluation to identify the emotion in each sample, a process complicated by the variability in emotional perception among raters  [8] . Furthermore, real-life settings are scarce in literature and often restricted from public use due to ethical and legal considerations. As a result, while most emotional corpora are predominantly in English, owing to its global prevalence  [4] , the availability of natural datasets remains limited. Three exceptions including spontaneous emotions from real-life scenarios were found in the literature: NATURAL  [9] , Lee & Narayanan  [10]  and SUSAS  [11]  datasets. The first two include call-center recordings but have restricted public access. SUSAS includes a commercially available subset, but it is limited to recordings of isolated words from aircraft communications. The availability of emotional datasets is considerably limited for languages other than English, including Spanish. We found only two spontaneous datasets in Spanish: MOUD  [12]  and CMU-MOSEAS  [13] . Although both datasets are publicly available, they consist of monologue clips sourced from YouTube, thus recorded in staged controlled settings. The limited availability of natural datasets from real-life scenarios restricts the advancement of SER models in real-world settings.\n\nTo fill the gap in existing literature on emotional datasets with spontaneous emotions from real-life scenarios, we collected the Emotional Voice Messages (EMOVOME) dataset. It contains 999 audio messages collected from real WhatsApp conversations of 100 Spanish speakers. To the best of our knowledge, this is the first public dataset with spontaneous emotions from conversations collected in a real-life scenario. We created SER models using acoustic features and speech embeddings derived from state-of-the-art pre-trained models. To simulate more realistic and challenging conditions, we assessed the models under a speaker-independent approach, using different speakers for training and evaluation. We conducted a comprehensive analysis to assess how various EMOVOME properties impact on SER model results. We analyzed the influence of the expert and non-expert annotators, and evaluated model fairness, specifically assessing differences in performance outcomes based on the speakers' gender. We compared the results with the widely used IEMOCAP  [14]  and RAVDESS  [15]  datasets.\n\nThe main contributions of the article are: (1) presenting EMOVOME, the first public dataset with spontaneous reallife emotions for SER, and making it available to the scientific community; (2) analyzing the SER performance of the stateof-the-art pre-trained speech models in real-life spontaneous conversations, and comparing the results with two non-natural reference datasets; (3) analyzing the influence of EMOVOME annotators on SER results; and (4) evaluating models' gender fairness. This study emphasizes the disparity between staged or laboratory settings and real-life scenarios, aiding in the advancement of recognizing authentic emotions.\n\nThe paper is organized as follows. Section 2 reviews existing SER datasets and related works. Section 3 describes EMOVOME and Section 4 other datasets used. Section 5 explains the methodology for creating SER models, and Section 6 reports the results. Section 7 discusses the results, and the paper concludes with a summary and future directions.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work A. Spontaneous Datasets For Ser",
      "text": "The availability of spontaneous datasets in the literature is limited. We conducted a search using KAPODI  [16] , a recently published searchable database of emotional stimulus sets, along with a review of the relevant literature. Table  I  presents the spontaneous datasets identified for English and Spanish, indicating the recording environment (RE) and its details, and whether they are free to access. Dataset RE Details Free English AFEW  [17]  Staged TV shows Yes Belfast Naturalistic  [18]  Staged TV shows Yes Castaway Reality TV  [19]  Staged TV shows Yes MSP-PODCAST  [20]  Staged Podcasts Yes AVIC  [21]  Lab Guided interaction Yes NATURAL  [9]  Real-life Call-center (human) No Lee & Narayanan  [10]  Real-life Call-center (machine) No SUSAS  [11]  Real-life Pilot communications Yes Doctor-patient interview No Spanish MOUD  [12]  Staged YouTube videos Yes CMU-MOSEAS *  [13]  Staged YouTube videos Yes\n\nFor English, we identified four  [17] -  [20]  spontaneous datasets recorded in staged scenarios, primarily featuring clips from TV shows, and one  [21]  recorded in a laboratory setting, capturing interactions during a simulated product presentation. Regarding real-life scenarios, NATURAL  [9]  and Lee & Narayanan  [10]  datasets include recordings from call-centers, with interactions involving a human agent and a machine agent, respectively. Both datasets primarily contain neutral speech, resulting in highly unbalanced data. Additionally, unlike previous datasets, those are restricted to public use. SUSAS dataset  [11]  contains two real-life sets: isolated words from aircraft communications and doctor-patient interviews, but only the former is commercially available.\n\nFor Spanish, we found only two datasets in the literature: MOUD  [12]  and CMU-MOSEAS  [13] . Both datasets are publicly available; however, they primarily consist of monologue clips sourced from YouTube videos, recorded in staged settings. Additionally, the original clips in CMU-MOSEAS are not disclosed, with only high-level features provided. The MOUD dataset presents another limitation, with the majority of speakers being female (84 out of 105), while the CMU-MOSEAS dataset does not specify gender distribution, making it challenging to develop gender-fair SER models.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Performance Of Ser Models",
      "text": "Research on SER using Spanish datasets is limited. For this reason, we reviewed prior studies that utilized Spanish datasets, including non-spontaneous ones, focusing on adult speech. Our analysis identified twelve Spanish datasets. Table  II  summarizes key details: expression authenticity (acted, elicited, or spontaneous), recording environment (RE), number of samples (N), number of speakers (Spk) with gender distribution (males/female, M/F), emotion model (with the number of categories for discrete models), and free use. For the four multilingual datasets (marked with an asterisk), only the Spanish partition is detailed. Additionally, we report the classification performance of SER models from the literature for each Spanish datasets, using two evaluation metrics: Weighted Accuracy (WA) and Unweighted Accuracy (UA). WA, or simply \"accuracy\", measures the overall proportion of correct predictions but can be biased toward majority classes in imbalanced datasets. UA, commonly used in SER studies, provides a fairer evaluation by averaging accuracy across all classes. The results indicate if a speaker-dependent (SD) or speaker-independent (SI) approach was used, i.e. whether same or different speakers were used for training and testing (if reported in the source articles). Multilingual and cross-lingual models are excluded from the table, as they pose greater challenges, and monolingual models typically achieve better performance when trained on comparable sample sizes  [22] . SER performance is conditioned by the type of data used. For acted datasets, the table shows WA and UA values equal to or exceeding 85% (e.g.  [26] ,  [27] ,  [30] ,  [33] -  [36] ,  [38] ). However, performance drops to 70 to 75%  [22] ,  [29]  when SI models are used, an approach which is known to yield lower results compared to SD models  [5] . An exception is seen in  [25] , where an accuracy of 55% was achieved for the classification of five emotions. The model was trained on actors simulating emotions while reading sentences and tested on movie clips, leading to a domain mismatch that likely caused the significant performance drop. Lower results on SI models also occur in the elicited EmoSpanishDB dataset  [39] , where seven emotions where classified with UA scores of 56.3% and 42.1% for SD and SI, respectively. A refined version of the dataset, EmoMatchSpanishDB, improved performance by removing mislabeled samples that did not align with the intended elicited emotions, achieving UA scores of 65.0% (SD) and 64.2% (SI). Finally, the spontaneous dataset shows the lowest performance. In  [12] , a classification accuracy of 46.75% was achieved for differentiating between positive and negative samples, highlighting the challenges of spontaneous emotional data in SER.\n\nRegarding the techniques used to create the SER models, a noteworthy paradigm shift involves leveraging large pretrained models in emotion recognition tasks. The study in  [22]  used the pre-trained wav2vec2-large-robust model finetuned on MSP-Podcasts dataset from  [40]  to extract speech embeddings. These embeddings were then fed into a support vector machine, obtaining 69.15% unweighted accuracy for five emotion categories on the EmoFilm dataset. To our knowledge, this is the only previous study using pre-trained models for monolingual SER models in Spanish. Nevertheless, an increasing number of recent publications have focused on the use of pre-trained models with English datasets for emotion categories and less frequently for dimensions.\n\nAs for studies focused on discrete emotion models, a very recent study  [40]  summarized the state-of-the-art results for 4-class emotion classification in the widely used IEMOCAP dataset, including studies using the pre-trained wav2vec 2.0 and HuBERT models. Comparing the cross-validation results, the UA values range from 60.0% to 74.3%, while the weighted average recall values vary from 62.6% to 79.6%, both top models. Overall, HuBERT surpassed the performance of the wav2vec 2.0 models. Other works have explored several pretrained models and emotional datasets. In  [41] , they used eight pre-trained models (including wav2vec 2.0) and four datasets to predict 6-7 emotions. The results showed that pretrained models for speaker recognition (x-vector and ECAPA) achieved the highest scores, possibly due to their learned ability to identify unique features within an individual's speech. Subsequently, UniSpeech-SAT achieved the best results, a model pre-trained using multitask learning, including the speaker identity. In another study  [42] , the authors compared nineteen pre-trained models (including wav2vec 2.0, HuBERT, UniSpeech-SAT and wavLM) and five datasets to predict 4-6 Staged 10000 341 (-/-) Discrete (  6 ) Yes --emotion categories using a SI approach. They found that the best scores were achieved with WavLM, UniSpeech-SAT, and HuBERT, all three in the large version.\n\nAs for studies on continuous emotion models,  [43]  used a multimodal model (audio + text) as a teacher to finetune HuBERT embeddings to predict valence, arousal and dominance on MSP-Podcast dataset  [44] . They obtained stateof-the-art Concordance Correlation Coefficient (CCC) values of 0.757, 0.627 and 0.671 for arousal, valence and dominance, respectively. The previous state-of-the-art performance for valence was 0.377  [45] , so 0.627 represents a substantial improvement. They also replicated the results for the IEMO-CAP dataset, achieving again state-of-the-art CCC results for valence (0.667), arousal (0.582) and dominance (0.545). In  [40] , the authors conducted an exhaustive analysis using different variants of the pre-trained wav2vec 2.0 and HuBERT models for valence, arousal and dominance prediction on MSP-Podcast. They obtained the best result in the literature for valence prediction using only audio, with a wav2vec2-largerobust model that achieved a CCC of 0.638. Notably, their results showed that data used for pre-training the models and the fine-tuning of the transformer layers had a strong influence on valence prediction. Both factors played a role in shaping the models' ability to implicitly incorporate linguistic information embedded in the audio signal. This, in turn, accounts for their success in valence prediction, achieving similar performance to multimodal models that integrate explicit textual information.",
      "page_start": 2,
      "page_end": 4
    },
    {
      "section_name": "C. Fairness Evaluation",
      "text": "Finally, research on evaluating model fairness is limited, particularly in the context of pre-trained models  [46] . Some previous works have investigated gender-based fairness in Spanish datasets, revealing differences in model performance between females and males. In  [33] , the authors reported a female accuracy of 90.9% and a male accuracy of 89.4%, yet the dataset only included one male and one female speaker. Conversely,  [30]  and  [38]  showed a different pattern, with males achieving higher accuracy than females. Specifically,  [30]  presented 89.49% for females and 93.90% for males (with 4 female and 4 male speakers), while  [38]  obtained 91.8% for females and 97.7% for males using pre-trained models (with 33 male and 24 female speakers). The same trend has been found in some works on SER for English data. In  [46] , they studied the fairness of SER systems using pretrained models and observed a reduction of 0.234 in CCC for arousal among females as opposed to males on MSP-Podcast. In  [40] , they found that pre-trained models tend to exhibit greater fairness in predicting arousal and dominance than in valence. Notably, the majority of models showed higher CCC for females than for males for valence. Nevertheless, overall the speech representations obtained with pre-trained models seem to be invariant to domain, speaker, and gender. Additionally, the authors in  [40]  also explored fairness across individual speakers and found that different pre-trained models show overall consensus on categorizing speakers as 'good' or 'bad', obtaining lower CCC values for some individuals in the latter group. These findings underline the importance of incorporating fairness assessment in future research.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iii. Emotional Voice Messages (Emovome)",
      "text": "The Emotional Voice Messages (EMOVOME) dataset was created from scratch for this study is the first collection of spontaneous emotions from real voice messages. Figure  1  presents a diagram illustrating the steps followed to create the EMOVOME dataset, detailed in the following subsections. The EMOVOME dataset is publicly available at the following Zenodo repository https://zenodo.org/records/10694370. Access to the raw audio files requires completing and signing an agreement form to ensure the speakers' privacy.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Participants",
      "text": "The dataset includes 100 Spanish speakers (50% females, 50% males) equally distributed across four age ranges  (18-25, 26-35, 36-45 , and 46-55 years old), with no self-reported speech disorder, and who regularly send audio messages to their contacts. Additional demographic information can be found in the Appendix. Participants were sourced via a recruitment agency, which identified suitable individuals from their dataset based on the specified criteria and invited them to participate in the study for a monetary compensation of 25 C. All methods and experimental protocols were performed in accordance with the regulations of the local ethics committee of the Universitat Politècnica de València (No. P02 04 06 20).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Data Collection",
      "text": "The sample collection was conducted through a web-based application designed for the study. Participants used their computer to complete the study, following on-screen instructions. Initially, they read the study protocol, provided informed consent, and completed a sociodemographic questionnaire. They then recorded an audio reading the short text in the Appendix. They also completed the NEO Five Factor Inventory (NEO-FFI)  [47] , a 60-item questionnaire assessing personality traits. Next, participants were asked to upload 12 voice messages they had previously sent to other contacts, with a third of them being positive, neutral, and negative to ensure a balanced sample in terms of valence. Valence was chosen over specific emotion categories to simplify the task, as obtaining multiple samples for each distinct emotion can be challenging. Platform instructions clarified the concepts of positive, neutral, and negative valence, providing examples for each. Voice messages were encoded using the Ogg Vorbis format.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Quality Check",
      "text": "Voice messages were generated in real-life settings before participants were recruited, minimizing any bias associated with laboratory and staged environments. Recordings took place in participants' natural surroundings, ranging from quiet rooms at home to busy streets. To ensure high quality data, all recordings underwent a manual screening process. Audio files with critical background noise conditions (such as microphone malfunctions, background music or TV sounds) were identified and rejected. Participants were instructed to upload low-noise recordings, so if an audio was not considered suitable for the study, they were asked to send a new audio. We also excluded recordings that seemed to be recorded specifically for the study, as some participants may have attempted to fraudulently create these recordings after recruitment, despite instructions to upload voice messages from prior conversations. As a result, we received 1605 audio recordings, but 574 were not included in EMOVOME for this reason, resulting in 1031 audios. Finally, 32 audios exceeding 60 seconds were rejected to obtain a homogeneous sample concerning duration. Therefore, a total of 999 audios were considered in the final dataset (total duration of 293min), plus 100 audios reading the text.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "D. Data Labeling",
      "text": "The emotional content of the audios was labeled along two dimensions: valence, i.e., the degree to which an emotion is perceived as positive or negative; and arousal, i.e., how strongly the emotion is felt. Three non-experts and two expert evaluators were recruited for the task. The latter consisted of two clinical psychologists who rated half of the audios each. They are considered experts due to their professional training in recognizing and interpreting emotions. The demographic information of the annotators is indicated in the Appendix. Using the Self-Assessment Manikin (SAM) procedure  [48] , the evaluators rated each audio's valence and arousal on a 5-point pictorial scale. Additionally, the experts provided an extra label for 7 emotion categories: the six basic emotions (happiness, disgust, anger, surprise, fear and sadness) defined by Ekman  [1]  plus a neutral category.\n\nThe valence and arousal ratings of the non-experts and expert evaluators present a V-shaped relation, that is, arousal increases with positive or negative valence (see the Appendix). To compare the ratings among evaluators, three categories for valence (positive, neutral and negative) and three for arousal (high, neutral and low) were defined by grouping negative and positive scores in the scale [-2, 2] and keeping 0 as neutral. The agreement between evaluators is presented in Table III using Cohen's kappa score, for both valence (V) and arousal (A). For valence, the kappa score goes from 0.614 to 0.754, indicating substantial agreement among annotators. In contrast, the agreement decreased considerably for the arousal dimension, with kappa values ranging from 0.075 to a maximum of 0.407. These results suggest valence recognition was easier for raters than arousal recognition.\n\nThe non-experts and expert evaluations were combined to obtain a final audio label in terms of valence and arousal  Additionally, the expert annotators provided an extra label corresponding to 7 categories of emotions. The classes are distributed as follows: happiness (342), disgust  (8) , anger (199), surprise (118), fear  (35) , sadness (72) and neutral (225). Figure  3  shows the correspondence between categories and the valence and arousal dimensions provided by the experts. In this work, only the four most frequent emotions were used for classification, i.e., happiness, anger, neutral and surprise.\n\n9DOHQFH\n\n)HDU 'LVJXVW Fig.  3 . Valence and arousal ratings for each emotion. The diameter represents the number of samples using a logarithmic scale",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "E. Transcription",
      "text": "In addition to the speech files, audio transcriptions have been included in the dataset to offer an extra modality for analysis. The voice messages were transcribed using Amazon Transcribe and subsequently manually corrected to ensure accuracy and reliability.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Iv. Reference Datasets: Iemocap And Ravdess",
      "text": "In order to compare the novel natural dataset EMOVOME with the current state of the art, we selected two of the most used datasets: IEMOCAP and RAVDESS. A comparison of the main features is presented in Table  IV .\n\nIEMOCAP  [14]  stands out as a benchmark dataset for studying emotional expression and communication. It contains dyadic interactions of 10 English-speaking actors engaged in scripted and improvised dialogues designed to elicit different emotions. In this work, only the audio files corresponding to the segmentation of the conversations into utterances were used to create the SER models. This set includes 10039 utterances annotated into 9 emotion categories and 3 dimensions.\n\nTo align the classification results with the EMOVOME dataset, valence and arousal scores were stratified into three categories. Samples scoring below 2.5 were deemed negative/low, those between 2.5 and 3.5 were neutral, and those above 3.5 were positive/high. To facilitate comparison with the literature, only the four most frequent emotions were used for the classification into categories, i.e., angry, neutral, sad, happy and excited (the latter two were merged, following previous studies).\n\nRAVDESS  [15]  is a multimodal dataset with speech and song recordings expressing different emotions. In this study, only the voice data was used, where 24 actors recorded two phrases with normal and strong emotional intensity for each of eight emotions (excluding neutral), totaling 1,440 samples. To compare the results with EMOVOME, the emotion labels were transformed from the discrete model to the dimensional model, as also studied in  [49] . The emotion categories were converted into valence and arousal dimensions following their distribution in Russell's circumplex model of affect  [2] . For valence, the labels were: negative (sad, angry, fearful, disgust), neutral (neutral, surprised) and positive (happy, calm). For arousal, the labels were: low (calm, sad), neutral (neutral, disgust) and high (happy, angry, fearful, surprised).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "V. Methods",
      "text": "Three approaches were implemented to create the SER models. First,a baseline was established using a standard feature set with classical machine learning algorithms (Section V-A). Second, pre-trained models were used as feature extractors, followed by a linear layer for classification (Section V-B). Finally, a combination of pre-trained models and standard features was analyzed (Section V-C). The Python implementation is available on the following GitHub repository: https://github.com/LuciaGomZa/SER EMOVOME.git.\n\nFor evaluation, the datasets were split into 80% for training and 20% for testing using a speaker-independent approach. To facilitate reproducibility and comparison of results, for testing we used: a proposed set of 20 participants (50% female) available in the EMOVOME Zenodo repository indicated in Section III; session 5 for IEMOCAP; and \"fold 0\" proposed in  [50]  for RAVDESS. Details of the label distribution are provided in the Appendix. Additionally, parameter tuning also employed a SI cross-validation scheme with 4 folds for IEMOCAP and 5 folds for EMOVOME and RAVDESS. All methods were evaluated using weighted and unweighted accuracy, described in Section II-B. A. Baseline: eGeMAPS and machine learning\n\nThe voice messages in EMOVOME were recorded in realworld conditions, using various devices -mostly smartphone microphones-which resulted in different sampling rates (94% at 48 kHz and 6% at 44.1 kHz). To standardize, EMOVOME audio samples were resampled to 44.1 kHz, while RAVDESS and IEMOCAP maintained 48 kHz and 16 kHz, respectively. Next, the extended Geneva Minimalistic Acoustic Parameter Set (eGeMAPS)  [51]  was extracted with the openSMILE toolkit  [52] . The use of this standard set of features facilitates the understanding and reproducibility of the results. The 88 features per audio were normalized by subtracting the mean and dividing by the standard deviation of the development samples. For feature selection, high-correlated features (p > 0.95) were first eliminated using Pearson's correlation matrix, and a filter method was then used to select 25%, 50% or 75% of the features based on the highest ANOVA Fvalues. Finally, Support Vector Machine (SVM) and K-Nearest Neighbours (KNN) models were fitted on the development set according to the cross-validation scheme indicated above. Hyperparameter tuning was performed for both models. For SVM, we changed the kernel (radial basis function, sigmoid), gamma (0.001, 0.01, 0.1, 1, 'auto', 'scale') and C (1, 10, 100, 1000). For KNN, we optimized the number of neighbors (1, 3, 5, 7), weights (uniform, distance) and metric (Euclidean, Manhattan, Minkowski). The chosen combination of features and hyperparameters was used to train a model on the entire development set, followed by an evaluation of the test set.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "B. Pre-Trained Model Embeddings",
      "text": "We evaluated several pre-trained models with different architectures, pre-training methodologies and pre-training audio data. First, several variations of the widely used Wav2vec 2.0 model were selected. Considering that EMOVOME is in Spanish, and we wanted to compare it with other two datasets in English, we selected models pre-trained using a multilingual approach: facebook/wav2vec2-xls-r-300m (w2v2xlsr-128)  [53]  pre-trained on 436k hours of audios in 128 languages, and facebook/wav2vec2-large-xlsr-53 (w2v2-xlsr-53)  [54] , pre-trained on 56k hours of audios in 53 languages. Both models included Spanish in the pre-training data. Despite prior research suggesting that fine-tuning models for automatic speech recognition (ASR) do not help with speech emotion recognition  [55] ,  [56] , we decided to include a fine-tuned model for Spanish ASR. This decision aimed to explore whether this approach outperforms the utilization of a pretrained model in a different language since  [55]  and  [56]  used pre-trained models in the same language as the dataset tested (English). Therefore, we also used the fine-tuned version facebook/wav2vec2-large-xlsr-53-spanish (w2v2-xlsr-53spa)  [54]  to obtain the embeddings. We also included a model pre-trained using noisy audios, the model facebook/wav2vec2large-robust (w2v2-L-robust)  [57] , as it may be useful for the EMOVOME dataset despite being in English. As a widely-used alternative to Wav2Vec2, we used a HuBERT model, particularly the large version facebook/hubert-large-ll60k (hubert-L)  [58] . Additionally, recent studies  [41] ,  [59] ,  [60]  have indicated that including information about the speaker is helpful for speech emotion recognition. Therefore, following these investigations, two more models were selected: Microsoft's UniSpeech-SAT-Large (unispeech-L)  [61]  and a Statistics Pooling Time Delay Neural Network to obtain xvector embeddings  [62] . The former is a model pre-trained using multitask learning, including also the speaker identity during training. The latter provides a speaker embedding learning during a speaker verification task. All the pre-trained models are available in HuggingFace. The x-vector also requires the Speechbrain toolkit  [63] . All of them require the input audio to be resampled to 16 kHz.\n\nPrevious investigations have adapted the architecture of the pre-trained models by adding a classification head and fully or partially fine-tuning the model. Nevertheless, this process requires high computational resources due to the large size of some models and the audio lengths (particularly in EMOVOME). For this reason, the embedding extraction process was implemented offline, saving the audio embeddings in independent files. For all pre-trained models except for the x-vectors, we extracted the last hidden state of the last transformer layer. As a result, we obtained a vector of dimensions (X, 1024), where X varies depending on the audio length. Following previous research  [40] ,  [55] , we applied the average over the time dimension to obtain a 1024-dimension vector per audio sample. For x-vectors, the pre-trained model has a built-in statistics pooling layer that computes the mean and standard deviation of information from the last framelevel layer. These statistics are combined and input into a 512-dimensional hidden layer that is finally used to obtain the embeddings. As a result, the pre-trained model consistently outputs a 512-dimensional vector, independent of audio length, eliminating the need for extra aggregation strategies. Subsequently, we trained a neural network comprised solely of a linear layer, which took the embeddings as input and produced the output corresponding to the number of labels. This architecture, proven effective in previous literature  [55] , simplifies the model while still capturing essential features in the merged embeddings. The model was trained during a maximum of 3000 epochs during cross-validation using Adam optimization, a learning rate of 0.001 and a batch size of 128. The early stopping callback was applied, set to monitor the validation loss with a patience of 50. For the selected hyperparameters, a final model was trained on the development set for a fixed number of epochs selected based on the crossvalidation results and evaluated on the test.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Pre-Trained Model Embeddings And Egemaps",
      "text": "We integrated previous methods by combining speech embeddings with the eGeMAPS feature set. The 1024dimensional embeddings (except for the x-vector, which is 512-dimensional) were concatenated with the 88 eGeMAPS features. The combined set underwent normalization before being fed into the neural network, which consists of a linear layer, following the procedure detailed in the previous section.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Vi. Results",
      "text": "The cross-validation results in Fig.  4  show the performance across datasets and emotion labels. The UA of the optimal baseline model using eGeMAPS features and machine learning is indicated in gray. Next to this, results for models using embeddings are displayed, along with a third column for the method integrating embeddings and eGeMAPS features (Emb+eGeMAPS). The figure indicates the mean and standard deviation of the UA across the five folds (four for IEMOCAP).\n\nRegarding the baseline eGeMAPS models, the lowest UA values are from the EMOVOME dataset, with 41-44% for valence, 41-51% for arousal and 31.42% for four categories. IEMOCAP achieves 50.20%, 53.21% and 59.50% for valence, arousal and categories, respectively. RAVDESS achieves the highest UA results for valence and arousal, with 57.19% and 62.48%, respectively. For the eight categories, it obtains 45.52%. Overall, SVC was preferred to KNN in most cases.\n\nThe embedding approach demonstrates a notable enhancement in the models' performance compared to the baseline method. Again, EMOVOME yields the lowest scores, with UA values in the range 54-60% and 49-59% for valence and arousal, respectively, and 43.30% for the 4-way classification. IEMOCAP models have UA results of 61.73%, 57.68% and 68.79% for valence, arousal and categories, respectively. RAVDESS again reaches top UA values, with 71.91%, 74.09% and 67.81% for valence, arousal and categories, respectively. In general, unispeech-L consistently demonstrated superior performance compared to alternative options. Using pretrained embeddings results in an enhancement of approximately 10% in the UA across all combinations.\n\nFinally, the Emb+eGeMAPS method obtains similar results to the previous approach. EMOVOME achieves similar values in valence and arousal (53-59% and 52-60% respectively), while the UA in 4-emotion classification decreases (40.22%). A possible reason is that EMOVOME was recorded in-the-wild conditions. Therefore, eGeMAPS features may be affected by microphone quality and background noise  [51] . Conversely, IEMOCAP and RAVDESS were recorded in a controlled environment, and the UA values increased around 1-3% compared to the previous approach (except for the emotion categories with IEMOCAP, which slightly decreased). IEMOCAP achieves 62.48%, 60.41% and 68.04% for valence, arousal and  categories, respectively. RAVDESS obtains 72.70%, 75.27% and 69.58% for valence, arousal and categories, respectively. Again, unispeech-L is the best option in most cases. However other pre-trained models, with lower results in the previous approach, greatly improved when combined with eGeMAPS features, particularly w2v2-xlsr-53 and w2v2-L-robust.\n\nUsing the hyperparameters of the models obtaining the highest UA in cross-validation, new models were trained on the development set and evaluated on the test set, obtaining the results in Table  V . The model achieving the highest cross-validation results among the three implemented methods (eGeMAPS, Embeddings and Emb+eGeMAPS) is indicated. The test results follow the trends found in cross-validation, with RAVDESS outperforming IEMOCAP and EMOVOME across the two dimensions (73.54% in valence and 71.94% in arousal). IEMOCAP UA scores are higher than EMOVOME scores in arousal prediction (61.20% vs. 43.57-58.73% respectively), whereas, for valence prediction, IEMOCAP and EMOVOME achieve similar results (60.40% for the former and 57.53-61.64% for the latter). For EMOVOME, we also examined the differences between expert (E) and non-expert (N) annotations, as well as their combination (C). The confusion matrices are represented in Fig.  5 . Considering the emotion categories, test results for the three datasets are presented in Table  VI . It includes the model achieving the highest CV results among the three implemented methods. RAVDESS obtains the highest UA score (75.00%), followed by IEMOCAP (69.58%) and EMOVOME (42.58%). To evaluate which emotions are misclassified, Fig.  6  shows the confusion matrix for each dataset. Finally, we evaluated model fairness in terms of gender by calculating the difference between the UA for male speakers (U A M ) and the UA for female speakers (U A F ) on the test  set, which is presented in Fig.  7 . A positive difference means that the model had a better performance for male speakers.",
      "page_start": 8,
      "page_end": 10
    },
    {
      "section_name": "Vii. Discussion",
      "text": "In this study, we addressed the scarcity of spontaneous datasets from real-life environments by developing the EMOVOME dataset-the first public repository featuring genuine emotions from spontaneous conversations in reallife settings. This paper provides a detailed description of EMOVOME and extends access to the scientific community. We evaluated the performance of SER models trained with EMOVOME using various methodologies, and we compared the results with the well-known IEMOCAP dataset, which includes elicited speech, and RAVDESS, comprising acted recordings. Our comparison focused on predicting valence, arousal, and emotion categories. Finally, we also investigated the impact of annotators' labels and speakers' genders on model performance. UA was used for comparison since it is more suitable for unbalanced data.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "A. Ser Performance",
      "text": "We implemented three methods to create the SER models: the baseline using eGeMAPS features and machine learning, the embeddings from pre-trained models and the integration of embeddings and eGeMAPS features. As shown in Fig.  4 , overall, the embedding approach significantly improves model performance compared to the non-transformer baseline, leading to an approximately 10% improvement in unweighted accuracy across all combinations. Unispeech-L consistently showcased better results than other pre-trained models (as shown in  [41] ), closely followed by hubert-L (which also outperforms wav2vec2 models in previous studies  [40] ). For the English datasets, in general, the models pre-trained on multiple languages performed worse than those trained on Englishonly data (as in  [40] ). Regarding the Emb+eGeMAPS method, it shows similar cross-validation results to the embeddings for the EMOVOME dataset, and UA values slightly increase (max. 3%) for IEMOCAP and RAVDESS in some cases. One potential explanation is that EMOVOME was recorded in natural, uncontrolled conditions, which might impact the reliability of eGeMAPS features, unlike the other two datasets that were conducted in a controlled environment. Unispeech-L is again the top choice in most cases, but notably, other pretrained models that initially performed lower (e.g. w2v2-xlsr-53 and w2v2-L-robust) show significant improvement when combined with eGeMAPS features.\n\nConsidering the test results, the prediction of emotion categories (see Table  VI ) follow the trend in cross-validation, where EMOVOME obtains the lowest evaluation metric (45.58% UA), followed by IEMOCAP (69.58% UA) and finally RAVDESS (75.00% UA) (even though the first two classify four emotions and the later eight). Although IEMO-CAP contains elicited speech, part of the data consists of actors performing scripted dialogues, which could lead us to expect higher classification results. However, IEMOCAP contains utterances whose text content is different throughout the dataset, i.e., it is text-independent (same as EMOVOME). Conversely, RAVDESS is text-dependent, using two fixed sentences. This might have led the models for RAVDESS to prioritize emotional variations over differences in semantic content. Consequently, there is a performance gap in IEMO-CAP compared to the other acted datasets, as highlighted in  [64] . Nevertheless, our classification results (70.59% WA, 69.58% UA) are comparable to the state of the art for IEMOCAP, which is in the range from 60.0% to 74.3% UA  [40] , especially considering that we implemented a SI approach, unlike other previous studies. Additionally, we examined the misclassified emotions for each dataset in Fig.  6 . For EMOVOME, surprise is not correctly predicted in any case and is mainly confused with happy and neutral emotion, but this category is underrepresented in the data. For IEMOCAP, all emotions are mainly mistaken for the neutral category. For RAVDESS, most emotions are accurately classified (>72%).\n\nRegarding the test results for arousal and valence dimensions (see Table  V ), the SER models for the EMOVOME dataset achieve UA values of 61.64% for valence and 55.57% for arousal, considering the combined label between expert and non-experts. Surprisingly, the IEMOCAP dataset obtains similar values to EMOVOME, 60.04% for valence and 61.20% for arousal. As for RAVDESS, the UA values are 73.54% and 71.94% for valence and arousal, respectively. Initially, one might have anticipated better outcomes in valence, given that earlier studies  [40] ,  [65]  found that pre-trained models inherently capture linguistic information in the audio, aiding valence prediction. However, in our approach, we utilized pre-trained embeddings solely as feature extractors to obtain speech embeddings without fine-tuning the transformer layers for SER. This step has proven to be fundamental for the models to effectively learn the semantic content  [40] . Furthermore, the data used for pre-training significantly impacts the models' capacity to capture linguistic information, with the inclusion of multi-lingual data adding complexity to the task  [40] . These considerations might explain the relatively minor differences observed between valence and arousal predictions. In the case of RAVDESS, the use of fixed semantic content during recording prevented pre-trained models from leveraging text information for valence prediction. Furthermore, an important limitation in both IEMOCAP and RAVDESS is the label transformation applied to obtain the valence and arousal categories, as well as the unbalanced distribution of samples.\n\nIn summary, we found that embeddings and Emb+eGeMAPS give similar results for the top-performing pre-trained model, i.e., Unispeech-L. The EMOVOME dataset achieves lower results than other non-natural datasets in the literature. Both RAVDESS and IEMOCAP outperform EMOVOME in emotion categories, and the former also obtains higher valence and arousal prediction results. However, EMOVOME and IEMOCAP exhibit more comparable results in valence and arousal, possibly owing to their text-independent nature (unlike RAVDESS) and the approach used to transform the original labels in IEMOCAP into valence and arousal categories.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "B. Impact Of Annotator Labels On Ser Performance",
      "text": "Emotion prediction lacks a definitive ground truth due to its inherently subjective nature. In this study, we addressed this by asking participants to upload audio samples with self-assessed emotional valence (negative, neutral, or positive). To refine the emotional labels, we also incorporated an external evaluation by both experts and non-experts. While research on the impact of annotators' demographics on SER is limited, existing studies suggests that biases can arise based on their gender, age or educational level  [8] . We hypothesized that this bias may be more pronounced when evaluating natural datasets,such as EMOVOME, since they do not contain stereotypical emotion and thus can be more challenging to label. In this work, we explored the difference between expert (E) and non-expert (N) annotations, as well as their combination (C) (see Table  V ).\n\nSurprisingly, SER models using the expert's labels achieved the lowest results for both valence and arousal. Non-experts got higher UA values in arousal prediction (58.73%), compared to the combined labels (55.57%) and the expert's labels (43.57%). For valence, the combined label obtained the highest UA score (61.64%), closely followed by the non-experts (61.36%) and lastly the expert (57.53%). It's essential to note that while clinical psychologists are experts in the task, emotions remain highly subjective and can be influenced by individual experiences  [8] . In fact, the confusion matrix (see Fig.  5 ) reveals discernible biases towards specific emotion categories in both valence and arousal results, potentially shaping what the SER models learn. For valence, the model trained on expert labels shows a bias towards positive valence. Conversely, the model trained on non-expert labels tends to misclassify neutral samples as either positive or negative. The combined label model mitigates the bias towards positive expert categories but increases misclassification for negative and neutral categories. In arousal, unlike valence, there's an unbalanced data distribution. Expert labeled 43% of training data as neutral arousal, leading the model to often assign this category to test samples. Non-experts show a bias toward high arousal. Combining labels mitigates expert bias toward the neutral class and reduces non-expert bias toward the positive category. As a result, the model has fewer low arousal samples to train on, lowering accuracy in this category.\n\nOverall, the annotators' biases may cause differences in UA scores to up to 4% for valence and 15% for arousal. The better performance of the models based on the non-experts and the combined labels may be due to the higher number of annotators included, which may reduce individual biases in their interpretation of emotions. However, this number is still limited compared to the 319 raters in IEMOCAP.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "C. Fairness Evaluation",
      "text": "There is limited research on evaluating model fairness, particularly in the context of pre-trained models  [46] . Our focus here is on gender, given the insufficient information for other attributes considered in the reference datasets (see Fig.  7 ). For EMOVOME dataset, models trained using expert labels exhibit a notable bias towards males, as the UA is around 10% higher for males in valence and arousal prediction and 1.7% in emotion categories prediction. The use of non-expert labels resulted in an increase in UA for males of 4.6% valence, but it was 1.9% higher for females on arousal. Interestingly, the combined label yielded the most similar results for both genders, with 0.3% for valence and 1.4% for arousal. In the case of IEMOCAP, again, UA was higher for male speakers in valence (4%) but lower in arousal (-3.4%). For categories, the UA for males was +5.9% compared to females. Finally, RAVDESS presents the highest difference between both genders, with the UA for females being 29.7% higher for females compared to males. For arousal and valence, the results are also higher for females (1% and 6.5%, respectively).\n\nOverall, SER models obtain better test results for male speakers in EMOVOME, following previous studies in Spanish datasets  [30] ,  [38] . In the reference datasets, IEMOCAP aligns with the observed trend, while RAVDESS shows the opposite results. However, it's important to note that both datasets have a limited number of speakers in the test set (two for IEMOCAP and five for RAVDESS), so no significant conclusions can be drawn from these results.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "D. Limitations And Future Work",
      "text": "Firstly, while we manually screened for ad hoc recordings, complete control over this factor remains challenging, necessitating reliance on participant integrity. Secondly, improving the annotation process of EMOVOME samples within emotion categories by introducing new raters could alleviate potential individual biases that impact SER models. Similarly, expanding the pool of non-expert labels with more raters may help mitigate bias and achieve a more balanced data distribution regarding arousal. Additionally, other datasets, such as EmoSpanishDB or MOUD, could be explored to increase the number of training samples and assess potential improvements in model performance. Furthermore, in the creation of SER models based on pre-trained models, the current use of average time pooling as an aggregation approach may exhibit suboptimal performance, particularly noticeable in the EMOVOME dataset, where audio duration exhibits significant variability. Future research endeavors will focus on refining time aggregation methods and exploring alternative techniques to address these challenges effectively.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Viii. Conclusions",
      "text": "We developed and publicly released EMOVOME, the first public dataset with realistic emotions from spontaneous conversations in a real-life scenario. We developed speakerindependent SER models using EMOVOME, and compared the results with IEMOCAP and RAVDESS. Superior results were achieved with pre-trained transformer-based models compared to baseline models based on acoustic features. However, EMOVOME results demonstrated lower performance compared to the acted RAVDESS dataset. For the elicited IEMOCAP dataset, the prediction of emotion categories outperformed EMOVOME, but similar results were obtained for predicting valence and arousal. A comprehensive study was also conducted to assess the influence of different properties of EMOVOME on SER performance. Notably, we found variations depending on the labels provided by different annotators, with superior outcomes observed when utilizing combined labels from both expert and non-experts. Interestingly, this combined label also yielded the most equitable results when assessing gender fairness. The results highlight the importance of natural datasets for predicting genuine emotions, emphasizing the need for speech from real-life scenarios to accurately assess the challenges associated with authentic environments.",
      "page_start": 11,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: presents a diagram illustrating the steps followed to create",
      "page": 4
    },
    {
      "caption": "Figure 2: Concerning valence,",
      "page": 5
    },
    {
      "caption": "Figure 2: Distribution of audio samples based on their arousal and valence labels.",
      "page": 5
    },
    {
      "caption": "Figure 1: Overview of the methodology.",
      "page": 5
    },
    {
      "caption": "Figure 3: shows the correspondence between categories and",
      "page": 6
    },
    {
      "caption": "Figure 3: Valence and arousal ratings for each emotion. The diameter represents",
      "page": 6
    },
    {
      "caption": "Figure 4: show the performance",
      "page": 8
    },
    {
      "caption": "Figure 4: Cross-validation results for the three methods implemented (eGeMAPS, Embeddings and Emb+eGeMAPS) across the different datasets (EMOVOME,",
      "page": 8
    },
    {
      "caption": "Figure 5: Confusion matrix for the test samples for valence and arousal",
      "page": 9
    },
    {
      "caption": "Figure 6: Confusion matrix for the test samples for emotion category prediction",
      "page": 9
    },
    {
      "caption": "Figure 7: A positive difference means",
      "page": 10
    },
    {
      "caption": "Figure 7: Evaluation of gender fairness for the three labels and datasets.",
      "page": 10
    },
    {
      "caption": "Figure 4: , overall, the embedding approach significantly improves",
      "page": 10
    },
    {
      "caption": "Figure 6: For EMOVOME, surprise is not correctly predicted in any case",
      "page": 10
    },
    {
      "caption": "Figure 5: ) reveals discernible biases towards specific emotion",
      "page": 11
    },
    {
      "caption": "Figure 7: ). For EMOVOME dataset, models trained using expert labels",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "NE 1": "-",
          "NE 2": "V = 0.700\nA = 0.142",
          "NE 3": "V = 0.754\nA = 0.407",
          "E": "V = 0.614\nA = 0.275"
        },
        {
          "NE 1": "V = 0.700\nA = 0.142",
          "NE 2": "-",
          "NE 3": "V = 0.732\nA = 0.235",
          "E": "V = 0.646\nA = 0.075"
        },
        {
          "NE 1": "V = 0.754\nA = 0.407",
          "NE 2": "V = 0.732\nA = 0.235",
          "NE 3": "-",
          "E": "V = 0.649\nA = 0.241"
        },
        {
          "NE 1": "V = 0.614\nA = 0.275",
          "NE 2": "V = 0.646\nA = 0.075",
          "NE 3": "V = 0.649\nA = 0.241",
          "E": "-"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "\u0000\u0017\u0000\u001a\u0000\u0011\u0000\u0014\n\u0000\u0017\u0000\u001a\u0000\u0011\u0000\u0014": "\u0000\u0015\u0000\u0016\u0000\u0011\u0000\u001a\n\u0000\u0014\u0000\u0014\u0000\u0011\u0000\u0013",
          "\u0000\u0018\u0000\u0011\u0000\u001c\n\u0000\u0016\u0000\u0015\u0000\u0011\u0000\u001c\n\u0000\u0017\u0000\u0013\u0000\u0011\u0000\u0015": ""
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Handbook of cognition and emotion",
      "authors": [
        "P Ekman"
      ],
      "year": "1999",
      "venue": "Handbook of cognition and emotion"
    },
    {
      "citation_id": "2",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "3",
      "title": "Databases, features and classifiers for speech emotion recognition: a review",
      "authors": [
        "M Swain",
        "A Routray",
        "P Kabisatpathy"
      ],
      "year": "2018",
      "venue": "International Journal of Speech Technology"
    },
    {
      "citation_id": "4",
      "title": "Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers",
      "authors": [
        "M Akc",
        "K Oguz"
      ],
      "year": "2020",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "5",
      "title": "Speech emotion recognition using machine learning-a systematic review",
      "authors": [
        "S Madanian",
        "T Chen",
        "O Adeleye",
        "J Templeton",
        "C Poellabauer",
        "D Parry",
        "S Schneider"
      ],
      "year": "2023",
      "venue": "Speech emotion recognition using machine learning-a systematic review"
    },
    {
      "citation_id": "6",
      "title": "The composite sensing of affect",
      "authors": [
        "G Mcintyre",
        "R Göcke"
      ],
      "year": "2008",
      "venue": "Affect and Emotion in Human-Computer Interaction: From Theory to Applications"
    },
    {
      "citation_id": "7",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "8",
      "title": "Impact of annotator demographics on sentiment dataset labeling",
      "authors": [
        "Y Ding",
        "J You",
        "T.-K Machulla",
        "J Jacobs",
        "P Sen",
        "T Höllerer"
      ],
      "year": "2022",
      "venue": "Proceedings of the ACM on Human-Computer Interaction"
    },
    {
      "citation_id": "9",
      "title": "Ensemble methods for spoken emotion recognition in call-centres",
      "authors": [
        "D Morrison",
        "R Wang",
        "L Silva"
      ],
      "year": "2007",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "10",
      "title": "Toward detecting emotions in spoken dialogs",
      "authors": [
        "C Lee",
        "S Narayanan"
      ],
      "year": "2005",
      "venue": "IEEE Transactions on Speech and Audio Processing"
    },
    {
      "citation_id": "11",
      "title": "Getting started with susas: a speech under simulated and actual stress database",
      "authors": [
        "J Hansen",
        "S Bou-Ghazale",
        "R Sarikaya",
        "B Pellom"
      ],
      "year": "1997",
      "venue": "Eurospeech"
    },
    {
      "citation_id": "12",
      "title": "Multimodal sentiment analysis of spanish online videos",
      "authors": [
        "V Rosas",
        "R Mihalcea",
        "L.-P Morency"
      ],
      "year": "2013",
      "venue": "IEEE intelligent Systems"
    },
    {
      "citation_id": "13",
      "title": "Cmu-moseas: A multimodal language dataset for spanish, portuguese, german and french",
      "authors": [
        "A Zadeh",
        "Y Cao",
        "S Hessner",
        "P Liang",
        "S Poria",
        "L.-P Morency"
      ],
      "year": "2020",
      "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "14",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "15",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "16",
      "title": "Presenting kapodi-the searchable database of emotional stimuli sets",
      "authors": [
        "K Diconne",
        "G Kountouriotis",
        "A Paltoglou",
        "A Parker",
        "T Hostler"
      ],
      "year": "2022",
      "venue": "Emotion Review"
    },
    {
      "citation_id": "17",
      "title": "Acted facial expressions in the wild database",
      "authors": [
        "A Dhall",
        "R Goecke",
        "S Lucey",
        "T Gedeon"
      ],
      "year": "2011",
      "venue": "Acted facial expressions in the wild database"
    },
    {
      "citation_id": "18",
      "title": "A new emotion database: considerations, sources and scope",
      "authors": [
        "E Douglas-Cowie",
        "R Cowie",
        "M Schröder"
      ],
      "year": "2000",
      "venue": "ISCA tutorial and research workshop (ITRW) on speech and emotion"
    },
    {
      "citation_id": "19",
      "title": "The humaine database: Addressing the collection and annotation of naturalistic and induced emotional data",
      "authors": [
        "E Douglas-Cowie",
        "R Cowie",
        "I Sneddon",
        "C Cox",
        "O Lowry",
        "M Mcrorie",
        "J.-C Martin",
        "L Devillers",
        "S Abrilian",
        "A Batliner"
      ],
      "year": "2007",
      "venue": "Affective Computing and Intelligent Interaction: Second International Conference"
    },
    {
      "citation_id": "20",
      "title": "",
      "authors": [
        "Portugal Lisbon"
      ],
      "year": "2007",
      "venue": ""
    },
    {
      "citation_id": "21",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "22",
      "title": "Being bored? recognising natural interest by extensive audiovisual integration for real-life application",
      "authors": [
        "B Schuller",
        "R Müller",
        "F Eyben",
        "J Gast",
        "B Hörnler",
        "M Wöllmer",
        "G Rigoll",
        "A Höthker",
        "H Konosu"
      ],
      "year": "2009",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "23",
      "title": "Multilingual, cross-lingual, and monolingual speech emotion recognition on emofilm dataset",
      "authors": [
        "B Atmaja",
        "A Sasou"
      ],
      "year": "2023",
      "venue": "2023 Asia Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "24",
      "title": "Validation of an acoustical modelling of emotional expression in spanish using speech synthesis techniques",
      "authors": [
        "I Iriondo",
        "R Guaus",
        "A Rodríguez",
        "P Lázaro",
        "N Montoya",
        "J Blanco",
        "D Bernadas",
        "J Oliver",
        "D Tena",
        "L Longhi"
      ],
      "year": "2000",
      "venue": "ISCA Tutorial and Research Workshop (ITRW) on Speech and Emotion"
    },
    {
      "citation_id": "25",
      "title": "Analysis and modelling of emotional speech in spanish",
      "authors": [
        "J Montero",
        "J Gutiérrez-Arriola",
        "J Colás",
        "E Enriquez",
        "J Pardo"
      ],
      "year": "1999",
      "venue": "Proceedings of international conference on phonetic sciences"
    },
    {
      "citation_id": "26",
      "title": "Emotion recognition in non-structured utterances for human-robot interaction",
      "authors": [
        "C Martínez",
        "A Cruz"
      ],
      "year": "2005",
      "venue": "ROMAN 2005. IEEE International Workshop on Robot and Human Interactive Communication"
    },
    {
      "citation_id": "27",
      "title": "Recognition of emotions in mexican spanish speech: An approach based on acoustic modelling of emotion-specific vowels",
      "authors": [
        "S.-O Caballero-Morales"
      ],
      "year": "2013",
      "venue": "The Scientific World Journal"
    },
    {
      "citation_id": "28",
      "title": "Spanish expressive voices: Corpus for emotion research in spanish",
      "authors": [
        "R Barra-Chicote",
        "J Montero",
        "J Macias-Guarasa",
        "S Lufti",
        "J Lucas",
        "F Fernandez",
        "R San-Segundo",
        "J Ferreiros",
        "R Cordoba"
      ],
      "year": "2008",
      "venue": "Proc. of LREC"
    },
    {
      "citation_id": "29",
      "title": "Validating a multilingual and multimodal affective database",
      "authors": [
        "J López",
        "I Cearreta",
        "I Fajardo",
        "N Garay"
      ],
      "year": "2007",
      "venue": "International Conference on Usability and Internationalization"
    },
    {
      "citation_id": "30",
      "title": "Feature selection for speech emotion recognition in spanish and basque: on the use of machine learning to improve human-computer interaction",
      "authors": [
        "A Arruti",
        "I Cearreta",
        "A Álvarez",
        "E Lazkano",
        "B Sierra"
      ],
      "year": "2014",
      "venue": "PloS one"
    },
    {
      "citation_id": "31",
      "title": "The mexican emotional speech database (mesd): elaboration and assessment based on machine learning",
      "authors": [
        "M Duville",
        "L Alonso-Valerdi",
        "D Ibarra-Zarate"
      ],
      "year": "2021",
      "venue": "IEEE Engineering in Medicine & Biology Society"
    },
    {
      "citation_id": "32",
      "title": "Emotional speech synthesis database elra-s0329",
      "year": "2011",
      "venue": "Emotional speech synthesis database elra-s0329"
    },
    {
      "citation_id": "33",
      "title": "Interface databases: Design and collection of a multilingual emotional speech database",
      "authors": [
        "V Hozjan",
        "Z Kacic",
        "A Moreno",
        "A Bonafonte",
        "A Nogueiras"
      ],
      "year": "2002",
      "venue": "Proceedings of the 3rd international conference on language"
    },
    {
      "citation_id": "34",
      "title": "Mexican emotional speech database based on semantic, frequency, familiarity, concreteness, and cultural shaping of affective prosody",
      "authors": [
        "M Duville",
        "L Alonso-Valerdi",
        "D Ibarra-Zarate"
      ],
      "year": "2021",
      "venue": "Data"
    },
    {
      "citation_id": "35",
      "title": "Automatic speech emotion recognition using an optimal combination of features based on emd-tkeo",
      "authors": [
        "L Kerkeni",
        "Y Serrestou",
        "K Raoof",
        "M Mbarki",
        "M Mahjoub",
        "C Cleder"
      ],
      "year": "2019",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "36",
      "title": "Intermediary fuzzification in speech emotion recognition",
      "authors": [
        "G Assunc",
        "P Menezes"
      ],
      "year": "2020",
      "venue": "2020 IEEE international conference on fuzzy systems"
    },
    {
      "citation_id": "37",
      "title": "Automatic speech emotion recognition using machine learning",
      "authors": [
        "L Kerkeni",
        "Y Serrestou",
        "M Mbarki",
        "K Raoof",
        "M Mahjoub",
        "C Cleder"
      ],
      "year": "2019",
      "venue": "Automatic speech emotion recognition using machine learning"
    },
    {
      "citation_id": "38",
      "title": "Categorical vs dimensional perception of italian emotional speech",
      "authors": [
        "E Parada-Cabaleiro",
        "G Costantini",
        "A Batliner",
        "A Baird",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "39",
      "title": "The emotion probe: On the universality of cross-linguistic and cross-gender speech emotion recognition via machine learning",
      "authors": [
        "G Costantini",
        "E Parada-Cabaleiro",
        "D Casali",
        "V Cesarini"
      ],
      "year": "2022",
      "venue": "Sensors"
    },
    {
      "citation_id": "40",
      "title": "Emomatchspanishdb: study of speech emotion recognition machine learning models in a new spanish elicited database",
      "authors": [
        "E Garcia-Cuesta",
        "A Salvador",
        "D Pãez"
      ],
      "year": "2023",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "41",
      "title": "Dawn of the transformer era in speech emotion recognition: closing the valence gap",
      "authors": [
        "J Wagner",
        "A Triantafyllopoulos",
        "H Wierstorf",
        "M Schmitt",
        "F Burkhardt",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "42",
      "title": "A comparative study of pre-trained speech and audio embeddings for speech emotion recognition",
      "authors": [
        "O Phukan",
        "A Buduru",
        "R Sharma"
      ],
      "year": "2023",
      "venue": "A comparative study of pre-trained speech and audio embeddings for speech emotion recognition",
      "arxiv": "arXiv:2304.11472"
    },
    {
      "citation_id": "43",
      "title": "Evaluating self-supervised speech representations for speech emotion recognition",
      "authors": [
        "B Atmaja",
        "A Sasou"
      ],
      "year": "2022",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "44",
      "title": "Representation learning through cross-modal conditional teacher-student training for speech emotion recognition",
      "authors": [
        "S Srinivasan",
        "Z Huang",
        "K Kirchhoff"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "45",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "46",
      "title": "Contrastive unsupervised learning for speech emotion recognition",
      "authors": [
        "M Li",
        "B Yang",
        "J Levy",
        "A Stolcke",
        "V Rozgic",
        "S Matsoukas",
        "C Papayiannis",
        "D Bone",
        "C Wang"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "47",
      "title": "Gender De-Biasing in Speech Emotion Recognition",
      "authors": [
        "C Gorrostieta",
        "R Lotfian",
        "K Taylor",
        "R Brutti",
        "J Kane"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "48",
      "title": "Neo five-factor inventory (neo-ffi)",
      "authors": [
        "P Costa",
        "R Mccrae"
      ],
      "year": "1989",
      "venue": "Psychological Assessment Resources"
    },
    {
      "citation_id": "49",
      "title": "Measuring emotion: the self-assessment manikin and the semantic differential",
      "authors": [
        "M Bradley",
        "P Lang"
      ],
      "year": "1994",
      "venue": "Journal of behavior therapy and experimental psychiatry"
    },
    {
      "citation_id": "50",
      "title": "Ryerson audiovisual database of emotional speech and song (ravdess): Arousal and valence validation",
      "authors": [
        "K Kovacek",
        "S Livingstone",
        "G Singh",
        "F Russo"
      ],
      "venue": "Ryerson audiovisual database of emotional speech and song (ravdess): Arousal and valence validation"
    },
    {
      "citation_id": "51",
      "title": "A proposal for multimodal emotion recognition using aural transformers and action units on ravdess dataset",
      "authors": [
        "C Luna-Jiménez",
        "R Kleinlein",
        "D Griol",
        "Z Callejas",
        "J Montero",
        "F Fernández-Martínez"
      ],
      "year": "2022",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "52",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "53",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "54",
      "title": "Xls-r: Self-supervised cross-lingual speech representation learning at scale",
      "authors": [
        "A Babu",
        "C Wang",
        "A Tjandra",
        "K Lakhotia",
        "Q Xu",
        "N Goyal",
        "K Singh",
        "P Von Platen",
        "Y Saraf",
        "J Pino"
      ],
      "year": "2021",
      "venue": "Xls-r: Self-supervised cross-lingual speech representation learning at scale",
      "arxiv": "arXiv:2111.09296"
    },
    {
      "citation_id": "55",
      "title": "Unsupervised Cross-Lingual Representation Learning for Speech Recognition",
      "authors": [
        "A Conneau",
        "A Baevski",
        "R Collobert",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "56",
      "title": "A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding",
      "authors": [
        "Y Wang",
        "A Boumadane",
        "A Heba"
      ],
      "year": "2021",
      "venue": "A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding",
      "arxiv": "arXiv:2111.02735"
    },
    {
      "citation_id": "57",
      "title": "Emotion Recognition from Speech Using wav2vec 2.0 Embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "58",
      "title": "Robust wav2vec 2.0: Analyzing domain shift in self-supervised pre-training",
      "authors": [
        "W.-N Hsu",
        "A Sriram",
        "A Baevski",
        "T Likhomanenko",
        "Q Xu",
        "V Pratap",
        "J Kahn",
        "A Lee",
        "R Collobert",
        "G Synnaeve"
      ],
      "year": "2021",
      "venue": "Robust wav2vec 2.0: Analyzing domain shift in self-supervised pre-training",
      "arxiv": "arXiv:2104.01027"
    },
    {
      "citation_id": "59",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "60",
      "title": "x-vectors meet emotions: A study on dependencies between emotion and speaker recognition",
      "authors": [
        "R Pappagari",
        "T Wang",
        "J Villalba",
        "N Chen",
        "N Dehak"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "61",
      "title": "Speaker Attentive Speech Emotion Recognition",
      "authors": [
        "C Moine",
        "N Obin",
        "A Roebel"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech 2021"
    },
    {
      "citation_id": "62",
      "title": "Unispeech-sat: Universal speech representation learning with speaker aware pre-training",
      "authors": [
        "S Chen",
        "Y Wu",
        "C Wang",
        "Z Chen",
        "Z Chen",
        "S Liu",
        "J Wu",
        "Y Qian",
        "F Wei",
        "J Li"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "63",
      "title": "Spoken language recognition using x-vectors",
      "authors": [
        "D Snyder",
        "D Garcia-Romero",
        "A Mccree",
        "G Sell",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2018",
      "venue": "Odyssey"
    },
    {
      "citation_id": "64",
      "title": "SpeechBrain: A generalpurpose speech toolkit",
      "authors": [
        "M Ravanelli",
        "T Parcollet",
        "P Plantinga",
        "A Rouhe",
        "S Cornell",
        "L Lugosch",
        "C Subakan",
        "N Dawalatabad",
        "A Heba",
        "J Zhong",
        "J.-C Chou",
        "S.-L Yeh",
        "S.-W Fu",
        "C.-F Liao",
        "E Rastorgueva",
        "F Grondin",
        "W Aris",
        "H Na",
        "Y Gao",
        "R Mori",
        "Y Bengio"
      ],
      "year": "2021",
      "venue": "SpeechBrain: A generalpurpose speech toolkit",
      "arxiv": "arXiv:2106.04624"
    },
    {
      "citation_id": "65",
      "title": "A survey of speech emotion recognition in natural environment",
      "authors": [
        "M Fahad",
        "A Ranjan",
        "J Yadav",
        "A Deepak"
      ],
      "year": "2021",
      "venue": "Digital signal processing"
    },
    {
      "citation_id": "66",
      "title": "Probing speech emotion recognition transformers for linguistic knowledge",
      "authors": [
        "A Triantafyllopoulos",
        "J Wagner",
        "H Wierstorf",
        "M Schmitt",
        "U Reichel",
        "F Eyben",
        "F Burkhardt",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "Probing speech emotion recognition transformers for linguistic knowledge",
      "arxiv": "arXiv:2204.00400"
    }
  ]
}