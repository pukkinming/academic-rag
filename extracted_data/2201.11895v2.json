{
  "paper_id": "2201.11895v2",
  "title": "\"That'S So Cute!\": The Care Dataset For Affective Response Detection",
  "published": "2022-01-28T02:17:50Z",
  "authors": [
    "Jane Dwivedi-Yu",
    "Alon Y. Halevy"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Social media plays an increasing role in our communication with friends and family, and our consumption of information and entertainment. Hence, to design effective ranking functions for posts on social media, it would be useful to predict the affective response to a post (e.g., whether the user is likely to be humored, inspired, angered, informed). Similar to work on emotion recognition (which focuses on the affect of the publisher of the post), the traditional approach to recognizing affective response would involve an expensive investment in human annotation of training data. We create and publicly release CARE db , a dataset of 230k social media annotations according to 7 affective responses using the Common Affective Response Expression (CARE) method. The CARE method is a means of leveraging the signal that is present in comments that are posted in response to a post, providing high-precision evidence about the affective response to the post without human annotation. Unlike human annotation, the annotation process we describe here can be iterated upon to expand the coverage of the method, particularly for new affective responses. We present experiments that demonstrate that the CARE annotations compare favorably with crowdsourced annotations. Finally, we use CARE db to train competitive BERT-based models for predicting affective response as well as emotion detection, demonstrating the utility of the dataset for related tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Social media and other online media platforms have become a common means of both interacting and connecting with others as well as finding interesting, informing, and entertaining content. Users of those platforms depend on the ranking systems of the recommendation systems to show them information they will be most interested in and to safeguard them against unfavorable experiences. Towards this end, a key technical problem is to predict the affective response that a user may have when they see a post. Some affective responses can be described by emotions (e.g., angry, joyful), and others may be described more as experiences (e.g., entertained, inspired). Predicting affective response differs from emotion detection in that the latter focuses on the emotions expressed by the publisher of the post (referred to as the publisher affect in  Chen et al. (2014) ) and not on the viewer of the content. While the publisher's emotion may be relevant to the affective response, it only provides a partial signal (dwi), and the two are not always equivalent (see Figure  D4  for an illustrative example). Affective response for recommender systems has shown to be critical in several applications such as music, emotional health monitoring systems, product and travel recommendations  (Rosa et al., 2015 (Rosa et al., , 2018;; Akram et al., 2020; Artemenko et al., 2020; dwi) .\n\nCurrent approaches to predicting affective response require obtaining training data from human annotators who try to classify content into classes of a given taxonomy. However, obtaining enough training data can be expensive, and moreover, due to the subjective nature of the problem, achieving consensus among annotators can be challenging. Some methods explore inferring responses from physiological data or facial expressions from users, but this is a highly invasive process and can be difficult to scale to multiple users.  (Tkalčič et al., 2017 (Tkalčič et al., , 2019;; Angelastro et al., 2019) . This paper introduces the Common Affective Response Expression method (CARE for short), a means of obtaining labels for affective response in an unsupervised way from the comments written in response to online posts. CARE uses patterns and a keyword-affect mapping to identify expressions in comments that provide high-precision evidence about the affective response of the readers to the post. For example, the expression \"What a hilarious story\" may indicate that a post is humorous and \"This is so cute\" may indicate that a post is adorable. We seed the system with a small number of high-precision patterns and mappings. We then iteratively expand on the initial set by considering frequent patterns and keywords in unlabeled comments on posts labeled by the previous iteration.\n\nUsing CARE, we create the largest dataset to date for affective response, CARE db , which contains 230k posts annotated according to 7 affective responses. We validate the effectiveness of CARE by comparing the CARE annotations with crowd-sourced annotations. Our experiments show that there is a high degree of agreement between the annotators and the labels proposed by CARE (e.g., in 90% of the cases, at least two out of three annotators agree with all the CARE labels). Furthermore, we show that the CARE patterns/lexicon have greater accuracy than applying SOTA emo-tion recognition techniques to the comments. Using CARE db , we train CARE-BERT 1  , a BERT-based model that can predict affective response without relying on comments. CARE-BERT provides strong baseline performance for the task of predicting affective response, on par with the SOTA models for emotion recognition. Furthermore, we show that CARE-BERT can be used for transfer learning to a different emotion-recognition task, achieving similar performance to  Demszky et al. (2020) , which relied on manually-labeled training data.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "We first situate our work with respect to previous research on related tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Detection In Text",
      "text": "Approaches to emotion detection can be broadly categorized into three groups: lexicon-based, machine learning, and combinations of the first two. The lexicon-based approach typically leverages lexical resources such as lexicons and encoded rules to guide emotion prediction  (Tao, 2004; Ma et al., 2005; Asghar et al., 2017) . Though these methods can be fast and interpretable, they are often not as robust and flexible because of the constraints of the lexicon  (Alswaidan and Menai, 2020; Acheampong et al., 2020) . Additionally, the scope of emotions predicted by these works is usually fairly small, ranging from two to five, and most datasets utilized are usually smaller than 10k, making it unclear if they extrapolate well. Among the ML approaches, many SOTA works employ deep learning methods  (Demszky et al., 2020; Felbo et al., 2017; Barbieri et al., 2018; Huang et al., 2019a; Baziotis et al., 2017; Huang et al., 2019b) , but while these show significant improvement over prior techniques, they are highly uninterpretable and often require prohibitively large human-labeled datasets to train. In both the lexicon-based approach and the ML-approach, the classes of emotions predicted in these works are usually non-extendable or require additional labeled data.\n\nWhile there are some commonalities between works in emotion detection and affective response detection, the problems are distinct enough that we cannot simply apply emotion recognition techniques to our setting. Emotion recognition focuses on the publisher affect (the affect of the person writing the text). The publisher affect may provide a signal about the affective response of the reader, but there is no simple mapping from one to the other. For example, being 'angered' is an affective response that does not only result from reading an angry post-it can result from a multitude of different publisher affects (e.g. excited, angry, sympathetic, embarrassed, or arrogant). For some affective responses, such as being 'grateful' or feeling 'connected' to a community, the corresponding publisher affect is highly unclear.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Affective Response Detection",
      "text": "There have been some works that address affective response in limited settings, such as understanding reader responses to online news  (Katz et al., 2007; Strapparava and Mihalcea, 2007; Lin et al., 2008; Lei et al., 2014) . In contrast, our goal is to address the breadth of content on social media. There are works which use Facebook reactions as a proxy for affective response, but these are constrained by the pre-defined set of reactions  (Clos et al., 2017; Raad et al., 2018; Pool and Nissim, 2016; Graziani et al., 2019; Krebs et al., 2017) . The work described in  Rao et al. (2014)  and  Bao et al. (2012)  attempts to associate emotions with topics, but a single topic can have a large variety of affective responses when seen on social media, and therefore their model does not apply to our case. Some works in the computer vision community study affective response to images  (Chen et al., 2014; Jou et al., 2014) ; as they note, most of the work in the vision community also focuses on publisher affect.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methods For Unsupervised Labeling",
      "text": "A major bottleneck in developing models for emotion and affective response detection is the need for large amounts of training data. As an alternative to manually-labeled data, many works utilize metadata such as hashtags, emoticons, and Facebook reactions as pseudo-labels  (Wang et al., 2012; Suttles and Ide, 2013; Hasan et al., 2014; Mohammad and Kiritchenko, 2015) . However, these can be highly noisy and limited in scope. For example, there exist only seven Facebook reactions, and they do not necessarily correspond to distinct affective responses. Additionally, for abstract concepts like emotions, hashtagged content may only capture a superficial interpretation of the concept. For example, #inspiring on Instagram will give many photos featuring selfies or obvious inspirational quotes, which do not sufficiently represent inspiration. The work we present here extracts labels from free-form text in comments rather than metadata. The work done in  Sintsova and Pu (2016)  is similar to our work in that it pseudo-labels tweets and extends its lexicon, but the classifier itself is a keyword, rule-based approach and is heavily reliant on the capacity of these lexicons. In contrast, our work leverages the high precision of CARE and uses these results to train a model, which is not constrained by the lexicon size in its predictions. Our method also employs bootstrapping to expand the set of patterns and lexicon, similar to  Agichtein and Gravano (2000)  and  Jones et al. (1999)  but focuses on extracting affect rather than relation tuples.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "The Care Method",
      "text": "In this section, we provide a formal description of CARE for annotating the affective response of posts. Before we proceed, we note two aspects of affective responses. First, there is no formal definition for what qualifies as an affective response. In practice, we use affective responses to understand the experience that the user has when seeing a piece of content, and these responses may be both emotional and cognitive. Second, the response a user may have to a particular piece of content is clearly a very personal one. Our goal here is to predict whether a piece of content is generally likely to elicit a particular affective response. In practice, if the recommendation system has models of user interests and behavior, these would need to be combined with the affect predictions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Care Patterns And The Care Lexicon",
      "text": "CARE is composed of two major components: CARE patterns, regular expressions used to extract information from the comments of a post, and the CARE lexicon, a keyword-affect dictionary used to map the comment to an affect. CARE patterns are not class or affect-specific and leverage common structure present in comments for affective response extraction. There is an unlimited number of possible CARE patterns, but we seeded the system with six CARE patterns and an additional 17 more were automatically discovered using the expansion method. In the same spirit as Hearst Patterns  (Hearst, 1992) , CARE patterns are tailored to extract specific relationships and rely on two sets of sub-patterns:\n\n• Exaggerators {E}: words that intensify or exaggerate a statement, e.g., so, very, or really.\n\n• Indicators {I}: words (up to 3) that exist in the CARE lexicon, which maps the indicator to a particular class. For example, 'funny' in \"This is so funny\" would map to amused.\n\nConsequently, one example of the six CARE patterns that were used to seed the system is the following: {this|that|those|these}{is|are}*{E} * {I} + (the symbol * (resp. + ) indicates that zero (resp. one) or more matches are required). An example of an instantiation of this pattern would be, 'This is so amazing!'.\n\nGiven the indicators extracted by the CARE patterns, the CARE lexicon is responsible for mapping the comment to particular affective responses. The lexicon contains 163 indicators for the 7 classes we consider (123 of which were automatically identified in the expansion process described in the next section). We also considered using other lexicons  (Strapparava and Valitutti, 2004; Poria et al., 2014; Staiano and Guerini, 2014; Esuli and Sebastiani, 2006; Mohammad et al., 2013 ), but we found that they were lacking enough application context to be useful in our setting. Table  1  shows the affects in the CARE lexicon and corresponding definitions and example comments that would fall under each affect (or class). The classes excited, angered, saddened, and scared were chosen since they are often proposed as the four basic emotions  (Wang et al., 2011; Jack et al., 2014; Gu et al., 2016; Zheng et al., 2016) . The classes adoring, amused, and approving were established because they are particularly important in the context of social media for identifying positive content that users enjoy. Overall, a qualitative inspection indicated that these seven have minimal conceptual overlap and sufficiently broad coverage. We note, however, that one of the benefits of the method we describe is that it is relatively easy to build a model for a new class of interest compared to the process of human annotation.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Labeling Posts",
      "text": "Here we describe how to combine and use the two major components (CARE patterns and lexicon) at the comment-level in order to annotate the postlevel affective response. The pipeline for labeling posts is shown in steps 1-3 of Figure  1  and described in detail in Algorithm 1 in the Appendix. We begin with reg-ex matching of CARE patterns and individual sentences of the comments. We truncate the front half of a sentence if it contains words like 'but' or 'however' because the latter half usually indicates their predominant sentiment. We also reject indicators that contain negation words such as 'never', 'not', or 'cannot' (although one could theoretically map this to the opposite affective response using Plutchik's Wheel of Emotions  (Plutchik, 1980) ). Note that contrary to traditional rule-based or machine-learning methods, we do not strip stop words like 'this' and 'very' because it is often crucial to the regular expression matching, and this specificity has a direct impact on the precision of the pipeline.\n\nGiven the reg-ex matches, we use the lexicon to map the indicators to the publisher affect of the comment (e.g., excited). Because the expressed affect of the comments equates to the affective response of a post, we obtain a post-level affective response label by aggregating the comment-level labels and filtering out labels that have a support smaller than t. Specifically, a post would be labeled with the affective response a if at least t of the comments were labeled with a. In our experiments, we used a value of t = 5, after qualitative inspection of CARE db , discussed in Section 4.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Expanding",
      "text": "CARE patterns/lexicon: We seeded our patterns and lexicon with a small intuitive set and then expanded them by looking at common n-grams that appear across posts with the same label (steps A-C of Figure  1 ). At a high level, for a given affect a, consider the set, comm(a), of all the comments on posts that were labeled a, but did not match any CARE pattern. From these comments, we extract new keywords (e.g. 'dope' for approving as in 'This is so dope.') for the CARE lexicon by taking the most frequent n-grams in comm(a) but infrequently in comm(b), where b includes all classes except a. On the other hand, the most common n-grams co-occuring with multiple classes were converted to regular expressions and then added as new CARE patterns (see Table  B1  for a few examples). We added CARE patterns according to their frequency and stopped when we had sufficient data to train our models. After two expansion rounds, the set of patterns and indicators increased from 6 to 23 and 40 to 163, respectively. Counting the possible combinations of patterns and indicators, there are roughly 3500 distinct expressions. When considering the possible 23 CARE patterns, 163 CARE lexicon indicators, and 37 exaggerators, there are a total of 130k possible instantiations of a matching comment.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Evaluating Care Db",
      "text": "In this section we apply our method to social media posts and validate these annotations using human evaluation (Section 4.1). Section 4.2 discusses class-wise error analysis, and in Section 4.3, we explore the alternative possibility of creating CARE db using a SOTA publisher-affect classifier  (Demszky et al., 2020)  to label the comments.\n\nCARE db : Our experiments use a dataset that is created from Reddit posts and comments in the pushshift.io database that were created between 2011 and 2019. We create our dataset, CARE db , as follows. We used CARE patterns and the CARE lexicon to annotate 34 million comments from 24 million distinct posts. After filtering with a threshold of t = 5, we obtained annotations for 400k posts (the total number of posts that have at least 5 comments was 150 million). The low recall is expected given the specificity of CARE patterns/lexicon. We also filtered out posts that have less than 10 characters, resulting in a total of 230k posts in CARE db . Table  1  shows the breakdown of cardinality per affective response. 195k of the posts were assigned a single label, whereas 26k (resp. 8k) were assigned two (resp. three) labels.\n\nNote that the distribution of examples per class in CARE db is not reflective of the distribution in the original data, because different classes have different recall rates. The CARE db dataset features the pushshift.io id and text of the post as well as the annotations using CARE.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Human Evaluation",
      "text": "In our next experiment, we evaluate the labels predicted by CARE with the help of human annotators using Amazon Mechanical Turk (AMT), restricting to those who qualify as AMT Masters and having lifetime approval rating greater than 80%. The dataset for annotation was created as follows. We sub-sampled a set of 6000 posts from CARE db , ensuring that we have at least 700 samples from each class and asked annotators to label the affective response of each post. Annotators were encouraged to select as many as appropriate and also permitted to choose 'None of the above' as shown in Fig-  ure C1 . In addition to the post, we also showed annotators up to 10 sampled comments from the post in order to provide more context. Every post was shown to three of the 91 distinct annotators.\n\nFor quality control, we also verified that there no individual annotator provided answers that disagreed with the CARE labels more than 50% of the time on more than 100 posts. We observed an average Fleiss' kappa score 0.59, which is considered moderate to high agreement, the breakdown of which is shown in Table  C1 . Table 2 shows that the rate of agreement between the annotators and the labels proposed by the CARE method is high. For example, 94% of posts had at least one label proposed by CARE that was confirmed by 2 or more annotators, and 90% had all the labels confirmed. The last column measures the agreement among annotators on labels that were not suggested by CARE, which was 53% when confirmed by 2 or more annotators. We expected this value to be reasonably large because the CARE patterns/lexicon were designed to generate a highly precise set of labels, rather than highly comprehensive ones. However, the value is still much smaller relative to the agreement rate for the CARE labels. On average, each annotation answer contained around 1.8 labels per post (with a standard deviation of 0.9). We note that 'None of the above' was chosen less than 0.2% of the time. Table C2 and Figure  C2  present annotator agreement statistics and label prevalence, respectively, broken down by class. Figure  C3  shows the Spearman correlation between each class and a hierarchical clustering. Table  2 : The rate of agreement between the annotators and the labels proposed by CARE. The first column specifies the number of annotators to be used for consensus. The rest of the columns shows for all posts, the average rate of intersection of the human labels with at least one CARE label, all CARE labels, and any label that is not a CARE label.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Error Analysis",
      "text": "Evaluating CARE in the previous sections (Figure  C2 , Table  3 ) revealed that the accuracy of CARE varies by class and in particular, is lower for amused and excited. To better understand if certain pattern or indicator matches are at fault here, we investigate the precision and recall at the pattern and lexicon level.\n\nRecall that instantiating a match for a comment involves choosing a (pattern, keyword) combination. Separating the lexicon from the patterns enables us to encode a large number of instantiated patterns parsimoniously, but some pair combinations provide a much weaker signal than others, particularly for the class amused (see Figure  H7 ). Hence, for future iterations of CARE, we have implemented a mechanism to exclude certain pattern and keyword combinations and a means for using different thresholds for each class.\n\nAlternatively, another mechanism for accomodating these class-wise discrepancies in performance is by tuning for each class an optimal threshold t (i.e., the number of matched comments we need to see in order to reliably predict a label). Figure  2  shows how the precision and recall of each class varies according to different threshold values. To achieve precision and recall greater than 0.7, a threshold of 1 actually seems viable for most classes, while for amused and excited a threshold of at least 3 is needed. In fact, for most of the classes, using thresholds larger than 3 has negligible impact on the precision score, but does reduce the recall.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Can We Leverage Emotion Classification?",
      "text": "Recall, steps 1 and 2 of Figure  1  uses the CARE patterns and lexicon to label the publisher affect of the comments. Conceivably, this could have been done instead by using a SOTA emotion classifier such as the GoEmotions classifier  (Demszky et al., 2020) , which is trained specifically to predict the publisher affect of Reddit comments. Here we show that our method for labeling the publisher affect of comments performs comparatively better. Let us define the method CARE G , a modified version of the CARE method where steps 1 and 2 are replaced with labels using the GoEmotions classifer. We apply CARE G to our human annotated dataset (Section 4.1) by first applying the GoEmotions classifier to all comments of the posts. These GoEmotion labels are then mapped to our taxonomy in Table  1  using the mapping defined in Table  3 , which is based on the grouping of emotions at the Ekman level used in  Demszky et al. (2020) . We then, as usual, aggregate and filter post labels according to a threshold t.\n\nCARE G (Table  F4 ) shows a relative decrease of 12.9% and 18.0% in the rate of annotator agreement with any and all labels, respectively, compared to that of CARE. These decreases hold even when partitioning on each individual class. The comparatively lower performance of CARE G is most likely due to the low F1-scores (<0.4) of the GoEmotions classifer for nearly half of the 28 classes, as reported in the original work  Demszky et al. (2020, Table 4) . It is also important to note that in addition to demonstrating higher precision, CARE patterns and lexicon are valuable because they do not require human annotated data, unlike GoEmotions.\n\nIt may, however, be useful to leverage multiple emotion detection approaches and Section F discusses a potential ensembling strategy for this.\n\nTo validate the mapping in Table  3 , we applied steps 1 and 2 of CARE to the GoEmotions dataset (see Section E), and computed the rate of agreement among the labels in our defined mapping. We find this rate of agreement to be high (87.3% overall). Note, we perform this equivalence at the publisher affect level, because as discussed before, the affective response and publisher affect are not always equivalent. In addition to prior work (dwi), Section D presents experiments that indicate that affective response and publisher affect labels intersect only 44% of the time. The average across all datapoints was 87.3%.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Predicting Affective Response For Posts Without Comments",
      "text": "In this section we describe CARE-BERT, a multilabel affective response classifier that is trained only on the post-level text and annotations in CARE db . Such a model is important in order to make predictions early in the life of the post and in cases where the comments may not match any CARE patterns or keywords. Note that the model is not given the comments text and is therefore not restricted to the CARE pattern semantics. In section 5.2, we describe how CARE-BERT can be further fine-tuned for related tasks like emotion detection.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Creating And Evaluating Care-Bert",
      "text": "We train CARE-BERT with the CARE labels in CARE db , using the pre-trained model bert-baseuncased in the Huggingface library  (Wolf et al., 2020) . We use a max length of 512 and we add a dropout layer with a rate of 0.3 and a dense layer to allow for multi-label classification. We used an Adam optimizer with a learning rate of 5e-5, a batch size of 16, and 5 epochs. We used a train/validation/test split of 80/10/10%. See Section I for other settings we explored.\n\nThe evaluation on the human-annotated set (held out from training) is shown in Table  4 . We use labels with support from all annotators as ground truth. The classes of lowest prevalence, such as scared, had the poorest results, while the more frequent classes (adoring, approving, saddened) had the highest results. To put these results in perspective, we use the mapping in Table  3  and compare with the numbers from  Demszky et al. (2020) . Note, the comparison is not for the same dataset-our results pertain to predicting on the post whereas GoEmotions predicts the comments. Still, CARE-BERT demonstrates a 35% improvement in the overall micro-averaged F1-score.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Care Vs. Care-Bert:",
      "text": "Compared to the human annotators and CARE, CARE-BERT is disadvantaged by not having access to the comments. We use human annotated set of CARE db and find that 0.89 of the CARE labels are also proposed by human annotators, while this value is 0.72 for CARE-BERT (Table  J6 ). In Table  J7  we display select examples that may illustrate reasons for this discrepancy. Firstly, one of the challenges that CARE-BERT faces is that there may not be sufficient context in the post alone. In the example \"Who is this LIRIK guy, and why does he have 50K subscribers\" it is challenging to predict that some people find the subject adorable without additional context. Relatedly, the conversation that the post initiates can be challenging to foresee. The last example reads \"AskReddit: Imagine the last thing you ate has been made illegal. What would that be?\" In some cases, commenters ate something they didn't like and are therefore content with the premise. In other cases, commenters ate something they very much enjoy and are saddened by the hypothetical. Our results show that this is not particular to 'AskReddit' posts, and given these challenges, it is reasonable that the CARE method provides more reliable labels.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Transfer Learning To Emotion Detection",
      "text": "We now demonstrate that CARE-BERT is also useful for pre-training of another related task in a setting with limited annotated data. We consider transfer learning to the ISEAR Dataset  (Scherer and Wallbott, 1994) , which is a collection of 7666 statements from a diverse set of 3000 individuals labeled according to six categories (anger, disgust, fear, guilt, joy, sadness, and shame). The labels pertain to the publisher affect and not affective response, as considered in this work. Our experiment explores transfer learning to predict the labels in the ISEAR dataset using an additional drop-out layer of 0.3 and a dense layer.\n\nOur experiments follow closely to that of Demszky et al. (  2020 ) and uses different training set sizes (500, 1000, 2000, 4000, and 6000) for 10 different train-test splits. We plot the average and standard deviation in the F1-scores across these 10 splits in Figure  3 . We compare four different fine-tuning setups: the first two are trained using CARE-BERT and then fine-tuned on the benchmark dataset, one with no parameter freezing (no_freeze), and one with all layers but the last two frozen (freeze). The third setup is similar to CARE-BERT (no_freeze) but is trained on GoEmotions rather than CARE db . The last setup is the bert-base-uncased model trained only on ISEAR, where all setups use the same architecture and hyperparameters as discussed in Section 5.\n\nOur values differ slightly from that cited in Demszky et al. (  2020 ) due to the small differences in architecture and hyperparameters. However, the overall results corroborate that of  Demszky et al. (2020)  in that models with additional pre-training perform better than the baseline (no additional pretraining) for limited sample sizes. From Figure  3 , it is apparent that CARE-BERT and the model built from GoEmotions perform essentially on par in these transfer learning experiments in spite of the fact that CARE-BERT does not utilize human annotations. It is also worth noting that GoEmotions and the ISEAR dataset address the same task (emotion detection) while CARE-BERT predicts affective response. The comparable performance of CARE-BERT with the GoEmotions models demonstrates the utility of CARE-BERT for other tasks with limited data and the promise of CARE as a means of reliable unsupervised labeling.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Conclusion",
      "text": "We described a method for extracting training data for models that predicts the affective response to a post on social media. CARE is an efficient, accurate, and scalable way of collecting unsupervised labels and can be extended to new classes. Using CARE, we created CARE db , a large dataset which can be used for affective response detection and other related tasks, as demonstrated by the competitive performance of CARE-BERT to similar BERTbased models in emotion detection. We release the dataset and models in the hopes that this will unlock future research.\n\nIn particular, there are two main cases in which CARE can be improved upon: (1) when there does not exist a set of common phrases that are indicative of an affect, and (2) when an indicator maps to multiple affects. In the latter case, there is still partial information that can be gleaned from the labels. In addition to developing methods for the above cases, future work also includes incorporating emojis, negations, and punctuation, and extending to new classes. Finally, we also plan to investigate the use of CARE for predicting the affective response to images as well as multi-modal content such as memes.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A Broader Impact",
      "text": "Any work that touches upon emotion recognition or recognizing affective response needs to ensure that it is sensitive to the various ways of expressing affect in different cultures and individuals. Clearly, applying the ideas described in this paper in a production setting would have to first test for cultural biases. To make \"broad assumptions about emotional universalism [would be] not just unwise, but actively deleterious\" to the general community  (Stark and Hoey, 2021) . We also note that emotion recognition methods belong to a taxonomy of conceptual models for emotion (such as that of  Stark and Hoey (2021)  and these \"paradigms for human emotions [...] should [not] be taken naively ground truth.\"\n\nBefore being put in production, the method would also need to be re-evaluated when applied to a new domain to ensure reliable performance in order to prevent unintended consequences. Additionally, our work in detecting affective response is intended for understanding content, not the emotional state of individuals. This work is intended to identify or recommend content, which aligns with the user's preferences. This work should not be used for ill-intended purposes such as purposefully recommending particular content to manipulate a user's perception or preferences.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "B Details On Expanding Care",
      "text": "The six CARE patterns that were used to seed the system are the following: (The symbol * (resp. + ) indicates that zero (resp. one) or more matches are required.) Example: This is so amazing!   Figure  C1  shows the interface used for crowdsourcing human annotations for evaluating CARE patterns. To better understand annotation results for each class, we present Table  C2 , which shows annotator agreement statistics broken down by class. We also computed Fleiss' kappa for each class, where a value between 0.41-0.60 is generally considered moderate agreement and a value between 0.61-0.80 is substantial agreement. As can be seen, classes such as adoring have high average annotator support and Fleiss' kappa while others like amused have low average annotator support and Fleiss' kappa, an observation that aligns with the findings in Section 4.2. Table  C2 : The percent of CARE-labeled examples (maximum of 100) with agreement from at least one labeler by class and of those examples, the average number of annotator agreement (maximum of 3). The third column shows the Fleiss' kappa, which was computed for class a based on the presence and absence of label a by each annotator for a given post. The bottom row is the average over all classes.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "C Annotation Details",
      "text": "Figure  C2 : Prevalence of class labels according to annotations from AMT on which at least two annotators agree upon (blue) and according to CARE (orange). The prevalence of approving was much higher from AMT, likely due to a large perceived overlap in the definitions of approving and other classes such excited.  The GoEmotions dataset and classifier target the publisher affect (of comments), whereas CARE-BERT and CARE target the affective response (of posts). In an effort to study the correlation between affective response and publisher affect, we compare the following sets of labels: 1) human annotations of GoEmotion and the predicted affective responses using CARE-BERT applied to GoEmotions and 2) CARE labels for posts in CARE db and the predicted publisher affects using the GoEmotions classifier applied to CARE db . Specifically, for every annotated label (i.e., not from a classifier) we count the percentage of the time where there is intersection with the set of predicted labels (i.e., from a classifier). The results of these experiments are shown in Table  D3 , broken down according to the class of the annotated label. Overall, the percentage of affective response and publisher affect label agreement (44%) is moderate but seems to indicate that the affective response detection and emotion detection are not necessarily the same problem, in particular for scared and approving. The classes approving, excited, and angered have a large variance between the two datasets, where the first (Table D3, second column) uses comments and the second (Table  D3 , third column) uses posts. This could be due to the classification errors (either by GoEmotions or by CARE-BERT) or due to the type of the text (comment or post). More research and data collection is needed to understand the relationship between affective response and publisher affect, which as demonstrated by Figure  D4  are not necessarily equivalent. Table  D3 : Rate of intersection between affective response and publisher affect labels. The first column denotes the class. The second column denotes the percent of the time an annotated label in GoEmotions exists in the set of predicted labels by CARE-BERT when applied to the GoEmotions dataset. The third column denotes the percent of the time an annotated label in CARE-BERT exists in the set of predicted labels by the GoEmotions classifier when applied to CARE db . The last column is the row-wise average.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "E Using Care Patterns/Lexicon To Predict Publisher Affect In Goemotions",
      "text": "The GoEmotions dataset  (Demszky et al., 2020 ) is a collection of 58k Reddit comments labeled according to the publisher affect from a taxonomy of 28 emotions. There exists a natural mapping from 6 of our classes to those of GoEmotions (the exception being adoring) based on the definitions alone. Hence, applying CARE patterns/lexicon to the GoEmotions dataset presents another way of validating the quality of steps 1 and 2 of CARE.\n\nThe number of examples in GoEmotions with labels belonging to these 6 classes was 21.0k and the number of comments that were labeled by CARE patterns/lexicon was 1259. Table  3  compares the human annotations in the GoEmotions dataset with the labels that CARE patterns/lexicon assigned to the comments and shows that they have a high degree of agreement.\n\nWhile the low recall is certainly a limitation of CARE patterns and lexicon when applied to a specific small dataset, we emphasize that the primary intention of CARE patterns is to generate a labeled dataset in an unsupervised manner so one can start training classifiers for that affective response. Given the abundance of freely available unlabeled data (e.g., on Reddit, Twitter), recall is not a problem in practice. In the next section and in Section 4.3, however, we discuss how existing emotion classifiers, such as the GoEmotions classifier  (Demszky et al., 2020)  can also be leveraged in the CARE method.\n\nF CARE and CARE G evaluation details CARE G refers to the CARE method where steps 1 and 2 of Figure  1  use the GoEmotions classifier instead of CARE patterns. To evaluate how CARE and CARE G compares, we use the same humanlabeled dataset described in Section 4.1 and applied the GoEmotions classifier to all the comments belonging to these posts (72k comments). We then mapped the predicted GoEmotion labels to CARE pattern labels using the mapping in Table  3 . GoEmotion and CARE labels not in the mapping are excluded from this analysis.\n\nThe same metrics for ≥ 2 annotator agreement in Table  2  are shown in Table  F4  for multiple thresholds and for all classes, excluding adoring. CARE labels consistently demonstrate higher agreement with human annotations than those of CARE G . The last row of Table  F4  shows results for an ensem-bling approach where steps 1 and 2 use labels from both CARE patterns in addition to the labels from the GoEmotions classifier, where the former uses t = 5 and the latter uses t = 4 in step 3 (optimal values for each approach, respectively). This ensembling approach does reasonably well and can be used to include classes in the GoEmotions taxonomy that do not exist in the taxonomy of Table  1 . Given other emotion classifiers, one could potentially include those as well. Table  F4 : The rate of intersection between labels agreed upon by at least two annotators and the labels proposed by CARE G . The first column indicates the threshold t used in CARE G . Using annotations agreed upon by at least two annotators, the rest of the columns show the rate of agreement with at least one predicted label, all predicted labels, and any human-annotated label that was not predicted. The row labeled 'max' refers to choosing the comment-level label with the highest frequency for each post. For context, the results for CARE using t = 5 are shown in the penultimate row. The last row presents results from combining the CARE pattern labels and the GoEmotion labels using t = 4.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "G Multi-Dimensional Scaling Pairwise Plots",
      "text": "We visualize the degree of overlap between the sentence embeddings (using Sentence-Bert  (Reimers and Gurevych, 2019 )) of 100 comments in CARE db for each class. We then use multi-dimensional scaling or MDS  (Cox and Cox, 2008)  to map the embeddings to the same twodimensional space using euclidean distance as the similarity metric, as shown in Figure  G5  and Figure G6. Note that the MDS process does not use the class labels. As can be seen, there is substantial overlap between amused and other classes as well as between excited and approving. Given that the average number of human annotations per post was 1.8 (Section 4.1), it is likely that a portion of this overlap can attributed to the multi-label nature of the problem as well as the moderate correlation between certain classes such as excited and approving (Figure  C3 ). See Figure  G6  for plots of multi-dimensional scaling for every pair of classes, as referenced in Section 4.2.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "H Pattern Match Analysis",
      "text": "To investigate why higher thresholds would be needed for certain classes, we analyze the CARE patterns and lexicon at the class level.\n\nLet us define a match as a tuple containing the pattern name and the word or phrase which maps the comment to an affect according to the CARE lexicon. We could also consider exaggerators in our analysis but here we assume a negligible effect on differentiating reliability. We previously assumed that each instantiated match should have the same weight of 1, but this may not be appropriate considering that some patterns or words may be more reliable.\n\nAs can be seen in Figure  H7 , there are some cases in which the keyword in general seems to have a high false positive rate (e.g., happy) and in other cases it appears the erroneous combination of a particular pattern and keyword can lead to high false positive rates. For example, while the match '(so very, funny)' has a low false positive rate of 0.2, '(I, funny)' has a much higher false positive rate of 0.57, which intuitively makes since 'I'm funny' does not indicate being amused. We also investigated whether individual patterns are prone to higher false positive rates, which does not seem to be the case. For future iterations of CARE, one could also use the true positive rate as the weight of a match to obtain a weighted sum when aggregating over comments to label a post.\n\nFigure  H7 : Scatter plot of the total frequency of a match versus its false positive rate. Ground truth labels used here are those from AMT and agreed upon by at least 2 annotators. For clarity, a match is shown only if its total count was 10 or more and if it belongs to one of the three classes (adoring, amused, and excited). Only those which contain the keywords 'sweet' (adoring), 'funny' (amused), and 'happy' (excited) are labeled. We began with the hyper-parameter settings in  Demszky et al. (2020)  and explored other hyperparameter settings (batch sizes  [16, 32, 64] ,",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "I Modeling Details",
      "text": "max length  [64, 256, 512] , drop out rate [0.3, 0.5, 0.7], epochs [2-10]) but found minimal improvements in the F1-score, as computed by the scikit-learn package in python. Running this on two Tesla P100-SXM2-16GB GPUs took roughly 19 hours. We also experimented with higher thresholds for the parameter t (see Section 3.2) but saw marginal improvements, if any. We developed two versions of CARE-BERT: one using the classes in Table  1 , and a simpler one using only the classes POSITIVE, and NEGATIVE. The first four rows in Table  1  are considered positive while the last three are negative, the results of which are featured in Table  I5 . Naturally, the two-class model that blurs the differences between classes with the same valence has higher results. Table  J6 : Percentage of agreement between annotation schemes. Each entry corresponds to the percentage of all labels the annotation scheme along the row agrees with the annotation scheme along the column.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "J Modeling Analysis",
      "text": "Algorithm 1: Algorithm for producing candidates for new CARE patterns and indicators in the CARE lexicon. Algorithm uses three hyperparameters t (the minimum number of comments to label a post), f _lexicon (the minimum frequency of a ngram to be added to the lexicon), and f _pattern (the minimum frequency of an ngram to be a candidate pattern) which was set to 5, 1000, and 100, respectively. The resulting list of candidate patterns needs to be manually converted into a regular expression matching the structure outlined in Section 3.",
      "page_start": 17,
      "page_end": 17
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of the CARE Method (pseudo-",
      "page": 1
    },
    {
      "caption": "Figure 1: uses the CARE",
      "page": 6
    },
    {
      "caption": "Figure 2: Precision versus recall of each class using",
      "page": 6
    },
    {
      "caption": "Figure 3: We compare four differ-",
      "page": 8
    },
    {
      "caption": "Figure 3: The F1-score of each model using varying",
      "page": 8
    },
    {
      "caption": "Figure 1: The n-grams above the middle line are added",
      "page": 12
    },
    {
      "caption": "Figure 1: use the GoEmotions classiﬁer",
      "page": 14
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Deﬁnition of affective responses (AR), examples of comments which would map to each affective re-",
      "page": 5
    },
    {
      "caption": "Table 1: shows the break-",
      "page": 5
    },
    {
      "caption": "Table 2: The rate of agreement between the annotators",
      "page": 6
    },
    {
      "caption": "Table 3: ) revealed that the accuracy of",
      "page": 6
    },
    {
      "caption": "Table 1: using the mapping deﬁned in",
      "page": 6
    },
    {
      "caption": "Table 3: , which is based on the grouping of emo-",
      "page": 6
    },
    {
      "caption": "Table 4: ). It is also important to note that in addition",
      "page": 6
    },
    {
      "caption": "Table 3: , we applied",
      "page": 7
    },
    {
      "caption": "Table 3: CARE to GoEmotions mapping. The last col-",
      "page": 7
    },
    {
      "caption": "Table 4: Precision (P), recall (R), and F1 of CARE-",
      "page": 8
    },
    {
      "caption": "Table 1: were shown to them.",
      "page": 12
    },
    {
      "caption": "Table 3: compares the",
      "page": 14
    },
    {
      "caption": "Table 2: are shown in Table F4 for multiple thresh-",
      "page": 14
    },
    {
      "caption": "Table 1: Given other emotion classiﬁers, one could poten-",
      "page": 15
    },
    {
      "caption": "Table 1: , and a simpler one",
      "page": 16
    },
    {
      "caption": "Table 1: are considered pos-",
      "page": 16
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Text-based emotion detection: Advances, challenges, and opportunities",
      "authors": [
        "Francisca Adoma",
        "Chen Wenyu",
        "Henry Nunoo-Mensah"
      ],
      "year": "2020",
      "venue": "Engineering Reports",
      "doi": "10.1002/eng2.12189"
    },
    {
      "citation_id": "2",
      "title": "Snowball: Extracting relations from large plain-text collections",
      "authors": [
        "Eugene Agichtein",
        "Luis Gravano"
      ],
      "year": "2000",
      "venue": "Proceedings of the fifth ACM conference on Digital libraries",
      "doi": "10.1145/336597.336644"
    },
    {
      "citation_id": "3",
      "title": "Choseamobile: A web-based recommendation system for mobile phone products",
      "authors": [
        "Sheeraz Akram",
        "Shariq Hussain",
        "Ibrahima Kalil Toure",
        "Shunkun Yang",
        "Humza Jalal"
      ],
      "year": "2020",
      "venue": "Journal of Internet Technology"
    },
    {
      "citation_id": "4",
      "title": "Knowledge and Information Systems, 62. Sergio Angelastro, B Carolis, and Stefano Ferilli",
      "authors": [
        "Nourah Alswaidan",
        "Mohamed Menai"
      ],
      "year": "2019",
      "venue": "Knowledge and Information Systems, 62. Sergio Angelastro, B Carolis, and Stefano Ferilli",
      "doi": "10.1007/s10115-020-01449-0"
    },
    {
      "citation_id": "5",
      "title": "Using sentiment text analysis of user reviews in social media for e-tourism mobile recommender systems",
      "authors": [
        "Olga Artemenko",
        "Volodymyr Pasichnyk",
        "Nataliia Kunanets",
        "Khrystyna Shunevych"
      ],
      "year": "2020",
      "venue": "In COLINS"
    },
    {
      "citation_id": "6",
      "title": "Sentencelevel emotion detection framework using rule-based classification",
      "authors": [
        "Muhammad Dr",
        "Aurangzeb Asghar",
        "Afsana Khan",
        "Fazal Bibi",
        "Hussain Kundi",
        "Ahmad"
      ],
      "year": "2017",
      "venue": "Cognitive Computation",
      "doi": "10.1007/s12559-017-9503-3"
    },
    {
      "citation_id": "7",
      "title": "Mining social emotions from affective text",
      "authors": [
        "Shenghua Bao",
        "Shengliang Xu",
        "Li Zhang",
        "Rong Yan",
        "Zhong Su",
        "Dingyi Han",
        "Yong Yu"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "doi": "10.1109/TKDE.2011.188"
    },
    {
      "citation_id": "8",
      "title": "",
      "authors": [
        "Francesco Barbieri",
        "Jose Camacho-Collados"
      ],
      "venue": ""
    },
    {
      "citation_id": "9",
      "title": "DataStories at SemEval-2017 task 4: Deep LSTM with attention for message-level and topic-based sentiment analysis",
      "authors": [
        "Francesco Ronzano",
        "Luis Espinosa-Anke",
        "Miguel Ballesteros",
        "Valerio Basile",
        "Viviana Patti",
        "Horacio Saggion"
      ],
      "year": "2017",
      "venue": "Proceedings of The 12th International Workshop on Semantic Evaluation",
      "doi": "10.18653/v1/S17-2126"
    },
    {
      "citation_id": "10",
      "title": "Predicting viewer affective comments based on image content in social media",
      "authors": [
        "Yan-Ying Chen",
        "Tao Chen",
        "Winston Hsu",
        "Hong-Yuan Mark Liao",
        "Shih-Fu Chang"
      ],
      "year": "2014",
      "venue": "proceedings of international conference on multimedia retrieval",
      "doi": "10.1145/2578726.2578756"
    },
    {
      "citation_id": "11",
      "title": "Predicting emotional reaction in social networks",
      "authors": [
        "Jérémie Clos",
        "Anil Bandhakavi",
        "Nirmalie Wiratunga",
        "Guillaume Cabanac"
      ],
      "year": "2017",
      "venue": "European Conference on Information Retrieval",
      "doi": "10.1007/978-3-319-56608-5_44"
    },
    {
      "citation_id": "12",
      "title": "Multidimensional scaling",
      "authors": [
        "A Michael",
        "Trevor Cox",
        "Cox"
      ],
      "year": "2008",
      "venue": "Handbook of data visualization"
    },
    {
      "citation_id": "13",
      "title": "GoEmotions: A dataset of fine-grained emotions",
      "authors": [
        "Dorottya Demszky",
        "Dana Movshovitz-Attias",
        "Jeongwoo Ko",
        "Alan Cowen",
        "Gaurav Nemade",
        "Sujith Ravi"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2020.acl-main.372"
    },
    {
      "citation_id": "14",
      "title": "SENTI-WORDNET: A publicly available lexical resource for opinion mining",
      "authors": [
        "Andrea Esuli",
        "Fabrizio Sebastiani"
      ],
      "year": "2006",
      "venue": "Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC'06)"
    },
    {
      "citation_id": "15",
      "title": "Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm",
      "authors": [
        "Bjarke Felbo",
        "Alan Mislove",
        "Anders Søgaard",
        "Iyad Rahwan",
        "Sune Lehmann"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/D17-1169"
    },
    {
      "citation_id": "16",
      "title": "Jointly learning to detect emotions and predict facebook reactions",
      "authors": [
        "Lisa Graziani",
        "Stefano Melacci",
        "Marco Gori"
      ],
      "year": "2019",
      "venue": "International Conference on Artificial Neural Networks",
      "doi": "10.1007/978-3-030-30490-4_16"
    },
    {
      "citation_id": "17",
      "title": "Neuromodulator and emotion biomarker for stress induced mental disorders",
      "authors": [
        "Simeng Gu",
        "Wei Wang",
        "Fushun Wang",
        "Jason Huang"
      ],
      "year": "2016",
      "venue": "Neuromodulator and emotion biomarker for stress induced mental disorders"
    },
    {
      "citation_id": "18",
      "title": "Using hashtags as labels for supervised learning of emotions in twitter messages",
      "authors": [
        "Maryam Hasan",
        "Emmanuel Agu",
        "Elke Rundensteiner"
      ],
      "year": "2014",
      "venue": "ACM SIGKDD workshop on health informatics"
    },
    {
      "citation_id": "19",
      "title": "Automatic acquisition of hyponyms from large text corpora",
      "authors": [
        "Marti Hearst"
      ],
      "year": "1992",
      "venue": "The 15th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "20",
      "title": "2019a. ANA at SemEval-2019 task 3: Contextual emotion detection in conversations through hierarchical LSTMs and BERT",
      "authors": [
        "Chenyang Huang",
        "Amine Trabelsi",
        "Osmar Zaïane"
      ],
      "venue": "Proceedings of the 13th International Workshop on Semantic Evaluation",
      "doi": "10.18653/v1/S19-2006"
    },
    {
      "citation_id": "21",
      "title": "Emotionx-idea: Emotion bert-an affectional model for conversation",
      "authors": [
        "Yen-Hao Huang",
        "Ssu-Rui Lee",
        "Mau-Yun Ma",
        "Yi-Hsin Chen",
        "Ya-Wen Yu",
        "Yi-Shin Chen"
      ],
      "year": "2019",
      "venue": "Emotionx-idea: Emotion bert-an affectional model for conversation",
      "arxiv": "arXiv:1908.06264"
    },
    {
      "citation_id": "22",
      "title": "Dynamic facial expressions of emotion transmit an evolving hierarchy of signals over time",
      "authors": [
        "Rachael Jack",
        "Oliver Garrod",
        "Philippe Schyns"
      ],
      "year": "2014",
      "venue": "Current biology"
    },
    {
      "citation_id": "23",
      "title": "Bootstrapping for text learning tasks",
      "authors": [
        "Rosie Jones",
        "Andrew Mccallum",
        "Kamal Nigam",
        "Ellen Riloff"
      ],
      "year": "1999",
      "venue": "IJCAI-99 Workshop on Text Mining: Foundations, Techniques and Applications"
    },
    {
      "citation_id": "24",
      "title": "Predicting viewer perceived emotions in animated gifs",
      "authors": [
        "Brendan Jou",
        "Subhabrata Bhattacharya",
        "Shih-Fu Chang"
      ],
      "year": "2014",
      "venue": "Proceedings of the 22nd ACM international conference on Multimedia",
      "doi": "10.1145/2647868.2656408"
    },
    {
      "citation_id": "25",
      "title": "SWAT-MP:the SemEval-2007 systems for task 5 and task 14",
      "authors": [
        "Phil Katz",
        "Matt Singleton",
        "Richard Wicentowski"
      ],
      "year": "2007",
      "venue": "Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007)"
    },
    {
      "citation_id": "26",
      "title": "Social emotion mining techniques for facebook posts reaction prediction",
      "authors": [
        "Florian Krebs",
        "Bruno Lubascher",
        "Tobias Moers",
        "Pieter Schaap",
        "Gerasimos Spanakis"
      ],
      "year": "2017",
      "venue": "Social emotion mining techniques for facebook posts reaction prediction",
      "arxiv": "arXiv:1712.03249"
    },
    {
      "citation_id": "27",
      "title": "Towards building a social emotion detection system for online news",
      "authors": [
        "Jingsheng Lei",
        "Yanghui Rao",
        "Qing Li",
        "Xiaojun Quan",
        "Liu Wenyin"
      ],
      "year": "2014",
      "venue": "Special Section: Innovative Methods and Algorithms for Advanced Data-Intensive Computing Special Section: Semantics, Intelligent processing and services for big data Special Section: Advances in Data-Intensive Modelling and Simulation Special Section: Hybrid Intelligence for Growing Internet and its Applications",
      "doi": "10.1016/j.future.2013.09.024"
    },
    {
      "citation_id": "28",
      "title": "Emotion classification of online news articles from the reader's perspective",
      "authors": [
        "Kevin Hsin-Yih Lin",
        "Changhua Yang",
        "Hsin-Hsi Chen"
      ],
      "year": "2008",
      "venue": "2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology",
      "doi": "10.1109/WIIAT.2008.197"
    },
    {
      "citation_id": "29",
      "title": "Emotion estimation and reasoning based on affective textual interaction",
      "authors": [
        "Chunling Ma",
        "Helmut Prendinger",
        "Mitsuru Ishizuka"
      ],
      "year": "2005",
      "venue": "Proceedings of the First International Conference on Affective Computing and Intelligent Interaction, ACII'05",
      "doi": "10.1007/11573548_80"
    },
    {
      "citation_id": "30",
      "title": "Using hashtags to capture fine emotion categories from tweets",
      "authors": [
        "M Saif",
        "Svetlana Mohammad",
        "Kiritchenko"
      ],
      "year": "2015",
      "venue": "Computational Intelligence",
      "doi": "10.1111/coin.12024"
    },
    {
      "citation_id": "31",
      "title": "Nrc-canada: Building the stateof-the-art in sentiment analysis of tweets",
      "authors": [
        "M Saif",
        "Svetlana Mohammad",
        "Xiaodan Kiritchenko",
        "Zhu"
      ],
      "year": "2013",
      "venue": "Nrc-canada: Building the stateof-the-art in sentiment analysis of tweets",
      "arxiv": "arXiv:1308.6242"
    },
    {
      "citation_id": "32",
      "title": "Chapter 1 -a general psychoevolutionary theory of emotion",
      "authors": [
        "Robert Plutchik"
      ],
      "year": "1980",
      "venue": "Theories of Emotion",
      "doi": "10.1016/B978-0-12-558701-3.50007-7"
    },
    {
      "citation_id": "33",
      "title": "Distant supervision for emotion detection using Facebook reactions",
      "authors": [
        "Chris Pool",
        "Malvina Nissim"
      ],
      "year": "2016",
      "venue": "Proceedings of the Workshop on Computational Modeling of People's Opinions, Personality, and Emotions in Social Media (PEOPLES)"
    },
    {
      "citation_id": "34",
      "title": "Emosenticspace: A novel framework for affective common-sense reasoning. Knowledge-Based Systems",
      "authors": [
        "Soujanya Poria",
        "Alexander Gelbukh",
        "Erik Cambria",
        "Amir Hussain",
        "Guang-Bin Huang"
      ],
      "year": "2014",
      "venue": "Emosenticspace: A novel framework for affective common-sense reasoning. Knowledge-Based Systems",
      "doi": "10.1016/j.knosys.2014.06.011"
    },
    {
      "citation_id": "35",
      "title": "Aseds: Towards automatic social emotion detection system using facebook reactions",
      "authors": [
        "Bin Tareaf Raad",
        "Berger Philipp",
        "Hennig Patrick",
        "Meinel Christoph"
      ],
      "year": "2018",
      "venue": "2018 IEEE 20th International Conference on High Performance Computing and Communications; IEEE 16th International Conference on Smart City; IEEE 4th International Conference on Data Science and Systems",
      "doi": "10.1109/HPCC/SmartCity/DSS.2018.00143"
    },
    {
      "citation_id": "36",
      "title": "Affective topic model for social emotion detection",
      "authors": [
        "Yanghui Rao",
        "Qing Li",
        "Liu Wenyin",
        "Qingyuan Wu",
        "Xiaojun Quan"
      ],
      "year": "2014",
      "venue": "Special Issue on \"Affective Neural Networks and Cognitive Learning Systems for Big Data Analysis",
      "doi": "10.1016/j.neunet.2014.05.007"
    },
    {
      "citation_id": "37",
      "title": "Sentencebert: Sentence embeddings using siamese bertnetworks",
      "authors": [
        "Nils Reimers",
        "Iryna Gurevych"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "38",
      "title": "Music recommendation system based on user's sentiments extracted from social networks",
      "authors": [
        "Renata L Rosa",
        "Demsteneso Rodriguez",
        "Graça Bressan"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Consumer Electronics"
    },
    {
      "citation_id": "39",
      "title": "A knowledge-based recommendation system that includes sentiment analysis and deep learning",
      "authors": [
        "Renata Lopes",
        "Gisele Schwartz",
        "Wilson Vicente Ruggiero",
        "Demóstenes Zegarra"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Industrial Informatics"
    },
    {
      "citation_id": "40",
      "title": "Evidence for universality and cultural variation of differential emotion response patterning",
      "authors": [
        "Klaus Scherer",
        "Harald Wallbott"
      ],
      "year": "1994",
      "venue": "Journal of personality and social psychology",
      "doi": "10.1037//0022-3514.66.2.310"
    },
    {
      "citation_id": "41",
      "title": "Dystemo: Distant supervision method for multi-category emotion recognition in tweets",
      "authors": [
        "Valentina Sintsova",
        "Pearl Pu"
      ],
      "year": "2016",
      "venue": "ACM Trans. Intell. Syst. Technol",
      "doi": "10.1145/2912147"
    },
    {
      "citation_id": "42",
      "title": "Depeche mood: a lexicon for emotion analysis from crowd annotated news",
      "authors": [
        "Jacopo Staiano",
        "Marco Guerini"
      ],
      "year": "2014",
      "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.3115/v1/P14-2070"
    },
    {
      "citation_id": "43",
      "title": "ethics of emotion in artificial intelligence systems",
      "authors": [
        "Luke Stark",
        "Jesse Hoey"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",
      "doi": "10.1145/3442188.3445939"
    },
    {
      "citation_id": "44",
      "title": "Semeval-2007 task 14: Affective text. Association for Computational Linguistics",
      "authors": [
        "Carlo Strapparava",
        "Rada Mihalcea"
      ],
      "year": "2007",
      "venue": "Semeval-2007 task 14: Affective text. Association for Computational Linguistics"
    },
    {
      "citation_id": "45",
      "title": "WordNet affect: an affective extension of Word-Net",
      "authors": [
        "Carlo Strapparava",
        "Alessandro Valitutti"
      ],
      "year": "2004",
      "venue": "Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC'04)"
    },
    {
      "citation_id": "46",
      "title": "Distant supervision for emotion classification with discrete binary values",
      "authors": [
        "Jared Suttles",
        "Nancy Ide"
      ],
      "year": "2013",
      "venue": "International Conference on Intelligent Text Processing and Computational Linguistics",
      "doi": "10.1007/978-3-642-37256-8_11"
    },
    {
      "citation_id": "47",
      "title": "Context based emotion detection from text input",
      "authors": [
        "Jianhua Tao"
      ],
      "year": "2004",
      "venue": "Eighth International Conference on Spoken Language Processing"
    },
    {
      "citation_id": "48",
      "title": "Clown on black background. Shutterstock",
      "authors": [
        "Luis Tapia"
      ],
      "venue": "Clown on black background. Shutterstock"
    },
    {
      "citation_id": "49",
      "title": "A research tool for user preferences elicitation with facial expressions",
      "authors": [
        "Marko Tkalčič",
        "Nima Maleki",
        "Matevž Pesek",
        "Mehdi Elahi",
        "Francesco Ricci",
        "Matija Marolt"
      ],
      "year": "2017",
      "venue": "Proceedings of the eleventh acm conference on recommender systems"
    },
    {
      "citation_id": "50",
      "title": "Prediction of music pairwise preferences from facial expressions",
      "authors": [
        "Marko Tkalčič",
        "Nima Maleki",
        "Matevž Pesek",
        "Mehdi Elahi",
        "Francesco Ricci",
        "Matija Marolt"
      ],
      "year": "2019",
      "venue": "Proceedings of the 24th International Conference on Intelligent User Interfaces"
    },
    {
      "citation_id": "51",
      "title": "Drosophila trpa channel painless inhibits male-male courtship behavior through modulating olfactory sensation",
      "authors": [
        "Kaiyu Wang",
        "Yanmeng Guo",
        "Fei Wang",
        "Zuoren Wang"
      ],
      "year": "2011",
      "venue": "PLoS One"
    },
    {
      "citation_id": "52",
      "title": "Harnessing twitter \"big data\" for automatic emotion identification",
      "authors": [
        "Wenbo Wang",
        "Lu Chen",
        "Krishnaprasad Thirunarayan",
        "Amit P Sheth"
      ],
      "year": "2012",
      "venue": "2012 International Conference on Privacy, Security, Risk and Trust and 2012 International Confernece on Social Computing",
      "doi": "10.1109/SocialCom-PASSAT.2012.119"
    },
    {
      "citation_id": "53",
      "title": "Transformers: State-of-the-art natural language processing",
      "authors": [
        "Thomas Wolf",
        "Lysandre Debut",
        "Victor Sanh",
        "Julien Chaumond",
        "Clement Delangue",
        "Anthony Moi",
        "Pierric Cistac",
        "Tim Rault",
        "Remi Louf",
        "Morgan Funtowicz",
        "Joe Davison",
        "Sam Shleifer",
        "Clara Patrick Von Platen",
        "Yacine Ma",
        "Julien Jernite",
        "Canwen Plu",
        "Teven Xu",
        "Sylvain Le Scao",
        "Mariama Gugger",
        "Quentin Drame",
        "Alexander Lhoest",
        "Rush"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
      "doi": "10.18653/v1/2020.emnlp-demos.6"
    },
    {
      "citation_id": "54",
      "title": "Safety needs mediate stressful events induced mental disorders",
      "authors": [
        "Zheng Zheng",
        "Simeng Gu",
        "Yu Lei",
        "Shanshan Lu",
        "Wei Wang",
        "Yang Li",
        "Fushun Wang"
      ],
      "year": "2016",
      "venue": "Neural plasticity"
    }
  ]
}