{
  "paper_id": "2211.11049v2",
  "title": "Explaining (Sarcastic) Utterances To Enhance Affect Understanding In Multimodal Dialogues",
  "published": "2022-11-20T18:05:43Z",
  "authors": [
    "Shivani Kumar",
    "Ishani Mondal",
    "Md Shad Akhtar",
    "Tanmoy Chakraborty"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Conversations emerge as the primary media for exchanging ideas and conceptions. From the listener's perspective, identifying various affective qualities, such as sarcasm, humour, and emotions, is paramount for comprehending the true connotation of the emitted utterance. However, one of the major hurdles faced in learning these affect dimensions is the presence of figurative language viz. irony, metaphor, or sarcasm. We hypothesize that any detection system constituting the exhaustive and explicit presentation of the emitted utterance would improve the overall comprehension of the dialogue. To this end, we explore the task of Sarcasm Explanation in Dialogues that aims to unfold the hidden irony behind sarcastic utterances. We propose MOSES, a deep neural network, which takes a multimodal (sarcastic) dialogue instance as an input and generates a natural language sentence as its explanation. Subsequently, we leverage the generated explanation for various natural language understanding tasks in a conversational dialogue setup, such as sarcasm detection, humour identification, and emotion recognition. Our evaluation shows that MOSES outperforms the state-of-the-art system for SED by an average of ∼ 2% on different evaluation metrics, such as ROUGE, BLEU, and METEOR. Further, we observe that leveraging the generated explanation advances three downstream tasks for affect classification -an average improvement of ∼ 14% F1-score in the sarcasm detection task and and ∼ 2% in the humour identification and emotion recognition task. We also perform extensive analyses to assess the quality of the results.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Expressing oneself eloquently to our conversation partner requires employing multiple affective components such as emotion, humour, and sarcasm. All such attributes interact with each another to present a concrete definition of an uttered statement (Roberts and  Kreuz 1994) . While affects such as emotion and humour deem easier to comprehend, sarcasm, on the other hand, is a challenging aspect to comprehend  (Olkoniemi, Ranta, and Kaakinen 2016) . Consequently, it becomes imperative for NLP systems to capture and understand sarcasm in its entirety. Sarcasm Explanation in Dialogues (SED) is a new task proposed recently in this direction  (Kumar et al. 2022) . In this work, we scour the Figure  1 : [Best viewed in color] Effect of multimodality on sarcasm. We do not show all possible combinations for brevity.\n\ntask of SED and considerably improve the performance by proposing MOSES, a deep neural network which leverages the peculiarities of the benchmark dataset, WITS.\n\nCongruent to how humans make decisions after compiling data from all their available senses, (predominantly optical and auditory) multimodal analysis helps the machine mimic this behaviour. Particularly in the case of sarcasm interpretation, multimodal information provides us with essential indications to interpret irony. Figure  1  shows cases where multimodal knowledge such as audio and video can assist in comprehending sarcasm. The identical text, \"This party is so fun\", can result in diverse classes when integrated with different multimodal signals. For instance, when said with a neutral tone and a disgusted face (Case II), it results in a 'sarcastic' verdict. On the other hand, when the same utterance is expressed with a low tone and while the speaker is dancing (Case III), we can assume the speaker means what they say without any sarcasm. To capture multimodality efficaciously, it is vital to grant a prime prerogative to each input modality in order to capture its' peculiarities. To this end, we propose a spotlight-aware fusion mechanism, where the final multimodal amalgamated vector is tailored by paying special attention to individual modalities.\n\nAll affective components, such as sarcasm, humour, and emotion, work in tandem to convey a statement's intended meaning  (Hasan et al. 2021; Chauhan et al. 2020) . Accordingly, we hypothesize that understanding one of the affective markers, like sarcasm, in its entirety will influence comprehending others. Consequently, in this work, we deal with leveraging sarcasm explanations for three affect understanding tasks in dialogues, namely sarcasm detection, humour identification, and emotion recognition. The performance obtained from these tasks can be employed as a method to estimate the relevance of the SED task extrinsically.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Sarcasm. Figurative language such as sarcasm plays an integral role in resolving the veiled meaning of an uttered statement. Earlier studies dealt with sarcasm identification in standalone texts like tweets and reviews.  (Kreuz and Caucci 2007; Tsur, Davidov, and Rappoport 2010; Joshi, Sharma, and Bhattacharyya 2015; Peled and Reichart 2017) . A detailed summary of these studies can be found in the survey compiled by  Joshi, Bhattacharyya, and Carman (2017) . Several work explored sarcasm in other languages such as Hindi  (Bharti, Sathya Babu, and Jena 2017) , Arabic (Abu Farha and Magdy 2020), Spanish  (Ortega-Bueno et al. 2019) , Italian  (Cignarella et al. 2018) , or even code-mixed  (Swami et al. 2018)  languages.\n\nSarcasm and Dialogues. Linguistic and lexical traits were the primary sources of sarcasm markers in previous investigations  (Kreuz and Caucci 2007; Tsur, Davidov, and Rappoport 2010) . However, in more contemporary studies, attention-based approaches are used to capture the interand intra-sentence interactions in the text  (Tay et al. 2018; Xiong et al. 2019; Srivastava et al. 2020) . In terms of conversations,  Ghosh, Richard Fabbri, and Muresan (2017)  harnessed attention-based RNNs to capture context and determinate sarcasm.\n\nSarcasm and Multimodality.  Castro et al. (2019)  proposed a multimodal, multiparty, English dataset called MUStARD to benchmark the task of multimodal sarcasm identification in conversation. Subsequently,  Chauhan et al. (2020)  devised a multi-task framework by leveraging interdependency between emotions and sarcasm to solve the task of multimodal sarcasm detection. Another work  (Hasan et al. 2021)  established the interdependency of humour with sarcasm by suggesting a humour knowledge enriched Transformer model for sarcasm detection. In the code-mixed scenario,  Bedi et al. (2021)  proposed MASAC, a multimodal, multiparty, code-mixed dialogue dataset for humour and sarcasm detection. In the bimodal setting, sarcasm identification with tweets containing images has also been well explored  (Cai, Cai, and Wan 2019; Xu, Zeng, and Mao 2020; Pan et al. 2020)  .\n\nBeyond Sarcasm Detection.Sarcasm generation is another direction that practitioners are inquisitive about due to its forthright benefit in enhancing chatbot engagement. Thereby,  Mishra, Tater, and Sankaranarayanan (2019)  induced sarcastic utterances by presenting context incongruity through fact removal and incongruous phrase insertion. A retrieve-and-edit-based unsupervised approach for generating sarcasm was proposed by  Chakrabarty et al. (2020)  that exploits semantic incongruity and valence reversal to convert non-sarcastic instances to sarcastic ones. On the other hand, while detecting irony is crucial, it is insufficient to capture the cardinal connotation of the statement. Consequently,  Dubey, Joshi, and Bhattacharyya (2019)  examined the task of converting sarcastic utterances into their nonsarcastic counterparts using deep neural networks.\n\nIn this work, we explore the task of Sarcasm Explanation in Dialogues, the second attempt after  Kumar et al. (2022) . SED aims to generate natural language explanations for a disseminated multimodal sarcastic conversation. We present a new model, MOSES, which enhances the current state-of-the-art for the SED task. However, unlike  Kumar et al. (2022) , we perform both intrinsic and extrinsic evaluations to show the efficacy and usefulness of our model. We leverage the generated explanations to improve three affect understanding tasks -sarcasm detection, humour identification, and emotion recognition in dialogues.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Dataset",
      "text": "Human conversations often take place employing a variety of languages. The phenomenon of using a blend of more than one language to communicate is dubbed code-mixing. Due to the prevalence of code-mixing in today's world, we consider the WITS dataset  (Kumar et al. 2022 ), which contains code-mixed dialogues (English-Hindi) from an Indian TV series. The dataset comprises multimodal, multiparty, code-mixed, sarcastic conversations where each sarcastic instance is annotated with a corresponding natural language code-mixed explanation.\n\nIn order to gauge the effect of sarcasm explanation on affective attributes, we augment the WITS dataset to perform sarcasm detection, humour identification, and emotion recognition on it. We create instances for sarcastic and non-sarcastic utterances with their context to perform sarcasm detection. We call this variation of the dataset sWITS. Adapted from MASAC  (Bedi et al. 2021) , WITS can also be mapped to annotations for humour identification, where each utterance contains a binary marker showcasing whether the utterance is amusing or not. Consequently, we map each instance in sWITS to its corresponding humour annotation. Additionally, we determine emotion labels for the instances at hand and identify the following emotions -sadness, joy, anger, and neutral. Three annotators were involved in this phase and achieved an inter-annotator agreement of 0.86. More information can be found in the supplementary. Accordingly, we obtain four variations of the dataset: The existing MAF module consists of an adapter-based module comprising two modules. The two modules -Multimodal Context Aware Attention (MCA2) and Global Information Fusion (GIF) together make up the Multimodal Aware Fusion (MAF) module. Given the three input signals, namely text, audio, and video, the MCA2 module effectively introduces multimodal information in the textual representations. Further, the GIF module combines the multimodal infused textual representations. We insert another module in the pipeline, Modality Spotlight (MS), which is responsible for attending to each modality by treating it as the primary modality and the rest as the context. We explain each of these modules below.\n\nMultimodal Context-Aware Attention (MCA2). The textual modality directly interacts with the other modalities in the standard fusion scheme, which uses dot-product based cross-modal attention. The multimodal representation acts as the key and value vectors while the text serves as the query. However, such a direct fusion of multimodal information from different embedding subspaces can lead to an inefficient representation that cannot capture contextual information. Consequently, inspired by  Yang et al. (2019) , a context-aware attention block is used instead of dot product attention in the MCA2 module.\n\nThe multimodal context-aware attention first generates multimodal information conditioned key and value vectors using a primary representation, H which depends on the choice of dominant modality in consideration. For example, H can be obtained from any language model, such as BART  (Lewis et al. 2020) , if text is to be considered as the primary modality. To generate information fused vectors, the MCA2 module first needs to convert H into key, query, and value vectors, q, k, v ∈ R n×d , respectively, as illustrated in Equation 1. Here, W q , W k , and W v ∈ R d×d are learnable parameters where the input, of maximum length n, is represented by a vector of dimension d.\n\n(1)\n\nIf we consider M ∈ R n×dc a multimodal vector, the multimodal information infused key and value vectors k m , v m ∈ R n×d , are generated following Equation  2 . Here, λ ∈ R n×1 decides the amount of multimodal information to capture into the primary modality.\n\nThe parameter λ is learned using a gating mechanism, as shown in Equation  3 . Note that the matrices,\n\n, in Equations 2 and 3 are trained with the model.\n\nOnce the modified key and value pair is obtained, the traditional dot-product attention is used to obtain the final multimodal fused vectors.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Modality Spotlight (Ms).",
      "text": "We discussed how we can generate multimodal infused vector representation considering one modality as primary and the rest as context. Our work deals with three modalities -text, acoustic, and visual. The spotlight module is responsible for treating each of these modalities as the primary modality at a time and generating the corresponding fused vectors. For instance, if text is considered the primary modality, then we need to calculate two multimodal fused vectors, H T a and H T v , such as audio and video, play the role of context in the representations, respectively. Similarly, when audio and video are considered the primary source of information, H tA and H tV are calculated. Note that we do not calculate H Av or H aV because we are dealing with a textual generation task where the textual information plays the preliminary role.\n\nApart from bi-modal interactions, we also deal with trimodal interactions in our work, where all three modalities are infused using the GIF module, as discussed later. Unlike bi-modal fusion, it is unfair to let text be the only primary modality in the tri-modal fusion. Consequently, we compute three tri-modal vectors, H T av , H tAv , and H taV , such that text, audio, and video individually play the primary role, respectively.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Global Information Fusion (Gif).",
      "text": "The GIF module is responsible for combining the information from multiple modalities together in an efficient manner. G gates are used to control the amount of information disseminated by each modality, where 2 ≤ G ≤ 3 is the number of modalities to fuse. For instance, if we calculate the interaction between the text and audio modalities with text being the primary source of information, we will first need to calculate the gated information from the audio representation using Equation  4 .\n\nwhere W a and b a are learnable matrices, and ⊕ denotes vector concatenation. The final representation to be passed on to the next encoder layer, in this case, will be obtained using Equation  5 .\n\n(5) On similar lines, if we are to calculate the tri-modal representation keeping the text as the primary modality, we first compute the gated vector for audio and video and then compute a weighted combination of the three modalities. The following sequence of equations illustrates this process,\n\nLikewise, we calculate the following set of vectors: H T a , H tA , H T v , H tV , H T av , H tAv , and H taV . Further, another GIF module is used to conglomerate these seven vectors, as shown in Equation  6 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiment And Results",
      "text": "This section illustrates the feature extraction strategy we use and the baseline systems to which we compare our model, followed by the results we obtain for the SED task. We use the standard generative metrics -ROUGE-1/2/L  (Lin 2004 ), BLEU-1/2/3/4  (Papineni et al. 2002) , and METEOR  (Denkowski and Lavie 2014)  to capture the syntactic and semantic performance of our system. Details about the execution process and the hyperparameters used are mentioned in the supplementary.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Feature Extraction",
      "text": "The primary challenges for generating vector representations for the instances in WITS come from the code-mixed and multimodal aspects of the dataset. We alleviate these by proposing intelligent feature extraction methods.\n\nText: The textual input in WITS is present in romanised code-mixed format. Thereby, it may contain terms with the same meaning but varying spellings that are phonetically identical. For instance, the word \"main\" in Hindi (translating to \"I\" in English) can be written as \"main\" or \"mein\".\n\nTo capture the similarity between all these spelling variations, we propose using Pronunciation Embeddings (PE) that capture the phonetic equivalence between the words of the input text. We convert the text into a standard speech format using python's gTTS library 1  . This converted audio does not contain any tone or pitch variation for any term and thus, sounds the same for phonetically similar terms. We then extract the audio features from this converted speech using the method described below. This pronunciation vector is fused with the text representation, obtained from any encoder model like BART, using the GIF module to obtain the final text representation.\n\nAudio: We use audio features provided by the authors of WITS.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "Textual: Pronunciation Embeddings (PE): Due to the subjective nature of how other languages (Hindi, in our case) are written in a romanised format, the spellings of the words come from their phonetic understanding. To resolve the ambiguity between the same words with differing spellings, we propose to use pronunciation embeddings. As illustrated in Table 2, we observe that by adding the PE component to the model with the help of the GIF module, the performance of text-based systems jumps by an average of ∼ 4% across all evaluation metrics.\n\nMultimodality: After we obtain the representation for the code-mixed text by fusing textual representation with pronunciation embeddings, we move on to adding multimodality to the system. We experimented with an established SED method (MAF-TAV) to estimate the effect of multimodality. Table  2  exhibits that while the addition of acoustic signals does not result in a performance gain, the addition of visual cues boosts the performance by ∼ 1% across all metrics. This phenomenon can be attributed to the fact that audio alone may cause confusion while understanding sarcasm, and visual hints may help in such times (Case III in Figure  1 ). Thereby, improving the visual feature representations can be one of the future directions. Finally, when we add all multimodal signals together, we observe the best performance yet with an average increase of further ∼ 1% across majority metrics.\n\nModality Spotlight: As hypothesised, we obtain the best performance for sarcasm understanding when all the three modalities are used in tandem. However, earlier methods for SED provided limelight to only textual representations  (Kumar et al. 2022) . We argue that especially in the case of sarcasm, multimodal signals such as audio and video might play the principal role in many instances. To comprehend this rotating importance of modalities, we use the spotlight module that aims to treat each modality as the primary modality while calculating the final representation. We observe an increase of ∼ 2% across all evaluation metrics as shown in Table  2 . These results directly support our hypothesis of the effect of multimodality in sarcasm analysis.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Ablation Study",
      "text": "To highlight the importance of all modules in consideration, we perform extensive ablation studies on the WITS dataset. Table  3  shows the results when we add the different proposed modules to our system sequentially. The first row highlights the BART model's results for the text modality which results in a ROUGE-2 of 11.91%. As illustrated, the use of naive trimodal concatenation (T ⊕ A ⊕ V ) of text, audio, and video representations produces a noisy fusion resulting in decreased performance (-10.2% ROUGE-2).\n\nNext, we try with the standard dot-product attention, which, being a comparatively smarter way of multimodal fusion, results in a slightly improved performance over the text-only modality (+2% ROUGE-2). Further, adding the multimodal context-aware attention module (MCA2) and replacing standard dot-product attention, produces a further performance boost by ∼ 1% across all metrics, signifying the importance of the intelligent fusion that the MCA2 module provides us.\n\nThe performance is increased even more when the GIF module is introduced to compute the final multimodal vector representation (+4% ROUGE-2), signifying the positive effect gated fusion has on efficient multimodal representations.  the model and observe another performance boost across majority metrics (∼ 1%), suggesting that we can obtain better code-mixed representations by reducing the spelling ambiguities. Finally, our entire model with modality spotlight included produces the best performance, verifying the necessary use of each module discussed.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Result Analysis",
      "text": "Quantitative Analysis. MOSES is evaluated on its ability to capture sarcasm source and target in the generated explanations. We compare MOSES with mBART, BART, and MAF. Table  5  shows that BART performs better than mBART for both source and target detection. The inclusion of multimodal signals, even without pronunciation embeddings and modality spotlight, improves the source identification performance by ∼ 14%. MOSES is able to detect the sarcasm source most efficiently, resulting in an improvement of ∼ 4% over the next best result. Consequently, we can relate the presence of multimodal capabilities to capture speaker-specific peculiarities more efficiently, resulting in better source/target identification. Table  5 : Accuracy for the sarcasm source and target for BART-based systems.\n\nQualitative Analysis. We sample a few dialogues from the test set of WITS and show their generated explanations by MOSES and the best baseline, MAF along with the ground-truth explanations in Table  4 . We show one of the many instances where our model generates the correct explanation for the given sarcastic instance in the first row. On the other hand, a lemon-picked instance is shown in the second row, where our model fails to generate the proper description for the explanation. The last row, highlights a case where the generated explanation is not syntactically similar to the ground-truth explanation but resembles it semantically. To evaluate the semantic similarity properly, we perform a human evaluation as explained in the following section.\n\nHuman Evaluation. We sample a total of 25 random instances from the test set and ask 20 human evaluators 4 to 4 The evaluators are fluent in English and their age ranges in 25-30 years.\n\nevaluate the generated explanations (on a scale of 1 to 5) on the following basis:\n\n• Coherence: Checks the generated explanation for correct structure and grammar. • On topic: Measures the extent to which the generated explanation revolves around the dialogue topic. • Capturing sarcasm: Estimates the level of emitted sarcasm being captured in the generated output.\n\nWe show the average score for the human evaluation parameters in Table  6 . As illustrated, the proposed MOSES model exhibits more coherent, on topic, and sarcasm related explanations. However, there is still a scope for improvement, which can be taken up as future work.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Understanding Affects With Explanation",
      "text": "We study three understanding tasks in dialogues -sarcasm detection, humour identification, and emotion recognition using sWITS, hWITS, and eWITS, respectively. A trained SED system is used to obtain the explanations for all the instances present in these datasets. We show the qualitative analysis of the generated explanations by MOSES in the supplementary. To verify our hypothesis that sarcasm explanation helps affect understanding, we perform experiments with and without explanations, as explained in the subsequent sections.\n\nSarcasm Detection. We take a base RoBERTa model  (Liu et al. 2019 ) and perform the task of sarcasm detection over sWITS. The experimentation is performed using three setups as described below:\n\n1. When we do not provide any utterance explanation to the input dialogue. 2. When we provide utterances appended with their generated explanation at the training time. Plain dialogues are given at the testing time in this case. 3. When dialogue instances are appended with their corresponding explanations during train and test time.\n\nTable  7  illustrates the results we obtain for all the settings for MOSES and the best baseline, MAF. As can be seen, RoBERTa obtains 62% F1 score when we do not use any explanations. However, with the use of the generated explanations by MOSES during the train time along with the input dialogues, we obtain an improvement of 6% F1-score.\n\nOn the other hand, the best performance is achieved by the last case, where the input instances are appended with their corresponding explanations both at the train and test time, with an increase of 8% F1-score. Consistent to the results obtained by MOSES's generation, MAF also reports an improved performance over no explanation model. However, the improvement shown by MAF is not at par with the improvement obtained by MOSES. These results directly support our hypothesis that utterance explanations can assist an efficient detection of sarcasm in the input instances. Humour Identification. Another RoBERTa base is used to perform humour identification on hWITS. As for sarcasm detection, humour identification is also evaluated for the three setups described in the previous section. Table  7  illustrates the results obtained for the described setups. When no explanations are used during the training or testing time, we get an F1-score of 73%. This score is comparable to the performance we get when input instances are appended with their corresponding explanations generated by MOSES at the training time. This performance is boosted by 3% when the explanations are provided at the train/test time. However, it is important to note that the explanations generated by the MAF model resulted in a slightly decreased performance indicating the superiority of MOSES.\n\nEmotion Recognition. Table  7  illustrates the results obtained for the task of emotion recognition on eWITS. We see the same value for the weighted F1 when we add explanations during the training phase of the system for both MAF and MOSES. However, when explanations assist both the training and testing phase, we observe an increase of 2% in the weighted F1 score for MOSESand 1% increase for MAF, indicating the positive effect explanations deliver for emotion recognition. Performance analysis for sarcastic and non-sarcastic instances can be found in the supplementary.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Error Analysis",
      "text": "Quantitative. To capture the improvement exhibited by explanations in affect understanding, we show the confusion matrices emitted by the understanding models with and without using explanations. Table  8  illustrates these matrices -and as can be seen, the methods with explanation obtains higher true positive rate with a decreased false positive and false negative rates for majority of the classes among sarcasm, humour, and emotion labels. (c) Emotion recognition on eWITS.\n\nTable  8 : Confusion matrix of the systems with and without explanations.\n\nQualitative. While quantitative results confirm that explanations assist in identifying affects efficiently, qualitative analysis can further corroborate this hypothesis. Table  9  shows one instance from the test set where the presence of explanation helps for all affective tasks in question. More such examples can be found in the supplementary. Table  9 : True and predicted labels for the three affect tasks with and without using MOSES's explanation.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "The inability of existing systems to understand sarcasm results in a performance gap for various affect understanding tasks like emotion recognition, humour identification, and sarcasm detection. To mitigate this issue, we proposed MOSES to explore the task of Sarcasm Explanation in Dialogues (SED). MOSES takes multimodal code-mixed sarcastic conversation instances as input and results in a natural language explanation describing the sarcasm present in it.\n\nWe further explored the effect of the generated explanations on three dialogue-based affect understanding tasks -sarcasm detection, humour identification, and emotion recognition. We observed that explanations improved the performance of all three tasks, thus verifying our hypothesis.\n\nOlkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016. Individual differences in the processing of written sarcasm and metaphor: Evidence from eye movements.   (Kumar et al. 2022)  where the aim is to generate a natural language explanation for a given multimodal sarcastic conversation. We propose MOSES, a deep neural architecture with the aim to solve SED. To exhibit the influence of sarcasm on other affective components, like humour and emotions, we leverage the generated explanation by MOSES and augment the input instances for affective tasks with them. We experiment with three affect understanding tasks in dialogues -sarcasm detection, humour identification, and emotion recognition. To perform appropriate experimentation for the tasks with and without explanations, we require supporting datasets. Consequently, we curate sWITS, hWITS, and eWITSfor the task of sarcasm detection, humour identification, and emotion recognition, respectively, from the WITS dataset  (Kumar et al. 2022) .",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Swits:",
      "text": "The parent dataset, WITS contains sarcastic instances along with their explanations. Each instance of WITS contains a sequence of utterances where the last utterance is sarcastic. However, for the sarcasm detection task, we need both sarcastic as well as non-sarcastic instances.\n\nTo create the non-sarcastic instances, we randomly sample utterances from the context of the instances present in WITS. Figure  3  illustrates the process of creating sWITS from WITS.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Hwits:",
      "text": "To gauge the effect of sarcasm explanation on humour identification, we will need instances with humour labels. As a result, we explore the mapping existing between WITS and the MASAC dataset  (Bedi et al. 2021) . MASAC is a multiparty, multimodal, code-mixed dialogue dataset collected from an Indian TV series. It contains binary markers identifying the presence of sarcasm and humour in all utterances. The WITS dataset is an extended version of the MASAC dataset where all the sarcastic utterances are appended with their corresponding natural language explana-  tion. Ergo, we map the humour labels from MASAC to the instances present in sWITS and get hWITS.\n\neWITS: Sarcasm significantly affects the emitted emotion of an utterance. Wherefore, we hypothesize that emotion recognition in conversation can be improved in the presence of utterance explanations. Therefore, we need emotion labels for the instances present in sWITS to create eWITS. We annotate the instances for emotion labels following the Ekman (Ekman 1992) emotion scheme. Out of the seven possible emotion labels, namely anger, fear, disgust, sadness, joy, and surprise, neutral, we were able to identify four for our set of instances -anger, sadness, joy, and neutral. Three annotators (ABC) were involved and achieved Krippendorff's Alpha  (Krippendorff 2011)  inter-annotator scores as α AB = 0.84, α BC = 0.88, and α AC = 0.86 giving an average score of 0.86. We show couple of instances from all the discussed datasets in Table  10 .",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Quality Of Generated Explanations",
      "text": "Our model, MOSES, being taught the Sarcasm Explanation in Dialogues (SED) task, is supplied only with sarcastic instances during training. Consequently, it is able to efficiently generate explanations for unseen sarcastic instances. However, when supplied with non-sarcastic instances, as is done to generate sWITS, hWITS, and eWITS, one can argue that the model might not work. Nevertheless, MOSES is able to return appropriate explanations for the majority of the input.\n\nAlthough, there are some explanations generated containing an undertone of sarcasm even for non-sarcastic input instances. Table  11  illustrates a few sample explanations generated for sarcastic and non-sarcastic inputs. The first two instances are sarcastic in nature and the corresponding explanations try to reveal this irony. While the first explanation seems relevant to the input dialogue, the explanation in the second instance is an example of incorrect generation. The last instance in Table  11  is non-sarcastic and as shown, the explanation generated seems correct for the dialogue. However, improving upon the quality of explanations, especially for non-sarcastic inputs, can be an interesting future direction of research.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Error Analysis For Affect Understanding",
      "text": "Quantitative Since MOSES is trained on the task of SED, there is a need to investigate its performance separately for sarcastic and non-sarcastic instances. We examine the obtained performance for sarcastic and non-sarcastic instances separately with and without using the generated dialogue explanation and produce those results in Table  12  for the task of humour identification and emotion recognition. We observe that while the performance of sarcastic humorous in-",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: [Best viewed in color] Effect of multimodality",
      "page": 1
    },
    {
      "caption": "Figure 1: shows cases where",
      "page": 1
    },
    {
      "caption": "Figure 2: The existing SED model,",
      "page": 3
    },
    {
      "caption": "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context",
      "page": 4
    },
    {
      "caption": "Figure 3: illustrates the process of creating sWITS",
      "page": 9
    },
    {
      "caption": "Figure 3: Construction of sWITS from WITS.",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "nition task. We also perform extensive analyses to assess the": ""
        },
        {
          "nition task. We also perform extensive analyses to assess the": "quality of the results."
        },
        {
          "nition task. We also perform extensive analyses to assess the": ""
        },
        {
          "nition task. We also perform extensive analyses to assess the": ""
        },
        {
          "nition task. We also perform extensive analyses to assess the": ""
        },
        {
          "nition task. We also perform extensive analyses to assess the": "Introduction"
        },
        {
          "nition task. We also perform extensive analyses to assess the": ""
        },
        {
          "nition task. We also perform extensive analyses to assess the": "Expressing oneself eloquently to our conversation partner"
        },
        {
          "nition task. We also perform extensive analyses to assess the": ""
        },
        {
          "nition task. We also perform extensive analyses to assess the": "requires employing multiple affective components such as"
        },
        {
          "nition task. We also perform extensive analyses to assess the": ""
        },
        {
          "nition task. We also perform extensive analyses to assess the": "emotion, humour, and sarcasm. All such attributes interact"
        },
        {
          "nition task. We also perform extensive analyses to assess the": ""
        },
        {
          "nition task. We also perform extensive analyses to assess the": "with each another to present a concrete deﬁnition of an ut-"
        },
        {
          "nition task. We also perform extensive analyses to assess the": ""
        },
        {
          "nition task. We also perform extensive analyses to assess the": "tered statement\n(Roberts\nand Kreuz 1994). While\naffects"
        },
        {
          "nition task. We also perform extensive analyses to assess the": ""
        },
        {
          "nition task. We also perform extensive analyses to assess the": "such as emotion and humour deem easier\nto comprehend,"
        },
        {
          "nition task. We also perform extensive analyses to assess the": ""
        },
        {
          "nition task. We also perform extensive analyses to assess the": "sarcasm, on the other hand,\nis a challenging aspect\nto com-"
        },
        {
          "nition task. We also perform extensive analyses to assess the": ""
        },
        {
          "nition task. We also perform extensive analyses to assess the": "prehend (Olkoniemi, Ranta, and Kaakinen 2016). Conse-"
        },
        {
          "nition task. We also perform extensive analyses to assess the": ""
        },
        {
          "nition task. We also perform extensive analyses to assess the": "quently,\nit becomes imperative for NLP systems to capture"
        },
        {
          "nition task. We also perform extensive analyses to assess the": ""
        },
        {
          "nition task. We also perform extensive analyses to assess the": "and understand sarcasm in its entirety. Sarcasm Explanation"
        },
        {
          "nition task. We also perform extensive analyses to assess the": ""
        },
        {
          "nition task. We also perform extensive analyses to assess the": "in Dialogues (SED) is a new task proposed recently in this"
        },
        {
          "nition task. We also perform extensive analyses to assess the": ""
        },
        {
          "nition task. We also perform extensive analyses to assess the": "direction (Kumar et al. 2022).\nIn this work, we scour\nthe"
        },
        {
          "nition task. We also perform extensive analyses to assess the": ""
        },
        {
          "nition task. We also perform extensive analyses to assess the": "Copyright © 2023, Association for the Advancement of Artiﬁcial"
        },
        {
          "nition task. We also perform extensive analyses to assess the": "Intelligence (www.aaai.org). All rights reserved."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "Abstract"
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "Conversations emerge as the primary media for exchanging"
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "ideas and conceptions. From the listener’s perspective,\niden-"
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "tifying various affective qualities, such as sarcasm, humour,"
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "and emotions, is paramount for comprehending the true con-"
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "notation of the emitted utterance. However, one of the major"
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "hurdles faced in learning these affect dimensions is the pres-"
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": ""
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "ence of ﬁgurative language viz.\nirony, metaphor, or sarcasm."
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": ""
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "We hypothesize that any detection system constituting the"
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": ""
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "exhaustive and explicit presentation of the emitted utterance"
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "would improve the overall comprehension of\nthe dialogue."
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": ""
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "To this end, we explore the task of Sarcasm Explanation in"
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": ""
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "Dialogues that aims to unfold the hidden irony behind sarcas-"
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "tic utterances. We propose MOSES, a deep neural network,"
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "which takes a multimodal (sarcastic) dialogue instance as an"
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "input and generates a natural\nlanguage sentence as its expla-"
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "nation. Subsequently, we leverage the generated explanation"
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": ""
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "for various natural language understanding tasks in a conver-"
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": ""
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "sational dialogue setup, such as sarcasm detection, humour"
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": ""
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "identiﬁcation, and emotion recognition. Our evaluation shows"
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": ""
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "that MOSES outperforms the state-of-the-art system for SED"
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": ""
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "by an average of ∼ 2% on different evaluation metrics, such"
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": ""
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "as ROUGE, BLEU, and METEOR. Further, we observe that"
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "leveraging the generated explanation advances three down-"
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "stream tasks for affect classiﬁcation – an average improve-"
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "ment of ∼ 14% F1-score in the sarcasm detection task and"
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "and ∼ 2% in the humour identiﬁcation and emotion recog-"
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "nition task. We also perform extensive analyses to assess the"
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": ""
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "quality of the results."
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": ""
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": ""
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": ""
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "Introduction"
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": ""
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "Expressing oneself eloquently to our conversation partner"
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": ""
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "requires employing multiple affective components such as"
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": ""
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "emotion, humour, and sarcasm. All such attributes interact"
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": ""
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "with each another to present a concrete deﬁnition of an ut-"
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": ""
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "tered statement\n(Roberts\nand Kreuz 1994). While\naffects"
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": ""
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "such as emotion and humour deem easier\nto comprehend,"
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": ""
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "sarcasm, on the other hand,\nis a challenging aspect\nto com-"
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": ""
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "prehend (Olkoniemi, Ranta, and Kaakinen 2016). Conse-"
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": ""
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "quently,\nit becomes imperative for NLP systems to capture"
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": ""
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "and understand sarcasm in its entirety. Sarcasm Explanation"
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": ""
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "in Dialogues (SED) is a new task proposed recently in this"
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": ""
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "direction (Kumar et al. 2022).\nIn this work, we scour\nthe"
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": ""
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "Copyright © 2023, Association for the Advancement of Artiﬁcial"
        },
        {
          "shivaniku@iiitd.ac.in, ishani340@gmail.com, shad.akhtar@iiitd.ac.in, tanchak@iitd.ac.in": "Intelligence (www.aaai.org). All rights reserved."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "We summarize our contributions below:": "1. We explore the new task of SED and propose a novel",
          "through fact\nremoval and incongruous phrase insertion. A": "retrieve-and-edit-based unsupervised approach for generat-"
        },
        {
          "We summarize our contributions below:": "model, MOSES, for it.",
          "through fact\nremoval and incongruous phrase insertion. A": "ing sarcasm was proposed by Chakrabarty et al. (2020) that"
        },
        {
          "We summarize our contributions below:": "2. We compare MOSES with existing baselines and obtain",
          "through fact\nremoval and incongruous phrase insertion. A": "exploits semantic incongruity and valence reversal\nto con-"
        },
        {
          "We summarize our contributions below:": "state-of-the-art results for the SED task.",
          "through fact\nremoval and incongruous phrase insertion. A": "vert non-sarcastic instances to sarcastic ones. On the other"
        },
        {
          "We summarize our contributions below:": "3. We show the application of the generated explanations in",
          "through fact\nremoval and incongruous phrase insertion. A": "hand, while detecting irony is crucial,\nit\nis\ninsufﬁcient\nto"
        },
        {
          "We summarize our contributions below:": "understanding different affective components – sarcasm,",
          "through fact\nremoval and incongruous phrase insertion. A": "capture the cardinal connotation of\nthe statement. Conse-"
        },
        {
          "We summarize our contributions below:": "emotions and humour.",
          "through fact\nremoval and incongruous phrase insertion. A": "quently, Dubey, Joshi, and Bhattacharyya (2019) examined"
        },
        {
          "We summarize our contributions below:": "4. We show extensive quantitative and qualitative studies",
          "through fact\nremoval and incongruous phrase insertion. A": "the task of converting sarcastic utterances\ninto their non-"
        },
        {
          "We summarize our contributions below:": "for all our experiments.",
          "through fact\nremoval and incongruous phrase insertion. A": "sarcastic counterparts using deep neural networks."
        },
        {
          "We summarize our contributions below:": "Reproducibility: The source code for MOSES with the",
          "through fact\nremoval and incongruous phrase insertion. A": "In this work, we explore the task of Sarcasm Explana-"
        },
        {
          "We summarize our contributions below:": "execution instructions are present here: https://github.com/",
          "through fact\nremoval and incongruous phrase insertion. A": "tion in Dialogues,\nthe\nsecond attempt\nafter Kumar\net\nal."
        },
        {
          "We summarize our contributions below:": "LCS2-IIITD/MOSES.git.",
          "through fact\nremoval and incongruous phrase insertion. A": "(2022). SED aims to generate natural language explanations"
        },
        {
          "We summarize our contributions below:": "",
          "through fact\nremoval and incongruous phrase insertion. A": "for a disseminated multimodal\nsarcastic conversation. We"
        },
        {
          "We summarize our contributions below:": "Related Work",
          "through fact\nremoval and incongruous phrase insertion. A": "present a new model, MOSES, which enhances the current"
        },
        {
          "We summarize our contributions below:": "",
          "through fact\nremoval and incongruous phrase insertion. A": "state-of-the-art\nfor\nthe SED task. However, unlike Kumar"
        },
        {
          "We summarize our contributions below:": "Sarcasm. Figurative language such as sarcasm plays an in-",
          "through fact\nremoval and incongruous phrase insertion. A": ""
        },
        {
          "We summarize our contributions below:": "",
          "through fact\nremoval and incongruous phrase insertion. A": "et al. (2022), we perform both intrinsic and extrinsic evalu-"
        },
        {
          "We summarize our contributions below:": "tegral\nrole in resolving the veiled meaning of an uttered",
          "through fact\nremoval and incongruous phrase insertion. A": ""
        },
        {
          "We summarize our contributions below:": "",
          "through fact\nremoval and incongruous phrase insertion. A": "ations to show the efﬁcacy and usefulness of our model. We"
        },
        {
          "We summarize our contributions below:": "statement. Earlier studies dealt with sarcasm identiﬁcation in",
          "through fact\nremoval and incongruous phrase insertion. A": ""
        },
        {
          "We summarize our contributions below:": "",
          "through fact\nremoval and incongruous phrase insertion. A": "leverage the generated explanations to improve three affect"
        },
        {
          "We summarize our contributions below:": "standalone texts like tweets and reviews. (Kreuz and Caucci",
          "through fact\nremoval and incongruous phrase insertion. A": ""
        },
        {
          "We summarize our contributions below:": "",
          "through fact\nremoval and incongruous phrase insertion. A": "understanding tasks – sarcasm detection, humour identiﬁca-"
        },
        {
          "We summarize our contributions below:": "2007; Tsur, Davidov, and Rappoport 2010; Joshi, Sharma,",
          "through fact\nremoval and incongruous phrase insertion. A": ""
        },
        {
          "We summarize our contributions below:": "",
          "through fact\nremoval and incongruous phrase insertion. A": "tion, and emotion recognition in dialogues."
        },
        {
          "We summarize our contributions below:": "and Bhattacharyya 2015; Peled and Reichart 2017). A de-",
          "through fact\nremoval and incongruous phrase insertion. A": ""
        },
        {
          "We summarize our contributions below:": "tailed summary of these studies can be found in the survey",
          "through fact\nremoval and incongruous phrase insertion. A": ""
        },
        {
          "We summarize our contributions below:": "compiled by Joshi, Bhattacharyya, and Carman (2017). Sev-",
          "through fact\nremoval and incongruous phrase insertion. A": "Dataset"
        },
        {
          "We summarize our contributions below:": "eral work explored sarcasm in other languages such as Hindi",
          "through fact\nremoval and incongruous phrase insertion. A": "Human conversations often take place employing a variety"
        },
        {
          "We summarize our contributions below:": "(Bharti, Sathya Babu, and Jena 2017), Arabic (Abu Farha",
          "through fact\nremoval and incongruous phrase insertion. A": "of\nlanguages. The phenomenon of using a blend of more"
        },
        {
          "We summarize our contributions below:": "and Magdy 2020), Spanish (Ortega-Bueno et al. 2019), Ital-",
          "through fact\nremoval and incongruous phrase insertion. A": "than one language to communicate is dubbed code-mixing."
        },
        {
          "We summarize our contributions below:": "ian (Cignarella et al. 2018), or even code-mixed (Swami",
          "through fact\nremoval and incongruous phrase insertion. A": "Due to the prevalence of code-mixing in today’s world, we"
        },
        {
          "We summarize our contributions below:": "et al. 2018) languages.",
          "through fact\nremoval and incongruous phrase insertion. A": "consider the WITS dataset (Kumar et al. 2022), which con-"
        },
        {
          "We summarize our contributions below:": "Sarcasm and Dialogues. Linguistic\nand\nlexical\ntraits",
          "through fact\nremoval and incongruous phrase insertion. A": "tains code-mixed dialogues (English-Hindi) from an Indian"
        },
        {
          "We summarize our contributions below:": "were the primary sources of\nsarcasm markers\nin previous",
          "through fact\nremoval and incongruous phrase insertion. A": "TV series. The dataset comprises multimodal, multiparty,"
        },
        {
          "We summarize our contributions below:": "investigations (Kreuz and Caucci 2007; Tsur, Davidov, and",
          "through fact\nremoval and incongruous phrase insertion. A": "code-mixed, sarcastic conversations where each sarcastic in-"
        },
        {
          "We summarize our contributions below:": "Rappoport 2010). However,\nin more contemporary studies,",
          "through fact\nremoval and incongruous phrase insertion. A": "stance is annotated with a corresponding natural\nlanguage"
        },
        {
          "We summarize our contributions below:": "attention-based approaches\nare used to capture\nthe\ninter-",
          "through fact\nremoval and incongruous phrase insertion. A": "code-mixed explanation."
        },
        {
          "We summarize our contributions below:": "and intra-sentence interactions in the text (Tay et al. 2018;",
          "through fact\nremoval and incongruous phrase insertion. A": "In order\nto gauge the effect of\nsarcasm explanation on"
        },
        {
          "We summarize our contributions below:": "Xiong et al. 2019; Srivastava et al. 2020). In terms of con-",
          "through fact\nremoval and incongruous phrase insertion. A": "affective attributes, we augment\nthe WITS dataset\nto per-"
        },
        {
          "We summarize our contributions below:": "versations, Ghosh, Richard Fabbri, and Muresan (2017) har-",
          "through fact\nremoval and incongruous phrase insertion. A": "form sarcasm detection, humour\nidentiﬁcation,\nand emo-"
        },
        {
          "We summarize our contributions below:": "nessed attention-based RNNs to capture context and deter-",
          "through fact\nremoval and incongruous phrase insertion. A": "tion recognition on it. We create instances for sarcastic and"
        },
        {
          "We summarize our contributions below:": "minate sarcasm.",
          "through fact\nremoval and incongruous phrase insertion. A": "non-sarcastic utterances with their context\nto perform sar-"
        },
        {
          "We summarize our contributions below:": "Sarcasm and Multimodality. Castro et al.\n(2019) pro-",
          "through fact\nremoval and incongruous phrase insertion. A": "casm detection. We call this variation of the dataset sWITS."
        },
        {
          "We summarize our contributions below:": "posed\na multimodal, multiparty,\nEnglish\ndataset\ncalled",
          "through fact\nremoval and incongruous phrase insertion. A": "Adapted from MASAC (Bedi et al. 2021), WITS can also"
        },
        {
          "We summarize our contributions below:": "MUStARD to benchmark the task of multimodal sarcasm",
          "through fact\nremoval and incongruous phrase insertion. A": "be mapped to annotations for humour identiﬁcation, where"
        },
        {
          "We summarize our contributions below:": "identiﬁcation in conversation. Subsequently, Chauhan et al.",
          "through fact\nremoval and incongruous phrase insertion. A": "each utterance contains a binary marker showcasing whether"
        },
        {
          "We summarize our contributions below:": "(2020) devised a multi-task framework by leveraging in-",
          "through fact\nremoval and incongruous phrase insertion. A": "the utterance is amusing or not. Consequently, we map each"
        },
        {
          "We summarize our contributions below:": "terdependency between emotions and sarcasm to solve the",
          "through fact\nremoval and incongruous phrase insertion. A": "instance in sWITS to its corresponding humour annotation."
        },
        {
          "We summarize our contributions below:": "task of multimodal sarcasm detection. Another work (Hasan",
          "through fact\nremoval and incongruous phrase insertion. A": "Additionally, we determine emotion labels for the instances"
        },
        {
          "We summarize our contributions below:": "et al. 2021) established the interdependency of humour with",
          "through fact\nremoval and incongruous phrase insertion. A": "joy,\nat hand and identify the following emotions – sadness,"
        },
        {
          "We summarize our contributions below:": "sarcasm by suggesting a humour knowledge enriched Trans-",
          "through fact\nremoval and incongruous phrase insertion. A": "anger, and neutral. Three annotators were involved in this"
        },
        {
          "We summarize our contributions below:": "former model for sarcasm detection. In the code-mixed sce-",
          "through fact\nremoval and incongruous phrase insertion. A": "phase and achieved an inter-annotator agreement of 0.86."
        },
        {
          "We summarize our contributions below:": "nario, Bedi et al.\n(2021) proposed MASAC, a multimodal,",
          "through fact\nremoval and incongruous phrase insertion. A": "More information can be found in the supplementary. Ac-"
        },
        {
          "We summarize our contributions below:": "multiparty, code-mixed dialogue dataset for humour and sar-",
          "through fact\nremoval and incongruous phrase insertion. A": "cordingly, we obtain four variations of the dataset:"
        },
        {
          "We summarize our contributions below:": "casm detection.\nIn the bimodal setting, sarcasm identiﬁca-",
          "through fact\nremoval and incongruous phrase insertion. A": ""
        },
        {
          "We summarize our contributions below:": "",
          "through fact\nremoval and incongruous phrase insertion. A": "1. WITS:\nIt contains multimodal, multiparty, code-mixed,"
        },
        {
          "We summarize our contributions below:": "tion with tweets containing images has also been well ex-",
          "through fact\nremoval and incongruous phrase insertion. A": ""
        },
        {
          "We summarize our contributions below:": "",
          "through fact\nremoval and incongruous phrase insertion. A": "sarcastic instances with associated explanations."
        },
        {
          "We summarize our contributions below:": "plored (Cai, Cai, and Wan 2019; Xu, Zeng, and Mao 2020;",
          "through fact\nremoval and incongruous phrase insertion. A": ""
        },
        {
          "We summarize our contributions below:": "",
          "through fact\nremoval and incongruous phrase insertion. A": "2. sWITS: It contains sarcastic and non-sarcastic instances"
        },
        {
          "We summarize our contributions below:": "Pan et al. 2020) .",
          "through fact\nremoval and incongruous phrase insertion. A": ""
        },
        {
          "We summarize our contributions below:": "",
          "through fact\nremoval and incongruous phrase insertion. A": "constructed from WITS. The last utterance of each in-"
        },
        {
          "We summarize our contributions below:": "Beyond Sarcasm Detection.Sarcasm generation is\nan-",
          "through fact\nremoval and incongruous phrase insertion. A": ""
        },
        {
          "We summarize our contributions below:": "",
          "through fact\nremoval and incongruous phrase insertion. A": "stance is marked by a binary tag indicating whether the"
        },
        {
          "We summarize our contributions below:": "other direction that practitioners are inquisitive about due",
          "through fact\nremoval and incongruous phrase insertion. A": ""
        },
        {
          "We summarize our contributions below:": "",
          "through fact\nremoval and incongruous phrase insertion. A": "statement contains sarcasm or not."
        },
        {
          "We summarize our contributions below:": "to its\nforthright beneﬁt\nin enhancing chatbot engagement.",
          "through fact\nremoval and incongruous phrase insertion. A": ""
        },
        {
          "We summarize our contributions below:": "Thereby, Mishra, Tater, and Sankaranarayanan (2019)\nin-",
          "through fact\nremoval and incongruous phrase insertion. A": "3. hWITS: For each instance created in sWITS, each target"
        },
        {
          "We summarize our contributions below:": "duced sarcastic utterances by presenting context incongruity",
          "through fact\nremoval and incongruous phrase insertion. A": "utterance is marked with another binary label\nrevealing"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: Statistics of the sarcasm, humour, and emo-",
      "data": [
        {
          "WITS\nsWITS\nhWITS\neWITS": ""
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "#S\n#NS\n#S\n#NH\n#H\n#Neutral\n#Sadness\n#Joy\n#Anger"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": ""
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "1792\n1669\n1792\n2795\n995\n1590\n1147\n623\n429\nTrain"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "224\n213\n224\n362\n112\n196\n133\n87\n57\nVal"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "224\n218\n224\n367\n106\n195\n141\n70\n67\nTest"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "2240\n2100\n2240\n3524\n1213\n1981\n1421\n780\n553\nTotal"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": ""
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "Table\n1:\nStatistics\nof\nthe\nsarcasm,\nhumour,\nand\nemo-"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": ""
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "tion\ndatasets\nin\nconsideration\n(number\nof\ndialogue\nin-"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": ""
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "stances marked as sarcastic (#S), non-sarcastic (#NS), non-"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": ""
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "humorous (#NH), and humorous (#H).)."
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": ""
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": ""
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "the existence of humour in it."
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": ""
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "4. eWITS: Similar to hWITS, this variant contains emotion"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "labels for the target utterances."
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "Table 1 illustrates the elementary statistics for the explained"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": ""
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "dataset variations. More details about the dataset are present"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": ""
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "in the supplementary."
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": ""
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": ""
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "Proposed Method"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "This section illustrates the working of our proposed model,"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": ""
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "MOSES as presented in Figure 2. The existing SED model,"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": ""
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "MAF (Kumar et al. 2022), which uses a modiﬁed version of"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "context-aware attention (Yang et al. 2019),\ntakes the multi-"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": ""
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "modal (audio/video) vectors as context and fuses them with"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": ""
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "the text modality to generate multimodal\nfused text vec-"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": ""
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "tors. This way of multimodal fusion makes text the primary"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": ""
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "modality and treats the other signals (acoustic and visual) as"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": ""
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "secondary. Such a fusion technique might result in the down-"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": ""
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": ""
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "play of the audio and video modalities. However, in the com-"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "plete duration of\nthe discourse, modalities other\nthan text"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": ""
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "could play the deciding role in resolving the affects in con-"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": ""
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "sideration. Consequently, we propose using context-aware"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": ""
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "fusion in such a way that each modality gets a chance to"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "play a pivotal role in the fusion computation."
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "The existing MAF module consists of an adapter-based"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "module comprising two modules. The two modules – Mul-"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "timodal Context Aware Attention (MCA2) and Global\nIn-"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "formation Fusion (GIF)\ntogether make up the Multimodal"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "Aware Fusion (MAF) module. Given the three input signals,"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "namely text, audio, and video, the MCA2 module effectively"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "introduces multimodal\ninformation in the textual\nrepresen-"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "tations. Further,\nthe GIF module combines the multimodal"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "infused textual representations. We insert another module in"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "the pipeline, Modality Spotlight\n(MS), which is\nresponsi-"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "ble for attending to each modality by treating it as the pri-"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "mary modality and the rest as the context. We explain each"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "of these modules below."
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": ""
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "Multimodal Context-Aware Attention\n(MCA2).\nThe"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": ""
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "textual modality directly interacts with the other modalities"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": ""
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "in the standard fusion scheme, which uses dot-product based"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": ""
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "cross-modal attention. The multimodal\nrepresentation acts"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": ""
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "as\nthe key and value vectors while the text\nserves as\nthe"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": ""
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "query. However, such a direct\nfusion of multimodal\ninfor-"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": ""
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "mation from different embedding subspaces can lead to an"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": ""
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "inefﬁcient representation that cannot capture contextual\nin-"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": ""
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "formation. Consequently,\ninspired by Yang et al.\n(2019), a"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "context-aware attention block is used instead of dot product"
        },
        {
          "WITS\nsWITS\nhWITS\neWITS": "attention in the MCA2 module."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": "Aware Attention (MCA2) and combines them using Global Information Fusion (GIF). Each modality is kept in spotlight using"
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": "the Modality Spotlight (MS) module. To capture the subjectivity in the code-mixed spellings, we propose to use pronunciation"
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": "embeddings."
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": "modalities together in an efﬁcient manner. G gates are used"
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": "to control\nthe amount of information disseminated by each"
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": ""
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": "modality, where 2 ≤ G ≤ 3 is the number of modalities to"
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": ""
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": "fuse. For instance, if we calculate the interaction between the"
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": ""
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": "text and audio modalities with text being the primary source"
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": ""
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": "of information, we will ﬁrst need to calculate the gated in-"
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": "formation from the audio representation using Equation 4."
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": ""
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": "(4)\nga = [H ⊕ Ha]Wa + ba"
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": ""
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": "where Wa and ba are learnable matrices, and ⊕ denotes vec-"
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": "tor concatenation. The ﬁnal representation to be passed on"
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": "to the next encoder layer, in this case, will be obtained using"
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": "Equation 5."
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": ""
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": "(5)\nHT a = H + ga (cid:12) Ha"
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": ""
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": "On similar lines,\nif we are to calculate the tri-modal repre-"
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": ""
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": "sentation keeping the text as the primary modality, we ﬁrst"
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": ""
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": "compute the gated vector for audio and video and then com-"
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": ""
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": "pute a weighted combination of\nthe three modalities. The"
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": ""
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": "following sequence of equations illustrates this process,"
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": ""
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": "ga = [H ⊕ Ha]Wa + ba"
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": ""
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": "gv = [H ⊕ Hv]Wv + bv"
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": ""
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": "HT av = H + ga (cid:12) Ha + gv (cid:12) Hv"
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": ""
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": "Likewise, we calculate the following set of vectors: HT a,"
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": "HtA, HT v, HtV , HT av, HtAv, and HtaV . Further, another"
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": "GIF module is used to conglomerate these seven vectors, as"
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": "shown in Equation 6."
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": ""
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": "Hall = gt (cid:12) H + gT a (cid:12) HT a + gtA (cid:12) HtA +"
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": ""
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": "gT v (cid:12) HT v + gtV (cid:12) HtV + gT av (cid:12) HT av +"
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": ""
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": "(6)\ngtAv (cid:12) HtAv + gtaV (cid:12) HtaV"
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": ""
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": "Experiment and Results"
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": "This section illustrates the feature extraction strategy we use"
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": "and the baseline systems to which we compare our model,"
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": "followed by the\nresults we obtain for\nthe SED task. We"
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": "use the standard generative metrics – ROUGE-1/2/L (Lin"
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": "2004), BLEU-1/2/3/4 (Papineni et al. 2002), and METEOR"
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": "(Denkowski and Lavie 2014)\nto capture the syntactic and"
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": "semantic performance of our system. Details about the exe-"
        },
        {
          "Figure 2: Model architecture for MOSES. The MAF model captures acoustic and visual hints using the Multimodal Context": "cution process and the hyperparameters used are mentioned"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: exhibits that while theaddition of acoustic sig-",
      "data": [
        {
          "Comparative Systems": "",
          "method (MAF-TAV) to estimate the effect of multimodal-": "ity. Table 2 exhibits that while the addition of acoustic sig-"
        },
        {
          "Comparative Systems": "We use various established sequence-to-sequence (seq2seq)",
          "method (MAF-TAV) to estimate the effect of multimodal-": ""
        },
        {
          "Comparative Systems": "",
          "method (MAF-TAV) to estimate the effect of multimodal-": "nals does not\nresult\nin a performance gain,\nthe addition of"
        },
        {
          "Comparative Systems": "models to obtain the most promising textual representations",
          "method (MAF-TAV) to estimate the effect of multimodal-": ""
        },
        {
          "Comparative Systems": "",
          "method (MAF-TAV) to estimate the effect of multimodal-": "visual cues boosts the performance by ∼ 1% across all met-"
        },
        {
          "Comparative Systems": "for the discourse. RNN: The openNMT42 implementation of",
          "method (MAF-TAV) to estimate the effect of multimodal-": ""
        },
        {
          "Comparative Systems": "",
          "method (MAF-TAV) to estimate the effect of multimodal-": "rics. This phenomenon can be attributed to the fact that audio"
        },
        {
          "Comparative Systems": "the RNN seq2seq architecture is used to obtain the results.",
          "method (MAF-TAV) to estimate the effect of multimodal-": ""
        },
        {
          "Comparative Systems": "",
          "method (MAF-TAV) to estimate the effect of multimodal-": "alone may cause confusion while understanding sarcasm,"
        },
        {
          "Comparative Systems": "Transformer (Vaswani et al. 2017): Explanations are gener-",
          "method (MAF-TAV) to estimate the effect of multimodal-": ""
        },
        {
          "Comparative Systems": "",
          "method (MAF-TAV) to estimate the effect of multimodal-": "and visual hints may help in such times (Case III\nin Fig-"
        },
        {
          "Comparative Systems": "ated using the vanilla Transformer encoder-decoder model.",
          "method (MAF-TAV) to estimate the effect of multimodal-": ""
        },
        {
          "Comparative Systems": "",
          "method (MAF-TAV) to estimate the effect of multimodal-": "ure 1). Thereby, improving the visual feature representations"
        },
        {
          "Comparative Systems": "Pointer Generator Network (PGN)\n(See, Liu, and Man-",
          "method (MAF-TAV) to estimate the effect of multimodal-": ""
        },
        {
          "Comparative Systems": "",
          "method (MAF-TAV) to estimate the effect of multimodal-": "can be one of\nthe future directions. Finally, when we add"
        },
        {
          "Comparative Systems": "ning 2017): A combination of generation and copying mech-",
          "method (MAF-TAV) to estimate the effect of multimodal-": ""
        },
        {
          "Comparative Systems": "",
          "method (MAF-TAV) to estimate the effect of multimodal-": "all multimodal signals together, we observe the best perfor-"
        },
        {
          "Comparative Systems": "anisms is used in this seq2seq architecture. BART (Lewis",
          "method (MAF-TAV) to estimate the effect of multimodal-": ""
        },
        {
          "Comparative Systems": "",
          "method (MAF-TAV) to estimate the effect of multimodal-": "mance yet with an average increase of further ∼ 1% across"
        },
        {
          "Comparative Systems": "et al. 2020): We use the base version of this denoising au-",
          "method (MAF-TAV) to estimate the effect of multimodal-": ""
        },
        {
          "Comparative Systems": "",
          "method (MAF-TAV) to estimate the effect of multimodal-": "majority metrics."
        },
        {
          "Comparative Systems": "toencoder model. It has a bidirectional encoder with an auto-",
          "method (MAF-TAV) to estimate the effect of multimodal-": ""
        },
        {
          "Comparative Systems": "regressive left-to-right decoder built on standard machine",
          "method (MAF-TAV) to estimate the effect of multimodal-": ""
        },
        {
          "Comparative Systems": "",
          "method (MAF-TAV) to estimate the effect of multimodal-": "Modality Spotlight:\nAs hypothesised, we obtain the best"
        },
        {
          "Comparative Systems": "translation architecture. mBART (Liu et al. 2020): Trained",
          "method (MAF-TAV) to estimate the effect of multimodal-": ""
        },
        {
          "Comparative Systems": "",
          "method (MAF-TAV) to estimate the effect of multimodal-": "performance for sarcasm understanding when all\nthe three"
        },
        {
          "Comparative Systems": "on multiple large-scale monolingual corpora, mBART fol-",
          "method (MAF-TAV) to estimate the effect of multimodal-": ""
        },
        {
          "Comparative Systems": "",
          "method (MAF-TAV) to estimate the effect of multimodal-": "modalities are used in tandem. However, earlier methods"
        },
        {
          "Comparative Systems": "lows the same objective and architecture as BART3.",
          "method (MAF-TAV) to estimate the effect of multimodal-": ""
        },
        {
          "Comparative Systems": "",
          "method (MAF-TAV) to estimate the effect of multimodal-": "for SED provided limelight\nto only textual\nrepresentations"
        },
        {
          "Comparative Systems": "",
          "method (MAF-TAV) to estimate the effect of multimodal-": "(Kumar et al. 2022). We argue that especially in the case"
        },
        {
          "Comparative Systems": "Results",
          "method (MAF-TAV) to estimate the effect of multimodal-": ""
        },
        {
          "Comparative Systems": "",
          "method (MAF-TAV) to estimate the effect of multimodal-": "of\nsarcasm, multimodal\nsignals\nsuch as\naudio and video"
        },
        {
          "Comparative Systems": "Textual:\nTable 2 shows the results obtained when textual",
          "method (MAF-TAV) to estimate the effect of multimodal-": "might play the principal\nrole in many instances. To com-"
        },
        {
          "Comparative Systems": "systems are used to obtain the generated explanations. We",
          "method (MAF-TAV) to estimate the effect of multimodal-": "prehend this rotating importance of modalities, we use the"
        },
        {
          "Comparative Systems": "notice that while PGN delivers us with the least performance",
          "method (MAF-TAV) to estimate the effect of multimodal-": "spotlight module that aims to treat each modality as the pri-"
        },
        {
          "Comparative Systems": "across most metrics, BART-based representations outper-",
          "method (MAF-TAV) to estimate the effect of multimodal-": "mary modality while calculating the ﬁnal representation. We"
        },
        {
          "Comparative Systems": "form the rest by providing the best performance across the",
          "method (MAF-TAV) to estimate the effect of multimodal-": "observe an increase of ∼ 2% across all evaluation metrics as"
        },
        {
          "Comparative Systems": "majority of all evaluation metrics.",
          "method (MAF-TAV) to estimate the effect of multimodal-": "shown in Table 2. These results directly support our hypoth-"
        },
        {
          "Comparative Systems": "",
          "method (MAF-TAV) to estimate the effect of multimodal-": "esis of the effect of multimodality in sarcasm analysis."
        },
        {
          "Comparative Systems": "Mode\nModel\nR1\nR2\nRL\nB1\nB2\nB3\nB4\nM",
          "method (MAF-TAV) to estimate the effect of multimodal-": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 2: exhibits that while theaddition of acoustic sig-",
      "data": [
        {
          "form the rest by providing the best performance across the": "majority of all evaluation metrics.",
          "observe an increase of ∼ 2% across all evaluation metrics as": "shown in Table 2. These results directly support our hypoth-"
        },
        {
          "form the rest by providing the best performance across the": "",
          "observe an increase of ∼ 2% across all evaluation metrics as": "esis of the effect of multimodality in sarcasm analysis."
        },
        {
          "form the rest by providing the best performance across the": "Model",
          "observe an increase of ∼ 2% across all evaluation metrics as": ""
        },
        {
          "form the rest by providing the best performance across the": "RNN",
          "observe an increase of ∼ 2% across all evaluation metrics as": "R1"
        },
        {
          "form the rest by providing the best performance across the": "Transformer",
          "observe an increase of ∼ 2% across all evaluation metrics as": ""
        },
        {
          "form the rest by providing the best performance across the": "",
          "observe an increase of ∼ 2% across all evaluation metrics as": "36.88"
        },
        {
          "form the rest by providing the best performance across the": "PGN",
          "observe an increase of ∼ 2% across all evaluation metrics as": ""
        },
        {
          "form the rest by providing the best performance across the": "",
          "observe an increase of ∼ 2% across all evaluation metrics as": "17.22"
        },
        {
          "form the rest by providing the best performance across the": "mBART",
          "observe an increase of ∼ 2% across all evaluation metrics as": ""
        },
        {
          "form the rest by providing the best performance across the": "",
          "observe an increase of ∼ 2% across all evaluation metrics as": "36.43"
        },
        {
          "form the rest by providing the best performance across the": "BART",
          "observe an increase of ∼ 2% across all evaluation metrics as": ""
        },
        {
          "form the rest by providing the best performance across the": "",
          "observe an increase of ∼ 2% across all evaluation metrics as": "36.37"
        },
        {
          "form the rest by providing the best performance across the": "MAF-TA",
          "observe an increase of ∼ 2% across all evaluation metrics as": "39.69"
        },
        {
          "form the rest by providing the best performance across the": "MAF-TV",
          "observe an increase of ∼ 2% across all evaluation metrics as": "40.88"
        },
        {
          "form the rest by providing the best performance across the": "MAF-TAV",
          "observe an increase of ∼ 2% across all evaluation metrics as": "42.17\n+ MS (MOSES)"
        },
        {
          "form the rest by providing the best performance across the": "MOSES-TA",
          "observe an increase of ∼ 2% across all evaluation metrics as": ""
        },
        {
          "form the rest by providing the best performance across the": "MOSES-TV",
          "observe an increase of ∼ 2% across all evaluation metrics as": ""
        },
        {
          "form the rest by providing the best performance across the": "MOSES-TAV",
          "observe an increase of ∼ 2% across all evaluation metrics as": "Table 3: Ablation results on MOSES (DPA: Dot Product At-"
        },
        {
          "form the rest by providing the best performance across the": "MOSES",
          "observe an increase of ∼ 2% across all evaluation metrics as": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 6: As illustrated, the proposed MOSES",
      "data": [
        {
          "Dialogue": "KISMI: Bas na Sahil bhai, meri ﬁrki kheech rahe ho na!? (Enough",
          "Ground Truth": "Sahil Kismi ko taunt maarta hai kyuki use",
          "MAF": "Sahil Kismi ko taunt maarta hai ki use",
          "MOSES": "Sahil Kismi ko taunt maarta hai kyuki use"
        },
        {
          "Dialogue": "brother Sahil, are you teasing me?!)",
          "Ground Truth": "rail gaadi ki awaaj sunni hai. (Sahil taunts",
          "MAF": "pasand nahi. (Sahil taunts Kismi that he",
          "MOSES": "rail gaadi ki awaaj sunni hai. (Sahil taunts"
        },
        {
          "Dialogue": "SAHIL: Nahi, nahi, kya hai ki, mere CD ki collection mein na, ye train",
          "Ground Truth": "Kismi that she wants to hear the sound of a",
          "MAF": "doesn’t like)",
          "MOSES": "Kismi that she wants to hear the sound of a"
        },
        {
          "Dialogue": "ke awaaj vali CD nahi hai... (No no, see I don’t have train’s sound in",
          "Ground Truth": "train)",
          "MAF": "",
          "MOSES": "train)"
        },
        {
          "Dialogue": "my CD collection...)",
          "Ground Truth": "",
          "MAF": "",
          "MOSES": ""
        },
        {
          "Dialogue": "MADHUSUDHAN: Kitne saal ka ho jaaega vo? (How old will he be?)",
          "Ground Truth": "Indravardhan Madhusudan ke questions se",
          "MAF": "Indravardhan Madhusudan ke behare pan se",
          "MOSES": "Indravardhan Madhusudan se pareshaan hai."
        },
        {
          "Dialogue": "(What\nINDRAVARDHAN: Aap ko ka lena dena, panchaanyati\nlaal!",
          "Ground Truth": "pareshaan hai. (Indravardhan is irritated by",
          "MAF": "pareshaan hai. (Indravardhan is tired of",
          "MOSES": "(Indravardhan is tired of Madhusudhan)"
        },
        {
          "Dialogue": "does it have to do with you, Mr. Poke-a-nose?)",
          "Ground Truth": "Madhusudhan’s questions)",
          "MAF": "Madhusudhan’s deafness)",
          "MOSES": ""
        },
        {
          "Dialogue": "(Say hello to Tommy the\nMONISHA: Say hello to Tommy the dog.",
          "Ground Truth": "Maya monisha ko tana marti hai kyunki usne",
          "MAF": "Maya kehti hai ki uske kutte ka naam tommy",
          "MOSES": "Maya taunts monisha kyunki usne apne kutte"
        },
        {
          "Dialogue": "dog.)",
          "Ground Truth": "apne kutte ka naam tommy the dog rakha",
          "MAF": "the dog rakha hai. (Maya says that her dog’s",
          "MOSES": "ka naam tommy the dog rakha hai. (Maya"
        },
        {
          "Dialogue": "MAYA: Tumne iss kutte ka naam Tommy the dog rakha? (Did you",
          "Ground Truth": "hai. (Maya taunts Monisha on naming her",
          "MAF": "name is Tommy the dog.)",
          "MOSES": "taunts Monisha that she has named her dog"
        },
        {
          "Dialogue": "name your dog Tommy the dog?)",
          "Ground Truth": "dog Tommy the dog.)",
          "MAF": "",
          "MOSES": "Tommy the dog.)"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 6: As illustrated, the proposed MOSES",
      "data": [
        {
          "does it have to do with you, Mr. Poke-a-nose?)\nMadhusudhan’s questions)": "(Say hello to Tommy the\nMONISHA: Say hello to Tommy the dog.\nMaya monisha ko tana marti hai kyunki usne",
          "Madhusudhan’s deafness)": "Maya taunts monisha kyunki usne apne kutte\nMaya kehti hai ki uske kutte ka naam tommy"
        },
        {
          "does it have to do with you, Mr. Poke-a-nose?)\nMadhusudhan’s questions)": "dog.)\napne kutte ka naam tommy the dog rakha",
          "Madhusudhan’s deafness)": "ka naam tommy the dog rakha hai. (Maya\nthe dog rakha hai. (Maya says that her dog’s"
        },
        {
          "does it have to do with you, Mr. Poke-a-nose?)\nMadhusudhan’s questions)": "MAYA: Tumne iss kutte ka naam Tommy the dog rakha? (Did you\nhai. (Maya taunts Monisha on naming her",
          "Madhusudhan’s deafness)": "name is Tommy the dog.)\ntaunts Monisha that she has named her dog"
        },
        {
          "does it have to do with you, Mr. Poke-a-nose?)\nMadhusudhan’s questions)": "name your dog Tommy the dog?)\ndog Tommy the dog.)",
          "Madhusudhan’s deafness)": "Tommy the dog.)"
        },
        {
          "does it have to do with you, Mr. Poke-a-nose?)\nMadhusudhan’s questions)": "Table 4: Actual and generated explanations for sample dialogues from test set. The last utterance is the sarcastic utterance for",
          "Madhusudhan’s deafness)": ""
        },
        {
          "does it have to do with you, Mr. Poke-a-nose?)\nMadhusudhan’s questions)": "each dialogue.",
          "Madhusudhan’s deafness)": ""
        },
        {
          "does it have to do with you, Mr. Poke-a-nose?)\nMadhusudhan’s questions)": "the model and observe another performance boost across",
          "Madhusudhan’s deafness)": "evaluate the generated explanations (on a scale of 1 to 5) on"
        },
        {
          "does it have to do with you, Mr. Poke-a-nose?)\nMadhusudhan’s questions)": "majority metrics (∼ 1%), suggesting that we can obtain bet-",
          "Madhusudhan’s deafness)": "the following basis:"
        },
        {
          "does it have to do with you, Mr. Poke-a-nose?)\nMadhusudhan’s questions)": "ter code-mixed representations by reducing the spelling am-",
          "Madhusudhan’s deafness)": ""
        },
        {
          "does it have to do with you, Mr. Poke-a-nose?)\nMadhusudhan’s questions)": "",
          "Madhusudhan’s deafness)": "• Coherence: Checks the generated explanation for cor-"
        },
        {
          "does it have to do with you, Mr. Poke-a-nose?)\nMadhusudhan’s questions)": "biguities. Finally, our entire model with modality spotlight",
          "Madhusudhan’s deafness)": ""
        },
        {
          "does it have to do with you, Mr. Poke-a-nose?)\nMadhusudhan’s questions)": "",
          "Madhusudhan’s deafness)": "rect structure and grammar."
        },
        {
          "does it have to do with you, Mr. Poke-a-nose?)\nMadhusudhan’s questions)": "included produces the best performance, verifying the nec-",
          "Madhusudhan’s deafness)": ""
        },
        {
          "does it have to do with you, Mr. Poke-a-nose?)\nMadhusudhan’s questions)": "",
          "Madhusudhan’s deafness)": "• On topic: Measures the extent\nto which the generated"
        },
        {
          "does it have to do with you, Mr. Poke-a-nose?)\nMadhusudhan’s questions)": "essary use of each module discussed.",
          "Madhusudhan’s deafness)": ""
        },
        {
          "does it have to do with you, Mr. Poke-a-nose?)\nMadhusudhan’s questions)": "",
          "Madhusudhan’s deafness)": "explanation revolves around the dialogue topic."
        },
        {
          "does it have to do with you, Mr. Poke-a-nose?)\nMadhusudhan’s questions)": "",
          "Madhusudhan’s deafness)": "• Capturing sarcasm: Estimates the level of emitted sar-"
        },
        {
          "does it have to do with you, Mr. Poke-a-nose?)\nMadhusudhan’s questions)": "Result Analysis",
          "Madhusudhan’s deafness)": ""
        },
        {
          "does it have to do with you, Mr. Poke-a-nose?)\nMadhusudhan’s questions)": "",
          "Madhusudhan’s deafness)": "casm being captured in the generated output."
        },
        {
          "does it have to do with you, Mr. Poke-a-nose?)\nMadhusudhan’s questions)": "Quantitative Analysis.\nMOSES is evaluated on its abil-",
          "Madhusudhan’s deafness)": ""
        },
        {
          "does it have to do with you, Mr. Poke-a-nose?)\nMadhusudhan’s questions)": "",
          "Madhusudhan’s deafness)": "We show the average score for the human evaluation pa-"
        },
        {
          "does it have to do with you, Mr. Poke-a-nose?)\nMadhusudhan’s questions)": "ity to capture sarcasm source and target\nin the generated",
          "Madhusudhan’s deafness)": ""
        },
        {
          "does it have to do with you, Mr. Poke-a-nose?)\nMadhusudhan’s questions)": "",
          "Madhusudhan’s deafness)": "rameters\nin Table 6. As\nillustrated,\nthe proposed MOSES"
        },
        {
          "does it have to do with you, Mr. Poke-a-nose?)\nMadhusudhan’s questions)": "explanations. We\ncompare MOSES with mBART, BART,",
          "Madhusudhan’s deafness)": ""
        },
        {
          "does it have to do with you, Mr. Poke-a-nose?)\nMadhusudhan’s questions)": "",
          "Madhusudhan’s deafness)": "model exhibits more coherent, on topic, and sarcasm related"
        },
        {
          "does it have to do with you, Mr. Poke-a-nose?)\nMadhusudhan’s questions)": "and MAF. Table 5 shows that BART performs better\nthan",
          "Madhusudhan’s deafness)": ""
        },
        {
          "does it have to do with you, Mr. Poke-a-nose?)\nMadhusudhan’s questions)": "",
          "Madhusudhan’s deafness)": "explanations. However,\nthere is\nstill a scope for\nimprove-"
        },
        {
          "does it have to do with you, Mr. Poke-a-nose?)\nMadhusudhan’s questions)": "mBART for both source and target detection. The inclusion",
          "Madhusudhan’s deafness)": ""
        },
        {
          "does it have to do with you, Mr. Poke-a-nose?)\nMadhusudhan’s questions)": "",
          "Madhusudhan’s deafness)": "ment, which can be taken up as future work."
        },
        {
          "does it have to do with you, Mr. Poke-a-nose?)\nMadhusudhan’s questions)": "of multimodal signals, even without pronunciation embed-",
          "Madhusudhan’s deafness)": ""
        },
        {
          "does it have to do with you, Mr. Poke-a-nose?)\nMadhusudhan’s questions)": "dings and modality spotlight,\nimproves\nthe source identi-",
          "Madhusudhan’s deafness)": ""
        },
        {
          "does it have to do with you, Mr. Poke-a-nose?)\nMadhusudhan’s questions)": "ﬁcation performance by ∼ 14%. MOSES is able to detect",
          "Madhusudhan’s deafness)": "Coherency\nOn topic\nCapturing sarcasm"
        },
        {
          "does it have to do with you, Mr. Poke-a-nose?)\nMadhusudhan’s questions)": "the sarcasm source most efﬁciently, resulting in an improve-",
          "Madhusudhan’s deafness)": "mBART\n2.57\n2.66\n2.15"
        },
        {
          "does it have to do with you, Mr. Poke-a-nose?)\nMadhusudhan’s questions)": "",
          "Madhusudhan’s deafness)": "BART\n2.73\n2.56\n2.18"
        },
        {
          "does it have to do with you, Mr. Poke-a-nose?)\nMadhusudhan’s questions)": "ment of ∼ 4% over the next best result. Consequently, we",
          "Madhusudhan’s deafness)": ""
        },
        {
          "does it have to do with you, Mr. Poke-a-nose?)\nMadhusudhan’s questions)": "",
          "Madhusudhan’s deafness)": "MAF\n3.03\n3.11\n2.77"
        },
        {
          "does it have to do with you, Mr. Poke-a-nose?)\nMadhusudhan’s questions)": "can relate the presence of multimodal capabilities\nto cap-",
          "Madhusudhan’s deafness)": ""
        },
        {
          "does it have to do with you, Mr. Poke-a-nose?)\nMadhusudhan’s questions)": "",
          "Madhusudhan’s deafness)": "MOSES\n3.96\n3.27\n3.10"
        },
        {
          "does it have to do with you, Mr. Poke-a-nose?)\nMadhusudhan’s questions)": "ture speaker-speciﬁc peculiarities more efﬁciently, resulting",
          "Madhusudhan’s deafness)": ""
        },
        {
          "does it have to do with you, Mr. Poke-a-nose?)\nMadhusudhan’s questions)": "in better source/target identiﬁcation.",
          "Madhusudhan’s deafness)": ""
        },
        {
          "does it have to do with you, Mr. Poke-a-nose?)\nMadhusudhan’s questions)": "",
          "Madhusudhan’s deafness)": "Table 6: Human evaluation statistics – comparing different"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 7: illustrates the results we obtain for all the settings highertruepositiveratewithadecreasedfalsepositiveand",
      "data": [
        {
          "Table 7 illustrates the results we obtain for all\nthe settings": "for MOSES and the best baseline, MAF. As can be seen,",
          "higher true positive rate with a decreased false positive and": "false negative rates for majority of"
        },
        {
          "Table 7 illustrates the results we obtain for all\nthe settings": "RoBERTa obtains 62% F1 score when we do not use any",
          "higher true positive rate with a decreased false positive and": "casm, humour, and emotion labels."
        },
        {
          "Table 7 illustrates the results we obtain for all\nthe settings": "explanations. However, with the use of the generated expla-",
          "higher true positive rate with a decreased false positive and": ""
        },
        {
          "Table 7 illustrates the results we obtain for all\nthe settings": "nations by MOSES during the train time along with the in-",
          "higher true positive rate with a decreased false positive and": ""
        },
        {
          "Table 7 illustrates the results we obtain for all\nthe settings": "put dialogues, we obtain an improvement of 6% F1-score.",
          "higher true positive rate with a decreased false positive and": ""
        },
        {
          "Table 7 illustrates the results we obtain for all\nthe settings": "On the other hand,\nthe best performance is achieved by the",
          "higher true positive rate with a decreased false positive and": "NS"
        },
        {
          "Table 7 illustrates the results we obtain for all\nthe settings": "",
          "higher true positive rate with a decreased false positive and": "S"
        },
        {
          "Table 7 illustrates the results we obtain for all\nthe settings": "last case, where the input\ninstances are appended with their",
          "higher true positive rate with a decreased false positive and": ""
        },
        {
          "Table 7 illustrates the results we obtain for all\nthe settings": "corresponding explanations both at\nthe train and test\ntime,",
          "higher true positive rate with a decreased false positive and": ""
        },
        {
          "Table 7 illustrates the results we obtain for all\nthe settings": "with an increase of 8% F1-score. Consistent\nto the results",
          "higher true positive rate with a decreased false positive and": ""
        },
        {
          "Table 7 illustrates the results we obtain for all\nthe settings": "obtained by MOSES’s generation, MAF also reports an im-",
          "higher true positive rate with a decreased false positive and": ""
        },
        {
          "Table 7 illustrates the results we obtain for all\nthe settings": "proved performance over no explanation model. However,",
          "higher true positive rate with a decreased false positive and": ""
        },
        {
          "Table 7 illustrates the results we obtain for all\nthe settings": "the improvement shown by MAF is not at par with the im-",
          "higher true positive rate with a decreased false positive and": ""
        },
        {
          "Table 7 illustrates the results we obtain for all\nthe settings": "provement obtained by MOSES. These results directly sup-",
          "higher true positive rate with a decreased false positive and": ""
        },
        {
          "Table 7 illustrates the results we obtain for all\nthe settings": "",
          "higher true positive rate with a decreased false positive and": ""
        },
        {
          "Table 7 illustrates the results we obtain for all\nthe settings": "port our hypothesis that utterance explanations can assist an",
          "higher true positive rate with a decreased false positive and": ""
        },
        {
          "Table 7 illustrates the results we obtain for all\nthe settings": "",
          "higher true positive rate with a decreased false positive and": ""
        },
        {
          "Table 7 illustrates the results we obtain for all\nthe settings": "efﬁcient detection of sarcasm in the input instances.",
          "higher true positive rate with a decreased false positive and": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 7: illustrates the results we obtain for all the settings highertruepositiveratewithadecreasedfalsepositiveand",
      "data": [
        {
          "port our hypothesis that utterance explanations can assist an": ""
        },
        {
          "port our hypothesis that utterance explanations can assist an": "efﬁcient detection of sarcasm in the input instances."
        },
        {
          "port our hypothesis that utterance explanations can assist an": ""
        },
        {
          "port our hypothesis that utterance explanations can assist an": ""
        },
        {
          "port our hypothesis that utterance explanations can assist an": "Model"
        },
        {
          "port our hypothesis that utterance explanations can assist an": ""
        },
        {
          "port our hypothesis that utterance explanations can assist an": "None"
        },
        {
          "port our hypothesis that utterance explanations can assist an": ""
        },
        {
          "port our hypothesis that utterance explanations can assist an": "MAF"
        },
        {
          "port our hypothesis that utterance explanations can assist an": ""
        },
        {
          "port our hypothesis that utterance explanations can assist an": ""
        },
        {
          "port our hypothesis that utterance explanations can assist an": "MOSES"
        },
        {
          "port our hypothesis that utterance explanations can assist an": ""
        },
        {
          "port our hypothesis that utterance explanations can assist an": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "Role of Conversation Context for Sarcasm Detection in On-"
        },
        {
          "References": "Abu Farha, I.; and Magdy, W. 2020. From Arabic Sentiment",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": ""
        },
        {
          "References": "",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "line Interactions. In Proceedings of the 18th Annual SIGdial"
        },
        {
          "References": "Analysis to Sarcasm Detection: The ArSarcasm Dataset.\nIn",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": ""
        },
        {
          "References": "",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "Meeting on Discourse and Dialogue, 186–196."
        },
        {
          "References": "Proceedings of\nthe 4th Workshop on Open-Source Arabic",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": ""
        },
        {
          "References": "Corpora and Processing Tools, with a Shared Task on Of-",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "Hara, K.; Kataoka, H.; and Satoh, Y. 2018. Can Spatiotem-"
        },
        {
          "References": "fensive Language Detection, 32–39.",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "poral 3D CNNs Retrace the History of 2D CNNs and Ima-"
        },
        {
          "References": "",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "geNet? In Proceedings of the IEEE Conference on Computer"
        },
        {
          "References": "Bedi, M.; Kumar, S.; Akhtar, M. S.; and Chakraborty, T.",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": ""
        },
        {
          "References": "",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "Vision and Pattern Recognition (CVPR), 6546–6555."
        },
        {
          "References": "2021. Multi-modal Sarcasm Detection and Humor Classi-",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": ""
        },
        {
          "References": "IEEE Transactions\nﬁcation in Code-mixed Conversations.",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "Hasan, M. K.; Lee, S.; Rahman, W.; Zadeh, A.; Mihalcea,"
        },
        {
          "References": "on Affective Computing.",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "R.; Morency, L.-P.; and Hoque, E. 2021.\nHumor Knowl-"
        },
        {
          "References": "",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "edge Enriched Transformer\nfor Understanding Multimodal"
        },
        {
          "References": "Bharti, S. K.; Sathya Babu, K.;\nand\nJena, S. K.\n2017.",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": ""
        },
        {
          "References": "",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "Proceedings of\nthe AAAI Conference on Artiﬁcial\nHumor."
        },
        {
          "References": "Harnessing Online News\nfor Sarcasm Detection in Hindi",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": ""
        },
        {
          "References": "",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "Intelligence, 12972–12980."
        },
        {
          "References": "Tweets.\nIn Shankar, B. U.; Ghosh, K.; Mandal, D. P.; Ray,",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": ""
        },
        {
          "References": "S. S.; Zhang, D.; and Pal, S. K., eds., Pattern Recognition",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "Joshi, A.; Bhattacharyya, P.; and Carman, M. J. 2017. Au-"
        },
        {
          "References": "and Machine Intelligence, 679–686.",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "tomatic Sarcasm Detection: A Survey. ACM Comput. Surv.,"
        },
        {
          "References": "",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "1–22."
        },
        {
          "References": "Cai, Y.; Cai, H.; and Wan, X. 2019. Multi-Modal Sarcasm",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": ""
        },
        {
          "References": "Detection in Twitter with Hierarchical Fusion Model.\nIn",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "Joshi, A.; Sharma, V.; and Bhattacharyya, P. 2015. Harness-"
        },
        {
          "References": "Proceedings of\nthe 57th Annual Meeting of\nthe Association",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "ing Context Incongruity for Sarcasm Detection. In Proceed-"
        },
        {
          "References": "for Computational Linguistics, 2506–2515.",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "ings of the 53rd Annual Meeting of the Association for Com-"
        },
        {
          "References": "",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "putational Linguistics and the 7th International Joint Con-"
        },
        {
          "References": "Castro, S.; Hazarika, D.; P´erez-Rosas, V.; Zimmermann, R.;",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": ""
        },
        {
          "References": "",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "ference on Natural Language Processing (Volume 2: Short"
        },
        {
          "References": "Mihalcea, R.; and Poria, S. 2019. Towards Multimodal Sar-",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": ""
        },
        {
          "References": "",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "Papers), 757–762."
        },
        {
          "References": "casm Detection (An Obviously\nPerfect Paper). In Proceed-",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": ""
        },
        {
          "References": "ings of the 57th Annual Meeting of the Association for Com-",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "Kreuz, R.; and Caucci, G. 2007. Lexical Inﬂuences on the"
        },
        {
          "References": "putational Linguistics, 4619–4629.",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "the Workshop on\nPerception of Sarcasm.\nIn Proceedings of"
        },
        {
          "References": "",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "Computational Approaches to Figurative Language, 1–4."
        },
        {
          "References": "Chakrabarty, T.; Ghosh, D.; Muresan, S.; and Peng, N. 2020.",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": ""
        },
        {
          "References": "Rˆ3: Reverse, Retrieve, and Rank for Sarcasm Generation",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "Krippendorff, K. 2011.\nComputing Krippendorff’s alpha-"
        },
        {
          "References": "with Commonsense Knowledge.\nIn Proceedings of the 58th",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "reliability."
        },
        {
          "References": "Annual Meeting of\nthe Association for Computational Lin-",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": ""
        },
        {
          "References": "",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "Kumar, S.; Kulkarni, A.; Akhtar, M. S.; and Chakraborty,"
        },
        {
          "References": "guistics, 7976–7986.",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "T. 2022. When did you become so smart, oh wise one?!"
        },
        {
          "References": "Chauhan, D. S.; S R, D.; Ekbal, A.; and Bhattacharyya, P.",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "Sarcasm Explanation in Multi-modal Multi-party Dialogues."
        },
        {
          "References": "2020. Sentiment and Emotion help Sarcasm? A Multi-task",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "the 60th Annual Meeting of\nthe Associa-\nIn Proceedings of"
        },
        {
          "References": "Learning Framework for Multi-Modal Sarcasm, Sentiment",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "tion for Computational Linguistics (Volume 1: Long Papers),"
        },
        {
          "References": "the 58th Annual\nand Emotion Analysis.\nIn Proceedings of",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "5956–5968."
        },
        {
          "References": "Meeting of\nthe Association for Computational Linguistics,",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": ""
        },
        {
          "References": "",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "Lewis, M.; Liu, Y.; Goyal, N.; Ghazvininejad, M.; Mo-"
        },
        {
          "References": "4351–4360.",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "hamed, A.; Levy, O.; Stoyanov, V.;\nand Zettlemoyer, L."
        },
        {
          "References": "Cignarella, A. T.; Frenda, S.; Basile, V.; Bosco, C.; Patti, V.;",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "2020. BART: Denoising Sequence-to-Sequence Pre-training"
        },
        {
          "References": "Rosso, P.; et al. 2018. Overview of the evalita 2018 task on",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "for Natural Language Generation, Translation, and Compre-"
        },
        {
          "References": "irony detection in italian tweets (ironita).\nIn Sixth Evalua-",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "the 58th Annual Meeting of\nthe\nhension.\nIn Proceedings of"
        },
        {
          "References": "tion Campaign of Natural Language Processing and Speech",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "Association for Computational Linguistics, 7871–7880."
        },
        {
          "References": "Tools for Italian (EVALITA 2018), 1–6. CEUR-WS.",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "Lin, C.-Y. 2004. ROUGE: A Package for Automatic Evalu-"
        },
        {
          "References": "Denkowski, M.; and Lavie, A. 2014. Meteor Universal: Lan-",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "ation of Summaries.\nIn Text Summarization Branches Out,"
        },
        {
          "References": "guage Speciﬁc Translation Evaluation for Any Target Lan-",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "74–81."
        },
        {
          "References": "guage.\nIn Proceedings of the Ninth Workshop on Statistical",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "Liu, Y.; Gu, J.; Goyal, N.; Li, X.; Edunov, S.; Ghazvinine-"
        },
        {
          "References": "Machine Translation, 376–380.",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "jad, M.; Lewis, M.; and Zettlemoyer, L. 2020. Multilin-"
        },
        {
          "References": "Dubey, A.;\nJoshi, A.; and Bhattacharyya, P. 2019.\nDeep",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "gual Denoising Pre-training for Neural Machine Translation."
        },
        {
          "References": "Models for Converting Sarcastic Utterances into Their Non",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "Transactions of the Association for Computational Linguis-"
        },
        {
          "References": "the ACM India\nSarcastic Interpretation.\nIn Proceedings of",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "tics, 726–742."
        },
        {
          "References": "Joint\nInternational Conference on Data Science and Man-",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": ""
        },
        {
          "References": "",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "Liu, Y.; Ott, M.; Goyal, N.; Du,\nJ.;\nJoshi, M.; Chen, D.;"
        },
        {
          "References": "agement of Data, 289–292.",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "Levy, O.; Lewis, M.; Zettlemoyer, L.;\nand Stoyanov, V."
        },
        {
          "References": "Ekman, P. 1992. An argument for basic emotions. Cognition",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "2019.\nRoberta: A robustly optimized bert pretraining ap-"
        },
        {
          "References": "and Emotion, 169–200.",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "proach. arXiv preprint arXiv:1907.11692."
        },
        {
          "References": "Eyben, F.; Scherer, K. R.; Schuller, B. W.; Sundberg,\nJ.;",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "Mishra, A.; Tater, T.; and Sankaranarayanan, K. 2019.\nA"
        },
        {
          "References": "Andr´e, E.; Busso, C.; Devillers, L. Y.; Epps, J.; Laukka, P.;",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "Modular Architecture\nfor Unsupervised Sarcasm Genera-"
        },
        {
          "References": "Narayanan, S. S.; and Truong, K. P. 2016. The Geneva Min-",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "the 2019 Conference on Empirical\ntion.\nIn Proceedings of"
        },
        {
          "References": "imalistic Acoustic Parameter Set (GeMAPS) for Voice Re-",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "Methods in Natural Language Processing and the 9th Inter-"
        },
        {
          "References": "IEEE Transactions on Af-\nsearch and Affective Computing.",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "national Joint Conference on Natural Language Processing"
        },
        {
          "References": "fective Computing, 190–202.",
          "Ghosh, D.; Richard Fabbri, A.; and Muresan, S. 2017. The": "(EMNLP-IJCNLP), 6144–6154."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "vidual differences in the processing of written sarcasm and",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "casm Detection with Self-Matching Networks\nand Low-"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "Journal of Ex-\nmetaphor: Evidence from eye movements.",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "Rank Bilinear Pooling.\nIn The World Wide Web Conference,"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "perimental Psychology: Learning, Memory, and Cognition,",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "2115–2124."
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "42(3): 433–450.",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": ""
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "Xu, N.; Zeng, Z.; and Mao, W. 2020. Reasoning with Multi-"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "Ortega-Bueno, R.; Rangel, F.; Hern´andez Farıas, D.; Rosso,",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "modal Sarcastic Tweets via Modeling Cross-Modality Con-"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "P.; Montes-y G´omez, M.; and Medina Pagola, J. E. 2019.",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "the 58th\ntrast and Semantic Association.\nIn Proceedings of"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "Overview of\nthe task on irony detection in Spanish vari-",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "Annual Meeting of\nthe Association for Computational Lin-"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "the Iberian languages evaluation\nants.\nIn Proceedings of",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "guistics, 3777–3786."
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "forum (IberLEF 2019), co-located with 34th conference of",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": ""
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "Yang, B.; Li, J.; Wong, D. F.; Chao, L. S.; Wang, X.; and"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "the Spanish Society for natural\nlanguage processing (SE-",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": ""
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "Tu, Z. 2019. Context-Aware Self-Attention Networks. Pro-"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "PLN 2019). CEUR-WS. org, 229–256.",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": ""
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "ceedings of\nthe AAAI Conference on Artiﬁcial Intelligence,"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "Pan, H.; Lin, Z.; Fu, P.; Qi, Y.; and Wang, W. 2020. Mod-",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "387–394."
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "eling Intra and Inter-modality Incongruity for Multi-Modal",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": ""
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "Sarcasm Detection.\nIn Findings of the Association for Com-",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": ""
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "Appendix"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "putational Linguistics: EMNLP 2020, 1383–1392.",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": ""
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "Dataset"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": ""
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "Bleu: a Method for Automatic Evaluation of Machine Trans-",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "Curation of sWITS, hWITS, and eWITS\nAll affects such"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "the 40th Annual Meeting of\nthe\nlation.\nIn Proceedings of",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "as sarcasm, humour, and emotions work in tandem to unveil"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "Association for Computational Linguistics, 311–318.",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "the true connotation of an utterance in a dialogue. Thereby, a"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "comprehensive justiﬁcation for one component should help"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "Peled, L.; and Reichart, R. 2017. Sarcasm SIGN: Interpret-",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": ""
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "in improving the understanding of the others. In this work,"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "ing Sarcasm with Sentiment Based Monolingual Machine",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": ""
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "we deal with the task of Sarcasm Explanation in Dialogues"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "Translation.\nIn Proceedings of",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": ""
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "the Association for Computational Linguistics\n(Volume 1:",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "(SED)\n(Kumar et al. 2022) where the aim is to generate a"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "natural language explanation for a given multimodal sarcas-"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "Long Papers), 1690–1700.",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": ""
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "tic conversation. We propose MOSES, a deep neural archi-"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "Roberts, R. M.; and Kreuz, R. J. 1994. Why Do People Use",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": ""
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "tecture with the aim to solve SED. To exhibit\nthe inﬂuence"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "Figurative Language?\nPsychological Science, 5(3): 159–",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": ""
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "of sarcasm on other affective components,\nlike humour and"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "163.",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": ""
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "emotions, we leverage the generated explanation by MOSES"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "See, A.; Liu, P. J.; and Manning, C. D. 2017. Get To The",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": ""
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "and augment\nthe\ninput\ninstances\nfor\naffective\ntasks with"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "Point: Summarization with Pointer-Generator Networks.\nIn",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": ""
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "them. We experiment with three affect understanding tasks"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "Proceedings of\nthe 55th Annual Meeting of\nthe Associa-",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": ""
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "in dialogues – sarcasm detection, humour identiﬁcation, and"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "tion for Computational Linguistics (Volume 1: Long Papers),",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": ""
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "emotion recognition. To perform appropriate experimenta-"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "1073–1083.",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": ""
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "tion for\nthe\ntasks with and without\nexplanations, we\nre-"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "Srivastava, H.; Varshney, V.; Kumari, S.; and Srivastava, S.",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "quire supporting datasets. Consequently, we curate sWITS,"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "2020. A Novel Hierarchical BERT Architecture for Sarcasm",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "hWITS, and eWITSfor\nthe task of sarcasm detection, hu-"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "the Second Workshop on Fig-\nDetection.\nIn Proceedings of",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "mour\nidentiﬁcation, and emotion recognition,\nrespectively,"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "urative Language Processing, 93–97. Association for Com-",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "from the WITS dataset (Kumar et al. 2022)."
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "putational Linguistics.",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": ""
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "sWITS:\nThe parent dataset, WITS contains sarcastic in-"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "Swami,\nS.; Khandelwal, A.;\nSingh, V.; Akhtar,\nS.\nS.;",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": ""
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "stances\nalong with\ntheir\nexplanations. Each\ninstance\nof"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "and\nShrivastava, M.\n2018.\nA corpus\nof\nenglish-hindi",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": ""
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "WITS contains a sequence of utterances where the last utter-"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "arXiv preprint\ncode-mixed tweets\nfor\nsarcasm detection.",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": ""
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "ance is sarcastic. However,\nfor\nthe sarcasm detection task,"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "arXiv:1805.11869.",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": ""
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "we need both sarcastic as well as non-sarcastic instances."
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "Tay, Y.; Luu, A. T.; Hui, S. C.; and Su, J. 2018. Reasoning",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "To create\nthe non-sarcastic\ninstances, we\nrandomly sam-"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "with Sarcasm by Reading In-Between. In Proceedings of the",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "ple utterances from the context of\nthe instances present\nin"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "56th Annual Meeting of\nthe Association for Computational",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "WITS. Figure 3 illustrates the process of creating sWITS"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "Linguistics (Volume 1: Long Papers), 1010–1020.",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "from WITS."
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "Tsur, O.; Davidov, D.; and Rappoport, A. 2010.\nICWSM",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": ""
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "hWITS:\nTo gauge the effect of sarcasm explanation on hu-"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "— A Great Catchy Name: Semi-Supervised Recognition of",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": ""
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "mour identiﬁcation, we will need instances with humour la-"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "Proceed-\nSarcastic Sentences in Online Product Reviews.",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": ""
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "bels. As a result, we explore the mapping existing between"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "ings of\nthe International AAAI Conference on Web and So-",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": ""
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "WITS and the MASAC dataset (Bedi et al. 2021). MASAC is"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "cial Media, 162–169.",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": ""
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "a multiparty, multimodal, code-mixed dialogue dataset col-"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "lected from an Indian TV series. It contains binary markers"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "L.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "identifying the presence of sarcasm and humour\nin all ut-"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "the 31st Inter-\ntention is All You Need.\nIn Proceedings of",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "terances. The WITS dataset\nis an extended version of\nthe"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "national Conference on Neural Information Processing Sys-",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "MASAC dataset where all\nthe sarcastic utterances are ap-"
        },
        {
          "Olkoniemi, H.; Ranta, H.; and Kaakinen, J. K. 2016.\nIndi-": "tems, 6000–6010.",
          "Xiong, T.; Zhang, P.; Zhu, H.; and Yang, Y. 2019.\nSar-": "pended with their corresponding natural\nlanguage explana-"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 11: is non-sarcastic and as shown, the",
      "data": [
        {
          "Context Speakers": "INDRAVARDHAN",
          "Context Utterances": "Accha suno Monisha tumhaare ghar mein been ya",
          "Target Speaker": "MAYA",
          "Target Sarcastic Utterance": "hogi? Monisha",
          "Explanation": "Maya Monisha ko tana marti hai safai ka dhyan"
        },
        {
          "Context Speakers": "",
          "Context Utterances": "aisa kuuch hain? (Listen Monisha, do you have a",
          "Target Speaker": "",
          "Target Sarcastic Utterance": "mushkil se rakhti hain to snake charmer waali been",
          "Explanation": "na rakhne ke liye. (Maya taunts Monisha for not"
        },
        {
          "Context Speakers": "",
          "Context Utterances": "ﬂute or something similar?)",
          "Target Speaker": "",
          "Target Sarcastic Utterance": "kaha se rakhegi? (How will",
          "Explanation": "keeping a check of cleanliness)"
        },
        {
          "Context Speakers": "",
          "Context Utterances": "",
          "Target Speaker": "",
          "Target Sarcastic Utterance": "hardly keeps a dustbin in her home so how will",
          "Explanation": ""
        },
        {
          "Context Speakers": "",
          "Context Utterances": "",
          "Target Speaker": "",
          "Target Sarcastic Utterance": "she has a snake charmer’s ﬂute?)",
          "Explanation": ""
        },
        {
          "Context Speakers": "SAHIL",
          "Context Utterances": "Ab tumne ghar ki\nitni saaf safai ki hai and sec-",
          "Target Speaker": "",
          "Target Sarcastic Utterance": "Walnut brownie, matlab wo khane wali? (You",
          "Explanation": "Sahil monisha ki cooking ka mazak udata hai."
        },
        {
          "Context Speakers": "",
          "Context Utterances": "",
          "Target Speaker": "SAHIL",
          "Target Sarcastic Utterance": "",
          "Explanation": ""
        },
        {
          "Context Speakers": "",
          "Context Utterances": "ondly\nus Karan Verma\nke\nliye\npasta,\nlasagne,",
          "Target Speaker": "",
          "Target Sarcastic Utterance": "mean edible walnut brownie?)",
          "Explanation": "(Sahil makes fun of Monisha’s cooking.)"
        },
        {
          "Context Speakers": "",
          "Context Utterances": "(Now you have cleaned\ncaramel custard banaya.",
          "Target Speaker": "",
          "Target Sarcastic Utterance": "",
          "Explanation": ""
        },
        {
          "Context Speakers": "",
          "Context Utterances": "the\nhouse\nso much\nand\nsecondly made\npasta,",
          "Target Speaker": "",
          "Target Sarcastic Utterance": "",
          "Explanation": ""
        },
        {
          "Context Speakers": "",
          "Context Utterances": "lasagne, caramel custard for that Karan Verma.)",
          "Target Speaker": "",
          "Target Sarcastic Utterance": "",
          "Explanation": ""
        },
        {
          "Context Speakers": "MONISHA",
          "Context Utterances": "Walnut brownie bhi. (And walnut brownie too.)",
          "Target Speaker": "",
          "Target Sarcastic Utterance": "",
          "Explanation": ""
        },
        {
          "Context Speakers": "",
          "Context Utterances": "",
          "Target Speaker": "",
          "Target Sarcastic Utterance": "",
          "Explanation": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 11: is non-sarcastic and as shown, the",
      "data": [
        {
          "MONISHA": "",
          "Walnut brownie bhi. (And walnut brownie too.)": ""
        },
        {
          "MONISHA": "Context Speakers",
          "Walnut brownie bhi. (And walnut brownie too.)": "Context Utterances"
        },
        {
          "MONISHA": "MONISHA",
          "Walnut brownie bhi. (And walnut brownie too.)": "Dukan se yaad aya, mummy ji wahan pe South-"
        },
        {
          "MONISHA": "",
          "Walnut brownie bhi. (And walnut brownie too.)": ""
        },
        {
          "MONISHA": "",
          "Walnut brownie bhi. (And walnut brownie too.)": "hall ya Wembley ki kisi dukan se please mere liye"
        },
        {
          "MONISHA": "",
          "Walnut brownie bhi. (And walnut brownie too.)": "chandi ka mangalsutra le aaiega na (Talking about"
        },
        {
          "MONISHA": "",
          "Walnut brownie bhi. (And walnut brownie too.)": "shops, mom please get me a silver necklace from"
        },
        {
          "MONISHA": "",
          "Walnut brownie bhi. (And walnut brownie too.)": "any shop from Southhall or Wembley.)"
        },
        {
          "MONISHA": "SAHIL",
          "Walnut brownie bhi. (And walnut brownie too.)": "(You want a necklace\nMangalsutra London se?"
        },
        {
          "MONISHA": "",
          "Walnut brownie bhi. (And walnut brownie too.)": "from London?)"
        },
        {
          "MONISHA": "ROSESH",
          "Walnut brownie bhi. (And walnut brownie too.)": "Momma mujhe bohot achi lagti hai. (I like momma"
        },
        {
          "MONISHA": "",
          "Walnut brownie bhi. (And walnut brownie too.)": "very much)"
        },
        {
          "MONISHA": "INDRAVARDHAN",
          "Walnut brownie bhi. (And walnut brownie too.)": "(I know that.\nI know that. Momma pari hai pari!"
        },
        {
          "MONISHA": "",
          "Walnut brownie bhi. (And walnut brownie too.)": "Your mother is like a fairy.)"
        },
        {
          "MONISHA": "ROSESH",
          "Walnut brownie bhi. (And walnut brownie too.)": "Aur me? (And me?)"
        },
        {
          "MONISHA": "",
          "Walnut brownie bhi. (And walnut brownie too.)": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 11: is non-sarcastic and as shown, the",
      "data": [
        {
          "Table 10: Sample instances for WITS, sWITS, hWITS, and eWITS": ""
        },
        {
          "Table 10: Sample instances for WITS, sWITS, hWITS, and eWITS": ""
        },
        {
          "Table 10: Sample instances for WITS, sWITS, hWITS, and eWITS": ""
        },
        {
          "Table 10: Sample instances for WITS, sWITS, hWITS, and eWITS": ""
        },
        {
          "Table 10: Sample instances for WITS, sWITS, hWITS, and eWITS": ""
        },
        {
          "Table 10: Sample instances for WITS, sWITS, hWITS, and eWITS": ""
        },
        {
          "Table 10: Sample instances for WITS, sWITS, hWITS, and eWITS": ""
        },
        {
          "Table 10: Sample instances for WITS, sWITS, hWITS, and eWITS": ""
        },
        {
          "Table 10: Sample instances for WITS, sWITS, hWITS, and eWITS": ""
        },
        {
          "Table 10: Sample instances for WITS, sWITS, hWITS, and eWITS": ""
        },
        {
          "Table 10: Sample instances for WITS, sWITS, hWITS, and eWITS": "Figure 3: Construction of sWITS from WITS."
        },
        {
          "Table 10: Sample instances for WITS, sWITS, hWITS, and eWITS": ""
        },
        {
          "Table 10: Sample instances for WITS, sWITS, hWITS, and eWITS": ""
        },
        {
          "Table 10: Sample instances for WITS, sWITS, hWITS, and eWITS": ""
        },
        {
          "Table 10: Sample instances for WITS, sWITS, hWITS, and eWITS": "tion. Ergo, we map the humour labels from MASAC to the"
        },
        {
          "Table 10: Sample instances for WITS, sWITS, hWITS, and eWITS": "instances present in sWITS and get hWITS."
        },
        {
          "Table 10: Sample instances for WITS, sWITS, hWITS, and eWITS": ""
        },
        {
          "Table 10: Sample instances for WITS, sWITS, hWITS, and eWITS": ""
        },
        {
          "Table 10: Sample instances for WITS, sWITS, hWITS, and eWITS": "eWITS:\nSarcasm signiﬁcantly affects the emitted emotion"
        },
        {
          "Table 10: Sample instances for WITS, sWITS, hWITS, and eWITS": ""
        },
        {
          "Table 10: Sample instances for WITS, sWITS, hWITS, and eWITS": "of an utterance. Wherefore, we hypothesize that emotion"
        },
        {
          "Table 10: Sample instances for WITS, sWITS, hWITS, and eWITS": ""
        },
        {
          "Table 10: Sample instances for WITS, sWITS, hWITS, and eWITS": "recognition in conversation can be improved in the presence"
        },
        {
          "Table 10: Sample instances for WITS, sWITS, hWITS, and eWITS": ""
        },
        {
          "Table 10: Sample instances for WITS, sWITS, hWITS, and eWITS": "of utterance explanations. Therefore, we need emotion la-"
        },
        {
          "Table 10: Sample instances for WITS, sWITS, hWITS, and eWITS": ""
        },
        {
          "Table 10: Sample instances for WITS, sWITS, hWITS, and eWITS": "bels for\nthe instances present\nin sWITS to create eWITS."
        },
        {
          "Table 10: Sample instances for WITS, sWITS, hWITS, and eWITS": "We annotate the instances for emotion labels following the"
        },
        {
          "Table 10: Sample instances for WITS, sWITS, hWITS, and eWITS": ""
        },
        {
          "Table 10: Sample instances for WITS, sWITS, hWITS, and eWITS": "Ekman (Ekman 1992) emotion scheme. Out of\nthe seven"
        },
        {
          "Table 10: Sample instances for WITS, sWITS, hWITS, and eWITS": "possible emotion labels, namely anger,\nfear, disgust, sad-"
        },
        {
          "Table 10: Sample instances for WITS, sWITS, hWITS, and eWITS": "ness, joy, and surprise, neutral, we were able to identify four"
        },
        {
          "Table 10: Sample instances for WITS, sWITS, hWITS, and eWITS": "for our set of\ninstances – anger, sadness,\njoy, and neutral."
        },
        {
          "Table 10: Sample instances for WITS, sWITS, hWITS, and eWITS": "Three annotators (ABC) were involved and achieved Krip-"
        },
        {
          "Table 10: Sample instances for WITS, sWITS, hWITS, and eWITS": "pendorff’s Alpha (Krippendorff 2011) inter-annotator scores"
        },
        {
          "Table 10: Sample instances for WITS, sWITS, hWITS, and eWITS": "as αAB = 0.84, αBC = 0.88, and αAC = 0.86 giving an"
        },
        {
          "Table 10: Sample instances for WITS, sWITS, hWITS, and eWITS": "average score of 0.86. We show couple of instances from all"
        },
        {
          "Table 10: Sample instances for WITS, sWITS, hWITS, and eWITS": "the discussed datasets in Table 10."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 12: Experimental results on sarcastic and non- MAYA:Indravadanplease.Dharampatniistelevisionserialmiddleclass.",
      "data": [
        {
          "MAYA : Tum log pagal ho gye ho? (Have you guys\nlog pagal ho gaya hai.": "(Maya thinks these\ngone mad?) [Non Sarcastic]",
          "resulted in haphazard learning. Additionally the number of": ""
        },
        {
          "MAYA : Tum log pagal ho gye ho? (Have you guys\nlog pagal ho gaya hai.": "",
          "resulted in haphazard learning. Additionally the number of": "layers and heads for each Transformer encoder used is se-"
        },
        {
          "MAYA : Tum log pagal ho gye ho? (Have you guys\nlog pagal ho gaya hai.": "people have gone mad.)",
          "resulted in haphazard learning. Additionally the number of": ""
        },
        {
          "MAYA : Tum log pagal ho gye ho? (Have you guys\nlog pagal ho gaya hai.": "",
          "resulted in haphazard learning. Additionally the number of": ""
        },
        {
          "MAYA : Tum log pagal ho gye ho? (Have you guys\nlog pagal ho gaya hai.": "Table 11: Examples of explanations generated by MOSES for",
          "resulted in haphazard learning. Additionally the number of": ""
        },
        {
          "MAYA : Tum log pagal ho gye ho? (Have you guys\nlog pagal ho gaya hai.": "",
          "resulted in haphazard learning. Additionally the number of": "MONISHA: Dukan se yaad aya mummy ji, wahan pe Southhall ya Wem-"
        },
        {
          "MAYA : Tum log pagal ho gye ho? (Have you guys\nlog pagal ho gaya hai.": "sarcastic and non-sarcastic instances.",
          "resulted in haphazard learning. Additionally the number of": "bley ki kisi dukan se please mere liye chandi ka mangalsutra le aayiega na."
        },
        {
          "MAYA : Tum log pagal ho gye ho? (Have you guys\nlog pagal ho gaya hai.": "",
          "resulted in haphazard learning. Additionally the number of": "(Talking about shops, mom please get me a silver necklace from any shop"
        },
        {
          "MAYA : Tum log pagal ho gye ho? (Have you guys\nlog pagal ho gaya hai.": "",
          "resulted in haphazard learning. Additionally the number of": "from Southhall or Wembley.)"
        },
        {
          "MAYA : Tum log pagal ho gye ho? (Have you guys\nlog pagal ho gaya hai.": "stances decreased after\nthe addition of generated explana-",
          "resulted in haphazard learning. Additionally the number of": "SAHIL: Magalsutra, London se? (You want a necklace from London?)"
        },
        {
          "MAYA : Tum log pagal ho gye ho? (Have you guys\nlog pagal ho gaya hai.": "",
          "resulted in haphazard learning. Additionally the number of": "MONISHA: Haan, wahan pe kali mani ke neeche big ben ke pendant wala"
        },
        {
          "MAYA : Tum log pagal ho gye ho? (Have you guys\nlog pagal ho gaya hai.": "tions; the performance of non-sarcastic humorous dialogues",
          "resulted in haphazard learning. Additionally the number of": ""
        },
        {
          "MAYA : Tum log pagal ho gye ho? (Have you guys\nlog pagal ho gaya hai.": "",
          "resulted in haphazard learning. Additionally the number of": "(Yes, we can get a necklace of black beads\nfrom\nmangalsutra milta hai."
        },
        {
          "MAYA : Tum log pagal ho gye ho? (Have you guys\nlog pagal ho gaya hai.": "increased by 8% when explanations are appended with the",
          "resulted in haphazard learning. Additionally the number of": "there.)"
        },
        {
          "MAYA : Tum log pagal ho gye ho? (Have you guys\nlog pagal ho gaya hai.": "",
          "resulted in haphazard learning. Additionally the number of": "MAYA: Monisha me Central London me rehne wali hun, Piccadilly Circus"
        },
        {
          "MAYA : Tum log pagal ho gye ho? (Have you guys\nlog pagal ho gaya hai.": "input. On the contrary, for the sarcastic emotion instances,",
          "resulted in haphazard learning. Additionally the number of": "ke paas. Wembley ya southhall jesi middle class jagah me nhi. (Monisha, I’ll"
        },
        {
          "MAYA : Tum log pagal ho gye ho? (Have you guys\nlog pagal ho gaya hai.": "the performance is enhanced in the presence of explanations",
          "resulted in haphazard learning. Additionally the number of": "be staying in Central London, near Piccadilly Circus. Not at some middle"
        },
        {
          "MAYA : Tum log pagal ho gye ho? (Have you guys\nlog pagal ho gaya hai.": "",
          "resulted in haphazard learning. Additionally the number of": "class place like Wembley or Southhall.)"
        },
        {
          "MAYA : Tum log pagal ho gye ho? (Have you guys\nlog pagal ho gaya hai.": "while it remains comparable for non-sarcastic instances.",
          "resulted in haphazard learning. Additionally the number of": ""
        },
        {
          "MAYA : Tum log pagal ho gye ho? (Have you guys\nlog pagal ho gaya hai.": "",
          "resulted in haphazard learning. Additionally the number of": "Maya Monisha ke style ka mazak udati hai. (Maya makes fun of Monisha’s"
        },
        {
          "MAYA : Tum log pagal ho gye ho? (Have you guys\nlog pagal ho gaya hai.": "",
          "resulted in haphazard learning. Additionally the number of": "style.)"
        },
        {
          "MAYA : Tum log pagal ho gye ho? (Have you guys\nlog pagal ho gaya hai.": "",
          "resulted in haphazard learning. Additionally the number of": "Sarcasm\nHumour\nEmotion"
        },
        {
          "MAYA : Tum log pagal ho gye ho? (Have you guys\nlog pagal ho gaya hai.": "Humour\nEmotion",
          "resulted in haphazard learning. Additionally the number of": ""
        }
      ],
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "From Arabic Sentiment Analysis to Sarcasm Detection: The ArSarcasm Dataset",
      "authors": [
        "I Abu Farha",
        "W Magdy"
      ],
      "year": "2020",
      "venue": "Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection"
    },
    {
      "citation_id": "2",
      "title": "Multi-modal Sarcasm Detection and Humor Classification in Code-mixed Conversations",
      "authors": [
        "M Bedi",
        "S Kumar",
        "M Akhtar",
        "T Chakraborty"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "3",
      "title": "Harnessing Online News for Sarcasm Detection in Hindi Tweets",
      "authors": [
        "S Bharti",
        "K Sathya Babu",
        "S Jena"
      ],
      "year": "2017",
      "venue": "Pattern Recognition and Machine Intelligence"
    },
    {
      "citation_id": "4",
      "title": "Multi-Modal Sarcasm Detection in Twitter with Hierarchical Fusion Model",
      "authors": [
        "Y Cai",
        "H Cai",
        "X Wan"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "5",
      "title": "Towards Multimodal Sarcasm Detection (An Obviously Perfect Paper)",
      "authors": [
        "S Castro",
        "D Hazarika",
        "V Pérez-Rosas",
        "R Zimmermann",
        "R Mihalcea",
        "S Poria"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "6",
      "title": "Rˆ3: Reverse, Retrieve, and Rank for Sarcasm Generation with Commonsense Knowledge",
      "authors": [
        "T Chakrabarty",
        "D Ghosh",
        "S Muresan",
        "N Peng"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "7",
      "title": "Sentiment and Emotion help Sarcasm? A Multi-task Learning Framework for Multi-Modal Sarcasm, Sentiment and Emotion Analysis",
      "authors": [
        "D Chauhan",
        "D .; S R",
        "A Ekbal",
        "P Bhattacharyya"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "8",
      "title": "Overview of the evalita 2018 task on irony detection in italian tweets (ironita)",
      "authors": [
        "A Cignarella",
        "S Frenda",
        "V Basile",
        "C Bosco",
        "V Patti",
        "P Rosso"
      ],
      "year": "2018",
      "venue": "Sixth Evaluation Campaign of Natural Language Processing and Speech Tools for Italian"
    },
    {
      "citation_id": "9",
      "title": "Meteor Universal: Language Specific Translation Evaluation for Any Target Language",
      "authors": [
        "M Denkowski",
        "A Lavie"
      ],
      "year": "2014",
      "venue": "Proceedings of the Ninth Workshop on Statistical Machine Translation"
    },
    {
      "citation_id": "10",
      "title": "Deep Models for Converting Sarcastic Utterances into Their Non Sarcastic Interpretation",
      "authors": [
        "A Dubey",
        "A Joshi",
        "P Bhattacharyya"
      ],
      "year": "2019",
      "venue": "Proceedings of the ACM India Joint International Conference on Data Science and Management of Data"
    },
    {
      "citation_id": "11",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cognition and Emotion"
    },
    {
      "citation_id": "12",
      "title": "The Geneva Minimalistic Acoustic Parameter Set (GeMAPS) for Voice Research and Affective Computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan",
        "K Truong"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "The Role of Conversation Context for Sarcasm Detection in Online Interactions",
      "authors": [
        "D Ghosh",
        "A Richard Fabbri",
        "S Muresan"
      ],
      "year": "2017",
      "venue": "Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue"
    },
    {
      "citation_id": "14",
      "title": "Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and Ima-geNet?",
      "authors": [
        "K Hara",
        "H Kataoka",
        "Y Satoh"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "15",
      "title": "Humor Knowledge Enriched Transformer for Understanding Multimodal Humor",
      "authors": [
        "M Hasan",
        "S Lee",
        "W Rahman",
        "A Zadeh",
        "R Mihalcea",
        "L.-P Morency",
        "E Hoque"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "16",
      "title": "Automatic Sarcasm Detection: A Survey",
      "authors": [
        "A Joshi",
        "P Bhattacharyya",
        "M Carman"
      ],
      "year": "2017",
      "venue": "ACM Comput. Surv"
    },
    {
      "citation_id": "17",
      "title": "Harnessing Context Incongruity for Sarcasm Detection",
      "authors": [
        "A Joshi",
        "V Sharma",
        "P Bhattacharyya"
      ],
      "year": "2015",
      "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "18",
      "title": "Lexical Influences on the Perception of Sarcasm",
      "authors": [
        "R Kreuz",
        "G Caucci"
      ],
      "year": "2007",
      "venue": "Proceedings of the Workshop on Computational Approaches to Figurative Language"
    },
    {
      "citation_id": "19",
      "title": "Computing Krippendorff's alphareliability",
      "authors": [
        "K Krippendorff"
      ],
      "year": "2011",
      "venue": "Computing Krippendorff's alphareliability"
    },
    {
      "citation_id": "20",
      "title": "When did you become so smart, oh wise one?! Sarcasm Explanation in Multi-modal Multi-party Dialogues",
      "authors": [
        "S Kumar",
        "A Kulkarni",
        "M Akhtar",
        "T Chakraborty"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "21",
      "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
      "authors": [
        "M Lewis",
        "Y Liu",
        "N Goyal",
        "M Ghazvininejad",
        "A Mohamed",
        "O Levy",
        "V Stoyanov",
        "L Zettlemoyer"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "22",
      "title": "ROUGE: A Package for Automatic Evaluation of Summaries",
      "authors": [
        "C.-Y Lin"
      ],
      "year": "2004",
      "venue": "Text Summarization Branches Out"
    },
    {
      "citation_id": "23",
      "title": "Multilingual Denoising Pre-training for Neural Machine Translation",
      "authors": [
        "Y Liu",
        "J Gu",
        "N Goyal",
        "X Li",
        "S Edunov",
        "M Ghazvininejad",
        "M Lewis",
        "L Zettlemoyer"
      ],
      "year": "2020",
      "venue": "Transactions of the Association for Computational Linguistics"
    },
    {
      "citation_id": "24",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "25",
      "title": "A Modular Architecture for Unsupervised Sarcasm Generation",
      "authors": [
        "A Mishra",
        "T Tater",
        "K Sankaranarayanan"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing"
    }
  ]
}