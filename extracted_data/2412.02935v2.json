{
  "paper_id": "2412.02935v2",
  "title": "Dynamic Graph Neural Ordinary Differential Equation Network For Multi-Modal Emotion Recognition In Conversation",
  "published": "2024-12-04T01:07:59Z",
  "authors": [
    "Yuntao Shou",
    "Tao Meng",
    "Wei Ai",
    "Keqin Li"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal emotion recognition in conversation (MERC) refers to identifying and classifying human emotional states by combining data from multiple different modalities (e.g.,  audio, images, text, video, etc.). Most existing multimodal emotion recognition methods use GCN to improve performance, but existing GCN methods are prone to overfitting and cannot capture the temporal dependency of the speaker's emotions. To address the above problems, we propose a Dynamic Graph Neural Ordinary Differential Equation Network (DGODE) for MERC, which combines the dynamic changes of emotions to capture the temporal dependency of speakers' emotions, and effectively alleviates the overfitting problem of GCNs. Technically, the key idea of DGODE is to utilize an adaptive mixhop mechanism to improve the generalization ability of GCNs and use the graph ODE evolution network to characterize the continuous dynamics of node representations over time and capture temporal dependencies. Extensive experiments on two publicly available multimodal emotion recognition datasets demonstrate that the proposed DGODE model has superior performance compared to various baselines. Furthermore, the proposed DGODE can also alleviate the oversmoothing problem, thereby enabling the construction of a deep GCN network.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Multimodal Emotion Recognition in Conversation (MERC) technology significantly improves the accuracy and wide application of emotion recognition by integrating data from multiple modalities (e.g., audio, image, text, and video)  (Shou et al., 2022b (Shou et al., , 2025 (Shou et al., , 2022a, 2023e, 2024e;, 2023e, 2024e; Meng et al., 2024b; Shou et al., 2023d; Ai et al., 2023a Meng et al., 2024a; Shou et al., 2024c,b,a) . MERC can not only improve the intelligence of humancomputer interaction, but also bring important im- provements in practical scenarios (e.g., health monitoring, education, entertainment, and security)  (Ying et al., 2021; Shou et al., 2023b,a; Meng et al., 2024d; Shou et al., 2024f; Ai et al., 2024f; Zhang et al., 2024; Ai et al., 2023b; Meng et al., 2024c; Ai et al., 2024a) .\n\nMany existing studies improve the performance of MERC by using graph convolutional neural networks (GCNs)  (Yin et al., 2023b (Yin et al., ,a,c, 2022b (Yin et al., ,a, 2024b,a;,a; Yin et al.)  to effectively model the conversational relations between speakers. However, as shown in Fig.  1 , we find that existing GCNs only contain 4 layers of GCN (e.g., MMGCN  (Hu et al., 2021)  and M3Net  (Chen et al., 2023) ), while the performance decreases significantly as the number of layers increases. The reason for the performance degradation may be attributed to the fact that information aggregation in vanilla GCNs is just simple message smoothing within the neighborhood, causing neighboring nodes to converge to the same value as the number of layers is stacked. Therefore, it is necessary to improve the stability of information diffusion on the graph and alleviate the problem of over-smoothing of nodes. Furthermore, MERC usually relies on dynamic temporal information, while traditional GCNs can only process static images. How to model the temporal features in emotion recognition tasks as a continuous dynamic process so that the network can capture and model the complex patterns of emotion changing over time remains a challenge. Therefore, this paper aims to propose a Dynamic Graph Neural Ordinary Differential Equation Network (DGODE) to dynamically model the temporal dependency of emotion changes and improve the expressiveness of node features as the number of GCN layers increases  (Shou et al., 2023c; Ai et al., 2024c; Shou et al., 2024d,g; Ai et al., 2024b,d) .\n\nSpecifically, we introduce the Dynamic Graph Neural Ordinary Differential Equation Network (DGODE) based on the perspective of continuous time. Our DGODE method introduces an adaptive mixhop mechanism to extract node information from different hop count neighbors simultaneously and uses ordinary differential equations to model the temporal dependence of emotion changes. DGODE shows stable performance as the number of GCN layers increases. We use two publicly available multimodal emotion recognition datasets to verify the effectiveness of DGODE.\n\nOverall, the main contributions of this paper are as follows:\n\n• We propose the Dynamic Graph Neural Ordinary Differential Equation Network (DGODE), which combines the dynamic changes of emotions to capture the temporal dependency of speakers' emotions.\n\n• We design an adaptive mixhop mechanism to capture the relationship between distant nodes and combine ODE to capture the temporal dependency of speakers' emotions.\n\n• Extensive experiments are conducted to demonstrate its superiority in MERC compared to various baselines on IEMOCAP and MELD datasets.\n\n2 Related Work",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multimodal Emotion Recognition In Conversation",
      "text": "MERC aims to identify and understand the emotional state in a conversation by analyzing data in multiple modalities  (Zhang et al., 2023) . With the rapid development of social media technology, people are increasingly communicating in a multimodal way. Therefore, how to accurately under-stand the emotional information in multimodal data has become a key issue  (Ai et al., 2024e) . Initially, researchers used recurrent neural networks (RNNs) to model conversations, which mainly capture emotional information by processing utterances or entire conversations sequentially  (Poria et al., 2017) . For instance, HiGRU  (Jiao et al., 2019)  proposed a hierarchical GRU model to capture the information in the conversation, which not only considers the emotional features at the word level, but also extends to the utterance level, thereby generating a conversation representation that contains richer contextual information. Similarly, DialogueRNN  (Majumder et al., 2019 ) also uses GRU units to capture the emotional dynamics in the conversation, while taking into account the state of the utterance itself and the emotional state of the speaker. Since RNNs cannot achieve parallel computing, Transformers have become a better alternative for sequence modeling  (Fan et al., 2023) . For example, CTNet  (Lian et al., 2021)  uses the powerful representation ability of Transformers to model the emotional dynamics in conversations through a self-attention mechanism. SDT  (Ma et al., 2023)  effectively integrates multimodal information through Transformer, and uses selfdistillation technology to better learn the potential information in multimodal data.\n\nHowever, studies have shown  (Hu et al., 2021 ) that the discourse in the conversation is not just a sequential relationship, but a more complex speaker dependency. Therefore, the DialogGCN  (Ghosal et al., 2019)  introduces a graph network to model the dependency between the self and the speaker in the conversation. By using a graph convolutional network (GCN), DialogGCN can effectively propagate contextual information to capture more detailed emotional dependencies. Based on the idea of DialogGCN, SumAggGIN  (Sheng et al., 2020)  further emphasizes the emotional fluctuations in the conversation by referencing global topic-related emotional phrases and local dependencies. Meaningwhlie, the DAG-ERC  (Shen et al., 2021b)  believes that the discourse in the conversation is not a simple continuous relationship, but a directed dependency structure.\n\nWith the development of pre-trained language models (PLMs)  (Min et al., 2023) , researchers began to explore the application of PLM's powerful representation capabilities to emotion recognition tasks. For example, DialogXL  (Shen et al., 2021a)  applies XLNet to emotion recognition and designs an enhanced memory module for storing historical context, while modifying the original self-attention mechanism to capture complex dependencies within and between speakers. The CoMPM  (Lee and Lee, 2022)  further leverages PLM by building a pre-trained memory based on the speaker's previous utterances, and then combining the context embedding generated by another PLM to generate the final representation of emotion recognition. CoG-BART  (Li et al., 2022a)  introduces the BART model to understand the contextual background and generate the next utterance as an auxiliary task.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Continuous Graph Neural Networks",
      "text": "Neural ordinary differential equations (ODEs) are a novel approach to modeling continuous dynamic systems  (Chen et al., 2018) . They parameterize the derivatives of hidden states through neural networks, allowing the model to perform continuous inference in the time dimension, rather than relying solely on the discrete sequence of hidden layers in traditional neural networks. ODEs can more accurately describe the changing process over time and are suitable for complex tasks involving time evolution. Continuous Graph Neural Network (CGNN)  (Xhonneux et al., 2020)  first extended this ODE approach to graph data. Specifically, CGNN developed a continuous message passing layer to achieve continuous dynamic modeling of node states. Unlike traditional graph neural networks (GCNs), CGNNs no longer rely on a fixed number of layers for information propagation, but instead solve ordinary differential equations to enable continuous propagation of information between nodes. CGNN also introduces a restart distribution to \"reset\" the node state to the initial state in a timely manner during the information propagation process, thereby avoiding the over-smoothing.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Preliminaries",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Graph Neural Networks",
      "text": "Given a graph G = (V, E), where V is a set of nodes and E is a set of edges. Each node v ∈ V constitutes the node feature matrix X ∈ R |V |×d , where d represents the dimension of the feature. Each row of X corresponds to the feature representation of a node. we use the binary adjacency matrix A ∈ R |V |×|V | to represent the connection relationship between node i and node j. If a ij = 1, it means that there is an edge between node i and node j; if a ij = 0, it means that there is no connection. Our goal is to learn a node representation matrix H that can capture the structural information and feature information of the nodes in the graph.\n\nWe usually normalize the adjacency matrix A. The degree matrix D is a diagonal matrix whose diagonal elements D ii represent the degree of node i. However, the eigenvalues of the normalized matrix may include negative values. Therefore, we follow the previous methods (Kipf and Welling, 2022) and use a regularized matrix to represent the graph structure. Specifically, we use the following symmetric normalized adjacency matrix:\n\nwhere α is a hyperparameter.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Neural Ordinary Differential Equation",
      "text": "Neural ODEs provides a new method for continuous-time dynamic modeling by modeling the forward propagation process of a neural network as the solution process of an ODE. Specifically, consider an input data x(t) and describe its evolution in the form of an ODE:\n\nwhere x(t) represents the hidden state at time t, f is a neural function with parameter θ.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multi-Modal Feature Extraction",
      "text": "Word Embedding: Following previous studies  (Chudasama et al., 2022; Li et al., 2022b) , we use RoBERTa  (Liu, 2019)  to obtain contextual embedding representations of text in this paper.\n\nVisual and Audio Feature Extraction: Following previous work  (Ma et al., 2023; Lian et al., 2021) , we selected DenseNet  (Huang et al., 2017)  and openSMILE  (Eyben et al., 2010)  as feature extraction tools for video and audio.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Problem Definition",
      "text": "In the multimodal conversational emotion recognition task, given a conversation C, the conversation consists of a series of utterances and S different speakers. The goal of multimodal emotion recognition in a conversation is to predict the emotion label of each utterance in the emotion set Y . Specifically, the conversation C can be represented as a sequence C = [(u 1 , s 1 ), (u 2 , s 2 ), . . . , (u M , s M )], where u i represents the i-th utterance in the conversation and s i represents the unique speaker s i ∈ S associated with the utterance. Each utterance u i contains audio data v a , video data v f , and text data v t . These multimodal data together express the meaning and emotion of the utterance. For each utterance u i , we need to determine its emotional state, which is represented by an emotion label y e i .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "As shown in Fig.  2 , the key steps of DGODE are to alleviate the overfitting problem of existing GCNs and capture the temporal dependency of speaker emotions. DGODE constructs an adaptive mixhop mechanism in the process of node aggregation to reduce the excessive dependence on local features and thus reduce overfitting. Furthermore, we introduce Graph ODE to model multimodal data in continuous time through differential equations and capture the temporal dependence of speakers. By solving the ODE equation, the emotional state of the previous moment is propagated to the subsequent moment, allowing the model to capture the changing process of the speaker's emotion over a longer time range. In this section, we theoretically introduce the Dynamic Graph Neural Ordinary Dif-ferential Equation Network (DGODE) and explain the implementation details.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Modality Encoding",
      "text": "The essence of a conversation is a continuous interactive process in which multiple speakers participate and communicate with each other. Therefore, when processing a conversation, we need to consider the identity of each speaker and the contextual information of the conversation to obtain semantic information that reflects the current discourse and capture the speaker's characteristics and the contextual association information of the conversation. Specifically, we first mark each speaker with a one-hot vector p i to uniquely identify the speaker.\n\nFor the i-th round of conversation, we extract the corresponding speaker embedding P i based on the one-hot vector p i , which contains the characteristic information of the current speaker and can be combined with the semantic features of the current discourse to generate a speaker-aware and contextaware unimodal representation. The formula for discourse embedding is defined as follows:\n\nwhere W p is the learnable parameters.\n\nTo effectively encode the features of the conversation text, we use GRU to capture the contextual semantic information in the sequence data and generate a more comprehensive text representation. Specifically, we can make full use of the time order and dependency relationship in the conversation text through GRU, and incorporate the information of the previous and next context into the encoding of each round of conversation, so that the generated text features can better reflect the context and meaning of the entire conversation. Mathematically:\n\nwhere c i m(+,-) represents the cell state. To obtain a unimodal representation that reflects both the speaker identity and the context information, we add the speaker embedding to the representation of each modality. Specifically, for the i-th round of speech in the conversation, we calculate the text modality representation h i t , the audio modality representation h i a , and the visual modality representation h i f and add the speaker embedding S i to these modality representations to generate the final unimodal representations that incorporate the speaker information as follows:",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Adaptive Mixhop Graph",
      "text": "The core idea of GCN is to perform convolution operations on graph structured data to capture the complex relationships between nodes and the topological structure of the graph and learn the representation of node features. However, traditional GCN only aggregates information from directly adjacent nodes, and may not be able to fully capture the information of more distant nodes. To capture high-order neighbor relationships, we construct an adaptive mixhop graph to simultaneously extract information from different hop neighbors and improve the understanding of the global graph structure. Furthermore, to model the interaction between different features, we use the residual idea to discretely model the adaptive mixHop GCN as follows:\n\nwhere W ∈ R d×d represents the learnable weight matrix. Essentially, using the idea of residuals we can model the interaction of different features, so that we can learn the representation of nodes more effectively.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Temporal Graph Ode",
      "text": "However, information aggregation via Eq. 6 cannot model the speaker's emotional changes over time. Therefore, we aim to model the discrete information propagation process of vanilla GCN as a continuous process and use ODE to characterize this dynamic information propagation process. By solving the ODE equation, the emotional state of the previous moment is propagated to the subsequent moments, allowing the model to capture the changing process of the speaker's emotions over a longer time frame. Specifically, we view Eq. 6 as the Riemann sum of integrals from t = 0 to t = n, as described in the following proposition. Proposition 1. Suppose A -I = PΛ ′ P -1 , W -I = QΦ ′ Q -1 , then Eq. 6 is discretized as the ODE as follows:\n\nwhere\n\nis the output of the encoder f . F(t) is defined as follows:\n\nwhere E = P -1 EQ. Eq. 8 can be approximated by an ODE solver to calculate the dynamic evolution of the system in discrete time steps as follows:",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Model Training",
      "text": "The final multi-modal embedding representation H i is passed to a fully connected layer for further integration and transformation, and a deeper feature representation is extracted as follows:   (Xing et al., 2020)  50.6 76.8 62.9 56.5 77.9 55.7 64.3 78.9 55.3 8.6 24.9 57.4 3.4 40.9 60.4 DialogueGCN  (Ghosal et al., 2019)   where p i contains the model's predicted probability for each emotion category, reflecting the model's confidence in identifying different emotions on the utterance, W l , W smax , b l , and b smax are trainable parameters. To obtain the final emotion prediction result, we select the emotion category label ŷi with the highest probability from p i as the predicted emotion of the utterance as follows:",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Implementation Details",
      "text": "We used PyTorch to implement the proposed DGODE model and chose Adam as the optimizer.\n\nFor the IEMOCAP dataset, the learning rate of the model was set to 1e-4, while for the MELD dataset, the learning rate was set to 5e-6. During training, the batch size of IEMOCAP was 16, while the batch size of MELD was 8. In the setting of the Bi-GRU layer, we set different numbers of channels for different modal inputs. In the IEMO-CAP dataset, the number of input channels for text, acoustic, and visual modalities are 1024, 1582, and 342, respectively. In the MELD dataset, the number of input channels for text, acoustic, and visual modalities are set to 1024, 300, and 342, respectively. In addition, for the graph encoder, we set the size of the hidden layer to 512. To prevent overfitting of the model, we introduced L2 weight decay in training, with the coefficient set to 1e-5, and applied a dropout rate of 0.5 in the key layers.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Experiments",
      "text": "Our experimental results are the average of 10 runs and are statistically significant under paired t-test (all p < 0.05).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Datasets And Evaluation Metrics",
      "text": "We used two used MERC datasets in our experiments: IEMOCAP  (Busso et al., 2008)  and MELD  (Poria et al., 2019) . Both datasets contain data in three modalities: text, audio, and video. The IEMO-CAP dataset is collected from dialogue scenes performed by actors. The MELD dataset consists of dialogue clips from the American TV series Friends.\n\nWe report the F1 and the weighted F1 (W-F1).",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Baselines",
      "text": "To verify the superior performance of our proposed method DGODE, we compared it with other comparison methods, including three RNN algorithms (i.e., bc-LSTM  (Poria et al., 2017) , A-DMN  (Xing et al., 2020) , CoMPM (Lee and Lee, 2022)), six GNN algorithms (i.e., DialogueGCN  (Ghosal et al., 2019) , LR-GCN  (Ren et al., 2021) , MMGCN  (Hu et al., 2021) , AdaGIN  (Tu et al., 2024) , DER-GCN  (Ai et al., 2024e) , RGAT  (Ishiwatari et al., 2020) ), one HGNN algorithm (i.e., M3Net  (Chen et al., 2023) ), and two Transformer algorithm (i.e., CT-Net  (Lian et al., 2021) , EmoBERTa  (Kim and Vossen, 2021) ).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Overall Results",
      "text": "As shown in Table  1 , the experimental results show that our proposed method DGODE significantly improves the performance in the emotion recognition task. The performance improvement may be due to the fact that the dynamic graph ODE network can effectively capture the temporal dependency of the discourse and effectively alleviate the over-smoothing problem when processing graph data. To further verify the superiority of the model, we also report the W-F1 of each emotion category. Specifically, in the IEMOCAP dataset, our model has better W-F1 scores than other methods on the three categories of emotions \"happy\", \"neutral\" and \"frustrated\". Similarly, in the MELD dataset, our model also achieves the best W-F1 scores on the three categories of emotions \"surprise\", \"neutral\" and \"sadness\", further verifying the robustness of the model. Therefore, our method not only performs well in emotion recognition tasks, but also has significant advantages in model complexity.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Effectiveness Of Multimodal Features",
      "text": "We analyze the impact of different modal features on the results of emotion recognition experiments to verify the effect of different modal feature combinations. Specifically, we observe the contribution of different modal features (text, audio, and video) to the emotion recognition performance by inputting them into the model. The experimental results are shown in Fig.  3 . 1) In the single-modal experiment, the emotion recognition accuracy of the text modality is significantly better than the audio and video modalities. 2) When we combine the features of the two modalities for the experiment, the effect of emotion recognition is significantly better than the results of any single modality. 3) When we use the features of the three modalities for emotion recognition at the same time, the performance of the model reaches the best level.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Error Analysis",
      "text": "Although the proposed DGODE model has shown good results in the emotion recognition task, it still faces some challenges, especially in the recognition of some emotions. To analyze the misclassification of the model in more depth, we analyzed the confusion matrix of the test set on the two datasets. As shown in Fig.  4 , DGODE has the problem of misclassifying similar emotions on the IEMOCAP  dataset. For example, the model often misclassifies \"happy\" as \"excited\" or \"angry\" as \"frustrated\". The slight differences between emotions lead to the difficulty of the model in distinguishing them. Secondly, on the MELD dataset, DGODE also shows a similar misclassification trend, such as misclassifying \"surprise\" as \"angry\". In addition, since the \"neutral\" emotion is the majority class in the MELD dataset, the model tends to misclassify other emotions as \"neutral\", which makes the model's performance in dealing with other emotion categories decrease. Finally, the model also encounters significant difficulties in identifying minority emotions. In particular, in the MELD dataset, the two emotions \"fear\" and \"disgust\" belong to the minority class, and it is difficult for the model to accurately detect these emotions.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Abalation Study",
      "text": "To analyze the components of DGODE, we performed ablation experiments on the IEMOCAP and MELD datasets. The results in Fig.  5  show that DGODE consistently outperforms all variants on W-F1 and is also the best on the partially classified sentiment categories. Removing ODE degrades the In multimodal emotion recognition, emotion labels are usually annotated for the overall emotion of a certain period of time. However, DGODE focuses on dynamic changes, which may lead to the problem that the subtle dynamic changes captured by the model do not match the overall emotion labels.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Performance comparison of different methods",
      "page": 1
    },
    {
      "caption": "Figure 1: , we find that existing GCNs",
      "page": 1
    },
    {
      "caption": "Figure 2: The overall architecture of DGODE.",
      "page": 4
    },
    {
      "caption": "Figure 2: , the key steps of DGODE are to",
      "page": 4
    },
    {
      "caption": "Figure 3: Verify the effectiveness of multimodal fea-",
      "page": 7
    },
    {
      "caption": "Figure 3: 1) In the single-modal",
      "page": 7
    },
    {
      "caption": "Figure 4: , DGODE has the problem of",
      "page": 7
    },
    {
      "caption": "Figure 4: We performed a analysis of the classification",
      "page": 7
    },
    {
      "caption": "Figure 5: On the IEMOCAP and MELD datasets, we",
      "page": 7
    },
    {
      "caption": "Figure 5: show that",
      "page": 7
    },
    {
      "caption": "Figure 6: Visualization of the learned embeddings.",
      "page": 8
    },
    {
      "caption": "Figure 6: , on the IEMOCAP",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dynamic Graph Neural Ordinary Differential Equation Network for": "Multi-modal Emotion Recognition in Conversation"
        },
        {
          "Dynamic Graph Neural Ordinary Differential Equation Network for": "Yuntao Shou1,2, Tao Meng3,†, Wei Ai3, Keqin Li4"
        },
        {
          "Dynamic Graph Neural Ordinary Differential Equation Network for": "1School of Computer Science and Technology, Xi’an Jiaotong University, Xi’an, 710049, China"
        },
        {
          "Dynamic Graph Neural Ordinary Differential Equation Network for": "2Ministry of Education Key Laboratory of Intelligent Networks and Network Security,"
        },
        {
          "Dynamic Graph Neural Ordinary Differential Equation Network for": "Xi’an Jiaotong University, Xi’an, 710049, China"
        },
        {
          "Dynamic Graph Neural Ordinary Differential Equation Network for": "3College of Computer and Mathematics, Central South University of Forestry and Technology,"
        },
        {
          "Dynamic Graph Neural Ordinary Differential Equation Network for": "Changsha, Hunan, 410004, China"
        },
        {
          "Dynamic Graph Neural Ordinary Differential Equation Network for": "4Department of Computer Science, State University of New York, New Paltz, New York 12561, USA"
        },
        {
          "Dynamic Graph Neural Ordinary Differential Equation Network for": "†Corresponding Author: mengtao@hnu.edu.cn"
        },
        {
          "Dynamic Graph Neural Ordinary Differential Equation Network for": "MMGCN\nAbstract"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Changsha, Hunan, 410004, China": ""
        },
        {
          "Changsha, Hunan, 410004, China": "†Corresponding Author: mengtao@hnu.edu.cn"
        },
        {
          "Changsha, Hunan, 410004, China": "MMGCN"
        },
        {
          "Changsha, Hunan, 410004, China": "M3Net"
        },
        {
          "Changsha, Hunan, 410004, China": "DGODE\n72"
        },
        {
          "Changsha, Hunan, 410004, China": ""
        },
        {
          "Changsha, Hunan, 410004, China": "70"
        },
        {
          "Changsha, Hunan, 410004, China": "68"
        },
        {
          "Changsha, Hunan, 410004, China": "Accuracy(%)"
        },
        {
          "Changsha, Hunan, 410004, China": "66"
        },
        {
          "Changsha, Hunan, 410004, China": ""
        },
        {
          "Changsha, Hunan, 410004, China": "64"
        },
        {
          "Changsha, Hunan, 410004, China": ""
        },
        {
          "Changsha, Hunan, 410004, China": "62"
        },
        {
          "Changsha, Hunan, 410004, China": ""
        },
        {
          "Changsha, Hunan, 410004, China": "1\n2\n3\n4\n5\n6\n7\n8\n9"
        },
        {
          "Changsha, Hunan, 410004, China": "#Layer"
        },
        {
          "Changsha, Hunan, 410004, China": ""
        },
        {
          "Changsha, Hunan, 410004, China": "Figure 1: Performance comparison of different methods"
        },
        {
          "Changsha, Hunan, 410004, China": "for different numbers of GCN layers on the IEMOCAP"
        },
        {
          "Changsha, Hunan, 410004, China": "dataset."
        },
        {
          "Changsha, Hunan, 410004, China": ""
        },
        {
          "Changsha, Hunan, 410004, China": "provements in practical scenarios (e.g., health mon-"
        },
        {
          "Changsha, Hunan, 410004, China": "itoring,\neducation,\nentertainment,\nand security)"
        },
        {
          "Changsha, Hunan, 410004, China": ""
        },
        {
          "Changsha, Hunan, 410004, China": "(Ying et al., 2021; Shou et al., 2023b,a; Meng et al.,"
        },
        {
          "Changsha, Hunan, 410004, China": ""
        },
        {
          "Changsha, Hunan, 410004, China": "2024d; Shou et al., 2024f; Ai et al., 2024f; Zhang"
        },
        {
          "Changsha, Hunan, 410004, China": ""
        },
        {
          "Changsha, Hunan, 410004, China": "et al., 2024; Ai et al., 2023b; Meng et al., 2024c;"
        },
        {
          "Changsha, Hunan, 410004, China": ""
        },
        {
          "Changsha, Hunan, 410004, China": "Ai et al., 2024a)."
        },
        {
          "Changsha, Hunan, 410004, China": ""
        },
        {
          "Changsha, Hunan, 410004, China": "Many existing studies improve the performance"
        },
        {
          "Changsha, Hunan, 410004, China": ""
        },
        {
          "Changsha, Hunan, 410004, China": "of MERC by using graph convolutional neural"
        },
        {
          "Changsha, Hunan, 410004, China": "networks (GCNs) (Yin et al., 2023b,a,c, 2022b,a,"
        },
        {
          "Changsha, Hunan, 410004, China": ""
        },
        {
          "Changsha, Hunan, 410004, China": "2024b,a; Yin et al.)\nto effectively model\nthe con-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Overall, the main contributions of this paper are": "as follows:",
          "(Ma et al., 2023) effectively integrates multimodal": "information through Transformer, and uses self-"
        },
        {
          "Overall, the main contributions of this paper are": "",
          "(Ma et al., 2023) effectively integrates multimodal": "distillation technology to better learn the potential"
        },
        {
          "Overall, the main contributions of this paper are": "• We\npropose\nthe Dynamic Graph Neural",
          "(Ma et al., 2023) effectively integrates multimodal": ""
        },
        {
          "Overall, the main contributions of this paper are": "",
          "(Ma et al., 2023) effectively integrates multimodal": "information in multimodal data."
        },
        {
          "Overall, the main contributions of this paper are": "Ordinary\nDifferential\nEquation\nNetwork",
          "(Ma et al., 2023) effectively integrates multimodal": ""
        },
        {
          "Overall, the main contributions of this paper are": "",
          "(Ma et al., 2023) effectively integrates multimodal": "However, studies have shown (Hu et al., 2021)"
        },
        {
          "Overall, the main contributions of this paper are": "(DGODE), which\ncombines\nthe\ndynamic",
          "(Ma et al., 2023) effectively integrates multimodal": ""
        },
        {
          "Overall, the main contributions of this paper are": "",
          "(Ma et al., 2023) effectively integrates multimodal": "that the discourse in the conversation is not just a se-"
        },
        {
          "Overall, the main contributions of this paper are": "changes of emotions to capture the temporal",
          "(Ma et al., 2023) effectively integrates multimodal": ""
        },
        {
          "Overall, the main contributions of this paper are": "",
          "(Ma et al., 2023) effectively integrates multimodal": "quential relationship, but a more complex speaker"
        },
        {
          "Overall, the main contributions of this paper are": "dependency of speakers’ emotions.",
          "(Ma et al., 2023) effectively integrates multimodal": ""
        },
        {
          "Overall, the main contributions of this paper are": "",
          "(Ma et al., 2023) effectively integrates multimodal": "dependency. Therefore,\nthe DialogGCN (Ghosal"
        },
        {
          "Overall, the main contributions of this paper are": "",
          "(Ma et al., 2023) effectively integrates multimodal": "et al., 2019) introduces a graph network to model"
        },
        {
          "Overall, the main contributions of this paper are": "• We design an adaptive mixhop mechanism to",
          "(Ma et al., 2023) effectively integrates multimodal": ""
        },
        {
          "Overall, the main contributions of this paper are": "",
          "(Ma et al., 2023) effectively integrates multimodal": "the dependency between the self and the speaker in"
        },
        {
          "Overall, the main contributions of this paper are": "capture the relationship between distant nodes",
          "(Ma et al., 2023) effectively integrates multimodal": ""
        },
        {
          "Overall, the main contributions of this paper are": "",
          "(Ma et al., 2023) effectively integrates multimodal": "the conversation. By using a graph convolutional"
        },
        {
          "Overall, the main contributions of this paper are": "and combine ODE to capture the temporal",
          "(Ma et al., 2023) effectively integrates multimodal": ""
        },
        {
          "Overall, the main contributions of this paper are": "",
          "(Ma et al., 2023) effectively integrates multimodal": "network (GCN), DialogGCN can effectively prop-"
        },
        {
          "Overall, the main contributions of this paper are": "dependency of speakers’ emotions.",
          "(Ma et al., 2023) effectively integrates multimodal": ""
        },
        {
          "Overall, the main contributions of this paper are": "",
          "(Ma et al., 2023) effectively integrates multimodal": "agate contextual\ninformation to capture more de-"
        },
        {
          "Overall, the main contributions of this paper are": "",
          "(Ma et al., 2023) effectively integrates multimodal": "tailed emotional dependencies. Based on the idea"
        },
        {
          "Overall, the main contributions of this paper are": "• Extensive\nexperiments\nare\nconducted\nto",
          "(Ma et al., 2023) effectively integrates multimodal": ""
        },
        {
          "Overall, the main contributions of this paper are": "",
          "(Ma et al., 2023) effectively integrates multimodal": "of DialogGCN, SumAggGIN (Sheng et al., 2020)"
        },
        {
          "Overall, the main contributions of this paper are": "demonstrate its\nsuperiority in MERC com-",
          "(Ma et al., 2023) effectively integrates multimodal": ""
        },
        {
          "Overall, the main contributions of this paper are": "",
          "(Ma et al., 2023) effectively integrates multimodal": "further emphasizes the emotional fluctuations in"
        },
        {
          "Overall, the main contributions of this paper are": "pared to various baselines on IEMOCAP and",
          "(Ma et al., 2023) effectively integrates multimodal": ""
        },
        {
          "Overall, the main contributions of this paper are": "",
          "(Ma et al., 2023) effectively integrates multimodal": "the conversation by referencing global topic-related"
        },
        {
          "Overall, the main contributions of this paper are": "MELD datasets.",
          "(Ma et al., 2023) effectively integrates multimodal": ""
        },
        {
          "Overall, the main contributions of this paper are": "",
          "(Ma et al., 2023) effectively integrates multimodal": "emotional phrases and local dependencies. Mean-"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "static images.\nHow to model\nthe temporal\nfea-": "tures in emotion recognition tasks as a continuous",
          "stand the emotional information in multimodal data": "has become a key issue (Ai et al., 2024e)."
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "dynamic process so that\nthe network can capture",
          "stand the emotional information in multimodal data": "Initially, researchers used recurrent neural net-"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "and model the complex patterns of emotion chang-",
          "stand the emotional information in multimodal data": "works\n(RNNs)\nto model\nconversations, which"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "ing over time remains a challenge. Therefore, this",
          "stand the emotional information in multimodal data": "mainly capture emotional information by process-"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "paper aims to propose a Dynamic Graph Neural",
          "stand the emotional information in multimodal data": "ing utterances or entire conversations sequentially"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "Ordinary Differential Equation Network (DGODE)",
          "stand the emotional information in multimodal data": "(Poria et al., 2017).\nFor\ninstance, HiGRU (Jiao"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "to dynamically model the temporal dependency of",
          "stand the emotional information in multimodal data": "et al., 2019) proposed a hierarchical GRU model to"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "emotion changes and improve the expressiveness",
          "stand the emotional information in multimodal data": "capture the information in the conversation, which"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "of node features as the number of GCN layers in-",
          "stand the emotional information in multimodal data": "not only considers the emotional\nfeatures at\nthe"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "creases (Shou et al., 2023c; Ai et al., 2024c; Shou",
          "stand the emotional information in multimodal data": "word level, but also extends to the utterance level,"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "et al., 2024d,g; Ai et al., 2024b,d).",
          "stand the emotional information in multimodal data": "thereby generating a conversation representation"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "Specifically, we introduce the Dynamic Graph",
          "stand the emotional information in multimodal data": "that contains richer contextual information. Simi-"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "Neural Ordinary Differential Equation Network",
          "stand the emotional information in multimodal data": "larly, DialogueRNN (Majumder et al., 2019) also"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "(DGODE) based on the perspective of continu-",
          "stand the emotional information in multimodal data": "uses GRU units to capture the emotional dynam-"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "ous\ntime.\nOur DGODE method introduces an",
          "stand the emotional information in multimodal data": "ics in the conversation, while taking into account"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "adaptive mixhop mechanism to extract node in-",
          "stand the emotional information in multimodal data": "the state of the utterance itself and the emotional"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "formation from different hop count neighbors si-",
          "stand the emotional information in multimodal data": "state of the speaker. Since RNNs cannot achieve"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "multaneously and uses ordinary differential equa-",
          "stand the emotional information in multimodal data": "parallel computing, Transformers have become a"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "tions to model the temporal dependence of emotion",
          "stand the emotional information in multimodal data": "better alternative for sequence modeling (Fan et al.,"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "changes. DGODE shows stable performance as",
          "stand the emotional information in multimodal data": "2023). For example, CTNet (Lian et al., 2021) uses"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "the number of GCN layers increases. We use two",
          "stand the emotional information in multimodal data": "the powerful representation ability of Transform-"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "publicly available multimodal emotion recognition",
          "stand the emotional information in multimodal data": "ers to model\nthe emotional dynamics in conver-"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "datasets to verify the effectiveness of DGODE.",
          "stand the emotional information in multimodal data": "sations through a self-attention mechanism. SDT"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "Overall, the main contributions of this paper are",
          "stand the emotional information in multimodal data": "(Ma et al., 2023) effectively integrates multimodal"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "as follows:",
          "stand the emotional information in multimodal data": "information through Transformer, and uses self-"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "",
          "stand the emotional information in multimodal data": "distillation technology to better learn the potential"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "• We\npropose\nthe Dynamic Graph Neural",
          "stand the emotional information in multimodal data": ""
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "",
          "stand the emotional information in multimodal data": "information in multimodal data."
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "Ordinary\nDifferential\nEquation\nNetwork",
          "stand the emotional information in multimodal data": ""
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "",
          "stand the emotional information in multimodal data": "However, studies have shown (Hu et al., 2021)"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "(DGODE), which\ncombines\nthe\ndynamic",
          "stand the emotional information in multimodal data": ""
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "",
          "stand the emotional information in multimodal data": "that the discourse in the conversation is not just a se-"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "changes of emotions to capture the temporal",
          "stand the emotional information in multimodal data": ""
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "",
          "stand the emotional information in multimodal data": "quential relationship, but a more complex speaker"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "dependency of speakers’ emotions.",
          "stand the emotional information in multimodal data": ""
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "",
          "stand the emotional information in multimodal data": "dependency. Therefore,\nthe DialogGCN (Ghosal"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "",
          "stand the emotional information in multimodal data": "et al., 2019) introduces a graph network to model"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "• We design an adaptive mixhop mechanism to",
          "stand the emotional information in multimodal data": ""
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "",
          "stand the emotional information in multimodal data": "the dependency between the self and the speaker in"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "capture the relationship between distant nodes",
          "stand the emotional information in multimodal data": ""
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "",
          "stand the emotional information in multimodal data": "the conversation. By using a graph convolutional"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "and combine ODE to capture the temporal",
          "stand the emotional information in multimodal data": ""
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "",
          "stand the emotional information in multimodal data": "network (GCN), DialogGCN can effectively prop-"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "dependency of speakers’ emotions.",
          "stand the emotional information in multimodal data": ""
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "",
          "stand the emotional information in multimodal data": "agate contextual\ninformation to capture more de-"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "",
          "stand the emotional information in multimodal data": "tailed emotional dependencies. Based on the idea"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "• Extensive\nexperiments\nare\nconducted\nto",
          "stand the emotional information in multimodal data": ""
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "",
          "stand the emotional information in multimodal data": "of DialogGCN, SumAggGIN (Sheng et al., 2020)"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "demonstrate its\nsuperiority in MERC com-",
          "stand the emotional information in multimodal data": ""
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "",
          "stand the emotional information in multimodal data": "further emphasizes the emotional fluctuations in"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "pared to various baselines on IEMOCAP and",
          "stand the emotional information in multimodal data": ""
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "",
          "stand the emotional information in multimodal data": "the conversation by referencing global topic-related"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "MELD datasets.",
          "stand the emotional information in multimodal data": ""
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "",
          "stand the emotional information in multimodal data": "emotional phrases and local dependencies. Mean-"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "2\nRelated Work",
          "stand the emotional information in multimodal data": "ingwhlie,\nthe DAG-ERC (Shen et al., 2021b) be-"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "",
          "stand the emotional information in multimodal data": "lieves that the discourse in the conversation is not"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "2.1\nMultimodal Emotion Recognition in",
          "stand the emotional information in multimodal data": ""
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "",
          "stand the emotional information in multimodal data": "a simple continuous relationship, but a directed"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "Conversation",
          "stand the emotional information in multimodal data": ""
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "",
          "stand the emotional information in multimodal data": "dependency structure."
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "MERC aims to identify and understand the emo-",
          "stand the emotional information in multimodal data": "With the development of pre-trained language"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "tional state in a conversation by analyzing data",
          "stand the emotional information in multimodal data": "models (PLMs) (Min et al., 2023), researchers be-"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "in multiple modalities (Zhang et al., 2023). With",
          "stand the emotional information in multimodal data": "gan to explore the application of PLM’s power-"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "the rapid development of social media technology,",
          "stand the emotional information in multimodal data": "ful representation capabilities to emotion recogni-"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "people are increasingly communicating in a mul-",
          "stand the emotional information in multimodal data": "tion tasks.\nFor example, DialogXL (Shen et al.,"
        },
        {
          "static images.\nHow to model\nthe temporal\nfea-": "timodal way. Therefore, how to accurately under-",
          "stand the emotional information in multimodal data": "2021a) applies XLNet to emotion recognition and"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "designs an enhanced memory module for storing": "historical context, while modifying the original",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "nection. Our goal is to learn a node representation"
        },
        {
          "designs an enhanced memory module for storing": "self-attention mechanism to capture complex de-",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "matrix H that can capture the structural\ninforma-"
        },
        {
          "designs an enhanced memory module for storing": "pendencies within and between speakers.\nThe",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "tion and feature information of\nthe nodes in the"
        },
        {
          "designs an enhanced memory module for storing": "CoMPM (Lee and Lee, 2022)\nfurther\nleverages",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "graph."
        },
        {
          "designs an enhanced memory module for storing": "PLM by building a pre-trained memory based on",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "We usually normalize the adjacency matrix A."
        },
        {
          "designs an enhanced memory module for storing": "the speaker’s previous utterances, and then combin-",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "The degree matrix D is a diagonal matrix whose"
        },
        {
          "designs an enhanced memory module for storing": "ing the context embedding generated by another",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "diagonal elements Dii represent the degree of node"
        },
        {
          "designs an enhanced memory module for storing": "PLM to generate the final representation of emo-",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "i. However, the eigenvalues of the normalized ma-"
        },
        {
          "designs an enhanced memory module for storing": "tion recognition.\nCoG-BART (Li et al., 2022a)",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "trix may include negative values. Therefore, we"
        },
        {
          "designs an enhanced memory module for storing": "introduces the BART model to understand the con-",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "follow the previous methods (Kipf and Welling,"
        },
        {
          "designs an enhanced memory module for storing": "textual background and generate the next utterance",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "2022) and use a regularized matrix to represent the"
        },
        {
          "designs an enhanced memory module for storing": "as an auxiliary task.",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "graph structure. Specifically, we use the following"
        },
        {
          "designs an enhanced memory module for storing": "",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "symmetric normalized adjacency matrix:"
        },
        {
          "designs an enhanced memory module for storing": "2.2\nContinuous Graph Neural Networks",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": ""
        },
        {
          "designs an enhanced memory module for storing": "",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "(cid:16)\n(cid:17)"
        },
        {
          "designs an enhanced memory module for storing": "Neural ordinary differential equations (ODEs) are",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "α 2\nˆ\nA =\nI + D− 1\n2 AD− 1\n(1)"
        },
        {
          "designs an enhanced memory module for storing": "a novel approach to modeling continuous dynamic",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": ""
        },
        {
          "designs an enhanced memory module for storing": "systems (Chen et al., 2018). They parameterize",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "where α is a hyperparameter."
        },
        {
          "designs an enhanced memory module for storing": "the derivatives of hidden states through neural net-",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": ""
        },
        {
          "designs an enhanced memory module for storing": "",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "3.2\nNeural Ordinary Differential Equation"
        },
        {
          "designs an enhanced memory module for storing": "works, allowing the model to perform continuous",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": ""
        },
        {
          "designs an enhanced memory module for storing": "inference in the time dimension,\nrather\nthan re-",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "Neural\nODEs\nprovides\na\nnew\nmethod\nfor"
        },
        {
          "designs an enhanced memory module for storing": "lying solely on the discrete sequence of hidden",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "continuous-time dynamic modeling by modeling"
        },
        {
          "designs an enhanced memory module for storing": "layers in traditional neural networks. ODEs can",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "the forward propagation process of a neural net-"
        },
        {
          "designs an enhanced memory module for storing": "more accurately describe the changing process over",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "work as the solution process of an ODE. Specifi-"
        },
        {
          "designs an enhanced memory module for storing": "time and are suitable for complex tasks involving",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "cally, consider an input data x(t) and describe its"
        },
        {
          "designs an enhanced memory module for storing": "time evolution. Continuous Graph Neural Network",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "evolution in the form of an ODE:"
        },
        {
          "designs an enhanced memory module for storing": "(CGNN) (Xhonneux et al., 2020) first extended this",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": ""
        },
        {
          "designs an enhanced memory module for storing": "",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "dx(t)"
        },
        {
          "designs an enhanced memory module for storing": "ODE approach to graph data. Specifically, CGNN",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "= f (x(t), t, θ)\n(2)"
        },
        {
          "designs an enhanced memory module for storing": "",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "dt"
        },
        {
          "designs an enhanced memory module for storing": "developed a continuous message passing layer to",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": ""
        },
        {
          "designs an enhanced memory module for storing": "achieve continuous dynamic modeling of node",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "where x(t) represents the hidden state at time t, f"
        },
        {
          "designs an enhanced memory module for storing": "states. Unlike traditional graph neural networks",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "is a neural function with parameter θ."
        },
        {
          "designs an enhanced memory module for storing": "(GCNs), CGNNs no longer rely on a fixed number",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": ""
        },
        {
          "designs an enhanced memory module for storing": "",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "3.3\nMulti-modal Feature Extraction"
        },
        {
          "designs an enhanced memory module for storing": "of layers for information propagation, but instead",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": ""
        },
        {
          "designs an enhanced memory module for storing": "solve ordinary differential equations to enable con-",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "Word Embedding:\nFollowing previous studies"
        },
        {
          "designs an enhanced memory module for storing": "tinuous propagation of information between nodes.",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "(Chudasama et al., 2022; Li et al., 2022b), we use"
        },
        {
          "designs an enhanced memory module for storing": "CGNN also introduces a restart distribution to \"re-",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "RoBERTa (Liu, 2019) to obtain contextual embed-"
        },
        {
          "designs an enhanced memory module for storing": "set\" the node state to the initial state in a timely",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "ding representations of text in this paper."
        },
        {
          "designs an enhanced memory module for storing": "manner during the information propagation pro-",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "Visual and Audio Feature Extraction: Follow-"
        },
        {
          "designs an enhanced memory module for storing": "cess, thereby avoiding the over-smoothing.",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "ing previous work (Ma et al., 2023; Lian et al.,"
        },
        {
          "designs an enhanced memory module for storing": "",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "2021), we selected DenseNet (Huang et al., 2017)"
        },
        {
          "designs an enhanced memory module for storing": "3\nPreliminaries",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "and openSMILE (Eyben et al., 2010) as feature"
        },
        {
          "designs an enhanced memory module for storing": "",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "extraction tools for video and audio."
        },
        {
          "designs an enhanced memory module for storing": "3.1\nGraph Neural Networks",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": ""
        },
        {
          "designs an enhanced memory module for storing": "",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "3.4\nProblem Definition"
        },
        {
          "designs an enhanced memory module for storing": "Given a graph G = (V, E), where V is a set of",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": ""
        },
        {
          "designs an enhanced memory module for storing": "nodes and E is a set of edges. Each node v ∈ V",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "In the multimodal conversational emotion recogni-"
        },
        {
          "designs an enhanced memory module for storing": "constitutes the node feature matrix X ∈ R|V |×d,",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "tion task, given a conversation C, the conversation"
        },
        {
          "designs an enhanced memory module for storing": "where d represents the dimension of\nthe feature.",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "consists of a series of utterances and S different"
        },
        {
          "designs an enhanced memory module for storing": "Each row of X corresponds to the feature repre-",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "speakers. The goal of multimodal emotion recog-"
        },
        {
          "designs an enhanced memory module for storing": "sentation of a node. we use the binary adjacency",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "nition in a conversation is to predict\nthe emotion"
        },
        {
          "designs an enhanced memory module for storing": "matrix A ∈ R|V |×|V |\nto represent the connection",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "label of each utterance in the emotion set Y . Specif-"
        },
        {
          "designs an enhanced memory module for storing": "relationship between node i and node j. If aij = 1,",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "ically, the conversation C can be represented as a"
        },
        {
          "designs an enhanced memory module for storing": "it means that there is an edge between node i and",
          "node j;\nit means that\nthere is no con-\nif aij = 0,": "sequence C = [(u1, s1), (u2, s2), . . . , (uM , sM )],"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Multimodal Feature Extraction": "",
          "Multi-modal Graph Construction": "Embedding Spaces",
          "Information Aggregation": "",
          "Graph ODE": "",
          "Emotion Classifier": ""
        },
        {
          "Multimodal Feature Extraction": "NN-1",
          "Multi-modal Graph Construction": "",
          "Information Aggregation": "",
          "Graph ODE": "",
          "Emotion Classifier": ""
        },
        {
          "Multimodal Feature Extraction": "",
          "Multi-modal Graph Construction": "",
          "Information Aggregation": "",
          "Graph ODE": "Temporal Dependency",
          "Emotion Classifier": ""
        },
        {
          "Multimodal Feature Extraction": "",
          "Multi-modal Graph Construction": "",
          "Information Aggregation": "",
          "Graph ODE": "",
          "Emotion Classifier": "Linear"
        },
        {
          "Multimodal Feature Extraction": "",
          "Multi-modal Graph Construction": "",
          "Information Aggregation": "",
          "Graph ODE": "Capture",
          "Emotion Classifier": ""
        },
        {
          "Multimodal Feature Extraction": "",
          "Multi-modal Graph Construction": "",
          "Information Aggregation": "",
          "Graph ODE": "",
          "Emotion Classifier": "ReLU"
        },
        {
          "Multimodal Feature Extraction": "",
          "Multi-modal Graph Construction": "",
          "Information Aggregation": "",
          "Graph ODE": "NN-3",
          "Emotion Classifier": ""
        },
        {
          "Multimodal Feature Extraction": "Audio",
          "Multi-modal Graph Construction": "",
          "Information Aggregation": "",
          "Graph ODE": "",
          "Emotion Classifier": ""
        },
        {
          "Multimodal Feature Extraction": "",
          "Multi-modal Graph Construction": "",
          "Information Aggregation": "",
          "Graph ODE": "",
          "Emotion Classifier": ""
        },
        {
          "Multimodal Feature Extraction": "",
          "Multi-modal Graph Construction": "",
          "Information Aggregation": "",
          "Graph ODE": "",
          "Emotion Classifier": ""
        },
        {
          "Multimodal Feature Extraction": "",
          "Multi-modal Graph Construction": "",
          "Information Aggregation": "",
          "Graph ODE": "",
          "Emotion Classifier": "Linear"
        },
        {
          "Multimodal Feature Extraction": "",
          "Multi-modal Graph Construction": "",
          "Information Aggregation": "",
          "Graph ODE": "",
          "Emotion Classifier": "…"
        },
        {
          "Multimodal Feature Extraction": "",
          "Multi-modal Graph Construction": "",
          "Information Aggregation": "μ4",
          "Graph ODE": "",
          "Emotion Classifier": ""
        },
        {
          "Multimodal Feature Extraction": "",
          "Multi-modal Graph Construction": "",
          "Information Aggregation": "",
          "Graph ODE": "",
          "Emotion Classifier": "ReLU"
        },
        {
          "Multimodal Feature Extraction": "Video",
          "Multi-modal Graph Construction": "",
          "Information Aggregation": "μ10",
          "Graph ODE": "",
          "Emotion Classifier": ""
        },
        {
          "Multimodal Feature Extraction": "",
          "Multi-modal Graph Construction": "",
          "Information Aggregation": "",
          "Graph ODE": "",
          "Emotion Classifier": ""
        },
        {
          "Multimodal Feature Extraction": "",
          "Multi-modal Graph Construction": "",
          "Information Aggregation": "μ3 \nμ11",
          "Graph ODE": "",
          "Emotion Classifier": ""
        },
        {
          "Multimodal Feature Extraction": "",
          "Multi-modal Graph Construction": "",
          "Information Aggregation": "",
          "Graph ODE": "",
          "Emotion Classifier": "Linear"
        },
        {
          "Multimodal Feature Extraction": "",
          "Multi-modal Graph Construction": "",
          "Information Aggregation": "",
          "Graph ODE": "",
          "Emotion Classifier": ""
        },
        {
          "Multimodal Feature Extraction": "",
          "Multi-modal Graph Construction": "",
          "Information Aggregation": "",
          "Graph ODE": "",
          "Emotion Classifier": "…"
        },
        {
          "Multimodal Feature Extraction": "",
          "Multi-modal Graph Construction": "",
          "Information Aggregation": "",
          "Graph ODE": "",
          "Emotion Classifier": ""
        },
        {
          "Multimodal Feature Extraction": "",
          "Multi-modal Graph Construction": "",
          "Information Aggregation": "μ5",
          "Graph ODE": "",
          "Emotion Classifier": ""
        },
        {
          "Multimodal Feature Extraction": "Text",
          "Multi-modal Graph Construction": "",
          "Information Aggregation": "",
          "Graph ODE": "",
          "Emotion Classifier": ""
        },
        {
          "Multimodal Feature Extraction": "",
          "Multi-modal Graph Construction": "",
          "Information Aggregation": "",
          "Graph ODE": "",
          "Emotion Classifier": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 2: The overall architecture of DGODE.": "where ui represents the i-th utterance in the conver-"
        },
        {
          "Figure 2: The overall architecture of DGODE.": "sation and si represents the unique speaker si ∈ S"
        },
        {
          "Figure 2: The overall architecture of DGODE.": "associated with the utterance. Each utterance ui"
        },
        {
          "Figure 2: The overall architecture of DGODE.": ""
        },
        {
          "Figure 2: The overall architecture of DGODE.": "contains audio data va, video data vf , and text data"
        },
        {
          "Figure 2: The overall architecture of DGODE.": "These multimodal data together express the\nvt."
        },
        {
          "Figure 2: The overall architecture of DGODE.": "meaning and emotion of the utterance. For each"
        },
        {
          "Figure 2: The overall architecture of DGODE.": "utterance ui, we need to determine its emotional"
        },
        {
          "Figure 2: The overall architecture of DGODE.": "state, which is represented by an emotion label ye"
        },
        {
          "Figure 2: The overall architecture of DGODE.": "i ."
        },
        {
          "Figure 2: The overall architecture of DGODE.": ""
        },
        {
          "Figure 2: The overall architecture of DGODE.": "4\nMethodology"
        },
        {
          "Figure 2: The overall architecture of DGODE.": ""
        },
        {
          "Figure 2: The overall architecture of DGODE.": "As shown in Fig. 2, the key steps of DGODE are to"
        },
        {
          "Figure 2: The overall architecture of DGODE.": ""
        },
        {
          "Figure 2: The overall architecture of DGODE.": "alleviate the overfitting problem of existing GCNs"
        },
        {
          "Figure 2: The overall architecture of DGODE.": ""
        },
        {
          "Figure 2: The overall architecture of DGODE.": "and capture the temporal dependency of speaker"
        },
        {
          "Figure 2: The overall architecture of DGODE.": ""
        },
        {
          "Figure 2: The overall architecture of DGODE.": "emotions. DGODE constructs an adaptive mixhop"
        },
        {
          "Figure 2: The overall architecture of DGODE.": ""
        },
        {
          "Figure 2: The overall architecture of DGODE.": "mechanism in the process of node aggregation to"
        },
        {
          "Figure 2: The overall architecture of DGODE.": ""
        },
        {
          "Figure 2: The overall architecture of DGODE.": "reduce the excessive dependence on local features"
        },
        {
          "Figure 2: The overall architecture of DGODE.": ""
        },
        {
          "Figure 2: The overall architecture of DGODE.": "and thus reduce overfitting. Furthermore, we in-"
        },
        {
          "Figure 2: The overall architecture of DGODE.": ""
        },
        {
          "Figure 2: The overall architecture of DGODE.": "troduce Graph ODE to model multimodal data in"
        },
        {
          "Figure 2: The overall architecture of DGODE.": ""
        },
        {
          "Figure 2: The overall architecture of DGODE.": "continuous time through differential equations and"
        },
        {
          "Figure 2: The overall architecture of DGODE.": ""
        },
        {
          "Figure 2: The overall architecture of DGODE.": "capture the temporal dependence of speakers. By"
        },
        {
          "Figure 2: The overall architecture of DGODE.": ""
        },
        {
          "Figure 2: The overall architecture of DGODE.": "solving the ODE equation, the emotional state of"
        },
        {
          "Figure 2: The overall architecture of DGODE.": ""
        },
        {
          "Figure 2: The overall architecture of DGODE.": "the previous moment\nis propagated to the subse-"
        },
        {
          "Figure 2: The overall architecture of DGODE.": ""
        },
        {
          "Figure 2: The overall architecture of DGODE.": "quent moment, allowing the model to capture the"
        },
        {
          "Figure 2: The overall architecture of DGODE.": "changing process of the speaker’s emotion over a"
        },
        {
          "Figure 2: The overall architecture of DGODE.": ""
        },
        {
          "Figure 2: The overall architecture of DGODE.": "longer time range. In this section, we theoretically"
        },
        {
          "Figure 2: The overall architecture of DGODE.": "introduce the Dynamic Graph Neural Ordinary Dif-"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "To effectively encode the features of the conver-": "sation text, we use GRU to capture the contextual",
          "can model the interaction of different features, so": "that we can learn the representation of nodes more"
        },
        {
          "To effectively encode the features of the conver-": "semantic information in the sequence data and gen-",
          "can model the interaction of different features, so": "effectively."
        },
        {
          "To effectively encode the features of the conver-": "erate a more comprehensive text\nrepresentation.",
          "can model the interaction of different features, so": ""
        },
        {
          "To effectively encode the features of the conver-": "",
          "can model the interaction of different features, so": "4.3\nTemporal Graph ODE"
        },
        {
          "To effectively encode the features of the conver-": "Specifically, we can make full use of the time order",
          "can model the interaction of different features, so": ""
        },
        {
          "To effectively encode the features of the conver-": "and dependency relationship in the conversation",
          "can model the interaction of different features, so": "However, information aggregation via Eq. 6 can-"
        },
        {
          "To effectively encode the features of the conver-": "text through GRU, and incorporate the information",
          "can model the interaction of different features, so": "not model\nthe speaker’s emotional changes over"
        },
        {
          "To effectively encode the features of the conver-": "of the previous and next context into the encoding",
          "can model the interaction of different features, so": "time. Therefore, we aim to model the discrete in-"
        },
        {
          "To effectively encode the features of the conver-": "of each round of conversation, so that the generated",
          "can model the interaction of different features, so": "formation propagation process of vanilla GCN as"
        },
        {
          "To effectively encode the features of the conver-": "text features can better reflect the context and mean-",
          "can model the interaction of different features, so": "a continuous process and use ODE to characterize"
        },
        {
          "To effectively encode the features of the conver-": "ing of the entire conversation. Mathematically:",
          "can model the interaction of different features, so": "this dynamic information propagation process. By"
        },
        {
          "To effectively encode the features of the conver-": "",
          "can model the interaction of different features, so": "solving the ODE equation, the emotional state of"
        },
        {
          "To effectively encode the features of the conver-": "←\n−−→",
          "can model the interaction of different features, so": ""
        },
        {
          "To effectively encode the features of the conver-": "vi\nGRU (vi",
          "can model the interaction of different features, so": ""
        },
        {
          "To effectively encode the features of the conver-": "(4)\nm =\nm, ci\nm(+,−)), m ∈ {a, v, f }",
          "can model the interaction of different features, so": "the previous moment\nis propagated to the subse-"
        },
        {
          "To effectively encode the features of the conver-": "",
          "can model the interaction of different features, so": "quent moments, allowing the model to capture the"
        },
        {
          "To effectively encode the features of the conver-": "where ci",
          "can model the interaction of different features, so": ""
        },
        {
          "To effectively encode the features of the conver-": "m(+,−) represents the cell state.",
          "can model the interaction of different features, so": "changing process of the speaker’s emotions over a"
        },
        {
          "To effectively encode the features of the conver-": "To obtain a unimodal representation that reflects",
          "can model the interaction of different features, so": "longer time frame. Specifically, we view Eq. 6 as"
        },
        {
          "To effectively encode the features of the conver-": "both the speaker identity and the context informa-",
          "can model the interaction of different features, so": "the Riemann sum of integrals from t = 0 to t = n,"
        },
        {
          "To effectively encode the features of the conver-": "tion, we add the speaker embedding to the repre-",
          "can model the interaction of different features, so": "as described in the following proposition."
        },
        {
          "To effectively encode the features of the conver-": "sentation of each modality.\nSpecifically,\nfor\nthe",
          "can model the interaction of different features, so": "Proposition 1.\nSuppose A − I = PΛ′P−1,"
        },
        {
          "To effectively encode the features of the conver-": "i-th round of speech in the conversation, we calcu-",
          "can model the interaction of different features, so": "W − I = QΦ′Q−1,\nthen Eq. 6 is discretized as"
        },
        {
          "To effectively encode the features of the conver-": "late the text modality representation hi\nt, the audio",
          "can model the interaction of different features, so": "the ODE as follows:"
        },
        {
          "To effectively encode the features of the conver-": "modality representation hi\na, and the visual modality",
          "can model the interaction of different features, so": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "M3Net (Chen et al., 2023)": "DGODE",
          "60.9": "71.8",
          "78.8": "71.0",
          "70.1": "74.9",
          "68.1": "55.7",
          "77.1": "78.6",
          "67.0": "75.2",
          "71.1": "72.8",
          "79.1": "82.6",
          "59.5": "60.9",
          "13.3": "5.1",
          "42.9": "45.5",
          "65.1": "63.4",
          "21.7": "10.6",
          "53.5": "54.0",
          "65.8": "67.2"
        },
        {
          "M3Net (Chen et al., 2023)": "where pi contains the model’s predicted probability",
          "60.9": "",
          "78.8": "",
          "70.1": "",
          "68.1": "",
          "77.1": "",
          "67.0": "5.1",
          "71.1": "",
          "79.1": "Datasets and Evaluation Metrics",
          "59.5": "",
          "13.3": "",
          "42.9": "",
          "65.1": "",
          "21.7": "",
          "53.5": "",
          "65.8": ""
        },
        {
          "M3Net (Chen et al., 2023)": "",
          "60.9": "for each emotion category, reflecting the model’s",
          "78.8": "",
          "70.1": "",
          "68.1": "",
          "77.1": "",
          "67.0": "",
          "71.1": "",
          "79.1": "",
          "59.5": "",
          "13.3": "",
          "42.9": "",
          "65.1": "",
          "21.7": "",
          "53.5": "",
          "65.8": ""
        },
        {
          "M3Net (Chen et al., 2023)": "",
          "60.9": "",
          "78.8": "",
          "70.1": "",
          "68.1": "",
          "77.1": "",
          "67.0": "",
          "71.1": "We used two used MERC datasets in our experi-",
          "79.1": "",
          "59.5": "",
          "13.3": "",
          "42.9": "",
          "65.1": "",
          "21.7": "",
          "53.5": "",
          "65.8": ""
        },
        {
          "M3Net (Chen et al., 2023)": "",
          "60.9": "confidence in identifying different emotions on the",
          "78.8": "",
          "70.1": "",
          "68.1": "",
          "77.1": "",
          "67.0": "",
          "71.1": "",
          "79.1": "",
          "59.5": "",
          "13.3": "",
          "42.9": "",
          "65.1": "",
          "21.7": "",
          "53.5": "",
          "65.8": ""
        },
        {
          "M3Net (Chen et al., 2023)": "",
          "60.9": "",
          "78.8": "",
          "70.1": "",
          "68.1": "",
          "77.1": "",
          "67.0": "",
          "71.1": "ments: IEMOCAP (Busso et al., 2008) and MELD",
          "79.1": "",
          "59.5": "",
          "13.3": "",
          "42.9": "",
          "65.1": "",
          "21.7": "",
          "53.5": "",
          "65.8": ""
        },
        {
          "M3Net (Chen et al., 2023)": "",
          "60.9": "utterance, W l, W smax, bl, and bsmax are trainable",
          "78.8": "",
          "70.1": "",
          "68.1": "",
          "77.1": "",
          "67.0": "",
          "71.1": "",
          "79.1": "",
          "59.5": "",
          "13.3": "",
          "42.9": "",
          "65.1": "",
          "21.7": "",
          "53.5": "",
          "65.8": ""
        },
        {
          "M3Net (Chen et al., 2023)": "",
          "60.9": "",
          "78.8": "",
          "70.1": "",
          "68.1": "",
          "77.1": "",
          "67.0": "",
          "71.1": "(Poria et al., 2019). Both datasets contain data in",
          "79.1": "",
          "59.5": "",
          "13.3": "",
          "42.9": "",
          "65.1": "",
          "21.7": "",
          "53.5": "",
          "65.8": ""
        },
        {
          "M3Net (Chen et al., 2023)": "",
          "60.9": "parameters. To obtain the final emotion prediction",
          "78.8": "",
          "70.1": "",
          "68.1": "",
          "77.1": "",
          "67.0": "",
          "71.1": "",
          "79.1": "",
          "59.5": "",
          "13.3": "",
          "42.9": "",
          "65.1": "",
          "21.7": "",
          "53.5": "",
          "65.8": ""
        },
        {
          "M3Net (Chen et al., 2023)": "",
          "60.9": "",
          "78.8": "",
          "70.1": "",
          "68.1": "",
          "77.1": "",
          "67.0": "",
          "71.1": "three modalities:",
          "79.1": "",
          "59.5": "",
          "13.3": "text, audio, and video. The IEMO-",
          "42.9": "",
          "65.1": "",
          "21.7": "",
          "53.5": "",
          "65.8": ""
        },
        {
          "M3Net (Chen et al., 2023)": "",
          "60.9": "result, we select the emotion category label ˆyi with",
          "78.8": "",
          "70.1": "",
          "68.1": "",
          "77.1": "",
          "67.0": "",
          "71.1": "",
          "79.1": "",
          "59.5": "",
          "13.3": "",
          "42.9": "",
          "65.1": "",
          "21.7": "",
          "53.5": "",
          "65.8": ""
        },
        {
          "M3Net (Chen et al., 2023)": "",
          "60.9": "",
          "78.8": "",
          "70.1": "",
          "68.1": "",
          "77.1": "",
          "67.0": "",
          "71.1": "CAP dataset is collected from dialogue scenes per-",
          "79.1": "",
          "59.5": "",
          "13.3": "",
          "42.9": "",
          "65.1": "",
          "21.7": "",
          "53.5": "",
          "65.8": ""
        },
        {
          "M3Net (Chen et al., 2023)": "",
          "60.9": "the highest probability from pi as the predicted",
          "78.8": "",
          "70.1": "",
          "68.1": "",
          "77.1": "",
          "67.0": "",
          "71.1": "",
          "79.1": "",
          "59.5": "",
          "13.3": "",
          "42.9": "",
          "65.1": "",
          "21.7": "",
          "53.5": "",
          "65.8": ""
        },
        {
          "M3Net (Chen et al., 2023)": "",
          "60.9": "",
          "78.8": "",
          "70.1": "",
          "68.1": "",
          "77.1": "",
          "67.0": "",
          "71.1": "formed by actors. The MELD dataset consists of di-",
          "79.1": "",
          "59.5": "",
          "13.3": "",
          "42.9": "",
          "65.1": "",
          "21.7": "",
          "53.5": "",
          "65.8": ""
        },
        {
          "M3Net (Chen et al., 2023)": "",
          "60.9": "emotion of the utterance as follows:",
          "78.8": "",
          "70.1": "",
          "68.1": "",
          "77.1": "",
          "67.0": "",
          "71.1": "",
          "79.1": "",
          "59.5": "",
          "13.3": "",
          "42.9": "",
          "65.1": "",
          "21.7": "",
          "53.5": "",
          "65.8": ""
        },
        {
          "M3Net (Chen et al., 2023)": "",
          "60.9": "",
          "78.8": "",
          "70.1": "",
          "68.1": "",
          "77.1": "",
          "67.0": "",
          "71.1": "alogue clips from the American TV series Friends.",
          "79.1": "",
          "59.5": "",
          "13.3": "",
          "42.9": "",
          "65.1": "",
          "21.7": "",
          "53.5": "",
          "65.8": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": ""
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": ""
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "Happy Sad Neutral Angry Excited Frustrated W-F1 Neutral Surprise Fear Sadness"
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "34.4"
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "50.6"
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "42.7"
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "51.6"
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "60.7"
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "56.4"
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "51.3"
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "55.5"
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "47.1"
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "53.0"
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "58.8"
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "60.9"
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "71.8"
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "where pi contains the model’s predicted probability"
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "for each emotion category, reflecting the model’s"
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": ""
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "confidence in identifying different emotions on the"
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": ""
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "utterance, W l, W smax, bl, and bsmax are trainable"
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": ""
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "parameters. To obtain the final emotion prediction"
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": ""
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "result, we select the emotion category label ˆyi with"
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": ""
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "the highest probability from pi as the predicted"
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": ""
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "emotion of the utterance as follows:"
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": ""
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": ""
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "yi = arg max"
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "j"
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": ""
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "Implementation Details"
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": ""
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "implement"
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": ""
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "DGODE model and chose Adam as the optimizer."
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": ""
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": ""
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": ""
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "to 1e-4, while for the MELD"
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": ""
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "the learning rate was set"
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": ""
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "ing training, the batch size of IEMOCAP was 16,"
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": ""
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "while the batch size of MELD was 8. In the setting"
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": ""
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "of the Bi-GRU layer, we set different numbers of"
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": ""
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "channels for different modal inputs. In the IEMO-"
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": ""
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "CAP dataset, the number of input channels for text,"
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": ""
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "acoustic, and visual modalities are 1024, 1582, and"
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": ""
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "342, respectively. In the MELD dataset, the num-"
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "ber of input channels for text, acoustic, and visual"
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": ""
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "to 1024, 300, and 342, respec-"
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": ""
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "In addition, for the graph encoder, we set"
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": ""
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": ""
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": ""
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "overfitting of the model, we introduced L2 weight"
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": ""
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "decay in training, with the coefficient set to 1e-5,"
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": ""
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "and applied a dropout rate of 0.5 in the key layers."
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": ""
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": ""
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": ""
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": ""
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "Our experimental results are the average of 10 runs"
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": "and are statistically significant under paired t-test"
        },
        {
          "Table 1: Comparison with other baselines on the IEMOCAP and MELD dataset.": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "T\nA": "",
          "V": ""
        },
        {
          "T\nA": "T+A\nT+V",
          "V": "T+A+V"
        },
        {
          "T\nA": "",
          "V": ""
        },
        {
          "T\nA": "",
          "V": ""
        },
        {
          "T\nA": "72.8",
          "V": ""
        },
        {
          "T\nA": "",
          "V": ""
        },
        {
          "T\nA": "",
          "V": ""
        },
        {
          "T\nA": "71.6",
          "V": ""
        },
        {
          "T\nA": "",
          "V": ""
        },
        {
          "T\nA": "69.8",
          "V": ""
        },
        {
          "T\nA": "",
          "V": ""
        },
        {
          "T\nA": "",
          "V": ""
        },
        {
          "T\nA": "",
          "V": ""
        },
        {
          "T\nA": "",
          "V": "67.2"
        },
        {
          "T\nA": "",
          "V": ""
        },
        {
          "T\nA": "",
          "V": ""
        },
        {
          "T\nA": "65.7",
          "V": ""
        },
        {
          "T\nA": "",
          "V": "65.2"
        },
        {
          "T\nA": "",
          "V": ""
        },
        {
          "T\nA": "63.7",
          "V": ""
        },
        {
          "T\nA": "",
          "V": ""
        },
        {
          "T\nA": "",
          "V": ""
        },
        {
          "T\nA": "",
          "V": ""
        },
        {
          "T\nA": "",
          "V": ""
        },
        {
          "T\nA": "",
          "V": ""
        },
        {
          "T\nA": "IEMOCAP",
          "V": ""
        },
        {
          "T\nA": "",
          "V": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "60": ""
        },
        {
          "60": "IEMOCAP\nMELD"
        },
        {
          "60": ""
        },
        {
          "60": "Figure 3: Verify the effectiveness of multimodal fea-"
        },
        {
          "60": "tures."
        },
        {
          "60": ""
        },
        {
          "60": ""
        },
        {
          "60": ""
        },
        {
          "60": ""
        },
        {
          "60": "has better W-F1 scores than other methods on the"
        },
        {
          "60": ""
        },
        {
          "60": ""
        },
        {
          "60": ""
        },
        {
          "60": "three categories of emotions \"happy\", \"neutral\" and"
        },
        {
          "60": ""
        },
        {
          "60": "\"frustrated\". Similarly, in the MELD dataset, our"
        },
        {
          "60": ""
        },
        {
          "60": "model also achieves the best W-F1 scores on the"
        },
        {
          "60": ""
        },
        {
          "60": "three categories of emotions \"surprise\", \"neutral\""
        },
        {
          "60": "and \"sadness\", further verifying the robustness of"
        },
        {
          "60": "the model. Therefore, our method not only per-"
        },
        {
          "60": "forms well in emotion recognition tasks, but also"
        },
        {
          "60": "has significant advantages in model complexity."
        },
        {
          "60": ""
        },
        {
          "60": ""
        },
        {
          "60": "5.4\nEffectiveness of Multimodal Features"
        },
        {
          "60": ""
        },
        {
          "60": "We analyze the impact of different modal features"
        },
        {
          "60": "on the results of emotion recognition experiments"
        },
        {
          "60": ""
        },
        {
          "60": "to verify the effect of different modal feature com-"
        },
        {
          "60": ""
        },
        {
          "60": "binations. Specifically, we observe the contribu-"
        },
        {
          "60": ""
        },
        {
          "60": "tion of different modal features (text, audio, and"
        },
        {
          "60": ""
        },
        {
          "60": "video) to the emotion recognition performance by"
        },
        {
          "60": ""
        },
        {
          "60": "inputting them into the model. The experimental"
        },
        {
          "60": ""
        },
        {
          "60": "results are shown in Fig. 3. 1) In the single-modal"
        },
        {
          "60": ""
        },
        {
          "60": "experiment,\nthe emotion recognition accuracy of"
        },
        {
          "60": ""
        },
        {
          "60": "the text modality is significantly better than the au-"
        },
        {
          "60": ""
        },
        {
          "60": "dio and video modalities. 2) When we combine the"
        },
        {
          "60": ""
        },
        {
          "60": "features of the two modalities for the experiment,"
        },
        {
          "60": ""
        },
        {
          "60": "the effect of emotion recognition is significantly"
        },
        {
          "60": ""
        },
        {
          "60": "better than the results of any single modality. 3)"
        },
        {
          "60": ""
        },
        {
          "60": "When we use the features of the three modalities"
        },
        {
          "60": ""
        },
        {
          "60": "for emotion recognition at the same time, the per-"
        },
        {
          "60": ""
        },
        {
          "60": "formance of the model reaches the best level."
        },
        {
          "60": ""
        },
        {
          "60": "5.5\nError Analysis"
        },
        {
          "60": "Although the proposed DGODE model has shown"
        },
        {
          "60": ""
        },
        {
          "60": "good results in the emotion recognition task, it still"
        },
        {
          "60": "faces some challenges, especially in the recognition"
        },
        {
          "60": "of some emotions. To analyze the misclassification"
        },
        {
          "60": "of the model in more depth, we analyzed the con-"
        },
        {
          "60": "fusion matrix of\nthe test set on the two datasets."
        },
        {
          "60": "As shown in Fig. 4, DGODE has the problem of"
        },
        {
          "60": "misclassifying similar emotions on the IEMOCAP"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Happy": "Sad",
          "Neutral": "Surprise"
        },
        {
          "Happy": "Neutral",
          "Neutral": "Fear"
        },
        {
          "Happy": "Angry",
          "Neutral": "Sadness"
        },
        {
          "Happy": "Excited",
          "Neutral": "Joy"
        },
        {
          "Happy": "Frustrated",
          "Neutral": "Disgust"
        },
        {
          "Happy": "",
          "Neutral": "Anger"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 6: Visualization of the learned embeddings.": "performance, which highlights the role of ODE in"
        },
        {
          "Figure 6: Visualization of the learned embeddings.": "capturing the dynamics of multimodal data. Re-"
        },
        {
          "Figure 6: Visualization of the learned embeddings.": ""
        },
        {
          "Figure 6: Visualization of the learned embeddings.": "moving the adaptive mixHop graph also degrades"
        },
        {
          "Figure 6: Visualization of the learned embeddings.": ""
        },
        {
          "Figure 6: Visualization of the learned embeddings.": "the performance, which emphasizes the importance"
        },
        {
          "Figure 6: Visualization of the learned embeddings.": ""
        },
        {
          "Figure 6: Visualization of the learned embeddings.": "of capturing high-order relationships."
        },
        {
          "Figure 6: Visualization of the learned embeddings.": ""
        },
        {
          "Figure 6: Visualization of the learned embeddings.": ""
        },
        {
          "Figure 6: Visualization of the learned embeddings.": ""
        },
        {
          "Figure 6: Visualization of the learned embeddings.": "5.7\nVisualization"
        },
        {
          "Figure 6: Visualization of the learned embeddings.": ""
        },
        {
          "Figure 6: Visualization of the learned embeddings.": ""
        },
        {
          "Figure 6: Visualization of the learned embeddings.": "To more intuitively demonstrate the classification"
        },
        {
          "Figure 6: Visualization of the learned embeddings.": ""
        },
        {
          "Figure 6: Visualization of the learned embeddings.": "effect of DGODE method in the MERC task, we"
        },
        {
          "Figure 6: Visualization of the learned embeddings.": ""
        },
        {
          "Figure 6: Visualization of the learned embeddings.": "use T-SNE to visualize the generated sentence vec-"
        },
        {
          "Figure 6: Visualization of the learned embeddings.": ""
        },
        {
          "Figure 6: Visualization of the learned embeddings.": "tors.\nAs\nshown in Fig.\n6, on the IEMOCAP"
        },
        {
          "Figure 6: Visualization of the learned embeddings.": ""
        },
        {
          "Figure 6: Visualization of the learned embeddings.": "dataset,\nthe DGODE model performs well, and"
        },
        {
          "Figure 6: Visualization of the learned embeddings.": ""
        },
        {
          "Figure 6: Visualization of the learned embeddings.": "samples of different emotion categories are effec-"
        },
        {
          "Figure 6: Visualization of the learned embeddings.": "tively separated in the visualization.\nIn contrast,"
        },
        {
          "Figure 6: Visualization of the learned embeddings.": ""
        },
        {
          "Figure 6: Visualization of the learned embeddings.": "although the MMGCN model can also distinguish"
        },
        {
          "Figure 6: Visualization of the learned embeddings.": "samples of different emotion categories to some"
        },
        {
          "Figure 6: Visualization of the learned embeddings.": ""
        },
        {
          "Figure 6: Visualization of the learned embeddings.": "extent, its classification performance is obviously"
        },
        {
          "Figure 6: Visualization of the learned embeddings.": ""
        },
        {
          "Figure 6: Visualization of the learned embeddings.": "inferior to DGODE. The distribution of samples"
        },
        {
          "Figure 6: Visualization of the learned embeddings.": ""
        },
        {
          "Figure 6: Visualization of the learned embeddings.": "generated by MMGCN is relatively chaotic, and the"
        },
        {
          "Figure 6: Visualization of the learned embeddings.": ""
        },
        {
          "Figure 6: Visualization of the learned embeddings.": "boundaries between different emotion categories"
        },
        {
          "Figure 6: Visualization of the learned embeddings.": ""
        },
        {
          "Figure 6: Visualization of the learned embeddings.": "are unclear. Meaningwhile, we also compared the"
        },
        {
          "Figure 6: Visualization of the learned embeddings.": "classification effect of the M3Net model. Similar"
        },
        {
          "Figure 6: Visualization of the learned embeddings.": ""
        },
        {
          "Figure 6: Visualization of the learned embeddings.": "to DGODE, M3Net also showed good classifica-"
        },
        {
          "Figure 6: Visualization of the learned embeddings.": "tion performance on the IEMOCAP dataset, and"
        },
        {
          "Figure 6: Visualization of the learned embeddings.": "was able to clearly separate samples of different"
        },
        {
          "Figure 6: Visualization of the learned embeddings.": "emotion categories.\nIn the experimental\nresults"
        },
        {
          "Figure 6: Visualization of the learned embeddings.": "on the MELD dataset, we observed a similar phe-"
        },
        {
          "Figure 6: Visualization of the learned embeddings.": "nomenon."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "9\nLimitations": "",
          "dyadic motion capture database. Language resources": "and evaluation, 42:335–359."
        },
        {
          "9\nLimitations": "In multimodal emotion recognition, emotion labels",
          "dyadic motion capture database. Language resources": ""
        },
        {
          "9\nLimitations": "",
          "dyadic motion capture database. Language resources": "Feiyu Chen,\nJie Shao, Shuyuan Zhu, and Heng Tao"
        },
        {
          "9\nLimitations": "are usually annotated for the overall emotion of a",
          "dyadic motion capture database. Language resources": ""
        },
        {
          "9\nLimitations": "",
          "dyadic motion capture database. Language resources": "Shen. 2023. Multivariate, multi-frequency and multi-"
        },
        {
          "9\nLimitations": "certain period of time. However, DGODE focuses",
          "dyadic motion capture database. Language resources": ""
        },
        {
          "9\nLimitations": "",
          "dyadic motion capture database. Language resources": "modal: Rethinking graph neural networks for emo-"
        },
        {
          "9\nLimitations": "on dynamic changes, which may lead to the prob-",
          "dyadic motion capture database. Language resources": ""
        },
        {
          "9\nLimitations": "",
          "dyadic motion capture database. Language resources": "tion recognition in conversation.\nIn Proceedings of"
        },
        {
          "9\nLimitations": "lem that the subtle dynamic changes captured by",
          "dyadic motion capture database. Language resources": "the IEEE/CVF Conference on Computer Vision and"
        },
        {
          "9\nLimitations": "",
          "dyadic motion capture database. Language resources": "Pattern Recognition, pages 10761–10770."
        },
        {
          "9\nLimitations": "the model do not match the overall emotion labels.",
          "dyadic motion capture database. Language resources": ""
        },
        {
          "9\nLimitations": "",
          "dyadic motion capture database. Language resources": "Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt,"
        },
        {
          "9\nLimitations": "",
          "dyadic motion capture database. Language resources": "and David K Duvenaud. 2018. Neural ordinary dif-"
        },
        {
          "9\nLimitations": "References",
          "dyadic motion capture database. Language resources": ""
        },
        {
          "9\nLimitations": "",
          "dyadic motion capture database. Language resources": "ferential equations. Advances in Neural Information"
        },
        {
          "9\nLimitations": "",
          "dyadic motion capture database. Language resources": "Processing Systems, 31."
        },
        {
          "9\nLimitations": "Wei Ai, Wen Deng, Hongyi Chen, Jiayi Du, Tao Meng,",
          "dyadic motion capture database. Language resources": ""
        },
        {
          "9\nLimitations": "and Yuntao Shou. 2024a. Mcsff: Multi-modal con-",
          "dyadic motion capture database. Language resources": ""
        },
        {
          "9\nLimitations": "",
          "dyadic motion capture database. Language resources": "Vishal Chudasama, Purbayan Kar, Ashish Gudmalwar,"
        },
        {
          "9\nLimitations": "sistency and specificity fusion framework for entity",
          "dyadic motion capture database. Language resources": ""
        },
        {
          "9\nLimitations": "",
          "dyadic motion capture database. Language resources": "Nirmesh Shah, Pankaj Wasnik, and Naoyuki Onoe."
        },
        {
          "9\nLimitations": "alignment. arXiv preprint arXiv:2410.14584.",
          "dyadic motion capture database. Language resources": ""
        },
        {
          "9\nLimitations": "",
          "dyadic motion capture database. Language resources": "2022. M2fnet: Multi-modal fusion network for emo-"
        },
        {
          "9\nLimitations": "",
          "dyadic motion capture database. Language resources": "tion recognition in conversation.\nIn Proceedings of"
        },
        {
          "9\nLimitations": "Wei Ai, Yinghui Gao,\nJianbin Li,\nJiayi Du,\nTao",
          "dyadic motion capture database. Language resources": ""
        },
        {
          "9\nLimitations": "",
          "dyadic motion capture database. Language resources": "the IEEE/CVF Conference on Computer Vision and"
        },
        {
          "9\nLimitations": "Meng, Yuntao Shou, and Keqin Li. 2024b.\nSeg:",
          "dyadic motion capture database. Language resources": ""
        },
        {
          "9\nLimitations": "",
          "dyadic motion capture database. Language resources": "Pattern Recognition, pages 4652–4661."
        },
        {
          "9\nLimitations": "Seeds-enhanced iterative refinement graph neural",
          "dyadic motion capture database. Language resources": ""
        },
        {
          "9\nLimitations": "arXiv\npreprint\nnetwork\nfor\nentity\nalignment.",
          "dyadic motion capture database. Language resources": ""
        },
        {
          "9\nLimitations": "",
          "dyadic motion capture database. Language resources": "Florian Eyben, Martin Wöllmer, and Björn Schuller."
        },
        {
          "9\nLimitations": "arXiv:2410.20733.",
          "dyadic motion capture database. Language resources": ""
        },
        {
          "9\nLimitations": "",
          "dyadic motion capture database. Language resources": "2010. Opensmile:\nthe munich versatile and fast open-"
        },
        {
          "9\nLimitations": "",
          "dyadic motion capture database. Language resources": "source audio feature extractor.\nIn Proceedings of the"
        },
        {
          "9\nLimitations": "Wei Ai, Jianbin Li, Ze Wang, Jiayi Du, Tao Meng, Yun-",
          "dyadic motion capture database. Language resources": ""
        },
        {
          "9\nLimitations": "",
          "dyadic motion capture database. Language resources": "18th ACM International Conference on Multimedia,"
        },
        {
          "9\nLimitations": "tao Shou, and Keqin Li. 2024c. Graph contrastive",
          "dyadic motion capture database. Language resources": ""
        },
        {
          "9\nLimitations": "",
          "dyadic motion capture database. Language resources": "pages 1459–1462."
        },
        {
          "9\nLimitations": "learning via cluster-refined negative sampling for",
          "dyadic motion capture database. Language resources": ""
        },
        {
          "9\nLimitations": "arXiv preprint\nsemi-supervised text classification.",
          "dyadic motion capture database. Language resources": ""
        },
        {
          "9\nLimitations": "",
          "dyadic motion capture database. Language resources": "Weiquan Fan, Xiaofen Xing, Bolun Cai, and Xiangmin"
        },
        {
          "9\nLimitations": "arXiv:2410.18130.",
          "dyadic motion capture database. Language resources": ""
        },
        {
          "9\nLimitations": "",
          "dyadic motion capture database. Language resources": "Xu. 2023. Mgat: Multi-granularity attention based"
        },
        {
          "9\nLimitations": "",
          "dyadic motion capture database. Language resources": "transformers for multi-modal emotion recognition."
        },
        {
          "9\nLimitations": "Wei Ai, Jianbin Li, Ze Wang, Yingying Wei, Tao Meng,",
          "dyadic motion capture database. Language resources": ""
        },
        {
          "9\nLimitations": "",
          "dyadic motion capture database. Language resources": "In ICASSP 2023-2023 IEEE International Confer-"
        },
        {
          "9\nLimitations": "Yuntao Shou, and Keqin Lib. 2024d.\nContrastive",
          "dyadic motion capture database. Language resources": ""
        },
        {
          "9\nLimitations": "",
          "dyadic motion capture database. Language resources": "ence on Acoustics, Speech and Signal Processing"
        },
        {
          "9\nLimitations": "multi-graph learning with neighbor hierarchical sift-",
          "dyadic motion capture database. Language resources": ""
        },
        {
          "9\nLimitations": "",
          "dyadic motion capture database. Language resources": "(ICASSP), pages 1–5. IEEE."
        },
        {
          "9\nLimitations": "arXiv\ning for semi-supervised text classification.",
          "dyadic motion capture database. Language resources": ""
        },
        {
          "9\nLimitations": "preprint arXiv:2411.16787.",
          "dyadic motion capture database. Language resources": ""
        },
        {
          "9\nLimitations": "",
          "dyadic motion capture database. Language resources": "Deepanway Ghosal, Navonil Majumder, Soujanya Poria,"
        },
        {
          "9\nLimitations": "",
          "dyadic motion capture database. Language resources": "Niyati Chhaya, and Alexander Gelbukh. 2019. Dia-"
        },
        {
          "9\nLimitations": "Wei Ai, Yuntao Shou, Tao Meng, and Keqin Li. 2024e.",
          "dyadic motion capture database. Language resources": ""
        },
        {
          "9\nLimitations": "",
          "dyadic motion capture database. Language resources": "loguegcn: A graph convolutional neural network for"
        },
        {
          "9\nLimitations": "Der-gcn: Dialog and event relation-aware graph con-",
          "dyadic motion capture database. Language resources": ""
        },
        {
          "9\nLimitations": "",
          "dyadic motion capture database. Language resources": "emotion recognition in conversation.\nIn Proceedings"
        },
        {
          "9\nLimitations": "volutional neural network for multimodal dialog emo-",
          "dyadic motion capture database. Language resources": ""
        },
        {
          "9\nLimitations": "",
          "dyadic motion capture database. Language resources": "of the 2019 Conference on Empirical Methods in Nat-"
        },
        {
          "9\nLimitations": "IEEE Transactions on Neural Net-\ntion recognition.",
          "dyadic motion capture database. Language resources": ""
        },
        {
          "9\nLimitations": "",
          "dyadic motion capture database. Language resources": "ural Language Processing and the 9th International"
        },
        {
          "9\nLimitations": "works and Learning Systems.",
          "dyadic motion capture database. Language resources": ""
        },
        {
          "9\nLimitations": "",
          "dyadic motion capture database. Language resources": "Joint Conference on Natural Language Processing"
        },
        {
          "9\nLimitations": "Wei Ai, Yuntao Shou, Tao Meng, Nan Yin, and Keqin",
          "dyadic motion capture database. Language resources": "(EMNLP-IJCNLP), pages 154–164."
        },
        {
          "9\nLimitations": "Li. 2023a. Der-gcn: Dialogue and event\nrelation-",
          "dyadic motion capture database. Language resources": ""
        },
        {
          "9\nLimitations": "",
          "dyadic motion capture database. Language resources": "Jingwen Hu, Yuchen Liu, Jinming Zhao, and Qin Jin."
        },
        {
          "9\nLimitations": "aware graph convolutional neural network for multi-",
          "dyadic motion capture database. Language resources": ""
        },
        {
          "9\nLimitations": "",
          "dyadic motion capture database. Language resources": "2021. Mmgcn: Multimodal fusion via deep graph"
        },
        {
          "9\nLimitations": "modal dialogue emotion recognition. arXiv preprint",
          "dyadic motion capture database. Language resources": ""
        },
        {
          "9\nLimitations": "",
          "dyadic motion capture database. Language resources": "convolution network for emotion recognition in con-"
        },
        {
          "9\nLimitations": "arXiv:2312.10579.",
          "dyadic motion capture database. Language resources": ""
        },
        {
          "9\nLimitations": "",
          "dyadic motion capture database. Language resources": "versation.\nIn Proceedings of the 59th Annual Meet-"
        },
        {
          "9\nLimitations": "Wei Ai, Yingying Wei, Hongen Shao, Yuntao Shou,",
          "dyadic motion capture database. Language resources": "ing of the Association for Computational Linguistics"
        },
        {
          "9\nLimitations": "Tao Meng, and Keqin Li. 2024f.\nEdge-enhanced",
          "dyadic motion capture database. Language resources": "and the 11th International Joint Conference on Natu-"
        },
        {
          "9\nLimitations": "minimum-margin graph attention network for short",
          "dyadic motion capture database. Language resources": "ral Language Processing (Volume 1: Long Papers),"
        },
        {
          "9\nLimitations": "text classification. Expert Systems with Applications,",
          "dyadic motion capture database. Language resources": "pages 5666–5675."
        },
        {
          "9\nLimitations": "251:124069.",
          "dyadic motion capture database. Language resources": ""
        },
        {
          "9\nLimitations": "",
          "dyadic motion capture database. Language resources": "Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and"
        },
        {
          "9\nLimitations": "Wei Ai, FuChen Zhang, Tao Meng, YunTao Shou, Hon-",
          "dyadic motion capture database. Language resources": "Kilian Q Weinberger. 2017. Densely connected con-"
        },
        {
          "9\nLimitations": "gEn Shao, and Keqin Li. 2023b. A two-stage mul-",
          "dyadic motion capture database. Language resources": "the IEEE\nvolutional networks.\nIn Proceedings of"
        },
        {
          "9\nLimitations": "timodal emotion recognition model based on graph",
          "dyadic motion capture database. Language resources": "Conference on Computer Vision and Pattern Recog-"
        },
        {
          "9\nLimitations": "contrastive learning.\nIn 2023 IEEE 29th Interna-",
          "dyadic motion capture database. Language resources": "nition, pages 4700–4708."
        },
        {
          "9\nLimitations": "tional Conference on Parallel and Distributed Sys-",
          "dyadic motion capture database. Language resources": ""
        },
        {
          "9\nLimitations": "tems (ICPADS), pages 397–404. IEEE.",
          "dyadic motion capture database. Language resources": "Taichi Ishiwatari, Yuki Yasuda, Taro Miyazaki, and Jun"
        },
        {
          "9\nLimitations": "",
          "dyadic motion capture database. Language resources": "Goto. 2020. Relation-aware graph attention networks"
        },
        {
          "9\nLimitations": "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe",
          "dyadic motion capture database. Language resources": "with relational position encodings for emotion recog-"
        },
        {
          "9\nLimitations": "Kazemzadeh, Emily Mower, Samuel Kim,\nJean-",
          "dyadic motion capture database. Language resources": "nition in conversations.\nIn Proceedings of the 2020"
        },
        {
          "9\nLimitations": "nette N Chang,\nSungbok Lee,\nand Shrikanth S",
          "dyadic motion capture database. Language resources": "conference on empirical methods in natural language"
        },
        {
          "9\nLimitations": "Narayanan. 2008.\nIemocap:\nInteractive emotional",
          "dyadic motion capture database. Language resources": "processing (EMNLP), pages 7360–7370."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Lyu. 2019. Higru: Hierarchical gated recurrent units",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "Li. 2024b. Deep imbalanced learning for multimodal"
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "for utterance-level emotion recognition.\nIn Proceed-",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "IEEE Transac-\nemotion recognition in conversations."
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "ings of the 2019 Conference of the North American",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "tions on Artificial Intelligence."
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Chapter of\nthe Association for Computational Lin-",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": ""
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "Tao Meng,\nFuchen Zhang, Yuntao Shou, Wei Ai,"
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "guistics: Human Language Technologies, Volume 1",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": ""
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "Nan Yin,\nand Keqin Li. 2024c.\nRevisiting mul-"
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "(Long and Short Papers), pages 397–406.",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": ""
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "timodal emotion recognition in conversation from"
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "arXiv preprint\nthe perspective of graph spectrum."
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Taewoon Kim and Piek Vossen. 2021.\nEmoberta:",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": ""
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "arXiv:2404.17862."
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Speaker-aware emotion recognition in conversation",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": ""
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "with roberta. arXiv preprint arXiv:2108.12009.",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": ""
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "Tao Meng, Fuchen Zhang, Yuntao Shou, Hongen Shao,"
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "Wei Ai, and Keqin Li. 2024d. Masked graph learn-"
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Thomas N Kipf\nand Max Welling.\n2022.\nSemi-",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": ""
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "ing with recurrent alignment for multimodal emotion"
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "supervised classification with graph convolutional",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": ""
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "IEEE/ACM Transac-\nrecognition in conversation."
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "networks.\nIn International Conference on Learning",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": ""
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "tions on Audio, Speech, and Language Processing."
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Representations.",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": ""
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "Bonan Min,\nHayley\nRoss,\nElior\nSulem,\nAmir"
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Joosung Lee and Wooin Lee. 2022. Compm: Context",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "Pouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz,"
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "modeling with speaker’s pre-trained memory track-",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "Eneko Agirre,\nIlana Heintz, and Dan Roth. 2023."
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "ing for emotion recognition in conversation.\nIn Pro-",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "Recent advances in natural language processing via"
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "ceedings of the 2022 Conference of the North Amer-",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "large pre-trained language models: A survey. ACM"
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "ican Chapter of the Association for Computational",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "Computing Surveys, 56(2):1–40."
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Linguistics: Human Language Technologies, pages",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": ""
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "5669–5679.",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "Soujanya Poria, Erik Cambria, Devamanyu Hazarika,"
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "Navonil Majumder, Amir Zadeh, and Louis-Philippe"
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "Morency. 2017. Context-dependent sentiment anal-"
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Shimin Li, Hang Yan, and Xipeng Qiu. 2022a. Contrast",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": ""
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "ysis in user-generated videos.\nIn Proceedings of the"
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "and generation make bart a good dialogue emotion",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": ""
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "55th annual meeting of\nthe association for compu-"
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "recognizer.\nIn Proceedings of the AAAI Conference",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": ""
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "tational linguistics (volume 1: Long papers), pages"
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "on Artificial Intelligence, volume 36, pages 11002–",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": ""
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "873–883."
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "11010.",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": ""
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-"
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Zaijing Li, Fengxiao Tang, Ming Zhao, and Yusen Zhu.",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": ""
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "jumder, Gautam Naik, Erik Cambria, and Rada Mi-"
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "2022b. Emocaps: Emotion capsule based model for",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": ""
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "halcea. 2019.\nMeld:\nA multimodal multi-party"
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "conversational emotion recognition.\nIn Findings of",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": ""
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "dataset for emotion recognition in conversations.\nIn"
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "the Association for Computational Linguistics: ACL",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": ""
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "Proceedings of the 57th Annual Meeting of the Associ-"
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "2022, pages 1610–1618.",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": ""
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "ation for Computational Linguistics, pages 527–536."
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Zheng Lian, Bin Liu,\nand Jianhua Tao. 2021.\nCt-",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "Minjie Ren, Xiangdong Huang, Wenhui Li, Dan Song,"
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "net: Conversational\ntransformer network for emo-",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "and Weizhi Nie. 2021. Lr-gcn: Latent relation-aware"
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "IEEE/ACM Transactions on Audio,\ntion recognition.",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "graph convolutional network for conversational emo-"
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Speech, and Language Processing, 29:985–1000.",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "tion recognition.\nIEEE Transactions on Multimedia,"
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "24:4422–4432."
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Y Liu. 2019. Roberta: A robustly optimized bert pre-",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": ""
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "Weizhou Shen, Junqing Chen, Xiaojun Quan, and Zhix-"
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "training approach. arXiv preprint arXiv:1907.11692.",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": ""
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "ian Xie. 2021a. Dialogxl: All-in-one xlnet for multi-"
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "party conversation emotion recognition.\nIn Proceed-"
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Hui Ma,\nJian Wang, Hongfei Lin, Bo Zhang, Yijia",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": ""
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "ings of the AAAI Conference on Artificial Intelligence,"
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Zhang, and Bo Xu. 2023. A transformer-based model",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": ""
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "volume 35, pages 13789–13797."
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "with self-distillation for multimodal emotion recog-",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": ""
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "IEEE Transactions on Multi-\nnition in conversations.",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": ""
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "Weizhou Shen, Siyue Wu, Yunyi Yang, and Xiaojun"
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "media.",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": ""
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "Quan. 2021b. Directed acyclic graph network for"
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "conversational emotion recognition.\nIn Proceedings"
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Navonil Majumder, Soujanya Poria, Devamanyu Haz-",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": ""
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "of\nthe 59th Annual Meeting of\nthe Association for"
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "arika, Rada Mihalcea, Alexander Gelbukh, and Erik",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": ""
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "Computational Linguistics and the 11th International"
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Cambria. 2019. Dialoguernn: An attentive rnn for",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": ""
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "Joint Conference on Natural Language Processing"
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "emotion detection in conversations.\nIn Proceedings",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": ""
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "(Volume 1: Long Papers), pages 1551–1560."
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "of\nthe AAAI Conference on Artificial\nIntelligence,",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": ""
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "volume 33, pages 6818–6825.",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "Dongming Sheng, Dong Wang, Ying Shen, Haitao"
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "Zheng, and Haozhuang Liu. 2020. Summarize before"
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Tao Meng, Yuntao Shou, Wei Ai,\nJiayi Du, Haiyan",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "aggregate: A global-to-local heterogeneous graph in-"
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Liu, and Keqin Li. 2024a. A multi-message passing",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "ference network for conversational emotion recogni-"
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "framework based on heterogeneous graphs in con-",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "tion.\nIn Proceedings of the 28th International Con-"
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "versational emotion recognition. Neurocomputing,",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "ference on Computational Linguistics, pages 4153–"
        },
        {
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "569:127109.",
          "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin": "4163."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "Liu. 2024a. Efficient\nlong-distance latent relation-",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "Yin, and Keqin Li. 2024e. Adversarial alignment"
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "aware graph neural network for multi-modal emo-",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "and graph fusion via information bottleneck for mul-"
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "arXiv preprint\ntion recognition in conversations.",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "Infor-\ntimodal emotion recognition in conversations."
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "arXiv:2407.00119.",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "mation Fusion, 112:102590."
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "Yuntao Shou, Wei Ai,\nTao Meng,\nand Keqin Li.",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "Yuntao Shou, Tao Meng, Fuchen Zhang, Nan Yin,"
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "2023a. Czl-ciae: Clip-driven zero-shot learning for",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "and Keqin Li.\n2024f.\nRevisiting multi-modal"
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "arXiv preprint\ncorrecting inverse age estimation.",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "emotion learning with broad state\nspace models"
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "arXiv:2312.01758.",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "arXiv preprint\nand probability-guidance\nfusion."
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "arXiv:2404.17858."
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "Yuntao Shou, Wei Ai, Tao Meng, and Nan Yin. 2023b.",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": ""
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "Graph information bottleneck for remote sensing seg-",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": ""
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "Yuntao Shou, Peiqiang Yan, Xingjian Yuan, Xiangyong"
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "mentation. arXiv preprint arXiv:2312.02545.",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": ""
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "Cao, Qian Zhao, and Deyu Meng. 2024g. Graph"
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "domain adaptation with dual-branch encoder and two-"
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "YunTao Shou, Wei Ai, Tao Meng, FuChen Zhang, and",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": ""
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "level alignment for whole slide image-based survival"
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "KeQin Li. 2023c. Graphunet: Graph make strong",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": ""
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "prediction. arXiv preprint arXiv:2411.14001."
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "encoders for remote sensing segmentation.\nIn 2023",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": ""
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "IEEE 29th International Conference on Parallel and",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": ""
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "Geng Tu, Tian Xie, Bin Liang, Hongpeng Wang, and"
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "Distributed Systems\n(ICPADS), pages 2734–2737.",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": ""
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "Ruifeng Xu. 2024. Adaptive graph learning for multi-"
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "IEEE.",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": ""
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "modal conversational emotion detection.\nIn Proceed-"
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "ings of the AAAI Conference on Artificial Intelligence,"
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "Yuntao Shou, Xiangyong Cao, Huan Liu, and Deyu",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": ""
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "volume 38, pages 19089–19097."
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "Meng. 2025. Masked contrastive graph representa-",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": ""
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "tion learning for age estimation. Pattern Recognition,",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": ""
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "Louis-Pascal Xhonneux, Meng Qu,\nand Jian Tang."
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "158:110974.",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": ""
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "2020.\nContinuous graph neural networks.\nIn In-"
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "ternational Conference on Machine Learning, pages"
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "Yuntao Shou, Xiangyong Cao, and Deyu Meng. 2024b.",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": ""
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "10432–10441. PMLR."
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "Spegcl: Self-supervised graph spectrum contrastive",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": ""
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "arXiv preprint\nlearning without positive samples.",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": ""
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "Songlong Xing, Sijie Mai,\nand Haifeng Hu. 2020."
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "arXiv:2410.10365.",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": ""
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "Adapted\ndynamic memory\nnetwork\nfor\nemotion"
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "IEEE Transactions on\nrecognition in conversation."
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "Yuntao Shou, Haozhi Lan, and Xiangyong Cao. 2024c.",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": ""
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "Affective Computing, 13(3):1426–1439."
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "Contrastive graph representation learning with ad-",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": ""
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "versarial cross-view reconstruction and information",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": ""
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "Nan Yin, Fuli Feng, Zhigang Luo, Xiang Zhang, Wenjie"
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "bottleneck. arXiv preprint arXiv:2408.00295.",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": ""
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "Wang, Xiao Luo, Chong Chen, and Xian-Sheng Hua."
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "2022a. Dynamic hypergraph convolutional network."
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "Yuntao Shou, Huan Liu, Xiangyong Cao, Deyu Meng,",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": ""
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "In 2022 IEEE 38th International Conference on Data"
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "and Bo Dong. 2024d. A low-rank matching attention",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": ""
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "Engineering (ICDE), pages 1621–1634. IEEE."
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "based cross-modal feature fusion method for conver-",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": ""
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "IEEE Transactions on\nsational emotion recognition.",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": ""
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "Nan Yin, Li Shen, Chong Chen, Xian-Sheng Hua, and"
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "Affective Computing.",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": ""
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "Xiao Luo. Sport: A subgraph perspective on graph"
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "classification with label noise. ACM Transactions on"
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "Yuntao Shou, Tao Meng, Wei Ai, and Keqin Li. 2023d.",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": ""
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "Knowledge Discovery from Data."
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "Adversarial representation with intra-modal and inter-",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": ""
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "modal graph contrastive learning for multimodal emo-",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": ""
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "Nan Yin, Li Shen, Baopu Li, Mengzhu Wang, Xiao"
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "tion recognition. arXiv preprint arXiv:2312.16778.",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": ""
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "Luo, Chong Chen, Zhigang Luo, and Xian-Sheng"
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "Hua. 2022b. Deal: An unsupervised domain adaptive"
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "Yuntao Shou, Tao Meng, Wei Ai, Canhao Xie, Haiyan",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": ""
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "framework for graph-level classification.\nIn Proceed-"
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "Liu, and Yina Wang. 2022a.\nObject detection in",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": ""
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "ings of\nthe 30th ACM International Conference on"
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "medical\nimages based on hierarchical\ntransformer",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": ""
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "Multimedia, pages 3470–3479."
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "and mask mechanism. Computational Intelligence",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": ""
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "and Neuroscience, 2022(1):5863782.",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": ""
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "Nan Yin, Li Shen, Mengzhu Wang, Long Lan, Zeyu"
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "Yuntao Shou, Tao Meng, Wei Ai, Sihan Yang, and Ke-",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "Ma, Chong Chen, Xian-Sheng Hua, and Xiao Luo."
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "qin Li. 2022b. Conversational emotion recognition",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "2023a. Coco: A coupled contrastive framework for"
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "studies based on graph convolutional neural networks",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "unsupervised domain adaptive graph classification."
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "and a dependent syntactic analysis. Neurocomputing,",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "In International Conference on Machine Learning,"
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "501:629–639.",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "pages 40040–40053. PMLR."
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "Yuntao Shou, Tao Meng, Wei Ai, Nan Yin, and Keqin Li.",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "Nan Yin, Li Shen, Mengzhu Wang, Xiao Luo, Zhigang"
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "2023e. A comprehensive survey on multi-modal con-",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "Luo, and Dacheng Tao. 2023b. Omg:\ntowards effec-"
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "versational emotion recognition with deep learning.",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "IEEE\ntive graph classification against\nlabel noise."
        },
        {
          "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, and Haiyan": "arXiv preprint arXiv:2312.05735.",
          "Yuntao Shou, Tao Meng, Wei Ai, Fuchen Zhang, Nan": "Transactions on Knowledge and Data Engineering."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Nan Yin, Li Shen, Huan Xiong, Bin Gu, Chong Chen,": "Xian-Sheng Hua, Siwei Liu, and Xiao Luo. 2023c."
        },
        {
          "Nan Yin, Li Shen, Huan Xiong, Bin Gu, Chong Chen,": "Messages are never propagated alone: Collaborative"
        },
        {
          "Nan Yin, Li Shen, Huan Xiong, Bin Gu, Chong Chen,": "hypergraph neural network for time-series forecast-"
        },
        {
          "Nan Yin, Li Shen, Huan Xiong, Bin Gu, Chong Chen,": "IEEE Transactions on Pattern Analysis and\ning."
        },
        {
          "Nan Yin, Li Shen, Huan Xiong, Bin Gu, Chong Chen,": "Machine Intelligence."
        },
        {
          "Nan Yin, Li Shen, Huan Xiong, Bin Gu, Chong Chen,": "Nan Yin, Mengzhu Wan, Li Shen, Hitesh Laxmichand"
        },
        {
          "Nan Yin, Li Shen, Huan Xiong, Bin Gu, Chong Chen,": "Patel, Baopu Li, Bin Gu, and Huan Xiong. 2024a."
        },
        {
          "Nan Yin, Li Shen, Huan Xiong, Bin Gu, Chong Chen,": "arXiv\nContinuous spiking graph neural networks."
        },
        {
          "Nan Yin, Li Shen, Huan Xiong, Bin Gu, Chong Chen,": "preprint arXiv:2404.01897."
        },
        {
          "Nan Yin, Li Shen, Huan Xiong, Bin Gu, Chong Chen,": "Nan Yin, Mengzhu Wang, Zhenghan Chen, Giulia"
        },
        {
          "Nan Yin, Li Shen, Huan Xiong, Bin Gu, Chong Chen,": "De Masi, Huan Xiong, and Bin Gu. 2024b. Dynamic"
        },
        {
          "Nan Yin, Li Shen, Huan Xiong, Bin Gu, Chong Chen,": "spiking graph neural networks.\nIn Proceedings of"
        },
        {
          "Nan Yin, Li Shen, Huan Xiong, Bin Gu, Chong Chen,": "the AAAI Conference on Artificial Intelligence, vol-"
        },
        {
          "Nan Yin, Li Shen, Huan Xiong, Bin Gu, Chong Chen,": "ume 38, pages 16495–16503."
        },
        {
          "Nan Yin, Li Shen, Huan Xiong, Bin Gu, Chong Chen,": "RunKai Ying, Yuntao Shou, and Chang Liu. 2021. Pre-"
        },
        {
          "Nan Yin, Li Shen, Huan Xiong, Bin Gu, Chong Chen,": "diction model of dow jones index based on lstm-"
        },
        {
          "Nan Yin, Li Shen, Huan Xiong, Bin Gu, Chong Chen,": "adaboost.\nIn 2021 International Conference on Com-"
        },
        {
          "Nan Yin, Li Shen, Huan Xiong, Bin Gu, Chong Chen,": "munications, Information System and Computer En-"
        },
        {
          "Nan Yin, Li Shen, Huan Xiong, Bin Gu, Chong Chen,": "gineering (CISCE), pages 808–812. IEEE."
        },
        {
          "Nan Yin, Li Shen, Huan Xiong, Bin Gu, Chong Chen,": "Duzhen Zhang, Feilong Chen, and Xiuyi Chen. 2023."
        },
        {
          "Nan Yin, Li Shen, Huan Xiong, Bin Gu, Chong Chen,": "Dualgats: Dual graph attention networks for emotion"
        },
        {
          "Nan Yin, Li Shen, Huan Xiong, Bin Gu, Chong Chen,": "recognition in conversations.\nIn Proceedings of the"
        },
        {
          "Nan Yin, Li Shen, Huan Xiong, Bin Gu, Chong Chen,": "61st Annual Meeting of the Association for Compu-"
        },
        {
          "Nan Yin, Li Shen, Huan Xiong, Bin Gu, Chong Chen,": "tational Linguistics (Volume 1: Long Papers), pages"
        },
        {
          "Nan Yin, Li Shen, Huan Xiong, Bin Gu, Chong Chen,": "7395–7408."
        },
        {
          "Nan Yin, Li Shen, Huan Xiong, Bin Gu, Chong Chen,": "Yiping Zhang, Yuntao Shou, Tao Meng, Wei Ai, and Ke-"
        },
        {
          "Nan Yin, Li Shen, Huan Xiong, Bin Gu, Chong Chen,": "qin Li. 2024. A multi-view mask contrastive learning"
        },
        {
          "Nan Yin, Li Shen, Huan Xiong, Bin Gu, Chong Chen,": "graph convolutional neural network for age estima-"
        },
        {
          "Nan Yin, Li Shen, Huan Xiong, Bin Gu, Chong Chen,": "tion.\nKnowledge and Information Systems, pages"
        },
        {
          "Nan Yin, Li Shen, Huan Xiong, Bin Gu, Chong Chen,": "1–26."
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A\nAppendix": "",
          "Therefore, the discrete form of GCN information": "aggregation can be converted into the continuous"
        },
        {
          "A\nAppendix": "Proof of Proposition 1. Eq. 6 can be rewritten as",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "",
          "Therefore, the discrete form of GCN information": "form of ODE as follows:"
        },
        {
          "A\nAppendix": "follows:",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "",
          "Therefore, the discrete form of GCN information": "(cid:16)\n(cid:17)\ndH(t)"
        },
        {
          "A\nAppendix": "",
          "Therefore, the discrete form of GCN information": "1 N\nN(cid:88) n\n=\nln ˆAH(t) + H(t)lnW + E"
        },
        {
          "A\nAppendix": "An",
          "Therefore, the discrete form of GCN information": "dt"
        },
        {
          "A\nAppendix": "N(cid:88) n\nn(cid:88) k\nHn =\nk EWn",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "",
          "Therefore, the discrete form of GCN information": "=1"
        },
        {
          "A\nAppendix": "=1\n=0",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "",
          "Therefore, the discrete form of GCN information": "(21)"
        },
        {
          "A\nAppendix": "We use Riemann integration to convert Eq. 13",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "into a continuous form as follows:",
          "Therefore, the discrete form of GCN information": "H(t) can be further solved by an ODE solver"
        },
        {
          "A\nAppendix": "",
          "Therefore, the discrete form of GCN information": "(e.g., the Runge-Kutta method) to obtain."
        },
        {
          "A\nAppendix": "(cid:90) t+1",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "1 N\nN(cid:88) n\nAsEWsds.\nH(t) =",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "0",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "=1",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "Taking the derivative of H(t) with respect to t,",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "we get the following ODE:",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "dH(t)",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "1 N\nN(cid:88) n\nAt+1EWt+1.\n=",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "dt",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "=1",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "To alleviate the problem of information loss, we",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "take the second-order derivative of H(t) to obtain",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "an ODE expression with better information aggre-",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "gation as follows:",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "d2H(t)",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "1 N\nN(cid:88) n\n(t) =\n(cid:0)ln AAt+1EWt+1",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "dt2",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "=1",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "+At+1EWt+1 ln W(cid:1)",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "(cid:18)",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "dH(t)\ndH(t)",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "1 N\nN(cid:88) n\n=\nln A\n+",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "dt\ndt",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "=1",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "Integrating both sides of Eq. 16 with respect to",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "t, we can obtain:",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "dH(t)",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "(t) = ln AH(t) + H(t) ln W + c.",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "dt",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "The initial value of H(0) is defined as follows:",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "Λii (cid:101)EijΦjj − (cid:101)Eij",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "(cid:0)P−1H(0)Q(cid:1)",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "ij =",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "ln Λii + ln Φjj",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "When t = 0, we can get:",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "dH(t)",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "(cid:12)(cid:12)(cid:12)(cid:12)\n= AEW",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "dt",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "t=0",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "=⇒ AEW − ln AH(0) − H(0) ln W = c",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "Combining Eq. 12 and Eq. 13 we can derive:",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "(cid:0)P−1cQ(cid:1)",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "ij = Λii (cid:101)EijΦjj −",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "Λii (cid:101)EijΦjj − (cid:101)Eij",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "−\nln Φjj",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "ln Λii + ln Φjj",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "c = P (cid:101)EQ−1 = E",
          "Therefore, the discrete form of GCN information": ""
        },
        {
          "A\nAppendix": "",
          "Therefore, the discrete form of GCN information": ""
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "2024a. Mcsff: Multi-modal consistency and specificity fusion framework for entity alignment",
      "authors": [
        "Wei Ai",
        "Wen Deng",
        "Hongyi Chen",
        "Jiayi Du",
        "Tao Meng",
        "Yuntao Shou"
      ],
      "venue": "2024a. Mcsff: Multi-modal consistency and specificity fusion framework for entity alignment",
      "arxiv": "arXiv:2410.14584"
    },
    {
      "citation_id": "2",
      "title": "Seg: Seeds-enhanced iterative refinement graph neural network for entity alignment",
      "authors": [
        "Wei Ai",
        "Yinghui Gao",
        "Jianbin Li",
        "Jiayi Du",
        "Tao Meng",
        "Yuntao Shou",
        "Keqin Li"
      ],
      "year": "2024",
      "venue": "Seg: Seeds-enhanced iterative refinement graph neural network for entity alignment",
      "arxiv": "arXiv:2410.20733"
    },
    {
      "citation_id": "3",
      "title": "2024c. Graph contrastive learning via cluster-refined negative sampling for semi-supervised text classification",
      "authors": [
        "Wei Ai",
        "Jianbin Li",
        "Ze Wang",
        "Jiayi Du",
        "Tao Meng",
        "Yuntao Shou",
        "Keqin Li"
      ],
      "venue": "2024c. Graph contrastive learning via cluster-refined negative sampling for semi-supervised text classification",
      "arxiv": "arXiv:2410.18130"
    },
    {
      "citation_id": "4",
      "title": "Contrastive multi-graph learning with neighbor hierarchical sifting for semi-supervised text classification",
      "authors": [
        "Wei Ai",
        "Jianbin Li",
        "Ze Wang",
        "Yingying Wei",
        "Tao Meng",
        "Yuntao Shou",
        "Keqin Lib"
      ],
      "year": "2024",
      "venue": "Contrastive multi-graph learning with neighbor hierarchical sifting for semi-supervised text classification",
      "arxiv": "arXiv:2411.16787"
    },
    {
      "citation_id": "5",
      "title": "2024e. Der-gcn: Dialog and event relation-aware graph convolutional neural network for multimodal dialog emotion recognition",
      "authors": [
        "Wei Ai",
        "Yuntao Shou",
        "Tao Meng",
        "Keqin Li"
      ],
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "6",
      "title": "2023a. Der-gcn: Dialogue and event relationaware graph convolutional neural network for multimodal dialogue emotion recognition",
      "authors": [
        "Wei Ai",
        "Yuntao Shou",
        "Tao Meng",
        "Nan Yin",
        "Keqin Li"
      ],
      "venue": "2023a. Der-gcn: Dialogue and event relationaware graph convolutional neural network for multimodal dialogue emotion recognition",
      "arxiv": "arXiv:2312.10579"
    },
    {
      "citation_id": "7",
      "title": "Edge-enhanced minimum-margin graph attention network for short text classification",
      "authors": [
        "Wei Ai",
        "Yingying Wei",
        "Hongen Shao",
        "Yuntao Shou",
        "Tao Meng",
        "Keqin Li"
      ],
      "year": "2024",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "8",
      "title": "2023b. A two-stage multimodal emotion recognition model based on graph contrastive learning",
      "authors": [
        "Wei Ai",
        "Fuchen Zhang",
        "Tao Meng",
        "Yuntao Shou",
        "Hon-Gen Shao",
        "Keqin Li"
      ],
      "venue": "2023 IEEE 29th International Conference on Parallel and Distributed Systems (ICPADS)"
    },
    {
      "citation_id": "9",
      "title": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation"
    },
    {
      "citation_id": "10",
      "title": "Multivariate, multi-frequency and multimodal: Rethinking graph neural networks for emotion recognition in conversation",
      "authors": [
        "Feiyu Chen",
        "Jie Shao",
        "Shuyuan Zhu",
        "Heng Tao Shen"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "11",
      "title": "Neural ordinary differential equations",
      "authors": [
        "Yulia Ricky Tq Chen",
        "Jesse Rubanova",
        "David Bettencourt",
        "Duvenaud"
      ],
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "12",
      "title": "Pankaj Wasnik, and Naoyuki Onoe. 2022. M2fnet: Multi-modal fusion network for emotion recognition in conversation",
      "authors": [
        "Purbayan Vishal Chudasama",
        "Ashish Kar",
        "Nirmesh Gudmalwar",
        "Shah"
      ],
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "13",
      "title": "Opensmile: the munich versatile and fast opensource audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin Wöllmer",
        "Björn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "14",
      "title": "Mgat: Multi-granularity attention based transformers for multi-modal emotion recognition",
      "authors": [
        "Weiquan Fan",
        "Xiaofen Xing",
        "Bolun Cai",
        "Xiangmin Xu"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "15",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"
    },
    {
      "citation_id": "16",
      "title": "Mmgcn: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "Jingwen Hu",
        "Yuchen Liu",
        "Jinming Zhao",
        "Qin Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "17",
      "title": "Densely connected convolutional networks",
      "authors": [
        "Gao Huang",
        "Zhuang Liu",
        "Laurens Van Der Maaten",
        "Kilian Weinberger"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "18",
      "title": "Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations",
      "authors": [
        "Taichi Ishiwatari",
        "Yuki Yasuda",
        "Taro Miyazaki",
        "Jun Goto"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP)"
    },
    {
      "citation_id": "19",
      "title": "Higru: Hierarchical gated recurrent units for utterance-level emotion recognition",
      "authors": [
        "Wenxiang Jiao",
        "Haiqin Yang",
        "Irwin King",
        "Michael R Lyu"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "20",
      "title": "Emoberta: Speaker-aware emotion recognition in conversation with roberta",
      "authors": [
        "Taewoon Kim",
        "Piek Vossen"
      ],
      "year": "2021",
      "venue": "Emoberta: Speaker-aware emotion recognition in conversation with roberta",
      "arxiv": "arXiv:2108.12009"
    },
    {
      "citation_id": "21",
      "title": "Semisupervised classification with graph convolutional networks",
      "authors": [
        "N Thomas",
        "Max Kipf",
        "Welling"
      ],
      "year": "2022",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "22",
      "title": "Compm: Context modeling with speaker's pre-trained memory tracking for emotion recognition in conversation",
      "authors": [
        "Joosung Lee",
        "Wooin Lee"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference of the North American Chapter"
    },
    {
      "citation_id": "23",
      "title": "2022a. Contrast and generation make bart a good dialogue emotion recognizer",
      "authors": [
        "Shimin Li",
        "Hang Yan",
        "Xipeng Qiu"
      ],
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "24",
      "title": "2022b. Emocaps: Emotion capsule based model for conversational emotion recognition",
      "authors": [
        "Zaijing Li",
        "Fengxiao Tang",
        "Ming Zhao",
        "Yusen Zhu"
      ],
      "venue": "Findings of the Association for Computational Linguistics: ACL 2022"
    },
    {
      "citation_id": "25",
      "title": "Ctnet: Conversational transformer network for emotion recognition",
      "authors": [
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "26",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Liu"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "27",
      "title": "A transformer-based model with self-distillation for multimodal emotion recognition in conversations",
      "authors": [
        "Hui Ma",
        "Jian Wang",
        "Hongfei Lin",
        "Bo Zhang",
        "Yijia Zhang",
        "Bo Xu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "28",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "29",
      "title": "2024a. A multi-message passing framework based on heterogeneous graphs in conversational emotion recognition",
      "authors": [
        "Tao Meng",
        "Yuntao Shou",
        "Wei Ai",
        "Jiayi Du",
        "Haiyan Liu",
        "Keqin Li"
      ],
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "30",
      "title": "2024b. Deep imbalanced learning for multimodal emotion recognition in conversations",
      "authors": [
        "Tao Meng",
        "Yuntao Shou",
        "Wei Ai",
        "Nan Yin",
        "Keqin Li"
      ],
      "venue": "IEEE Transactions on Artificial Intelligence"
    },
    {
      "citation_id": "31",
      "title": "Revisiting multimodal emotion recognition in conversation from the perspective of graph spectrum",
      "authors": [
        "Tao Meng",
        "Fuchen Zhang",
        "Yuntao Shou",
        "Wei Ai",
        "Nan Yin",
        "Keqin Li"
      ],
      "year": "2024",
      "venue": "Revisiting multimodal emotion recognition in conversation from the perspective of graph spectrum",
      "arxiv": "arXiv:2404.17862"
    },
    {
      "citation_id": "32",
      "title": "Masked graph learning with recurrent alignment for multimodal emotion recognition in conversation",
      "authors": [
        "Tao Meng",
        "Fuchen Zhang",
        "Yuntao Shou",
        "Hongen Shao",
        "Wei Ai",
        "Keqin Li"
      ],
      "year": "2024",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "33",
      "title": "Recent advances in natural language processing via large pre-trained language models: A survey",
      "authors": [
        "Bonan Min",
        "Hayley Ross",
        "Elior Sulem",
        "Amir Pouran",
        "Ben Veyseh",
        "Thien Huu Nguyen",
        "Oscar Sainz",
        "Eneko Agirre",
        "Ilana Heintz",
        "Dan Roth"
      ],
      "year": "2023",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "34",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "35",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Gautam Naik",
        "Erik Cambria",
        "Rada Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "36",
      "title": "Lr-gcn: Latent relation-aware graph convolutional network for conversational emotion recognition",
      "authors": [
        "Minjie Ren",
        "Xiangdong Huang",
        "Wenhui Li",
        "Dan Song",
        "Weizhi Nie"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "37",
      "title": "Dialogxl: All-in-one xlnet for multiparty conversation emotion recognition",
      "authors": [
        "Weizhou Shen",
        "Junqing Chen",
        "Xiaojun Quan",
        "Zhixian Xie"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "38",
      "title": "2021b. Directed acyclic graph network for conversational emotion recognition",
      "authors": [
        "Weizhou Shen",
        "Siyue Wu",
        "Yunyi Yang",
        "Xiaojun Quan"
      ],
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "39",
      "title": "Summarize before aggregate: A global-to-local heterogeneous graph inference network for conversational emotion recognition",
      "authors": [
        "Dongming Sheng",
        "Dong Wang",
        "Ying Shen",
        "Haitao Zheng",
        "Haozhuang Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "40",
      "title": "2024a. Efficient long-distance latent relationaware graph neural network for multi-modal emotion recognition in conversations",
      "authors": [
        "Yuntao Shou",
        "Wei Ai",
        "Jiayi Du",
        "Tao Meng",
        "Haiyan Liu"
      ],
      "venue": "2024a. Efficient long-distance latent relationaware graph neural network for multi-modal emotion recognition in conversations",
      "arxiv": "arXiv:2407.00119"
    },
    {
      "citation_id": "41",
      "title": "2023a. Czl-ciae: Clip-driven zero-shot learning for correcting inverse age estimation",
      "authors": [
        "Yuntao Shou",
        "Wei Ai",
        "Tao Meng",
        "Keqin Li"
      ],
      "venue": "2023a. Czl-ciae: Clip-driven zero-shot learning for correcting inverse age estimation",
      "arxiv": "arXiv:2312.01758"
    },
    {
      "citation_id": "42",
      "title": "2023b. Graph information bottleneck for remote sensing segmentation",
      "authors": [
        "Yuntao Shou",
        "Wei Ai",
        "Tao Meng",
        "Nan Yin"
      ],
      "venue": "2023b. Graph information bottleneck for remote sensing segmentation",
      "arxiv": "arXiv:2312.02545"
    },
    {
      "citation_id": "43",
      "title": "Graphunet: Graph make strong encoders for remote sensing segmentation",
      "authors": [
        "Yuntao Shou",
        "Wei Ai",
        "Tao Meng",
        "Fuchen Zhang",
        "Keqin Li"
      ],
      "year": "2023",
      "venue": "2023 IEEE 29th International Conference on Parallel and Distributed Systems (ICPADS)"
    },
    {
      "citation_id": "44",
      "title": "Masked contrastive graph representation learning for age estimation",
      "authors": [
        "Yuntao Shou",
        "Xiangyong Cao",
        "Huan Liu",
        "Deyu Meng"
      ],
      "year": "2025",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "45",
      "title": "Xiangyong Cao, and Deyu Meng. 2024b. Spegcl: Self-supervised graph spectrum contrastive learning without positive samples",
      "authors": [
        "Yuntao Shou"
      ],
      "venue": "Xiangyong Cao, and Deyu Meng. 2024b. Spegcl: Self-supervised graph spectrum contrastive learning without positive samples",
      "arxiv": "arXiv:2410.10365"
    },
    {
      "citation_id": "46",
      "title": "2024c. Contrastive graph representation learning with adversarial cross-view reconstruction and information bottleneck",
      "authors": [
        "Yuntao Shou",
        "Haozhi Lan",
        "Xiangyong Cao"
      ],
      "venue": "2024c. Contrastive graph representation learning with adversarial cross-view reconstruction and information bottleneck",
      "arxiv": "arXiv:2408.00295"
    },
    {
      "citation_id": "47",
      "title": "A low-rank matching attention based cross-modal feature fusion method for conversational emotion recognition",
      "authors": [
        "Yuntao Shou",
        "Huan Liu",
        "Xiangyong Cao",
        "Deyu Meng",
        "Bo Dong"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "48",
      "title": "Adversarial representation with intra-modal and intermodal graph contrastive learning for multimodal emotion recognition",
      "authors": [
        "Yuntao Shou",
        "Tao Meng",
        "Wei Ai",
        "Keqin Li"
      ],
      "year": "2023",
      "venue": "Adversarial representation with intra-modal and intermodal graph contrastive learning for multimodal emotion recognition",
      "arxiv": "arXiv:2312.16778"
    },
    {
      "citation_id": "49",
      "title": "2022a. Object detection in medical images based on hierarchical transformer and mask mechanism",
      "authors": [
        "Yuntao Shou",
        "Tao Meng",
        "Wei Ai",
        "Canhao Xie",
        "Haiyan Liu",
        "Yina Wang"
      ],
      "venue": "Computational Intelligence and Neuroscience"
    },
    {
      "citation_id": "50",
      "title": "Conversational emotion recognition studies based on graph convolutional neural networks and a dependent syntactic analysis",
      "authors": [
        "Yuntao Shou",
        "Tao Meng",
        "Wei Ai",
        "Sihan Yang",
        "Keqin Li"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "51",
      "title": "A comprehensive survey on multi-modal conversational emotion recognition with deep learning",
      "authors": [
        "Yuntao Shou",
        "Tao Meng",
        "Wei Ai",
        "Nan Yin",
        "Keqin Li"
      ],
      "year": "2023",
      "venue": "A comprehensive survey on multi-modal conversational emotion recognition with deep learning",
      "arxiv": "arXiv:2312.05735"
    },
    {
      "citation_id": "52",
      "title": "2024e. Adversarial alignment and graph fusion via information bottleneck for multimodal emotion recognition in conversations",
      "authors": [
        "Yuntao Shou",
        "Tao Meng",
        "Wei Ai",
        "Fuchen Zhang",
        "Nan Yin",
        "Keqin Li"
      ],
      "venue": "Information Fusion"
    },
    {
      "citation_id": "53",
      "title": "Revisiting multi-modal emotion learning with broad state space models and probability-guidance fusion",
      "authors": [
        "Yuntao Shou",
        "Tao Meng",
        "Fuchen Zhang",
        "Nan Yin",
        "Keqin Li"
      ],
      "year": "2024",
      "venue": "Revisiting multi-modal emotion learning with broad state space models and probability-guidance fusion",
      "arxiv": "arXiv:2404.17858"
    },
    {
      "citation_id": "54",
      "title": "Xiangyong Cao, Qian Zhao, and Deyu Meng. 2024g. Graph domain adaptation with dual-branch encoder and twolevel alignment for whole slide image-based survival prediction",
      "authors": [
        "Yuntao Shou",
        "Peiqiang Yan",
        "Xingjian Yuan"
      ],
      "venue": "Xiangyong Cao, Qian Zhao, and Deyu Meng. 2024g. Graph domain adaptation with dual-branch encoder and twolevel alignment for whole slide image-based survival prediction",
      "arxiv": "arXiv:2411.14001"
    },
    {
      "citation_id": "55",
      "title": "Adaptive graph learning for multimodal conversational emotion detection",
      "authors": [
        "Geng Tu",
        "Tian Xie",
        "Bin Liang",
        "Hongpeng Wang",
        "Ruifeng Xu"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "56",
      "title": "Continuous graph neural networks",
      "authors": [
        "Louis-Pascal Xhonneux",
        "Meng Qu",
        "Jian Tang"
      ],
      "year": "2020",
      "venue": "In International Conference on Machine Learning"
    },
    {
      "citation_id": "57",
      "title": "Adapted dynamic memory network for emotion recognition in conversation",
      "authors": [
        "Songlong Xing",
        "Sijie Mai",
        "Haifeng Hu"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "58",
      "title": "Dynamic hypergraph convolutional network",
      "authors": [
        "Nan Yin",
        "Fuli Feng",
        "Zhigang Luo",
        "Xiang Zhang",
        "Wenjie Wang",
        "Xiao Luo",
        "Chong Chen",
        "Xian-Sheng Hua"
      ],
      "year": "2022",
      "venue": "2022 IEEE 38th International Conference on Data Engineering (ICDE)"
    },
    {
      "citation_id": "59",
      "title": "Sport: A subgraph perspective on graph classification with label noise",
      "authors": [
        "Nan Yin",
        "Li Shen",
        "Chong Chen",
        "Xian-Sheng Hua",
        "Xiao Luo"
      ],
      "venue": "ACM Transactions on Knowledge Discovery from Data"
    },
    {
      "citation_id": "60",
      "title": "Deal: An unsupervised domain adaptive framework for graph-level classification",
      "authors": [
        "Nan Yin",
        "Li Shen",
        "Baopu Li",
        "Mengzhu Wang",
        "Xiao Luo",
        "Chong Chen",
        "Zhigang Luo",
        "Xian-Sheng Hua"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "61",
      "title": "Xian-Sheng Hua, and Xiao Luo. 2023a. Coco: A coupled contrastive framework for unsupervised domain adaptive graph classification",
      "authors": [
        "Nan Yin",
        "Li Shen",
        "Mengzhu Wang",
        "Long Lan",
        "Zeyu Ma",
        "Chong Chen"
      ],
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "62",
      "title": "Zhigang Luo, and Dacheng Tao. 2023b. Omg: towards effective graph classification against label noise",
      "authors": [
        "Nan Yin",
        "Li Shen",
        "Mengzhu Wang",
        "Xiao Luo"
      ],
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "63",
      "title": "2023c. Messages are never propagated alone: Collaborative hypergraph neural network for time-series forecasting",
      "authors": [
        "Nan Yin",
        "Li Shen",
        "Huan Xiong",
        "Bin Gu",
        "Chong Chen",
        "Xian-Sheng Hua",
        "Siwei Liu",
        "Xiao Luo"
      ],
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "64",
      "title": "Bin Gu, and Huan Xiong. 2024a. Continuous spiking graph neural networks",
      "authors": [
        "Nan Yin",
        "Mengzhu Wan",
        "Li Shen",
        "Laxmichand Hitesh",
        "Baopu Patel",
        "Li"
      ],
      "venue": "Bin Gu, and Huan Xiong. 2024a. Continuous spiking graph neural networks",
      "arxiv": "arXiv:2404.01897"
    },
    {
      "citation_id": "65",
      "title": "Huan Xiong, and Bin Gu. 2024b. Dynamic spiking graph neural networks",
      "authors": [
        "Nan Yin",
        "Mengzhu Wang",
        "Zhenghan Chen",
        "Giulia De Masi"
      ],
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "66",
      "title": "Prediction model of dow jones index based on lstmadaboost",
      "authors": [
        "Runkai Ying",
        "Yuntao Shou",
        "Chang Liu"
      ],
      "year": "2021",
      "venue": "2021 International Conference on Communications, Information System and Computer Engineering (CISCE)"
    },
    {
      "citation_id": "67",
      "title": "Dualgats: Dual graph attention networks for emotion recognition in conversations",
      "authors": [
        "Duzhen Zhang",
        "Feilong Chen",
        "Xiuyi Chen"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "68",
      "title": "A multi-view mask contrastive learning graph convolutional neural network for age estimation",
      "authors": [
        "Yiping Zhang",
        "Yuntao Shou",
        "Tao Meng",
        "Wei Ai",
        "Keqin Li"
      ],
      "year": "2024",
      "venue": "Knowledge and Information Systems"
    }
  ]
}