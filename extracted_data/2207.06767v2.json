{
  "paper_id": "2207.06767v2",
  "title": "Semi-Supervised Cross-Lingual Speech Emotion Recognition",
  "published": "2022-07-14T09:24:55Z",
  "authors": [
    "Mirko Agarla",
    "Simone Bianco",
    "Luigi Celona",
    "Paolo Napoletano",
    "Alexey Petrovsky",
    "Flavio Piccoli",
    "Raimondo Schettini",
    "Ivan Shanin"
  ],
  "keywords": [
    "semi-supervised domain adaptation",
    "speech emotion recognition",
    "cross-lingual",
    "semi-supervised learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Performance in Speech Emotion Recognition (SER) on a single language has increased greatly in the last few years thanks to the use of deep learning techniques. However, cross-lingual SER remains a challenge in real-world applications due to two main factors: the first is the big gap among the source and the target domain distributions; the second factor is the major availability of unlabeled utterances in contrast to the labeled ones for the new language. Taking into account previous aspects, we propose a Semi-Supervised Learning (SSL) method for cross-lingual emotion recognition when only few labeled examples in the target domain (i.e. the new language) are available. Our method is based on a Transformer and it adapts to the new domain by exploiting a pseudolabeling strategy on the unlabeled utterances. In particular, the use of a hard and soft pseudo-labels approach is investigated. We thoroughly evaluate the performance of the proposed method in a speaker-independent setup on both the source and the new language and show its robustness across five languages belonging to different linguistic strains. The experimental findings indicate that the unweighted accuracy is increased by an average of 40% compared to stateof-the-art methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "SER is a fundamental aspect of computational paralinguistics as it concerns the analysis of the non-verbal elements of speech  (Schuller & Batliner, 2013) . SER, which aims to infer the emotional state of a speaker  (El Ayadi et al., 2011) , could support a wide range of domains, including human-computer interaction  (Schuller & Batliner, 2013) , healthcare  (Tumanova et al., 2020) , and public safety  (Lefter & Jonker, 2017) . For instance, SER systems could be employed in interactive dialogue systems to make them empathetic  (Bertero et al., 2016) , in healthcare systems for the diagnosis of disorders and diseases  (Hansen et al., 2022) , and in commercial applications for detecting customer satisfaction in call-centers and by employment agencies to find suitable candidates  (Perez-Toro et al., 2021) . Existing SER models have achieved satisfactory results for valence/arousal estimation  (Xiao et al., 2016)  and emotion classification  (Scheidwasser-Clow et al., 2022)  when the training and test data are from the same corpus. However, the performance of these models degrades when applied to new corpora of same/different languages due to domain shift  (Feraru et al., 2015) . This problem occurs mainly in real-world scenarios where the people using a given SER system may differ or speak languages other than those used to train the system.\n\nOver the years, several methodologies have been developed to speed up the adaptation of a pre-trained system to new people or a new language by leveraging semi-supervised/incremental learning  (Zhang et al., 2016)  and transfer learning  (Feraru et al., 2015) . Numerous approaches have been proposed to reduce the domain shift problem for cross-corpus or cross-lingual SER, namely eliminate or reduce the difference between the source and target data distribution  (Cai et al., 2021; Tamulevičius et al., 2020) . Most of these approaches are based on deep learning techniques as they generally prove to be more effective than traditional machine learning techniques also for SER  (Tamulevičius et al., 2020) . Supervised Domain Adaptation (SDA) methods for SER exploit labeled utterances of the target corpus to adapt the recognition model to work properly on the new set of data  (Neumann et al., 2018; Zhou & Chen, 2019; Tamulevičius et al., 2020) . However, these methods require the new language utterances to be labeled, which may not be possible as their collection is expensive. Therefore, a more practical solution is Unsupervised Domain Adaptation (UDA) which only demands unlabeled utterances from the new language. Many UDA methods try to reduce the distribution shift between the source and target languages  (Latif et al., 2019; Cai et al., 2021; Li et al., 2021; Ocquaye et al., 2021) .\n\nIn this paper we formulate the cross-lingual SER as a SSL problem. This scenario assumes that for the new language there are few labeled and many unlabeled utterances. We first train a deep learning based SER model on the source language dataset in which all utterances are annotated with the emotion label (see Fig.  1a ). The SER model is then adapted to a new language for which the emotion of most training utterances is unknown. The labeled data of the first language are available (see Fig.  1b ). Pseudo-labeling is adopted to generate labels for the unlabeled utterances and guide the learning process.\n\nUnlike most cross-lingual SER methods which focus on the binary classification of valence, our approach deals with the prediction of five emotion categories. In our experiments we consider English as the source language since it is the most widespread language in the world  (Berlitz, 2021) .\n\nThe proposed method for cross-lingual SER based on pseudo-labeling is suitable for use in all-day consumer technologies, such as smartphones, smartmirrors, and smartwatches. These devices collect massive amounts of unlabeled data, making traditional supervised learning methods difficult to implement. The proposed method overcomes this challenge by requiring only small amounts of labeled data and large amounts of unlabeled data, lowering manual annotation costs and shortening data preparation time. As a result, the method can be used in consumer technologies at a much lower cost than traditional supervised learning-based methods, making it a more practical and accessible solution. By implementing this method, consumer technologies can accurately recognize and respond to emotions expressed in different languages, improving communication and user experience. For example, a smartmirror with cross-lingual SER could provide personalized recommendations based on a user's emotional state, or could adjust lighting and temperature to create a more comfortable environment based on the user's emotions.\n\nApart from a method for cross-lingual SER, this work provides an analysis of different models as utterance encoder. In particular, it is demonstrated that a Transformer-based utterance encoder trained to build meaningful representations of speech boosts the performance compared to state of the art methods. Furthermore, in the adaptation procedure it is verified that balancing pseudolabeled vs. labeled utterances helps to improve the generalization capabilities of the learned model.\n\nTo summarize, the main contributions of this paper are:\n\n• A cross-lingual SER framework spanning five languages.\n\n• A SSL based cross-lingual SER method for emotion categorization.\n\n• The experimentation of several utterance encoders, i.e. a CNN for speech emotion classification, a CNN and a Transformer trained for speech representation learning.\n\n• Two different approaches for generating pseudo-labels are investigated.\n\n• An utterance rebalancing strategy to reduce the cardinality gap between the labeled utterances available for the source language and the labeled or pseudo-labeled utterances for the new language.\n\n• A thorough analysis of how the variation in the number of labeled utterances for the new language impacts performance.\n\nThe rest of the paper is organized as follows. Section 2 introduces some previous works on cross-lingual emotion recognition. In Section 3, SSL based cross-lingual SER is formalized and then the proposed method is described. Experimental setup and result analysis are presented in Sections 4 and 5, respectively. Finally, we conclude in Section 6.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Cross-lingual SER methods involve the use of two languages, the source language for which the emotion information is available for all the samples and the target language, for which only few labeled samples are available. The aim of cross-lingual SER is then to learn from the source language and extend the learned knowledge on the unlabeled samples of the target language. This propagation is an active process that involves the use of the few samples available and, if present, of auxiliary information available for both languages. At each learning episode, unlabeled samples of the target language which are believed to belong to an emotional class, are labeled. This process is called pseudo-labeling and in turn will support future learning episodes for evaluating the remaining samples. The choice of the acoustic features is quite important and must be kept into account  (Tamulevičius et al., 2020) .\n\nThe primary taxonomy of cross-lingual SER methods is given by the strategy used to convey knowledge on the new language. Two are the main schemes used in the state of the art. The first one is the use of auxiliary information that is available on both languages to learn a shared feature space. In this context, the emotion recognition task and the side task are performed simultaneously through a joint-training. Cai et al.  (Cai et al., 2021)  proposed a neural network with two branches, leading respectively to emotion and language classification. During training, the first branch is trained only with the source language corpus with emotion labels while the second by both languages. This training schema allows to exploit all existing information of the first and the second language to create a shared feature space. Gradient Reversal Layer (GRL) is adopted to force the features to be meaningful for the primary task (emotion classification) and at the same time to be indistinguishable for the auxiliary task (language classification). Performance is measured in terms of valence and arousal on Urdu, Estonian, IEMOCAP, Persian, and German (4 emotions). Li et al.  (Li et al., 2021)  proposed an SSDA memory-based system called Neural Network with Pseudo Multilabel (NNPM). In first place, they use a siamese network with self attention for projecting source and target utterances in a learned feature space. Then, the source-domain features are dynamically stored in the dynamic external memory. Emotion similarity is gained through cosine distance between features in the memory and of the target utterance. Pseudo-labels are given to the target domain utterances based on the similarity score. Hard negative sample mining strategy is used to improve the learning whereas the features result less representative. Performance is measured with weighted and unweighted accuracy. Ocquaye et al.  (Ocquaye et al., 2021)  exploited joint training to perform SSDA. A neural network with one common branch and a set of task-specific branches is proposed. Two branches perform emotion recognition respectively on the source samples and the pseudo-labeled target samples. The evaluation has been conducted on SAVEE, IEMOCAP, EMO-DB, FAU-AIBO (German), and EMOVO for valence classification.\n\nThe second strategy widely used in the context of cross-lingual SER is the use of adversarial training. This technology showed great ability in domain transfer and thus is very effective in this scenario.  Latif et al. (Latif et al., 2019)  proposed a method for learning a language-independent emotion recognition feature vector in the context of UDA. This system is based on Generative Adversarial Networks (GANs) as this technology has shown great potential in learning the underlying data distribution. Specifically, the proposed method has two generators to project respectively the source and the target utterance in a common feature space that is later evaluated through a critic. The feature space is then constrained to carry emotion information through a classification loss. In other words, the adversarial loss makes the feature space homogeneous both for the source and the target languages while the classification loss makes the features meaningful for the task of emotion recognition. Performance assessed on EMOVO, SAVEE, Urdu, and EMO-DB for valence classification.\n\nDomain Adversarial Neural Network (DANN) is a method that generates domain invariant feature representations. This allows to reduce the gap among source and target domain features  (Abdelwahab & Busso, 2018) .\n\nHowever, the effectiveness of domain adversarial training strongly depends on the distribution of the two databases: in fact, adversarial attacks and instabilities may occur in the training phase if the data points are significantly different from each other. Aggregate multi-task Learning (AL) is another technique that has been used to improve the generalization of the trained model by incorporating information of gender and naturalness  (Kim et al., 2017) .\n\nExtending the work of Sung et al.  (Sung et al., 2018) , Ahn et al.  (Ahn et al., 2021)  presented Few-shot Learning and Unsupervised Domain Adaptation (FLUDA), which aims to train an embedding and a metric module that respectively project the utterances in a meaningful shared feature space and learn the differences among classes. The embedding and metric module are optimized to predict class similarity for each episode by exploiting few samples composing the support set and pseudo-labels assigned in the previous episode. During training, an auxiliary module is used to determine whether the labeled sample is real or pseudo-labeled. The proposed method estimates four categorical emotions (neutral, happy, sad and angry) and uses IEMOCAP and CREMA-D as source corpora while MSP-IMPROV, EMO-DB or KME were used as target corpus. However, the samples in few-shot learning significantly depend on the choice of the support set, that can make its application challenging to a practical setup. Furthermore, the strong assumption that the support set is uniformly sampled from a single distribution, leads to the selection of an unstable number of samples for each class during training. With the aim of solving the previous challenges, Zhou et al.  (Zhou & Chen, 2019 ) used adversarial network to perform SSDA. Specifically, a GAN is modified such that the generator projects the utterance in a feature space carrying emotion information and the critic has to determine which emotion class belongs the input feature and the used language. This stage is trained with the source language for which the labels are known. In second place the critic is frozen and the encoder is adapted to the new language. This second step forces the encoder to adapt and generate compatible features with the ones obtained in the first step. The method has been benchmarked on EMO-DB and Aibo in terms of positive and negative emotions.\n\nRecently, Das et al.  (Das et al., 2022)  presented a Variational AutoEncoder (VAE) for learning a latent space able to discriminate emotions and to generalize on different languages simultaneously. They achieved this goal by (i) exploiting a Kullback-Leibler (KL) loss annealing using cyclic scheduling to improve emotion discrimination, (ii) employing semi-supervised training of the VAE by incorporating a clustering loss in the learning function. Experimental results have been collected for IEMOCAP, SAVEE, EMO-DB, CaFE, and AESD in terms of four emotions.  Kshirsagar et al. (Kshirsagar & Falk, 2022)  explored the combined use of Bag-of-Words (BoW) methodology, domain adaptation and data augmentation as strategies to counter the damaging effects of cross-lingual SER. The authors also proposed a new method called N-CORAL in which all languages are mapped to a common distribution. Experiments with the German, Hungarian, Chinese, and French languages show the advantages of the proposed N-CORAL method, combined with data augmentation and BoW for valence-arousal estimation.\n\nThe related works can be thus summarized as follows:\n\n• Several studies in the literature demonstrate that it is possible to perform cross-language SER without labeled utterances for the new language using auxiliary information, generative methods, or adversarial and few-shot learning.\n\n• Domain adaptation approaches based on adversarial neural networks are widely used for cross-lingual SER; however, there is still room for performance improvement.\n\n• The use of the newer Transformer architectures for utterance encoding has not been explored and used for cross-lingual SER.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Method",
      "text": "The problem of cross-lingual SER is first formulated in Section 3.1 and then the proposed SSL method for cross-lingual SER is described in Section 3.2.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Problem Formulation",
      "text": "We represent sets with special Latin characters (e.g., S). Lower or uppercase normal fonts, e.g., K denote scalars. Matrices are in uppercase bold letters (e.g., M), while lowercase bold letters represent vectors as in v. We use lowercase Latin letters to represent indices (e.g., i).\n\nWe formulate our cross-lingual SER as the following domain adaptation task. We have a source language corpus with N s labeled utterances as source domain,\n\n, and a new language corpus, D t , as target domain. The new language corpus,\n\nThe number of utterances of the source language N s is much higher than the number of labeled utterances for the new language N k , i.e. N s ≫ N k . Utterances X s i and X t i are elements of R F ×T , where F and T are the number of frequency bins and the number of time frames, respectively. The utterance labels y s i and y t i are scalar values such that y ∈ Z : 1 ≤ y ≤ C where C is the number of emotion categories within the corpora. We consider that the source and target corpora contain the same number C of emotion categories.\n\nOur goal is to learn a reliable emotion classifier on D s , U t , and K t , which preserves performance on source language D s and generalizes well on D t .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ssl For Cross-Lingual Ser",
      "text": "The proposed semi-supervised cross-lingual SER is a deep learning model f parameterized with θ that maps an input utterance X into a basic emotion y, y = f (X, θ).\n\nAs depicted in Figure  2 , our method consists of two modules, namely the SER recognition backbone and the adaptation module. The SER recognition backbone (f θ ) classifies emotions for both the source and new language utterances. During the training phase, we introduce the adaptation module to improve the discriminative power and generalization ability of f on the new language. The adaptation module relies on a pseudo-labeling strategy to allow model training on the unlabeled utterances of the new language. Furthermore, we include an utterance rebalancing mechanism to avoid that the model is biased on the source language due to the higher cardinality of utterances compared to those of the new language.\n\nIn the next sections we detail the previously introduced modules.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ser Recognition Backbone",
      "text": "It is the core of our method that deals with utterance classification. It consists of two modules, i.e., the utterance encoder e and the classification head h. The utterance encoder e is a deep architecture like a CNN or a Transformer  that takes a raw waveform or an audio representation as input, X, and returns a d-dimensional feature vector x ∈ R D . The choice of the utterance encoder is important for the performance of the proposed method. Therefore, three different deep architectures are considered, namely EmotionCNN  (Tamulevičius et al., 2020) , Bootstrapping Your Own Latent for Speech (BYOL-S) (Scheidwasser-Clow et al., 2022), and Hidden-unit BERT (HuBERT)  (Hsu et al., 2021) . Emo-tionCNN is a CNN architecture for cross-lingual speech emotion recognition. It is composed of three convolutional layers. Each convolutional layer is followed by a ReLU, a batch normalization layer, and 3 × 3 max pooling, respectively. The model is fed with a cochleagram-based representation of the raw waveform and outputs a 128-dimensional feature vector. BYOL-S is a CNN model for audio representation inspired by the Bootstrapping Your Own Latent (BYOL) model initially proposed for self-supervised image classification  (Grill et al., 2020) . It is trained on the speech utterances of the AudioSet  (Gemmeke et al., 2017)  dataset. BYOL-S is currently the state of the art in SER  (Scheidwasser-Clow et al., 2022) . The model accepts input utterances of variable length and returns a single 1024-dimensional feature vector per input utterance. All utterances are converted to a log-scaled Mel spectrogram with a window size of 64ms, hop size of 10ms, and mel-spaced frequency bins F = 64 in the range 60-7,800 Hz. Each spectrogram is normalized by subtracting the mean and dividing by the estimated standard deviation for the frames of the spectrogram.\n\nHuBERT is a Transformer-based approach for self-supervised speech representation learning. It consists of a convolutional waveform encoder, a BERT-Base encoder  (Devlin et al., 2019) , a projection layer, and a code embedding layer. It is trained on the Librispeech 960h dataset  (Panayotov et al., 2015)  to classify randomly masked frames to pseudo-labels. The labels are generated by running K-Means clustering with 100 clusters on 39-dimensional MFCC features. The model accepts raw waveforms of variable length as input and returns a single 768-dimensional feature vector per input utterance. The main characteristics of the three architectures are summarized in Table  1 .\n\nThe feature vector obtained from one of the previously described utterance encoders is processed by the classification head h to predict p ∈ R C , i.e., the probability distribution over the C emotion categories. The classification head consists of a linear layer followed by a softmax:\n\nwhere θ h is the set of weights W ∈ R D×C and bias b ∈ R C .",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Adaptation Module",
      "text": "This module aims to exploit the unlabeled utterances of the new language in the model training. To this purpose, an SSL approach is used to generate pseudo-labels ỹ for the unlabeled utterances of the new language dataset, D t . This operation is repeated at each step to take advantage of the knowledge learned in previous steps and results in the creation of a hybrid dataset D composed by real samples (X i , y i ) and samples with generated ground truth (X i , ỹi ).\n\nA key decision is how to generate the pseudo-labels ỹ for the N u unlabeled utterances. In this paper, we experiment with the use of hard pseudo-labels and soft pseudo-labels.\n\nHard pseudo-labels. In this approach hard pseudo-labels are directly obtained from network predictions. Let p i be the probability outputs of our trained h θ model for the utterance. Using the probability vector, the pseudo-label for the utterance X i corresponds to ỹi = arg max(p i ). We select the subset of pseudolabels which are less noisy to limit the confirmation bias, i.e. the overfitting of incorrect pseudo-labels predicted by the model. In particular, we select only the pseudo-labels corresponding to high-confidence predictions. Let g = {g b } B b=1 be a binary vector representing the selected pseudo-labels in a mini-batch B. This vector is obtained as follows:\n\nwhere τ ∈ (0, 1) is a confidence threshold.\n\nSoft pseudo-labels. We investigate the use of soft pseudo-label inspired by  (Arazo et al., 2020)  since it has demonstrated in some cases to perform better than hard pseudo-labels  (Tanaka et al., 2018) .\n\nLet p i be the probability outputs of our trained h θ model for the utterance X i . Two regularization terms are used to improve convergence. The first regularization term discourages the model to assign all samples to a single class by adding:\n\nwhere p c is the prior probability distribution for class c assumed as a uniform distribution p c = 1/C and hc denotes the mean softmax probability of the model for class c across batch utterances.\n\nThe second regularization is the average per-sample entropy (R H stands for entropy regularization) that forces the probability distribution to peak on a single class:\n\nwhere h c θ (X i ) denotes the c class value of the softmax output h θ (X i ) and it is estimated on a mini-batch B. The total loss is the following:\n\nwhere λ A and λ H control the contribution of each regularization term.\n\nTo limit the confirmation bias problem, we exploit the mixup data augmentation technique proposed in  (Zhang et al., 2018) . It combines data augmentation with label smoothing to reduce the confidence of the model on its predictions. Mixup trains on convex combinations of sample pairs (X p and X q ) and corresponding labels (y p and y q ):\n\nwhere α is randomly sampled in the range (0, 1).",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Utterance Rebalancing",
      "text": "As stated in Section 3.1, the number of labeled utterances for the new language N k is much lower than the number of labeled utterances for the source language N s . The use of pseudo-labeling to adapt the model to the new language can only partially reduce the imbalance between source and new language corpus. The imbalanced ratio γ l between the number of source utterances, N s , and the number of labeled utterances for the new language, N k , is defined as N k /N s and a γ l far from 1 indicates more severe utterance imbalance.\n\nTo tackle the utterance imbalance and get γ l = 1 we exploit random oversampling. Specifically, the utterances of the new language are randomly replicated to match the number of utterances of the source language.\n\nThe rebalancing algorithm is run at the end of each pseudo-labeling iteration during the adaptation procedure. Furthermore, it is performed at the beginning of the training procedure if the number of labeled utterances for the new language is non-zero.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Training Procedure",
      "text": "The proposed model f θ (X) is trained end-to-end for E epochs using a set of N training utterances belonging to the joint domain D = {D s ∪ K t }. D contains a set of N s +N k labeled utterances {D s , K t } coming from both the source and the target language. Every W epochs, the model adaptation procedure is performed, in which supervised learning is accompanied by pseudo-labeling on the set U t of unlabeled utterances for the new language. The set D is then expanded with the generated pseudo-labels K ′ t }. The complete training procedure is presented in Algorithm 1.\n\nThe parameters θ of the model are optimized using categorical cross-entropy:\n\nwhere f θ (x) are the softmax probabilities predicted by the model, log(•) is applied element-wise and y i can be a real or pseudo label, and B is the number of batch utterances.\n\nTo mitigate the risk of overfitting, early stopping is implemented by selecting the model weights from the epoch that achieves the best performance on the validation set for both the source and new languages.\n\nAlgorithm 1 Our training procedure.\n\n1: Input: Total training epochs E, interval of epochs for pseudo-labeling W , source language corpus D s , labeled utterances for the new language K t , unlabeled utterances for the new language U t . 2: Initialize the model f θ . 3: Initialize labeled corpus D = D s ∪ K t . 4: for e=1 to E do 5:\n\nTrain and update f θ on D.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "6:",
      "text": "if mod(e, W ) = 0 then 7:\n\nGenerate pseudo-labels for U t using f θ .",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "8:",
      "text": "Form K ′ t by applying pseudo-label policy on U t .",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "9:",
      "text": "Expand labeled set by D = D ∪ K ′ t .\n\n10:\n\nend if 11: end for 12: Return: f θ",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Experiments",
      "text": "In this section the datasets considered for experiments and the experimental setup are presented.   2 . We consider seven speech emotion classification datasets in five languages: three in English (RAVDESS, SAVEE and TESS), one in French (CaFE), German (EmoDB), Italian (EMOVO), and Persian (ShEMO). In each dataset, speech samples have three attributes: audio data (i.e., the raw waveform, in mono), speaker identifier, and emotion label (e.g., angry, happy, sad). The datasets comprise scripted and acted utterances and vary in size (i.e., number of utterances), number of speakers, sample rate, and number of classes. All of them comprise utterances in which the speaker acts a specific emotion.\n\nThe considered datasets share the same five primary emotions, which are anger (A), fear (F), happiness (H), neutral (N), and sadness (S). Following  (Tamulevičius et al., 2020) , in this study we consider only utterances annotated with one of the previous five emotions and discard the remaining utterances. Thus, the number of emotion categories is C = 5. TESS, SAVEE, and RAVDESS datasets are merged to obtain a large English language dataset. A summary of the distributions of utterances by emotion for each language is shown in the Table  3 .",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Experimental Setup",
      "text": "Each dataset was split into training, validation, and testing sets to respectively train, optimize and evaluate task-specific emotion speech classifiers. Following SERAB  (Scheidwasser-Clow et al., 2022) , each dataset is split into 60% training, 20% validation, and 20% testing sets. Each data partition is speakerindependent, i.e., the sets of speakers included in each part are mutually disjoint. Importantly, the utterances of the various datasets have different sampling rates, for this reason they were all resampled to 16kHz before any processing.\n\nDuring training a sequence of 2 seconds is randomly sampled from the whole utterance for augmentation, while the whole utterance is used at testing time. Voice Activity Detection (VAD) is not used in training and testing. The linear layer for emotion speech classification is randomly initialized.\n\nAll experiments are run three times with different random seeds, and the unweighted accuracy is chosen as our evaluation criterion.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Hyperparameters",
      "text": "In our experiments, we train the model for a total of 100 epochs using the Adam optimizer with an initial learning rate equal to 1 × 10 -3 which decays by a factor of 0.95 every 10 epochs, a batch of 100 utterances, and exponential decay rates β 1 and β 2 equal to 0.9 and 0.999. The pseudo-labeling procedure is executed every 30 epochs, i.e. W = 30. We experiment with different values of τ (see Sec. 5.6.2 for a study of this hyperparameter) which lead to the choice of τ = 0.50, but do not attempt a careful adjustment of the regularization weights λ A and λ H and simply set them to 0.8 and 0.4 as done in  (Tanaka et al., 2018) .",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Results",
      "text": "In this section the results achieved for different configurations are described. In all the experiments we consider English as the first language while the other languages were chosen one at a time as the second language.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Cross-Lingual Results",
      "text": "In this section the performance obtainable by our method on a totally unknown new language is measured. These results give an idea of the worst accuracy, defined as the lower bound, achievable for cross-lingual SER. To this end, experiments using only the training set of the source language (i.e. English) and testing on the new language data are performed. Table  4  reports accuracy results achieved by the three utterance encoders for the four new languages. As it is possible to see, HuBERT achieves the best accuracy for all the languages, while the EmotionCNN obtains the worst performance. Regarding HuBERT, the highest accuracy equal to 78.87% is achieved for the German language while the lowest accuracy of 33.24% is obtained for Persian. The same gap is registered for all utterance encoders and depends on the fact that as stated by several linguistic distances  (Chiswick & Miller, 2005; Petroni & Serva, 2008; Gamallo et al., 2017)  Persian belongs to a linguistic strain very different from the English one. Therefore, it is conceivable that learning on utterance in Persian will produce an important gain in performance.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Multi-Lingual Results",
      "text": "In this section, multi-lingual SER experiments are performed following a supervised training that uses all data of the source and the new language. The results obtained give the upper bound for cross-lingual SER, i.e. the best accuracy obtainable having all the labels of the new language available. Since we want to evaluate whether the SER classifier generalizes on the new language but also if it preserves the performance on the source language. Table  5  shows the accuracy on the source language (i.e. first column of the table) and the accuracy on the new language (i.e. second column of the table) achieved by the three utterance encoders.\n\nFrom the results it is possible to make various considerations. First, as expected, there is an increase in performance on the new language by using the training data of both the source language and the new one. This result is particularly evident for Persian, where the performance increases of about 50% for all the utterance encoders. Second, EmotionCNN achieves significantly lower performance than the other two encoders, i.e. about 20% less than BYOL-S and 30% less than HuBERT. Third, the performance on English, which is the source language, does not degrade by adding the training data of the new language. For HuBERT, which confirms itself as the best encoder, starting from the 81.50% of accuracy obtained by training only on English, we obtain a loss of about 3% for both French and Persian. On the other hand, for Italian and German, the variation in accuracy is less than 1%.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "Pseudo-Labeling Results",
      "text": "In this subsection, the results of using the proposed method with HuBERT for cross-lingual SER are presented. Performance is reported while changing the number of labeled utterances for the new language. The numbers of labeled utterances considered for the new language are 0, 25, 50, and 100, while all the utterances for the source language are labeled (i.e. 1682 utterances). From the previous numbers we obtain the imbalanced ratios, γ l , equal to 0, 0.01, 0.03 and 0.06. Figure  3  shows the performance achieved by using hard pseudo-labels (see Fig.  3a ) and soft pseudo-labels (see Fig.  3b ). Each colored bar represents the accuracy of a given pseudo-label approach with a given number of available labels for the new language. Black narrow bars represent the accuracy on the source language.\n\nSeveral considerations can be made. First, overall hard pseudo-labels obtain better performance than soft pseudo-labels. Second, soft pseudo-labels not only result in worse performance than hard pseudo-labels, but also cause significant performance degradation on the source language. This behavior can be due to the effect of mixup augmentation which results in too noisy pseudo-labels that do not allow the model to converge properly. Third, for both pseudolabel approaches and for all languages, having more available labels for the new language allows to achieve higher accuracy. The pseudo-label approaches cannot reach the upper bound in any language but in any case they manage to improve the performance of the lower bound (for the Persian language it is even possible to have an increase of about 60%).\n\nFigure  5  illustrates the accuracy trends of the SER model with HuBERT and 100 labeled utterances across training epochs, depicting the performance of the training and validation sets. Each plot showcases accuracy curves for both the source and new languages. Despite having disjoint sets of speakers, the accuracy remains relatively consistent across the train and validation sets. However, the French and Italian languages, identified as the weakest performers, exhibit a tendency to overfit the model.",
      "page_start": 15,
      "page_end": 16
    },
    {
      "section_name": "Discussion",
      "text": "Figure  4  summarizes the results achieved by our best method with HuBERT in the different configurations evaluated for the recognition of emotions on new languages. The aim is to highlight the gap between training the method with all the data labeled or with the use of pseudo-labeled samples. In the chart we compare the performance for (i) the cross-lingual experiment (results collected from the Table  4 ), (ii) the multi-lingual experiment (accuracy reported in the Table  5 ), and (iii) the SSL cross-lingual experiment based on the use of hard pseudo-labels having 100 labeled utterances for the new language (see Fig.  3a ). As expected, multi-lingual SER with all the utterances labeled results in a noticeable increase in performance compared to the cross-lingual SER. This increase is particularly significant for Persian (+60%), a language that has very different linguistic traits from those of English and for this reason the adaptation of the model is very important. Cross-lingual SSL based on hard pseudo-labels   improves the performance of the cross-lingual configuration for all the considered languages (the increase is 50% for the Persian language).\n\nAn analysis of the feature space before and after the adaptation to the new language is also provided. The analysis aims to verify whether the pseudo-label approach can effectively reduce the variations between different languages while retaining information related to emotions. To illustrate this, we use Principal Component Analysis (PCA) to project the learned feature representation, i.e., the output of the utterance encoder into 3D space. Furthermore, the silhouette score  (Rousseeuw, 1987)  is exploited to estimate the ability of the learned representation to discriminate emotions independently of language. The silhouette's best score value is 1 and the worst value is -1. Values close to 0 indicate overlapping clusters.\n\nThe first row of Figure  6  displays the test utterances for both English and the new language encoded using HuBERT trained solely on English. Conversely, the second row shows the test utterances for both English and the new language encoded using HuBERT adapted on the new language via hard pseudo-labeling, and without any labeled utterances for the new language. The results indicate that the representation learned solely on English is unable to capture emotions for the new language, resulting in a very low silhouette score of approximately 0.07. In contrast, the adaptation procedure produces well-defined clusters based on emotion, independent of language, as evidenced by an average increase in the silhouette index of 0.36.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "French",
      "text": "",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "German",
      "text": "Italian Persian Each plot reports the accuracy on both the source language, i.e. English, and on each of the languages considered (better see in color and magnified).",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "French",
      "text": "",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Comparison With State-Of-The-Art",
      "text": "In this section, we compare the performance of the proposed method with four recent state-of-the-art methods, AL  (Kim et al., 2017) ,  DANN (Abdelwahab & Busso, 2018) , FLUDA  (Ahn et al., 2021) , and NNPM  (Li et al., 2021) . The above methods are trained for discriminating 4 emotion categories namely anger, happiness, neutral, and sadness. Implementations of the methods are not publicly available so we first report their performance on the German language from the original documents or reimplementations. For a more in-depth comparative analysis we reimplemented the considered methods in order to be able to estimate the performance also on the other three languages, namely French, Italian and Persian. We only report the results obtained for DANN and NNPM, for which the performance are in line with those declared. For a fair comparison with previous methods, we exclude from the all corpus the utterances labeled with emotion of fear and train the methods.\n\nOur method is retrained without using labeled utterances of the new language and taking advantage of hard pseudo-labels. Table  6  shows the comparison between two versions of the proposed method, i.e. with the backbone based on BYOL-S and HuBERT, and other methods. The results achieved for our implementations are respectively \"DANN (our reimplementation)\" and \"NNPM (our reimplementation)\". Results show that both versions of the proposed method perform better than recent state-of-the-art methods. More specifically, our HuBERT-based method outperforms the second best method, which is also based on pseudo-labeling, namely NNPM, with an improvement in relative accuracy of 30%. We get better performance than the multi-task learning method, i.e. AL, with 57% better accuracy, and the unsupervised cross-corpus SER model based on few-shot learning (i.e., FLUDA) of an increase of 64%.\n\nThis high gain in performance with respect to previous methods is due to three main aspects. First, the state-of-the-art methods consist of very simple architectures compared to that of HuBERT. Second, the model training procedure is profoundly different between the previous methods and that used for HuBERT. While the purpose of previous methods is to directly learn a specialized mapping of a speech signal into an emotion category, HuBERT and BYOL-S are trained to learn a general-purpose and robust representation of a speech signal. This last aspect allows the obtained representation to be more effective for the different tasks, including the recognition of emotions. Ultimately, the difference between HuBERT and BYOL-S is due to both the architectural aspect of the model and the cardinality of the dataset used for the pre-training. In fact, HuBERT is trained on a much larger and challenging dataset than the one used for BYOL-S.",
      "page_start": 19,
      "page_end": 20
    },
    {
      "section_name": "Ablation Study",
      "text": "This section presents an ablation study of the main design choices that led to the definition of the final method. The adaptability to the new language of CNN vs. Transformer based utterance encoders is evaluated. The effect of different values for the hard pseudo-labeling τ parameter is investigated. Finally, the impact of utterance rebalancing on the performance is estimated. Cross-and multi-lingual results demonstrate that HuBERT and BYOL-S provide more effective utterance encoding for emotion classification than Emo-tionCNN (see Sec. 5.1 and 5.2). In this section we perform the comparison for the different languages using hard pseudo-labels. The results for cross-lingual SER by varying the number of utterances labeled for the new language from 0 to 100 are shown in Figure  7 . As it is possible to see, HuBERT outperforms BYOL-S by a large gap (about +20% accuracy). This gap might be motivated by several reasons. First, HuBERT is trained on a larger and more diverse speech corpus (i.e. Librispeech), with both spontaneous and anechoic scripted speech, while BYOL-S is trained on a subset of AudioSet  (Elbanna et al., 2022) . Second, HuBERT's transformer-based architecture coupled with direct encoding of the raw waveform provides a more robust and powerful representation of speech.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Effect Of Τ",
      "text": "Among all the hyperparameters, the confidence threshold τ used for hard pseudo-labels (see Sec. 3.2.2) is the one that needs to be carefully tuned. This subsection studies the effects of τ on our hard pseudo-labeling approach. Table  7  reports the cross-lingual SER results by considering 100 labeled utterance for the new language and varying τ . From the results it is possible to observe that the choice of the best τ does not generalize to all languages. However, for 0.50 the best performance is obtained in most cases and for this reason it was chosen.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Effect Of Utterance Rebalancing",
      "text": "Here we quantitatively evaluate the contribution of the utterance rebalancing procedure on the performance of our method (see Sec. 3.2.3 for details). Table  8  shows the cross-lingual SER results of our method without utterance rebalance \"w/o rebalance\" and with utterance rebalance \"w rebalance\". As it is possible to see, the version of the method with utterance rebalancing outperforms the   version without utterance rebalancing for all languages. The highest accuracy improvement corresponding to 40% is registered for the Persian language. The lowest gaps of 8% and 15% between the two versions are obtained for French and Italian, respectively.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Conclusions",
      "text": "In cross-lingual SER it is common to have many labeled utterances for the English language and a lower availability of labels for other languages. Based on this consideration, an SSL approach for cross-lingual speech emotion recognition is proposed.\n\nThe proposed method consists of a transformer able to classify an utterance into an emotion category. For SSL, we experimented with the use of hard and soft pseudo-labels for unlabeled utterances. The proposed method is evaluated using English as source language and four different languages (French, German, Italian and Persian) as new languages. It is revealed that the use of hard over soft pseudo-labels allows for better results on the new language at the expense of a drop in performance on the source language.\n\nExperimental results show that the average accuracy has increased by 40% in comparison with state-of-the-art methods.\n\nThe proposed method has some limitations, of course. First, the method assumes that the number of emotions in the source and the new language is the same. Nonetheless, in real-wold applications this constraint could be too stringent. To overcome this limitation, prototypes could be learned from representations for newly-introduced emotion categories in the target language, using a methodology similar to the one described in  (Bucher et al., 2021) . Second, for the hard-pseudo labeling approach there is no handling of confirmation bias, i.e. overfitting to incorrect pseudo-labels predicted by the network. In fact, if the model makes several wrong unlabeled predictions, pseudo-labeling can act like a bad feedback loop and deteriorate performance. As future work we plan to handle the confirmation bias issue by averaging the predictions for different views of the unlabeled data as done in  (Berthelot et al., 2019)  or by using reinforcement learning  (Latif et al., 2022) .",
      "page_start": 22,
      "page_end": 23
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: a). The SER model is then adapted to a new language for",
      "page": 2
    },
    {
      "caption": "Figure 1: b). Pseudo-labeling is adopted",
      "page": 2
    },
    {
      "caption": "Figure 1: Pipeline of the proposed cross-lingual SER method. (a) The model is first trained",
      "page": 4
    },
    {
      "caption": "Figure 2: , our method consists of two modules, namely the",
      "page": 7
    },
    {
      "caption": "Figure 2: The pipeline of our method.",
      "page": 8
    },
    {
      "caption": "Figure 3: shows the performance achieved by using hard pseudo-labels",
      "page": 15
    },
    {
      "caption": "Figure 3: a) and soft pseudo-labels (see Fig. 3b). Each colored bar represents",
      "page": 15
    },
    {
      "caption": "Figure 5: illustrates the accuracy trends of the SER model with HuBERT",
      "page": 15
    },
    {
      "caption": "Figure 4: summarizes the results achieved by our best method with HuBERT",
      "page": 15
    },
    {
      "caption": "Figure 3: SER accuracy by varying the number of labeled utterances for the new language and",
      "page": 16
    },
    {
      "caption": "Figure 4: Comparison of the results on new languages between the cross-lingual, multi-lingual",
      "page": 17
    },
    {
      "caption": "Figure 6: displays the test utterances for both English and",
      "page": 17
    },
    {
      "caption": "Figure 5: Accuracy curves for training and validation sets over the training of HuBERT with",
      "page": 18
    },
    {
      "caption": "Figure 6: PCA plot of the learned feature representation with emotion and language labels",
      "page": 18
    },
    {
      "caption": "Figure 7: As it is possible to see, HuBERT outperforms",
      "page": 20
    },
    {
      "caption": "Figure 7: HuBERT vs.",
      "page": 21
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "100\n90\n)%(\n80 ycaruccA\n70": "60\n50\n0 20 40 60 80\nEpoch",
          "Column_2": "",
          "100\n90\n80\n70": "60\n50\n0 20 40 60 80\nEpoch",
          "Column_4": "",
          "Column_6": "",
          "Column_8": ""
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Silhouette score: 0.08": "⇓",
          "Silhouette score: 0.07": "",
          "Column_5": ""
        },
        {
          "Silhouette score: 0.08": "Silhouette score: 0.40",
          "Silhouette score: 0.07": "Silhouette score: 0.31",
          "Column_5": ""
        }
      ],
      "page": 18
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Domain adversarial for acoustic emotion recognition",
      "authors": [
        "M Abdelwahab",
        "C Busso"
      ],
      "year": "2018",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "2",
      "title": "Cross-corpus speech emotion recognition based on few-shot learning and domain adaptation",
      "authors": [
        "Y Ahn",
        "S Lee",
        "J Shin"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "3",
      "title": "Pseudo-labeling and confirmation bias in deep semi-supervised learning",
      "authors": [
        "E Arazo",
        "D Ortego",
        "P Albert",
        "N O'connor",
        "K Mcguinness"
      ],
      "year": "2020",
      "venue": "ternational Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "4",
      "title": "The most spoken languages in the world",
      "authors": [
        "Berlitz"
      ],
      "year": "2021",
      "venue": "The most spoken languages in the world"
    },
    {
      "citation_id": "5",
      "title": "Real-time speech emotion and sentiment recognition for interactive dialogue systems",
      "authors": [
        "D Bertero",
        "F Siddique",
        "C.-S Wu",
        "Y Wan",
        "R Chan",
        "P Fung"
      ],
      "year": "2016",
      "venue": "Conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "6",
      "title": "Mixmatch: A holistic approach to semi-supervised learning",
      "authors": [
        "D Berthelot",
        "N Carlini",
        "I Goodfellow",
        "N Papernot",
        "A Oliver",
        "C Raffel"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "7",
      "title": "Handling new target classes in semantic segmentation with domain adaptation",
      "authors": [
        "M Bucher",
        "T.-H Vu",
        "M Cord",
        "P Pérez"
      ],
      "year": "2021",
      "venue": "Handling new target classes in semantic segmentation with domain adaptation"
    },
    {
      "citation_id": "8",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Interspeech"
    },
    {
      "citation_id": "9",
      "title": "Unsupervised cross-lingual speech emotion recognition using domain adversarial neural network",
      "authors": [
        "X Cai",
        "Z Wu",
        "K Zhong",
        "B Su",
        "D Dai",
        "H Meng"
      ],
      "year": "2021",
      "venue": "International Symposium on Chinese Spoken Language Processing"
    },
    {
      "citation_id": "10",
      "title": "Linguistic distance: A quantitative measure of the distance between english and other languages",
      "authors": [
        "B Chiswick",
        "P Miller"
      ],
      "year": "2005",
      "venue": "Journal of Multilingual and Multicultural Development"
    },
    {
      "citation_id": "11",
      "title": "Emovo corpus: an italian emotional speech database",
      "authors": [
        "G Costantini",
        "I Iaderola",
        "A Paoloni",
        "M Todisco"
      ],
      "year": "2014",
      "venue": "International Conference on Language Resources and Evaluation"
    },
    {
      "citation_id": "12",
      "title": "Towards transferable speech emotion representation: On loss functions for cross-lingual latent representations",
      "authors": [
        "S Das",
        "N Lønfeldt",
        "A Pagsberg",
        "L Clemmensen"
      ],
      "year": "2022",
      "venue": "International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "Bert: Pretraining of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "NAACL-HLT"
    },
    {
      "citation_id": "14",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M El Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Survey on speech emotion recognition: Features, classification schemes, and databases"
    },
    {
      "citation_id": "15",
      "title": "Byol-s: Learning self-supervised speech representations by bootstrapping",
      "authors": [
        "G Elbanna",
        "N Scheidwasser-Clow",
        "M Kegler",
        "P Beckmann",
        "K Hajal",
        "M Cernak"
      ],
      "year": "2022",
      "venue": "Byol-s: Learning self-supervised speech representations by bootstrapping"
    },
    {
      "citation_id": "16",
      "title": "Cross-language acoustic emotion recognition: An overview and some tendencies",
      "authors": [
        "S Feraru",
        "D Schuller"
      ],
      "year": "2015",
      "venue": "International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "17",
      "title": "From language identification to language distance",
      "authors": [
        "P Gamallo",
        "J Pichel",
        "I Alegria"
      ],
      "year": "2017",
      "venue": "Physica A: Statistical Mechanics and its Applications"
    },
    {
      "citation_id": "18",
      "title": "Audio set: An ontology and humanlabeled dataset for audio events",
      "authors": [
        "J Gemmeke",
        "D Ellis",
        "D Freedman",
        "A Jansen",
        "W Lawrence",
        "R Moore",
        "M Plakal",
        "M Ritter"
      ],
      "year": "2017",
      "venue": "International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "A canadian french emotional speech dataset",
      "authors": [
        "P Gournay",
        "O Lahaie",
        "R Lefebvre"
      ],
      "year": "2018",
      "venue": "Multimedia Systems"
    },
    {
      "citation_id": "20",
      "title": "Bootstrap your own latent-a new approach to self-supervised learning",
      "authors": [
        "J.-B Grill",
        "F Strub",
        "F Altché",
        "C Tallec",
        "P Richemond",
        "E Buchatskaya",
        "C Doersch",
        "B Avila Pires",
        "Z Guo",
        "M Gheshlaghi Azar"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "21",
      "title": "A generalizable speech emotion recognition model reveals depression and remission",
      "authors": [
        "L Hansen",
        "Y.-P Zhang",
        "D Wolf",
        "K Sechidis",
        "N Ladegaard",
        "R Fusaroli"
      ],
      "year": "2022",
      "venue": "Acta Psychiatrica Scandinavica"
    },
    {
      "citation_id": "22",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "23",
      "title": "Towards speech emotion recognition \"in the wild\" using aggregated corpora and deep multitask learning",
      "authors": [
        "J Kim",
        "G Englebienne",
        "K Truong",
        "V Evers"
      ],
      "year": "2017",
      "venue": "Towards speech emotion recognition \"in the wild\" using aggregated corpora and deep multitask learning"
    },
    {
      "citation_id": "24",
      "title": "Cross-language speech emotion recognition using bag-of-word representations, domain adaptation, and data augmentation",
      "authors": [
        "S Kshirsagar",
        "T Falk"
      ],
      "year": "2022",
      "venue": "Cross-language speech emotion recognition using bag-of-word representations, domain adaptation, and data augmentation"
    },
    {
      "citation_id": "25",
      "title": "A survey on deep reinforcement learning for audio-based applications",
      "authors": [
        "S Latif",
        "H Cuayáhuitl",
        "F Pervez",
        "F Shamshad",
        "H Ali",
        "E Cambria"
      ],
      "year": "2022",
      "venue": "A survey on deep reinforcement learning for audio-based applications"
    },
    {
      "citation_id": "26",
      "title": "Unsupervised adversarial domain adaptation for cross-lingual speech emotion recognition",
      "authors": [
        "S Latif",
        "J Qadir",
        "M Bilal"
      ],
      "year": "2019",
      "venue": "International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "27",
      "title": "Aggression recognition using overlapping speech",
      "authors": [
        "I Lefter",
        "C Jonker"
      ],
      "year": "2017",
      "venue": "International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "28",
      "title": "Unsupervised cross-lingual speech emotion recognition using pseudo multilabel",
      "authors": [
        "J Li",
        "N Yan",
        "L Wang"
      ],
      "year": "2021",
      "venue": "Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "29",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "30",
      "title": "Cross-lingual and multilingual speech emotion recognition on english and french",
      "authors": [
        "M Neumann"
      ],
      "year": "2018",
      "venue": "International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "31",
      "title": "Shemo: a large-scale validated database for persian speech emotion detection. Language Resources and Evaluation",
      "authors": [
        "O Nezami",
        "P Lou",
        "M Karami"
      ],
      "year": "2019",
      "venue": "Shemo: a large-scale validated database for persian speech emotion detection. Language Resources and Evaluation"
    },
    {
      "citation_id": "32",
      "title": "Cross lingual speech emotion recognition via triple attentive asymmetric convolutional neural network",
      "authors": [
        "E Ocquaye",
        "Q Mao",
        "Y Xue",
        "H Song"
      ],
      "year": "2021",
      "venue": "International Journal of Intelligent Systems"
    },
    {
      "citation_id": "33",
      "title": "Librispeech: an asr corpus based on public domain audio books",
      "authors": [
        "V Panayotov",
        "G Chen",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "34",
      "title": "User state modeling based on the arousal-valence plane: Applications in customer satisfaction and health-care",
      "authors": [
        "P Perez-Toro",
        "J Vasquez-Correa",
        "T Bocklet",
        "E Noth",
        "J Orozco-Arroyave"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "35",
      "title": "Language distance and tree reconstruction",
      "authors": [
        "F Petroni",
        "M Serva"
      ],
      "year": "2008",
      "venue": "Journal of Statistical Mechanics: Theory and Experiment"
    },
    {
      "citation_id": "36",
      "title": "Toronto emotional speech set (TESS)",
      "authors": [
        "M Pichora-Fuller",
        "K Dupuis"
      ],
      "year": "2020",
      "venue": "Toronto emotional speech set (TESS)",
      "doi": "10.5683/SP2/E8H2MF"
    },
    {
      "citation_id": "37",
      "title": "Silhouettes: a graphical aid to the interpretation and validation of cluster analysis",
      "authors": [
        "P Rousseeuw"
      ],
      "year": "1987",
      "venue": "Journal of computational and applied mathematics"
    },
    {
      "citation_id": "38",
      "title": "Serab: A multi-lingual benchmark for speech emotion recognition",
      "authors": [
        "N Scheidwasser-Clow",
        "M Kegler",
        "P Beckmann",
        "M Cernak"
      ],
      "year": "2022",
      "venue": "International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "39",
      "title": "Computational paralinguistics: emotion, affect and personality in speech and language processing",
      "authors": [
        "B Schuller",
        "A Batliner"
      ],
      "year": "2013",
      "venue": "Computational paralinguistics: emotion, affect and personality in speech and language processing"
    },
    {
      "citation_id": "40",
      "title": "Learning to compare: Relation network for few-shot learning",
      "authors": [
        "F Sung",
        "Y Yang",
        "L Zhang",
        "T Xiang",
        "P Torr",
        "T Hospedales"
      ],
      "year": "2018",
      "venue": "Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "41",
      "title": "A study of cross-linguistic speech emotion recognition based on 2d feature spaces",
      "authors": [
        "G Tamulevičius",
        "G Korvel",
        "A Yayak",
        "P Treigys",
        "J Bernatavičienė",
        "B Kostek"
      ],
      "year": "2020",
      "venue": "A study of cross-linguistic speech emotion recognition based on 2d feature spaces"
    },
    {
      "citation_id": "42",
      "title": "Joint optimization framework for learning with noisy labels",
      "authors": [
        "D Tanaka",
        "D Ikami",
        "T Yamasaki",
        "K Aizawa"
      ],
      "year": "2018",
      "venue": "Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "43",
      "title": "Effects of physiological arousal on speech motor control and speech motor practice in preschool-age children who do and do not stutter",
      "authors": [
        "V Tumanova",
        "C Woods",
        "Q Wang"
      ],
      "year": "2020",
      "venue": "Journal of Speech, Language, and Hearing Research"
    },
    {
      "citation_id": "44",
      "title": "Machine Audition: Principles, Algorithms and Systems: Principles, Algorithms and Systems",
      "authors": [
        "W Wang"
      ],
      "year": "2010",
      "venue": "Machine Audition: Principles, Algorithms and Systems: Principles, Algorithms and Systems"
    },
    {
      "citation_id": "45",
      "title": "Speech emotion recognition cross language families: Mandarin vs. western languages",
      "authors": [
        "Z Xiao",
        "D Wu",
        "X Zhang",
        "Z Tao"
      ],
      "year": "2016",
      "venue": "International Conference on Progress in Informatics and Computing (PIC)"
    },
    {
      "citation_id": "46",
      "title": "Mixup: Beyond empirical risk minimization",
      "authors": [
        "H Zhang",
        "M Cisse",
        "Y Dauphin",
        "D Lopez-Paz"
      ],
      "year": "2018",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "47",
      "title": "Enhanced semi-supervised learning for multimodal emotion recognition",
      "authors": [
        "Z Zhang",
        "F Ringeval",
        "B Dong",
        "E Coutinho",
        "E Marchi",
        "B Schüller"
      ],
      "year": "2016",
      "venue": "International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "48",
      "title": "Transferable positive/negative speech emotion recognition via class-wise adversarial domain adaptation",
      "authors": [
        "H Zhou",
        "K Chen"
      ],
      "year": "2019",
      "venue": "International Conference on Acoustics, Speech and Signal Processing"
    }
  ]
}