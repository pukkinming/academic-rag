{
  "paper_id": "2104.07767v2",
  "title": "Exploring Visual Engagement Signals For Representation Learning",
  "published": "2021-04-15T20:50:40Z",
  "authors": [
    "Menglin Jia",
    "Zuxuan Wu",
    "Austin Reiter",
    "Claire Cardie",
    "Serge Belongie",
    "Ser-Nam Lim"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Visual engagement in social media platforms comprises interactions with photo posts including comments, shares, and likes. In this paper, we leverage such Visual Engagement clues as supervisory signals for representation learning. However, learning from engagement signals is non-trivial as it is not clear how to bridge the gap between low-level visual information and high-level social interactions. We present VisE, a weakly supervised learning approach, which maps social images to pseudo labels derived by clustered engagement signals. We then study how models trained in this way benefit subjective downstream computer vision tasks such as emotion recognition or political bias detection. Through extensive studies, we empirically demonstrate the effectiveness of VisE across a diverse set of classification tasks beyond the scope of conventional recognition 1 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "People post photos on social media to invite engagement and seek connections. A photo of a cute dog can resonate with other dog lovers and trigger reactions such as the \"like\" or \"love\" button or comments including \"what an adorable dog\" and \"look at those blue eyes!\" The widely available interactions with the photos posted on social media, which we call visual engagement, contain rich semantic descriptions (\"dog\", \"blue eyes\") and are far less expensive to obtain than manual annotations in standard computer vision tasks, including coarse and fine-grained class labels  [11, 80, 95] , bounding boxes  [51, 23] , and image captions  [8] .\n\nMore importantly, visual engagement, including comments, replies, likes, and shares, provides emotional and cultural context that goes beyond the image content therein. For example, the image in Fig.  1  could be described in a standard captioning task as \"a dog sits next to a stuffed animal.\" The social media audience of this post may react to the cuteness of the dog, comment on the torn stuffed animal with whimsical responses, or initiate a conversation. Visual Engagement:\n\n• Does he know how cute he looks?\n\n• He looks so innocent • Who me?\n\n• !!!!❤❤\n\n• Haha. They call them canine teeth for a reason. • Not destroyed, just well loved.\n\n• LOL! Figure  1 . Visual engagement vs. other common supervisory signals. Given the same image, visual engagement provide semantically and contextually richer information than conventional recognition and captioning tasks.\n\nThe resulting textual descriptions depart from the exactly what it says on the tin approach of standard image captioning tasks and express private states  [65, 82] : opinions, emotions, and speculations, for example. We argue that visual engagement can also serve as supervisory signals for representation learning and transfer well to subjective downstream computer vision tasks like emotion recognition or political bias classification.\n\nMotivated by this observation, we propose to learn image representations from semantically and contextually rich Visual Engagement signals (VisE). We hypothesize that such learned representations, as a byproduct of mapping image content to human reactions, are able to infer private states expressed by images. This is beneficial and could serve as a nice addition to current computer vision research which in general focuses on the objectively present factual information from images (e.g., \"this is a dog\" vs. \"what a cute dog!\").\n\nOpen-world visual engagement contains dense subjectivity clues, but is inherently noisy in nature. How to properly leverage such signals for representation learning is a challenging, open question. Inspired by recent studies on feature learning from proxy tasks  [19, 3, 84] , we cluster each type of visual engagement and obtain cluster assignment indices for all the responses associated with a training image. These cluster assignments are used as supervisory cues. We then train a network from scratch to map images to cluster assignments in a multi-task fashion for representation learning, where each task is to predict the cluster index for that type of response. In this paper, we consider two forms of human responses:  (1)  comments and (2) reactions. In the former case, we conduct the clustering on representations encoded by a textual model. Unlike most existing multi-modal methods that perform pre-training for both language and vision modules  [12, 66]  with hundreds of millions of parameters, we simply use an off-the-shelf encoder to embed comments, which is computationally efficient. We then evaluate representations learned from engagement signals on downstream tasks.\n\nOur main contribution is to demonstrate that social media engagement can provide supervision for learning image representations that benefit subjective downstream tasks. To this end, we explore VisE pre-trained on 250 million publicly available social media posts. Through extensive experiments, we show that in three downstream tasks related to private states detection, the learned VisE models can outperform the ImageNet-supervised counterpart by a substantial margin in some cases. These results highlight that VisE broadens the current representation learning paradigms, thereby narrowing the gap between machine and human intelligence.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Visual representation learning Learning discriminative features for targeted datasets is a core problem for many computer vision research efforts. Feature learning on small datasets is particularly difficult since high-capacity deep neural networks suffer from overfitting when the amount of target training data is limited. One popular way to mitigate this issue is to pre-train on large-scale image datasets with manually curated class labels such as ImageNet and COCO  [13, 15] . However, training on ImageNet requires manually labeled data, which are expensive to obtain and hard to scale. This motivates a plethora of work investigating weakly-supervised  [73, 55, 41, 42, 47] , semisupervised  [86, 87, 84]  and self-supervised learning  [3, 96, 14, 4, 22, 48, 90] . These methods tap into alternative forms of supervision, such as user-provided tags  [76, 55]  and hand-crafted pretext tasks (e.g., inpainting  [64] , colorization  [91, 92] , predicting jigsaw permutations  [60] , rotations of inputs  [17] ). More recently, contrastive learning  [24, 25]  is used for feature learning by bringing images closer to their augmented versions than other samples in the training set  [83, 89, 26, 58, 6] . In this paper, we use visual engagement, which encompasses high-level semantics, as supervisory signals for representation learning.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Learning Image Representations Using Natural Language",
      "text": "There is growing interest in learning joint visual-language representations  [16, 72, 39, 49, 54, 5, 9] . Other studies convert language to discrete labels or continuous probability distributions, such as individual word, part-of-speech (POS) tags, clustering assignments of sentence features, and topic modeling  [38, 46, 19, 2] . Some approaches learn visual representations from a pretext task that predicts the natural language captions from images  [2, 12, 69] . Recently introduced methods including ConVIRT  [93] ,CLIP  [66]  and ALIGN  [32]  allow one to learn visual representations with contrastive objectives using image-text pairs. These works use objective natural language, which describes and informs the content of the images, and mostly evaluate their methods on conventional recognition datasets. We instead utilize the density of subjectivity clues in social media engagement and explore the transferability of the learnt representation to alternative downstream tasks.\n\nBeyond conventional recognition Traditional computer vision tasks focus on the recognition of tangible properties of images, such as objects (both entry-level  [11, 51]  and subordinate categories  [80, 78, 33] ) and scenes  [95] . The research on representation learning mentioned above focuses on this type of task. Relatively little attention has been paid to tasks that involve private states  [65, 82]  where subjectivity analysis is relevant. This area includes (1) detecting cyberbullying and hate speech  [29, 71, 20, 40] , (2) identifying emotions  [43, 1, 59, 62, 81] , (3) understanding rhetoric and intentions  [36, 37, 70, 30, 31, 88, 74, 45, 34] . The present work aims to advance research in this area by learning effective features from high-level engagement signals.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Approach",
      "text": "With the aim of learning representations that capture the relationships between image content and human responses, we introduce a simple yet effective framework, VisE. It infers visual engagement signals from images (Sec. 3.1). VisE is trained on large scale image-engagement pairs from a social media platform in a multi-task fashion, which will be described in Sec. 3.2 and Sec. 3.3.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "From Engagement To Labels",
      "text": "How people react to images is more telling than the content itself. In this work, we propose to predict raw social engagement signals by converting them into a bag-of-word multi-label multi-task classification task. Fig.  2  illustrates the pipeline we use in this work.\n\nMore formally, let (x, {e}) be an image and a set of corresponding engagement clues (e.g., comments, replies, likes, etc.). Let φ be a general engagement feature extractor that transforms e into a numerical representation φ(e) ∈ R D . We describe the proposed method to preprocess and obtain pseudo-label for e below. The pseudo-label is the clustered assignment index that is computed by first transforming the raw engagement (e) to a numerical representation φ(e). See the main text for more details.\n\nStep 1 (Cluster Generation) We first collect e from a randomly sampled subset of the entire image-engagement pairs and generate k clusters using K-means algorithm. This portion of the dataset will not be used during pretraining.\n\nStep 2 (Label Creation) Given the i th example from the unprocessed subset, (x i , {e} i ), we obtain a set of features {φ(e)} i from the engagement set of this example, using the same function φ. Next we collect the resulting cluster assignments indices as a set of labels {y e } for this image x i . Fig.  2 (a) summarizes this step.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Engagement Types",
      "text": "We use textual comments (C) and raw reactions (R) from publicly available Facebook posts as a first step to investigate visual engagement signals for representation learning.\n\nComments Comments are direct human responses from image posts. Maximal 100 comments are randomly sampled from a post. We use the bag-of-word approach with the term frequency-inverse document frequency (TF-IDF) weighted document embeddings  [68]  as comment feature extractor φ C . The cluster assignments derived from the comment set are used as multi-label classification targets, one label for each sampled comment associated with this specific image.\n\nRaw reactions Reactions are encoded as a normalized distribution over the five reaction buttons (\"haha\", \"sorry\", \"angry\", \"wow\", \"love\"). More specifically, we count the total occurrences of the 5 reaction buttons for each image post and normalize (L2) them to account for differences in followers and popularity of the post. Each post is mapped to a single cluster centroid index.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Training Vise Models",
      "text": "VisE learning objective VisE is trained in a multi-task learning framework by minimizing the following loss function:\n\nwhere L C is a cross-entropy loss with soft probability  [55] , L R is the standard cross-entropy loss, f represents an image feature encoder parameterized by W . D represents the training data which the image-engagement pairs are sampled from. In addition, {y C } and {y R } denote the pseudo labels of comments and reactions, respectively.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "We evaluate the effectiveness of feature representations learned by VisE for a wide range of downstream tasks. In our experiment, we aim to show that image representations learned from engagement signals are beneficial for image classification that beyond the scope of conventional recognition tasks. We begin by describing our experimental setups, including a comparison of alternative representation learning methods (Sec. 4.1), implementation details (Sec. 4.2), and a summary of evaluated downstream tasks (Sec. 4.3). Finally we present results and discussion in Sec. 4.4 and Sec. 4.5.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Compared Methods",
      "text": "To train VisE, we collect a total of 270 million public image posts from a social media platform with 20 million used for cluster generation (see Sec. 3.1). To facilitate a fair comparisons with the ImageNet-supervised method, we also randomly sample 1.23 million images for pre-training. We compare VisE pre-trained on 1.23 million (VisE-1.2M) and 250 million data (VisE-250M) with other feature representation learning methods.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Uni-Modal Learning Methods",
      "text": "We first compare VisE trained with pseudo labels derived from clustering assignments with networks that are trained with a pre-defined set of object labels:\n\nIN-Sup images object labels ImageNet  [11]  1.28M CNN IG-940M-IN  [55]  images hashtags + labels IG  [55]  + ImageNet  [11]  940M + 1.28M CNN MoCo-v2  [7]  image pairs -ImageNet  [11]  83.9B CNN VQAGrid  [35]  images object + attribute VisualGenome  [44]  103k Faster-RCNN\n\nVirTex  [12]  images captions COCO-caption  [8]  118k CNN + Transformer ICMLM  [2]  images + captions masked token from captions COCO-caption  [8]  118k CNN + Transformer CLIP  [66]  images for the textual module. We include the negative image-image / image-text pairs when counting the total data size for MoCo-v2 and CLIP. See Appendices for more details. We also acknowledge that the effective data size is also affected by other factors such as data augmentations in other approaches. For simplicity, we use the actual dataset size for non-contrastive learning methods.\n\n• ImageNet-supervised (IN-Sup): the image encoder is pre-trained on ILSVRC 2012  [11]  train split (1.28M images) 2 . The dataset has 1000 classes, which is based on the concepts in WordNet  [56] .\n\n• IG-imagenet (IG-940M-IN)  [55] : the visual encoder is pre-trained on 940 million public images with 1.5K hashtags in weakly-supervised fashion; the encoder is further fine-tuned on ImageNet dataset.\n\n• VQAGrid  [35] : a pre-training method primarily for visual question answering and image captioning tasks. It learns visual representations by training a Faster-RCNN  [67]  on the Visual Genome dataset  [44]  which has 1600 object categories and 400 attributes. We use the outputs from the last bottleneck block of the ResNet-50 as the pre-trained image representations.\n\n• MoCo-v2  [27] : a self-supervised contrastive method using a momentum-based encoder and a memory queue trained on ImageNet. Given an image sample, it is trained to be closer to its randomly augmented version on a hypersphere than other samples in the dataset. We use the improved version  [7]  that is trained with 800 epochs. Note that this method uses image as supervision labels instead of the ImageNet class labels.\n\nCross-modality pre-training Learning methods that use natural language as supervisory signals are also considered:\n\n• ICMLM  [2] : it uses 118K image-text pairs from COCO-captions  [8]  and uses masked language modeling to learn visual representations from text. We include two versions of this method, ICMLM tfm and ICMLM att-fc , which respectively use a transformer and an attention-based mechanism for joint fine-tuning. 2 Pre-trained models are from torchvision package for ResNet-50/110.\n\n• VirTex  [12] : this method also pre-trains on COCOcaptions but on a different task: generating captions based on images.\n\n• CLIP  [66] : Contrastive Language-Image Pre-training method (CLIP) utilizes an image encoder and a text encoder to predict which images are paired with which textual descriptions in a large-scale dataset of 400M image-text pairs.\n\nIt is worth pointing out all of these approaches train a text encoder to learn better natural language representations during pre-training stage, while VisE simply uses an off-theshelf text encoder to compute representations for clustering purposes. We expect better performance of VisE if these textual representations are further fine-tuned  [3]  as these aforementioned methods. We also report results of \"Random Init\", where no pretrained features are used. Table  1  summarizes the differences between VisE and all of the baseline methods used in the experiment.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Evaluation Protocols And Details",
      "text": "We adopt two common protocols for evaluating the effectiveness of feature representations  [55, 21, 57, 27] :  (1)  Linear evaluation: all pre-trained models are used as visual feature extractors, where the weights of the image encoders are fixed. This protocol is preferred for applications where computational resources or training data for target tasks are limited. The test performance indicates how effective the learned representations are for specific tasks. (2) Finetuning: the parameters of the pre-trained image encoders are used as an advanced weight initialization method; these encoders are fine-tuned in an end-to-end manner for downstream tasks. Prior studies  [18, 67]  have shown that the latter protocol outperforms the linear evaluation approach due to its flexibility and adaptability to a wider range of downstream tasks.\n\nSee the Appendix C for more implementation details, including a full list of hyperparameters used (batch sizes, learning rates, decay schedules, etc.) and sensitivity to hyperparameters for both linear and fine-tuned experiments.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Downstream Tasks",
      "text": "We evaluate these visual representation methods on four downstream tasks, including sentiment classification, political bias, hate speech detection and fine-grained bird species classification.\n\nUnbiasedEmotion Dataset This dataset  [62]  contains 3045 images annotated into six emotional categories. To reduce object biases in the dataset, different emotion labels contain the same set of objects/scenes. Since there is no official split of this dataset, we random split the images into train (70%), val (10%), test (20%) set five times and report mean and standard deviation of the resulting accuracy.\n\nPolitics The task of this dataset  [75]  is to predict the political leaning (left and right) of images from news media. This dataset contains 749,932 images in total. Since only train and testset are publicly available, we randomly split the training set into train (90%) and val (10%) and report accuracy scores.\n\nHateful Memes Hateful Memes dataset  [40]  contains multimodal memes including images and text. The task is to detect each meme is hate speech or not. We use the data from Phase 1 of the Hateful Memes challenge 3 , which has 8500 training and 500 validation data. We obtain the sentence embeddings from a pre-trained RoBERTa model  [52] , and concatenate the text features and image features together before linear evaluation that map the feature to the label space. We report the macro averaged ROC AUC score and accuracy score on the val set.\n\nCaltech-UCSD Birds-200-2011 (CUB-200-2011) In addition to the above subjective classification tasks, we also evaluate our approach on standard image classification tasks. To this end, we use the CUB-200-2011 dataset. CUB-200-2011  [80]  has a total of 11,788 images allocated over 200 (mostly North American) bird species. It is a benchmarking dataset for subordinate categorization. We train on the publicly available train set and report top-1 accuracy on the val set.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Vise For Subjective Recognition Tasks",
      "text": "Following the two protocols described in Sec. 4.2, we compare the transfer-learning capabilities of VisE with other baseline approaches for three subjective tasks. Fig.  3  presents the results for the linear evaluation and fine-tuned protocols, respectively. 3 Hateful Memes: Phase 1 Challenge VisE vs. other uni-modal methods From Fig.  3 , we can see that: (1) VisE is consistently better than other imageencoder only methods on three datasets over two visual backbone choices, except for MoCo-v2 in Politics. Even pre-trained with similar amounts of data (1.28M vs. 1.23M), VisE-1.2M still achieves better performance across all three tasks with a ResNet-50 backbone than models trained with labels from In-Sup. (2) VisE-250M substantially outperforms IG-940M-IN, a method trained with a substantially larger amount of pre-trained data (950M vs. 250M). (3) MoCo-v2, a self-supervised approach that does not require object category annotations, yields the best accuracy scores on Politics among approaches with ResNet-50 backbone. This also highlights the limitation of using object labels as pre-training supervision for such subjective tasks.\n\nVisE vs. other methods that learn from language Under both protocols, VisE offers better or comparable results than other methods that leverage textual information during pre-training. VisE achieves consistently better performance across all three tasks. This suggests that features learned with visual engagement signals are more suitable for subjective downstream tasks. We also observe that all other four visual-language approaches obtain better results than IN-Sup on UnbiasedEmotion dataset when fine-tuned, but they are worse than IN-Sup with linear evaluation. Such discrepancy might be caused by the scale of the dataset, since UnbiasedEmotion is the smallest among the other tasks.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Vise For Standard Recognition Tasks",
      "text": "We compare the effectiveness of features learned from visual engagement signals with those trained using Ima-geNet labels on CUB-200-2011. This task extends the general object classification from ImageNet and focuses on distinguishing fine-grained differences among 200 bird species. Moreover, 59 out of 1000 classes in ImageNet are already bird categories, including overlapping definitions with CUB-200-2011  [77] . Thus Image-Net based approaches should transfer to this task better than VisE. Table 2 shows the results of both linear evaluation and finetuned transfer protocols.\n\nIndeed, IN-Sup and IG-940M-IN achieve decent accuracy scores using linear classifiers alone to map the features to 200 bird species, outperforming VisE by a large margin. This is foreseeable since visual engagement signals do not necessarily contain object information. It is understandable that VisE features are not as transferable to this task as models trained on ImageNet.\n\nWhen fine-tuning is performed, VisE with ResNet-50 have comparable or better performance than IN-Sup. This highlights that fine-tuning the whole network can sometimes compensate for the inflexibility of learned features, which is in line with discussions in  [66] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Analysis",
      "text": "To better understand the values of visual engagement signals and our pre-trained VisE models, we conduct ablation studies and qualitative analysis using the same set of subjective target tasks. All experiments use the fine-tuned protocol unless otherwise specified. Additional results and analysis are included in the Appendix B).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Pre-Training Ablation",
      "text": "Effect of pre-training data size We additionally train VisE models using randomly sampled {123k, 308k, 615k} images using fine-tuned setting. Fig.  4  presents the results for three datasets compared with other baselines for easy reference. From Fig.  4 (a) and 4(b), we can see that there is a positive correlation between training data size and model performance for VisE. VirTex and ICMLM achieve similar results as VisE with less training data (118k vs. {615k, 1.23M}). This shows the noisy pseudo-labels from visual engagement might need more data to compensate its weakly-supervised nature.\n\nFor a multi-modal dataset like Hateful Memes, the correlation between data size and performance is less clear than the other two tasks, as shown in Fig.  4(c ). When fine-tuned, VisE is able to perform better than other baselines, demonstrating the effectiveness of visual engagement signals.   Effect of pre-training tasks VisE is trained using cluster assignments from both comments and raw reactions. Table  3  shows the ablation study where we evaluate the contribution of different engagement formats. In general, VisE obtains the best performance when trained with a multi-task objective.\n\nEffect of pre-training visual backbone Fig.  5  presents ablation studies using different visual backbones. VisE-1.2M is better than Random Init and IN-Sup across all three backbone choices and three downstream tasks. We also note that the advantages of VisE is diminishing as the number of parameters of backbone getting larger possibly due to overfitting on the downstream training set.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Multimodal Fine-Tuning Ablation",
      "text": "We used an image encoder and a frozen textual encoder for experiments on Hateful Memes dataset in Sec 4. To isolate the effects of both module for this dataset, we compare VisE with other baselines under the following setups: Methods with VisE are able to achieve better or comparable results compared with other baselines. And visuallanguage methods outperform IN-Sup in Fig.  6 (a) except for CLIP. This shows the visual backbones that are pretrained with a textual module are better when both modules are fine-tuned together.   From Figs. 6(a)-6(c), the ROC AUC scores are getting smaller and smaller as the textual encoder makes less and less contribution. This demonstrates that the text module has dominant effect on predicting if a multi-modal meme is hateful or not. If using image encoders only, linear evaluation obtains better results than the fine-tuned protocol (Fig.  6 (c) vs. 6(d)). This also suggests textual information is more important on this dataset.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Qualitative Analysis",
      "text": "We also conduct qualitative studies using UnbiasedEmotion to further understand the benefit of visual engagement. Fig.  7  shows sample predictions produced by our VisE-250M and 3 other approaches. The predicted classes for each approach are color coded (green as correct, red as incorrect). We also use class activation mappings  [94]  to visualize the discriminate image regions for the predicted emotion. Although all methods that are initialized with pretrained models can detect the object of interest in the image, IN-Sup and IG-940M-IN are more likely to predict \"joy\" and \"love\" for the cats and dogs photos in Fig.  7 , while VisE-250M yields more diverse predictions (see row 2 left vs. row1 right as an example). This seems to suggest that IN-Sup and IG-940M-IN map dogs and cat to positive emotions. VisE-250M, on the other hand, does not rely on the ImageNet object labels during pre-training and is able to distinguish the subtle emotional differences among different images with dogs and cats. However, all methods failed to infer \"surprise\" from the bottom left image, possibly due to imbalanced training data of UnbiasedEmotion. Additional visualization can be found in the Appendix A.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "We explored social media visual engagement as supervisory signals for representation learning. We presented VisE, a streamlined pre-training method that uses pseudolabels derived from human responses to social media posts, including reactions and comments. Experiments and analysis show that visual engagement signals transfer well to various downstream tasks that go beyond conventional visual recognition. VisE is able to outperform various representation learning models on these datasets. We therefore hope that VisE could inspire and facilitate future research that focuses on the cognitive aspects of images. Pre-trained models will be released upon acceptance of the work.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "B. Supplementary Results And Discussion",
      "text": "Transfer learning on ImageNet Table  4  presents results and comparisons on ImageNet. We fine-tune VisE-250M with a ResNeXt-101 backbone on ImageNet, and compare the val accuracy scores with the same ResNeXt-101 model trained from scratch (IN-Sup). We also show the results of IG-940M-IN  [55] , which is pre-trained on 940 million images with 1.5K hashtags and fine-tuned on ImageNet using the same visual backbone. We see from Table  4 that     5  shows the linear evaluation results on UnbiasedEmotion, which shows the engagement signals, not the images, are beneficial for this dataset. We will include the full results in the final version.  6  and 7 present full transfer learning results including performance on the val split and an additional metric for the Hateful Memes dataset. These two tables can be read in conjunction with the main figure and the backbone ablation studies in the main text. Note that we use in-house baselines instead of copying results from prior work for fair-comparison purposes. All the experiments are trained using the same grid search range, validation set, learning rate schedule, etc. We use validation accuracy and ROC AUC for Hateful Memes to select the best set of hyper-parameters. See Appendix C.3 for details.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Additional Results Table",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Datasize Calculation For Contrastive Learning Methods",
      "text": "In size ablation studies, we sort all pre-training methods by the training inputs size. We consider the negative input pairs for MoCo-v2 and CLIP as the effective training data size.   follow  [50, 10]  to stabilize the training processing by initializing the the bias for the last linear classification layer with b = -log ((1 -π) /π), where the prior probability π is set to 0.01.\n\nTo obtain the pseudo-labels for the visual engagement signals, we set the number of clusters as 5000 and 128, for comments and raw reactions respectively.\n\nOther details We spend around 9 hours to mine the data for pretraining with 3 server nodes (144 cpus). For the 1.23M data, the total word count for comments is 178M, the average ± std number of comments per image is 20.25 ± 54.43 , the average ± std reactions count per image is 81.21 ± 601.4 . We use Pytorch  [63]  to implement and train all the models on NVIDIA Tesla V100 GPUs.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "C.2. Other Pre-Training Methods",
      "text": "We use the publicly available pre-trained models for other compared baseline methods  4  except for ImageNet pretraining with ResNeXt-101 backbone. We train that model with 100 epochs with learning rate decay schedule of  (30, 60, 90)  and scaling factor",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: could be described in a",
      "page": 1
    },
    {
      "caption": "Figure 1: Visual engagement vs. other common supervisory sig-",
      "page": 1
    },
    {
      "caption": "Figure 2: illustrates",
      "page": 2
    },
    {
      "caption": "Figure 2: Learning from visual engagement. (a) We learn visual",
      "page": 3
    },
    {
      "caption": "Figure 2: (a) summarizes this step.",
      "page": 3
    },
    {
      "caption": "Figure 3: presents the results for the linear evaluation and ﬁne-tuned",
      "page": 5
    },
    {
      "caption": "Figure 3: Linear evaluation (top) and ﬁne-tuned (bottom) results among VisE, uni-modal (ResNet-50), cross-modal (ResNet-50), and uni-",
      "page": 6
    },
    {
      "caption": "Figure 4: presents the results",
      "page": 6
    },
    {
      "caption": "Figure 4: (a) and 4(b), we can see that there is",
      "page": 6
    },
    {
      "caption": "Figure 4: (c). When ﬁne-tuned,",
      "page": 6
    },
    {
      "caption": "Figure 4: Data size ablation using ResNet-50 (top) and ResNeXt-101 32×16d (bottom) backbones. We only present one of ICMLMatt-fc",
      "page": 7
    },
    {
      "caption": "Figure 5: Visual backbone ablations.",
      "page": 7
    },
    {
      "caption": "Figure 3: (c)) uses concatenated features from both encoders.",
      "page": 7
    },
    {
      "caption": "Figure 6: presents the results.",
      "page": 7
    },
    {
      "caption": "Figure 6: (a) except",
      "page": 7
    },
    {
      "caption": "Figure 6: Multi-modal ablation for Hateful Memes val split. For reference, using text encoder alone give ROC AUC scores: 0.6363",
      "page": 8
    },
    {
      "caption": "Figure 7: Qualitative results on UnbiasedEmotion dataset using ResNeXt-101 32×16d.",
      "page": 8
    },
    {
      "caption": "Figure 6: (c) vs. 6(d)). This also suggests textual information",
      "page": 8
    },
    {
      "caption": "Figure 7: shows sample predictions produced by our",
      "page": 8
    },
    {
      "caption": "Figure 8: presents more predicted examples including images with",
      "page": 9
    },
    {
      "caption": "Figure 8: Qualitative results on UnbiasedEmotion dataset using",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table 2: Validation accuracy on CUB-200-2011. Features learnt lar results as VisE with less training data (118k vs. {615k,",
      "data": [
        {
          "ResNet-50\n23.8\nRandom Init\n44.36\nIN-Sup\n26.23\nMoCo-v2\n33.57\nVQAGrid\n45.74\nVisE-1.2M\n53.05\nVisE-250M": "ResNeXt-101\n42.17\nVirTex\n23.51\nICMLM-att\n31.87\nICMLM-tfm\n45.41\nCLIP\n38.43\nRandom Init\n62.59\nIN-Sup\n56.26\nIG-IN-940M\n56.26\nVisE-1.2M"
        },
        {
          "ResNet-50\n23.8\nRandom Init\n44.36\nIN-Sup\n26.23\nMoCo-v2\n33.57\nVQAGrid\n45.74\nVisE-1.2M\n53.05\nVisE-250M": "69.44\nVisE-250M"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 2: Validation accuracy on CUB-200-2011. Features learnt lar results as VisE with less training data (118k vs. {615k,",
      "data": [
        {
          "56.57\nRandom Init\n59.45\nIN-Sup\n58.3\nMoCo-v2\n57.31\nVQAGrid\n59.3\nVisE-1.2M": "ResNet-50\n60.31\nVisE-250M"
        },
        {
          "56.57\nRandom Init\n59.45\nIN-Sup\n58.3\nMoCo-v2\n57.31\nVQAGrid\n59.3\nVisE-1.2M": "ResNeXt-101\n58.44\nVirTex\n58.41\nICMLM-att\n58.86\nICMLM-tfm\n56.42\nCLIP\n56.92\nRandom Init\n59.42\nIN-Sup\n61.15\nIG-940M-IN\n59.89\nVisE-1.2M"
        },
        {
          "56.57\nRandom Init\n59.45\nIN-Sup\n58.3\nMoCo-v2\n57.31\nVQAGrid\n59.3\nVisE-1.2M": "61.01\nVisE-250M"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 2: Validation accuracy on CUB-200-2011. Features learnt lar results as VisE with less training data (118k vs. {615k,",
      "data": [
        {
          "0.5335\nRandom Init\n0.5691\nIN-Sup\n0.5947\nMoCo-v2\n0.5517\nVQAGrid\n0.61\nVisE-1.2M": "ResNet-50\n0.5784\nVisE-250M"
        },
        {
          "0.5335\nRandom Init\n0.5691\nIN-Sup\n0.5947\nMoCo-v2\n0.5517\nVQAGrid\n0.61\nVisE-1.2M": "ResNeXt-101\n0.5659\nVirTex\n0.5702\nICMLM-att\n0.5631\nICMLM-tfm\n0.6147\nCLIP\n0.5466\nRandom Init\n0.5542\nIN-Sup\n0.5482\nIG-IN-940M\n0.5621\nVisE-1.2M"
        },
        {
          "0.5335\nRandom Init\n0.5691\nIN-Sup\n0.5947\nMoCo-v2\n0.5517\nVQAGrid\n0.61\nVisE-1.2M": "0.5795\nVisE-250M"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 2: Validation accuracy on CUB-200-2011. Features learnt lar results as VisE with less training data (118k vs. {615k,",
      "data": [
        {
          "ResNet-50\n37.25\nRandom Init\n67.94\nIN-Sup\n76.23\nMoCo-v2\n43.93\nVQAGrid\n74.2\nVisE-1.2M\n78.89\nVisE-250M": "ResNeXt-101\n73.61\nVirTex\n70.98\nICMLM-att\n71.48\nICMLM-tfm\n74.46\nCLIP\n38.59\nRandom Init\n77.92\nIN-Sup\n81.52\nIG-IN-940M\n78.33\nVisE-1.2M"
        },
        {
          "ResNet-50\n37.25\nRandom Init\n67.94\nIN-Sup\n76.23\nMoCo-v2\n43.93\nVQAGrid\n74.2\nVisE-1.2M\n78.89\nVisE-250M": "85.21\nVisE-250M"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 2: Validation accuracy on CUB-200-2011. Features learnt lar results as VisE with less training data (118k vs. {615k,",
      "data": [
        {
          "58.31\nRandom Init\n63.64\nIN-Sup\n66.37\nMoCo-v2\n58.1\nVQAGrid\n64.3\nVisE-1.2M": "ResNet-50\n65.79\nVisE-250M"
        },
        {
          "58.31\nRandom Init\n63.64\nIN-Sup\n66.37\nMoCo-v2\n58.1\nVQAGrid\n64.3\nVisE-1.2M": "ResNeXt-101\n63.06\nVirTex\n63.2\nICMLM-att\n63.21\nICMLM-tfm\n58.07\nCLIP\n58.39\nRandom Init\n64.25\nIN-Sup\n65.58\nIG-IN-940M\n64.44\nVisE-1.2M"
        },
        {
          "58.31\nRandom Init\n63.64\nIN-Sup\n66.37\nMoCo-v2\n58.1\nVQAGrid\n64.3\nVisE-1.2M": "67.64\nVisE-250M"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 2: Validation accuracy on CUB-200-2011. Features learnt lar results as VisE with less training data (118k vs. {615k,",
      "data": [
        {
          "0.5833\nRandom Init\n0.6005\nIN-Sup\n0.5884\nMoCo-v2\n0.5906\nVQAGrid\n0.6044\nVisE-1.2M": "ResNet-50\n0.606\nVisE-250M"
        },
        {
          "0.5833\nRandom Init\n0.6005\nIN-Sup\n0.5884\nMoCo-v2\n0.5906\nVQAGrid\n0.6044\nVisE-1.2M": "ResNeXt-101\n0.5898\nVirTex\n0.5846\nICMLM-att\n0.5842\nICMLM-tfm\n0.547\nCLIP\n0.5959\nRandom Init\n0.5903\nIN-Sup\n0.5951\nIG-IN-940M\n0.5976\nVisE-1.2M"
        },
        {
          "0.5833\nRandom Init\n0.6005\nIN-Sup\n0.5884\nMoCo-v2\n0.5906\nVQAGrid\n0.6044\nVisE-1.2M": "0.5957\nVisE-250M"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.5844\nRandom Init\n0.607\nIN-Sup\n0.6117\nMoCo-v2\n0.6012\nVQAGrid\n0.6033\nVisE-1.2M": "ResNet-50\n0.6301\nVisE-250M"
        },
        {
          "0.5844\nRandom Init\n0.607\nIN-Sup\n0.6117\nMoCo-v2\n0.6012\nVQAGrid\n0.6033\nVisE-1.2M": "ResNeXt-101\n0.6027\nVirTex\n0.6228\nICMLM-att\n0.6172\nICMLM-tfm\n0.5806\nCLIP\n0.5633\nIG-940M-IN\n0.629\nVisE-250M*"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.6115\nRandom Init\n0.6588\nIN-Sup\n0.658\nMoCo-v2\n0.6305\nVQAGrid\n0.6613\nVisE-1.2M": "ResNet-50\n0.6669\nVisE-250M"
        },
        {
          "0.6115\nRandom Init\n0.6588\nIN-Sup\n0.658\nMoCo-v2\n0.6305\nVQAGrid\n0.6613\nVisE-1.2M": "0.6717\nVirTex\n0.6692\nICMLM-att\n0.6634\nICMLM-tfm\n0.615\nCLIP"
        },
        {
          "0.6115\nRandom Init\n0.6588\nIN-Sup\n0.658\nMoCo-v2\n0.6305\nVQAGrid\n0.6613\nVisE-1.2M": "ResNeXt-101\n0.664\nIG-940M-IN"
        },
        {
          "0.6115\nRandom Init\n0.6588\nIN-Sup\n0.658\nMoCo-v2\n0.6305\nVQAGrid\n0.6613\nVisE-1.2M": "0.6659\nVisE-250M*"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "International affective picture system: Technical manual and affective ratings. NIMH Center for the Study of Emotion and Attention",
      "authors": [
        "Bradley",
        "Cuthbert",
        "Lang"
      ],
      "year": "2005",
      "venue": "International affective picture system: Technical manual and affective ratings. NIMH Center for the Study of Emotion and Attention"
    },
    {
      "citation_id": "2",
      "title": "Learning visual representations with caption annotations",
      "authors": [
        "Mert Bulent Sariyildiz",
        "Julien Perez",
        "Diane Larlus"
      ],
      "year": "2004",
      "venue": "ECCV"
    },
    {
      "citation_id": "3",
      "title": "Deep clustering for unsupervised learning of visual features",
      "authors": [
        "Mathilde Caron",
        "Piotr Bojanowski",
        "Armand Joulin",
        "Matthijs Douze"
      ],
      "year": "2004",
      "venue": "ECCV"
    },
    {
      "citation_id": "4",
      "title": "Unsupervised learning of visual features by contrasting cluster assignments",
      "authors": [
        "Mathilde Caron",
        "Ishan Misra",
        "Julien Mairal",
        "Priya Goyal",
        "Piotr Bojanowski",
        "Armand Joulin"
      ],
      "year": "2020",
      "venue": "Unsupervised learning of visual features by contrasting cluster assignments"
    },
    {
      "citation_id": "5",
      "title": "Learning the best pooling strategy for visual semantic embedding",
      "authors": [
        "Jiacheng Chen",
        "Hexiang Hu",
        "Hao Wu",
        "Yuning Jiang",
        "Changhu Wang"
      ],
      "year": "2020",
      "venue": "Learning the best pooling strategy for visual semantic embedding",
      "arxiv": "arXiv:2011.04305"
    },
    {
      "citation_id": "6",
      "title": "A simple framework for contrastive learning of visual representations",
      "authors": [
        "Ting Chen",
        "Simon Kornblith",
        "Mohammad Norouzi",
        "Geoffrey Hinton"
      ],
      "year": "2020",
      "venue": "ICML"
    },
    {
      "citation_id": "7",
      "title": "Improved baselines with momentum contrastive learning",
      "authors": [
        "Xinlei Chen",
        "Haoqi Fan",
        "Ross Girshick",
        "Kaiming He"
      ],
      "year": "2020",
      "venue": "Improved baselines with momentum contrastive learning",
      "arxiv": "arXiv:2003.04297"
    },
    {
      "citation_id": "8",
      "title": "Microsoft COCO captions: Data collection and evaluation server",
      "authors": [
        "Xinlei Chen",
        "Hao Fang",
        "Tsung-Yi Lin",
        "Ramakrishna Vedantam",
        "Saurabh Gupta",
        "Piotr Dollár",
        "C Lawrence"
      ],
      "year": "2015",
      "venue": "Microsoft COCO captions: Data collection and evaluation server",
      "arxiv": "arXiv:1504.00325"
    },
    {
      "citation_id": "9",
      "title": "Uniter: Universal image-text representation learning",
      "authors": [
        "Yen-Chun Chen",
        "Linjie Li",
        "Licheng Yu",
        "Ahmed Kholy",
        "Faisal Ahmed",
        "Zhe Gan",
        "Yu Cheng",
        "Jingjing Liu"
      ],
      "year": "2020",
      "venue": "ECCV"
    },
    {
      "citation_id": "10",
      "title": "Class-balanced loss based on effective number of samples",
      "authors": [
        "Yin Cui",
        "Menglin Jia",
        "Tsung-Yi Lin",
        "Yang Song",
        "Serge Belongie"
      ],
      "year": "2019",
      "venue": "CVPR"
    },
    {
      "citation_id": "11",
      "title": "Imagenet: A large-scale hierarchical image database",
      "authors": [
        "Jia Deng",
        "Wei Dong",
        "Richard Socher",
        "Li-Jia Li",
        "Kai Li",
        "Li Fei-Fei"
      ],
      "year": "2004",
      "venue": "CVPR"
    },
    {
      "citation_id": "12",
      "title": "VirTex: Learning Visual Representations from Textual Annotations",
      "authors": [
        "Karan Desai",
        "Justin Johnson"
      ],
      "year": "2021",
      "venue": "CVPR"
    },
    {
      "citation_id": "13",
      "title": "Decaf: A deep convolutional activation feature for generic visual recognition",
      "authors": [
        "Jeff Donahue",
        "Yangqing Jia",
        "Oriol Vinyals",
        "Judy Hoffman",
        "Ning Zhang",
        "Eric Tzeng",
        "Trevor Darrell"
      ],
      "year": "2014",
      "venue": "ICML"
    },
    {
      "citation_id": "14",
      "title": "Large scale adversarial representation learning",
      "authors": [
        "Jeff Donahue",
        "Karen Simonyan"
      ],
      "year": "2019",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "15",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer",
        "Alexander Kolesnikov",
        "Dirk Weissenborn",
        "Xiaohua Zhai",
        "Thomas Unterthiner",
        "Mostafa Dehghani",
        "Matthias Minderer",
        "Georg Heigold",
        "Sylvain Gelly",
        "Jakob Uszkoreit",
        "Neil Houlsby"
      ],
      "year": "2021",
      "venue": "Proceedings of International Conference on Learning Representations"
    },
    {
      "citation_id": "16",
      "title": "Devise: A deep visual-semantic embedding model",
      "authors": [
        "Andrea Frome",
        "Greg Corrado",
        "Jonathon Shlens",
        "Samy Bengio",
        "Jeffrey Dean",
        "Marc' Aurelio Ranzato",
        "Tomas Mikolov"
      ],
      "year": "2013",
      "venue": "Devise: A deep visual-semantic embedding model"
    },
    {
      "citation_id": "17",
      "title": "Unsupervised representation learning by predicting image rotations",
      "authors": [
        "Spyros Gidaris",
        "Praveer Singh",
        "Nikos Komodakis"
      ],
      "year": "2018",
      "venue": "ICLR"
    },
    {
      "citation_id": "18",
      "title": "Rich feature hierarchies for accurate object detection and semantic segmentation",
      "authors": [
        "Ross Girshick",
        "Jeff Donahue",
        "Trevor Darrell",
        "Jitendra Malik"
      ],
      "year": "2014",
      "venue": "CVPR"
    },
    {
      "citation_id": "19",
      "title": "Self-supervised learning of visual features through embedding images into text topic spaces",
      "authors": [
        "Lluis Gomez",
        "Yash Patel",
        "Dimosthenis Marc ¸al Rusiñol",
        "C Karatzas",
        "Jawahar"
      ],
      "year": "2017",
      "venue": "CVPR"
    },
    {
      "citation_id": "20",
      "title": "Exploring hate speech detection in multimodal publications",
      "authors": [
        "Raul Gomez",
        "Jaume Gibert",
        "Lluis Gomez",
        "Dimosthenis Karatzas"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "21",
      "title": "Scaling and benchmarking self-supervised visual representation learning",
      "authors": [
        "Priya Goyal",
        "Dhruv Mahajan",
        "Abhinav Gupta",
        "Ishan Misra"
      ],
      "year": "2019",
      "venue": "ICCV"
    },
    {
      "citation_id": "22",
      "title": "Bootstrap your own latent -a new approach to self-supervised learning",
      "authors": [
        "Jean-Bastien Grill",
        "Florian Strub",
        "Florent Altché",
        "Corentin Tallec",
        "Pierre Richemond",
        "Elena Buchatskaya",
        "Carl Doersch",
        "Bernardo Avila Pires",
        "Zhaohan Guo",
        "Mohammad Azar",
        "Bilal Piot",
        "Remi Munos",
        "Michal Valko"
      ],
      "year": "2020",
      "venue": "Bootstrap your own latent -a new approach to self-supervised learning"
    },
    {
      "citation_id": "23",
      "title": "Lvis: A dataset for large vocabulary instance segmentation",
      "authors": [
        "Agrim Gupta",
        "Piotr Dollar",
        "Ross Girshick"
      ],
      "year": "2019",
      "venue": "CVPR"
    },
    {
      "citation_id": "24",
      "title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models",
      "authors": [
        "Michael Gutmann",
        "Aapo Hyvärinen"
      ],
      "year": "2010",
      "venue": "AISTATS"
    },
    {
      "citation_id": "25",
      "title": "Dimensionality reduction by learning an invariant mapping",
      "authors": [
        "Raia Hadsell",
        "Sumit Chopra",
        "Yann Lecun"
      ],
      "year": "2006",
      "venue": "CVPR"
    },
    {
      "citation_id": "26",
      "title": "Momentum contrast for unsupervised visual representation learning",
      "authors": [
        "Kaiming He",
        "Haoqi Fan",
        "Yuxin Wu",
        "Saining Xie",
        "Ross Girshick"
      ],
      "year": "2020",
      "venue": "CVPR"
    },
    {
      "citation_id": "27",
      "title": "Momentum contrast for unsupervised visual representation learning",
      "authors": [
        "Kaiming He",
        "Haoqi Fan",
        "Yuxin Wu",
        "Saining Xie",
        "Ross Girshick"
      ],
      "year": "2020",
      "venue": "CVPR"
    },
    {
      "citation_id": "28",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "CVPR"
    },
    {
      "citation_id": "29",
      "title": "Prediction of cyberbullying incidents in a media-based social network",
      "authors": [
        "Homa Hosseinmardi",
        "Sabrina Arredondo Mattson",
        "Rahat Ibn Rafiq",
        "Richard Han",
        "Qin Lv",
        "Shivakant Mishra"
      ],
      "year": "2016",
      "venue": "2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)"
    },
    {
      "citation_id": "30",
      "title": "Inferring visual persuasion via body language, setting, and deep features",
      "authors": [
        "Xinyue Huang",
        "Adriana Kovashka"
      ],
      "year": "2016",
      "venue": "CVPRW"
    },
    {
      "citation_id": "31",
      "title": "Automatic understanding of image and video advertisements",
      "authors": [
        "Zaeem Hussain",
        "Mingda Zhang",
        "Xiaozhong Zhang",
        "Keren Ye",
        "Christopher Thomas",
        "Zuha Agha",
        "Nathan Ong",
        "Adriana Kovashka"
      ],
      "year": "2017",
      "venue": "CVPR"
    },
    {
      "citation_id": "32",
      "title": "Scaling up visual and vision-language representation learning with noisy text supervision",
      "authors": [
        "Chao Jia",
        "Yinfei Yang",
        "Ye Xia",
        "Yi-Ting Chen",
        "Zarana Parekh",
        "Hieu Pham",
        "V Quoc",
        "Yunhsuan Le",
        "Zhen Sung",
        "Tom Li",
        "Duerig"
      ],
      "year": "2021",
      "venue": "Scaling up visual and vision-language representation learning with noisy text supervision",
      "arxiv": "arXiv:2102.05918"
    },
    {
      "citation_id": "33",
      "title": "Fashionpedia: Ontology, segmentation, and an attribute localization dataset",
      "authors": [
        "Menglin Jia",
        "Mengyun Shi",
        "Mikhail Sirotenko",
        "Yin Cui",
        "Claire Cardie",
        "Bharath Hariharan",
        "Hartwig Adam",
        "Serge Belongie"
      ],
      "year": "2020",
      "venue": "ECCV"
    },
    {
      "citation_id": "34",
      "title": "Intentonomy: a dataset and study towards human intent understanding",
      "authors": [
        "Menglin Jia",
        "Zuxuan Wu",
        "Austin Reiter",
        "Claire Cardie",
        "Serge Belongie",
        "Ser-Nam Lim"
      ],
      "year": "2021",
      "venue": "CVPR"
    },
    {
      "citation_id": "35",
      "title": "In defense of grid features for visual question answering",
      "authors": [
        "Huaizu Jiang",
        "Ishan Misra",
        "Marcus Rohrbach",
        "Erik Learned-Miller",
        "Xinlei Chen"
      ],
      "year": "2020",
      "venue": "CVPR"
    },
    {
      "citation_id": "36",
      "title": "Visual persuasion: Inferring communicative intents of images",
      "authors": [
        "Jungseock Joo",
        "Weixin Li",
        "Francis Steen",
        "Song-Chun Zhu"
      ],
      "year": "2014",
      "venue": "CVPR"
    },
    {
      "citation_id": "37",
      "title": "Automated facial trait judgment and election outcome prediction: Social dimensions of face",
      "authors": [
        "Jungseock Joo",
        "Francis Steen",
        "Song-Chun Zhu"
      ],
      "year": "2015",
      "venue": "ICCV"
    },
    {
      "citation_id": "38",
      "title": "Learning visual features from large weakly supervised data",
      "authors": [
        "Armand Joulin",
        "Laurens Van Der Maaten",
        "Allan Jabri",
        "Nicolas Vasilache"
      ],
      "year": "2015",
      "venue": "ECCV"
    },
    {
      "citation_id": "39",
      "title": "Deep fragment embeddings for bidirectional image sentence mapping",
      "authors": [
        "Andrej Karpathy",
        "Armand Joulin",
        "Fei-Fei Li"
      ],
      "year": "2014",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "40",
      "title": "The hateful memes challenge: Detecting hate speech in multimodal memes",
      "authors": [
        "Douwe Kiela",
        "Hamed Firooz",
        "Aravind Mohan",
        "Vedanuj Goswami",
        "Amanpreet Singh",
        "Pratik Ringshia",
        "Davide Testuggine"
      ],
      "year": "2020",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "41",
      "title": "Large scale learning of general visual representations for transfer",
      "authors": [
        "Alexander Kolesnikov",
        "Lucas Beyer",
        "Xiaohua Zhai",
        "Joan Puigcerver",
        "Jessica Yung",
        "Sylvain Gelly",
        "Neil Houlsby"
      ],
      "year": "2019",
      "venue": "Large scale learning of general visual representations for transfer",
      "arxiv": "arXiv:1912.11370"
    },
    {
      "citation_id": "42",
      "title": "Big transfer (bit): General visual representation learning",
      "authors": [
        "Alexander Kolesnikov",
        "Lucas Beyer",
        "Xiaohua Zhai",
        "Joan Puigcerver",
        "Jessica Yung",
        "Sylvain Gelly",
        "Neil Houlsby"
      ],
      "year": "2020",
      "venue": "ECCV"
    },
    {
      "citation_id": "43",
      "title": "Emotion recognition in context",
      "authors": [
        "Ronak Kosti",
        "Jose Alvarez",
        "Adria Recasens",
        "Agata Lapedriza"
      ],
      "year": "2017",
      "venue": "CVPR"
    },
    {
      "citation_id": "44",
      "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
      "authors": [
        "Ranjay Krishna",
        "Yuke Zhu",
        "Oliver Groth",
        "Justin Johnson",
        "Kenji Hata",
        "Joshua Kravitz",
        "Stephanie Chen",
        "Yannis Kalantidis",
        "Li-Jia Li",
        "David Shamma",
        "Michael Bernstein",
        "Li Fei-Fei"
      ],
      "year": "2017",
      "venue": "IJCV"
    },
    {
      "citation_id": "45",
      "title": "Integrating text and image: Determining multimodal document intent in instagram posts",
      "authors": [
        "Julia Kruk",
        "Jonah Lubin",
        "Karan Sikka",
        "Xiao Lin",
        "Dan Jurafsky",
        "Ajay Divakaran"
      ],
      "year": "2019",
      "venue": "EMNLP"
    },
    {
      "citation_id": "46",
      "title": "Learning visual n-grams from web data",
      "authors": [
        "Ang Li",
        "Allan Jabri",
        "Armand Joulin",
        "Laurens Van Der Maaten"
      ],
      "year": "2017",
      "venue": "ICCV"
    },
    {
      "citation_id": "47",
      "title": "Mopro: Webly supervised learning with momentum prototypes",
      "authors": [
        "Junnan Li",
        "Caiming Xiong",
        "Steven Hoi"
      ],
      "year": "2021",
      "venue": "ICLR"
    },
    {
      "citation_id": "48",
      "title": "Prototypical contrastive learning of unsupervised representations",
      "authors": [
        "Junnan Li",
        "Pan Zhou",
        "Caiming Xiong",
        "Steven Hoi"
      ],
      "year": "2021",
      "venue": "ICLR"
    },
    {
      "citation_id": "49",
      "title": "Visual semantic reasoning for image-text matching",
      "authors": [
        "Kunpeng Li",
        "Yulun Zhang",
        "Kai Li",
        "Yuanyuan Li",
        "Yun Fu"
      ],
      "year": "2019",
      "venue": "ICCV"
    },
    {
      "citation_id": "50",
      "title": "Focal loss for dense object detection",
      "authors": [
        "Tsung-Yi Lin",
        "Priya Goyal",
        "Ross Girshick",
        "Kaiming He",
        "Piotr Dollár"
      ],
      "year": "2018",
      "venue": "CVPR"
    },
    {
      "citation_id": "51",
      "title": "Microsoft coco: Common objects in context",
      "authors": [
        "Tsung-Yi Lin",
        "Michael Maire",
        "Serge Belongie",
        "James Hays",
        "Pietro Perona",
        "Deva Ramanan",
        "Piotr Dollár",
        "C Lawrence"
      ],
      "year": "2014",
      "venue": "ECCV"
    },
    {
      "citation_id": "52",
      "title": "A robustly optimized bert pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov",
        "Roberta"
      ],
      "year": "2019",
      "venue": "A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "53",
      "title": "Decoupled weight decay regularization",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2019",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "54",
      "title": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
      "authors": [
        "Jiasen Lu",
        "Dhruv Batra",
        "Devi Parikh",
        "Stefan Lee"
      ],
      "year": "2019",
      "venue": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks"
    },
    {
      "citation_id": "55",
      "title": "Exploring the limits of weakly supervised pretraining",
      "authors": [
        "Dhruv Mahajan",
        "Ross Girshick",
        "Vignesh Ramanathan",
        "Kaiming He",
        "Manohar Paluri",
        "Yixuan Li",
        "Ashwin Bharambe",
        "Laurens Van Der Maaten"
      ],
      "year": "2009",
      "venue": "ECCV"
    },
    {
      "citation_id": "56",
      "title": "Wordnet: a lexical database for english",
      "authors": [
        "George Miller"
      ],
      "year": "1995",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "57",
      "title": "Self-supervised learning of pretext-invariant representations",
      "authors": [
        "Ishan Misra",
        "Laurens Van Der Maaten"
      ],
      "year": "2020",
      "venue": "CVPR"
    },
    {
      "citation_id": "58",
      "title": "Self-supervised learning of pretext-invariant representations",
      "authors": [
        "Ishan Misra",
        "Laurens Van Der Maaten"
      ],
      "year": "2020",
      "venue": "CVPR"
    },
    {
      "citation_id": "59",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "Ali Mollahosseini",
        "Behzad Hasani",
        "Mohammad Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "60",
      "title": "Unsupervised learning of visual representations by solving jigsaw puzzles",
      "authors": [
        "Mehdi Noroozi",
        "Paolo Favaro"
      ],
      "year": "2016",
      "venue": "ECCV"
    },
    {
      "citation_id": "61",
      "title": "fairseq: A fast, extensible toolkit for sequence modeling",
      "authors": [
        "Myle Ott",
        "Sergey Edunov",
        "Alexei Baevski",
        "Angela Fan",
        "Sam Gross",
        "Nathan Ng",
        "David Grangier",
        "Michael Auli"
      ],
      "year": "2019",
      "venue": "Proceedings of NAACL-HLT"
    },
    {
      "citation_id": "62",
      "title": "Contemplating visual emotions: Understanding and overcoming dataset bias",
      "authors": [
        "Rameswar Panda",
        "Jianming Zhang",
        "Haoxiang Li",
        "Joon-Young Lee",
        "Xin Lu"
      ],
      "year": "2018",
      "venue": "ECCV"
    },
    {
      "citation_id": "63",
      "title": "Automatic differentiation in PyTorch",
      "authors": [
        "A Paszke",
        "S Gross",
        "S Chintala",
        "G Chanan",
        "E Yang",
        "Z De-Vito",
        "Z Lin",
        "A Desmaison",
        "L Antiga",
        "A Lerer"
      ],
      "year": "2017",
      "venue": "NeurIPS Autodiff Workshop"
    },
    {
      "citation_id": "64",
      "title": "Context encoders: Feature learning by inpainting",
      "authors": [
        "Deepak Pathak",
        "Philipp Krahenbuhl",
        "Jeff Donahue",
        "Trevor Darrell",
        "Alexei Efros"
      ],
      "year": "2016",
      "venue": "CVPR"
    },
    {
      "citation_id": "65",
      "title": "A comprehensive grammar of the English language",
      "authors": [
        "Randolph Quirk"
      ],
      "year": "2010",
      "venue": "A comprehensive grammar of the English language"
    },
    {
      "citation_id": "66",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark"
      ],
      "year": "2021",
      "venue": "Learning transferable visual models from natural language supervision",
      "arxiv": "arXiv:2103.00020"
    },
    {
      "citation_id": "67",
      "title": "Towards real-time object detection with region proposal networks",
      "authors": [
        "Kaiming Shaoqing Ren",
        "Ross He",
        "Jian Girshick",
        "Sun",
        "R-Cnn Faster"
      ],
      "year": "2015",
      "venue": "Towards real-time object detection with region proposal networks"
    },
    {
      "citation_id": "68",
      "title": "Introduction to Modern Information Retrieval",
      "authors": [
        "Gerard Salton",
        "Michael Mcgill"
      ],
      "year": "1986",
      "venue": "Introduction to Modern Information Retrieval"
    },
    {
      "citation_id": "69",
      "title": "Learning visual representations with caption annotations",
      "authors": [
        "Mert Bulent Sariyildiz",
        "Julien Perez",
        "Diane Larlus"
      ],
      "year": "2020",
      "venue": "Learning visual representations with caption annotations",
      "arxiv": "arXiv:2008.01392"
    },
    {
      "citation_id": "70",
      "title": "Exploiting multimodal affect and semantics to identify politically persuasive web videos",
      "authors": [
        "Behjat Siddiquie",
        "Dave Chisholm",
        "Ajay Divakaran"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 ACM on International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "71",
      "title": "Toward multimodal cyberbullying detection",
      "authors": [
        "K Vivek",
        "Souvick Singh",
        "Christin Ghosh",
        "Jose"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems"
    },
    {
      "citation_id": "72",
      "title": "Grounded compositional semantics for finding and describing images with sentences",
      "authors": [
        "Richard Socher",
        "Andrej Karpathy",
        "Quoc Le",
        "Christopher Manning",
        "Andrew Ng"
      ],
      "year": "2014",
      "venue": "Grounded compositional semantics for finding and describing images with sentences"
    },
    {
      "citation_id": "73",
      "title": "Revisiting unreasonable effectiveness of data in deep learning era",
      "authors": [
        "Chen Sun",
        "Abhinav Shrivastava",
        "Saurabh Sigh",
        "Abhinav Gupta"
      ],
      "year": "2017",
      "venue": "ICCV"
    },
    {
      "citation_id": "74",
      "title": "Predicting the politics of an image using webly supervised data",
      "authors": [
        "Christopher Thomas",
        "Adriana Kovashka"
      ],
      "year": "2019",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "75",
      "title": "Predicting the politics of an image using webly supervised data",
      "authors": [
        "Christopher Thomas",
        "Adriana Kovashka"
      ],
      "year": "2019",
      "venue": "Predicting the politics of an image using webly supervised data"
    },
    {
      "citation_id": "76",
      "title": "YFCC100M: The new data in multimedia research",
      "authors": [
        "Bart Thomee",
        "David Shamma",
        "Gerald Friedland",
        "Benjamin Elizalde",
        "Karl Ni",
        "Douglas Poland",
        "Damian Borth",
        "Li-Jia Li"
      ],
      "year": "2016",
      "venue": "YFCC100M: The new data in multimedia research"
    },
    {
      "citation_id": "77",
      "title": "Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection",
      "authors": [
        "Grant Van Horn",
        "Steve Branson",
        "Ryan Farrell",
        "Scott Haber",
        "Jessie Barry"
      ],
      "year": "2015",
      "venue": "CVPR"
    },
    {
      "citation_id": "78",
      "title": "The inaturalist species classification and detection dataset",
      "authors": [
        "Grant Van Horn",
        "Oisin Mac Aodha",
        "Yang Song",
        "Yin Cui",
        "Chen Sun",
        "Alex Shepard",
        "Hartwig Adam",
        "Pietro Perona",
        "Serge Belongie"
      ],
      "year": "2018",
      "venue": "CVPR"
    },
    {
      "citation_id": "79",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Attention is all you need"
    },
    {
      "citation_id": "80",
      "title": "The caltech-ucsd birds-200-2011 dataset",
      "authors": [
        "Catherine Wah",
        "Steve Branson",
        "Peter Welinder",
        "Pietro Perona",
        "Serge Belongie"
      ],
      "year": "2011",
      "venue": "The caltech-ucsd birds-200-2011 dataset"
    },
    {
      "citation_id": "81",
      "title": "Learning visual emotion representations from web data",
      "authors": [
        "Zijun Wei",
        "Jianming Zhang",
        "Zhe Lin",
        "Joon-Young Lee",
        "Niranjan Balasubramanian",
        "Minh Hoai",
        "Dimitris Samaras"
      ],
      "year": "2002",
      "venue": "CVPR"
    },
    {
      "citation_id": "82",
      "title": "Learning subjective language",
      "authors": [
        "Janyce Wiebe",
        "Theresa Wilson",
        "Rebecca Bruce",
        "Matthew Bell",
        "Melanie Martin"
      ],
      "year": "2004",
      "venue": "Computational linguistics"
    },
    {
      "citation_id": "83",
      "title": "Unsupervised feature learning via non-parametric instance-level discrimination",
      "authors": [
        "Zhirong Wu",
        "Yuanjun Xiong",
        "Stella Yu",
        "Dahua Lin"
      ],
      "year": "2018",
      "venue": "CVPR"
    },
    {
      "citation_id": "84",
      "title": "Self-training with noisy student improves imagenet classification",
      "authors": [
        "Qizhe Xie",
        "Minh-Thang Luong",
        "Eduard Hovy",
        "V Quoc",
        "Le"
      ],
      "year": "2020",
      "venue": "CVPR"
    },
    {
      "citation_id": "85",
      "title": "Aggregated residual transformations for deep neural networks",
      "authors": [
        "Saining Xie",
        "Ross Girshick",
        "Piotr Dollár",
        "Zhuowen Tu",
        "Kaiming He"
      ],
      "year": "2017",
      "venue": "CVPR"
    },
    {
      "citation_id": "86",
      "title": "Billion-scale semi-supervised learning for image classification",
      "authors": [
        "Hervé Zeki Yalniz",
        "Kan Jégou",
        "Manohar Chen",
        "Dhruv Paluri",
        "Mahajan"
      ],
      "year": "2019",
      "venue": "Billion-scale semi-supervised learning for image classification",
      "arxiv": "arXiv:1905.00546"
    },
    {
      "citation_id": "87",
      "title": "Clusterfit: Improving generalization of visual representations",
      "authors": [
        "Xueting Yan",
        "Ishan Misra",
        "Abhinav Gupta",
        "Deepti Ghadiyaram",
        "Dhruv Mahajan"
      ],
      "year": "2020",
      "venue": "CVPR"
    },
    {
      "citation_id": "88",
      "title": "Interpreting the rhetoric of visual advertisements",
      "authors": [
        "Keren Ye",
        "Narges Honarvar Nazari",
        "James Hahn",
        "Zaeem Hussain",
        "Mingda Zhang",
        "Adriana Kovashka"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "89",
      "title": "Unsupervised embedding learning via invariant and spreading instance feature",
      "authors": [
        "Mang Ye",
        "Xu Zhang",
        "Pong Yuen",
        "Shih-Fu Chang"
      ],
      "year": "2019",
      "venue": "CVPR"
    },
    {
      "citation_id": "90",
      "title": "Barlow twins: Self-supervised learning via redundancy reduction",
      "authors": [
        "Jure Zbontar",
        "Li Jing",
        "Ishan Misra",
        "Yann Lecun",
        "Stephane Deny"
      ],
      "year": "2021",
      "venue": "CVPR"
    },
    {
      "citation_id": "91",
      "title": "Colorful image colorization",
      "authors": [
        "Richard Zhang",
        "Phillip Isola",
        "Alexei Efros"
      ],
      "year": "2016",
      "venue": "ECCV"
    },
    {
      "citation_id": "92",
      "title": "Split-brain autoencoders: Unsupervised learning by cross-channel prediction",
      "authors": [
        "Richard Zhang",
        "Phillip Isola",
        "Alexei Efros"
      ],
      "year": "2017",
      "venue": "CVPR"
    },
    {
      "citation_id": "93",
      "title": "Contrastive learning of medical visual representations from paired images and text",
      "authors": [
        "Yuhao Zhang",
        "Hang Jiang",
        "Yasuhide Miura",
        "Christopher Manning",
        "Curtis Langlotz"
      ],
      "year": "2020",
      "venue": "Contrastive learning of medical visual representations from paired images and text",
      "arxiv": "arXiv:2010.00747"
    },
    {
      "citation_id": "94",
      "title": "Learning deep features for discriminative localization",
      "authors": [
        "Bolei Zhou",
        "Aditya Khosla",
        "Agata Lapedriza",
        "Aude Oliva",
        "Antonio Torralba"
      ],
      "year": "2016",
      "venue": "CVPR"
    },
    {
      "citation_id": "95",
      "title": "Places: A 10 million image database for scene recognition",
      "authors": [
        "Bolei Zhou",
        "Agata Lapedriza",
        "Aditya Khosla",
        "Aude Oliva",
        "Antonio Torralba"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "96",
      "title": "Local aggregation for unsupervised learning of visual embeddings",
      "authors": [
        "Chengxu Zhuang",
        "Alex Zhai",
        "Daniel Yamins"
      ],
      "year": "2019",
      "venue": "ICCV"
    }
  ]
}