{
  "paper_id": "2303.02994v1",
  "title": "Fighting Noise And Imbalance In Action Unit Detection Problems",
  "published": "2023-03-06T09:41:40Z",
  "authors": [
    "Gauthier Tallec",
    "Arnaud Dapogny",
    "Kevin Bailly"
  ],
  "keywords": [
    "Computer Vision",
    "Affective Computing",
    "Action Unit Detection"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Action Unit (AU) detection aims at automatically caracterizing facial expressions with the muscular activations they involve. Its main interest is to provide a low-level face representation that can be used to assist higher level affective computing tasks learning. Yet, it is a challenging task. Indeed, the available databases display limited face variability and are imbalanced toward neutral expressions. Furthermore, as AU involve subtle face movements they are difficult to annotate so that some of the few provided datapoints may be misslabeled. In this work, we aim at exploiting label smoothing ability to mitigate noisy examples impact by reducing confidence [1]. However, applying label smoothing as it is may aggravate imbalance-based pre-existing under-confidence issue and degrade performance. To circumvent this issue, we propose Robin Hood Label Smoothing (RHLS). RHLS principle is to restrain label smoothing confidence reduction to the majority class. In that extent, it alleviates both the imbalancebased over-confidence issue and the negative impact of noisy majority class examples. From an experimental standpoint, we show that RHLS provides a free performance improvement in AU detection. In particular, by applying it on top of a modern multi-task baseline we get promising results on BP4D and outperform state-of-the-art methods on DISFA.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Facial expressions convey information galore about how humans feel. Consequently, efficient computational face representation could unlock better automatic comprehension of human behaviours and in turn improve human-machine interaction. For that purpose, the Facial Action Coding System (FACS) provides an anatomic representation that decompose faces using muscular activations called Action Units (AU).\n\nFrom a machine learning point of view, AU detection can be formulated as a multi-task problem where each task consists in the detection of a single AU. In practice, its performance are hindered by data scarcity. Indeed, the AU labeling process consists in frame-by-frame video annotations of subtle facial activations and is therefore hardly scalable. As a result existing AU datasets display low face variability with only few positively annotated examples (because AU are short events). Finally, as shown in Figure  1 , AU are often so subtle that even expertly trained annotators may miss-annotate edgy examples  [2] , resulting in label noise. Altogether, training in such setting is prone to overfitting and model over-confidence toward predicting neutral expressions.\n\nTo tackle low face variability, the main line of research make use of prior geometric information (typically facial keypoints) to help the learning process by either guiding the network attention  [3, 4, 5, 6]  or normalizing face geometry  [7] . In the same vein , several works  [8, 9, 10]  attempted to incorporate prior AU dependencies to better structure predictions. For imbalance problems, the widely adopted approach is loss frequency reweighting  [11, 12, 4, 6] .\n\nInterestingly, very few methods address the label noise problem. The work in  [13]  is among the only that takes AU uncertainty into account by using the method in  [14]  to learn to adapt the contribution of each AU to the total loss. This work lies in the continuity of  [13]  since it focuses on AU noise modelling. Yet, we stand aside from it arguing that uncertainty learning intuitively requires large amount of data  [15]  and may therefore not be fully efficient in AU detection low data regime.\n\nInstead, we aim at taking advantage from the recent success of label smoothing  [16]  at mitigating noise  [1]  by reducing over-confidence. However, vanilla label smoothing reduces over-confidence uniformly in all classes. Therefore, applying it in imbalanced situations may worsen the pre-existing under-confidence in the minority class. For that purpose, we propose Robin Hood Label Smoothing (RHLS) that takes its name from the fact that, by smoothing only the majority class, it introduces a probability to take examples from the rich class to give them to the poor. By doing so, it reduces both the imbalance-based over-confidence issue and the negative impact of noisy majority class examples. To summarize our contributions are as follows :\n\n• We introduce RHLS that adapts label smoothing to imbalanced situations by restraining over-confidence reduction to the majority class. Consequently it mitigates both imbalance over-confidence issue and the negative impact of majority class noisy examples.\n\n• Experimentally, we show that AU detection performance benefits from the use of RHLS without any additional computational overhead. More precisely we demonstrate that applying RHLS on a modern multitask baseline is competitive on BP4D and significantly outperforms state-of-the-art results on DISFA.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "In this work, we use a multi-task binary classification dataset composed of couples (x, y) with x ∈ R H×W ×3 a face image and y ∈ {0, 1} T the labels for each of the T target AU.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Vanilla Label Smoothing",
      "text": "AU detection involves subtle changes in skin texture that are difficult to detect, even for expertly trained annotators. As a consequence, the main available annotated datasets display noise. Figure  1  highlights the existence of this noise by showing several arguably wrong annotated examples. Prior work  [1]  showed that label smoothing could help mitigate the influence of annotation noise by reducing model confidence  [16]  and consequently preventing it from over-fitting on noisy examples. For that purpose, label smoothing introduces uniform noise into the ground truth labels with probability α. From a concrete point of view, label smoothing with coefficient α modifies ground truth label of task i as follows:\n\nHowever, we experimentally show that label smoothing degrades AU detection performance (see section 3.3). We hypothesize that such performance drop is due to AU datasets Fig.  2 . Action Unit frequencies for BP4D and DISFA. BP4D is slightly imbalanced toward negativeness compared to DISFA where most AU are represented in less than 1/10 frame. Minimizing cross-entropy in such imbalanced situations tend to push the network toward over-confidence in the majority class i.e toward ignoring the minority class and predicting the majority class with high probability.\n\nimbalance. Indeed, as shown by figure  2 , in nowadays most popular AU datasets, several AU display low empirical frequencies. In particular, such imbalance has been shown  [17]  to push model toward under-confident predictions for the minority class. Therefore, by reducing confidence of both positive and negative examples, label smoothing may worsen the pre-existing confidence problem on the minority class and in turn explain the observed performance gap.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Robin Hood Label Smoothing (Rhls)",
      "text": "In order to address that problem we extend label smoothing to Robin Hood Label Smoothing (RHLS). RHLS takes its name from the fact that, by smoothing only the majority class, it introduces a probability to steal examples from the majority (the rich) class to give it to the minority class (the poor). Formally, it first introduces α + i and α - i that respectively denote the uniform noise probability for positive and negative values for task i so that :\n\nThen it parametrizes α + i and α - i with respect to task i empirical frequencies f i so that only the majority class is smoothed : where β ∈ [0, 1] quantifies the amount of noise introduced in the majority class from 0 (No noise is applied) to 1 (noise is applied so that the resulting dataset is balanced).\n\nThrough the introduction of noise in the majority class using β > 0, RHLS encourage less-confident prediction for negative examples. It consequently reduces the negative impact of noisy negative examples, alleviates the imbalancebased overconfidence problem an may improve performance w.r.t vanilla label smoothing.\n\nIn what follows, we validate the RHLS superiority over vanilla label smoothing in imbalanced situations and discuss the significant improvements it provides in AU detection.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Experiments",
      "text": "In this section, we first introduce Action Unit Detection datasets (Section 3.1) along with details about our architecture and its optimization (Section 3.2). Then, in Section 3.3 we both validate our method hyperparameters and perform a comparative analysis between RHLS and vanilla methods for fighting noise and imbalance. Finally In Section 3.4, we compare RHLS with state-of-the-art approaches.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Datasets",
      "text": "DISFA  [18]  is a dataset for facial action unit detection. It consists of 27 single subject videos for a total of ≈ 100k face images. Each image is annotated with 12 AU intensity scores that range from 0 to 5. In detection, intensity scores higher than 2 are considered positive  [19] . As far as evaluation is concerned, the 27 videos are split into 3 folds of 9  [4]  and performance is measured by averaging the 3 mean F1-Scores obtained from training on 2 folds and evaluating on the last. For stability concerns  [4, 11] , we run such evaluation protocol 5 times and report mean performance. For validation, we follow the protocol in  [4]  i.e we perform 6 fold cross-validation on each of the 3 two-folds training set and compute the validation scores by averaging F1-Score on those 18 runs.\n\nBP4D  [2]  dataset is composed of approximately 140k face images in which 41 people (23 females, 18 males) with different ethnicities are represented. Each image is annotated with the presence of 12 AU. Similarly to DISFA, performance evaluation consists in measuring F1-Scores on all 12 AUs using a subject exclusive 3-fold cross evaluation with the same fold distribution as in  [4] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Implementation Details",
      "text": "For all our experiments, we inspire from the M architectures in  [20] . However, as no face-based pretrained ViT is publicly available, we replace this part by a resnet50 pretrained on VG-GFACE  [21] . To fit with the ViT encoder outputs in  [20]  that is P × d where P = 196 and d = 768, we replace the last convolutional block by a conv1D of size d = 768 on top of which a layer of self-attention is built. For the decoder part we use as many class tokens in the cross-attention as there are AU in the dataset (T = 8 for DISFA and T = 12 for BP4D) and we feed each of the resulting T representations to an AU specific dense layer with sigmoidal activation.\n\nFor optimization we use AdamW  [22]  with exponential decay β = 0.75 for 2 epochs. In the convolutional part we use initial learning rate λ c = 5e-5 for BP4D and λ c = 1e-5 for DISFA. In the transformer part we scale the initial learning rate w.r.t number of queries q (P in self attention and T in cross-attention), model size d and batchsize B = 32, so that λ t = λ",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Ablation Study",
      "text": "In this section we validate smoothing intensity β and compare RHLS to vanilla existing methods for noise mitigation (label smoothing  [16] ) and imbalance (frequency weighted crossentropy  [4, 11, 12] ) on DISFA.\n\nFigure  3  shows the evolution of the validation score with respect to β. For low values of β, RHLS significantly boosts the model predictive performance by reducing overconfidence in the majority class and consequently lowering the negative influence of both imbalance and noisy negative examples. However, passed a certain threshold for β, RHLS introduces too much false positive in training which hurts the learning process and results in performance drops. Therefore, we select β = 0.25 for evaluation.\n\nSecond, Table  1  compares the performance of the proposed RHLS with other existing label smoothing methods. First, it is noticeable that frequency weighted BCE hurts performance. This may be caused by the difference of scales across AU frequencies (eg : f AU1 ∼ 1e -2, while f AU25 ∼ 1e -1) so that weighting AU loss contribution using w i =",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Method",
      "text": "Mean F1Score Baseline 63.0 ± 1.9 Label Smoothing(α = 0. Finally, figure  4  show the histogram of predictions for different smoothing method on DISFA. We observe that baseline results display majority class overconfidence as many positive examples are predicted negative with probability 0. Label smoothing mitigates that problem but worsens the imbalancebased minority class low confidence problem which in turn reduce overall performance (see Table  1 ). By smoothing only the majority class, RHLS mitigates majority class confidence without any influence on the minority class and consequently obtains the performance boost reported in Table  1 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Comparaison With State-Of-The-Art Methods",
      "text": "In this section we compare RHLS with state-of-the-art AU detection methods.\n\nTable  2  provides RHLS results on BP4D. Interestingly, applying RHLS over a modern multi-task baseline is competitive with several recent methods  [11, 6]  including other uncertainty modelling strategies  [13] . However, it gets outperformed by the most recent ones that either involve more complex landmark guided transformer architecture  [12]  or refined AU dependency modelling  [8, 10] . Nonetheless, the increment RHLS provides to the baseline shows that it is a simple yet efficient way to improve performance without any additional computational overhead. In that extent, attempting to plug it on top of more complex methods could be a promising track toward better overall AU detection performance.\n\nMean F1 Score BP4D DISFA JAANet  [4]  60.0 56.0 ARL  [11]  61.1 58.7\n\nJ ÂANET  [6]  62.4 63.5\n\nUGN-B  [13]  63.3 60.0\n\nFAUwT  [12]  64.2 61.5\n\nMONET  [8]  64   2  shows that the free increment RHLS allows the baseline architecture to surpass state-of-the-art performance. To explain those excellent results, it is worth noticing that most state-of-the-art methods  [4, 11, 8, 12]  use frequency weighted loss. Therefore the superiority of RHLS against the frequency weighted loss on DISFA (see Table  1 ) may justify the significant increment that we observe.\n\nBeyond that, it is also worth noticing that RHLS simple and free label modification pushes a simple baseline above more complex methods with spatial prior guidance  [6]  or explicit AU dependency modelling  [8] . On the one hand, it highlights that imbalance reduction and noise modelling are as critical as input feature extraction or dependency modelling in AU detection performance. On the other hand, it offers potential improvement perspective as integrating RHLS with more complex methods could further improve performance.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we investigated the impact of label smoothing to fight against AU datasets imbalance and noise problems. In particular, we showed that vanilla label smoothing is ill-adapted to imbalanced situations as it may worsen pre-existing under-confidence problems and degrade performance. To alleviate this issue, we proposed Robin Hood Label Smoothing that constrains label smoothing to the majority class by introducing a probability to steal examples from the rich (the majority class) to give it to the poor (the minority class). In that extent RHLS reduces both imbalance issues and majority class noise negative impact Experimentally, we showed that applying RHLS on top of a multi-task baseline provides competitive performance on BP4D and significantly outperforms state-of-the-art on DISFA. In particular, on DISFA, the excellent results obtained indicates that RHLS is a better option that the frequency weighted loss. In future works, we will inspire from the successes of AU dependency modelling methods  [10, 8]  and try structuring label smoothing noise to prevent smoothed labels from displaying unrealistic dependencies that may hurt the training process.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Noisy examples from DISFA and BP4D. For exam-",
      "page": 1
    },
    {
      "caption": "Figure 1: , AU are often so subtle",
      "page": 1
    },
    {
      "caption": "Figure 1: highlights the existence of this noise by show-",
      "page": 2
    },
    {
      "caption": "Figure 2: Action Unit frequencies for BP4D and DISFA.",
      "page": 2
    },
    {
      "caption": "Figure 3: RHLS β validation on DISFA.",
      "page": 3
    },
    {
      "caption": "Figure 3: shows the evolution of the validation score with",
      "page": 3
    },
    {
      "caption": "Figure 4: Histogram of predictions on positive and negative",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 2: Comparaison of RHLS with state-of-the-art deep",
      "data": [
        {
          "Mean F1 Score": "JAANet [4]\nARL [11]\nJ ˆAANET [6]\nUGN-B [13]\nFAUwT [12]\nMONET [8]",
          "BP4D": "60.0\n61.1\n62.4\n63.3\n64.2\n64.5",
          "DISFA": "56.0\n58.7\n63.5\n60.0\n61.5\n63.9"
        },
        {
          "Mean F1 Score": "Baseline\nRHLS",
          "BP4D": "61.9\n63.0",
          "DISFA": "63.1\n65.8"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: Comparaison of RHLS with state-of-the-art deep",
      "data": [
        {
          "Method": "Baseline\nLabel Smoothing(α = 0.1)\nFrequency Weighted BCE\nRobin Hood Label Smoothing",
          "Mean F1Score": "63.0 ± 1.9\n62.0 ± 1.8\n61.7 ± 2.1\n65.8 ± 1.4"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Does label smoothing mitigate label noise?",
      "authors": [
        "Michal Lukasik",
        "Srinadh Bhojanapalli",
        "Aditya Menon",
        "Sanjiv Kumar"
      ],
      "year": "2020",
      "venue": "ICML"
    },
    {
      "citation_id": "3",
      "title": "spontaneous: a high-resolution spontaneous 3d dynamic facial expression database",
      "authors": [
        "Xing Zhang",
        "Lijun Yin",
        "Shaun Jeffrey F Cohn",
        "Michael Canavan",
        "Andy Reale",
        "Peng Horowitz",
        "Jeffrey Liu",
        "Girard"
      ],
      "year": "2014",
      "venue": "IVC"
    },
    {
      "citation_id": "4",
      "title": "Eac-net: Deep nets with enhancing and cropping for facial action unit detection",
      "authors": [
        "Wei Li",
        "Farnaz Abtahi",
        "Zhigang Zhu",
        "Lijun Yin"
      ],
      "year": "2018",
      "venue": "TPAMI"
    },
    {
      "citation_id": "5",
      "title": "Deep adaptive attention for joint facial action unit detection and face alignment",
      "authors": [
        "Zhiwen Shao",
        "Zhilei Liu",
        "Jianfei Cai",
        "Lizhuang Ma"
      ],
      "year": "2018",
      "venue": "ECCV"
    },
    {
      "citation_id": "6",
      "title": "Privileged attribution constrained deep networks for facial expression recognition",
      "authors": [
        "Jules Bonnard",
        "Arnaud Dapogny",
        "Ferdinand Dhombres",
        "Kevin Bailly"
      ],
      "year": "2022",
      "venue": "ICPR"
    },
    {
      "citation_id": "7",
      "title": "Jaa-net: Joint facial action unit detection and face alignment via adaptive attention",
      "authors": [
        "Zhiwen Shao",
        "Zhilei Liu",
        "Jianfei Cai",
        "Lizhuang Ma"
      ],
      "year": "2021",
      "venue": "IJCV"
    },
    {
      "citation_id": "8",
      "title": "Local relationship learning with person-specific shape regularization for facial action unit detection",
      "authors": [
        "Xuesong Niu",
        "Hu Han",
        "Songfan Yang",
        "Yan Huang",
        "Shiguang Shan"
      ],
      "year": "2019",
      "venue": "CVPR"
    },
    {
      "citation_id": "9",
      "title": "Multi-order networks for action unit detection",
      "authors": [
        "Arnaud Gauthier Tallec",
        "Kevin Dapogny",
        "Bailly"
      ],
      "year": "2022",
      "venue": "TAFFC"
    },
    {
      "citation_id": "10",
      "title": "Deep structure inference network for facial action unit recognition",
      "authors": [
        "Ciprian Corneanu",
        "Meysam Madadi",
        "Sergio Escalera"
      ],
      "year": "2018",
      "venue": "ECCV"
    },
    {
      "citation_id": "11",
      "title": "Hybrid message passing with performance-driven structures for facial action unit detection",
      "authors": [
        "Tengfei Song",
        "Zijun Cui",
        "Wenming Zheng",
        "Qiang Ji"
      ],
      "year": "2021",
      "venue": "CVPR"
    },
    {
      "citation_id": "12",
      "title": "Facial action unit detection using attention and relation learning",
      "authors": [
        "Zhiwen Shao",
        "Zhilei Liu",
        "Jianfei Cai",
        "Yunsheng Wu",
        "Lizhuang Ma"
      ],
      "year": "2004",
      "venue": "TAFFC"
    },
    {
      "citation_id": "13",
      "title": "Facial action unit detection with transformers",
      "authors": [
        "Miriam Geethu",
        "Bjorn Jacob",
        "Stenger"
      ],
      "year": "2004",
      "venue": "CVPR"
    },
    {
      "citation_id": "14",
      "title": "Uncertain graph neural networks for facial action unit detection",
      "authors": [
        "Tengfei Song",
        "Lisha Chen",
        "Wenming Zheng",
        "Qiang Ji"
      ],
      "year": "2021",
      "venue": "AAAI"
    },
    {
      "citation_id": "15",
      "title": "Multitask learning using uncertainty to weigh losses for scene geometry and semantics",
      "authors": [
        "Alex Kendall",
        "Yarin Gal",
        "Roberto Cipolla"
      ],
      "year": "2018",
      "venue": "CVPR"
    },
    {
      "citation_id": "16",
      "title": "Training convolutional networks with noisy labels",
      "authors": [
        "Sainbayar Sukhbaatar",
        "Joan Bruna",
        "Manohar Paluri",
        "Lubomir Bourdev",
        "Rob Fergus"
      ],
      "year": "2014",
      "venue": "Training convolutional networks with noisy labels",
      "arxiv": "arXiv:1406.2080"
    },
    {
      "citation_id": "17",
      "title": "Rethinking the inception architecture for computer vision",
      "authors": [
        "Christian Szegedy",
        "Vincent Vanhoucke",
        "Sergey Ioffe",
        "Jon Shlens",
        "Zbigniew Wojna"
      ],
      "year": "2016",
      "venue": "CVPR"
    },
    {
      "citation_id": "18",
      "title": "Focal loss for dense object detection",
      "authors": [
        "Tsung-Yi Lin",
        "Priya Goyal",
        "Ross Girshick",
        "Kaiming He",
        "Piotr Dollár"
      ],
      "year": "2017",
      "venue": "ICCV"
    },
    {
      "citation_id": "19",
      "title": "Disfa: A spontaneous facial action intensity database",
      "authors": [
        "Mohammad Mohammad Mavadati",
        "Kevin Mahoor",
        "Philip Bartlett",
        "Jeffrey Trinh",
        "Cohn"
      ],
      "year": "2013",
      "venue": "TAFFC"
    },
    {
      "citation_id": "20",
      "title": "Deep region and multi-label learning for facial action unit detection",
      "authors": [
        "Kaili Zhao",
        "Wen-Sheng Chu",
        "Honggang Zhang"
      ],
      "year": "2016",
      "venue": "CVPR"
    },
    {
      "citation_id": "21",
      "title": "Going deeper with image transformers",
      "authors": [
        "Hugo Touvron",
        "Matthieu Cord",
        "Alexandre Sablayrolles",
        "Gabriel Synnaeve",
        "Hervé Jégou"
      ],
      "venue": "CVPR"
    },
    {
      "citation_id": "22",
      "title": "Vggface2: A dataset for recognising faces across pose and age",
      "authors": [
        "Qiong Cao",
        "Li Shen",
        "Weidi Xie",
        "Omkar Parkhi",
        "Andrew Zisserman"
      ],
      "year": "2018",
      "venue": "IEEE"
    },
    {
      "citation_id": "23",
      "title": "Decoupled weight decay regularization",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2019",
      "venue": "ICLR"
    }
  ]
}