{
  "paper_id": "2204.05649v2",
  "title": "Adff: Attention Based Deep Feature Fusion Approach For Music Emotion Recognition",
  "published": "2022-04-12T09:15:48Z",
  "authors": [
    "Zi Huang",
    "Shulei Ji",
    "Zhilan Hu",
    "Chuangjian Cai",
    "Jing Luo",
    "Xinyu Yang"
  ],
  "keywords": [
    "music emotion recognition",
    "squeeze-andexcitation attention",
    "multi-level feature fusion"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Music emotion recognition (MER), a sub-task of music information retrieval (MIR), has developed rapidly in recent years. However, the learning of affect-salient features remains a challenge. In this paper, we propose an end-to-end attention-based deep feature fusion (ADFF) approach for MER. Only taking log Mel-spectrogram as input, this method uses adapted VG-GNet as spatial feature learning module (SFLM) to obtain spatial features across different levels. Then, these features are fed into squeeze-and-excitation (SE) attention-based temporal feature learning module (TFLM) to get multi-level emotion-related spatial-temporal features (ESTFs), which can discriminate emotions well in the final emotion space. In addition, a novel data processing is devised to cut the single-channel input into multichannel to improve calculative efficiency while ensuring the quality of MER. Experiments show that our proposed method achieves 10.43% and 4.82% relative improvement of valence and arousal respectively on the R 2 score compared to the stateof-the-art model, meanwhile, performs better on datasets with distinct scales and in multi-task learning.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "As an important entertainment for human beings, music has wide appeal. Studies indicate that people attach importance to music mainly because of the connotation and essential characteristics involved in emotion  [1] . More and more users retrieve music according to emotion in MIR systems, where MER plays a crucial role  [2, 3] . However, the abstraction of emotion makes it difficult to analyze. Therefore, how to construct an effective MER method has attracted extensive attention.\n\nThe existing MER methods can be divided into two categories including classification and regression according to different emotion models. The former selects some emotional adjectives to classify music. However, the limited words cannot describe human emotions exactly  [4, 5, 6] . The latter uses the spatial position of emotion space to express human internal emotion. The two-dimensional valence-arousal (V-A) emotion model proposed by Russell  [7]  is one of the mainstream emotion models for regression tasks  [8, 9, 10, 11] . It represents emotion by valence and arousal, which stand for the degrees of pleasantness and bodily activation, respectively. In this paper, we aim to use a certain point in the V-A emotion space to describe the whole emotion for clip.\n\nFeature engineering and model designing are the common solutions in MER. The first one improves recognition perfor-*Corresponding author mance by constructing the effective feature sets  [12, 13, 14] . However, it requires a lot of manpower to design efficient feature sets for different datasets and recognition targets. For this reason, many researchers hope that the model can autonomously learn the affect-salient features. In the past, traditional machine learning methods (SVR, RF, etc.) were used to recognize music emotion  [9, 15] , but gradually eliminated due to low flexibility and poor generalization. In recent years, deep learning has made amazing achievements in MER. The convolutional long-short-term-memory deep network based models proposed in  [16, 17]  can adaptively learn affect-salient features in music. But these methods are generally suitable for a certain length and do not consider the relations among different clips. Besides audio,  [18, 19]  also introduce lyrics and other information to conduct multi-modal learning. Unfortunately, the lack of high-quality multi-modal aligned dataset leads to a significant reduction in applicable scenarios. Moreover, some researchers try to obtain more information by manipulating audio to improve the ability of their models. For example,  [8]  tries to separate multiple sound sources in music (vocals, bass, drums, etc.) to explore the influence of different musical elements on MER.  [20]  fuses multi-scale features of different lengths to strengthen predicting performance. Nevertheless, these methods are not strictly end-to-end structures, which may lead to additional errors when data flows through various modules.\n\nTo improve the above problems, we propose an end-to-end attention-based deep feature fusion (ADFF) approach for MER. We first take log Mel-spectrogram as input, then use adapted VGGNet as spatial feature learning module (SFLM) to obtain low-to-high level spatial structures. After that, these spatial features turn into multi-level emotion-related spatial-temporal features (ESTFs) by squeeze-and-excitation (SE) attention-based temporal feature learning module (TFLM). Finally, the prediction module maps the fusion into the emotion space. A series of experiments on the PMEmo dataset  [21]  demonstrate that the ADFF model achieves an R 2 score of 0.4575 for valence and 0.6394 for arousal respectively, which is a relative improvement of 10.43% and 4.82% compared to the state-of-the-art approach. It should be noted that the prediction of valence is notoriously more challenging than arousal  [12] . Furthermore, extended experiments on datasets with distinct scales and in multi-task also show that our method can effectively learn the affect-salient features from music clips and complete various tasks in MER.\n\nOur contributions are as follows: (1) our proposed ADFF model for MER achieves a better performance than the state-ofthe-art method; (2) we introduce SE attention that enhances the weight of emotion-related features to help our model work better; (3) we design a novel data processing to improve calculative efficiency of the model while ensuring the quality of MER. Figure  1 : An overview of the ADFF model. OP1 means cutting the input into N segments, OP2 means stacking these segments into multi-channel. SFLM learns low-to-high level spatial features from input, and these multi-levels spatial features will be fed into the corresponding subnet of TFLM in each layer to obtain multi-level ESTFs. Finally, the fusion of ESTFs is mapped into the emotion space by the prediction module. You can choose the number of the last FC layer in prediction module for single-task or multi-task.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Propsed Method",
      "text": "In this section, we mainly describe the ADFF model which consists of three modules: data processing, multi-level spatialtemporal feature learning, and prediction module.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Data Processing",
      "text": "In this work, we use log Mel-spectrogram as input, whose dimension is (1, f rame len, dim), where f rame len and dim represent the frame length and the number of Mel bands, respectively. The designed data processing method sequentially cut the spectrogram into seg num parts in the time dimension, and each part will form a new channel and be stacked into an image with dimension of (seg num, f rame len/seg num, dim), as shown in the data processing module in Figure  1 . We believe this method can reduce the distance among different intervals in the input and the long-term dependence. Experiments also show that our data processing method can improve the model computing efficiency while ensuring the model recognition performance.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Spatial-Temporal Feature Learning",
      "text": "Learning emotional features is of vital importance to the MER task. It is commonly thought that emotion is involved in spatial and temporal features. For this reason, we design a specific spatial-temporal feature learning module for emotion learning, as shown by SFLM and TFLM in Figure  1 .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Spatial Feature Leaning",
      "text": "The effectiveness of VGGNet in image processing has been widely validated  [22] . In recent years, with the increasing attention of MER, some researchers have applied the VGGNet structure in it  [23, 24, 25, 26] . However, they only explored their methods based on pipeline architecture in classification tasks. In this paper, we introduce the convolution subnetwork of VGGNet-16 as the spatial feature learning module (SFLM), which can be divided into 5 levels, and reform it by adding Relu and Batchnorm structures at the end of each level to get stronger and more robust spatial features. This adaptation makes SFLM more suitable for regression tasks. Suppose the N-th level of SFLM can be represented as SF LMN , and its output is SN :\n\nwhere HN , WN , CN represent the height, width and the number of channels of SN , respectively. If we divide SN according to the number of channels, it also can be expressed as",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Se Attention-Based Temporal Feature Learning",
      "text": "As a kind of temporal information, the music contains emotion changing over time. To capture temporally related emotions, we employ TFLM consisting of SE Attention and Bi-LSTM to learn temporal features. The SE attention is used to obtain the importance of different channels  [27] , which alleviates the problem of focusing on temporal structure only in traditional attention mechanisms. During the learning process, the input of a single channel is transformed into multi-channel through our data processing, which will strengthen the temporal correlation among distinct channels. In this case, the SE block can suppress the features that do not contribute much to emotion by learning the importance of different channels. Figure  2  shows the transformation of SE block, Fsq (•) represents the squeeze operation.\n\n, which is the global information embedding of the N-th level. To obtain the importance among the channels of SN , ZN needs to go through an excitation transform yet. where WN and AN represent learnable weight matrix and attention matrix. Then we get the weighted feature map SN by rescaling SN with AN :\n\nBy (  4 ), we get SN = S 1  N , S 2 N , ..., S C N N , which will be transformed to EN (referred to ESTF) by 2-layer Bi-LSTM to complete temporal structure learning.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Fusion Strategy And Emotion Prediction",
      "text": "Deep learning network can learn high-level abstract emotionrelated features for target tasks, but some low-level features useful may be lost in this process  [28] . To make full use of all the multi-layer emotion-related information, we simply concatenate all the ESTF as E = [E1, E2, ..., E5] to preserve the internal structure of ESTF from different levels maximumly.\n\nAfter that, E is fed into the prediction module to map into the emotion space. As shown in Figure .1, the prediction module is composed of several Fully Connected (FC) Layer and a Feedforward (FF) Layer. The number of the last FC layer is 1 or 2 according to single-task or multi-task, which maps the activation of FF layer to target output.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments And Results",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Datasets",
      "text": "The PMEmo dataset contains 794 chorus clips (provided by audio) of popular songs and their corresponding valence-arousal annotations. 457 annotators from different countries and majors are invited to annotate this dataset, and each chorus received at least 10 annotations. To compare our model with existing methods and make full use of the dataset, we split the dataset into two sizes. And for each size, we have 6 different lengths of input to explore the performance of the ADFF model while dealing with various segments, that is, seg lens=  [5, 10, 15, 20, 25, 30] .\n\n• Simple Datasets. Each chorus will be cut into a specified fixed length randomly. Namely, each chorus and its corresponding annotation appear only once in a simple dataset, which is consistent with  [8, 11] .\n\n• Full Datasets. To make the most of the PMEmo dataset, we cut each chorus into several segments with specified lengths in time order. If the length of the last segment is less than seg len/2, it will be dropped. Otherwise, it will be extended forward until satisfying the demand. All segments cut out from the same chorus share the same static annotation.\n\nNote that no matter which dataset is selected, if the original length of the chorus is less than specified seg len, the audio will be padding with zero first to ensure a fixed length. And all the valence and arousal annotations are linearly scaled to [-1, 1] to improve the robustness of the model.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Details",
      "text": "We use the R 2 score and root-mean-squared error (RMSE) in the regression task, and accuracy in the classification task as the evaluation metrics. The R 2 score ranges from negative infinity to 1, and the larger value is better. On the contrary, the smaller the RMSE is, the better. For a fairer comparison and to avoid accidental errors, we take the mean result of 5-fold cross-validation as the final result following  [8, 11] . The input log Mel-spectrogram is extracted by librosa 0.7.2 tool  [29] , with Mel bands of 128, a sampling rate of 44.1KHZ, window size and hop size of 60ms and 10ms respectively. Moreover, we use Adam optimizer for training, with a decay weight of 1e-5, learning rate of 1e-5, training epoch of 200, and batch size of 32. The decay steps are  [20, 45, 80, 110, 140, 170 ].",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Method Comparison And Ablation Analysis",
      "text": "To verify the advancement of our proposed model and explore the role of different modules, we choose EmoMucs  [8] , MLEM  [11] , and two variants of the ADFF model for comparison. EmoMucs recognizes music emotion based on the source separation algorithm and announced it as the most advanced method. MLEM designes a useful method to debug the biased MER model to perform better. The two variants of the ADFF model drop SE-block and TFLM, respectively. Note that EmoMucs and MLEM both experiment on the simple dataset with the input length of 20 seconds, and we compare the best performance of each model in the same condition.\n\nAs shown in Table  1 , our proposed ADFF model has obvious advantages over the others, which get lower RMSE and higher R 2 scores. Particularly, the R 2 score of valence and arousal increase relatively 10.43% and 4.82% than Emo-Mucs, and note that valence is more challenging to predict than arousal. If taking the deviation into account, our method is more stable than MLEM. Moreover, the ADFF model belongs to a strictly end-to-end architecture, but EmoMucs and MLEM both need source separation algorithms to process the original audio before training, which may introduce more errors.\n\nIn addition, ablation experiments show that achieving highperformance prediction for arousal doesn't need a complicated model, this conclusion is consistent with  [30] . And compared with the two variants, the ADFF model has a relative improvement of 5.61% and 10.11% in R 2 score for predicting valence, which means SE block enhances the weight of emotion-related features and TFLM further improves the emotion capture ability of model.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "The Influence Of Data Processing",
      "text": "To explore the influence of our data processing on the model, we compared the results of ADFF on simple datasets of 20 seconds when seg num changes in  [1, 2, 4, 6, 8, 10, 12, 14, 16] .  Figure  3  shows that seg num has different optimal values for various tasks. The best and the second-best results achieve when seg num is 1 and 6 for valence, but for arousal, the seg num becomes 6 and 1. If considering the metric only, it's a good choice to choose whichever seg len of 1 or 6 for input with a length of 20 seconds. Furthermore, we observe that along with the growth of seg num, the time cost will decrease clearly as shown in Table  2 . However, if we choose a too large seg num, the recognition performance of the model will reduce. We conjecture the reason is that the feature represented by each channel is too short to contain enough emotional information. Balancing the time cost and performance benefit, we choose seg num of 6 in the following experiments.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "The Performance Of Different Lengths",
      "text": "Previous studies have shown that too long or too short input will damage the performance of MER  [17, 31] . Exploring the influence of input with different lengths on the model can help to select the optimal input length and maximize the comprehensive benefits of recognition performance and speed. Therefore, we explore the performance of the proposed model when input length changes. In addition, we also explore the influence of data size on the performance of recognition.\n\nAs shown in Table  3 , different input lengths will result in the discrepancy of recognition performance. On simple datasets, the model works best when seg len is 20. Note that input with different lengths should have respective optimal seg num parameters, and we have not investigated one by one here. On full datasets, the recognition performance for all lengths have been improved. The R 2 score of the valence of 5 seconds and 10 seconds segments stand out particularly, which relative increased by 30.21% and 35.91% respectively than on simple datasets, even outpaced the performance of 20 seconds. This illustrates that the size of dataset will affect the perfor-   mance of the model. Even if a clip is short, excellent recognition performance can also be obtained when the model is trained fully. The conclusion may help the existing music software to improve its emotion-related recommendation function.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "The Performance Of Multi-Task And Classification Task",
      "text": "We also explore the ability of the proposed model to multi-task.\n\nAs shown in Table  4 , when the model predicts valence and arousal simultaneously, the performance is not much different from single-task. But doing so can avoid the trouble of model designing or training separately for various tasks. And it also demonstrates that our proposed model has an excellent ability to learn affect-salient features for both valence and arousal. Besides, we have extended our experiments on two-category and four-category tasks according to the positive and negative values of valence and arousal. Obviously, four-category tasks are more challenging than two-category, and predicting valence is more difficult than arousal. But to our surprise, different lengths of inputs have no obvious effect on the classification tasks.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion And Future Work",
      "text": "In this work, we propose an end-to-end attention-based deep feature fusion method for MER. The proposed model builds a bridge from affect-salient feature to emotion space effectively by using log Mel-spectrogram only. A series of experiments prove that the proposed model outperforms the state-of-theart method in performance and robustness, and maintains high recognition quality on different input lengths, dataset sizes, or tasks. Until now, we have only explored on PMEmo dataset, in which English songs account for the vast majority. However, there are various genres and languages of music in reality.\n\nIn future work, we will investigate the ability of our model on cross-language or cross-genre datasets, and try to introduce pretraining to further improve the effectiveness of our model.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An overview of the ADFF model. OP1 means cutting the input into N segments, OP2 means stacking these segments into",
      "page": 2
    },
    {
      "caption": "Figure 1: We believe",
      "page": 2
    },
    {
      "caption": "Figure 1: 2.2.1. Spatial feature leaning",
      "page": 2
    },
    {
      "caption": "Figure 2: SE block of TFLM1.",
      "page": 3
    },
    {
      "caption": "Figure 3: The R2 score and RMSE of Valence (V) and Arousal",
      "page": 4
    },
    {
      "caption": "Figure 3: shows that seg num has different optimal values",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 1: The performance of different models on simple dataset",
      "page": 3
    },
    {
      "caption": "Table 1: , our proposed ADFF model has",
      "page": 3
    },
    {
      "caption": "Table 2: The cost time of different seg num",
      "page": 4
    },
    {
      "caption": "Table 2: However, if we choose a too large",
      "page": 4
    },
    {
      "caption": "Table 3: , different input lengths will result",
      "page": 4
    },
    {
      "caption": "Table 3: The performance of different lengths on simple and full",
      "page": 4
    },
    {
      "caption": "Table 4: The performance of multi-task and classiﬁcation-task",
      "page": 4
    },
    {
      "caption": "Table 4: , when the model predicts valence and",
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Emotional responses to music: The need to consider underlying mechanisms",
      "authors": [
        "P Juslin",
        "D Västfjäll"
      ],
      "year": "2008",
      "venue": "Behavioral and brain sciences"
    },
    {
      "citation_id": "3",
      "title": "How people describe their music information needs: A grounded theory analysis of music queries",
      "authors": [
        "D Bainbridge",
        "S Cunningham",
        "J Downie"
      ],
      "year": "2003",
      "venue": "How people describe their music information needs: A grounded theory analysis of music queries"
    },
    {
      "citation_id": "4",
      "title": "Survey of music information needs, uses, and seeking behaviours: preliminary findings",
      "authors": [
        "J Lee",
        "J Downie"
      ],
      "year": "2004",
      "venue": "ISMIR"
    },
    {
      "citation_id": "5",
      "title": "Experimental studies of the elements of expression in music",
      "authors": [
        "K Hevner"
      ],
      "year": "1936",
      "venue": "The American Journal of Psychology"
    },
    {
      "citation_id": "6",
      "title": "Update of the hevner adjective checklist",
      "authors": [
        "E Schubert"
      ],
      "year": "2003",
      "venue": "Perceptual and motor skills"
    },
    {
      "citation_id": "7",
      "title": "Multi-label classification of music into emotions",
      "authors": [
        "K Trohidis",
        "G Tsoumakas",
        "G Kalliris",
        "I Vlahavas"
      ],
      "year": "2008",
      "venue": "ISMIR"
    },
    {
      "citation_id": "8",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "9",
      "title": "The multiple voices of musical emotions: Source separation for improving music emotion recognition models and their interpretability",
      "authors": [
        "J De Berardinis",
        "A Cangelosi",
        "E Coutinho"
      ],
      "year": "2020",
      "venue": "Proceedings of the 21st International Society for Music Information Retrieval Conference"
    },
    {
      "citation_id": "10",
      "title": "Analysis of the effect of dataset construction methodology on transferability of music emotion recognition models",
      "authors": [
        "S Hult",
        "L Kreiberg",
        "S Brandt",
        "B Jónsson"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 International Conference on Multimedia Retrieval"
    },
    {
      "citation_id": "11",
      "title": "Cochleogrambased approach for detecting perceived emotions in music",
      "authors": [
        "M Russo",
        "L Kraljević",
        "M Stella",
        "M Sikora"
      ],
      "year": "2020",
      "venue": "Information Processing & Management"
    },
    {
      "citation_id": "12",
      "title": "Tracing back music emotion predictions to sound sources and intuitive perceptual qualities",
      "authors": [
        "S Chowdhury",
        "V Praher",
        "G Widmer"
      ],
      "year": "2021",
      "venue": "Tracing back music emotion predictions to sound sources and intuitive perceptual qualities",
      "arxiv": "arXiv:2106.07787"
    },
    {
      "citation_id": "13",
      "title": "Popular music and the role of vocal melody in perceived emotion",
      "authors": [
        "S Beveridge",
        "D Knox"
      ],
      "year": "2018",
      "venue": "Psychology of Music"
    },
    {
      "citation_id": "14",
      "title": "Enhance popular music emotion regression by importing structure information",
      "authors": [
        "X Wang",
        "Y Wu",
        "X Chen",
        "D Yang"
      ],
      "year": "2013",
      "venue": "Enhance popular music emotion regression by importing structure information"
    },
    {
      "citation_id": "15",
      "title": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference",
      "year": "2013",
      "venue": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "16",
      "title": "Regression-based music emotion prediction using triplet neural networks",
      "authors": [
        "K Cheuk",
        "Y.-J Luo",
        "B Balamurali",
        "G Roig",
        "D Herremans"
      ],
      "year": "2020",
      "venue": "2020 international joint conference on neural networks (ijcnn)"
    },
    {
      "citation_id": "17",
      "title": "Music emotion maps in arousal-valence space",
      "authors": [
        "J Grekow"
      ],
      "year": "2016",
      "venue": "IFIP International Conference on Computer Information Systems and Industrial Management"
    },
    {
      "citation_id": "18",
      "title": "Bidirectional convolutional recurrent sparse network (bcrsn): an efficient model for music emotion recognition",
      "authors": [
        "Y Dong",
        "X Yang",
        "X Zhao",
        "J Li"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "19",
      "title": "Stacked convolutional and recurrent neural networks for music emotion recognition",
      "authors": [
        "M Malik",
        "S Adavanne",
        "K Drossos",
        "T Virtanen",
        "D Ticha",
        "R Jarina"
      ],
      "year": "2017",
      "venue": "Stacked convolutional and recurrent neural networks for music emotion recognition",
      "arxiv": "arXiv:1706.02292"
    },
    {
      "citation_id": "20",
      "title": "Music mood detection based on audio and lyrics with deep neural net",
      "authors": [
        "R Delbouys",
        "R Hennequin",
        "F Piccoli",
        "J Royo-Letelier",
        "M Moussallam"
      ],
      "year": "2018",
      "venue": "Music mood detection based on audio and lyrics with deep neural net",
      "arxiv": "arXiv:1809.07276"
    },
    {
      "citation_id": "21",
      "title": "Exploiting synchronized lyrics and vocal features for music emotion detection",
      "authors": [
        "L Parisi",
        "S Francia",
        "S Olivastri",
        "M Tavella"
      ],
      "year": "2019",
      "venue": "Exploiting synchronized lyrics and vocal features for music emotion detection",
      "arxiv": "arXiv:1901.04831"
    },
    {
      "citation_id": "22",
      "title": "Dblstm-based multiscale fusion for dynamic emotion prediction in music",
      "authors": [
        "X Li",
        "J Tian",
        "M Xu",
        "Y Ning",
        "L Cai"
      ],
      "year": "2016",
      "venue": "2016 IEEE International Conference on Multimedia and Expo (ICME"
    },
    {
      "citation_id": "23",
      "title": "The pmemo dataset for music emotion recognition",
      "authors": [
        "K Zhang",
        "H Zhang",
        "S Li",
        "C Yang",
        "L Sun"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 acm on international conference on multimedia retrieval"
    },
    {
      "citation_id": "24",
      "title": "Places205-vggnet models for scene recognition",
      "authors": [
        "L Wang",
        "S Guo",
        "W Huang",
        "Y Qiao"
      ],
      "year": "2015",
      "venue": "Places205-vggnet models for scene recognition",
      "arxiv": "arXiv:1508.01667"
    },
    {
      "citation_id": "25",
      "title": "Comparison and analysis of deep audio embeddings for music emotion recognition",
      "authors": [
        "E Koh",
        "S Dubnov"
      ],
      "year": "2021",
      "venue": "Comparison and analysis of deep audio embeddings for music emotion recognition",
      "arxiv": "arXiv:2104.06517"
    },
    {
      "citation_id": "26",
      "title": "Music emotion recognition by using chroma spectrogram and deep visual features",
      "authors": [
        "M Er",
        "I Aydilek"
      ],
      "year": "2019",
      "venue": "International Journal of Computational Intelligence Systems"
    },
    {
      "citation_id": "27",
      "title": "Recognition of emotion in music based on deep convolutional neural network",
      "authors": [
        "R Sarkar",
        "S Choudhury",
        "S Dutta",
        "A Roy",
        "S Saha"
      ],
      "year": "2020",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "28",
      "title": "Effective music emotion recognition by segment-based progressive learning",
      "authors": [
        "J.-H Su",
        "T.-P Hong",
        "Y.-H Hsieh",
        "S.-M Li"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)"
    },
    {
      "citation_id": "29",
      "title": "Squeeze-and-excitation networks",
      "authors": [
        "J Hu",
        "L Shen",
        "G Sun"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "30",
      "title": "Transfer learning for music classification and regression tasks",
      "authors": [
        "K Choi",
        "G Fazekas",
        "M Sandler",
        "K Cho"
      ],
      "year": "2017",
      "venue": "Transfer learning for music classification and regression tasks",
      "arxiv": "arXiv:1703.09179"
    },
    {
      "citation_id": "31",
      "title": "librosa: Audio and music signal analysis in python",
      "authors": [
        "B Mcfee",
        "C Raffel",
        "D Liang",
        "D Ellis",
        "M Mcvicar",
        "E Battenberg",
        "O Nieto"
      ],
      "year": "2015",
      "venue": "Proceedings of the 14th python in science conference"
    },
    {
      "citation_id": "32",
      "title": "Music emotion recognition using recurrent neural networks and pretrained models",
      "authors": [
        "J Grekow"
      ],
      "year": "2021",
      "venue": "Journal of Intelligent Information Systems"
    },
    {
      "citation_id": "33",
      "title": "Multi-scale context based attention for dynamic music emotion prediction",
      "authors": [
        "Y Ma",
        "X Li",
        "M Xu",
        "J Jia",
        "L Cai"
      ],
      "year": "2017",
      "venue": "Proceedings of the 25th ACM international conference on Multimedia"
    }
  ]
}