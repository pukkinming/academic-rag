{
  "paper_id": "2509.19965v1",
  "title": "Synchrorama : Lip-Synchronized And Emotion-Aware Talking Face Generation Via Multi-Modal Emotion Embedding",
  "published": "2025-09-24T10:21:29Z",
  "authors": [
    "Phyo Thet Yee",
    "Dimitrios Kollias",
    "Sudeepta Mishra",
    "Abhinav Dhall"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "A robot with a human-like face and head, which is adorned with gears and cogs. The robot appears to be stationary, with no discernible actions taking place.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Reference Image",
      "text": "Audio LLM Generated Description Generated Videos  Figure 1 . We propose SynchroRaMa, an expressive talking face generation framework. Given a single reference image, audio, and a textual description, our model can generate talking face videos featuring lip-synchronized, expressive facial expressions and emotional cues while maintaining identity consistency.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Talking face generation  [1, 6, 7, 18, 33, 36]  aims to animate a portrait image by integrating audio. It has gained popularity in various domains such as video games, film industries, social media, digital marketing and education sectors. Existing approaches to talking face generation primarily utilize either GANs  [8]  or diffusion models  [5, 11] . GAN-based methods  [14, 20]  use a combination of audio and visual encoders to extract features from speech and video frames, which are then processed by a generator network to produce synchronized lip movements. In contrast, diffusionbased models generate results through an iterative refinement process, leading to higher-quality and more temporally coherent outputs. Despite these improvements, generating a realistic talking face remains challenging, as it requires precise lip synchronization, and head poses with the given speech. In addition to lip synchronization, maintaining the visual coherence and capturing the richness of expression and emotion remain open challenges in talking face generation.\n\nTo address these limitations, we introduce Synchro-RaMa, a novel framework designed to generate highquality, emotionally expressive, and lip-synchronized talking faces from the audio input, textual description and a reference image. Most previous works on emotion-aware talking face generation primarily focus on a single modality, such as text, audio or visual cues for emotion embedding. However, relying on a single modality limits model performance, as each modality contains its specific constraints, and individually, none can fulfill all requirements perfectly. Therefore, our work leverages multi-modal emotion embedding by combining textual sentiment analysis, speechbased emotion recognition, and valence-arousal (VA) emotion embedding derived from audio signals.\n\nDiffusion-based talking face generation approaches typically use a reference network and denoising UNet as their backbone. Visual appearance information is fed into Reference network, which is then integrates with the Denoising UNet during the denoising process. However, providing only visual information is insufficient, as existing methods mostly use a single frame extracted from the input video as the reference image during training. Relying on a single frame to represent the entire video may fail to capture potential changes in scenes, subject's actions, and attributes in subsequent frames. As a result, the generated video may lack detailed appearance consistency. To overcome this, we incorporate textual descriptions that contain changes in scenes (temporal) information as additional input. Recent advancements in visual large language models such as Vide-oLLaMA2  [2] , can generate comprehensive textual descriptions of the entire video, capturing changes in scenes, actions and attributes. This textual information complements the detailed visual cues of the extracted reference image, enhancing the visual quality of the generated video.\n\nAnother important aspect in talking face generation is motion consistency. Generated video should exhibit realistic head movement and lip should be synchronized with the speech. To ensure this, we also introduce an audioto-motion module, producing motion frames driven by the audio. Training the model with these audio-driven motion frames guarantees realistic head movements and accurate lip synchronization. Furthermore, we introduce several loss functions in our work, including syncloss, emo loss, facial action units (AU) loss, and attr-action loss.\n\nThe main contributions of our paper can be summarized as: 1) We propose a novel talking face generation framework, which leverages multi-modal information, including visual, textual and audio data. 2) We introduce a multi-modal emotion embedding module to enrich emotional expressiveness in the generated videos. 3) We embed LLM generated scene description to make the generation better aligned to context. 4) We present an Audio-to-Motion (A2M) module, designed to generate realistic motion frames synchronized with the audio. Our quantitative and qualitative experiments demonstrate that our approach achieves superior performance compared to the state-of-theart (SOTA) methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Diffusion-Based Talking Face Generation",
      "text": "Diffusion models have recently gained significant attention in talking face generation. Hallo  [36]  leverages the Stable Diffusion Model  [25]  with the ReferenceNet to maintain appearance consistency and introduces a hierarchical audio-visual cross-attention mechanism to align audio and visual features. VASA-1  [37]  operates in a disentangled latent space to enable precise and expressive facial animations. AniTalker  [15]  use universal motion representation to capture a wide range of facial dynamics including subtle expressions and head movements. X-Portrait  [35]  employs a conditional diffusion model enhanced with a hierarchical patch-based local control module for accurate and coherent motion transfer. Diff2Lip  [19]  and DiffTalk  [26]  use Latent Diffusion Models (LDMs) conditioned on audio features, reference images, masked ground-truth images, and facial landmarks. GAIA  [10]  disentangles motion and appearance to preserve identity while enabling speech-driven motion synthesis. EchoMimic  [1]  and AniPortrait  [33]  employ concurrent training, conditioning simultaneously on audio signals and facial landmarks to generate realistic portrait animations. VividTalk  [27]  leverages a 3D hybrid prior to decompose facial expressions and head movements, using a learnable codebook for natural motion and a dual-branch Motion VAE to generate dense motion fields. EMO  [28]  utilizes Audio2Video diffusion model, integrating weak condi-tion constraints such as face locators and motion guidance, bypassing the need for intermediate 3D models. MODA  [16]  employs a mapping-once network with dual attention mechanisms to convert audio into motion representations, where one attention captures accurate lip-sync and the other models natural head and eye movements.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Method",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Model Architecture",
      "text": "Given a reference image, textual description, and input audio, our model generates a lip-synchronized talking face video while preserving natural head movements, facial expressions, emotions, and overall appearance consistency. As shown in Figure  2 , our framework integrates several critical components: Denoising UNet  [25]  as the backbone network, ReferenceNet  [12]  to encode the reference image, proposed Audio-to-Motion (A2M) module to generate the motion frames synchronized with the input audio, and proposed multi-modal emotion embedding module to align the generated video with the emotional content of the audio. The careful incorporation of these components ensures the generation of realistic and contextually coherent talking face videos. The following sections present a detailed explanation of each component.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Denoising Unet",
      "text": "In our framework, we adopt the Denoising UNet architecture from the Stable Diffusion  [25]  1.5 as the backbone network by incorporating 4 distinct attention mechanisms within each Transformer block (Figure  2(a) ). Specifically, Spatial Attention layer focuses on the important spatial regions, ensuring the model captures the relevant facial features such as mouth, eyes, and expressions while preserving the identity consistency. Audio Attention layer integrates the audio features into the generation process. Cross Attention layer bridges the multiple modalities such as reference image, audio, and textual description by dynamically aligning them throughout the denoising process. Finally, Temporal Attention layer ensures smooth and coherent motion transitions between the consecutive video frames. These integrated attention mechanisms enhance the model's ability to capture and integrate spatial and temporal relationships effectively within the generation process.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Referencenet",
      "text": "ReferenceNet employs the same architecture as the Denoising UNet and guides the generation process by embedding both reference image and the textual input (Figure  2(a) ). It ensures the preservation of facial identity and background consistency. Within the ReferenceNet, each Transformer block employs a Spatial Attention mechanism to extract features from the reference image. These features are then passed to the Denoising UNet via the corresponding Spatial Attention layers. Additionally, Cross Attention layer integrates the textual description that contains attributes and actions of the corresponding video as additional conditioning input. This provides semantic guidance to further enhance appearance consistency. This integration of visual and textual conditioning ensures identity and background consistency, as well as high-quality video generation.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Video Description",
      "text": "Previous works use one extracted frame from the original video as a reference image during training to provide appearance information (e.g., identity and background details). However, relying on a single image may fail to capture dynamic changes in actions and attributes that occur over time, e.g., a person sitting in the first frame might change position in subsequent frames -a single image cannot reflect these changes. To address this limitation, we incorporate a textual description that represents the entire video alongside the reference image. Specifically, we employ VideoLLaMA2  [2]  to generate a video description that captures fine-grained details such as attributes (e.g., \"wearing earrings\") and actions (e.g., \"turning head\"). This textual description is encoded using a CLIP Text Encoder and is incorporated into the model through cross-attention layers in the ReferenceNet and the Denoising UNet (Figure  2 (a)). During training, ReferenceNet processes the reference image and textual description to extract visual-textual appearance features, which are integrated into the Denoising UNet, then fused with emotion and audio features. This conditioning strategy ensures that high-level semantic cues from text influence both attributes and actions throughout the video.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Audio-To-Motion (A2M) Module",
      "text": "The main challenge in audio-driven motion generation is to generate temporally consistent and expressive motion sequences that align with the input audio. To address this, we propose a VAE-based Audio-to-Motion module, which takes both a reference image and audio features as input and generates motion frames (Figure  2(b) ). We first extract audio features using wav2vec 2.0, and use them as conditional inputs to the VAE encoder. In VAE, the encoder typically outputs parameters (µ, σ) that define a Gaussian distribution, from which a latent variable z q is sampled as z q ∼ N (0, 1). However, this Gaussian prior tends to force the posterior toward the mean, which limits output diversity and generative power. To overcome this, we follow  [24, 38, 39]  and introduce a Volume-Preserving Normalizing Flow (VP-Flow) that transforms the simple latent variable z q into a more expressive latent representation z p (i.e, z p = f V P -F low (z q )) (Figure  2(c) ). The VP-Flow is composed of a stack of residual affine coupling layers and channel-wise flip operations, which ensure invertibility and maintain the volume of the latent space during transformation. We then pass the transformed latent variable z p , together with the audio features to the VAE decoder, which then synthesizes a sequence of visual motion frames that are aligned with the input audio. Therefore, VP-Flow-based conditioning in our A2M module enhances the diversity and realism of generated motion, while the audio features ensure accurate lip sync and smooth transitions across frames.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Multi-Modal Emotion Embedding Module",
      "text": "Emotion embedding is an important aspect in talking face generation to provide more realistic and expressive facial movements aligned with the driven audio. To achieve this, we introduce a multi-modal emotion embedding mechanism that integrates emotional cues from three modalities -text, audio, and valence-arousal (VA) features (Figure  2 (a)), providing a richer perspective than any single modality alone. First, we transcribe the input audio using Whisper  [23]  and perform sentiment analysis on the resulting text using emotion-english-distilroberta  [9] . Second, we perform speech emotion recognition (SER) on the audio using  [22] . Third, we extract valence and arousal features from the audio using a fine-tuned wav2vec 2.0  [29] . To ensure accurate extraction of valence-arousal features, we remove background music and divide the audio into 50% overlapping segments to capture temporal variations. The features from each segment are then concatenated to form the final representation. Let E t , E ser , and E va denote emotion embeddings from textual sentiment analysis, SER, and VA features, respectively, computed as E t = f Transcription (a), E ser = f SER (a), E va = f VA (a). These embeddings are concatenated to form the final emotion embedding:\n\nThis combined embedding is fed into the denoising UNet via a cross-attention layer. By employing multiple modal-ities, our method captures a broader range of emotional nuances, resulting in a more expressive talking face that closely aligns with the speaker's intended emotion.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Loss Functions",
      "text": "Alongside standard diffusion loss, we introduce auxiliary losses to ensure that the generated video maintains temporal coherence, emotional expressiveness, identity consistency, and semantic alignment with the input modalities. Sync Loss. This loss enforces precise temporal alignment between generated lip movements and input audio, which is critical for perceptual realism in talking face. We evaluate synchronization using  [13] , which estimates the temporal offset between audio and video, and detects the location of misalignment. The loss compares the global temporal alignment signals, which are more robust to the local jitter and better reflect the human perception. It is defined as:\n\nwhere: ∆t p and ∆t gt are the predicted and ground truth temporal offset magnitudes between the audio and generated video, and t p and t gt are corresponding timestamps where misalignment occurs.\n\nEmo Loss. This loss enhances emotionally consistent facial expressions by aligning the temporal emotional dynamics of the generated video with those of the ground truth. Specifically, it encourages the output to reflect similar variations in valence and arousal over time. To compute it, we first remove the background music and divide the audio (from both generated and ground truth videos) into 50% overlapping segments. Employing overlapping segments ensures smoother emotional transitions and reduces sudden shifts or potential artifacts that may occur from non-overlapping segments. For each segment, we extract valence and arousal values using a transformer-based model  [29] , a fine-tuned variant of wav2vec 2.0 that predicts continuous emotional dimensions (valence and arousal) directly from the audio. This yields a sequence of valence and arousal pairs that captures the evolving emotional state throughout the video. The loss provides fine-grained, temporally-aware supervision, enhancing the emotional expressiveness and coherence of the generated facial behavior. The loss is defined as:\n\nwhere: K is the number of audio segments; v gt are the VA values of the audio of generated video and of ground truth video for segment k. Facial Action Unit (AU) Loss. This loss provides finegrained supervision to enhance the expressiveness and realism of the generated videos. It enables the model to accurately capture subtle facial muscle movements, such as eyebrow raises or lip stretches, which are often overlooked by global emotion descriptors such as valence-arousal values. This targeted supervision helps improve the semantic accuracy and coherence of facial expressions, ensuring that localized facial actions align with the intended emotion. Furthermore, AU loss promotes the temporal and spatial consistency across video frames. It also complements the multi-modal emotion embedding by refining the local expression details, while the embedding captures the overall emotional tone. AU loss is computed as the squared L2 distance between the predicted AUs of the generated video and the ground truth AUs from the original video. Formally, it is defined as:\n\nwhere: T is the number of frames; N is the number of AUs; AU (t) p,i is the i-th AU prediction at frame t; AU (t) gt,i is i-th AU ground truth at frame t. Attr-Action Loss This loss ensures that the generated video preserves the high-level semantic attributes and actions described in the original video, e.g., head movements, or appearance cues. We generate the textual descriptions for both predicted and ground truth videos using VideoLLaMA2  [2] , and compute the loss as the cosine distance between their textual embeddings:\n\nwhere: e p and e gt are the textual embeddings of the predicted and ground truth video, respectively; cos(•, •) denotes the cosine similarity between the two embedding vectors.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments",
      "text": "Implementation Details. We conduct all experiments on NVIDIA A100 GPUs. The model is trained in two stages, each for 30k steps, with batch size 4 and resolution 512x512. In the first stage, we train the model to encode appearance information using a reference image and its corresponding textual description as inputs to the Refer-enceNet. Each training sample consists of a 14-frame video clip, from which one frame is randomly selected as the reference frame and another as the target frame. Datasets. We train our model using VFHQ  [34] , HDTF  [41] , and a selection of in-the-wild scene clips shared by Hallo3  [3] . To enhance training data quality: all videos are resized to 512x512 resolution; we filter out extreme sideprofile views by detecting facial landmarks with MediaPipe  [17]  (ensuring clarity in the lip region); videos containing multiple speakers are excluded (maintaining speaker identity consistency). In the end, we obtain around 80 hours of video data, with individual clips ranging from 3 to 20 seconds in length. All videos are standardized to 25 fps to ensure temporal consistency across samples. Audio tracks are resampled to 16 kHz and normalized. When present, background music and noise are removed, as they can negatively impact emotion recognition and audio-to-motion alignment.\n\nFor evaluation, we use 100 videos from each of HDTF  [41]  and emotion-aware MEAD  [31] . Evaluation Metrics. We employ several metrics to evaluate the performance of our model. To assess the similarity between the generated and ground truth images, we compute PSNR, SSIM  [32] , and LPIPS  [40] . Additionally, we use FID and FVD to measure how closely the generated data matches that of the actual data. To evaluate the accuracy of facial expressions, we compute: (i) Expression-FID (E-FID) for AUs (following  [1, 28]  -expression parameters are extracted using face reconstruction model  [4] ); (ii) F1 score for AUs; (iii) Concordance Correlation Coefficient (CCC) for Valence (CCC V ) and Arousal (CCC A ). Following  [16] , we adopt the confidence score from SyncNet (Sync)  [21]  to measure audio-visual synchronization.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Comparison With State-Of-The-Art Methods",
      "text": "Quantitative Comparison. We perform quantitative comparison with several SOTA, Hallo  [36] , Aniportrait  [33] , Echomimic  [1]  and VExpress  [30]  on the HDTF and MEAD datasets. We evaluate all methods using their publicly available checkpoints. As shown in Table  1 , our approach outperforms existing methods across all metrics, except for sync confidence on the HDTF and MEAD datasets, and FVD on MEAD. In terms of image-level quality, our method achieves better FID, PSNR, SSIM, and LPIPS scores compared to the SOTA approaches. According to E-FID, F1 scores of AUs, CCC V and CCC A , our method outperforms competing methods in both expression and emotion preservation. Better performance across these metrics indicates that the generated face videos contain appro- priate expressions and emotions, which are reflected in the input audio and reference image. The lower FVD score demonstrates that our approach achieves better video quality. Additionally, our lip-synchronization performance is comparable to that of VExpress and Hallo. Notably, despite using less training data than the other methods, our approach achieves better results across various metrics. Qualitative Comparison. We perform a qualitative comparison of our proposed method with SOTA approaches  [1, 30, 33, 36]  on both the HDTF and MEAD datasets. To evaluate the comparison, we extract frames from the original videos and use the first frame as the reference image. The corresponding audio is also extracted and use it as the driving audio. Hallo, Echomimic, and VExpress achieve accurate lip sync on both datasets. However, in Figure  3 , Hallo produces artifacts in some frames (e.g., a black region around the teeth and unrealistic teeth), while Echomimic exhibits inconsistent motion between frames and unnatural lip (e.g., teeth appearing on the lips). VExpress often fail to generate the correct pose (Figure  3 , 4), does not maintain identity (Figure  3 ) and introduces excessive motion. Aniportrait struggles with lip sync accuracy (Figure  3 , 4), showing minimal or indistinct lip movements across frames. In contrast, our approach generates talking face videos with good overall quality, natural lip movements, and consistent identity. Furthermore, by incorporating emotional awareness in addition to lip synchronization, our model generates videos that are both realistic and emotionally expressive. As shown in Figure  4 , our method generates emotions that appear more realistic and better aligned with the ground truth than those generated by SOTA approaches. User Study. We conduct a user study to further evaluate the quality of videos generated by our method and by other SOTA methods  [1, 30, 33, 36] . All videos in the study include a balanced representation of genders, with varied ages, poses, and expressions. The study contained 20 participants, all of whom are Master's or PhD students with a background in Computer Science. Each participant is shown videos generated from the same image and audio inputs across all methods. They are then asked to rate each video on a scale from 1 to 5 based on lip sync, motion diversity, video smoothness, and overall naturalness. We collect the ratings and compute the average percentage score for each method. The results are presented in Table  2 .\n\nParticipants give higher ratings to our approach in terms of video quality, naturalness and motion consistency. Our performance in lip sync is comparable to that of other methods, which is consistent with the findings presented in Table  1 .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Ablation Studies",
      "text": "We perform the following ablation studies to evaluate contribution of different components of our method: Multi-modal Emotion Embedding Module. To evaluate the effectiveness of multi-modal emotion embedding module, we perform an experiment (Table  5 ) by excluding this component. Without the module, the model fails to generate appropriate emotional facial expressions that correspond to audio cues (Figure  5 , top-left). In contrast, when it is included, the model accurately captures and generates the intended emotional expressions based on the driving audio (Figure  5 , bottom-left). This is because our multi-modal   emotion embedding extracts emotional cues from multiple perspectives, including sentiment analysis, speech emotion recognition, and valence-arousal of the audio. Audio-to-Motion (A2M) Module. When we omit the A2M module and train our model using only the original video frames, the generated video fails to provide correct synchronization and lacks the necessary subject movements (Figure  5 , top-middle). By adding the motion frames driven by audio and training our model on these audio-aware frames, we ensure accurate lip movements. Based on FVD and Sync scores (Table  3 ), our approach provides better video quality while maintaining the accurate lip synchronization. LLM based Textual Integration. We evaluate our model without integrating the textual input, relying mainly on appearance cues from the reference image. Without additional semantic guidance from text, the visual quality of the generated video degrades, although lip sync remains unaffected. The model generates inconsistent facial attributes, such as noticeable artifacts in makeup near the eyes and distortions in lip shape (Figure  5 , top-right). Table  4  shows the quantitative results with and without textual integration. Emo Loss. Although the multi-modal emotion embedding module enables the model to generate emotional expressions, we found that training the model without emo loss results in less expressive and less accurate facial emotion (Figure  6 , top-left). By including the emo loss, which aligns the model's output with valence and arousal cues derived from audio, the generated expressions become more expressive and realistic (Figure  6 , bottom-left). The results in Table 5 validate the effectiveness of the emo loss in our model. AU Loss. When we exclude the AU loss, the generated faces exhibit limited facial actions; for example, lip region appears flat and expressionless, and eyes remain closed despite the emotional context (Figure  6 , top-right). With AU loss, the model is able to generate fine-grained facial movements, such as the appearance of expression lines near the mouth and properly opened eyes, even when the reference image shows closed eyes (Figure  6 , bottom-right). The results in Table  5  show that AU loss enhances the realism and expressiveness of the generated videos.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Conclusion, Limitations And Future Work",
      "text": "We propose a novel framework that effectively integrates multi-modal emotional nuances with audio-driven motion modules to generate high-quality, lip-synchronized talking face video. By conditioning the model on visual and textual info, we provide better visual details. Comprehensive experiments, ablation studies & user evaluations demonstrate our model outperforms SOTA. Results show that Synchro-RaMa is an effective tool for creating high-quality, emotionally rich, and lip-synchronized talking face videos. While our proposed approach achieves promising results, it has some limitations which we will address in future work. First, since our model is trained mainly on portrait images, it is currently unable to generate full-body talking videos. Additionally, because the model was trained only on English language data, its performance on other languages needs to be evaluated.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: We propose SynchroRaMa, an expressive talking face generation framework. Given a single reference image, audio, and a textual",
      "page": 1
    },
    {
      "caption": "Figure 2: , our framework integrates several",
      "page": 3
    },
    {
      "caption": "Figure 2: (a)). Specifically,",
      "page": 3
    },
    {
      "caption": "Figure 2: (a)). During training, ReferenceNet processes the refer-",
      "page": 3
    },
    {
      "caption": "Figure 2: (b)). We first extract",
      "page": 3
    },
    {
      "caption": "Figure 2: (c)). The VP-Flow is",
      "page": 3
    },
    {
      "caption": "Figure 2: (a) Overall architecture of SynchroRaMa. The framework takes a reference image, LLM-generated textual description, and",
      "page": 4
    },
    {
      "caption": "Figure 2: (a)), providing a richer perspective than any single modal-",
      "page": 4
    },
    {
      "caption": "Figure 3: Qualitative comparison with the SOTA approaches on",
      "page": 6
    },
    {
      "caption": "Figure 4: Qualitative comparison with the SOTA approaches on",
      "page": 7
    },
    {
      "caption": "Figure 3: , 4), does not maintain",
      "page": 7
    },
    {
      "caption": "Figure 3: ) and introduces excessive motion. Ani-",
      "page": 7
    },
    {
      "caption": "Figure 3: , 4), show-",
      "page": 7
    },
    {
      "caption": "Figure 4: , our method generates emotions that ap-",
      "page": 7
    },
    {
      "caption": "Figure 5: , top-left). In contrast, when it is",
      "page": 7
    },
    {
      "caption": "Figure 5: , bottom-left). This is because our multi-modal",
      "page": 7
    },
    {
      "caption": "Figure 5: Effects of Multi-modal Emotion Embedding (left), Audio-to-Motion (A2M) (middle) and Textual Integration (right).",
      "page": 8
    },
    {
      "caption": "Figure 6: Effects of Emo Loss (left) and AU Loss (right).",
      "page": 8
    },
    {
      "caption": "Figure 5: , top-middle). By adding the motion frames driven by au-",
      "page": 8
    },
    {
      "caption": "Figure 5: , top-right). Table 4 shows the quanti-",
      "page": 8
    },
    {
      "caption": "Figure 6: , top-left). By including the emo loss, which aligns",
      "page": 8
    },
    {
      "caption": "Figure 6: , bottom-left). The results in Ta-",
      "page": 8
    },
    {
      "caption": "Figure 6: , top-right). With AU",
      "page": 8
    },
    {
      "caption": "Figure 6: , bottom-right). The re-",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Attribute + Action\n(a) Architecture of SynchroRaMa \nA cartoon boy with brown hair and\nbrown eyes. He is wearing a yellow\nshirt. He is smiling and looking\nReference Net\nMotion Frames\ndirectly at the camera.\nVAE\nEncoder\nLLM\nA2M Module\nCLIP\nInput Video\nDecoder\nNoisy latents\nReference image\nDenoising Unet\nAudio\nAudio\nWav2Vec\nProjection\nGenerated Video\nSentiment\nSpatial Attention\nTranscription\nAnalysis\nCross Attention\nSyncLoss\nAULoss\nSER\nAudio Attention\nTemporal Attention\nAttr-Action\nVA Net \nEmoLoss\nLoss\nTunning\nFrozen\nMulti-modal Emotion Embedding": "",
          "Audio\nWav2Vec\nAudio Features\nImage\nVAE Encoder\nMotion Frames\nVP-Flow\nVAE Decoder\n(b) Audio-to-Motion (A2M) Module": "Flip\nx N\nResidual\nCoupling Layer\n(c) VP-Flow\n (Volume Preserving Normalization Flow)"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Echomimic: Lifelike audio-driven portrait animations through editable landmark conditions",
      "authors": [
        "Zhiyuan Chen",
        "Jiajiong Cao",
        "Zhiquan Chen",
        "Yuming Li",
        "Chenguang Ma"
      ],
      "year": "2024",
      "venue": "Echomimic: Lifelike audio-driven portrait animations through editable landmark conditions",
      "arxiv": "arXiv:2407.08136"
    },
    {
      "citation_id": "2",
      "title": "Videollama 2: Advancing spatialtemporal modeling and audio understanding in video-llms",
      "authors": [
        "Zesen Cheng",
        "Sicong Leng",
        "Hang Zhang",
        "Yifei Xin",
        "Xin Li",
        "Guanzheng Chen",
        "Yongxin Zhu",
        "Wenqi Zhang",
        "Ziyang Luo",
        "Deli Zhao"
      ],
      "year": "2024",
      "venue": "Videollama 2: Advancing spatialtemporal modeling and audio understanding in video-llms",
      "arxiv": "arXiv:2406.07476"
    },
    {
      "citation_id": "3",
      "title": "Hallo3: Highly dynamic and realistic portrait image animation with diffusion transformer networks",
      "authors": [
        "Jiahao Cui",
        "Hui Li",
        "Yun Zhan",
        "Hanlin Shang",
        "Kaihui Cheng",
        "Yuqi Ma",
        "Shan Mu",
        "Hang Zhou",
        "Jingdong Wang",
        "Siyu Zhu"
      ],
      "year": "2024",
      "venue": "Hallo3: Highly dynamic and realistic portrait image animation with diffusion transformer networks",
      "arxiv": "arXiv:2412.00733"
    },
    {
      "citation_id": "4",
      "title": "Accurate 3d face reconstruction with weakly-supervised learning: From single image to image set",
      "authors": [
        "Yu Deng",
        "Jiaolong Yang",
        "Sicheng Xu",
        "Dong Chen",
        "Yunde Jia",
        "Xin Tong"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops"
    },
    {
      "citation_id": "5",
      "title": "Advances in neural information processing systems",
      "authors": [
        "Prafulla Dhariwal",
        "Alexander Nichol"
      ],
      "year": "2021",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "6",
      "title": "Megaportraits: One-shot megapixel neural head avatars",
      "authors": [
        "Nikita Drobyshev",
        "Jenya Chelishev",
        "Taras Khakhulin",
        "Aleksei Ivakhnenko",
        "Victor Lempitsky",
        "Egor Zakharov"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "7",
      "title": "Stavros Petridis, and Maja Pantic. Emoportraits: Emotion-enhanced multimodal one-shot head avatars",
      "authors": [
        "Nikita Drobyshev",
        "Antoni Bigata Casademunt",
        "Konstantinos Vougioukas",
        "Zoe Landgraf"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "8",
      "title": "Generative adversarial networks",
      "authors": [
        "Ian Goodfellow",
        "Jean Pouget-Abadie",
        "Mehdi Mirza",
        "Bing Xu",
        "David Warde-Farley",
        "Sherjil Ozair",
        "Aaron Courville",
        "Yoshua Bengio"
      ],
      "year": "2020",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "9",
      "title": "Emotion english distilroberta-base",
      "authors": [
        "Jochen Hartmann"
      ],
      "venue": "Emotion english distilroberta-base"
    },
    {
      "citation_id": "10",
      "title": "Zeroshot talking avatar generation",
      "authors": [
        "Tianyu He",
        "Junliang Guo",
        "Runyi Yu",
        "Yuchi Wang",
        "Jialiang Zhu",
        "Kaikai An",
        "Leyi Li",
        "Xu Tan",
        "Chunyu Wang",
        "Han Hu",
        "Hsiangtao Wu",
        "Sheng Zhao",
        "Jiang Bian"
      ],
      "year": "2024",
      "venue": "Zeroshot talking avatar generation"
    },
    {
      "citation_id": "11",
      "title": "Denoising diffusion probabilistic models",
      "authors": [
        "Jonathan Ho",
        "Ajay Jain",
        "Pieter Abbeel"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "12",
      "title": "Animate anyone: Consistent and controllable imageto-video synthesis for character animation",
      "authors": [
        "Li Hu"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "13",
      "title": "Synchformer: Efficient synchronization from sparse cues",
      "authors": [
        "Vladimir Iashin",
        "Weidi Xie"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "14",
      "title": "Towards automatic face-to-face translation",
      "authors": [
        "K Prajwal",
        "Rudrabha Mukhopadhyay",
        "Jerin Philip",
        "Abhishek Jha",
        "Vinay Namboodiri",
        "Jawahar"
      ],
      "year": "2019",
      "venue": "Proceedings of the 27th ACM international conference on multimedia"
    },
    {
      "citation_id": "15",
      "title": "Anitalker: animate vivid and diverse talking faces through identity-decoupled facial motion encoding",
      "authors": [
        "Tao Liu",
        "Feilong Chen",
        "Shuai Fan",
        "Chenpeng Du",
        "Qi Chen",
        "Xie Chen",
        "Kai Yu"
      ],
      "year": "2024",
      "venue": "Proceedings of the 32nd ACM International Conference on Multimedia"
    },
    {
      "citation_id": "16",
      "title": "Moda: Mapping-once audio-driven portrait animation with dual attentions",
      "authors": [
        "Yunfei Liu",
        "Lijian Lin",
        "Fei Yu",
        "Changyin Zhou",
        "Yu Li"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "17",
      "title": "Mediapipe: A framework for building perception pipelines",
      "authors": [
        "Camillo Lugaresi",
        "Jiuqiang Tang",
        "Hadon Nash",
        "Chris Mc-Clanahan",
        "Esha Uboweja",
        "Michael Hays",
        "Fan Zhang",
        "Chuo-Ling Chang",
        "Ming Yong",
        "Juhyun Lee",
        "Wan-Teh Chang",
        "Wei Hua",
        "Manfred Georg",
        "Matthias Grundmann"
      ],
      "year": "2019",
      "venue": "Mediapipe: A framework for building perception pipelines"
    },
    {
      "citation_id": "18",
      "title": "Follow-your-emoji: Fine-controllable and expressive freestyle portrait animation",
      "authors": [
        "Yue Ma",
        "Hongyu Liu",
        "Hongfa Wang",
        "Heng Pan",
        "Yingqing He",
        "Junkun Yuan",
        "Ailing Zeng",
        "Chengfei Cai",
        "Heung-Yeung Shum",
        "Wei Liu"
      ],
      "year": "2024",
      "venue": "SIGGRAPH Asia 2024 Conference Papers"
    },
    {
      "citation_id": "19",
      "title": "Diff2lip: Audio conditioned diffusion models for lip-synchronization",
      "authors": [
        "Soumik Mukhopadhyay",
        "Saksham Suri",
        "Ravi Teja Gadde",
        "Abhinav Shrivastava"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "20",
      "title": "A lip sync expert is all you need for speech to lip generation in the wild",
      "authors": [
        "Rudrabha Kr Prajwal",
        "Mukhopadhyay",
        "P Vinay",
        "Namboodiri",
        "Jawahar"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM international conference on multimedia"
    },
    {
      "citation_id": "21",
      "title": "A lip sync expert is all you need for speech to lip generation in the wild",
      "authors": [
        "Rudrabha K R Prajwal",
        "Mukhopadhyay",
        "P Vinay",
        "C Namboodiri",
        "Jawahar"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "22",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "year": "2022",
      "venue": "Robust speech recognition via large-scale weak supervision"
    },
    {
      "citation_id": "23",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "24",
      "title": "Portaspeech: Portable and high-quality generative text-to-speech",
      "authors": [
        "Yi Ren",
        "Jinglin Liu",
        "Zhou Zhao"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "25",
      "title": "High-resolution image synthesis with latent diffusion models",
      "authors": [
        "Robin Rombach",
        "Andreas Blattmann",
        "Dominik Lorenz",
        "Patrick Esser",
        "Björn Ommer"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "26",
      "title": "Difftalk: Crafting diffusion models for generalized audio-driven portraits animation",
      "authors": [
        "Shuai Shen",
        "Wenliang Zhao",
        "Zibin Meng",
        "Wanhua Li",
        "Zheng Zhu",
        "Jie Zhou",
        "Jiwen Lu"
      ],
      "year": "1982",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "27",
      "title": "Vividtalk: One-shot audio-driven talking head generation based on 3d hybrid prior",
      "authors": [
        "Xusen Sun",
        "Longhao Zhang",
        "Hao Zhu",
        "Peng Zhang",
        "Bang Zhang",
        "Xinya Ji",
        "Kangneng Zhou",
        "Daiheng Gao",
        "Liefeng Bo",
        "Xun Cao"
      ],
      "year": "2023",
      "venue": "Vividtalk: One-shot audio-driven talking head generation based on 3d hybrid prior",
      "arxiv": "arXiv:2312.01841"
    },
    {
      "citation_id": "28",
      "title": "Emo: Emote portrait alive generating expressive portrait videos with audio2video diffusion model under weak conditions",
      "authors": [
        "Linrui Tian",
        "Qi Wang",
        "Bang Zhang",
        "Liefeng Bo"
      ],
      "year": "2024",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "29",
      "title": "Dawn of the transformer era in speech emotion recognition: closing the valence gap",
      "authors": [
        "Johannes Wagner",
        "Andreas Triantafyllopoulos",
        "Hagen Wierstorf",
        "Maximilian Schmitt",
        "Felix Burkhardt",
        "Florian Eyben",
        "Björn Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "30",
      "title": "V-express: Conditional dropout for progressive training of portrait video generation",
      "authors": [
        "Cong Wang",
        "Kuan Tian",
        "Jun Zhang",
        "Yonghang Guan",
        "Feng Luo",
        "Fei Shen",
        "Zhiwei Jiang",
        "Qing Gu",
        "Xiao Han",
        "Wei Yang"
      ],
      "year": "2024",
      "venue": "V-express: Conditional dropout for progressive training of portrait video generation",
      "arxiv": "arXiv:2406.02511"
    },
    {
      "citation_id": "31",
      "title": "Mead: A large-scale audio-visual dataset for emotional talking-face generation",
      "authors": [
        "Kaisiyuan Wang",
        "Qianyi Wu",
        "Linsen Song",
        "Zhuoqian Yang",
        "Wayne Wu",
        "Chen Qian",
        "Ran He",
        "Yu Qiao",
        "Chen Loy"
      ],
      "year": "2020",
      "venue": "European conference on computer vision"
    },
    {
      "citation_id": "32",
      "title": "Image quality assessment: from error visibility to structural similarity",
      "authors": [
        "Zhou Wang",
        "Alan Bovik",
        "Hamid Sheikh",
        "Eero Simoncelli"
      ],
      "year": "2004",
      "venue": "IEEE transactions on image processing"
    },
    {
      "citation_id": "33",
      "title": "Aniportrait: Audio-driven synthesis of photorealistic portrait animation",
      "authors": [
        "Huawei Wei",
        "Zejun Yang",
        "Zhisheng Wang"
      ],
      "year": "2024",
      "venue": "Aniportrait: Audio-driven synthesis of photorealistic portrait animation",
      "arxiv": "arXiv:2403.17694"
    },
    {
      "citation_id": "34",
      "title": "Vfhq: A high-quality dataset and benchmark for video face super-resolution",
      "authors": [
        "Liangbin Xie",
        "Xintao Wang",
        "Honglun Zhang",
        "Chao Dong",
        "Ying Shan"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "35",
      "title": "X-portrait: Expressive portrait animation with hierarchical motion attention",
      "authors": [
        "You Xie",
        "Hongyi Xu",
        "Guoxian Song",
        "Chao Wang",
        "Yichun Shi",
        "Linjie Luo"
      ],
      "year": "2024",
      "venue": "ACM SIGGRAPH 2024 Conference Papers"
    },
    {
      "citation_id": "36",
      "title": "Hierarchical audio-driven visual synthesis for portrait image animation",
      "authors": [
        "Mingwang Xu",
        "Hui Li",
        "Qingkun Su",
        "Hanlin Shang",
        "Liwei Zhang",
        "Ce Liu",
        "Jingdong Wang",
        "Yao Yao",
        "Siyu Zhu",
        "Hallo"
      ],
      "year": "2024",
      "venue": "Hierarchical audio-driven visual synthesis for portrait image animation",
      "arxiv": "arXiv:2406.08801"
    },
    {
      "citation_id": "37",
      "title": "Vasa-1: Lifelike audio-driven talking faces generated in real time",
      "authors": [
        "Sicheng Xu",
        "Guojun Chen",
        "Yu-Xiao Guo",
        "Jiaolong Yang",
        "Chong Li",
        "Zhenyu Zang",
        "Yizhong Zhang",
        "Xin Tong",
        "Baining Guo"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "38",
      "title": "Geneface++: Generalized and stable realtime audio-driven 3d talking face generation",
      "authors": [
        "Zhenhui Ye",
        "Jinzheng He",
        "Ziyue Jiang",
        "Rongjie Huang",
        "Jiawei Huang",
        "Jinglin Liu",
        "Yi Ren",
        "Xiang Yin",
        "Zejun Ma",
        "Zhou Zhao"
      ],
      "year": "2023",
      "venue": "Geneface++: Generalized and stable realtime audio-driven 3d talking face generation",
      "arxiv": "arXiv:2305.00787"
    },
    {
      "citation_id": "39",
      "title": "Geneface: Generalized and highfidelity audio-driven 3d talking face synthesis",
      "authors": [
        "Zhenhui Ye",
        "Ziyue Jiang",
        "Yi Ren",
        "Jinglin Liu",
        "Jinzheng He",
        "Zhou Zhao"
      ],
      "year": "2023",
      "venue": "Geneface: Generalized and highfidelity audio-driven 3d talking face synthesis",
      "arxiv": "arXiv:2301.13430"
    },
    {
      "citation_id": "40",
      "title": "The unreasonable effectiveness of deep features as a perceptual metric",
      "authors": [
        "Richard Zhang",
        "Phillip Isola",
        "Alexei Efros",
        "Eli Shechtman",
        "Oliver Wang"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "41",
      "title": "Flow-guided one-shot talking face generation with a high-resolution audio-visual dataset",
      "authors": [
        "Zhimeng Zhang",
        "Lincheng Li",
        "Yu Ding",
        "Changjie Fan"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    }
  ]
}