{
  "paper_id": "2201.05782v1",
  "title": "A Novel Multi-Task Learning Method For Symbolic Music Emotion Recognition",
  "published": "2022-01-15T07:45:10Z",
  "authors": [
    "Jibao Qiu",
    "C. L. Philip Chen",
    "Tong Zhang"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Symbolic Music Emotion Recognition(SMER) is to predict music emotion from symbolic data, such as MIDI and MusicXML. Previous work mainly focused on learning better representation via (mask) language model pre-training but ignored the intrinsic structure of the music, which is extremely important to the emotional expression of music. In this paper, we present a simple multi-task framework for SMER, which incorporates the emotion recognition task with other emotion-related auxiliary tasks derived from the intrinsic structure of the music. The results show that our multi-task framework can be adapted to different models. Moreover, the labels of auxiliary tasks are easy to be obtained, which means our multi-task methods do not require manually annotated labels other than emotion. Conducting on two publicly available datasets (EMOPIA and VGMIDI), the experiments show that our methods perform better in SMER task. Specifically, accuracy has been increased by 4.17 absolute point to 67.58 in EMOPIA dataset, and 1.97 absolute point to 55.85 in VGMIDI dataset. Ablation studies also show the effectiveness of multi-task methods designed in this paper.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition of music has attracted lots of attention in the field of music information retrieval(MIR). For a long time, the research on music emotion recognition has been mainly carried out in the audio domain  [Baume et al., 2014; Liu et al., 2018; Panda et al., 2018; Panda et al., 2020] . However, emotion recognition is less explored for music from symbolic data, such as MIDI and MusicXML formats. Thanks to the rapid development of the symbolic music generation  [Yang et al., 2017; Huang et al., 2019; Huang and Yang, 2020] , more and more research focuses on symbolic music understanding  [Zeng et al., 2021; Chou et al., 2021] , including symbolic music emotion recognition(SMER).\n\nRecently, researches in SMER mainly focused on learning better representation from large-scale unlabeled music pieces via pre-training model by masked or non-masked language model borrowed from NLP and then fine-tuning the pre-trained model directly for emotion recognition on a small dataset. However, simply employing such techniques from NLP may lack the understanding of music structure which is critical to emotion classification for symbolic data  [Zeng et al., 2021] .\n\nThe existing psychology and music theory literature have revealed the relationship between music structure and emotion.  Kaster [1990]  has demonstrated that positive emotion is related to listened music in major keys, while negative emotion is related to minor keys. Similar results can be found in  [Gerardi and Gerken, 1995; Gregory et al., 1996; Dalla Bella et al., 2001] .  Livingstone[2010]  found that the loudness of music can greatly affect the expression of emotion. However, the loudness is measured in the audio domain and is still an open problem to measure it in the symbolic domain.  Adli[2007]  has demonstrated that there is a linear relationship between the velocity in the symbolic domain and the loudness in the audio domain, which means that there is a connection between the velocity of music and emotion.\n\nRecognizing the importance of musical structure for emotion recognition, we present a simple framework called MT-SMNN that incorporates the emotion recognition task with other emotion-related auxiliary tasks derived from the intrinsic structure of the music. By combining the key classification and velocity classification tasks, MT-SMNN based models can better understand emotion classification. Although MT-SMNN is a multi-task framework, we only need the manually annotated emotion label because the velocity label can be extracted directly from symbolic data, and the key label can be obtained by the well-received  Krumhansl-Kessler algorithm[2001] , which means the proposed framework can be applied to all emotion-labeled symbolic music datasets.\n\nWe combine the MT-SMNN framework with existing models and evaluate them in both EMOPIA and VG-MIDI datasets. Results demonstrate that our proposed MT-SMNN based models achieve the new state-of-the-art on both datasets.\n\nThe chief contributions of this paper can be summarized as following aspects:\n\n• We present a novel multi-task framework called MT-SMNN, mainly focusing on emotion recognition for symbolic music. In addition to emotion recognition, a better understanding of the structure of music is also taken into account in this framework.\n\n• We propose two types of auxiliary tasks for SMER. Results show that both tasks can improve the performance of SMER, especially in the valence dimension.\n\n• MT-SMNN based models achieve new state-of-theart results due to the powerful ability to learn better emotion-based knowledge from auxiliary tasks.\n\n• We have reproduced most previous work for symbolic music emotion recognition on both exiting public available datasets which is helpful to building benchmarks.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "We divide previous work on symbolic music emotion recognition into the following two categories. compared the audio, lyric , and MIDI modal of the same music, finding that MIDI modal features performed better than audio modal features in emotion recognition. Specifically, 112 types of high-level musical features were extracted from MIDI files using the JSymbolic library  [McKay and Fujinaga, 2006] , and then SVM was employed to classify the data. Similarly,  Panda et al.[2013]  extracted 320 types of features from MIDI files using multiple tools and then classified them using SVM as well.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Machine",
      "text": "Deep Learning based Methods: In recent years, it has become a trend to encode symbolic music into MIDI-like musical representation  [Oore et al., 2020; Huang and Yang, 2020; Hsiao et al., 2021]  and then employ deep learning models to classify emotion. With encoding MIDI files into MIDI-like sequences, Ferreira  [Ferreira and Whitehead, 2019; Ferreira et al., 2020] used LSTM and GPT2[Radford et al., 2019]  for emotion classification. For simplicity, in the following, we use MIDIGPT to denote the approach proposed by  [Ferreira et al., 2020] . Inspired by the great success of BERT  [Devlin et al., 2019]",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Proposed Method",
      "text": "In this section, we introduce the Multi-Task Symbolic Music Neural Network(MT-SMNN), a multi-task framework for symbolic music emotion recognition, as illustrated in Figure  1 . Below, we describe the structure of MT-SMNN in detail.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Symbolic Music Encoder",
      "text": "A piece of music from symbolic data, such as MIDI and Mu-sicXML, can be encoded as a sequence of musical events, which are so called tokens in the previous literature. Existing method to encode symbloic music can be devided to single-word representation and compound-word(CP)  [Hsiao et al., 2021]  representation. The MT-SMNN framework can use either single-word representation or compound-word representation. Without losing generality, we show a singleword representation method  (Ferreira's[2020]  method) and a compound-word representation method  [Chou et al., 2021]  as example in this part. We simply describe these two types of symbolic music encoding method below. As illustrated in Figure  2 (b), the CP representation method encodes given piece of music to a sequence of super tokens. Each super token consists of four sub-tokens: Bar, Sub-beat, Pitch and Duration. The Ferreira's method, shown in Figure  2 (c), encodes given music as token sequence X =\n\n, where x v i , x d i and x p i denotes the velocity, duration and pitch for the i-th note respectively and S denotes the length of this sequence.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Embedding Layer",
      "text": "According to the symbolic music encoding method, the embedding layer of MT-SMNN can be divided into two types. The input token is directly mapped to an embedding space for the single-word representation method. Following  [Chou et al., 2021] , for the CP representation method, the embeddings of all sub-tokens inside the super token are concatenated, then fed to a linear layer to get a complete token embedding. The token embedding is added with the corresponding position embedding to capture the position information.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Transformer-Based Feature Extractor",
      "text": "The MT-SMNN employs a transformer-based model as a feature extractor. Given input representation l 1 = {x 0 , ..., x S }, the feature extractor generates output representation l 2 = {h 0 , ..., h S }, where S means the number of input tokens, x i ∈ R E , h i ∈ R E denotes the embedding and contextual representation of i-th input token respectively, and E represents the dimension of embedding space and hidden state.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Pooler",
      "text": "Since both sequence-level and note-level tasks are employed in the MT-SMNN framework, we use a pooler to aggregate information from the entire contextual representation sequence for the sequence-level classification task. At the same time, the pooler keeps a series of contextual representations for the note-level classification task.\n\nFor simplicity, the pooler applies identical mapping for the note-level classification task. For sequence-level classification tasks, the pooler can be designed by one of the following strategies: taking the first contextual representation(like BERT  [Devlin et al., 2019] ), taking the last(like MIDIGPT  [Ferreira et al., 2020] ), or attention-based weighting average(like  MIDIBERT-Piano[Chou et al., 2021] ). In MT-SMNN, the emotion and key classification task share the same sentence representation l 3 because both are sequencelevel tasks.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Task-Specific Classification",
      "text": "In this part, we first introduce the auxiliary tasks employed by MT-SMNN. Then, we describe more details about these classification outputs. Finally, we show the multi-task loss function used by MT-SMNN.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Auxiliary Tasks",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Classification Ouputs",
      "text": "Let H = {h 0 , ..., h S } be the contextual representation(l 2 ) of given piece of music, Z be the sentence representation(l 3 ), where h i ∈ R E , Z ∈ R K , E is the dimension of hidden state , and K is the dimension of sentence representation generated by the pooler. For sequence-level tasks(emotion and key classification), the probability that given a piece of music is predicted as class c by a classifier with softmax can be formalized as:\n\nwhere φ is the mapping function of classifier, t is to distinguish between different tasks.\n\nFor the note-level task(velocity classification), the probability that the i-th note in a piece of music is predicted as class c can be formalized as:\n\nwhere φ is the mapping function of classifier, i means the i-th item in coresponding sequence.\n\nFor more detail, the classifiers consist of two fully connected layers with the ReLU activation function in the middle.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Multi-Task Loss",
      "text": "For each task, we use cross-entropy loss as its objective. Let L 1 , L 2 , and L 3 be the loss of emotion recognition, key classification, and velocity classification, respectively. We employ the adaptive loss function proposed by Liebel  [2018] . The multi-task loss is formalized following: //b i is the i-th mini-batch of dataset ; 7:\n\n1. Predict 8:\n\nPredict emotion and key for music using Eq. 1; 9:\n\nPredict velocity for each note using Eq. 2; 10:",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Compute Loss 11:",
      "text": "Compute loss for each task using cross-entropy; 12:\n\nCompute loss L (Θ) for multi-task using Eq. 3; Check for early-stopping; 18: end for where L t indicates the loss of task t, σ t a learnable parameter which controls the contribution of t-th task, and the second term is a regularizer.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "In this section, we evaluate the proposed MT-SMNN based models on EMOPIA  [Hung et al., 2021]  and VGMIDI  [Ferreira and Whitehead, 2019; Ferreira et al., 2020]  datasets. We first overview the datasets and processing procedure. Then, we describe the baselines and our proposed models(MT-MIDIBERT and MT-MIDIGPT). Finally, we show the results and analysis.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Datasets And Preprocess",
      "text": "The information of the EMOPIA and VGMIDI datasets is summarized in Table  1 . The EMOPIA dataset 1  is a dataset of pop piano music for symbolic music emotion recognition. The clips is labeled to 4 class accoding to  Russell's 4Q[Russell, 1980] . The VGMIDI dataset 2  is a dataset of video game sound-tracks formatted in MIDI. Each clip in the VGMIDI dataset is labeled as valence-arousal pair, also according to the Russell's model.\n\nFor experimental consistency, we transfer the valencearousal pair in VGMIDI to the taxonomy of Russell's 4Q as EMOPIA. The initial VGMIDI dataset has been split into a training set and a testing set. We divide a portion(about 15%) of the original training set into the validation set. In this procedure, we ensure that the clips of the validation set and the training set will not come from the same song.\n\nThe MT-SMNN need two additional labels(key and velocity) besides emotion. The velocity for each note can directly derived from symbolic data. We extract the key label via the well-received  Krumhansl-Kessler algorithm[2001]  provided by the Music21 library  [Cuthbert and Ariza, 2010] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Implementation Details",
      "text": "For the sake of fair comparison, the vast majority of previous work mentioned in Section 2 is reproduced. Our implementation is based on the PyTorch code open-sourced by HugginFace  [Wolf et al., 2019] . Below, we describe the reproduced models in detail.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Configuration Of Machine Learning Based Methods",
      "text": "In this paper, we have reproduced the machine learning based models proposed in  [Lin et al., 2013]  and  [Panda et al., 2013] . After taking the best subset of features selected in  [Lin et al., 2013]  and  [Panda et al., 2013] , the dimension of features for Lin's method and Panda's method is 521 and 135 respectively. Both methods use the SVM classifier that works with the RBF kernel.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Configuration Of Deep Learning Based Methods",
      "text": "Global Settings: The reproduced MIDIBERT-Piano  [Chou et al., 2021] , MIDIGPT  [Ferreira et al., 2020]  and our proposed MT-SMNN based methods all share the following global configuration: (a) The AdamW  [Loshchilov and Hutter, 2019]  optimizer is adopt in this paper. The β 1 , β 2 and weight decay rate is set as 0.9, 0.999 and 0.01 repectively. (b) The batch size is set as 16. (c) The learning rate is set as 3e-5 with a linear scheduler. The other trick is setting warmup steps as 500. (d) We evaluate the models every training epoch in the validation set. The model is early stopping when the macro-F1 for emotion recognition have no improvement for T consecutive epochs, where T = 0.3 * N , N denotes the number of max training epochs. The checkpoint achieving the best metric in the validation set during the training procedure is saved and evaluated in the testing set. (e) All experiments are repeated ten times with different random seeds (from 0 to 9). Specific Settings: Following  [Chou et al., 2021] , the max sequence length of MIDIBERT-Piano is set as 512. The inside BERT model adopt BERT base . We start fine-tuning the MIDIBERT-Piano model from the released pre-trained checkpoint 3  . The max sequence length of MIDIGPT is set as 1024 and 2048 to cover the entire input sequence of tokens as much as possible when experimenting with VGMIDI and EMOPIA datasets, respectively. To accommodate different max sequence lengths, we pre-trained the MIDIGPT model according to  [Ferreira et al., 2020] , with remaining other settings unchanged except the max sequence length. We finetune the models mentioned above at most 30 epochs in VGMIDI, and 100 epochs in EMOPIA with early-stopping discussed above. We coin the model that combines the proposed MT-SMNN with MIDIBERT-Piano and MIDIGPT as \"MT-MIDIBERT\" and \"MT-MIDIGPT\" respectively.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "The Training Procedure Of Mt-Smnn",
      "text": "The training procedure of MT-SMNN is shown in Algorithm 1. We start our training from pre-trained checkpoints, and then we finetune the MT-SMNN based model using multitask loss. After every training epoch, we evaluate the model and check whether early-stopping.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Comparison Of State-Of-The-Art Methods",
      "text": "We compare MT-SMNN based models with previous stateof-the-art models. The result of symbolic music emotion recognition(SMER) is shown in Table  2 . We have reproduced all these baselines in Table  2  and described them in detail in 4.2.\n\nTable  2  shows that the deep learning based models outperform the traditional machine learning based models. In addition, models that work with the proposed MT-SMNN framework perform better than the counterpart for singletask and achieve new state-of-the-art results. Specifically, Compared with the MIDIBERT-Piano model, the proposed MT-MIDIBERT model pushes the accuracy to 67.58% and 49.8%, which amounts 4.2% and 2.5% absolution improvement on the EMOPIA and VGMIDI dataset, respectively. The proposed MT-MIDIGPT model also improves the accuracy by 3.8% and 2.0% to 62.50% and 55.85% for these two datasets, respectively.\n\nSince the MT-SMNN based models have no difference except for multiple classifiers, which have a minimal amount of parameters, are employed for different tasks compared with its single-task counterpart, the improvement of the above results is attributed to our proposed MT-SMNN framework.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Ablation Studies",
      "text": "In this section, we conduct experiments on the EMOPIA dataset to study auxiliary tasks' impact. The results are summarized in Table  3 . Table  3  shows that both key and velocity classification auxiliary tasks effectively affect emotion recognition. Moreover, the model taken in both auxiliary tasks outperforms models only taken in a single. The accuracy is increased by 3.6% and 1.3% to 67.03% and 64.73% after combing the SMER task with the key and velocity classification task, respectively, which means that the key classification task is a more critical auxiliary task than the velocity classification task.\n\nWe also plot the confusion matrices of these experiments, as shown in Figure  3 . In this figure, Q1, Q2, Q3 and Q4 denotes HVHA(high valence high arousal), LVHA(low valence high arousal), LVLA(low valence low arousal) and HVLA(high valence low arousal) respectively which also socalled Happy, Angry, Sad and Calm in some literatures. Compared Figure  3 (b) with Figure  3 (a), we have found that the key classification task can greatly improve the performance of emotion recognition in the class of Q1 and Q4. Similar results can be found in Figure  3 (c) and Figure3(d). Since Q1 and Q4 are both in the high valence region, we finally conclude that our proposed MT-SMNN framework can improve the performance of music recognition, especially in the valence dimension.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we present MT-SMNN, a multi-task framework that mainly focus on emotion recognition for symbolic music. The MT-SMNN framework combines emotion recognition with key classification and velocity classification tasks and conducts a multi-task training procedure in a single dataset.  MT-SMNN based models obtain new state-of-the-art results in both EMOPIA and VGMIDI datasets. Further analysis also verifies the effectiveness of both auxiliary tasks.\n\nWe would like to apply the MT-SMNN framework to other areas for future work. For example, the MT-SMNN based models can be employed to build a metric for evaluating the performance of emotion conditioned symbolic music generation models.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Below, we describe the structure of MT-SMNN in detail.",
      "page": 2
    },
    {
      "caption": "Figure 1: The framework of proposed MT-SMNN. The text inside",
      "page": 2
    },
    {
      "caption": "Figure 2: (b), the CP representation method",
      "page": 2
    },
    {
      "caption": "Figure 2: (c), encodes given music as token sequence X =",
      "page": 2
    },
    {
      "caption": "Figure 2: (a) An example of symbolic music. (b)The CP represen-",
      "page": 3
    },
    {
      "caption": "Figure 3: In this ﬁgure, Q1, Q2, Q3 and",
      "page": 5
    },
    {
      "caption": "Figure 3: (b) with Figure 3(a), we have found that the",
      "page": 5
    },
    {
      "caption": "Figure 3: (c) and Figure3(d). Since Q1",
      "page": 5
    },
    {
      "caption": "Figure 3: The result of baseline model(MIDIBERT-Piano, abbrevi-",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "multi-task methods designed in this paper.": "",
          "SMNN that": "",
          "incorporates the emotion recognition task with": "other emotion-related auxiliary tasks derived from the intrin-"
        },
        {
          "multi-task methods designed in this paper.": "",
          "SMNN that": "",
          "incorporates the emotion recognition task with": "sic structure of the music. By combining the key classiﬁca-"
        },
        {
          "multi-task methods designed in this paper.": "1\nIntroduction",
          "SMNN that": "",
          "incorporates the emotion recognition task with": "tion and velocity classiﬁcation tasks, MT-SMNN based mod-"
        },
        {
          "multi-task methods designed in this paper.": "",
          "SMNN that": "els can better understand emotion classiﬁcation.",
          "incorporates the emotion recognition task with": "Although"
        },
        {
          "multi-task methods designed in this paper.": "Emotion recognition of music has attracted lots of attention",
          "SMNN that": "",
          "incorporates the emotion recognition task with": ""
        },
        {
          "multi-task methods designed in this paper.": "",
          "SMNN that": "",
          "incorporates the emotion recognition task with": "MT-SMNN is a multi-task framework, we only need the man-"
        },
        {
          "multi-task methods designed in this paper.": "in the ﬁeld of music information retrieval(MIR). For a long",
          "SMNN that": "",
          "incorporates the emotion recognition task with": ""
        },
        {
          "multi-task methods designed in this paper.": "",
          "SMNN that": "",
          "incorporates the emotion recognition task with": "ually annotated emotion label because the velocity label can"
        },
        {
          "multi-task methods designed in this paper.": "time,\nthe research on music emotion recognition has been",
          "SMNN that": "",
          "incorporates the emotion recognition task with": ""
        },
        {
          "multi-task methods designed in this paper.": "",
          "SMNN that": "be\nextracted directly from symbolic data,",
          "incorporates the emotion recognition task with": "and the key la-"
        },
        {
          "multi-task methods designed in this paper.": "mainly carried out\nin the audio domain[Baume et al., 2014;",
          "SMNN that": "",
          "incorporates the emotion recognition task with": ""
        },
        {
          "multi-task methods designed in this paper.": "",
          "SMNN that": "",
          "incorporates the emotion recognition task with": "bel can be obtained by the well-received Krumhansl-Kessler"
        },
        {
          "multi-task methods designed in this paper.": "Liu et al., 2018; Panda et al., 2018; Panda et al., 2020].",
          "SMNN that": "",
          "incorporates the emotion recognition task with": ""
        },
        {
          "multi-task methods designed in this paper.": "",
          "SMNN that": "",
          "incorporates the emotion recognition task with": "algorithm[2001], which means the proposed framework can"
        },
        {
          "multi-task methods designed in this paper.": "However,\nemotion recognition is\nless\nexplored for music",
          "SMNN that": "",
          "incorporates the emotion recognition task with": ""
        },
        {
          "multi-task methods designed in this paper.": "",
          "SMNN that": "be applied to all emotion-labeled symbolic music datasets.",
          "incorporates the emotion recognition task with": ""
        },
        {
          "multi-task methods designed in this paper.": "from symbolic\ndata,\nsuch\nas MIDI\nand MusicXML for-",
          "SMNN that": "",
          "incorporates the emotion recognition task with": ""
        },
        {
          "multi-task methods designed in this paper.": "",
          "SMNN that": "We\ncombine\nthe MT-SMNN framework with",
          "incorporates the emotion recognition task with": "existing"
        },
        {
          "multi-task methods designed in this paper.": "mats.\nThanks\nto the\nrapid development of\nthe\nsymbolic",
          "SMNN that": "",
          "incorporates the emotion recognition task with": ""
        },
        {
          "multi-task methods designed in this paper.": "",
          "SMNN that": "models\nand\nevaluate\nthem in\nboth",
          "incorporates the emotion recognition task with": "EMOPIA and VG-"
        },
        {
          "multi-task methods designed in this paper.": "et\net\nmusic\ngeneration[Yang\nal.,\n2017; Huang\nal.,\n2019;",
          "SMNN that": "",
          "incorporates the emotion recognition task with": ""
        },
        {
          "multi-task methods designed in this paper.": "",
          "SMNN that": "",
          "incorporates the emotion recognition task with": "MIDI datasets. Results demonstrate that our proposed MT-"
        },
        {
          "multi-task methods designed in this paper.": "Huang and Yang, 2020], more and more research focuses",
          "SMNN that": "",
          "incorporates the emotion recognition task with": ""
        },
        {
          "multi-task methods designed in this paper.": "",
          "SMNN that": "",
          "incorporates the emotion recognition task with": "SMNN based models achieve the new state-of-the-art on both"
        },
        {
          "multi-task methods designed in this paper.": "on symbolic music understanding[Zeng et al., 2021; Chou",
          "SMNN that": "",
          "incorporates the emotion recognition task with": ""
        },
        {
          "multi-task methods designed in this paper.": "",
          "SMNN that": "datasets.",
          "incorporates the emotion recognition task with": ""
        },
        {
          "multi-task methods designed in this paper.": "et al.,\n2021],\nincluding symbolic music\nemotion recogni-",
          "SMNN that": "",
          "incorporates the emotion recognition task with": ""
        },
        {
          "multi-task methods designed in this paper.": "",
          "SMNN that": "",
          "incorporates the emotion recognition task with": "The chief contributions of this paper can be summarized as"
        },
        {
          "multi-task methods designed in this paper.": "tion(SMER).",
          "SMNN that": "",
          "incorporates the emotion recognition task with": ""
        },
        {
          "multi-task methods designed in this paper.": "",
          "SMNN that": "following aspects:",
          "incorporates the emotion recognition task with": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract": "",
          "Recently,\nresearches\nin SMER mainly focused on learn-": "ing better\nrepresentation from large-scale unlabeled music"
        },
        {
          "Abstract": "Symbolic Music Emotion Recognition(SMER)\nis",
          "Recently,\nresearches\nin SMER mainly focused on learn-": "pieces via pre-training model by masked or non-masked lan-"
        },
        {
          "Abstract": "to predict music emotion from symbolic data, such",
          "Recently,\nresearches\nin SMER mainly focused on learn-": "guage model borrowed from NLP and then ﬁne-tuning the"
        },
        {
          "Abstract": "as MIDI and MusicXML. Previous work mainly fo-",
          "Recently,\nresearches\nin SMER mainly focused on learn-": "pre-trained model directly for emotion recognition on a small"
        },
        {
          "Abstract": "cused on learning better representation via (mask)",
          "Recently,\nresearches\nin SMER mainly focused on learn-": "dataset. However,\nsimply employing such techniques from"
        },
        {
          "Abstract": "language model pre-training but ignored the intrin-",
          "Recently,\nresearches\nin SMER mainly focused on learn-": "NLP may lack the understanding of music structure which"
        },
        {
          "Abstract": "sic structure of the music, which is extremely im-",
          "Recently,\nresearches\nin SMER mainly focused on learn-": "is critical to emotion classiﬁcation for symbolic data[Zeng et"
        },
        {
          "Abstract": "portant\nto the emotional expression of music.\nIn",
          "Recently,\nresearches\nin SMER mainly focused on learn-": "al., 2021]."
        },
        {
          "Abstract": "this paper, we present a simple multi-task frame-",
          "Recently,\nresearches\nin SMER mainly focused on learn-": "The existing psychology and music theory literature have"
        },
        {
          "Abstract": "work for SMER, which incorporates the emotion",
          "Recently,\nresearches\nin SMER mainly focused on learn-": "revealed the relationship between music structure and emo-"
        },
        {
          "Abstract": "recognition task with other emotion-related auxil-",
          "Recently,\nresearches\nin SMER mainly focused on learn-": "tion. Kaster\n[1990] has demonstrated that positive emotion"
        },
        {
          "Abstract": "iary tasks derived from the intrinsic structure of the",
          "Recently,\nresearches\nin SMER mainly focused on learn-": "is\nrelated to listened music in major keys, while negative"
        },
        {
          "Abstract": "music. The results show that our multi-task frame-",
          "Recently,\nresearches\nin SMER mainly focused on learn-": "emotion is\nrelated to minor keys.\nSimilar\nresults\ncan be"
        },
        {
          "Abstract": "work can be adapted to different models. More-",
          "Recently,\nresearches\nin SMER mainly focused on learn-": "found in [Gerardi and Gerken, 1995; Gregory et al., 1996;"
        },
        {
          "Abstract": "over, the labels of auxiliary tasks are easy to be ob-",
          "Recently,\nresearches\nin SMER mainly focused on learn-": "Dalla Bella et al., 2001]. Livingstone[2010]\nfound that\nthe"
        },
        {
          "Abstract": "tained, which means our multi-task methods do not",
          "Recently,\nresearches\nin SMER mainly focused on learn-": "loudness of music can greatly affect\nthe expression of emo-"
        },
        {
          "Abstract": "require manually annotated labels other than emo-",
          "Recently,\nresearches\nin SMER mainly focused on learn-": "tion. However, the loudness is measured in the audio domain"
        },
        {
          "Abstract": "tion. Conducting on two publicly available datasets",
          "Recently,\nresearches\nin SMER mainly focused on learn-": "and is still an open problem to measure it\nin the symbolic"
        },
        {
          "Abstract": "(EMOPIA and VGMIDI),\nthe\nexperiments\nshow",
          "Recently,\nresearches\nin SMER mainly focused on learn-": "domain. Adli[2007] has demonstrated that\nthere is a linear"
        },
        {
          "Abstract": "that our methods perform better\nin SMER task.",
          "Recently,\nresearches\nin SMER mainly focused on learn-": "relationship between the velocity in the symbolic domain and"
        },
        {
          "Abstract": "Speciﬁcally, accuracy has been increased by 4.17",
          "Recently,\nresearches\nin SMER mainly focused on learn-": "the loudness in the audio domain, which means that there is a"
        },
        {
          "Abstract": "absolute point\nto 67.58 in EMOPIA dataset,\nand",
          "Recently,\nresearches\nin SMER mainly focused on learn-": "connection between the velocity of music and emotion."
        },
        {
          "Abstract": "1.97 absolute point\nto 55.85 in VGMIDI dataset.",
          "Recently,\nresearches\nin SMER mainly focused on learn-": "Recognizing the importance of musical structure for emo-"
        },
        {
          "Abstract": "Ablation\nstudies\nalso\nshow the\neffectiveness\nof",
          "Recently,\nresearches\nin SMER mainly focused on learn-": "tion recognition, we present a simple framework called MT–"
        },
        {
          "Abstract": "multi-task methods designed in this paper.",
          "Recently,\nresearches\nin SMER mainly focused on learn-": "SMNN that\nincorporates the emotion recognition task with"
        },
        {
          "Abstract": "",
          "Recently,\nresearches\nin SMER mainly focused on learn-": "other emotion-related auxiliary tasks derived from the intrin-"
        },
        {
          "Abstract": "",
          "Recently,\nresearches\nin SMER mainly focused on learn-": "sic structure of the music. By combining the key classiﬁca-"
        },
        {
          "Abstract": "1\nIntroduction",
          "Recently,\nresearches\nin SMER mainly focused on learn-": "tion and velocity classiﬁcation tasks, MT-SMNN based mod-"
        },
        {
          "Abstract": "",
          "Recently,\nresearches\nin SMER mainly focused on learn-": "els can better understand emotion classiﬁcation.\nAlthough"
        },
        {
          "Abstract": "Emotion recognition of music has attracted lots of attention",
          "Recently,\nresearches\nin SMER mainly focused on learn-": ""
        },
        {
          "Abstract": "",
          "Recently,\nresearches\nin SMER mainly focused on learn-": "MT-SMNN is a multi-task framework, we only need the man-"
        },
        {
          "Abstract": "in the ﬁeld of music information retrieval(MIR). For a long",
          "Recently,\nresearches\nin SMER mainly focused on learn-": ""
        },
        {
          "Abstract": "",
          "Recently,\nresearches\nin SMER mainly focused on learn-": "ually annotated emotion label because the velocity label can"
        },
        {
          "Abstract": "time,\nthe research on music emotion recognition has been",
          "Recently,\nresearches\nin SMER mainly focused on learn-": ""
        },
        {
          "Abstract": "",
          "Recently,\nresearches\nin SMER mainly focused on learn-": "be\nextracted directly from symbolic data,\nand the key la-"
        },
        {
          "Abstract": "mainly carried out\nin the audio domain[Baume et al., 2014;",
          "Recently,\nresearches\nin SMER mainly focused on learn-": ""
        },
        {
          "Abstract": "",
          "Recently,\nresearches\nin SMER mainly focused on learn-": "bel can be obtained by the well-received Krumhansl-Kessler"
        },
        {
          "Abstract": "Liu et al., 2018; Panda et al., 2018; Panda et al., 2020].",
          "Recently,\nresearches\nin SMER mainly focused on learn-": ""
        },
        {
          "Abstract": "",
          "Recently,\nresearches\nin SMER mainly focused on learn-": "algorithm[2001], which means the proposed framework can"
        },
        {
          "Abstract": "However,\nemotion recognition is\nless\nexplored for music",
          "Recently,\nresearches\nin SMER mainly focused on learn-": ""
        },
        {
          "Abstract": "",
          "Recently,\nresearches\nin SMER mainly focused on learn-": "be applied to all emotion-labeled symbolic music datasets."
        },
        {
          "Abstract": "from symbolic\ndata,\nsuch\nas MIDI\nand MusicXML for-",
          "Recently,\nresearches\nin SMER mainly focused on learn-": ""
        },
        {
          "Abstract": "",
          "Recently,\nresearches\nin SMER mainly focused on learn-": "We\ncombine\nthe MT-SMNN framework with\nexisting"
        },
        {
          "Abstract": "mats.\nThanks\nto the\nrapid development of\nthe\nsymbolic",
          "Recently,\nresearches\nin SMER mainly focused on learn-": ""
        },
        {
          "Abstract": "",
          "Recently,\nresearches\nin SMER mainly focused on learn-": "models\nand\nevaluate\nthem in\nboth\nEMOPIA and VG-"
        },
        {
          "Abstract": "et\net\nmusic\ngeneration[Yang\nal.,\n2017; Huang\nal.,\n2019;",
          "Recently,\nresearches\nin SMER mainly focused on learn-": ""
        },
        {
          "Abstract": "",
          "Recently,\nresearches\nin SMER mainly focused on learn-": "MIDI datasets. Results demonstrate that our proposed MT-"
        },
        {
          "Abstract": "Huang and Yang, 2020], more and more research focuses",
          "Recently,\nresearches\nin SMER mainly focused on learn-": ""
        },
        {
          "Abstract": "",
          "Recently,\nresearches\nin SMER mainly focused on learn-": "SMNN based models achieve the new state-of-the-art on both"
        },
        {
          "Abstract": "on symbolic music understanding[Zeng et al., 2021; Chou",
          "Recently,\nresearches\nin SMER mainly focused on learn-": ""
        },
        {
          "Abstract": "",
          "Recently,\nresearches\nin SMER mainly focused on learn-": "datasets."
        },
        {
          "Abstract": "et al.,\n2021],\nincluding symbolic music\nemotion recogni-",
          "Recently,\nresearches\nin SMER mainly focused on learn-": ""
        },
        {
          "Abstract": "",
          "Recently,\nresearches\nin SMER mainly focused on learn-": "The chief contributions of this paper can be summarized as"
        },
        {
          "Abstract": "tion(SMER).",
          "Recently,\nresearches\nin SMER mainly focused on learn-": ""
        },
        {
          "Abstract": "",
          "Recently,\nresearches\nin SMER mainly focused on learn-": "following aspects:"
        },
        {
          "Abstract": "∗Contact Author",
          "Recently,\nresearches\nin SMER mainly focused on learn-": "• We present a novel multi-task framework called MT-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "SMNN, mainly\nfocusing\non\nemotion\nrecognition\nfor": "symbolic music.\nIn addition to emotion recognition, a"
        },
        {
          "SMNN, mainly\nfocusing\non\nemotion\nrecognition\nfor": "better understanding of\nthe structure of music is also"
        },
        {
          "SMNN, mainly\nfocusing\non\nemotion\nrecognition\nfor": "taken into account in this framework."
        },
        {
          "SMNN, mainly\nfocusing\non\nemotion\nrecognition\nfor": "• We propose two types of auxiliary tasks for SMER. Re-"
        },
        {
          "SMNN, mainly\nfocusing\non\nemotion\nrecognition\nfor": "sults show that both tasks can improve the performance"
        },
        {
          "SMNN, mainly\nfocusing\non\nemotion\nrecognition\nfor": "of SMER, especially in the valence dimension."
        },
        {
          "SMNN, mainly\nfocusing\non\nemotion\nrecognition\nfor": "• MT-SMNN based models\nachieve\nnew state-of-the-"
        },
        {
          "SMNN, mainly\nfocusing\non\nemotion\nrecognition\nfor": "art\nresults due\nto the powerful\nability to learn better"
        },
        {
          "SMNN, mainly\nfocusing\non\nemotion\nrecognition\nfor": "emotion-based knowledge from auxiliary tasks."
        },
        {
          "SMNN, mainly\nfocusing\non\nemotion\nrecognition\nfor": "• We have reproduced most previous work for symbolic"
        },
        {
          "SMNN, mainly\nfocusing\non\nemotion\nrecognition\nfor": "music emotion recognition on both exiting public avail-"
        },
        {
          "SMNN, mainly\nfocusing\non\nemotion\nrecognition\nfor": "able datasets which is helpful to building benchmarks."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "able datasets which is helpful to building benchmarks.": "2\nRelated Work"
        },
        {
          "able datasets which is helpful to building benchmarks.": "We divide previous work on symbolic music emotion recog-"
        },
        {
          "able datasets which is helpful to building benchmarks.": "nition into the following two categories."
        },
        {
          "able datasets which is helpful to building benchmarks.": "Machine Learning based Methods:\nEarly studies used"
        },
        {
          "able datasets which is helpful to building benchmarks.": "manually extracted statistical musical\nfeatures and then fed"
        },
        {
          "able datasets which is helpful to building benchmarks.": "them into machine learning classiﬁers to predict the emotion"
        },
        {
          "able datasets which is helpful to building benchmarks.": "of\nsymbolic music.\nGrekow et al.[2009] extracted 63 fea-"
        },
        {
          "able datasets which is helpful to building benchmarks.": "tures from classical music in MIDI\nformat and used k-NN"
        },
        {
          "able datasets which is helpful to building benchmarks.": "to classify the music after feature selection. Lin et al.[2013]"
        },
        {
          "able datasets which is helpful to building benchmarks.": "compared the audio, lyric , and MIDI modal of the same mu-"
        },
        {
          "able datasets which is helpful to building benchmarks.": ""
        },
        {
          "able datasets which is helpful to building benchmarks.": "sic, ﬁnding that MIDI modal features performed better than"
        },
        {
          "able datasets which is helpful to building benchmarks.": ""
        },
        {
          "able datasets which is helpful to building benchmarks.": "audio modal\nfeatures\nin emotion recognition.\nSpeciﬁcally,"
        },
        {
          "able datasets which is helpful to building benchmarks.": "112 types of high-level musical features were extracted from"
        },
        {
          "able datasets which is helpful to building benchmarks.": "MIDI ﬁles using the JSymbolic library[McKay and Fujinaga,"
        },
        {
          "able datasets which is helpful to building benchmarks.": "2006], and then SVM was employed to classify the data. Sim-"
        },
        {
          "able datasets which is helpful to building benchmarks.": "ilarly, Panda et al.[2013] extracted 320 types of features from"
        },
        {
          "able datasets which is helpful to building benchmarks.": ""
        },
        {
          "able datasets which is helpful to building benchmarks.": "MIDI ﬁles using multiple tools and then classiﬁed them using"
        },
        {
          "able datasets which is helpful to building benchmarks.": ""
        },
        {
          "able datasets which is helpful to building benchmarks.": "SVM as well."
        },
        {
          "able datasets which is helpful to building benchmarks.": ""
        },
        {
          "able datasets which is helpful to building benchmarks.": ""
        },
        {
          "able datasets which is helpful to building benchmarks.": "Deep Learning based Methods:\nIn recent years, it has be-"
        },
        {
          "able datasets which is helpful to building benchmarks.": ""
        },
        {
          "able datasets which is helpful to building benchmarks.": "come a trend to encode symbolic music into MIDI-like musi-"
        },
        {
          "able datasets which is helpful to building benchmarks.": ""
        },
        {
          "able datasets which is helpful to building benchmarks.": "cal representation[Oore et al., 2020; Huang and Yang, 2020;"
        },
        {
          "able datasets which is helpful to building benchmarks.": "Hsiao et al.,\n2021]\nand then employ deep learning mod-"
        },
        {
          "able datasets which is helpful to building benchmarks.": "els\nto\nclassify\nemotion.\nWith\nencoding MIDI ﬁles\ninto"
        },
        {
          "able datasets which is helpful to building benchmarks.": "MIDI-like sequences, Ferreira[Ferreira and Whitehead, 2019;"
        },
        {
          "able datasets which is helpful to building benchmarks.": ""
        },
        {
          "able datasets which is helpful to building benchmarks.": "Ferreira et al., 2020] used LSTM and GPT2[Radford et al.,"
        },
        {
          "able datasets which is helpful to building benchmarks.": ""
        },
        {
          "able datasets which is helpful to building benchmarks.": "2019]\nfor emotion classiﬁcation.\nFor simplicity,\nin the fol-"
        },
        {
          "able datasets which is helpful to building benchmarks.": ""
        },
        {
          "able datasets which is helpful to building benchmarks.": "lowing, we use MIDIGPT to denote the approach proposed"
        },
        {
          "able datasets which is helpful to building benchmarks.": ""
        },
        {
          "able datasets which is helpful to building benchmarks.": "by [Ferreira et al., 2020].\nInspired by the great success of"
        },
        {
          "able datasets which is helpful to building benchmarks.": ""
        },
        {
          "able datasets which is helpful to building benchmarks.": "BERT[Devlin et al., 2019]\nin NLP, Chou et al.[2021] pre-"
        },
        {
          "able datasets which is helpful to building benchmarks.": ""
        },
        {
          "able datasets which is helpful to building benchmarks.": "sented a\nlarge-scale pre-training model\ncalled MidiBERT-"
        },
        {
          "able datasets which is helpful to building benchmarks.": ""
        },
        {
          "able datasets which is helpful to building benchmarks.": "Piano, which employed CP representation[Hsiao et al., 2021]"
        },
        {
          "able datasets which is helpful to building benchmarks.": ""
        },
        {
          "able datasets which is helpful to building benchmarks.": "and has shown good results in a number of ﬁelds,\nincluding"
        },
        {
          "able datasets which is helpful to building benchmarks.": ""
        },
        {
          "able datasets which is helpful to building benchmarks.": "symbolic music emotion recognition."
        },
        {
          "able datasets which is helpful to building benchmarks.": ""
        },
        {
          "able datasets which is helpful to building benchmarks.": ""
        },
        {
          "able datasets which is helpful to building benchmarks.": ""
        },
        {
          "able datasets which is helpful to building benchmarks.": "3\nProposed Method"
        },
        {
          "able datasets which is helpful to building benchmarks.": ""
        },
        {
          "able datasets which is helpful to building benchmarks.": "In this section, we introduce the Multi-Task Symbolic Music"
        },
        {
          "able datasets which is helpful to building benchmarks.": "Neural Network(MT-SMNN),\na multi-task\nframework\nfor"
        },
        {
          "able datasets which is helpful to building benchmarks.": "symbolic music emotion recognition, as illustrated in Figure"
        },
        {
          "able datasets which is helpful to building benchmarks.": "1. Below, we describe the structure of MT-SMNN in detail."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "ing average(like MIDIBERT-Piano[Chou et al., 2021]).\nIn"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "MT-SMNN, the emotion and key classiﬁcation task share the"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "same sentence representation l3 because both are sequence-"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "level tasks."
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "3.5\nTask-speciﬁc Classiﬁcation"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "In this part, we ﬁrst\nintroduce the auxiliary tasks employed"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "by MT-SMNN. Then, we describe more details about\nthese"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "classiﬁcation outputs.\nFinally, we show the multi-task loss"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "function used by MT-SMNN."
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "Auxiliary Tasks"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "Key Classiﬁcation:\nThis is a sequence-level classiﬁcation"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "task.\nThe target\nis to predict\nthe musical key for\nthe given"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": ""
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "sequence of musical\nrepresentation tokens collected from a"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": ""
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "piece of music. There are 24 possible keys: 12 major keys"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "and 12 minor keys[Thompson and Cuddy, 1997]."
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": ""
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "Velocity Classiﬁcation:\nThis is a note-level classiﬁcation"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": ""
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "task [Chou et al., 2021]. The target\nis to predict velocity for"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": ""
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "each individual note for the given sequence of notes collected"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": ""
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "from a piece of music.\nFollowing [Chou et al., 2021], we"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "quantize 128 possible MIDI velocity values(0-127)\ninto six"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": ""
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "f\nclasses: pp (0-31), p (32-47), mp (48-63), mf\n(64-79),\n(80-"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "95), and ff\n(96-127)."
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": ""
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "Classiﬁcation Ouputs"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "Let H = {h0, ..., hS} be the contextual representation(l2) of"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "given piece of music, Z be the sentence representation(l3),"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": ""
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "where hi ∈ RE, Z ∈ RK, E is the dimension of hidden state"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": ""
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": ", and K is the dimension of sentence representation gener-"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": ""
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "ated by the pooler. For sequence-level tasks(emotion and key"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": ""
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "classiﬁcation),\nthe probability that given a piece of music is"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": ""
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "predicted as class c by a classiﬁer with softmax can be for-"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": ""
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "malized as:"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": ""
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": ""
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "P t(ct|Z) = softmax(φt(Z))\n(1)"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": ""
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "where φ is the mapping function of classiﬁer, t is to distin-"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": ""
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "guish between different tasks."
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "For\nthe note-level\ntask(velocity classiﬁcation),\nthe proba-"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "bility that\nthe i-th note in a piece of music is predicted as"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "class c can be formalized as:"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": ""
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "P t"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "(2)\ni (ct|H) = softmax(φt(hi))"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": ""
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "where φ is the mapping function of classiﬁer, i means the i-th"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": ""
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "item in coresponding sequence."
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "For more detail,\nthe classiﬁers consist of\ntwo fully con-"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": ""
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "nected layers with the ReLU activation function in the mid-"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": ""
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "dle."
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": ""
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "Multi-task loss"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "For each task, we use cross-entropy loss as its objective. Let"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": ""
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "L1, L2, and L3 be the loss of emotion recognition, key classi-"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "ﬁcation, and velocity classiﬁcation, respectively. We employ"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "the adaptive loss\nfunction proposed by Liebel[2018].\nThe"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "multi-task loss is formalized following:"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": ""
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "1"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "(cid:1)"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "(cid:88) t\nL =\n(3)\nLt + ln (cid:0)1 + σ2"
        },
        {
          "MIDIGPT[Ferreira et al., 2020]), or attention-based weight-": "2 · σt"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: The EMOPIA dataset1 is a dataset inside BERT model adopt BERT . We start fine-tuning",
      "data": [
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "",
          "training set and a testing set. We divide a portion(about 15%)": "of the original training set into the validation set.\nIn this pro-"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "1: Load model parameters Θ from the pre-trained check-",
          "training set and a testing set. We divide a portion(about 15%)": ""
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "",
          "training set and a testing set. We divide a portion(about 15%)": "cedure, we ensure that\nthe clips of the validation set and the"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "point;",
          "training set and a testing set. We divide a portion(about 15%)": ""
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "",
          "training set and a testing set. We divide a portion(about 15%)": "training set will not come from the same song."
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "2: Set the max number of epoch: epochmax;",
          "training set and a testing set. We divide a portion(about 15%)": ""
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "",
          "training set and a testing set. We divide a portion(about 15%)": "The MT-SMNN need two additional labels(key and veloc-"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "3: Prepare dataset D;",
          "training set and a testing set. We divide a portion(about 15%)": ""
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "",
          "training set and a testing set. We divide a portion(about 15%)": "ity) besides emotion. The velocity for each note can directly"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "4:\nfor epoch in 1, ..., epochmax do",
          "training set and a testing set. We divide a portion(about 15%)": ""
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "",
          "training set and a testing set. We divide a portion(about 15%)": "derived from symbolic data. We extract the key label via the"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "5:\nin D do\nfor bi",
          "training set and a testing set. We divide a portion(about 15%)": ""
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "",
          "training set and a testing set. We divide a portion(about 15%)": "well-received Krumhansl-Kessler algorithm[2001] provided"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "6:\nis the i-th mini-batch of dataset ;\n//bi",
          "training set and a testing set. We divide a portion(about 15%)": ""
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "",
          "training set and a testing set. We divide a portion(about 15%)": "by the Music21 library [Cuthbert and Ariza, 2010]."
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "7:\n1. Predict",
          "training set and a testing set. We divide a portion(about 15%)": ""
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "8:\nPredict emotion and key for music using Eq. 1;",
          "training set and a testing set. We divide a portion(about 15%)": ""
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "",
          "training set and a testing set. We divide a portion(about 15%)": "4.2\nImplementation details"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "9:\nPredict velocity for each note using Eq. 2;",
          "training set and a testing set. We divide a portion(about 15%)": ""
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "",
          "training set and a testing set. We divide a portion(about 15%)": "For\nthe sake of\nfair comparison,\nthe vast majority of previ-"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "10:\n2. Compute loss",
          "training set and a testing set. We divide a portion(about 15%)": ""
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "",
          "training set and a testing set. We divide a portion(about 15%)": "ous work mentioned in Section 2 is\nreproduced.\nOur\nim-"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "11:\nCompute loss for each task using cross-entropy;",
          "training set and a testing set. We divide a portion(about 15%)": ""
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "",
          "training set and a testing set. We divide a portion(about 15%)": "plementation is based on the PyTorch code open-sourced by"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "12:\nCompute loss L (Θ) for multi-task using Eq. 3;",
          "training set and a testing set. We divide a portion(about 15%)": ""
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "",
          "training set and a testing set. We divide a portion(about 15%)": "HugginFace[Wolf et al., 2019]. Below, we describe the re-"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "13:\n4. Compute gradient: ∇(Θ) ;",
          "training set and a testing set. We divide a portion(about 15%)": ""
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "",
          "training set and a testing set. We divide a portion(about 15%)": "produced models in detail."
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "14:\n5. Update parameters: Θ = Θ − λ∇(Θ);",
          "training set and a testing set. We divide a portion(about 15%)": ""
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "15:\nend for",
          "training set and a testing set. We divide a portion(about 15%)": ""
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "",
          "training set and a testing set. We divide a portion(about 15%)": "Conﬁguration of Machine Learning based Methods"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "16:\nEvaluate the model in validation set;",
          "training set and a testing set. We divide a portion(about 15%)": ""
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "",
          "training set and a testing set. We divide a portion(about 15%)": "In this paper, we have reproduced the machine learning based"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "17:\nCheck for early-stopping;",
          "training set and a testing set. We divide a portion(about 15%)": ""
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "",
          "training set and a testing set. We divide a portion(about 15%)": "models proposed in [Lin et al., 2013] and [Panda et al., 2013]."
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "18:\nend for",
          "training set and a testing set. We divide a portion(about 15%)": ""
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "",
          "training set and a testing set. We divide a portion(about 15%)": "After\ntaking the best\nsubset of\nfeatures\nselected in [Lin et"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "",
          "training set and a testing set. We divide a portion(about 15%)": "al., 2013] and [Panda et al., 2013], the dimension of features"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "",
          "training set and a testing set. We divide a portion(about 15%)": "for Lin’s method and Panda’s method is 521 and 135 respec-"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "where Lt indicates the loss of task t, σt a learnable parameter",
          "training set and a testing set. We divide a portion(about 15%)": ""
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "",
          "training set and a testing set. We divide a portion(about 15%)": "tively. Both methods use the SVM classiﬁer that works with"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "which controls the contribution of t-th task, and the second",
          "training set and a testing set. We divide a portion(about 15%)": ""
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "",
          "training set and a testing set. We divide a portion(about 15%)": "the RBF kernel."
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "term is a regularizer.",
          "training set and a testing set. We divide a portion(about 15%)": ""
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "",
          "training set and a testing set. We divide a portion(about 15%)": "Conﬁguration of Deep Learning based Methods"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "4\nExperiments",
          "training set and a testing set. We divide a portion(about 15%)": "Global Settings:\nThe reproduced MIDIBERT-Piano[Chou"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "",
          "training set and a testing set. We divide a portion(about 15%)": "et al., 2021], MIDIGPT[Ferreira et al., 2020] and our pro-"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "In this section, we evaluate the proposed MT-SMNN based",
          "training set and a testing set. We divide a portion(about 15%)": ""
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "",
          "training set and a testing set. We divide a portion(about 15%)": "posed MT-SMNN based methods\nall\nshare\nthe\nfollowing"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "models on EMOPIA[Hung et al., 2021] and VGMIDI[Fer-",
          "training set and a testing set. We divide a portion(about 15%)": ""
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "",
          "training set and a testing set. We divide a portion(about 15%)": "global conﬁguration:\n(a) The AdamW[Loshchilov and Hut-"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "reira and Whitehead, 2019; Ferreira et al., 2020] datasets. We",
          "training set and a testing set. We divide a portion(about 15%)": ""
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "",
          "training set and a testing set. We divide a portion(about 15%)": "ter, 2019] optimizer\nis adopt\nin this paper. The β1, β2 and"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "ﬁrst overview the datasets and processing procedure. Then,",
          "training set and a testing set. We divide a portion(about 15%)": ""
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "",
          "training set and a testing set. We divide a portion(about 15%)": "weight decay rate is set as 0.9, 0.999 and 0.01 repectively."
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "we\ndescribe\nthe\nbaselines\nand\nour\nproposed models(MT-",
          "training set and a testing set. We divide a portion(about 15%)": ""
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "",
          "training set and a testing set. We divide a portion(about 15%)": "(b) The batch size is set as 16.\n(c) The learning rate is set as"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "MIDIBERT and MT-MIDIGPT). Finally, we show the results",
          "training set and a testing set. We divide a portion(about 15%)": ""
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "",
          "training set and a testing set. We divide a portion(about 15%)": "3e-5 with a linear scheduler. The other trick is setting warm-"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "and analysis.",
          "training set and a testing set. We divide a portion(about 15%)": ""
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "",
          "training set and a testing set. We divide a portion(about 15%)": "up steps as 500.\n(d) We evaluate the models every training"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "Table 1: Summary of the two datasets: EMOPIA and VGMIDI.",
          "training set and a testing set. We divide a portion(about 15%)": "epoch in the validation set. The model is early stopping when"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "",
          "training set and a testing set. We divide a portion(about 15%)": "the macro-F1 for emotion recognition have no improvement"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "",
          "training set and a testing set. We divide a portion(about 15%)": "for T consecutive epochs, where T = (cid:98)0.3 ∗ N (cid:99), N denotes"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "Datasets\n#Train\n#Valid\n#Test\n#Label",
          "training set and a testing set. We divide a portion(about 15%)": ""
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "",
          "training set and a testing set. We divide a portion(about 15%)": "the number of max training epochs. The checkpoint achiev-"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "EMOPIA\n869\n114\n88\n4",
          "training set and a testing set. We divide a portion(about 15%)": "ing the best metric in the validation set during the training"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "VGMIDI\n4,876\n879\n1,436\n4",
          "training set and a testing set. We divide a portion(about 15%)": "procedure is saved and evaluated in the testing set. (e) All ex-"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "",
          "training set and a testing set. We divide a portion(about 15%)": "periments are repeated ten times with different random seeds"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "",
          "training set and a testing set. We divide a portion(about 15%)": "(from 0 to 9)."
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "4.1\nDatasets and Preprocess",
          "training set and a testing set. We divide a portion(about 15%)": ""
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "",
          "training set and a testing set. We divide a portion(about 15%)": "Speciﬁc Settings:\nFollowing [Chou et al., 2021],\nthe max"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "The information of\nthe EMOPIA and VGMIDI datasets\nis",
          "training set and a testing set. We divide a portion(about 15%)": "sequence\nlength of MIDIBERT-Piano is\nset\nas 512.\nThe"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "summarized in Table 1. The EMOPIA dataset1\nis a dataset",
          "training set and a testing set. We divide a portion(about 15%)": "inside BERT model adopt BERTbase. We start ﬁne-tuning"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "of pop piano music for symbolic music emotion recognition.",
          "training set and a testing set. We divide a portion(about 15%)": "the MIDIBERT-Piano model\nfrom the released pre-trained"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "The clips is labeled to 4 class accoding to Russell’s 4Q[Rus-",
          "training set and a testing set. We divide a portion(about 15%)": "checkpoint3.\nThe max sequence length of MIDIGPT is set"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "sell, 1980]. The VGMIDI dataset2 is a dataset of video game",
          "training set and a testing set. We divide a portion(about 15%)": "as 1024 and 2048 to cover the entire input sequence of tokens"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "sound-tracks formatted in MIDI. Each clip in the VGMIDI",
          "training set and a testing set. We divide a portion(about 15%)": "as much as possible when experimenting with VGMIDI and"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "dataset\nis labeled as valence-arousal pair, also according to",
          "training set and a testing set. We divide a portion(about 15%)": "EMOPIA datasets,\nrespectively.\nTo accommodate different"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "the Russell’s model.",
          "training set and a testing set. We divide a portion(about 15%)": "max sequence lengths, we pre-trained the MIDIGPT model"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "For\nexperimental\nconsistency, we\ntransfer\nthe\nvalence-",
          "training set and a testing set. We divide a portion(about 15%)": "according to [Ferreira et al., 2020], with remaining other set-"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "arousal pair in VGMIDI to the taxonomy of Russell’s 4Q as",
          "training set and a testing set. We divide a portion(about 15%)": "tings unchanged except the max sequence length. We ﬁnetune"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "EMOPIA. The initial VGMIDI dataset has been split\ninto a",
          "training set and a testing set. We divide a portion(about 15%)": "the models mentioned above at most 30 epochs in VGMIDI,"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "",
          "training set and a testing set. We divide a portion(about 15%)": "and 100 epochs in EMOPIA with early-stopping discussed"
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "1https://zenodo.org/record/5257995",
          "training set and a testing set. We divide a portion(about 15%)": ""
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "",
          "training set and a testing set. We divide a portion(about 15%)": "above."
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "2https://github.com/lucasnfe/bardo-",
          "training set and a testing set. We divide a portion(about 15%)": ""
        },
        {
          "Algorithm 1 Training a MT-SMNN-based model.": "",
          "training set and a testing set. We divide a portion(about 15%)": "3https://github.com/wazenmai/MIDI-BERT"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: shows that the deep learning based models out-",
      "data": [
        {
          "Table 2: Main result of SMER in the EMOPIA and VGMIDI dataset.": ""
        },
        {
          "Table 2: Main result of SMER in the EMOPIA and VGMIDI dataset.": ""
        },
        {
          "Table 2: Main result of SMER in the EMOPIA and VGMIDI dataset.": "Accuracy(%)"
        },
        {
          "Table 2: Main result of SMER in the EMOPIA and VGMIDI dataset.": "47.72"
        },
        {
          "Table 2: Main result of SMER in the EMOPIA and VGMIDI dataset.": "39.77"
        },
        {
          "Table 2: Main result of SMER in the EMOPIA and VGMIDI dataset.": "58.75±3.13"
        },
        {
          "Table 2: Main result of SMER in the EMOPIA and VGMIDI dataset.": "63.41±3.52"
        },
        {
          "Table 2: Main result of SMER in the EMOPIA and VGMIDI dataset.": "62.50±4.45"
        },
        {
          "Table 2: Main result of SMER in the EMOPIA and VGMIDI dataset.": "67.58±2.39"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 2: shows that the deep learning based models out-",
      "data": [
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "67.58±2.39\nMT-MIDIBERT(proposed)",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "0.664±0.027\n49.81±2.52\n0.453±0.019"
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "Conﬁguration of the proposed MT-SMNN based models",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "4.5\nAblation Studies"
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "We apply the proposed MT-SMNN framework to existing",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "In this\nsection, we\nconduct\nexperiments on the EMOPIA"
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "deep learning based methods.\nFor\nthe MIDIBERT-Piano",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "dataset to study auxiliary tasks’ impact. The results are sum-"
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "model, we extend it by combining both key classiﬁcation and",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "marized in Table 3."
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "velocity classiﬁcation with the original emotion recognition",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": ""
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "Table 3: The contribution of different tasks on EMOPIA dataset us-"
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "task.\nHowever, we only incorporate the key classiﬁcation",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": ""
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "ing MIDIBERT backbone. Results are evaluated by the accurary of"
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "with the original emotion recognition task for MIDIGPT be-",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": ""
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "music emotion recognition task."
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "cause its representation method has already leaked the veloc-",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": ""
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "ity information.",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": ""
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "Key Classiﬁcation\nVelocity Classiﬁcaiton\nAccuracy"
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "We coin the model that combines the proposed MT-SMNN",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": ""
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "with MIDIBERT-Piano and MIDIGPT as “MT-MIDIBERT“",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "(cid:55)\n(cid:55)"
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "63.41±3.52"
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "and “MT-MIDIGPT“ respectively.",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "(cid:51)\n(cid:55)"
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "67.03±2.54"
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "(cid:55)\n(cid:51)"
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "64.73±5.47"
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "(cid:51)\n(cid:51)"
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "67.58±2.39"
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "4.3\nThe Training Procedure of MT-SMNN",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": ""
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "The training procedure of MT-SMNN is shown in Algorithm",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": ""
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "Table 3 shows that both key and velocity classiﬁcation aux-"
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "1. We start our\ntraining from pre-trained checkpoints, and",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": ""
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "iliary tasks effectively affect emotion recognition. Moreover,"
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "then we ﬁnetune the MT-SMNN based model using multi-",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": ""
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "the model\ntaken in both auxiliary tasks outperforms models"
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "task loss. After every training epoch, we evaluate the model",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": ""
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "only taken in a single.\nThe accuracy is increased by 3.6%"
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "and check whether early-stopping.",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": ""
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "and 1.3% to 67.03% and 64.73% after combing the SMER"
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "task with the key and velocity classiﬁcation task, respectively,"
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "4.4\nComparison of state-of-the-art Methods",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": ""
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "which means that the key classiﬁcation task is a more critical"
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "auxiliary task than the velocity classiﬁcation task."
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "We compare MT-SMNN based models with previous state-",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": ""
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "We also plot\nthe confusion matrices of these experiments,"
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "of-the-art models.\nThe result of\nsymbolic music emotion",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": ""
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "as\nshown\nin Figure\n3.\nIn\nthis ﬁgure, Q1, Q2, Q3\nand"
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "recognition(SMER) is shown in Table 2. We have reproduced",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": ""
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "Q4 denotes HVHA(high valence high arousal), LVHA(low"
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "all\nthese baselines in Table 2 and described them in detail\nin",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": ""
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "valence high arousal), LVLA(low valence low arousal) and"
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "4.2.",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": ""
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "HVLA(high valence low arousal) respectively which also so-"
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "Table 2 shows\nthat\nthe deep learning based models out-",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": ""
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "called Happy, Angry, Sad and Calm in some literatures. Com-"
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "perform the traditional machine learning based models.\nIn",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": ""
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "pared Figure 3(b) with Figure 3(a), we have found that\nthe"
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "addition, models\nthat work with the proposed MT-SMNN",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": ""
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "key classiﬁcation task can greatly improve the performance"
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "framework perform better\nthan the\ncounterpart\nfor\nsingle-",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": ""
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "of emotion recognition in the class of Q1 and Q4.\nSimilar"
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "task and achieve new state-of-the-art\nresults.\nSpeciﬁcally,",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": ""
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "results can be found in Figure 3(c) and Figure3(d). Since Q1"
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "Compared with the MIDIBERT-Piano model,\nthe proposed",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": ""
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "and Q4 are both in the high valence region, we ﬁnally con-"
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "MT-MIDIBERT model pushes the accuracy to 67.58% and",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": ""
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "clude that our proposed MT-SMNN framework can improve"
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "49.8%, which amounts 4.2% and 2.5% absolution improve-",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": ""
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "the performance of music recognition, especially in the va-"
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "ment on the EMOPIA and VGMIDI dataset,\nrespectively.",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": ""
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "lence dimension."
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "The proposed MT-MIDIGPT model also improves the accu-",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": ""
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "racy by 3.8% and 2.0% to 62.50% and 55.85% for these two",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": ""
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "5\nConclusion"
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "datasets, respectively.",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": ""
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "Since the MT-SMNN based models have no difference ex-",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "In this paper, we present MT-SMNN, a multi-task framework"
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "cept for multiple classiﬁers, which have a minimal amount of",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "that mainly focus on emotion recognition for symbolic music."
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "parameters, are employed for different\ntasks compared with",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "The MT-SMNN framework combines emotion recognition"
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "its single-task counterpart,\nthe improvement of the above re-",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "with key classiﬁcation and velocity classiﬁcation tasks and"
        },
        {
          "MT-MIDIGPT(proposed)\n62.50±4.45": "sults is attributed to our proposed MT-SMNN framework.",
          "55.85±1.97\n0.509±0.017\n0.611±0.047": "conducts a multi-task training procedure in a single dataset."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "available.": "",
          "[Hsiao et al., 2021] Wen-Yi Hsiao,\nJen-Yu Liu, Yin-Cheng Yeh,": "and Yi-Hsuan Yang.\nCompound word transformer:\nLearning"
        },
        {
          "available.": "",
          "[Hsiao et al., 2021] Wen-Yi Hsiao,\nJen-Yu Liu, Yin-Cheng Yeh,": "to compose full-song music over dynamic directed hypergraphs."
        },
        {
          "available.": "References",
          "[Hsiao et al., 2021] Wen-Yi Hsiao,\nJen-Yu Liu, Yin-Cheng Yeh,": ""
        },
        {
          "available.": "",
          "[Hsiao et al., 2021] Wen-Yi Hsiao,\nJen-Yu Liu, Yin-Cheng Yeh,": "In Thirty-Fifth AAAI Conference on Artiﬁcial Intelligence, AAAI"
        },
        {
          "available.": "[Adli et al., 2007] Alexander\nAdli,\nZensho\nNakao,\nToshiaki",
          "[Hsiao et al., 2021] Wen-Yi Hsiao,\nJen-Yu Liu, Yin-Cheng Yeh,": ""
        },
        {
          "available.": "",
          "[Hsiao et al., 2021] Wen-Yi Hsiao,\nJen-Yu Liu, Yin-Cheng Yeh,": "2021, Thirty-Third Conference on Innovative Applications of Ar-"
        },
        {
          "available.": "Yokoda, and Yasunori Nagata.\nPiano sound characteristics:\na",
          "[Hsiao et al., 2021] Wen-Yi Hsiao,\nJen-Yu Liu, Yin-Cheng Yeh,": ""
        },
        {
          "available.": "",
          "[Hsiao et al., 2021] Wen-Yi Hsiao,\nJen-Yu Liu, Yin-Cheng Yeh,": "tiﬁcial Intelligence, IAAI 2021, The Eleventh Symposium on Ed-"
        },
        {
          "available.": "study on some factors affecting loudness in digital and acoustic",
          "[Hsiao et al., 2021] Wen-Yi Hsiao,\nJen-Yu Liu, Yin-Cheng Yeh,": ""
        },
        {
          "available.": "",
          "[Hsiao et al., 2021] Wen-Yi Hsiao,\nJen-Yu Liu, Yin-Cheng Yeh,": "ucational Advances in Artiﬁcial Intelligence, EAAI 2021, Virtual"
        },
        {
          "available.": "Second\nInternational Conference\non\nInnovative\npianos.\nIn",
          "[Hsiao et al., 2021] Wen-Yi Hsiao,\nJen-Yu Liu, Yin-Cheng Yeh,": ""
        },
        {
          "available.": "",
          "[Hsiao et al., 2021] Wen-Yi Hsiao,\nJen-Yu Liu, Yin-Cheng Yeh,": "Event, February 2-9, 2021, pages 178–186. AAAI Press, 2021."
        },
        {
          "available.": "Computing,\nInformatio\nand Control\n(ICICIC 2007),\npages",
          "[Hsiao et al., 2021] Wen-Yi Hsiao,\nJen-Yu Liu, Yin-Cheng Yeh,": ""
        },
        {
          "available.": "",
          "[Hsiao et al., 2021] Wen-Yi Hsiao,\nJen-Yu Liu, Yin-Cheng Yeh,": "[Huang and Yang, 2020] Yu-Siang Huang and Yi-Hsuan Yang. Pop"
        },
        {
          "available.": "34–34. IEEE, 2007.",
          "[Hsiao et al., 2021] Wen-Yi Hsiao,\nJen-Yu Liu, Yin-Cheng Yeh,": ""
        },
        {
          "available.": "",
          "[Hsiao et al., 2021] Wen-Yi Hsiao,\nJen-Yu Liu, Yin-Cheng Yeh,": "music transformer: Beat-based modeling and generation of ex-"
        },
        {
          "available.": "[Baume et al., 2014] Chris Baume, Gy¨orgy Fazekas, Mathieu Bar-",
          "[Hsiao et al., 2021] Wen-Yi Hsiao,\nJen-Yu Liu, Yin-Cheng Yeh,": ""
        },
        {
          "available.": "",
          "[Hsiao et al., 2021] Wen-Yi Hsiao,\nJen-Yu Liu, Yin-Cheng Yeh,": "pressive pop piano compositions.\nIn Chang Wen Chen, Rita"
        },
        {
          "available.": "thet, David Marston, and Mark Sandler. Selection of audio fea-",
          "[Hsiao et al., 2021] Wen-Yi Hsiao,\nJen-Yu Liu, Yin-Cheng Yeh,": ""
        },
        {
          "available.": "",
          "[Hsiao et al., 2021] Wen-Yi Hsiao,\nJen-Yu Liu, Yin-Cheng Yeh,": "Cucchiara, Xian-Sheng Hua, Guo-Jun Qi, Elisa Ricci, Zhengyou"
        },
        {
          "available.": "tures for music emotion recognition using production music.\nIn",
          "[Hsiao et al., 2021] Wen-Yi Hsiao,\nJen-Yu Liu, Yin-Cheng Yeh,": ""
        },
        {
          "available.": "",
          "[Hsiao et al., 2021] Wen-Yi Hsiao,\nJen-Yu Liu, Yin-Cheng Yeh,": "The 28th\nZhang,\nand Roger Zimmermann,\neditors, MM ’20:"
        },
        {
          "available.": "Audio Engineering Society Conference: 53rd International Con-",
          "[Hsiao et al., 2021] Wen-Yi Hsiao,\nJen-Yu Liu, Yin-Cheng Yeh,": ""
        },
        {
          "available.": "",
          "[Hsiao et al., 2021] Wen-Yi Hsiao,\nJen-Yu Liu, Yin-Cheng Yeh,": "ACM International Conference on Multimedia, Virtual Event\n/"
        },
        {
          "available.": "ference: Semantic Audio. Audio Engineering Society, 2014.",
          "[Hsiao et al., 2021] Wen-Yi Hsiao,\nJen-Yu Liu, Yin-Cheng Yeh,": ""
        },
        {
          "available.": "",
          "[Hsiao et al., 2021] Wen-Yi Hsiao,\nJen-Yu Liu, Yin-Cheng Yeh,": "Seattle, WA, USA, October 12-16, 2020, pages 1180–1188. ACM,"
        },
        {
          "available.": "[Chou et al., 2021] Yi-Hui Chou,\nI-Chun Chen, Chin-Jui Chang,",
          "[Hsiao et al., 2021] Wen-Yi Hsiao,\nJen-Yu Liu, Yin-Cheng Yeh,": "2020."
        },
        {
          "available.": "Joann Ching,\nand Yi-Hsuan Yang.\nMidibert-piano:\nLarge-",
          "[Hsiao et al., 2021] Wen-Yi Hsiao,\nJen-Yu Liu, Yin-Cheng Yeh,": ""
        },
        {
          "available.": "",
          "[Hsiao et al., 2021] Wen-Yi Hsiao,\nJen-Yu Liu, Yin-Cheng Yeh,": "[Huang et al., 2019] Cheng-Zhi Anna Huang,\nAshish Vaswani,"
        },
        {
          "available.": "scale pre-training for\nsymbolic music understanding.\nCoRR,",
          "[Hsiao et al., 2021] Wen-Yi Hsiao,\nJen-Yu Liu, Yin-Cheng Yeh,": ""
        },
        {
          "available.": "",
          "[Hsiao et al., 2021] Wen-Yi Hsiao,\nJen-Yu Liu, Yin-Cheng Yeh,": "Jakob Uszkoreit,\nIan Simon, Curtis Hawthorne, Noam Shazeer,"
        },
        {
          "available.": "abs/2107.05223, 2021.",
          "[Hsiao et al., 2021] Wen-Yi Hsiao,\nJen-Yu Liu, Yin-Cheng Yeh,": ""
        },
        {
          "available.": "",
          "[Hsiao et al., 2021] Wen-Yi Hsiao,\nJen-Yu Liu, Yin-Cheng Yeh,": "Andrew M. Dai, Matthew D. Hoffman, Monica Dinculescu, and"
        },
        {
          "available.": "[Cuthbert and Ariza, 2010] Michael Scott Cuthbert\nand Christo-",
          "[Hsiao et al., 2021] Wen-Yi Hsiao,\nJen-Yu Liu, Yin-Cheng Yeh,": "Douglas Eck. Music transformer: Generating music with long-"
        },
        {
          "available.": "pher Ariza. Music21: A toolkit for computer-aided musicology",
          "[Hsiao et al., 2021] Wen-Yi Hsiao,\nJen-Yu Liu, Yin-Cheng Yeh,": "term structure. In 7th International Conference on Learning Rep-"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "the 11th International Soci-\nVeltkamp, editors, Proceedings of"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "ety for Music Information Retrieval Conference,\nISMIR 2010,"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "Utrecht, Netherlands, August 9-13, 2010, pages 637–642. Inter-"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "national Society for Music Information Retrieval, 2010."
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "[Dalla Bella et al., 2001] Simone Dalla Bella, Isabelle Peretz, Luc"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "Rousseau,\nand Nathalie Gosselin.\nA developmental\nstudy of"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "the affective value of\ntempo and mode in music.\nCognition,"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "80(3):B1–B10, 2001."
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": ""
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "[Devlin et al., 2019]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "and Kristina Toutanova. BERT: pre-training of deep bidirectional"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "the\ntransformers for language understanding.\nIn Proceedings of"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "2019 Conference of the North American Chapter of the Associa-"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "tion for Computational Linguistics: Human Language Technolo-"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "gies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019,"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "Volume 1 (Long and Short Papers), pages 4171–4186, 2019."
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "[Ferreira and Whitehead, 2019] Lucas Ferreira and Jim Whitehead."
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "Learning to generate music with sentiment. In Arthur Flexer, Ge-"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "offroy Peeters, Juli´an Urbano, and Anja Volk, editors, Proceed-"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": ""
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "ings of the 20th International Society for Music Information Re-"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "trieval Conference, ISMIR 2019, Delft, The Netherlands, Novem-"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": ""
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "ber 4-8, 2019, pages 384–390, 2019."
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": ""
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "[Ferreira et al., 2020] Lucas Ferreira, Levi Lelis, and Jim White-"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "head.\nComputer-generated music\nfor\ntabletop\nrole-playing"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "games. In Proceedings of the AAAI Conference on Artiﬁcial Intel-"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "ligence and Interactive Digital Entertainment, volume 16, pages"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": ""
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "59–65, 2020."
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": ""
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "[Gerardi and Gerken, 1995] Gina M Gerardi and Louann Gerken."
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": ""
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "The development of affective responses to modality and melodic"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": ""
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "contour. Music Perception, 12(3):279–290, 1995."
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": ""
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "[Gregory et al., 1996] Andrew H Gregory, Lisa Worrall, and Ann"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": ""
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "Sarge.\nThe development of emotional\nresponses\nto music in"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": ""
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "young children. Motivation and Emotion, 20(4):341–348, 1996."
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": ""
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "[Grekow and Ras, 2009]\nJacek Grekow and Zbigniew W. Ras. De-"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "tecting emotions\nin classical music\nfrom MIDI ﬁles.\nIn Jan"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": ""
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "Rauch, Zbigniew W. Ras, Petr Berka, and Tapio Elomaa, edi-"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "tors, Foundations of Intelligent Systems, 18th International Sym-"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "posium, ISMIS 2009, Prague, Czech Republic, September 14-17,"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "2009. Proceedings, volume 5722 of Lecture Notes in Computer"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "Science, pages 261–270. Springer, 2009."
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": ""
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "[Hsiao et al., 2021] Wen-Yi Hsiao,\nJen-Yu Liu, Yin-Cheng Yeh,"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "and Yi-Hsuan Yang.\nCompound word transformer:\nLearning"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "to compose full-song music over dynamic directed hypergraphs."
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": ""
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "In Thirty-Fifth AAAI Conference on Artiﬁcial Intelligence, AAAI"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": ""
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "2021, Thirty-Third Conference on Innovative Applications of Ar-"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": ""
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "tiﬁcial Intelligence, IAAI 2021, The Eleventh Symposium on Ed-"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": ""
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "ucational Advances in Artiﬁcial Intelligence, EAAI 2021, Virtual"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": ""
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "Event, February 2-9, 2021, pages 178–186. AAAI Press, 2021."
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": ""
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "[Huang and Yang, 2020] Yu-Siang Huang and Yi-Hsuan Yang. Pop"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": ""
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "music transformer: Beat-based modeling and generation of ex-"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": ""
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "pressive pop piano compositions.\nIn Chang Wen Chen, Rita"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": ""
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "Cucchiara, Xian-Sheng Hua, Guo-Jun Qi, Elisa Ricci, Zhengyou"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": ""
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "The 28th\nZhang,\nand Roger Zimmermann,\neditors, MM ’20:"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": ""
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "ACM International Conference on Multimedia, Virtual Event\n/"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": ""
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "Seattle, WA, USA, October 12-16, 2020, pages 1180–1188. ACM,"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "2020."
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": ""
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "[Huang et al., 2019] Cheng-Zhi Anna Huang,\nAshish Vaswani,"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": ""
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "Jakob Uszkoreit,\nIan Simon, Curtis Hawthorne, Noam Shazeer,"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": ""
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "Andrew M. Dai, Matthew D. Hoffman, Monica Dinculescu, and"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "Douglas Eck. Music transformer: Generating music with long-"
        },
        {
          "and symbolic music data.\nIn J. Stephen Downie and Remco C.": "term structure. In 7th International Conference on Learning Rep-"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "OpenReview.net, 2019.",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": "David Luan, Dario Amodei,\nIlya Sutskever,\net al.\nLanguage"
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": "models are unsupervised multitask learners. OpenAI blog, 1(8):9,"
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "[Hung et al., 2021] Hsiao-Tzu Hung,\nJoann Ching,\nSeungheon",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": "2019."
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "Doh, Nabin Kim, Juhan Nam, and Yi-Hsuan Yang.\nEMOPIA:",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "A multi-modal pop piano dataset\nfor emotion recognition and",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": "[Russell, 1980]\nJames A Russell.\nA circumplex model of affect."
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "emotion-based music generation.\nIn Jin Ha Lee, Alexander",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": "Journal of personality and social psychology, 39(6):1161, 1980."
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "Lerch, Zhiyao Duan,\nJuhan Nam, Preeti Rao, Peter van Kra-",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": "[Thompson and Cuddy, 1997] William\nForde\nThompson\nand"
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "the\nnenburg, and Ajay Srinivasamurthy, editors, Proceedings of",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": "Lola L Cuddy. Music performance and the perception of key."
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "22nd International Society for Music Information Retrieval Con-",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": "Journal of Experimental Psychology: Human Perception and"
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "ference, ISMIR 2021, Online, November 7-12, 2021, pages 318–",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": "Performance, 23(1):116, 1997."
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "325, 2021.",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": "[Wolf et al., 2019] Thomas Wolf, Lysandre Debut, Victor Sanh,"
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "[Kastner and Crowder, 1990] Marianna\nPinchot\nKastner\nand",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": "Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cis-"
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "Robert G. Crowder.\nPerception of\nthe Major/Minor Distinc-",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": "tac, Tim Rault, R´emi Louf, Morgan Funtowicz, et al. Hugging-"
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "Music\ntion:\nIV. Emotional Connotations\nin Young Children.",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": "face’s transformers: State-of-the-art natural language processing."
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "Perception, 8(2):189–201, 12 1990.",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": "arXiv preprint arXiv:1910.03771, 2019."
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "[Krumhansl, 2001] Carol L Krumhansl. Cognitive foundations of",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": "[Yang et al., 2017] Li-Chia Yang,\nSzu-Yu Chou,\nand Yi-Hsuan"
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "musical pitch. Oxford University Press, 2001.",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": "Yang. Midinet: A convolutional generative adversarial network"
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "[Liebel and K¨orner, 2018] Lukas\nLiebel\nand\nMarco\nK¨orner.",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": "for symbolic-domain music generation. In Sally Jo Cunningham,"
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "arXiv\npreprint\nAuxiliary\ntasks\nin\nmulti-task\nlearning.",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": "Zhiyao Duan, Xiao Hu, and Douglas Turnbull, editors, Proceed-"
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "arXiv:1805.06334, 2018.",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": "ings of the 18th International Society for Music Information Re-"
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": "trieval Conference, ISMIR 2017, Suzhou, China, October 23-27,"
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "[Lin et al., 2013] Yi Lin, Xiaoou Chen, and Deshun Yang. Explo-",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": "2017, pages 324–331, 2017."
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "ration of music emotion recognition based on MIDI.\nIn Alceu",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "de Souza Britto Jr., Fabien Gouyon, and Simon Dixon, editors,",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": "[Zeng et al., 2021] Mingliang Zeng, Xu Tan, Rui Wang, Zeqian Ju,"
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "Proceedings of the 14th International Society for Music Informa-",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": "Tao Qin, and Tie-Yan Liu. Musicbert: Symbolic music under-"
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "tion Retrieval Conference, ISMIR 2013, Curitiba, Brazil, Novem-",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": "standing with large-scale pre-training.\nIn Chengqing Zong, Fei"
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "ber 4-8, 2013, pages 221–226, 2013.",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": "the\nXia, Wenjie Li,\nand Roberto Navigli,\neditors, Findings of"
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": "Association for Computational Linguistics: ACL/IJCNLP 2021,"
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "[Liu et al., 2018] Tong Liu, Li Han, Liangkai Ma,\nand Dongwei",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": "Online Event, August 1-6, 2021, volume ACL/IJCNLP 2021 of"
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "Guo. Audio-based deep music emotion recognition. In AIP Con-",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": "Findings of ACL, pages 791–800. Association for Computational"
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "ference Proceedings, volume 1967, page 040021. AIP Publishing",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": "Linguistics, 2021."
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "LLC, 2018.",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "[Livingstone et al., 2010] Steven R Livingstone, Ralf Muhlberger,",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "Andrew R Brown, and William F Thompson. Changing musical",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "emotion: A computational rule system for modifying score and",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "performance. Computer Music Journal, 34(1):41–64, 2010.",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "[Loshchilov and Hutter, 2019]\nIlya Loshchilov and Frank Hutter.",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "Decoupled weight decay regularization.\nIn 7th International",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "Conference on Learning Representations,\nICLR 2019, New Or-",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "leans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "[McKay and Fujinaga, 2006] Cory McKay\nand\nIchiro\nFujinaga.",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "jsymbolic: A feature extractor for MIDI ﬁles.\nIn Proceedings of",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "the 2006 International Computer Music Conference, ICMC 2006,",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "New Orleans, Louisiana, USA, November 6-11, 2006. Michigan",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "Publishing, 2006.",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "[Oore et al., 2020] Sageev Oore,\nIan\nSimon,\nSander Dieleman,",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "Douglas Eck,\nand Karen Simonyan.\nThis\ntime with feeling:",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "learning expressive musical performance. Neural Comput. Appl.,",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "32(4):955–967, 2020.",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "[Panda et al., 2013] Renato Eduardo Silva Panda, Ricardo Mal-",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "heiro,\nBruno Rocha, Ant´onio\nPedro Oliveira,\nand Rui\nPe-",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "dro Paiva.\nMulti-modal music\nemotion recognition:\nA new",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "dataset, methodology and comparative analysis.\nIn 10th Inter-",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "national Symposium on Computer Music Multidisciplinary Re-",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "search (CMMR 2013), pages 570–582, 2013.",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "[Panda et al., 2018] Renato Panda, Ricardo Malheiro, and Rui Pe-",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "dro Paiva.\nNovel\naudio features\nfor music\nemotion recogni-",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "tion. IEEE Transactions on Affective Computing, 11(4):614–626,",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "2018.",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "[Panda et al., 2020] Renato Panda, Ricardo Manuel Malheiro, and",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "Rui Pedro Paiva. Audio features for music emotion recognition:",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        },
        {
          "resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.": "a survey.\nIEEE Transactions on Affective Computing, 2020.",
          "[Radford et al., 2019] Alec Radford,\nJeffrey Wu, Rewon Child,": ""
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Piano sound characteristics: a study on some factors affecting loudness in digital and acoustic pianos",
      "authors": [
        "Adli"
      ],
      "year": "2007",
      "venue": "Second International Conference on Innovative Computing, Informatio and Control (ICICIC 2007)"
    },
    {
      "citation_id": "2",
      "title": "Selection of audio features for music emotion recognition using production music",
      "authors": [
        "Baume"
      ],
      "year": "2014",
      "venue": "Audio Engineering Society Conference: 53rd International Conference: Semantic Audio. Audio Engineering Society"
    },
    {
      "citation_id": "3",
      "title": "Music21: A toolkit for computer-aided musicology and symbolic music data",
      "authors": [
        "Ariza Cuthbert",
        "Scott Michael",
        "Christopher Cuthbert",
        "Ariza"
      ],
      "year": "2010",
      "venue": "Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010"
    },
    {
      "citation_id": "4",
      "title": "A developmental study of the affective value of tempo and mode in music",
      "authors": [
        "Dalla Bella"
      ],
      "year": "2001",
      "venue": "Cognition"
    },
    {
      "citation_id": "5",
      "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Devlin"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019"
    },
    {
      "citation_id": "6",
      "title": "Computer-generated music for tabletop role-playing games",
      "authors": [
        "Ferreira"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment"
    },
    {
      "citation_id": "7",
      "title": "Grekow and Ras, 2009] Jacek Grekow and Zbigniew W. Ras. Detecting emotions in classical music from MIDI files",
      "authors": [
        "Gerken Gerardi",
        "Gina Gerardi",
        "Louann Gerken",
        "; Gregory"
      ],
      "year": "1995",
      "venue": "Jan Rauch, Zbigniew W. Ras, Petr Berka, and Tapio Elomaa, editors, Foundations of Intelligent Systems, 18th International Symposium"
    },
    {
      "citation_id": "8",
      "title": "Compound word transformer: Learning to compose full-song music over dynamic directed hypergraphs",
      "authors": [
        "Hsiao"
      ],
      "year": "2020",
      "venue": "Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event"
    },
    {
      "citation_id": "9",
      "title": "EMOPIA: A multi-modal pop piano dataset for emotion recognition and emotion-based music generation",
      "authors": [
        "Huang"
      ],
      "year": "2019",
      "venue": "Proceedings of the 22nd International Society for Music Information Retrieval Conference"
    },
    {
      "citation_id": "10",
      "title": "Perception of the Major/Minor Distinction: IV. Emotional Connotations in Young Children",
      "authors": [
        "Crowder Kastner",
        "Marianna Kastner",
        "Robert Crowder"
      ],
      "year": "1990",
      "venue": "Music Perception"
    },
    {
      "citation_id": "11",
      "title": "Exploration of music emotion recognition based on MIDI",
      "authors": [
        "Carol Krumhansl",
        "Lukas Krumhansl",
        "Marco Liebel",
        "; Körner",
        "Lin"
      ],
      "year": "2001",
      "venue": "Proceedings of the 14th International Society for Music Information Retrieval Conference",
      "arxiv": "arXiv:1805.06334"
    },
    {
      "citation_id": "12",
      "title": "Audio-based deep music emotion recognition",
      "authors": [
        "Liu"
      ],
      "year": "2018",
      "venue": "AIP Conference Proceedings"
    },
    {
      "citation_id": "13",
      "title": "McKay and Fujinaga, 2006] Cory McKay and Ichiro Fujinaga. jsymbolic: A feature extractor for MIDI files",
      "authors": [
        "Livingstone"
      ],
      "year": "2006",
      "venue": "7th International Conference on Learning Representations, ICLR 2019, New Orleans"
    },
    {
      "citation_id": "14",
      "title": "This time with feeling: learning expressive musical performance",
      "authors": [
        "Oore"
      ],
      "year": "2020",
      "venue": "Neural Comput. Appl"
    },
    {
      "citation_id": "15",
      "title": "Panda et al., 2018] Renato Panda, Ricardo Malheiro, and Rui Pedro Paiva. Novel audio features for music emotion recognition",
      "authors": [
        "Panda"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "Music performance and the perception of key",
      "authors": [
        "James Russell",
        "; Russell",
        "Forde William",
        "Lola Thompson",
        "Cuddy"
      ],
      "year": "1980",
      "venue": "Journal of Experimental Psychology: Human Perception and Performance"
    },
    {
      "citation_id": "17",
      "title": "Huggingface's transformers: State-of-the-art natural language processing",
      "authors": [
        "Wolf"
      ],
      "year": "2019",
      "venue": "Huggingface's transformers: State-of-the-art natural language processing",
      "arxiv": "arXiv:1910.03771"
    },
    {
      "citation_id": "18",
      "title": "Midinet: A convolutional generative adversarial network for symbolic-domain music generation",
      "year": "2017",
      "venue": "Proceedings of the 18th International Society for Music Information Retrieval Conference"
    },
    {
      "citation_id": "19",
      "title": "Symbolic music understanding with large-scale pre-training",
      "authors": [
        "Zeng"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021"
    }
  ]
}