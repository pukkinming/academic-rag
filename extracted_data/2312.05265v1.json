{
  "paper_id": "2312.05265v1",
  "title": "Multimodal Group Emotion Recognition In-The-Wild Using Privacy-Compliant Features Author Version",
  "published": "2023-12-06T08:58:11Z",
  "authors": [
    "Anderson Augusma",
    "Dominique Vaufreydaz",
    "Frédérique Letué"
  ],
  "keywords": [
    "Multimodal",
    "Privacy safe",
    "Transformer networks",
    "Group emotion recognition in-the-wild"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper explores privacy-compliant group-level emotion recognition \"in-the-wild\" within the EmotiW Challenge 2023. Group-level emotion recognition can be useful in many fields including social robotics, conversational agents, e-coaching and learning analytics. This research imposes itself using only global features avoiding individual ones, i.e. all features that can be used to identify or track people in videos (facial landmarks, body poses, audio diarization, etc.). The proposed multimodal model is composed of a video and an audio branches with a cross-attention between modalities. The video branch is based on a fine-tuned ViT architecture. The audio branch extracts Mel-spectrograms and feed them through CNN blocks into a transformer encoder. Our training paradigm includes a generated synthetic dataset to increase the sensitivity of our model on facial expression within the image in a data-driven way. The extensive experiments show the significance of our methodology. Our privacy-compliant proposal performs fairly on the EmotiW challenge, with 79.24% and 75.13% of accuracy respectively on validation and test set for the best models. Noticeably, our findings highlight that it is possible to reach this accuracy level with privacy-compliant features using only 5 frames uniformly distributed on the video.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition research is of interest in multimodal interaction for numerous applications like social robotics, conversational agents, e-coaching, or learning analytics. Among others, one challenge is the automatic recognition of grouplevel emotions in ecological or \"in-the-wild\" scenarios, which involves considering group dynamics, individual behavior, emotional expressions, postures, environmental elements, and different kinds of activities  [18] . This task is further complicated by cultural and ethnic factors and some technical issues like the quality of recording point of view. Adding concerns about ethics and privacy limits some choices in the data that can be employed as input and outputs of the machine learning algorithms.\n\nThis article presents a privacy-compliant architecture to classify audio-visual data labeled with group-level emotions: the VGAF dataset  [18] . This machine learning model materializes our participation in the EmotiW 2023  [2]  challenge. As our former research, Petrova et al.  [15] , we impose ourselves some privacy rules on the input of our machine learning model. Unlike other approaches  [12, 17, 19, 20] , our privacy-compliant model must not use any information that can be helpful to identify or track any person on the videos. It avoids individual data like postures or facial landmarks, for instance, focusing only on global features computed on images and sound.\n\nThe paper is structured as follows. After the presentation of related work (section 2) and the VGAF dataset (section 3), section 4 introduces our methodology. Section 5 details our extensive experiments while section 6 discusses the results in regard to the state-of-the-art.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Progresses in machine learning in the last decade permit to address emotion recognition in real-world or \"in the wild\" scenarios, and more recently group level emotion prediction in videos. While some research remains monomodal  [14, 15, 17] , most of them use multimodal inputs  [1, 6, 16] . Surrace et al.  [21]  used a method that examines both general and specific information. They focused on the overall scene in a top-down strategy. They also extracted personal information in a bottomup strategy, isolating faces from wider images. Other studies  [1, 8-12, 17, 19, 23, 25 ] also used individuals' information for emotion recognition. For example, Liu et al.  [12]  used a network for recognizing facial emotions and body poses, while Fisher et al  [25]  developed a method to detect facial expressions. This research goes in a different direction by focusing on global features to avoid harming as far as possible privacy.\n\nThe use of synthetic data for model training is a wellestablished practice in machine learning research. This datadriven method is often employed by researchers to enhance the generalizability or, a contrario, the specificity of their models. For instance, Dwibedi et al.  [4]  harnessed real videos to construct synthetic repetition videos for training their \"Counting Out Time\" model. Marín-Jiménez et al.  [13]  altered the relative head positions of individuals in images to comply with a real scenario. Their aim was to improve the generalizability of their Look At Each Other (LAEO) model. In research specifically related to the EmotiW challenge, Petrova et al.  [15]  employed monomodal synthetic static images to enable their model to concentrate on individuals' faces while disregarding background. Their method creates static images to train their feature extraction. This research extends this approach to generate coherent video sequences coupled with class-related sound.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Vgaf Dataset",
      "text": "The VGAF dataset  [18]  is a collection of web videos that include a wide variety of genders, ethnicities, event types, numbers of people, and poses. These videos are grouped into three categories of group level emotions: Positive, Neutral, and Negative (as shown in Figure  1 ). The classification of each video was determined by a voting process involving several annotators, with a majority vote deciding the final classification. The dataset is divided into a Training set, a Validation set, and a Test set, with 2661, 766, and 756 videos in each set respectively. The Validation set is unevenly distributed, with more videos in the Positive and Neutral categories compared to the Negative category.\n\nThe videos include groups of at least two people and feature a range of events like interviews, festivals, parties, and protests, among others. They vary in resolution, and are split up into 5-second labeled clips. The frame rate varies from 13 to 30 frames per second. The camera focuses on different positions in each video, resulting in frames that highlight different group organisations. Thus, the technical and content varieties of the videos, the association of images and sounds to identify emotions, make the classification complexity of the VGAF dataset.\n\nOne of the dataset's complexities is that some videos feature children playing and hitting tables or chairs, generating sounds that may be confused with protestation noises. However, these two events are different because playing with children creates a positive atmosphere, while protestations generate a negative atmosphere.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Model Architecture",
      "text": "The proposed neural architecture is illustrated in Figure  2 . This model encompasses two monomodal branches producing embedding for video and audio frames. The video branch exploits a pre-trained, then fine tuned, vision transformer (ViT-Large, 14 patches)  [3]  ending by a custom linear layer with an output dimension of 1024, i.e. the embedding dimension of the model (d model ). The audio branch consumes a sequence of Mel-Spectrograms as input through four specialized CNN blocks. As depicted, these blocks are composed of 2D convolutions stack with Relu and a final max pooling. The final section of the audio branch integrates a Transformer Encoder layer, as outlined in  [22]  with fixed positional-encoding and four self-attention heads. The feed-forward hidden-size is set at twice the value of d model , while output dimension remains d model . To integrate interaction between audio and video modalities, a late fusion mechanism concatenates embeddings from the monomodal branches jointly with a Cross Attention one. In cross-attention, the audio modality serves as query (Q), while the video modality serves as key (K) and value (V ). The resulting attention weights are then multiplied with the video embeddings to obtain the final transformed representation. An average operation is then applied to the concatenated sequence averaging all frame embeddings to one of size 3 * d model . The classification task is carried out using a fully-connected layer followed by a Softmax activation, which yields to the final classification result in 3 classes.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Privacy Compliant Features",
      "text": "As stated, one self-imposed constraint is to remain as privacy compliant as possible. To do so, our approach carefully avoids all individual features, i.e. features that could identify a person directly or within a group, focusing on global features.\n\nFor the video branch, our system does not employ body pose, shape, height, nor body language features like gesture or agitation. Computed facial information like FACS, emotion or person id are excluded, like counting and tracking individuals within the scene. The video input is a set of n still video frames uniformly taken over the video and resized to 224 × 224. n is chosen among two extrema: 1 frame per second (fps) and 15 frames per second on the 5-second videos of VGAF. From our experience, 1 fps is a lower band to get a non-random classification. As 15 fps is the lowest fps on the training videos, we set it as max fps to train on actual frames without repeating some of them. Thus, n is taken in {5,75} for each video in this research. For the audio processing, we avoid speaker identification, speaker diarization or any speech-to-text processing. The audio of all videos is standardized by resampling it at 16 Khz and converting it to mono channel. By compliance with video branch, we extract 5 or 75 audio frames per video. With 5 frames, the audio frame corresponds to 1 second with no overlap. In the case of 75 frames, a sliding window is set to 67 milliseconds to get the right number of frames. Last, each audio frame is converted into Mel-Spectrograms using 128 Mel filters, to produce an input image of 128 × 251 for the CNN blocks.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Synthetic Video Dataset",
      "text": "Inspired by former research  [15] , we augmented our training set with synthetic data. The purpose is to guide the neural network to concentrate on faces while searching for positive, neutral or negative contexts, ignoring the background. The original generation process involves placing real faces expressing basic emotions  [5]  onto random backgrounds  [26]  to create still images conveying emotions. To create synthetic videos, we take from 3 to 9 faces expressing the right emotions, for instance positive emotions to generate positive video, and randomly move them on a fixed background. To mitigate the impact of occlusions, we introduce masks in the generation process, which allows a maximum of ten percent of occlusion on faces. As generating audio is a very complex task in our context, we associated randomly an audio with the same class from the VGAF training set to the generated video sequence. The synthetic video dataset has the same size and class balance as the VGAF training dataset: 802 (30,16%), 923 (34,71%), 934 (35,13%) of positive, neutral and negative videos respectively.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Optimal Synthetic Video Ratio",
      "text": "We conducted experiments to measure the impact of synthetic data ratio on performance using 5 frames per video. The synthetic ratio was gradually increased from 10% to 50% with a 10% step of the total training data. The accuracies on the val- Following this result, in all subsequent experiments comprising standard and synthetic data, the synthetic ratio is set to 30%.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multimodal Ablation Study",
      "text": "To evaluate relevance of each of our training inputs, (VGAF video and audio, synthetic data), experiments of all their combinations with 5 and 75 frames were conducted. Models were trained using cross-entropy loss and the SGD optimizer with a fixed learning rate of 10 -5 . We used the VGAF (audio and/or video) and the synthetic datasets in monomodal and multimodal variants architecture. The ViT-Large model consists of 24 pre-trained transformer encoder blocks on 14 patches. Its weights are frozen for 10 epochs, and then released to be finetuned on our data. Cross attention is present only when both video and audio branches of the network are active. Results on the VGAF validation set are summarized in Table  2 .\n\nTraining on audio data only leads to average performance up to 56.4% with 5 frames per video. Using 75 audio frames, the performance decreases. This may be explained by the averaging effect of the sliding window applied in this case. Using only synthetic videos performs better with arround 60% of accuracy, showing interest of this approach. The best mono input performance is 74.15% using 75 video frames. As in many computer vision tasks, pre-trained vision transformers prove to be efficient to capture relevant features on the VGAF dataset. Combining audio and videos inputs produces various performance in term of validation accuracy. Joining synthetic video and audio does not improve much the performance of individual inputs. The best multimodal performance (78.07%) is obtained by combining all inputs using 5 frames. Our best accuracy concerns a monomodal combination of VGAF and synthetic video dataset during training phase. This model outperforms all other models with a validation accuracy of 79.29%.\n\nFive monomodal and multimodal versions of our model participated in the EmotiW challenge 2023. Performances on the test set are reported in Table  3 . As anticipated, audio and synthetic data models (v1 and v2) remain around 55% of test accuracy. Unlike validation performance, video plus synthetic model (v3) is less accurate than the 2 multimodal ones (v4 and v5). Noticeably, both multimodal systems, using 5 (v4) or 75 frames (v5) per video, have the same performance even if their predictions differ: their prediction agreement, i.e. when the same class is predicted for the same video, is 88%. As expected, prediction agreements of v5 versus v1 and v3 are respectively 53% and 90%, highlighting that, for our architecture, audio provides less information than video.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Discussion",
      "text": "Excluding systems from the EmotiW 2023  [2]  challenge as results were not available at the writing time of this article, one can compare the 5 versions of our model with the stateof-the-art. Accuracies on validation and test sets are reported in Table  4 . Using only audio, as we limited input to Melspectrograms, our v1 model do not reach score of Ottl et al.  [14]  proposal, due to their more complex usage of OpenS-MILE features  [7]  coupled with Deep Spectrum analysis. On ✓ 61.61% 66.00% Pinto et al.  [16]  65.74% -Wang et al.  [24]  66.19% 66.40% Sun et al.  [20]  ✓ 71.93% -Belova et al.  [1]  71.95% -Liu et al.  [  video only system, the v3 model have the best performance even if, as said, its test accuracy is lower than its validation counterpart. On multimodal systems, v4 and v5 remain under the proposal of Liu et al.  [12] . Nevertheless, our approach exposes fair performance while using only global thus privacycompliant features.\n\nLooking at intrinsic analysis of our models, they benefit from the pre-trained ViT network fine-tuned on the VGAF data and from our synthetic video approach but lack in the audio branch.\n\nIncreasing the number of parameters like number of attention heads wages unconditionally to overfitting, showing that such architectures require a lot of training data. Gathering more group emotion data, (transfer) learning on close or generated data can be part of the solution. Our synthetic generation process is straightforward and can be enhanced in several ways. Generative Adversarial Networks can be trained to create new data. But all these approaches are limited by the complexity and diversity of group emotion videos in terms of audio and video content.\n\nAs anticipated, avoiding individual features leads to set a thread-off between performance and privacy. In this research, our aim was to investigate the maximum performance that could be obtained without contravening this rule. Depending on the target application context, some additional information could be added to improve performance.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "This research introduces a privacy-compliant yet efficient method for recognizing group emotions in uncontrolled environments, proposing to eliminate the need for individual feature extraction. The privacy compliance is achieved through a selection of non-individual characteristics in signal inputs, providing only global information on the scene. The proposed model is composed of a video and an audio branches. The video branch is based on a ViT architecture fine tuned to compute relevant embedding on image sequences. The audio branch extracts Mel-spectrograms and feed them through CNN blocks into a transformer encoder. A cross-attention mechanism is added before the late multimodal fusion and the final classification. Our training paradigm includes a generated synthetic dataset to increase the sensitivity of our model on facial expression within the image in a data-driven way.\n\nThe extensive experiments show the relevance of our methodology. Our proposal performs fairly on the EmotiW challenge, with 79.24% and 75.13% of accuracy respectively on validation and test set for the best models. Contributions of each modality differ, the audio branch having rooms for larger improvement. Noticeably, our findings highlight that it is possible to reach 75% of accuracy with privacy-compliant features using only 5 frames uniformly distributed per video.\n\nTo conclude, the targeted privacy compliance of this research comes at a cost in performance. The accuracy remains slightly lower than other methods employing individual features. The next research challenge is thus to validate whatever it is possible, with individual features, to minimize the performance gap of privacy-compliant models, allowing to use them in more application contexts.",
      "page_start": 4,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Examples from the VGAF dataset. From left to right, examples of Positive, Neutral, and Negative classes.",
      "page": 1
    },
    {
      "caption": "Figure 1: ). The classification of each",
      "page": 2
    },
    {
      "caption": "Figure 2: This model encompasses two monomodal branches produc-",
      "page": 2
    },
    {
      "caption": "Figure 2: At left, the proposed model is a combination of two monomodal branches, a Cross-Attention and a late fusion",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Evaluation of synthetic video ratio on the validation",
      "page": 3
    },
    {
      "caption": "Table 1: . The best",
      "page": 3
    },
    {
      "caption": "Table 2: Training on audio data only leads to average performance up",
      "page": 3
    },
    {
      "caption": "Table 3: As anticipated, audio and syn-",
      "page": 4
    },
    {
      "caption": "Table 2: Ablation study investigating the impact of differ-",
      "page": 4
    },
    {
      "caption": "Table 4: Using only audio, as we limited input to Mel-",
      "page": 4
    },
    {
      "caption": "Table 3: Test Set Accuracy of 5 versions of our model.",
      "page": 4
    },
    {
      "caption": "Table 4: Comparison with SOTA systems on the VGAF",
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Group-level affect recognition in video using deviation of frame features",
      "authors": [
        "Natalya Belova"
      ],
      "year": "2021",
      "venue": "Analysis of Images, Social Networks and Texts: 10th International Conference, AIST 2021"
    },
    {
      "citation_id": "2",
      "title": "Emotiw 2023: Emotion recognition in the wild challenge",
      "authors": [
        "Abhinav Dhall",
        "Monisha Singh",
        "Roland Goecke",
        "Tom Gedeon",
        "Donghuo Zeng",
        "Yanan Wang",
        "Kazushi Ikeda"
      ],
      "venue": "Proceedings of the 25th International Conference on Multimodal Interaction (ICMI 2023)"
    },
    {
      "citation_id": "3",
      "title": "",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer"
      ],
      "venue": ""
    },
    {
      "citation_id": "4",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "Dirk Kolesnikov",
        "Xiaohua Weissenborn",
        "Thomas Zhai",
        "Mostafa Unterthiner",
        "Matthias Dehghani",
        "Georg Minderer",
        "Sylvain Heigold",
        "Jakob Gelly",
        "Neil Uszkoreit",
        "Houlsby"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale"
    },
    {
      "citation_id": "5",
      "title": "Counting out time: Class agnostic video repetition counting in the wild",
      "authors": [
        "Debidatta Dwibedi",
        "Yusuf Aytar",
        "Jonathan Tompson",
        "Pierre Sermanet",
        "Andrew Zisserman"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "6",
      "title": "Faces-a database of facial expressions in young, middle-aged, and older women and men: Development and validation",
      "authors": [
        "Natalie Ebner",
        "Michaela Riediger",
        "Ulman Lindenberger"
      ],
      "year": "2010",
      "venue": "Behavior Research Methods",
      "doi": "10.3758/BRM.42.1.351"
    },
    {
      "citation_id": "7",
      "title": "Multimodal end-to-end group emotion recognition using cross-modal attention",
      "authors": [
        "Lev Evtodienko"
      ],
      "year": "2021",
      "venue": "Multimodal end-to-end group emotion recognition using cross-modal attention"
    },
    {
      "citation_id": "8",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin Wöllmer",
        "Björn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "9",
      "title": "Group-level emotion recognition using hybrid deep models based on faces, scenes, skeletons and visual attentions",
      "authors": [
        "Xin Guo",
        "Bin Zhu",
        "Luisa Polanía",
        "Charles Boncelet",
        "Kenneth Barner"
      ],
      "year": "2018",
      "venue": "Proceedings of the International Conference on Multimodal Interaction (ICMI 2018)",
      "doi": "10.1145/3242969.3264990"
    },
    {
      "citation_id": "10",
      "title": "Graph neural networks for image understanding based on multiple cues: Group emotion recognition and event recognition as use cases",
      "authors": [
        "Xin Guo",
        "Luisa Polanía",
        "Bin Zhu",
        "Charles Boncelet",
        "Kenneth Barner"
      ],
      "year": "2019",
      "venue": "Graph neural networks for image understanding based on multiple cues: Group emotion recognition and event recognition as use cases"
    },
    {
      "citation_id": "11",
      "title": "An attention model for group-level emotion recognition",
      "authors": [
        "Aarush Gupta",
        "Dakshit Agrawal",
        "Hardik Chauhan",
        "Jose Dolz",
        "Marco Pedersoli"
      ],
      "year": "2018",
      "venue": "Proceedings of the International Conference on Multimodal Interaction (ICMI 2018)",
      "doi": "10.1145/3242969.3264985"
    },
    {
      "citation_id": "12",
      "title": "Emotion recognition using deep learning approach from audio-visual emotional big data",
      "authors": [
        "Shamim Hossain",
        "Ghulam Muhammad"
      ],
      "year": "2019",
      "venue": "Information Fusion",
      "doi": "10.1016/j.inffus.2018.09.008"
    },
    {
      "citation_id": "13",
      "title": "Group level audio-video emotion recognition using hybrid networks",
      "authors": [
        "Chuanhe Liu",
        "Wenqiang Jiang",
        "Minghao Wang",
        "Tianhao Tang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 International Conference on Multimodal Interaction (ICMI 2020)",
      "doi": "10.1145/3382507.3417968"
    },
    {
      "citation_id": "14",
      "title": "Laeo-net: revisiting people looking at each other in videos",
      "authors": [
        "J Manuel",
        "Vicky Marin-Jimenez",
        "Pablo Kalogeiton",
        "Andrew Medina-Suarez",
        "Zisserman"
      ],
      "year": "2019",
      "venue": "Laeo-net: revisiting people looking at each other in videos"
    },
    {
      "citation_id": "15",
      "title": "Group-level speech emotion recognition utilising deep spectrum features",
      "authors": [
        "Sandra Ottl",
        "Shahin Amiriparian",
        "Maurice Gerczuk",
        "Vincent Karas",
        "Björn Schuller"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "16",
      "title": "Group-Level Emotion Recognition Using a Unimodal Privacy-Safe Non-Individual Approach",
      "authors": [
        "Anastasia Petrova",
        "Dominique Vaufreydaz",
        "Philippe Dessus"
      ],
      "year": "2020",
      "venue": "EmotiW2020 Challenge at the 22nd ACM International Conference on Multimodal Interaction (ICMI2020)"
    },
    {
      "citation_id": "17",
      "title": "Audiovisual classification of group emotion valence using activity recognition networks",
      "authors": [
        "João Ribeiro Pinto",
        "Tiago Gonc ¸alves",
        "Carolina Pinto",
        "Luís Sanhudo",
        "Joaquim Fonseca",
        "Filipe Gonc ¸alves",
        "Pedro Carvalho",
        "Jaime Cardoso"
      ],
      "year": "2020",
      "venue": "IEEE 4th International Conference on Image Processing, Applications and Systems (IPAS 2020)"
    },
    {
      "citation_id": "18",
      "title": "Neural network model for video-based analysis of student's emotions in e-learning",
      "authors": [
        "V Andrey",
        "Savchenko",
        "Makarov"
      ],
      "year": "2022",
      "venue": "Optical Memory and Neural Networks"
    },
    {
      "citation_id": "19",
      "title": "Automatic group level affect and cohesion prediction in videos",
      "authors": [
        "Garima Sharma",
        "Shreya Ghosh",
        "Abhinav Dhall"
      ],
      "year": "2019",
      "venue": "th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos",
      "doi": "10.1109/ACIIW.2019.8925231"
    },
    {
      "citation_id": "20",
      "title": "Audiovisual automatic group affect analysis",
      "authors": [
        "Garima Sharma",
        "Abhinav Dhall",
        "Jianfei Cai"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "Multi-modal fusion using spatio-temporal and static features for group emotion recognition",
      "authors": [
        "Mo Sun",
        "Jian Li",
        "Hui Feng",
        "Wei Gou",
        "Haifeng Shen",
        "Jian Tang",
        "Yi Yang",
        "Jieping Ye"
      ],
      "venue": "Proceedings of the 2020 International Conference on Multimodal Interaction (ICMI 2020)",
      "doi": "10.1145/3382507.3417971"
    },
    {
      "citation_id": "22",
      "title": "Emotion recognition in the wild using deep neural networks and bayesian classifiers",
      "authors": [
        "Luca Surace",
        "Massimiliano Patacchiola",
        "Elena Sönmez",
        "William Spataro",
        "Angelo Cangelosi"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "23",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "24",
      "title": "Cascade attention networks for group emotion recognition with face, body and image cues",
      "authors": [
        "Kai Wang",
        "Debin Meng",
        "Xiaoxing Zeng",
        "Kaipeng Zhang",
        "Yu Qiao",
        "Jianfei Yang",
        "Xiaojiang Peng"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 International Conference on Multimodal Interaction (ICMI 2018)",
      "doi": "10.1145/3242969.3264991"
    },
    {
      "citation_id": "25",
      "title": "Implicit knowledge injectable cross attention audiovisual model for group emotion recognition",
      "authors": [
        "Yanan Wang",
        "Jianming Wu",
        "Panikos Heracleous",
        "Shinya Wada",
        "Rui Kimura",
        "Satoshi Kurihara"
      ],
      "year": "2020",
      "venue": "Proceedings of the International Conference on Multimodal Interaction (ICMI 2020)",
      "doi": "10.1145/3382507.3417960"
    },
    {
      "citation_id": "26",
      "title": "Group emotion recognition based on global and local features",
      "authors": [
        "Dai Yu",
        "Liu Xingyu",
        "Dong Shuzhan",
        "Yang Lei"
      ],
      "year": "2019",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2019.2932797"
    },
    {
      "citation_id": "27",
      "title": "LSUN: construction of a large-scale image dataset using deep learning with humans in the loop",
      "authors": [
        "Fisher Yu",
        "Yinda Zhang",
        "Shuran Song",
        "Ari Seff",
        "Jianxiong Xiao"
      ],
      "year": "2015",
      "venue": "LSUN: construction of a large-scale image dataset using deep learning with humans in the loop"
    }
  ]
}