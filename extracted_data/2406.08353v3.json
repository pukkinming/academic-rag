{
  "paper_id": "2406.08353v3",
  "title": "Speech Emotion Recognition With Asr Transcripts: A Comprehensive Study On Word Error Rate And Fusion Techniques",
  "published": "2024-06-12T15:59:25Z",
  "authors": [
    "Yuanchao Li",
    "Peter Bell",
    "Catherine Lai"
  ],
  "keywords": [
    "Speech Emotion Recognition",
    "Word Error Rate",
    "Fusion Techniques",
    "ASR Error Correction"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Text data is commonly utilized as a primary input to enhance Speech Emotion Recognition (SER) performance and reliability. However, the reliance on human-transcribed text in most studies impedes the development of practical SER systems, creating a gap between in-lab research and real-world scenarios where Automatic Speech Recognition (ASR) serves as the text source. Hence, this study benchmarks SER performance using ASR transcripts with varying Word Error Rates (WERs) from eleven models on three well-known corpora: IEMOCAP, CMU-MOSI, and MSP-Podcast. Our evaluation includes both text-only and bimodal SER with six fusion techniques, aiming for a comprehensive analysis that uncovers novel findings and challenges faced by current SER research. Additionally, we propose a unified ASR error-robust framework integrating ASR error correction and modality-gated fusion, achieving lower WER and higher SER results compared to the best-performing ASR transcript. These findings provide insights into SER with ASR assistance, especially for real-world applications.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion in speech is conveyed through both what is said (i.e., text) and how it is said (i.e., audio). Recent studies on Speech Emotion Recognition (SER) consistently highlight the benefits of combining textual and acoustic features  [1, 2, 3, 4] . Despite the advancements in SER over the years, its practical application in everyday contexts remains limited. One significant challenge is the reliance on human-annotated text, known as gold-standard manual transcripts. In contrast, transcripts generated by Automatic Speech Recognition (ASR) systems, even state-of-the-art ones, often suffer from high Word Error Rates (WERs), particularly in emotional speech  [5] . As a result, findings from lab settings may not effectively translate to real-world scenarios, impeding the true progress of SER.\n\nTo address this gap, it is crucial to investigate the impact of imperfect text data generated by ASR on SER. While previous studies have explored the question of \"how ASR performance affects SER\", the findings have been inconsistent and not comprehensive. For example, Schuller et al. observed that a WER of over 30% resulted in an SER accuracy drop of less than 3% on the FAU Aibo Emotion Corpus  [6] , whereas Li et al. reported a nearly 10% accuracy drop with a 30% WER on IEMOCAP  [5] . Furthermore, while multimodal fusion techniques have been proposed and extensively studied to enhance emotion recognition performance, their compatibility with ASR transcripts remains underexplored  [7, 8, 9] .\n\nIn this study, we begin with benchmarking SER performance using ASR transcripts. We generate ASR transcripts from multiple ASR systems to achieve varying WERs. Subsequently, SER is conducted exclusively on these transcripts to evaluate the impact of ASR errors on text-only SER performance. Following this, we integrate audio features and perform bimodal SER to investigate whether fusion techniques, effective with ground-truth text, also demonstrate efficacy with ASR transcripts (i.e., their resilience to ASR errors). Building on the benchmark study, we propose an ASR error-robust SER framework that integrates ASR error correction with modality-gated fusion to address both WER and fusion challenges in SER. The contributions of this work can be summarized as follows:\n\n• As the first work to benchmark SER performance with ASR transcripts, we examine a wide range of models, varying WERs, and multiple fusion techniques. This sets the stage for advancing SER research with ASR integration, enhancing its practical applications.\n\n• We propose an ASR error-robust framework that integrates two-stage ASR error correction and dynamic modalitygated fusion, aiming to reduce word errors and mitigate the negative effects of high WER on SER performance.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Over the past several years, significant progress has been achieved in SER through the utilization of ASR transcripts. Specifically, in chronological order:  Yoon et al. (2018)  proposed a deep dual recurrent encoder model that simultaneously utilizes audio signals and text data from the Google Cloud Speech API  [3] .  Sahu et al. (2019)  utilized two commercial ASR systems to generate transcripts for bimodal SER (audio + text), resulting in a relative loss of unweighted accuracy compared to ground-truth transcripts  [10] .  Li et al. (2020)  introduced a temporal alignment mean-max pooling mechanism to capture subtle and fine-grained emotions in utterances, alongside a crossmodality excitement module for sample-specific adjustments on embeddings  [11] .  Santoso et al. (2021)  proposed using a confidence measure to adjust the importance weights in ASR transcripts based on the likelihood of a speech recognition error in each word, effectively mitigating the effects of ASR errors on SER performance  [12] .  Wu et al. (2021)  introduced a dual-branches model for ASR error robustness, with a timesynchronous branch combining speech and text modalities and a time-asynchronous branch integrating sentence text embeddings from context utterances  [13] .  Shon et al.(2021)  generated pseudo labels on ASR transcripts for semi-supervised speech sentiment analysis  [14] .\n\nInspired by human perception mechanisms, Li et al. (2022) proposed hierarchical attention fusion of audio features, ASR hidden states, and ASR transcripts, achieving similar SER performance as ground-truth text  [15] .  Lin et al. (2023)  explored complementary semantic information from audio to mitigate the impact of ASR errors, using an attention mechanism to calculate weighted acoustic representations fused with text representations of ASR hypotheses  [16] .  He et al. (2024)  incorporated two auxiliary tasks, ASR error detection and ASR error correction, to enhance the semantic coherence of ASR text. They introduced a novel multi-modal fusion method to learn shared representations across modalities  [17] .  Feng et al. (2024)  fused audio with ASR transcript from a powerful ASR model and identified that ASR-generated output delivers competitive SER performance compared to ground-truth transcripts  [18] .\n\nWhile these studies have highlighted the effectiveness of integrating text features from ASR transcripts, there is still a lack of understanding regarding how WER and fusion techniques impact SER. Therefore, we undertake a benchmark study utilizing diverse ASR models and fusion techniques, conducting SER on various emotion corpora to get a clearer picture of the effect of transcription errors on SER. Furthermore, drawing from the existing literature, we develop an ASR error-robust framework aimed at mitigating the adverse effect of WER 1 .",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Asr Models And Emotion Corpora",
      "text": "ASR models. We adopt the following 11 models as they are widely used in the speech area and can provide varying WERs.\n\n• Wav2Vec2-base-{100h,960h} • Wav2Vec2-large-960h • Wav2Vec2-large-960h-lv60-self • HuBERT-large-ls960-ft 1 https://github.com/yc-li20/SER-on-WER-and-Fusion\n\n• WavLM-libri-clean-100h-base-plus • Whisper-{tiny, base, small, medium, large-v2}.en Emotion corpora. To ensure generalizability, we utilize three corpora: IEMOCAP  [19] , CMU-MOSI  [20] , and MSP-Podcast  [21] , to include diverse speech conditions and various evaluation metrics for both discrete and continuous emotions.\n\nIEMOCAP consists of five sessions of scripted and improvised dialogues conducted in a research lab. Our study focuses on four emotion classes: angry, happy (including excited), neutral, and sad. We exclude utterances with blank transcripts from this corpus, resulting in a total of 5,500 utterances. CMU-MOSI comprises 2,199 monologue video clips sourced from YouTube, each annotated with sentiment scores ranging from -3 to 3. MSP-Podcast contains English speech extracted from podcast recordings, annotated with valence, arousal, dominance scores in the range of 1 to 7, as well as categorical emotion labels. We use its Release 1.11 version and evaluate its performance on the Test1 set. We exclude utterances without an emotion label, resulting in a total of 104,663 utterances.\n\nFollowing the literature, we compute Acc4 (four-class accuracy) for IEMOCAP, Concordance Correlation Coefficient (CCC) for MSP-Podcast, and Acc2 (binary accuracy), Acc7 (seven-class accuracy), and Mean Absolute Error (MAE) for CMU-MOSI. Detailed explanations are omitted as these metrics are commonly used for their respective corpora  [7, 13] .\n\nWe initially considered including MELD  [22]  (TV sitcom data) in our investigation. However, its WERs are nearly double those of the other three corpora, ranging from 30% to 65%. Given that conducting SER using transcripts with such poor ASR performance is impractical in real-world scenarios, we decided to focus on the other three corpora for our subsequent study. Nevertheless, we present the WERs of MELD for reference (see Table  1 ).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Benchmark Study",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Ser Modeling",
      "text": "We employ RoBERTa-base as the text encoder. Given that transcripts generated by models other than Whisper lack punctuation, we remove punctuation from the transcripts of Whisper models to ensure a fair comparison. All letters are lowercased for consistency. A backbone SER model is built for all corpora. Since our goal is not to achieve state-of-the-art performance, the model simply comprises two dense layers: the first encodes RoBERTa output of dimension 768 into hidden states of dimension 128, and the second further encodes it into a dimension of 16. We use ReLU as the activation function between the dense layers. In the case of IEMOCAP, we apply one output layer with Softmax activation for classification. For MSP-Podcast and CMU-MOSI, which involve regression tasks, no final output activation is applied. We set the learning rate as 5e-4 for IEMOCAP and CMU-MOSI, and 1e-4 for MSP-Podcast, using the AdamW optimizer. The weight decay is set as 1e-5, and the batch size is 64. For training, we employ five-fold cross-validation on IEMOCAP (100 epochs) and follow the official training/validation/testing sets on CMU-MOSI (100 epochs) and MSP-Podcast (30 epochs). The random seeds are kept consistent across all experiments.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Benchmarking Ser With Wer",
      "text": "Firstly, we present the WERs and corresponding SER performance based on transcripts from different ASR models. From Table  1 , we observe that:\n\n1) SER performance generally decreases as WER increases. On IEMOCAP and MOSI, there is nearly a 10% accuracy decrease with WERs around 40%, regardless of Acc2, Acc4, or Acc7. However, exceptions exist. For example, on IEMOCAP, Whisper-tiny produces higher accuracy than its neighboring models, even though the WERs are similar. Additionally, W2V960-large-self yields worse accuracy compared to its neighboring models, despite having a relatively lower WER compared to others. The same phenomenon can also be found with CMU-MOSI: the Acc2 of W2V100 is higher than that of WavLM plus, and the Acc2 of Whisper-large is lower than that of Whisper-medium. This might be due to certain words being misrecognized as words that have little effect on or even positively contribute to their ground-truth emotion labels. Note that the values of our Acc4 are higher than those in  [5] , possibly because of differences in training models, language models (RoBERTa vs. BERT), and word embeddings (full hidden states vs. pooler output).\n\n2) SER is robust to relatively low WER, and in some cases, it is even better with ASR errors. From IEMOCAP, it is observed that a WER of approximately 12% has minimal impact on SER performance compared to ground-truth transcripts. Moreover, in CMU-MOSI, ASR errors can potentially enhance SER: transcripts generated by specific Whisper models outperform those based on ground truth. Prior research has shown that sentiment analysis remains resilient to ASR errors  [23] , as positive or negative sentiments can encompass varied emotional states. Additionally, we find that in certain cases (e.g., relatively low WER), ASR errors do not diminish SER performance. Recent research on speechbased dementia detection indicates that ASR errors can offer valuable insights for dementia classification  [24] , possibly due to specific types of errors (e.g., repetitions, disfluencies) that may reveal indicators of dementia. However, we do not believe this applies similarly to SER. The phenomenon observed in our study may align with our previous finding: certain words being misrecognized as words that positively contribute to the ground-truth emotions.\n\n3) Different metrics have different sensitivities to WER. On CMU-MOSI, Acc7 shows a distinct pattern compared to Acc2 and MAE. Acc2 and MAE demonstrate consistent and smooth variations, whereas Acc7 appears random Table  1 . Benchmarking SER performance based on ASR transcripts across corpora. ↑: higher the better. ↓: lower the better. Red bold: better performance than the ground truth. and lacks a discernible pattern. Interestingly, in nearly half of the transcripts (all from Whisper), Acc7 outperforms the ground truth. This discrepancy may stem from the mismatch between the regression model used during training (since MOSI is labeled with continuous values) and the classification metric applied to Acc7, where predicted and ground-truth values are grounded. This mismatch could potentially blur accuracy and render SER insensitive to WER.\n\n4) Different emotion dimensions exhibit varying degrees of robustness to ASR errors. From MSP-Podcast, it is evident that valence, arousal, and dominance exhibit distinct patterns. Firstly, the CCC of valence mirrors the pattern Table  2 . Benchmarking SER performance based on ASR transcripts with fusion techniques. Maximum diff: maximum difference between the performance of ground truth and that of ASR transcripts. Red bold: better performance than the ground truth. observed in Acc4 for IEMOCAP and Acc2 for CMU-MOSI, suggesting that valence shares similarities with categorical emotion in terms of robustness to ASR errors. Given that valence is conceptually similar to sentiment (indicating positivity or negativity), this alignment is plausible. Secondly, arousal and dominance do not exhibit a clear correlation with WERs. Since we jointly predict valence, arousal, and dominance, there may be shared information among these dimensions, resulting in minimal changes in arousal and dominance. However, our observations are consistent with the conventional understanding that valence is more influenced by textual content, whereas arousal and dominance are more influenced by audio cues  [25, 26, 27] . Indeed, when we replaced text features with middle-layer audio features from W2V960base in the SER model, we obtained CCC v, CCC a, and CCC d values of 0.531, 0.635, and 0.558, respectively, further validating our observations.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Benchmarking Ser With Wer And Fusion",
      "text": "Next, we integrate audio features to investigate the robustness of existing fusion techniques in handling ASR errors in realworld scenarios. Since audio is not the primary focus of this study, we simply use the middle-layer audio features from W2V960-base. For fusion techniques, we employ the following approaches: • Early fusion: text and audio features are concatenated at the embedding level.\n\n• Late fusion: Text and audio features are learned independently by their respective models, and the final decision is determined based on their outputs.\n\n• Cross-attention fusion: text and audio features are attended to each other via attention mechanism and then concatenated.\n\n• Tensor fusion  [28] : unimodal information and bimodal interactions are learned explicitly and then aggregated.\n\n• Non-local gate-based (NL-gate) Fusion  [29] : NL-gate is incorporated with multiple directions and at multiple positions with query-key-value within the attention mechanism.\n\n• Modality-invariant and -specific fusion (MISA)  [30] : combining both modality-invariant and modality-specific features.\n\nFor fairness, we keep the backbone SER model used in the previous section unchanged, modifying only the input dimension of the first dense layer to match the output dimension of the hidden states from each fusion model. Additionally, we modify Tensor Fusion and MISA to receive bimodal inputs, as they were originally designed for trimodal inputs. Due to limited space, we present results only for IEMOCAP and MSP-Podcast for brevity, as these two corpora are sufficient to cover both categorical and dimensional emotions. Regarding the patterns observed in CMU-MOSI (omitted here), integrating audio did not significantly improve performance, as text alone already yields powerful results. Moreover, the audio in CMU-MOSI does not strongly convey emotion, leading to incongruity and ambiguity, which may pose challenges for SER in certain cases. From Table  2 , it can be seen that:\n\n1) Fusing audio features largely mitigates the negative impact of increasing WER. The decrease in Acc4 based on WER reaches 10% without fusion on IEMOCAP, but only 4% with fusion. Moreover, with most fusion techniques, the SER performance is even better on transcripts with relatively low WER than on ground truth. These findings confirm the benefits of bimodal SER in real applications, which can be achieved by using a pre-trained ASR model as a base for jointly generating transcripts and audio features  [15, 31] .\n\n2) There is no optimal WER-robust fusion approach. Due to different amounts of encoded information, the performances of fusion techniques are not directly comparable. For instance, Tensor Fusion and MISA encode more information by incorporating both unimodal and bimodal features, or both modality-invariant and -specific features. However, their effectiveness varies across different corpora. For example, they perform well on IEMOCAP (categorical) but poorly on MSP-Podcast (dimensional), indicating significant inconsistency. Thus, it is difficult to identify a single best-performing technique that is universally effective in addressing ASR errors. Nevertheless, the maximum difference between the performance of ground truth and that of ASR transcripts is still indicative of the robustness of the fusion techniques. We observe that although cross-attention fusion may not produce the best performance, it yields a relatively small maximum difference, suggesting that it is less affected by WER. This observation is reasonable since cross-attention captures the relatedness between bimodal inputs, which changes dynamically once the words in text have changed, ensuring that the relatedness is least affected by ASR errors.\n\nSince our experimental design impacts the results to some extent (e.g., removing punctuation, converting letters to lowercase, excluding samples without corresponding text transcription, modifying existing fusion models), it could inevitably lead to variations and exceptions in some SER performances. However, our overall findings and conclusions remain valid, especially as some of them are consistent with existing findings  [15, 17, 18] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Asr Error-Robust Framework",
      "text": "To address both the WER and fusion issues, we propose an ASR error-robust framework involving ASR error correction and modality-gated fusion, as illustrated in Fig.  1  (left side).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Asr Error Correction",
      "text": "We combine the transcripts from all used ASR models, forming N-best (N=11) hypotheses. Compared to using raw Nbest hypotheses directly from a single ASR model, which offer limited diversity, such a combination offers a more diverse range of transcription results. Furthermore, this approach facilitates the comparison of SER performance with each ASR model, as depicted in Table  2 .\n\nThen, the ASR error correction process consists of a twostage process. First, we utilize the Alpaca prompt  [32] , which guides a Large Language Model (LLM) to generate the most likely transcript (i.e., 1-best hypothesis) from the N-best hypotheses and correct any grammatical or logical errors. An illustration of the Alpaca prompt is depicted on the right of Fig.  1 . We employ InstructGPT, which has demonstrated effectiveness in LLM-based ASR error correction  [33] .\n\nIn the second stage, we further refine the 1-best transcripts by utilizing a Sequence-to-Sequence (S2S) ASR error correction model  [34] , which was pre-trained on the Whisper-tiny output y ′ (i.e., 1-best hypothesis) of Common Voice  [35]  to recover the corrected sequence y (i.e., gold annotation), with an optimizable parameter θ:",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Modality-Gated Fusion",
      "text": "Based on the quality-enhanced transcripts, we adapt a trimodal incongruity-aware fusion method  [7]  to our bimodal inputs, considering potential distortion of original emotion in the spoken content due to ASR errors, which may introduce incongruence between emotions in audio and text. Specifically, each modality is initially assigned an equal trainable weight. These weights dynamically adjust during training based on the respective contribution of each task, with larger contributions resulting in higher values. The sum of the weights always equals 1, and they are updated in every training batch to ensure dynamic adaptation to any input batch size. Subsequently, the weights are multiplied by corresponding modality features to perform weighted cross-attention fusion. The pseudo-code is shown in Algorithm 1, where H represents the hidden representation as input to the backbone SER model. CrossAttn and Self Attn denote multihead attention, the former with the first argument as query and the second as key and value, and the latter with the argument as query, key, and value. The head number is set to eight.\n\nFinally, we apply the same backbone SER model, keeping all other settings unchanged (e.g., the text and audio encoders) as in Section 4. We believe that this combination of dynamic modality gate and weighted attention can effectively mitigate the negative impact of ASR errors by consistently focusing on the most salient parts in both inputs for SER.\n\nAT * H ; 16 end for",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results And Discussion",
      "text": "The results are presented in Table  3 . We include only the MAE on CMU-MOSI and the average CCC on MSP-Podcast for brevity. Our approach further reduces WER and improves SER performance, demonstrating its effectiveness. While it does not surpass the best transcript in terms of MAE on CMU-MOSI, the values are very close. Moreover, in line with the findings in  [17] , our results confirm that employing proper ASR error-robust approaches can exceed SER performance based on ground-truth text using ASR transcripts. This phe-nomenon likely occurs because certain misrecognized words are interpreted as contributing positively to the ground-truth emotions, rather than ASR errors being preferable in SER.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "In this study, we conducted a benchmark of SER performance using ASR transcripts with varying WERs and explored mainstream fusion techniques to assess the impact of ASR performance on SER. Our findings revealed several novel insights: 1) SER can tolerate relatively low WERs, especially in real-life speech scenarios. 2) Bimodal SER with transcripts containing approximately 10% errors may not perform worse than those with ground-truth text, particularly with powerful Whisper models. However, further analysis is necessary to understand the nature and locations of ASR errors, as well as the mechanisms underlying fusion techniques. Moreover, we proposed an ASR error-robust framework that integrates ASR error correction and modality-gated fusion. This framework demonstrated superior performance compared to baseline models in our experimental results and holds potential for various tasks where ASR transcription serves as the text source. In the near future, we plan to launch a challenge aimed at addressing ASR errors for robust multimodal SER.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (left side).",
      "page": 5
    },
    {
      "caption": "Figure 1: We employ InstructGPT, which has demonstrated ef-",
      "page": 5
    },
    {
      "caption": "Figure 1: Proposed ASR error-robust framework. Blue: ASR error correction; frozen. Pink: Modality-gated fusion; trainable.",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table 3: Performance comparison. ↑: higher the better. ↓:",
      "data": [
        {
          "WER↓ MAE↓": "18.69 0.8558\n0.00 0.8902"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "MMER: Multimodal multi-task learning for speech emotion recognition",
      "authors": [
        "Sreyan Ghosh",
        "Utkarsh Tyagi",
        "S Ramaneswaran",
        "Harshvardhan Srivastava",
        "Dinesh Manocha"
      ],
      "year": "2022",
      "venue": "MMER: Multimodal multi-task learning for speech emotion recognition",
      "arxiv": "arXiv:2203.16794"
    },
    {
      "citation_id": "3",
      "title": "Fusion Techniques for Utterance-Level Emotion Recognition Combining Speech and Transcripts",
      "authors": [
        "Jilt Sebastian",
        "Piero Pierucci"
      ],
      "year": "2019",
      "venue": "Proceedings of Interspeech 2019"
    },
    {
      "citation_id": "4",
      "title": "Multimodal speech emotion recognition using audio and text",
      "authors": [
        "Seunghyun Yoon",
        "Seokhyun Byun",
        "Kyomin Jung"
      ],
      "year": "2018",
      "venue": "2018 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "5",
      "title": "Dual memory fusion for multimodal speech emotion recognition",
      "authors": [
        "Darshana Priyasad",
        "Tharindu Fernando",
        "Sridha Sridharan",
        "Simon Denman",
        "Clinton Fookes"
      ],
      "venue": "Proc. INTERSPEECH, 2023"
    },
    {
      "citation_id": "6",
      "title": "ASR and Emotional Speech: A Word-Level Investigation of the Mutual Impact of Speech and Emotion Recognition",
      "authors": [
        "Yuanchao Li",
        "Zeyu Zhao",
        "Ondřej Klejch",
        "Peter Bell",
        "Catherine Lai"
      ],
      "year": "2023",
      "venue": "Proc. INTERSPEECH 2023"
    },
    {
      "citation_id": "7",
      "title": "Releasing a thoroughly annotated and processed spontaneous emotional database: the fau aibo emotion corpus",
      "authors": [
        "Batliner",
        "Steidl",
        "Nöth"
      ],
      "year": "2008",
      "venue": "Programme of the Workshop on Corpora for Research on Emotion and Affect"
    },
    {
      "citation_id": "8",
      "title": "Cross-attention is not enough: Incongruity-aware dynamic hierarchical fusion for multimodal affect recognition",
      "authors": [
        "Yaoting Wang",
        "Yuanchao Li",
        "Paul Liang",
        "Louis-Philippe Morency",
        "Peter Bell",
        "Catherine Lai"
      ],
      "year": "2023",
      "venue": "Cross-attention is not enough: Incongruity-aware dynamic hierarchical fusion for multimodal affect recognition"
    },
    {
      "citation_id": "9",
      "title": "Domain-adversarial autoencoder with attention based feature level fusion for speech emotion recognition",
      "authors": [
        "Yuan Gao",
        "Jiaxing Liu",
        "Longbiao Wang",
        "Jianwu Dang"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "10",
      "title": "Modality-specific learning rates for effective multimodal additive latefusion",
      "authors": [
        "Yiqun Yao",
        "Rada Mihalcea"
      ],
      "year": "2022",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2022"
    },
    {
      "citation_id": "11",
      "title": "Multi-Modal Learning for Speech Emotion Recognition: An Analysis and Comparison of ASR Outputs with Ground Truth Transcription",
      "authors": [
        "Saurabh Sahu",
        "Vikramjit Mitra",
        "Nadee Seneviratne",
        "Carol Espy-Wilson"
      ],
      "year": "2019",
      "venue": "Proceedings of Interspeech"
    },
    {
      "citation_id": "12",
      "title": "Learning fine-grained cross modality excitement for speech emotion recognition",
      "authors": [
        "Hang Li",
        "Wenbiao Ding",
        "Zhongqin Wu",
        "Zitao Liu"
      ],
      "year": "2020",
      "venue": "Learning fine-grained cross modality excitement for speech emotion recognition"
    },
    {
      "citation_id": "13",
      "title": "Speech emotion recognition based on attention weight correction using word-level confidence measure",
      "authors": [
        "Jennifer Santoso",
        "Takeshi Yamada",
        "Shoji Makino"
      ],
      "year": "2021",
      "venue": "Speech emotion recognition based on attention weight correction using word-level confidence measure"
    },
    {
      "citation_id": "14",
      "title": "Emotion recognition by fusing time synchronous and time asynchronous representations",
      "authors": [
        "Wen Wu",
        "Chao Zhang",
        "Philip Woodland"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "Leveraging pre-trained language model for speech sentiment analysis",
      "authors": [
        "Suwon Shon",
        "Pablo Brusco",
        "Jing Pan",
        "J Kyu",
        "Shinji Han",
        "Watanabe"
      ],
      "year": "2021",
      "venue": "22nd Annual Conference of the International Speech Communication Association, INTERSPEECH 2021. International Speech Communication Association"
    },
    {
      "citation_id": "16",
      "title": "Fusing ASR outputs in joint training for speech emotion recognition",
      "authors": [
        "Yuanchao Li",
        "Peter Bell",
        "Catherine Lai"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "Robust multi-modal speech emotion recognition with asr error adaptation",
      "authors": [
        "Binghuai Lin",
        "Liyuan Wang"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "18",
      "title": "Mf-aed-aec: Speech emotion recognition by leveraging multimodal fusion, asr error detection, and asr error correction",
      "authors": [
        "Jiajun He",
        "Xiaohan Shi",
        "Xingfeng Li",
        "Tomoki Toda"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "Foundation model assisted automatic speech emotion recognition: Transcribing, annotating, and augmenting",
      "authors": [
        "Tiantian Feng",
        "Shrikanth Narayanan"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "20",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "21",
      "title": "Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages",
      "authors": [
        "Amir Zadeh",
        "Rowan Zellers",
        "Eli Pincus",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "22",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "Reza Lotfian",
        "Carlos Busso"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "23",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Gautam Naik"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "24",
      "title": "Emotion classification using massive examples extracted from the web",
      "authors": [
        "Ryoko Tokuhisa",
        "Kentaro Inui",
        "Yuji Matsumoto"
      ],
      "year": "2008",
      "venue": "Proceedings of the 22nd International Conference on Computational Linguistics (Coling"
    },
    {
      "citation_id": "25",
      "title": "Useful blunders: Can automated speech recognition errors improve downstream dementia classification?",
      "authors": [
        "Changye Li",
        "Weizhe Xu",
        "Trevor Cohen",
        "Serguei Pakhomov"
      ],
      "year": "2024",
      "venue": "Journal of Biomedical Informatics"
    },
    {
      "citation_id": "26",
      "title": "Acoustic emotion recognition: A benchmark comparison of performances",
      "authors": [
        "Björn Schuller",
        "Bogdan Vlasenko",
        "Florian Eyben",
        "Gerhard Rigoll",
        "Andreas Wendemuth"
      ],
      "year": "2009",
      "venue": "2009 IEEE workshop on automatic speech recognition & understanding"
    },
    {
      "citation_id": "27",
      "title": "Dawn of the transformer era in speech emotion recognition: closing the valence gap",
      "authors": [
        "Johannes Wagner",
        "Andreas Triantafyllopoulos",
        "Hagen Wierstorf",
        "Maximilian Schmitt",
        "Felix Burkhardt",
        "Florian Eyben",
        "Björn Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "28",
      "title": "Expressing reactive emotion based on multimodal emotion recognition for natural conversation in human-robot interaction",
      "authors": [
        "Yuanchao Li",
        "Carlos Toshinori Ishi",
        "Koji Inoue",
        "Shizuka Nakamura",
        "Tatsuya Kawahara"
      ],
      "year": "2019",
      "venue": "Advanced Robotics"
    },
    {
      "citation_id": "29",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "Amir Zadeh",
        "Minghai Chen",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "30",
      "title": "Non-local neural networks",
      "authors": [
        "Xiaolong Wang",
        "Ross Girshick",
        "Abhinav Gupta",
        "Kaiming He"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "31",
      "title": "MISA: Modality-invariant and-specific representations for multimodal sentiment analysis",
      "authors": [
        "Devamanyu Hazarika",
        "Roger Zimmermann",
        "Soujanya Poria"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM international conference on multimedia"
    },
    {
      "citation_id": "32",
      "title": "Speech emotion recognition with multi-task learning",
      "authors": [
        "Xingyu Cai",
        "Jiahong Yuan",
        "Renjie Zheng",
        "Liang Huang",
        "Kenneth Church"
      ],
      "year": "2021",
      "venue": "Proceedings of Interspeech 2021"
    },
    {
      "citation_id": "33",
      "title": "Alpaca: A strong, replicable instruction-following model",
      "authors": [
        "Rohan Taori",
        "Ishaan Gulrajani",
        "Tianyi Zhang",
        "Yann Dubois",
        "Xuechen Li",
        "Carlos Guestrin",
        "Percy Liang",
        "Tatsunori B Hashimoto"
      ],
      "venue": "Stanford Center for Research on Foundation Models"
    },
    {
      "citation_id": "34",
      "title": "Whispering llama: A cross-modal generative error correction framework for speech recognition",
      "authors": [
        "Srijith Radhakrishnan",
        "Chao-Han Huck",
        "Sumeer Yang",
        "Rohit Ahmad Khan",
        "Narsis Kumar",
        "David Kiani",
        "Jesper Gomez-Cabrero",
        "Tegner"
      ],
      "year": "2023",
      "venue": "Whispering llama: A cross-modal generative error correction framework for speech recognition",
      "arxiv": "arXiv:2310.06434"
    },
    {
      "citation_id": "35",
      "title": "Crossmodal ASR error correction with discrete speech units",
      "authors": [
        "Yuanchao Li",
        "Pinzhen Chen",
        "Peter Bell",
        "Catherine Lai"
      ],
      "venue": "2024 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "36",
      "title": "Common voice: A massively-multilingual speech corpus",
      "authors": [
        "Rosana Ardila",
        "Megan Branson",
        "Kelly Davis",
        "Michael Henretty",
        "Michael Kohler",
        "Josh Meyer",
        "Reuben Morais",
        "Lindsay Saunders",
        "Francis Tyers",
        "Gregor Weber"
      ],
      "year": "2020",
      "venue": "Proceedings of the Twelfth Language Resources and Evaluation Conference (LREC)"
    }
  ]
}