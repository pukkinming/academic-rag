{
  "paper_id": "2201.05834v1",
  "title": "Tailor Versatile Multi-Modal Learning For Multi-Label Emotion Recognition",
  "published": "2022-01-15T12:02:28Z",
  "authors": [
    "Yi Zhang",
    "Mingyuan Chen",
    "Jundong Shen",
    "Chongjun Wang"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multi-modal Multi-label Emotion Recognition (MMER) aims to identify various human emotions from heterogeneous visual, audio and text modalities. Previous methods mainly focus on projecting multiple modalities into a common latent space and learning an identical representation for all labels, which neglects the diversity of each modality and fails to capture richer semantic information for each label from different perspectives. Besides, associated relationships of modalities and labels have not been fully exploited. In this paper, we propose versaTile multi-modAl learning for multI-labeL emOtion Recognition (TAILOR), aiming to refine multi-modal representations and enhance discriminative capacity of each label. Specifically, we design an adversarial multi-modal refinement module to sufficiently explore the commonality among different modalities and strengthen the diversity of each modality. To further exploit label-modal dependence, we devise a BERT-like cross-modal encoder to gradually fuse private and common modality representations in a granularity descent way, as well as a label-guided decoder to adaptively generate a tailored representation for each label with the guidance of label semantics. In addition, we conduct experiments on the benchmark MMER dataset CMU-MOSEI in both aligned and unaligned settings, which demonstrate the superiority of TAILOR over the state-of-the-arts. Code is available at https://github.com/kniter1/TAILOR.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In real-world applications, videos are often characterized by heterogeneous representations (i.e., visual, audio and text) and annotated with various emotion labels (e.g., happy, surprise). Multi-modal Multi-label Emotion Recognition (MMER)  (Ju et al. 2020; Zhang et al. 2021a ) refers to identifying various emotions by leveraging visual, audio and text modalities presented in videos.\n\nMulti-modal learning (Baltrusaitis, Ahuja, and Morency 2019) processes heterogeneous information collected from multiple sources, which gives rise to two emergent issues: intra-modal representation and inter-modal fusion. Intramodal representation learning mainly exploits consistency and complementarity of multiple modalities to bridge the heterogeneous modality gap. Previous methods project each Visual Encoder  (Transformer Encoder)  Audio Encoder  modality into a shared latent space to eliminate redundancy. However, they neglect the fact that different modalities reveal distinctive characteristic of emotions from different perspectives. Concerning the fusion manner, existing intermodal fusion methods can be divided into: aggregationbased fusion, alignment-based fusion and the mixture of them  (Baltrusaitis, Ahuja, and Morency 2019) . Aggregationbased fusion adopts concatenation  (Ngiam et al. 2011) , tensor  (Zadeh et al. 2017)  or attention  (Zadeh et al. 2018b ) to combine multiple modalities. Alignment-based fusion centers on latent cross-modal adaptation, which adapts streams from one modality to another  (Tsai et al. 2019) . The key challenge of multi-modal learning lies in 1) how to integrate commonality while preserving diversity of each individual modality; 2) how to align different modality distributions interactively for inter-modal fusion.\n\nMulti-label learning (Zhang and Zhou 2014) deals with rich semantic meanings of complicated objects, where label correlations are considered as the key to effective multilabel learning  (Zhu, Kwok, and Zhou 2018) . Many methods exploit label correlations by the similarity between label vectors, and then seamlessly incorporated into an identical representation. However, they are unable to reflect collaboration relationships among labels. On the other hand, many researches have been developed to improve the performance by learning label-specific representations in a label-by-label manner, which are generated independently and may lead to suboptimal problem due to the ignorance of label corre-lations  (Zhang, Fang, and Wang 2021) . The key challenge of multi-label learning is how to effectively encode inherent and discriminative characteristics of each label in both the feature space and the label space.\n\nTo address the above challenges, we propose versaTile multi-modAl learning for multI-labeL emOtion Recognition (TAILOR), which sufficiently copes with modality heterogeneity and label heterogeneity. To bridge the heterogeneity gap, we capture modality interactions, label correlations and label-modal dependence in the following 3 spaces.\n\n1) In the modality feature space, we emphasis less on pretraining. For intra-modal representation, we devise an adversarial network to explicitly extract commonality and diversity, constrained by common semantics and orthogonality. For inter-modal fusion, we propose a novel granularitybased fusion with BERT-like transformer encoder.\n\n2) In the label space, we adopt self-attention  (Vaswani et al. 2017)  to exploit high-order label correlations, which can be further integrated to capture label semantics.\n\n3) To bridge the gap between modality feature space and label space, we adapt Transformer decoder to align fused multi-modal representations with label semantics, which aims to learn tailored representation of each label with the guidance of label semantics.\n\nFigure  1  illustrates the difference between previous methods and our proposed method. The main contributions can be summarized as follows.\n\n• A novel framework of versaTile multi-modAl learning for multI-labeL emOtion Recognition (TAILOR) is proposed, which adversarially depicts commonality and diversity among multiple modalities, as well as enhance discriminative capability of label representations.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Emotion recognition is broadly studied with uni-modal  (Yang et al. 2018; Majumder et al. 2019; Saha et al. 2020; Jiao, Lyu, and King 2020; Huang et al. 2021) , bi-modal  (Mittal et al. 2020b; Liu et al. 2020; Zhao et al. 2020 ) and multi-modal  (Mittal et al. 2020a; Sun et al. 2020; Zhang et al. 2021a; Lv et al. 2021 ). More effective multi-modal fusion translates to better performance. The most straightforward way is to directly concatenating feature maps from each modality  (Ngiam et al. 2011 ). To leverage complementary information across different modalities, tensor fusion  (Zadeh et al. 2017; Liu et al. 2018) , memory fusion  (Zadeh et al. 2018a) , factorization fusion  (Valada, Mohan, and Burgard 2020)  explicitly account for intra-modal and inter-modal dynamics. The above mentioned methods are aggregation-based fusion and the modality gap heavily affects cross-modal fusion. To bridge the modality gap, GAN  (Goodfellow et al. 2014 ) has attracted significant interest in learning joint distributions between bi-modal or multimodal  (Pham et al. 2018; Tsai et al. 2018; Pham et al. 2019; Mai, Hu, and Xing 2020) , alignment-based fusion (Baltrusaitis, Ahuja, and Morency 2019) latently adapts streams from one modality to another via Transformer  (Goodfellow et al. 2014) . Even though, they tend to fuse into a joint embedding space, which neglects the specificity of each modality.  (Wang et al. 2020b ) adapts the fusion of modalityspecific streams and fuses only the relevant complementary information. For example,  (Wu et al. 2019; Hazarika et al. 2020)  integrates the common information across modalities, meanwhile preserving the specific patterns of each modality.\n\nIn multi-label learning, modeling label correlations has been proven to be an effective strategy  (Zhang and Zhou 2014; Zhu, Kwok, and Zhou 2018; Feng, An, and He 2019; Wang et al. 2020a ). It might be suboptimal to learn a subset of features shared by all the labels. Another significant strategy is label-specific learning  (Zhang and Wu 2014; Huang et al. 2016; Zhang, Fang, and Wang 2021) , where each label is determined by some discriminative characteristics, e.g., visual attention  (Chen et al. 2019a,b)  and textual attention  (Xiao et al. 2019; Zhang et al. 2021b) .\n\nRecently, multi-modal multi-label emotion recognition has aroused increasing interest. For example,  (Ju et al. 2020; Zhang et al. 2021a ) models modality-to-label and featureto-label dependence besides label correlations.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "In this section, we first give the formulation of Multi-modal Multi-label Emotion Recognition (MMER). We use lowercase for scalars (e.g., a), uppercase for vectors (e.g., A) and bold for matrices (e.g., a, A). Let X v = R dv×τv , X a = R da×τa , X t = R dt×τt be the visual (v), audio (a), text (t) feature space respectively, and Y = {y 1 , y 2 , • • • , y l } denote the label space with l labels, where d {v,a,t} represents modality dimension and τ {v,a,t} represents sequence length. Given a training dataset\n\nwith n data samples, the goal of MMER is to learn a function F : X v ×X a ×X t → 2 Y , which can assign a set of possible emotion labels for the unseen video. For the i-th video, X {v,a,t} i ∈ X {v,a,t} is the modality features and Y i ⊆ Y is the set of relevant labels. Fig.  2  shows the main framework of TAILOR, which comprises the following modules: Unimodal Extractor, Adversarial Multi-modal Refinement and Label-Modal Alignment.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Uni-Modal Extractor",
      "text": "The pre-extracted features for each modality in CMU-MOSEI  (Zadeh et al. 2018c ) dataset are represented by asynchronous coordinated sequences. To exploit long-term contextual information, we use n v -layer, n a -layer, n t -layer Transformer encoder  (Vaswani et al. 2017)  to enrich the visual features, audio features and text features with sequence level context separately. The transformer encoder consists of two sub-layers: multi-head self-attention layer and positionwise feed-forward layer, where residual connections  (He et al. 2016 ) are adopted, followed by layer normalization. As a result, we obtain new visual, audio and text embeddings, denoted as\n\n, where d is the embedding dimension and τ is the sequence length.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Adversarial Multi-Modal Refinement",
      "text": "It is well known that the greater the difference between inter-modal representations, the better the complementarity of inter-modal fusion  (Yu et al. 2020) . Though the uni-modal extractors capture long-term temporal context, they are unable to deal with feature redundancy due to modality gap.\n\nInspired by adversarial networks  (Goodfellow et al. 2014) , we design an adversarial multi-modal refinement module for the subsequent fusion. It inherently decomposes multiple modalities to two disjoint parts: common and private representations so as to extract consistency and specificity of heterogeneous modalities collaboratively and individually.\n\nTo maintain consistency, we design a generator G(•; θ G ) with parameters θ G , to project different modalities into a common latent subspace with distributional alignment. Apart from commonality, each modality contains specific information, which can complement with other modalities. We adopt fully connected deep neural networks f v (•; θ v ), f a (•; θ a ) and f t (•; θ t ) with parameters {θ v , θ a , θ t } to project uni-modal embedding {V , A, T } respectively. The common and private representations can be formulated as,\n\nwhere C {v,a,t} , P {v,a,t} ∈ R d×τ .\n\nAdversarial Training In order to guarantee the purity of common and specific representations, we design a modality discriminator D(, ; θ D ) which maps the input I ∈ R d×τ into a probability distribution and estimates which modality the representation comes from, where d is the modality dimen-sion and τ is the sequence length.\n\nwhere W ∈ R d×3 is the weight matrices, and b ∈ R τ ×3 is the bias matrices. The ground truth modality label of\n\nCommon representations C {v,a,t} are encoded in a shared latent subspace, which tends to be in the same distribution. Therefore, the generator G(, ; θ G ) are encouraged to confuse discriminator D(, ; θ D ) thus not to distinguish the source modality of C {v,a,t} . We reconstruct a training dataset\n\nfor common modality classification. The common adversarial loss is,\n\nwhere L C is trained with gradient reversal layer  (Ganin and Lempitsky 2015)  that leaves the input unchanged during forward propagation and multiply the gradient by -1 during the backpropagation. Private representations P {v,a,t} are encoded in diverse latent subspaces, which tends to be in different distributions. Therefore, the discriminator D(•; θ D ) are encouraged to distinguish the source of modality. We reconstruct a training dataset\n\nfor private modality classification. The private adversarial loss is,\n\nOrthogonal Constraint To encode different aspects of multi-modal data, we penalize redundancy in C {v,a,t} and P {v,a,t} with orthogonal loss as follows,\n\nwhere || • || 2 F is the squared Frobenius norm. Common Semantics Although the common generator G(, ; θ G ) and private extractors f v (, ; θ v ), f a (, ; θ a ) f t (, ; θ t ) are encouraged to encode different aspects of multi-modal information, they should exhibit the same semantics. We are motivated to design common semantic loss for multi-label classification with common representations C {v,a,t} ,\n\n))\n\n(7) where ŷj,m i is predicted with C m and y j i is the ground-truth. y j i = 1 if the j-th label is relevant, 0 otherwise.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Label-Modal Alignment",
      "text": "After projecting into private and common representations respectively and collectively, we need to fuse them into a joint representation for multi-label classification.\n\nHierarchical Cross-Modal Encoder The refined common and private modality representations contain consistent and complementary information, while few or no information with regard to modality interactions. Simply concatenating them together ignores modality interactions, which might introduce redundant information and lead to suboptimal problem  (Zhang et al. 2018 ). We propose a novel BERTlike (Kenton and Toutanova 2019) Cross-Modal Encoder to exploit modality interactions.  Given two modalities a and b with representations A ∈ R d×τa and B ∈ R d×τ b , where d is modality dimension and τ {a,b} is sequence length. On the one hand, to preserve the temporal information of the two modalities, we augment them with positional embeddings E ∈ R d×(τa+τ b ) . On the other hand, the feature distribution of various modalities are different due to heterogeneity, which poses a great challenge to multi-modal fusion. To bridge the large margin of the statistical properties between two modalities, we capture statistical regularities by adding two modality token embeddings E A ∈ R 1×τa and E B ∈ R 1×τ b to modality a and b respectively. As illustrated in Fig.  3 , the sum of modality representations, position embeddings and modality token embeddings is feed into n c -layer Transformer Encoder, which outputs the joint representation Z ∈ R d×(τa+τ b ) of modality a and b. Cross-Modal Encoder can be written as Z = CME(A, B).",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Visual Encoder",
      "text": "Besides, visual and audio modalities are more finegrained than text modality in terms of granularity  (Alayrac et al. 2020) , which is rarely considered in existing fusion methods. To remedy the deficiency, we devise Hierarchical Cross-Modal Encoder (HCME) to exploit interactions across modalities with different level of granularities. Private representations P {v,a,t} and common representations C {v,a,t} are fused in a hierarchical structure and gradually complement with each other in a granularity descent way.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Fine-Grained",
      "text": "Z va = CME(P v , P a )\n\nCoarse-grained Z vat = CME(Z va , P t )\n\nwhere\n\nis the label embedding for k. To exploit label correlations collaboratively, we adopt self-attention mechanism with h l heads. For the i-th head 1 ,\n\nAnd then label correlation matrix r can be calculated as,\n\nwhere r kk and r k k represent the label-specific relation, r k k and r kk represent the interactive relation of the k-th label with respect to other l -1 labels. r k k denotes the influence of other l -1 labels to the k-th label, while r kk denotes the influence of the k-th label to other l -1 labels. The label semantic embedding S of the i-th head is,\n\nwhere\n\n) is a row-wise, scaled softmax. For the k-th label, the label-specific semantic embedding is s k = σ(r kk )v k + σ(r k k )v k , which involves the collaboration of its own semantic implication and the semantic implication receiving from other labels. In addition, we add a residual connection followed by layer normalization (LN), to the final label-specific semantic embeddings,\n\nwhere\n\nLabel semantics determine inherent dependence between labels and modalities. Therefore, the obtained label-specific semantic embeddings L ∈ R l×d can be further considered as a teacher to guide the learning of tailored representation for each label. Inspired by transformer decoder  (Vaswani et al. 2017) , we design a label-guided decoder to select discriminative information from joint multi-modal representations M ∈ R d×4τ with the guidance of label semantics. The latent dependence from modality space to label space is captured by multi-head attention with h m heads,\n\nwhere\n\nThen the tailored representations\n\nis generated by a feed-forward network (FFN) and two layer normalization (LN) with residual connection.\n\nMulti-label Classification For the k-th label, its tailored representation H k is fed into a linear function followed by an output sigmoid for the final label classification,\n\nwhere W k ∈ R d is weight vector and b k ∈ R is the bias. The final multi-label classification loss can be computed with binary cross-entropy loss,\n\nwhere ŷj i is predicted by Eq. 15 and y j i is the ground-truth.\n\nAbove all, combining the final multi-label classification loss L ml , common adversarial loss L C , private adversarial loss L P , common semantic loss L cml and orthogonal loss L dif f together, the final objective function is computed as,\n\nwhere α, β and γ are the trade-off parameters.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiment",
      "text": "In this section, we give empirically evaluations and analysis of our proposed TAILOR method.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Setup",
      "text": "Dataset We conduct experiments on benchmark multimodal multi-label dataset CMU-MOSEI 2    (Zadeh et al. 2018c) , which contains 22, 856 video segments from 1, 000 distinct speakers. Each video inherently contains 3 modalities: visual, audio and text, while annotated with 6 discrete emotions: {angry, disgust, fear, happy, sad, surprise}. We pre-extract 35-dimensional visual features from video frames by FACET  (Baltrušaitis, Robinson, and Morency 2016) , 74-dimensional audio features from acoustic signals by COVAREP  (Degottex et al. 2014 ) and 300-dimensional text features from video transcripts by Glove  (Pennington, Socher, and Manning 2014) . Table  1  summarizes details of CMU-MOSEI in both word-aligned and unaligned settings. On the other hand, we compare with multi-modal multilabel methods. DFG  (Zadeh et al. 2018c ) studies the nature of cross-modal dynamics in multimodal language. RAVEN  (Wang et al. 2019 ) captures dynamic nature of nonverbal intents by shifting word representations based on the accompanying nonverbal behaviors. MulT  (Tsai et al. 2019 ) fuses multi-modal information by directly attending Table  2 : Predictive performance of TAILOR on multi-modal multi-label CMU-MOSEI dataset with aligned and unaligned multi-modal sequences compared with state-of-the-arts. The best performance for each criterion is bolded.   2015)  with an initial learning rate of 1e-5 for aligned setting, 1e-4 for unaligned setting and employ a liner decay learning rate schedule with a warm-up strategy. All experiments are running with one GTX 1080Ti GPU.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experimental Results And Analysis",
      "text": "Experimental Results Except MulT, we include CTC  (Graves et al. 2006 ) to be suitable for the unaligned setting.\n\nBased on the comparison results in Table  2 , we have the following observations. 1) Our proposed TAILOR significantly surpasses the state-of-the art methods on all evaluation metrics except recall (R), which is relatively less important than accuracy (Acc) and Micro-F1 for performance evaluation.\n\n2) CC performs best among 3 classic multi-label methods, which indicates the effectiveness of exploiting label correlations. 3) Text based multi-label methods SGM, LSAN and image based multi-label methods ML-GCN performs better than CC, which further conforms that label correlations conduce to capture more meaningful features. 4) MulT performs better than almost all multi-label methods that with concatenated modalities or only with text/image modality, which shows the necessity of exploiting modality complementarity. 5) Multi-modal multi-label methods such as HHMPN performs even better than aforementioned methods, which validates the effectiveness of exploiting modality information and label information simultaneously. and several observations are obtained as follows.\n\n• (1) is worst, which validates significance of adversarial multi-modal learning. As integrating L adv , L dif f , L cml to AMR optimization, (2), (  3 ), (12) gradually improves. • (4), (  5 ) are worse than (12), which reveals that joint consideration of the commonality and diversity of multimodal data leads to better performance. • Changing the fusion order of HCME leads to poor performance, while (6), (  7 ) is better than (1)-(  5 ). It validates the rationality and optimality of HCME.  Besides, we display the probability produced by discriminator D(•; θ D ) in the AMR. In Fig.  5 , for each modality m, the probabilities of common representations P (v|C m ), P (a|C m ), P (t|C m ) are centered around 0.33, which is hard to differentiate the source of common modalities. Contrarily, taking visual modality for example, P (v|P m ) is higher than P (a|P m ) and P (t|P m ) by a large margin, leading to increasingly separable representations. Visualization of Learned Adversarial Representations t-SNE (Van der Maaten and Hinton 2008) is adopted to investigate the efficacy of adversarial multi-modal refinement. We visualize the common and private representations C {v,a,t} and P {v,a,t} learned without or with adversarial training and orthogonal constraint in aligned CMU-MOSEI. As shown in Fig.  6 , the distributions of C {v,a,t} and P {v,a,t} are sometimes overlapped in the left subfigure. Contrarily, in the right subfigure, 1) the distributions of C {v,a,t} are mixed together and increasingly blurred, where adversarial training proves effective to align distributions of different modalities and minimize the modality gap; 2) the common latent subspace is separable from each private subspace, where redundant latent representations are penalized with orthogonal constraint. In all, commonality and specificity of different modalities are well characterized. ↵ = 0, = 0 C v C a C t ↵ 6 = 0, 6 = 0 P v P a P t label correlations differ from head to head, which jointly attends to rich semantic information from different perspectives. From the horizontal view, angry is highly correlated with fear, disgust and sad in different heads. In most cases, surprise is highly correlated with happy. All these correlations accord with our intuition.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose versaTile multi-modAl learning for multI-labeL emOtion Recognition (TAILOR), consisting of uni-modal extractor, adversarial multi-modal refinement and label-modal alignment. These modules cooperate closely to refine private and common representations adversarially, fuse multiple modalities in terms of granularity gradually, and leverage label semantics to guide the construction of label-specific representation. Experimental results and analysis on both aligned and unaligned settings verify effectiveness and generalization of our proposed method.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Previous methods versus our method.",
      "page": 1
    },
    {
      "caption": "Figure 1: illustrates the difference between previous meth-",
      "page": 2
    },
    {
      "caption": "Figure 2: shows the main framework",
      "page": 2
    },
    {
      "caption": "Figure 2: Overall structure of TAILOR.",
      "page": 3
    },
    {
      "caption": "Figure 3: Structure of Cross-Modal Encoder (CME) between",
      "page": 4
    },
    {
      "caption": "Figure 3: , the sum of modal-",
      "page": 4
    },
    {
      "caption": "Figure 4: Common adversarial loss LC, private adversarial",
      "page": 7
    },
    {
      "caption": "Figure 4: , private adversarial loss LP and overall loss LAll",
      "page": 7
    },
    {
      "caption": "Figure 5: Distribution of private and common representa-",
      "page": 7
    },
    {
      "caption": "Figure 5: , for each",
      "page": 7
    },
    {
      "caption": "Figure 6: , the distributions of C{v,a,t}",
      "page": 7
    },
    {
      "caption": "Figure 6: t-SNE visualization of common and private rep-",
      "page": 7
    },
    {
      "caption": "Figure 7: , the learned",
      "page": 7
    },
    {
      "caption": "Figure 7: Label correlations visualization, indicating the in-",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table 2: Predictive performance of TAILOR on multi-modal multi-label CMU-MOSEI dataset with aligned and unaligned",
      "data": [
        {
          "Approaches": "",
          "Aligned": "Acc\nP\nR\nMicro-F1",
          "Unaligned": "Acc\nP\nR\nMicro-F1"
        },
        {
          "Approaches": "BR (Boutell et al. 2004)\nLP (Tsoumakas and Katakis 2007)\nCC (Read et al. 2011)",
          "Aligned": "0.222\n0.309\n0.515\n0.386\n0.159\n0.231\n0.377\n0.286\n0.225\n0.306\n0.523\n0.386",
          "Unaligned": "0.233\n0.321\n0.545\n0.404\n0.185\n0.252\n0.427\n0.317\n0.235\n0.320\n0.550\n0.404"
        },
        {
          "Approaches": "SGM (Yang et al. 2018)\nLSAN (Xiao et al. 2019)\nML-GCN (Chen et al. 2019b)",
          "Aligned": "0.455\n0.595\n0.467\n0.523\n0.393\n0.550\n0.459\n0.501\n0.411\n0.546\n0.476\n0.509",
          "Unaligned": "0.449\n0.584\n0.476\n0.524\n0.403\n0.582\n0.460\n0.514\n0.437\n0.573\n0.482\n0.524"
        },
        {
          "Approaches": "DFG (Zadeh et al. 2018c)\nRAVEN (Wang et al. 2019)\nMulT (Tsai et al. 2019)\nSIMM (Wu et al. 2019)\nMISA (Hazarika et al. 2020)\nHHMPN (Zhang et al. 2021a)",
          "Aligned": "0.396\n0.595\n0.457\n0.517\n0.416\n0.588\n0.461\n0.517\n0.445\n0.619\n0.465\n0.531\n0.432\n0.561\n0.495\n0.525\n0.582\n0.430\n0.453\n0.509\n0.459\n0.602\n0.496\n0.556",
          "Unaligned": "0.386\n0.534\n0.456\n0.494\n0.403\n0.633\n0.429\n0.511\n0.423\n0.636\n0.445\n0.523\n0.418\n0.482\n0.486\n0.484\n0.571\n0.398\n0.371\n0.450\n0.434\n0.591\n0.476\n0.528"
        },
        {
          "Approaches": "TAILOR",
          "Aligned": "0.488\n0.641\n0.569\n0.512",
          "Unaligned": "0.460\n0.639\n0.529\n0.452"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 2: Predictive performance of TAILOR on multi-modal multi-label CMU-MOSEI dataset with aligned and unaligned",
      "data": [
        {
          "Approaches": "(1) w/o AMR\n(2) w/\nadv\nL\n(3) w/\nadv,\ndif f",
          "Acc\nP\nR\nMicro-F1": "0.446\n0.634\n0.474\n0.543\n0.432\n0.722\n0.419\n0.530\n0.462\n0.581\n0.520\n0.549"
        },
        {
          "Approaches": "L\nL\n(4) w/ C{v,a,t}\n(5) w/ P {v,a,t}\n(6) ψ = [v, t, a, c]\n(7) ψ = [a, t, v, c]\n(8) w/o MTE",
          "Acc\nP\nR\nMicro-F1": "0.458\n0.638\n0.481\n0.549\n0.449\n0.605\n0.496\n0.545\n0.465\n0.629\n0.496\n0.554\n0.470\n0.584\n0.524\n0.552\n0.478\n0.601\n0.528\n0.562"
        },
        {
          "Approaches": "(9) w/ identical\n(10) w/ LE\n(11) w/ LE, LC",
          "Acc\nP\nR\nMicro-F1": "0.462\n0.575\n0.528\n0.551\n0.556\n0.465\n0.558\n0.557\n0.473\n0.594\n0.538\n0.564"
        },
        {
          "Approaches": "(12) TAILOR",
          "Acc\nP\nR\nMicro-F1": "0.488\n0.641\n0.569\n0.512"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Self-Supervised MultiModal Versatile Networks",
      "authors": [
        "J.-B Alayrac",
        "A Recasens",
        "R Schneider",
        "R Arandjelovic",
        "J Ramapuram",
        "J De Fauw",
        "L Smaira",
        "S Dieleman",
        "A Zisserman"
      ],
      "year": "2020",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "2",
      "title": "Multimodal Machine Learning: A Survey and Taxonomy",
      "authors": [
        "T Baltrusaitis",
        "C Ahuja",
        "L.-P Morency"
      ],
      "year": "2019",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "3",
      "title": "Openface: an open source facial behavior analysis toolkit",
      "authors": [
        "T Baltrušaitis",
        "P Robinson",
        "L.-P Morency"
      ],
      "year": "2016",
      "venue": "2016 IEEE Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "4",
      "title": "Learning multi-label scene classification. Pattern recognition",
      "authors": [
        "M Boutell",
        "J Luo",
        "X Shen",
        "C Brown"
      ],
      "year": "2004",
      "venue": "Learning multi-label scene classification. Pattern recognition"
    },
    {
      "citation_id": "5",
      "title": "Learning semantic-specific graph representation for multi-label image recognition",
      "authors": [
        "T Chen",
        "M Xu",
        "X Hui",
        "H Wu",
        "L Lin"
      ],
      "year": "2019",
      "venue": "ICCV"
    },
    {
      "citation_id": "6",
      "title": "Multi-label image recognition with graph convolutional networks",
      "authors": [
        "Z.-M Chen",
        "X.-S Wei",
        "P Wang",
        "Y Guo"
      ],
      "year": "2019",
      "venue": "CVPR"
    },
    {
      "citation_id": "7",
      "title": "COVAREP-A collaborative voice analysis repository for speech technologies",
      "authors": [
        "G Degottex",
        "J Kane",
        "T Drugman",
        "T Raitio",
        "S Scherer"
      ],
      "year": "2014",
      "venue": "ICASSP, 960-964. IEEE. Feng, L.; An, B.; and He"
    },
    {
      "citation_id": "8",
      "title": "Unsupervised domain adaptation by backpropagation",
      "authors": [
        "Y Ganin",
        "V Lempitsky"
      ],
      "year": "2015",
      "venue": "ICML"
    },
    {
      "citation_id": "9",
      "title": "",
      "authors": [
        "I Goodfellow",
        "J Pouget-Abadie",
        "M Mirza",
        "B Xu",
        "D Warde-Farley",
        "S Ozair",
        "A Courville",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": ""
    },
    {
      "citation_id": "10",
      "title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
      "authors": [
        "A Graves",
        "S Fernández",
        "F Gomez",
        "J Schmidhuber"
      ],
      "year": "2006",
      "venue": "ICML"
    },
    {
      "citation_id": "11",
      "title": "Misa: Modality-invariant and-specific representations for multimodal sentiment analysis",
      "authors": [
        "D Hazarika",
        "R Zimmermann",
        "; Poria"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "12",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "CVPR"
    },
    {
      "citation_id": "13",
      "title": "Learning label-specific features and class-dependent labels for multilabel classification",
      "authors": [
        "J Huang",
        "G Li",
        "Q Huang",
        "X Wu"
      ],
      "year": "2016",
      "venue": "IEEE transactions on knowledge and data engineering"
    },
    {
      "citation_id": "14",
      "title": "Audio-Oriented Multimodal Machine Comprehension via Dynamic Inter-and Intra-modality Attention",
      "authors": [
        "Z Huang",
        "F Liu",
        "X Wu",
        "S Ge",
        "H Wang",
        "W Fan",
        "Y Zou"
      ],
      "year": "2021",
      "venue": "AAAI"
    },
    {
      "citation_id": "15",
      "title": "Real-time emotion recognition via attention gated hierarchical memory network",
      "authors": [
        "W Jiao",
        "M Lyu",
        "I King"
      ],
      "year": "2020",
      "venue": "AAAI"
    },
    {
      "citation_id": "16",
      "title": "Transformerbased label set generation for multi-modal multi-label emotion detection",
      "authors": [
        "X Ju",
        "D Zhang",
        "J Li",
        "G Zhou"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "17",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "J Kenton",
        "M.-W Toutanova"
      ],
      "year": "2019",
      "venue": "NAACL-HLT"
    },
    {
      "citation_id": "18",
      "title": "Adam: A Method for Stochastic Optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2015",
      "venue": "Adam: A Method for Stochastic Optimization"
    },
    {
      "citation_id": "19",
      "title": "Federated learning for vision-and-language grounding problems",
      "authors": [
        "F Liu",
        "X Wu",
        "S Ge",
        "W Fan",
        "Y Zou"
      ],
      "year": "2020",
      "venue": "AAAI"
    },
    {
      "citation_id": "20",
      "title": "Efficient Low-rank Multimodal Fusion With Modality-Specific Factors",
      "authors": [
        "Z Liu",
        "Y Shen",
        "V Lakshminarasimhan",
        "P Liang",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "ACL"
    },
    {
      "citation_id": "21",
      "title": "Progressive Modality Reinforcement for Human Multimodal Emotion Recognition From Unaligned Multimodal Sequences",
      "authors": [
        "F Lv",
        "X Chen",
        "Y Huang",
        "L Duan",
        "G Lin"
      ],
      "year": "2021",
      "venue": "CVPR"
    },
    {
      "citation_id": "22",
      "title": "Modality to modality translation: An adversarial representation learning and graph fusion network for multimodal fusion",
      "authors": [
        "S Mai",
        "H Hu",
        "S Xing"
      ],
      "year": "2020",
      "venue": "AAAI"
    },
    {
      "citation_id": "23",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "AAAI"
    },
    {
      "citation_id": "24",
      "title": "Multiplicative multimodal emotion recognition using facial, textual, and speech cues",
      "authors": [
        "T Mittal",
        "U Bhattacharya",
        "R Chandra",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "AAAI"
    },
    {
      "citation_id": "25",
      "title": "Emoticon: Context-aware multimodal emotion recognition using frege's principle",
      "authors": [
        "T Mittal",
        "P Guhan",
        "U Bhattacharya",
        "R Chandra",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "CVPR"
    },
    {
      "citation_id": "26",
      "title": "Multimodal deep learning",
      "authors": [
        "J Ngiam",
        "A Khosla",
        "M Kim",
        "J Nam",
        "H Lee",
        "A Ng"
      ],
      "year": "2011",
      "venue": "ICML"
    },
    {
      "citation_id": "27",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "J Pennington",
        "R Socher",
        "C Manning"
      ],
      "year": "2014",
      "venue": "EMNLP"
    },
    {
      "citation_id": "28",
      "title": "Found in translation: Learning robust joint representations by cyclic translations between modalities",
      "authors": [
        "H Pham",
        "P Liang",
        "T Manzini",
        "L.-P Morency",
        "B Póczos"
      ],
      "year": "2019",
      "venue": "AAAI"
    },
    {
      "citation_id": "29",
      "title": "Seq2Seq2Sentiment: Multimodal Sequence to Sequence Models for Sentiment Analysis",
      "authors": [
        "H Pham",
        "T Manzini",
        "P Liang",
        "B Poczós"
      ],
      "year": "2018",
      "venue": "ACL"
    },
    {
      "citation_id": "30",
      "title": "Classifier chains for multi-label classification",
      "authors": [
        "J Read",
        "B Pfahringer",
        "G Holmes",
        "E Frank"
      ],
      "year": "2011",
      "venue": "Machine learning"
    },
    {
      "citation_id": "31",
      "title": "Towards emotion-aided multi-modal dialogue act classification",
      "authors": [
        "T Saha",
        "A Patra",
        "S Saha",
        "P Bhattacharyya"
      ],
      "year": "2020",
      "venue": "ACL"
    },
    {
      "citation_id": "32",
      "title": "Learning relationships between text, audio, and video via deep canonical correlation for multimodal language analysis",
      "authors": [
        "Z Sun",
        "P Sarma",
        "W Sethares",
        "Y Liang"
      ],
      "year": "2020",
      "venue": "AAAI"
    },
    {
      "citation_id": "33",
      "title": "Multimodal Transformer for Unaligned Multimodal Language Sequences",
      "authors": [
        "Y.-H Tsai",
        "S Bai",
        "P Liang",
        "J Kolter",
        "L.-P Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "ACL"
    },
    {
      "citation_id": "34",
      "title": "Learning Factorized Multimodal Representations",
      "authors": [
        "Y.-H Tsai",
        "P Liang",
        "A Zadeh",
        "L.-P Morency",
        "R Salakhutdinov"
      ],
      "year": "2018",
      "venue": "ICLR"
    },
    {
      "citation_id": "35",
      "title": "Multi-label classification: An overview",
      "authors": [
        "G Tsoumakas",
        "I Katakis"
      ],
      "year": "2007",
      "venue": "International Journal of Data Warehousing and Mining (IJDWM)"
    },
    {
      "citation_id": "36",
      "title": "Selfsupervised model adaptation for multimodal semantic segmentation",
      "authors": [
        "A Valada",
        "R Mohan",
        "W Burgard"
      ],
      "year": "2020",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "37",
      "title": "Visualizing data using t-SNE",
      "authors": [
        "L Van Der Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "38",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Attention is all you need"
    },
    {
      "citation_id": "39",
      "title": "Collaboration Based Multi-Label Propagation for Fraud Detection",
      "authors": [
        "H Wang",
        "Z Li",
        "J Huang",
        "P Hui",
        "W Liu",
        "T Hu",
        "G Chen"
      ],
      "year": "2020",
      "venue": "IJCAI"
    },
    {
      "citation_id": "40",
      "title": "Deep multimodal fusion by channel exchanging",
      "authors": [
        "Y Wang",
        "W Huang",
        "F Sun",
        "T Xu",
        "Y Rong",
        "J Huang"
      ],
      "year": "2020",
      "venue": "Deep multimodal fusion by channel exchanging"
    },
    {
      "citation_id": "41",
      "title": "Words can shift: Dynamically adjusting word representations using nonverbal behaviors",
      "authors": [
        "Y Wang",
        "Y Shen",
        "Z Liu",
        "P Liang",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2019",
      "venue": "AAAI"
    },
    {
      "citation_id": "42",
      "title": "Multi-View Multi-Label Learning with View-Specific Information Extraction",
      "authors": [
        "X Wu",
        "Q.-G Chen",
        "Y Hu",
        "D Wang",
        "X Chang",
        "X Wang",
        "M.-L Zhang"
      ],
      "year": "2019",
      "venue": "IJCAI"
    },
    {
      "citation_id": "43",
      "title": "Labelspecific document representation for multi-label text classification",
      "authors": [
        "L Xiao",
        "X Huang",
        "B Chen",
        "L Jing"
      ],
      "year": "2019",
      "venue": "EMNLP-IJCNLP"
    },
    {
      "citation_id": "44",
      "title": "SGM: Sequence Generation Model for Multi-label Classification",
      "authors": [
        "P Yang",
        "X Sun",
        "W Li",
        "S Ma",
        "W Wu",
        "H Wang"
      ],
      "year": "2018",
      "venue": "COLING"
    },
    {
      "citation_id": "45",
      "title": "Ch-sims: A chinese multimodal sentiment analysis dataset with fine-grained annotation of modality",
      "authors": [
        "W Yu",
        "H Xu",
        "F Meng",
        "Y Zhu",
        "Y Ma",
        "J Wu",
        "J Zou",
        "K Yang"
      ],
      "year": "2020",
      "venue": "ACL"
    },
    {
      "citation_id": "46",
      "title": "Tensor Fusion Network for Multimodal Sentiment Analysis",
      "authors": [
        "A Zadeh",
        "M Chen",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "EMNLP"
    },
    {
      "citation_id": "47",
      "title": "Multi-attention recurrent network for human communication comprehension",
      "authors": [
        "A Zadeh",
        "P Liang",
        "N Mazumder",
        "S Poria",
        "E Cambria",
        "L.-P ; A Morency",
        "P Liang",
        "S Poria",
        "P Vij",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "AAAI"
    },
    {
      "citation_id": "48",
      "title": "Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "ACL"
    },
    {
      "citation_id": "49",
      "title": "Latent semantic aware multi-view multi-label classification",
      "authors": [
        "C Zhang",
        "Z Yu",
        "Q Hu",
        "P Zhu",
        "X Liu",
        "X Wang"
      ],
      "year": "2018",
      "venue": "AAAI"
    },
    {
      "citation_id": "50",
      "title": "2021a. Multi-modal Multi-label Emotion Recognition with Heterogeneous Hierarchical Message Passing",
      "authors": [
        "D Zhang",
        "X Ju",
        "W Zhang",
        "J Li",
        "S Li",
        "Q Zhu",
        "G Zhou"
      ],
      "venue": "AAAI"
    },
    {
      "citation_id": "51",
      "title": "BiLabel-Specific Features for Multi-Label Classification",
      "authors": [
        "M.-L Zhang",
        "J.-P Fang",
        "Y.-B Wang"
      ],
      "year": "2021",
      "venue": "ACM Transactions on Knowledge Discovery from Data (TKDD)"
    },
    {
      "citation_id": "52",
      "title": "Lift: Multi-label learning with label-specific features",
      "authors": [
        "M.-L Zhang",
        "L Wu"
      ],
      "year": "2014",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "53",
      "title": "A Review on Multi-Label Learning Algorithms",
      "authors": [
        "M.-L Zhang",
        "Z.-H Zhou"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Knowledge & Data Engineering"
    },
    {
      "citation_id": "54",
      "title": "L. 2021b. Correlation-Guided Representation for Multi-Label Text Classification",
      "authors": [
        "Q.-W Zhang",
        "X Zhang",
        "Z Yan",
        "R Liu",
        "Y Cao",
        "M Zhang"
      ],
      "venue": "IJCAI"
    },
    {
      "citation_id": "55",
      "title": "An End-to-End visual-audio attention network for emotion recognition in user-generated videos",
      "authors": [
        "S Zhao",
        "Y Ma",
        "Y Gu",
        "J Yang",
        "T Xing",
        "P Xu",
        "R Hu",
        "H Chai",
        "K Keutzer"
      ],
      "year": "2020",
      "venue": "AAAI"
    },
    {
      "citation_id": "56",
      "title": "Multi-Label Learning with Global and Local Label Correlation",
      "authors": [
        "Y Zhu",
        "J Kwok",
        "Z.-H Zhou"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Knowledge & Data Engineering"
    }
  ]
}