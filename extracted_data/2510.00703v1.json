{
  "paper_id": "2510.00703v1",
  "title": "Multiphysio-Hrc: Multimodal Physiological Signals Dataset For Industrial Human-Robot Collaboration",
  "published": "2025-10-01T09:18:59Z",
  "authors": [
    "Andrea Bussolan",
    "Stefano Baraldo",
    "Oliver Avram",
    "Pablo Urcola",
    "Luis Montesano",
    "Luca Maria Gambardella",
    "Anna Valente"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Human-robot collaboration (HRC) is a key focus of Industry 5.0, aiming to enhance worker productivity while ensuring well-being. The ability to perceive human psychophysical states, such as stress and cognitive load, is crucial for adaptive and human-aware robotics. This paper introduces MultiPhysio-HRC, a multimodal dataset containing physiological, audio, and facial data collected during real-world HRC scenarios. The dataset includes electroencephalography (EEG), electrocardiography (ECG), electrodermal activity (EDA), respiration (RESP), electromyography (EMG), voice recordings, and facial action units. The dataset integrates controlled cognitive tasks, immersive virtual reality experiences, and industrial disassembly activities performed manually and with robotic assistance, to capture a holistic view of the participants' mental states. Rich ground truth annotations were obtained using validated psychological self-assessment questionnaires. Baseline models were evaluated for stress and cognitive load classification, demonstrating the dataset's potential for affective computing and human-aware robotics research. MultiPhysio-HRC is publicly available to support research in humancentered automation, workplace well-being, and intelligent robotic systems.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "In the field of Human-Robot Collaboration (HRC), physiological signals are raising high interest thanks to their potential to capture human states such as stress, cognitive load, and fatigue  [1] . In the human-centric view promoted by Industry 5.0, industrial workplaces should aim at striking a balance between worker productivity and well-being  [2] . This includes conceiving robotic systems that can not only perform physical tasks in support of human workers but also change their behavior depending on the psycho-physical state of operators, coupled with context information. This approach of deliberative robotics  [3]  cannot unleash its full potential unless the human psycho-physical state can be perceived by the robot. This idea is the core goal of the Fluently project, which aims to enhance human-robot collaboration by enabling robots to adapt their behavior based on the psycho-physical state of human operators.\n\nTo develop robotic systems capable of adapting to human states, it is essential to build machine learning models that can reliably infer the mental state from physiological and behavioral signals. However, training such models requires datasets that not only include a diverse range of conditions but also reflect real-world industrial settings. Many existing datasets focus on a limited subset of modalities and are rarely collected outside of controlled laboratory conditions, limiting their applicability to HRC scenarios where multiple factors influence human states simultaneously.\n\nIn this paper, we present MultiPhysio-HRC, a dataset containing facial features, audio, and physiological signals -electrocardiogram (ECG), electrodermal activity (EDA), respiration (RESP), electromyography (EMG), and Electroencephalography (EEG). To the best of our knowledge, MultiPhysio-HRC is the first dataset to include this wide combination of data obtained during real-world human-robot collaboration, various psychological tests, and VR-based activities, designed to elicit multiple psychological states. Furthermore, the ground truth labels collected for this dataset enable the analysis of various aspects of the human mental state, including stress levels, cognitive load, and emotional dimensions. The dataset is publicly available at https://automation-robotics-machines. github.io/MultiPhysio-HRC.github.io/.\n\nWe summarize our main contributions as follows:\n\n• Real-World HRC Context -To the best of our knowledge, MultiPhysio-HRC is the first publicly available dataset to include real-world industrial-like HRC scenarios comprehensively. The remainder of this paper is organized as follows: Section II presents the related dataset with similar modalities combination; Section III explains the experimental protocol for data collection, describing tasks and data; in Section IV the processing pipelines for filtering and feature extraction are detailed; while Section V presents and discusses the results achieved using traditional models. In the end, Section VI concludes the work by presenting final remarks and future directions.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Works",
      "text": "The field of Affective Computing has a long history of public datasets for emotion and mental state recognition through diverse experimental setups and various physiological and behavioral data combinations.\n\nOne of the first publicly available datasets was published in  [4] . This dataset features ECG, EDA, RESP, and EMG data on driver stress during real-world driving tasks. The WESAD dataset  [5]  is a multimodal dataset aimed at stress and affect detection using wearable sensors. It includes physiological and motion data from 15 participants recorded via both wristworn (Empatica E4) and chest-worn (RespiBAN) devices. Sensor modalities include ECG, EDA, EMG, respiration, temperature, and acceleration. Participants were exposed to neutral, stress (via the Trier Social Stress Test), and amusement conditions. Ground truth was collected using PANAS, SAM, STAI, and SSSQ questionnaires. The dataset enables benchmarking of affective state classification with a focus on wearable technology. The DREAMER dataset  [6]  focuses on emotion recognition in response to audiovisual stimuli. It consists of EEG and ECG signals from 23 subjects exposed to 18 short emotional video clips. After each clip, participants self-assessed their emotional state in terms of valence, arousal, and dominance using the SAM (Self-Assessment Manikins) scale. The recordings were collected using low-cost, wireless devices, making the dataset particularly suitable for developing lightweight emotion recognition systems. In  [7] , AVCAffe, a large-scale audio-visual dataset that studies cognitive load and affect in remote work scenarios, is presented. This dataset includes data from 106 participants performing seven tasks via video conferencing. Tasks included open discussions and collaborative decision-making exercises, designed to elicit varying levels of cognitive load. AVCAffe includes annotations for arousal, valence, and cognitive load attributes. StressID  [8]  is a comprehensive multimodal dataset specifically designed for stress identification, containing synchronized recordings of facial expressions, audio, and physiological signals (ECG, EDA, respiration) from 65 participants. The dataset features annotated data collected during 11 tasks, including guided breathing, emotional video clips, cognitive tasks, and public speaking scenarios.\n\nHowever, the number of public datasets focusing on the physiological response of individuals during real-world HRC tasks is extremely limited. The SenseCobot dataset stands out as a structured effort to investigate operator stress during collaborative robot programming tasks  [9] ,  [10] . In this study, users were trained to program a UR10e cobot in a simulated industrial setup. The authors collected EEG, ECG, GSR, and facial expressions as input data and used NASA-TLX as ground truth labels. The SenseCobot dataset lacks exposure to complex, task-integrated HRC contexts such as physical collaboration or time-constrained industrial procedures. In contrast, the MultiPhysio-HRC dataset addresses this gap by incorporating a broader range of scenarios, including manual and robot-assisted battery disassembly, cognitive load induction through psychological tests (e.g., Stroop, N-back), and immersive virtual reality tasks. Moreover, MultiPhysio-HRC features a richer set of modalities-including EEG, ECG, EDA, EMG, respiration (RESP), facial action units, and audio features, together with detailed ground truth from validated self-assessment questionnaires (STAI-Y1, NASA-TLX, SAM, and NARS), enabling a more holistic assessment of stress, cognitive load, and emotional state in realistic industrial HRC settings.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Multiphysio-Hrc",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Experimental Protocol",
      "text": "The data collection campaign was designed to build a multimodal and multi-scenario dataset for mental state assessment, integrating psychological, physiological, and behavioral data. The protocol designed for this dataset acquisition is inspired by the work presented in  [11] . The protocol spans two days of activities, focusing on varying stress levels and operational conditions, including human-robot collaboration and manual tasks. A schematic representation of the overall protocol is represented in Fig.  1 .\n\n1) Day 1 -Baseline and Stress Induction: Participants began with a resting period to establish baseline physiological measures. Following this, they were asked to perform activities including cognitive load tests, breathing exercises, and VR games. In detail:\n\n1) Rest. The participant sits comfortably for two minutes and is invited to relax without specific instructions. 2) Cognitive tasks. The participant sits in front of a computer screen, using a keyboard and mouse to interact with different games aimed at increasing their cognitive load and eliciting psychological stress. The selected tasks are: a) Stroop Color Word Test (SCWT)  [12]  (three minutes). Color names (e.g., \"RED\") appear in different colors. The participants must push the keyboard button corresponding to the color of the displayed letters (e.g., \"B\" if the word \"RED\" is written in Blue characters). The task was performed with two difficulty levels: one second and half a second to answer. b) N-Back task  [13]  (two minutes). A single letter is shown on the screen every two seconds. The participant must press a key whenever the letter is equal to the N-th previous letter. c) Mental Arithmetic Task (two minutes). The participant must perform a mental calculation in three seconds and press an arrow key, selecting the correct answer among four possibilities. d) Hanoi Tower  [14] . The participant must rebuild the tower in another bin, without placing a larger block over a smaller one. There was no time constraint on this task. e) Breathing exercise (two minutes). A voice-guided controlled breathing exercise. The order of these tasks was randomly chosen for each participant. A representation of the displayed screen is shown in Fig.  2 . During the execution of these tasks (except the Hanoi tower and the breathing exercise), a ticking clock sound was reproduced to arouse a sense of hurry, and a buzzer sound was played in case of mistakes, to increase the psychological stress.\n\n3) VR games. Finally, participants performed immersive tasks in virtual reality environments such as Richie's Plank Experience 1 to elicit a high-intensity psychophysical state. In this game, participants had to walk on a bench suspended on top of a building. After each one of these tasks, the ground truth questionnaires were administered (see sec. III-D).\n\n2) Day 2 -Manual and Robot-Assisted Tasks: The second day was dedicated to a battery disassembly task (described in sec. III-B), designed to compare the experience of fully manual work with HRC. In detail, the second day was structured in the following phases:\n\n1) Rest. The participant sits comfortably for five minutes and is invited to relax without specific instructions.\n\n1 https://store.steampowered.com/app/517160/Richies Plank Experience 2) Manual disassembly. The participant uses bare hands or simple tools to partially disassemble an e-bike battery pack. 3) Collaborative disassembly. The participant is given instructions about how to interact with the robot by voice commands. Then, they perform the same disassembly by asking the cobot to perform support or parallel operations. The voice commands are not only used to give instructions to the robot naturally, but are also opportunities to collect voice data and observe human-robot dynamics under operational conditions. Each task (manual and robot-assisted) was repeated up to five times to elicit fatigue. After each one of these tasks, the ground truth data was collected.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Task And Robotic Cell Description",
      "text": "The industrial task described in III-A.2 involves e-bike battery disassembly, a task selected due to its fundamental importance for fostering sustainable industrial practices. Participants performed both manual and collaborative disassembly of various battery models, with procedures designed to adhere to real-world conditions safely. For safety reasons, the original battery cells were replaced with aluminum cylinders of the same shape and dimensions, eliminating soldering materials and hazardous components.\n\nDuring manual disassembly, the operator opened the battery cover, removed the Battery Management System (BMS), detached the cables, unscrewed the battery components, removed the soldering, and extracted the batteries. In the collaborative disassembly phase, given the difficulty associated with opening the battery casing, this step was conducted collaboratively: the robot pressed against the battery cover to stabilize it, while the human operator loosened the fixturing. Subsequently, while the operator disassembled the BMS, the robot simultaneously unscrewed other battery components. Once the operator finished disassembling the BMS, the human and robot cooperatively unscrewed the remaining components. In Fig.  4 , the complete set of steps of the collaborative disassembly is represented.\n\nA Fanuc CRX-20 2 collaborative robot was used for this task. To ensure operator safety, the Fanuc CRX-20 features built-in safety mechanisms, including force and contact sensors, enabling the robot to detect and respond to unexpected physical interactions. The robotic cell used for the data acquisition is shown in Fig.  3 . The robot was equipped with voice control capabilities, allowing the operator to issue verbal instructions for specific commands. The pipeline 2 https://www.fanuc.eu/eu-en/product/robot/crx-20ial consists of an Automatic Speech Recognition (ASR) module and a Natural Language Understanding (NLU) module, which translates the spoken word into robot instructions. This pipeline is presented in  [15] . After receiving the instructions, IPyHOP  [16] , a Hierarchical Task Network (HTN) planner, decomposed the high-level command into a sequence of atomic robotic actions. When required, the robot automatically switched tools to execute these actions effectively. The motion trajectories for the robot were computed using the Pilz industrial motion planner from MoveIt2  [17] , ensuring precise and safe manipulation.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "C. Participants",
      "text": "In total, 55 subjects participated on the first day of the data collection. The sample mean age is 27.98 ± 10.22. 48 subjects were male and 7 were female. Out of the 55, 42 also participated in the second day. Most subjects were invited from the author's research facility, while the others accepted an external invitation. Participant background varies from undergraduate engineering students to researchers, including professionals in other fields.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Ground Truth",
      "text": "Throughout the experiment, ground truth data were collected by administering multiple self-assessment questionnaires. After each task described in III-A, the subjects were asked to answer three questionnaires:\n\n• The Stress Trait Anxiety Inventory-Y1 (STAI-Y1)\n\n[18] consists of 20 questions that measure the subjective feeling of apprehension and worry, and it is often used as a stress measurement. • The NASA Task Load Index (NASA-TLX)  [19]  measures self-reported workload and comprises six metrics (mental demand, physical demand, temporal demand, performance, effort, and frustration level). • The Self-Assessment Manikin (SAM)  [20]  assesses participant valence, arousal, and dominance levels. The scale used in this dataset is from one to five.\n\nMoreover, at the beginning of the first part of the experiment, participants were asked to complete the Negative Attitude Towards Robots (NARS)  [21]  questionnaire to identify their attitude toward robots.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "E. Acquired Data",
      "text": "Electroencephalogram signals were acquired using the Bitbrain Diadem 3  , which is a wearable dry-EEG with 12 sensors over the pre-frontal, frontal, parietal, and occipital brain areas. In particular, the acquired channels are: AF7, Fp1, Fp2, AF8, F3, F4, P3, P4, PO7, O1, O2, PO8, plus ground and reference electrode on the left earlobe.\n\nFor the collection of electrocardiogram (ECG), electrodermal activity (EDA), respiration (RESP), and electromyography (EMG), we used the Versatile Bio 4  sensor from Bitbrain. The ECG sensor was placed in a V2 configuration to reduce signal noise caused by arm movements. To allow free movement during the experiment, the EDA sensor was placed on the index and middle fingers of the non-dominant hand. The EMG sensor was placed on the right trapezius, while the respiratory band was placed over the subject's chest. In Fig.  5 , a sample of the collected physiological signals is represented. These devices have been used in other HRC setups such as  [22] .\n\nVideo recordings of the participants were obtained using a standard computer webcam placed in front of the participant during the cognitive tasks and the industrial tasks. Finally, audio recordings were obtained using a commercially available Bluetooth microphone.\n\nAll the physiological signals were acquired at 256 Hz using the software SennsLab  5  . The software manages Bluetooth communication with the devices and synchronizes the physiological signals and audio-video data. The data are displayed in real time, allowing for a visual inspection during the experiment.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Iv. Methods",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Data Processing",
      "text": "The ECG signals were filtered using a combination of a band-pass filter (with a frequency range from 0.05 to 40 Hz) and a Savitzky-Golay filter.\n\nElectromyography signals were filtered using a band-pass filter with a frequency range from 10 to 500 Hz coupled with a detrending algorithm, which removes the signal trend by evaluating the linear least-squares fit of the data as specified in the SENIAM recommendations  [23] .\n\nThe Electrodermal activity signal was filtered using a low-pass filter with a cut-off frequency of 10 Hz, coupled with a convolutional signal smoothing. Then, the signal is down-sampled at 100 Hz and divided into phasic and tonic components using the algorithm presented in  [24] .\n\nRespiration signals were filtered using a second-order band-pass filter with a frequency range from 0.03 to 5 Hz.\n\nElectroencephalogram signals were processed using two filters: a second-order band-pass filter with a frequency range from 0.5 to 40 Hz and a band-stop filter from 49 to 51 Hz to remove the amplifier noise.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Features Extraction",
      "text": "1) Physiological data: Following the processing pipeline, a total of 250 features were extracted from the processed physiological signals, segmented in 60 seconds windows, using the Neurokit package  [25] . These features comprise time-domain, frequency-domain, and complexity measures. For ECG signals, Heart Rate Variability (HRV) features were computed, following the definitions outlined in  [26] . EMG feature descriptions can be found in  [27] , while EDA-related features are detailed in  [28] .\n\nConcerning the EEG signals, after processing we segment the signal in 5 seconds window and compute 7 for each of the 12 channels, together with the ratios over the right and left hemispheres (θ F 3 / α P 3 , and θ F 4 / α P 4 ), which where significant to discriminate between levels of mental workload in  [29] . We evaluate the power in the frequency bands (γ (30-80 Hz), β (13-30 Hz), α (8-13 Hz), θ (4-8 Hz), and δ (1-4 Hz)) using Welch's Power Spectral Density (PSD)  [30] . Welch's method estimates the power spectrum of a signal by segmenting it into overlapping windows, computing the Discrete Fourier Transform (DFT) for each window, and then averaging the squared magnitudes. The PSD is computed as follows:\n\nwhere X k (ω) is the DFT of the k-th windowed segment, and M is the number of points in each segment. Moreover, we compute Differential entropy (DiffEn) and Sample Entropy (SampEn) for each channel.\n\n2) Face Action Units: To optimize computational efficiency, facial data were analyzed at a reduced frame rate of 2 fps. Action Unit (AU) detection was performed using the pre-trained XGBoost model from Py-Feat  [31] , which identifies the presence of facial muscle activations. The model estimates a probability score for each of the 20 detected action units at every selected frame, forming a multivariate time series per repetition.\n\n3) Voice Features: The spoken segments were automatically detected using the Silero-VAD model  [32] . Features consisted of statistical measurements of the fundamental frequency, harmonicity, shimmer, and jitter. Moreover, the features include speech formats and Mel Frequency Cepstrum Coefficients (MFCCs). From the latter, we evaluated statistical measurements such as mean and standard deviation as in  [33] , but we also included median, kurtosis, and skewness measurements.\n\n4) Text embeddings: Given the spoken segments, we used the large variant of OpenAI's Whisper model  [34]  to transcribe the voice into text. This transcription is later fed into a Sentence Transformer model  [35]    mother tongue, we employed a model fine-tuned for the Italian language  [36] .",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "V. Results",
      "text": "The proposed experimental protocol allows for the identification of a wide range of mental states in the participants. In Fig.  6 , the average ground truth label for each of the tasks is presented. It can be seen that participants experienced different emotional states and cognitive load during the experiment, allowing the dataset to grasp a more holistic view of the participants' psycho-physical state.\n\nUsing the features mentioned in IV-B, we assess the performance of out-of-the-box baseline models in a regression and a classification task. As baseline models, we select RandomForest  [37] , AdaBoost  [38] , and XGBoost  [39] . To evaluate the baseline models, we performed Leave-One-Subject-Out validation and computed the performance as mean and standard deviation across subjects. Both features and labels are normalized (min-max) using the maximum and minimum values of each subject. For the sake of simplicity, we evaluated three modalities: the data obtained using the Versatile Bio (ECG, EDA, EMG, RESP), the EEG data, and the voice features.\n\nFirst, we performed the regression over the normalized scores of NASA and STAI. The results are presented in Tab. I. Here, it can be noticed that physiological data provided the lowest RMSE, suggesting that they carry the most relevant information for estimating stress and cognitive load.\n\nFor the classification task, we identified three classes from STAI and NASA-TLX subjects' specific scores collected throughout the entire experience. The Low class is identified as the tasks where the subject gave a score lower than µ-δ/2, where µ is the subject's mean score across all the tasks and  δ is the standard deviation. The Medium class consists of all tasks where the subject answered with a score between µ -δ/2 and µ + δ/2. Finally, the tasks with High class are the ones where the subject answered with a score higher than µ + δ/2. The results for the classification task are presented in Tab. II. In this task, physiological features (ECG, EDA, EMG, RESP) achieved the highest F1 scores, particularly for cognitive load classification. Overall, physiological signals provide the most informative features for both regression and classification tasks, outperforming EEG and voice-based features. EEG signals contain valuable information but are more susceptible to noise, making their performance slightly lower than physiological data. Voice-based features show the lowest predictive power, suggesting that vocal markers alone may not be sufficient for stress and cognitive load estimation. The results indicate that more advanced machine learning models or multimodal fusion techniques could further enhance predictive performance  [40] .",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "In this paper, we introduced MultiPhysio-HRC, a multimodal physiological signals dataset for industrial Human-Robot Collaboration (HRC). Our dataset provides a comprehensive collection of physiological signals (EEG, ECG, EDA, RESP, EMG), facial features, and voice data, recorded in multiple scenarios, including real-world industrial-like settings. Through the diversity of the proposed exercises, we elicited diverse cognitive and emotional states, enabling a rich understanding of human psycho-physical responses.\n\nThe baseline models applied to the dataset suggest that physiological signals contain valuable information for estimating cognitive load and stress levels. However, the results indicate that achieving high accuracy remains challenging, underscoring the need for advanced machine learning approaches and multi-modal fusion techniques.\n\nBy making MultiPhysio-HRC publicly available, we aim to accelerate research in affective computing and humanaware robotics, fostering safer and more human-centered industrial human-robot collaboration.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: 1) Day 1 - Baseline and Stress Induction: Participants",
      "page": 2
    },
    {
      "caption": "Figure 1: Data acquisition protocol.",
      "page": 3
    },
    {
      "caption": "Figure 2: During the execution of these tasks",
      "page": 3
    },
    {
      "caption": "Figure 2: Displayed screen of each cognitive task: SCWT (top left), N-Back",
      "page": 3
    },
    {
      "caption": "Figure 3: Experimental robotic cell setup. The multiple components of the",
      "page": 4
    },
    {
      "caption": "Figure 4: , the complete set of steps of the",
      "page": 4
    },
    {
      "caption": "Figure 3: The robot was equipped",
      "page": 4
    },
    {
      "caption": "Figure 4: Battery disassembly steps.",
      "page": 4
    },
    {
      "caption": "Figure 5: Sample of the acquired physiological data. The participant signals are filtered and normalized (min-max).",
      "page": 5
    },
    {
      "caption": "Figure 5: , a sample of the collected physiological",
      "page": 5
    },
    {
      "caption": "Figure 6: The radar chart displays the mean values of various ground truth",
      "page": 6
    },
    {
      "caption": "Figure 6: , the average ground truth label for each of the tasks",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "STAI-Y1\nµ = 31.86,\nmax = 55.00": "",
          "Physio\nn = 250": "EEG\nn = 88",
          "RF\nAB\nXGB": "RF\nAB\nXGB",
          "0.20 ± 0.09\n0.20 ± 0.09\n0.23 ± 0.09": "0.32 ± 0.08\n0.30 ± 0.08\n0.32 ± 0.08"
        },
        {
          "STAI-Y1\nµ = 31.86,\nmax = 55.00": "",
          "Physio\nn = 250": "Voice\nn = 439",
          "RF\nAB\nXGB": "RF\nAB\nXGB",
          "0.20 ± 0.09\n0.20 ± 0.09\n0.23 ± 0.09": "0.32 ± 0.08\n0.33 ± 0.08\n0.34 ± 0.07"
        },
        {
          "STAI-Y1\nµ = 31.86,\nmax = 55.00": "NASA-TLX\nµ = 39.56,\nmax = 91.11",
          "Physio\nn = 250": "Physio\nn = 250",
          "RF\nAB\nXGB": "RF\nAB\nXGB",
          "0.20 ± 0.09\n0.20 ± 0.09\n0.23 ± 0.09": "0.19 ± 0.08\n0.19 ± 0.09\n0.20 ± 0.09"
        },
        {
          "STAI-Y1\nµ = 31.86,\nmax = 55.00": "",
          "Physio\nn = 250": "EEG\nn = 88",
          "RF\nAB\nXGB": "RF\nAB\nXGB",
          "0.20 ± 0.09\n0.20 ± 0.09\n0.23 ± 0.09": "0.31 ± 0.08\n0.29 ± 0.08\n0.32 ± 0.08"
        },
        {
          "STAI-Y1\nµ = 31.86,\nmax = 55.00": "",
          "Physio\nn = 250": "Voice\nn = 439",
          "RF\nAB\nXGB": "RF\nAB\nXGB",
          "0.20 ± 0.09\n0.20 ± 0.09\n0.23 ± 0.09": "0.32 ± 0.08\n0.32 ± 0.08\n0.33 ± 0.08"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Stress Class": "",
          "Physio\nn = 250": "EEG\nn = 88",
          "RF\nAB\nXGB": "RF\nAB\nXGB",
          "0.30 ± 0.14\n0.33 ± 0.14\n0.329 ± 0.14": "0.37 ± 0.12\n0.34 ± 0.16\n0.37 ± 0.11"
        },
        {
          "Stress Class": "",
          "Physio\nn = 250": "Voice\nn = 439",
          "RF\nAB\nXGB": "RF\nAB\nXGB",
          "0.30 ± 0.14\n0.33 ± 0.14\n0.329 ± 0.14": "0.35 ± 0.15\n0.34 ± 0.12\n0.36 ± 0.12"
        },
        {
          "Stress Class": "Cognitive Load\nClass",
          "Physio\nn = 250": "Physio\nn = 250",
          "RF\nAB\nXGB": "RF\nAB\nXGB",
          "0.30 ± 0.14\n0.33 ± 0.14\n0.329 ± 0.14": "0.39 ± 0.14\n0.38 ± 0.10\n0.38 ± 0.14"
        },
        {
          "Stress Class": "",
          "Physio\nn = 250": "EEG\nn = 88",
          "RF\nAB\nXGB": "RF\nAB\nXGB",
          "0.30 ± 0.14\n0.33 ± 0.14\n0.329 ± 0.14": "0.39 ± 0.11\n0.38 ± 0.18\n0.40 ± 0.13"
        },
        {
          "Stress Class": "",
          "Physio\nn = 250": "Voice\nn = 439",
          "RF\nAB\nXGB": "RF\nAB\nXGB",
          "0.30 ± 0.14\n0.33 ± 0.14\n0.329 ± 0.14": "0.41 ± 0.15\n0.37 ± 0.14\n0.38 ± 0.15"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Ergonomic human-robot collaboration in industry: A review",
      "authors": [
        "M Lorenzini",
        "M Lagomarsino",
        "L Fortini",
        "S Gholami",
        "A Ajoudani"
      ],
      "year": "2023",
      "venue": "Frontiers in Robotics and AI",
      "doi": "10.3389/frobt.2022.813907"
    },
    {
      "citation_id": "2",
      "title": "Outlook on human-centric manufacturing towards industry 5.0",
      "authors": [
        "Y Lu",
        "H Zheng",
        "S Chand",
        "W Xia",
        "Z Liu",
        "X Xu",
        "L Wang",
        "Z Qin",
        "J Bao"
      ],
      "year": "2022",
      "venue": "Journal of Manufacturing Systems"
    },
    {
      "citation_id": "3",
      "title": "Deliberative robotics -a novel interactive control framework enhancing humanrobot collaboration",
      "authors": [
        "A Valente",
        "G Pavesi",
        "M Zamboni",
        "E Carpanzano"
      ],
      "year": "2022",
      "venue": "CIRP Annals"
    },
    {
      "citation_id": "4",
      "title": "Detecting Stress During Real-World Driving Tasks Using Physiological Sensors",
      "authors": [
        "J Healey",
        "R Picard"
      ],
      "year": "2005",
      "venue": "IEEE Transactions on Intelligent Transportation Systems"
    },
    {
      "citation_id": "5",
      "title": "Introducing WESAD, a Multimodal Dataset for Wearable Stress and Affect Detection",
      "authors": [
        "P Schmidt",
        "A Reiss",
        "R Duerichen",
        "C Marberger",
        "K Van Laerhoven"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM International Conference on Multimodal Interaction",
      "doi": "10.1145/3242969.3242985"
    },
    {
      "citation_id": "6",
      "title": "DREAMER: A Database for Emotion Recognition Through EEG and ECG Signals From Wireless Low-cost Off-the-Shelf Devices",
      "authors": [
        "S Katsigiannis",
        "N Ramzan"
      ],
      "year": "2018",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "7",
      "title": "AVCAffe: A Large Scale Audio-Visual Dataset of Cognitive Load and Affect for Remote Work",
      "authors": [
        "P Sarkar",
        "A Posen",
        "A Etemad"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "8",
      "title": "StressID: a Multimodal Dataset for Stress Identification",
      "authors": [
        "H Chaptoukaev",
        "V Strizhkova",
        "M Panariello",
        "B Dalpaos",
        "A Reka",
        "V Manera",
        "S Thümmler",
        "E Ismailova",
        "M Todisco",
        "M Zuluaga",
        "L Ferrari"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "9",
      "title": "SenseCobot",
      "authors": [
        "Sensecobot"
      ],
      "year": "2023",
      "venue": "SenseCobot"
    },
    {
      "citation_id": "10",
      "title": "Assessing operator stress in collaborative robotics: A multimodal approach",
      "authors": [
        "S Borghi",
        "A Ruo",
        "L Sabattini",
        "M Peruzzini",
        "V Villani"
      ],
      "year": "2025",
      "venue": "Applied Ergonomics"
    },
    {
      "citation_id": "11",
      "title": "Assessing the impact of human-robot collaboration on stress levels and cognitive load in industrial assembly tasks",
      "authors": [
        "A Bussolan",
        "S Baraldo",
        "L Gambardella",
        "A Valente"
      ],
      "year": "2023",
      "venue": "th International Symposium on Robotics"
    },
    {
      "citation_id": "12",
      "title": "The stroop color and word test",
      "authors": [
        "F Scarpina",
        "S Tagini"
      ],
      "year": "2017",
      "venue": "Frontiers in Psychology",
      "doi": "10.3389/fpsyg.2017.00557"
    },
    {
      "citation_id": "13",
      "title": "Reporting and interpreting working memory performance in n-back tasks",
      "authors": [
        "A Meule"
      ],
      "year": "2017",
      "venue": "Frontiers in Psychology",
      "doi": "10.3389/fpsyg.2017.00352"
    },
    {
      "citation_id": "14",
      "title": "Tower of Hanoi Problem",
      "authors": [
        "K Schmidtke"
      ],
      "year": "2010",
      "venue": "Tower of Hanoi Problem",
      "doi": "10.1002/9780470479216.corpsy1002"
    },
    {
      "citation_id": "15",
      "title": "Advancing humanrobot collaboration by robust speech recognition in smart manufacturing",
      "authors": [
        "O Avram",
        "C Fasana",
        "S Baraldo",
        "A Valente"
      ],
      "year": "2024",
      "venue": "European Robotics Forum"
    },
    {
      "citation_id": "16",
      "title": "Htn replanning from the middle",
      "authors": [
        "Y Bansod",
        "S Patra",
        "D Nau",
        "M Roberts"
      ],
      "year": "2022",
      "venue": "The International FLAIRS Conference Proceedings"
    },
    {
      "citation_id": "17",
      "title": "Reducing the barrier to entry of complex robotic software: a moveit! case study",
      "authors": [
        "D Coleman",
        "I Sucan",
        "S Chitta",
        "N Correll"
      ],
      "year": "2014",
      "venue": "Reducing the barrier to entry of complex robotic software: a moveit! case study",
      "arxiv": "arXiv:1404.3785"
    },
    {
      "citation_id": "18",
      "title": "Manual for the State-trait Anxiety Inventory (form Y) (\"self-evaluation Questionnaire\")",
      "authors": [
        "C Spielberger",
        "R Gorsuch"
      ],
      "year": "1983",
      "venue": "Manual for the State-trait Anxiety Inventory (form Y) (\"self-evaluation Questionnaire\")"
    },
    {
      "citation_id": "19",
      "title": "Development of NASA-TLX (Task Load Index): Results of Empirical and Theoretical Research",
      "authors": [
        "S Hart",
        "L Staveland"
      ],
      "year": "1988",
      "venue": "Advances in Psychology"
    },
    {
      "citation_id": "20",
      "title": "Measuring emotion: The selfassessment manikin and the semantic differential",
      "authors": [
        "M Bradley",
        "P Lang"
      ],
      "year": "1994",
      "venue": "Journal of Behavior Therapy and Experimental Psychiatry"
    },
    {
      "citation_id": "21",
      "title": "Psychology in humanrobot communication: An attempt through investigation of negative attitudes and anxiety toward robots",
      "authors": [
        "T Nomura",
        "T Kanda",
        "T Suzuki",
        "K Kato"
      ],
      "year": "2004",
      "venue": "RO-MAN 2004. 13th IEEE International Workshop on Robot and Human Interactive Communication"
    },
    {
      "citation_id": "22",
      "title": "Modelling and measuring trust in human-robot collaboration",
      "authors": [
        "E Loizaga",
        "L Bastida",
        "S Sillaurren",
        "A Moya",
        "N Toledo"
      ],
      "year": "2024",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "23",
      "title": "Standards for surface electromyography: The European project \"Surface EMG for non-invasive assessment of muscles (SENIAM)",
      "authors": [
        "D Stegeman",
        "H Hermens"
      ],
      "venue": "Standards for surface electromyography: The European project \"Surface EMG for non-invasive assessment of muscles (SENIAM)"
    },
    {
      "citation_id": "24",
      "title": "cvxEDA: A Convex Optimization Approach to Electrodermal Activity Processing",
      "authors": [
        "A Greco",
        "G Valenza",
        "A Lanata",
        "E Scilingo",
        "L Citi"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "25",
      "title": "NeuroKit2: A python toolbox for neurophysiological signal processing",
      "authors": [
        "D Makowski",
        "T Pham",
        "Z Lau",
        "J Brammer",
        "F Lespinasse",
        "H Pham",
        "C Schölzel",
        "S Chen"
      ],
      "year": "2021",
      "venue": "Behavior Research Methods"
    },
    {
      "citation_id": "26",
      "title": "Heart Rate Variability in Psychology: A Review of HRV Indices and an Analysis Tutorial",
      "authors": [
        "T Pham",
        "Z Lau",
        "S Chen",
        "D Makowski"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "27",
      "title": "EMG-based Real Time Facial Gesture Recognition for Stress Monitoring",
      "authors": [
        "S Orguc",
        "H Khurana",
        "K Stankovic",
        "H Leel",
        "A Chandrakasan"
      ],
      "year": "2018",
      "venue": "2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)"
    },
    {
      "citation_id": "28",
      "title": "Feature Extraction and Selection for Emotion Recognition from Electrodermal Activity",
      "authors": [
        "J Shukla",
        "M Barreda-Angeles",
        "J Oliver",
        "G Nandi",
        "D Puig"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "29",
      "title": "An evaluation of the EEG alpha-to-theta and theta-to-alpha band ratios as indexes of mental workload",
      "authors": [
        "B Raufi",
        "L Longo"
      ],
      "year": "2022",
      "venue": "Frontiers in Neuroinformatics",
      "doi": "10.3389/fninf.2022.861967/full"
    },
    {
      "citation_id": "30",
      "title": "The use of fast fourier transform for the estimation of power spectra: A method based on time averaging over short, modified periodograms",
      "authors": [
        "P Welch"
      ],
      "year": "1967",
      "venue": "IEEE Transactions on Audio and Electroacoustics"
    },
    {
      "citation_id": "31",
      "title": "Py-Feat: Python Facial Expression Analysis Toolbox",
      "authors": [
        "J Cheong",
        "E Jolly",
        "T Xie",
        "S Byrne",
        "M Kenney",
        "L Chang"
      ],
      "year": "2023",
      "venue": "Affective Science",
      "doi": "10.1007/s42761-023-00191-4"
    },
    {
      "citation_id": "32",
      "title": "Silero vad: pre-trained enterprise-grade voice activity detector (vad), number detector and language classifier",
      "authors": [
        "S Team"
      ],
      "year": "2021",
      "venue": "Silero vad: pre-trained enterprise-grade voice activity detector (vad), number detector and language classifier"
    },
    {
      "citation_id": "33",
      "title": "Stress Detection Through Speech Analysis",
      "authors": [
        "K Tomba",
        "J Dumoulin",
        "E Mugellini",
        "O Khaled",
        "S Hawila"
      ],
      "year": "2018",
      "venue": "Proceedings of the 15th International Joint Conference on e-Business and Telecommuni-cations"
    },
    {
      "citation_id": "34",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2023",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "35",
      "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
      "authors": [
        "N Reimers",
        "I Gurevych"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "36",
      "title": "",
      "authors": [
        "N Procopio"
      ],
      "venue": ""
    },
    {
      "citation_id": "37",
      "title": "Random forests",
      "authors": [
        "L Breiman"
      ],
      "year": "2001",
      "venue": "Mach. Learn",
      "doi": "10.1023/A:1010933404324"
    },
    {
      "citation_id": "38",
      "title": "A decision-theoretic generalization of on-line learning and an application to boosting",
      "authors": [
        "Y Freund",
        "R Schapire"
      ],
      "year": "1997",
      "venue": "Journal of Computer and System Sciences"
    },
    {
      "citation_id": "39",
      "title": "XGBoost: A scalable tree boosting system",
      "authors": [
        "T Chen",
        "C Guestrin"
      ],
      "year": "2016",
      "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser. KDD '16",
      "doi": "10.1145/2939672.2939785"
    },
    {
      "citation_id": "40",
      "title": "Multimodal fusion stress detector for enhanced human-robot collaboration in industrial assembly tasks",
      "authors": [
        "A Bussolan",
        "S Baraldo",
        "L Gambardella",
        "A Valente"
      ],
      "year": "2024",
      "venue": "2024 33rd IEEE International Conference on Robot and Human Interactive Communication (ROMAN)"
    }
  ]
}