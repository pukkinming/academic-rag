{
  "paper_id": "2401.03429v3",
  "title": "Merbench: A Unified Evaluation Benchmark For Multimodal Emotion Recognition",
  "published": "2024-01-07T09:09:32Z",
  "authors": [
    "Zheng Lian",
    "Licai Sun",
    "Yong Ren",
    "Hao Gu",
    "Haiyang Sun",
    "Lan Chen",
    "Bin Liu",
    "Jianhua Tao"
  ],
  "keywords": [
    "Multimodal Emotion Recognition Benchmark (MERBench)",
    "feature selection",
    "multimodal fusion",
    "cross-corpus performance",
    "robustness analysis"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal emotion recognition plays a crucial role in enhancing user experience in human-computer interaction. Over the past few decades, researchers have proposed a series of algorithms and achieved impressive progress. Although each method shows its superior performance, different methods lack a fair comparison due to inconsistencies in feature extractors, evaluation manners, and experimental settings. These inconsistencies severely hinder the development of this field. Therefore, we build MERBench, a unified evaluation benchmark for multimodal emotion recognition. We aim to reveal the contribution of some important techniques employed in previous works, such as feature selection, multimodal fusion, robustness analysis, fine-tuning, pre-training, etc. We hope this benchmark can provide clear and comprehensive guidance for follow-up researchers. Based on the evaluation results of MERBench, we further point out some promising research directions. Additionally, we introduce a new emotion dataset MER2023, focusing on the Chinese language environment. This dataset can serve as a benchmark dataset for research on multi-label learning, noise robustness, and semi-supervised learning. We encourage the follow-up researchers to evaluate their algorithms under the same experimental setup as MERBench for fair comparisons. Our code is available at: https://github.com/zeroQiaoba/MERTools.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "E MOTIONS are complex and can be conveyed through multiple modalities, such as video, audio, and text  [1] . Multimodal emotion recognition aims to integrate multisource information to identify human emotional states  [2] . Due to its importance in human-computer interactions  [3] ,  [4] , this task has become an active research topic.\n\nIn recent years, researchers have proposed various algorithms and achieved noteworthy advancements  [5] ,  [6] . Although each algorithm demonstrates its superior performance, it is difficult to fairly compare different algorithms due to their inconsistencies in feature extractors, evaluation manners, and experimental settings, which harms the development of this field. Take the feature extractor as an example. Different features contain distinct emotion-related information. Some researchers focus on handcrafted features  [7] ,  [8] , while others exploit deep features  [9] ,  [10] . Meanwhile, different deep features also perform differently in emotion recognition  [11] ,  [12] . In addition to features, there are also some differences in evaluation methods. Taking one of the widely used datasets IEMOCAP  [13]  as an example, some use the leave-one-session-out strategy  [14] ,  [15]  while some train on the first four sessions and evaluate on the last • Zheng Lian, Lan Chen, and Bin Liu are with the State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Bejing, China. E-mail: lianzheng2016@ia.ac.cn, chenlan.2016.cn@gmail.com, liubin@nlpr.ia.ac.cn. • Licai Sun, Yong Ren, Hao Gu, and Haiyang Sun are with the School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China. E-mail: sunlicai2019@ia.ac.cn, renyong2020@ia.ac.cn, guhao2022@ia.ac.cn, sunhaiyang2021@ia.ac.cn. • Jianhua Tao is with Department of Automation, Tsinghua University, Bejing, China. E-mail: jhtao@tsinghua.edu.cn.\n\nManuscript received xxxxxxxx; revised xxxxxxxx. (Corresponding author: Zheng Lian, Jianhua Tao)\n\nsession  [16] ,  [17] . These inconsistencies make it difficult to directly compare the performance of different algorithms. Therefore, we build MERBench, a unified evaluation benchmark for multimodal emotion recognition. This benchmark involves primary datasets, features, and multimodal fusion strategies in this field. Under the same experimental setup, we explore some key problems, including how to select appropriate features for different datasets, how to determine multimodal fusion strategies, how to obtain better performance in cross-corpus settings, how to improve noise robustness, what is the impact of missing punctuation, whether the feature extractor needs to be pretrained to adapt to downstream tasks, whether the feature extractor needs to be further fine-tuned with the classifier, etc. This paper aims to provide a comprehensive understanding of multimodal emotion recognition and provide guidance for follow-up researchers. Meanwhile, we hope that subsequent works can adopt the same experimental setup as our MERBench for fair comparisons.\n\nAdditionally, this paper proposes a new Chinese emotion dataset MER2023. Compared with existing datasets  [18] ,  [19] , we aim to provide a benchmark dataset for research on multi-label learning  [20] , noise robustness  [15] , and semi-supervised learning  [21] , since these directions are currently hot topics in this field. Specifically, MER2023 contains three subsets: a multi-label subset for studying discrete and dimension label correlations, a noisy subset for evaluating noise robustness, and an unlabeled subset for studying semi-supervised learning. The main contribution of this paper can be summarized as follows:\n\n• We build a unified evaluation benchmark for multimodal emotion recognition. To the best of our knowledge, this is the most comprehensive benchmark in this field, covering feature selection, multimodal fusion, cross-corpus performance, robustness analysis, language sensitivity analysis, etc. In this benchmark, we reproduce different methods under the same experimental setup for a fair comparison.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "•",
      "text": "We build MER2023, a Chinese emotion dataset designed to serve as a benchmark for evaluating multilabel learning, noise robustness, and semi-supervised learning in multimodal emotion recognition.\n\n• Based on the evaluation results of MERBench, we point out some promising research directions in this field. We will also open-source the code to facilitate follow-up researchers to evaluate their algorithms under the same experimental setting as MERBench.\n\nThe rest of this paper is organized as follows: In Section 2, we briefly review some recent works. In Section 3 and Section 4, we introduce MER2023 and establish a baseline for this dataset. In Section 5, we build MERBench, an evaluation benchmark for multimodal emotion recognition. Finally, we conclude this paper and discuss future work in Section 6.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Works",
      "text": "This paper covers various research topics. In this section, we primarily review recent works related to corpus design, feature selection, and multimodal fusion.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Emotional Corpus",
      "text": "Emotional corpora are the basis for training and evaluating emotion recognition algorithms. Current datasets gradually transition from laboratory-controlled to more challenging in-the-wild conditions. For example, the early dataset like IEMOCAP  [13]  is laboratory-controlled and contains conversations between two actors performing improvised or scripted scenarios. To mimic real-world conditions, researchers have directed their attention towards in-the-wild datasets, which are mainly collected from online websites, movies, and TV shows (e.g., CMU-MOSI  [22] , CMU-MOSEI  [23] , MELD  [24] , CH-SIMS  [18] , and CH-SIMS v2  [25] ).\n\nRecent research mainly involves multi-label learning, semi-supervised learning, and noise robustness, which puts forward new requirements for dataset design. 1) Discrete and dimensional emotions are closely related  [26] . For example, valence is a dimensional emotion that reflects the degree of pleasure  [27] . For negative emotions (such as anger and sadness), the valence score should be less than 0; for positive emotions (such as happiness), the valence score should be greater than 0  [28] . To study the impact of multi-label correlation, the dataset is required to provide both discrete and dimensional annotations; 2) Due to the high annotation cost, it is difficult to collect large amounts of samples with emotional labels. But training with limited data harms the generalization ability. To solve this problem, researchers exploit various pre-trained models, but they mainly focus on action recognition rather than facial expressions  [29] . This domain gap harms the performance of transfer learning  [30] . Therefore, the dataset is required to provide large amounts of unlabeled human-centered videos; 3) Noise and missing conditions increase the difficulty of emotion recognition  [31] ,  [32] . However, due to the lack of benchmark datasets, existing works mainly rely on their own simulated missing data  [33] ,  [34] . For a fair comparison, a benchmark test set that focuses on realistic missing conditions is needed.\n\nAmong existing datasets, some provide multi-label samples (such as IEMOCAP  [13]  and CMU-MOSEI  [23] ), some provide unlabeled samples (such as CH-SIMS v2  [25] ), and none of them provides a noise-corrupted test set. Therefore, we introduce MER2023, aiming to provide a benchmark dataset that can study these three directions simultaneously.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Unimodal Features",
      "text": "Different features lead to distinct results. To guide feature selection, we evaluate the performance of different features under the same experimental setup. This paper attempts to cover all typical features of each modality.\n\nAcoustic Modality. Paralinguistic information is important for understanding emotions  [35] . Previously, researchers explored various low-level descriptors (such as energy, duration, and pitch) and then used statistical functions to extract high-level features (e.g., IS09  [36] , IS10  [7] , and eGeMAPS  [37] ). Besides handcrafted features, selfsupervised models trained on large amounts of unlabeled data can also learn universal acoustic representations  [38] . Among them, wav2vec and wav2vec 2.0 are widely used due to their competitive performance. wav2vec  [39]  is a multi-layer convolutional network trained on the contrastive task, aiming to distinguish real future audio from negatives. Unlike wav2vec, wav2vec 2.0  [40]  uses quantized representations for self-supervised learning. HUBERT  [41]  extends wav2vec 2.0 and exploits an offline clustering module to provide more accurate quantized features.\n\nLexical Modality. The text also contains emotion-related clues  [17] ,  [42] . Language models can learn universal lexical representations, which are beneficial for many tasks  [43] . Among all language models, BERT  [44]  and its variants are widely utilized. BERT uses the \"masked language model (MLM)\" and \"next sentence prediction (NSP)\" objectives to learn word embeddings. MLM randomly masks words and attempts to predict the masked items based on their context; NSP tries to capture sentence relationships by distinguishing whether two sentences are continuous or not. To further improve its performance, RoBERTa  [45]  proposes to remove the NSP objective in BERT and train with dynamic masking. XLNet  [46]  overcomes the pretrain-finetune discrepancy of BERT via autoregressive pre-training. DeBERTa  [47]  improves BERT by disentangling attention matrices on the content and position. To reduce computational costs, AL-BERT  [48]  exploits parameter reduction techniques, while ELECTRA  [49]  replaces MLM with a compute-efficient task. Recently, large language models (LLMs) with massive parameters have attracted increasing attention due to their unprecedented performance. Therefore, we further evaluate some representative open-sourced LLMs, such as Llama  [50] , Falcon  [51] , and BLOOM  [52] . Due to GPU memory limitations, we evaluate the maximum 13B version of LLMs.\n\nVisual Modality. Facial expressions are natural signals to convey emotions  [53] . Compared with handcrafted features, deep features extracted from supervised models are useful for facial expression recognition  [54] . These models depend on the model structure (e.g., ResNet  [55] , SENet  [56] , and MA-Net  [57] ) and the training corpus (e.g., MS-Celeb-1M  [58] , FER2013  [59] , and RAF-DB  [60] ). Different combinations will result in distinct performance. Recently, visual features extracted from weakly-supervised or selfsupervised models have achieved remarkable results on many tasks  [61] ,  [62] . For example, CLIP  [63]  is a weaklysupervised model that learns universal representations by predicting correct image-text pairs. To further improve its performance, DINOv2  [64]  combines different techniques to increase data and model size. Besides weakly-supervised models, self-supervised models (such as VideoMAE  [29] ) can also serve as powerful visual encoders.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Multimodal Fusion",
      "text": "Multimodal fusion aims to integrate multi-view cues to produce more emotionally discriminative features  [65] ,  [66] . Current methods can be roughly divided into utterancelevel and sequence-level algorithms. This paper evaluates some representative algorithms in these two categories.\n\nUtterance-level Fusion. An intuitive idea is to compress different modalities to the unified utterance level and then fuse them. For example, TFN  [22]  utilizes a 3-fold Cartesian product to aggregate utterance-level features. To reduce computational complexity, LMF  [67]  employs low-rank decomposition to approximate the high-order tensor in TFN. To address multimodal heterogeneity, MISA  [68]  decomposes each modality into modality-invariant and modalityspecific features. MMIM  [69]  reduces the loss of emotionrelated information by maximizing mutual information.\n\nSequence-level Fusion. Although utterance-level fusion achieves superior performance, compressing features to the utterance level inevitably loses some important information. Consequently, researchers propose sequence-level fusion algorithms. For example, MFN  [70]  introduces a multi-view sequence learning framework to capture both view-specific and cross-view interactions. GMFN  [23]  extends MFN by applying a dynamical fusion graph, placing more emphasis on important modalities. MCTN  [71]  learns joint representations through cyclic translation from source to target feature sequences. To handle multimodal heterogeneity, MFM  [72]  factorizes features into multimodal discriminative factors and modality-specific factors. To capture long-term crossmodal interactions, MulT  [73]  uses the Transformer architecture to dynamically align different modalities.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Mer202Dataset",
      "text": "This paper introduces a new Chinese emotion dataset, MER2023, consisting of four subsets: Train&Val for training and validation, and MER-MULTI, MER-NOISE, MER-SEMI for testing. In this section, we provide an in-depth description of the data collection, annotation, and splitting. Statistics for each subset are shown in Table  1 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data Collection",
      "text": "Video Clip Generation. We collect a large number of movies and TV series from the Internet and split them into video clips. Considering that subtitles have relatively accurate timestamps, we try to use them for video segmentation. However, we observe that only a few videos have subtitle Video Filter. We use two filters to remove difficult-tolabel video clips.  (1)  Videos that are too short may not contain complete content to express emotions; videos that are too long may contain compound emotions, weakening the consistency of annotations. Therefore, we remove clips that are too long or too short. (2) For multi-speaker videos, it is necessary to eliminate the interference of other speakers during annotation. For convenience, we only select singlespeaker clips. Specifically, we first use the face detection tool, YuNet 6 , to ensure that most frames contain only one face. Then, we use the face recognition tool, face.evoLVe 7 , to ensure that most faces belong to the same person.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data Annotation",
      "text": "The labeling process includes four steps: emotion annotation, agreement check, reliability check, and valence annotation. Fig.  1  shows the entire annotation pipeline.\n\nEmotion Annotation. Each video clip is labeled by at least three annotators using six categories: neutral, anger, happiness, sadness, worry, and surprise. If neither of them accurately describes the emotional state, two special categories are also provided (i.e., other and unjudgeable). Since most annotators specialize in affective computing, we have confidence in the trustworthiness of their annotation results.\n\nAgreement Check. To further enhance the annotation quality, we adopt a majority voting strategy. For samples without major emotions or with the major emotion belonging to other or unjudgeable, we treat them as unlabeled data.\n\nReliability Check. In the reliability check, each (video, label) pair is labeled by two annotators using three categories: reliable, unreliable, and unjudgeable. To pass this check,    all annotators should assign reliable to a sample. For unpassed samples, we relabel them since they have a certain degree of agreement. If these samples still fail to pass these checks after relabeling, they are treated as unlabeled data.\n\nValence Annotation. For reliable samples, we hire six annotators to label valence using the self-assessment manikin  [74] . To obtain high-quality annotations, we exclude the lowest and highest values and calculate the average of the remaining four annotations as the final valence score.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Data Splitting",
      "text": "Due to the large amounts of collected samples and the high cost of emotion annotation, we only select a subset of samples for annotation (see Fig.  1 ). The reliable samples are divided into three subsets: Train&Val, MER-MULTI, and MER-NOISE. The remaining samples are treated as unlabeled data and form MER-SEMI. For MER-SEMI, we conduct additional annotation for samples likely to exhibit well-defined emotions (see Algorithm 1). Statistics of these subsets are shown in Fig.  2  and Fig.  3 . We observe that most samples have durations ranging from 2 to 6 seconds. Meanwhile, the distribution of emotions is not well-balanced, with higher proportions of neutral, anger, happiness, and sadness, similar to previous datasets  [19] ,  [24] .\n\nTrain&Val and MER-MULTI. These two subsets provide both discrete and dimensional annotations, and their rela-  tionship is shown in Fig.  4 . Valence represents the degree of pleasure and the value from small to large means the sentiment from negative to positive. From this figure, we observe that the valence distribution of different discrete labels is reasonable. For negative emotions (such as anger, sadness, and worry), the valence is mostly less than 0; for positive emotions (such as happiness), the valence is mostly greater than 0; the valence of neutral is around 0; surprise is a rather complex emotion that contains multiple meanings such as sadly surprised, angrily surprised, or happily surprised. Therefore, its valence ranges from negative to positive. These results ensure the quality of our labels and demonstrate the necessity of multi-label annotations, as they can help us distinguish some subtle emotional differences.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Mer-Noise.",
      "text": "This subset involves introducing noise to samples to assess noise robustness. For the audio modality, we randomly select noises from MUSAN  [75] , a noise corpus comprising three subsets: music, noise, and speech. Among these subsets, music may affect the original emotion of the input audio, while noise also conveys emotions. For example, rain and thunder may evoke negative sentiments; a pleasant breeze may evoke positive sentiments. Therefore, we select noises from the speech subset. Subsequently, we randomly choose a signal-to-noise ratio (SNR) between 5∼10dB, adjust the noise amplitude according to SNR, crop the noise to the same length as the audio, and finally merge them. For the visual modality, blurring is a common technique for obtaining low-quality images, as network limita- tions in transmission often occur. To mimic this process, we first downsample the image to lose some details and then upsample the low-resolution image to maintain its original size. For each video, we randomly select a downsampling factor from {1, 2, 4} and apply it to all frames. For convenience, this paper does not discuss the case where different frames have distinct downsampling factors.\n\nMER-SEMI. This subset consists of large amounts of unlabeled samples. To further annotate some of these samples, we utilize the maximum softmax probabilities  [76]  to measure confidence and only annotate samples with high confidence. We find this process helps reduce the difficulty of labeling. Suppose f (•) is a pre-trained emotion classifier, and f j (•) represents the prediction result for the class j. For sample x i , its confidence score is defined as s i = max j f j (x i )  [76] . After obtaining the confidence score, previous works often determine a threshold, and samples with scores higher than the threshold are considered reliable  [76] . However, an inherent class imbalance exists in the corpus (see Fig.  3 ). It is unreasonable to select a fixed threshold for different classes. Inspired by long-tail learning  [77] , we select the sample whose confidence falls in the highest η portion for each class to form a class-specific threshold. The more detailed procedure is presented in Algorithm 1.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Mer2023 Baselines",
      "text": "This section first formulates the problem definition and introduces the notations used. Subsequently, we introduce the data preprocessing, model structure, and implementation details of the baseline. Finally, we present unimodal and multimodal results and provide an in-depth discussion.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Problem Definition And Notations",
      "text": "Let us define a labeled dataset\n\n, where e i ∈ {1, 2, • • • C} and v i ∈ [-5, 5] represent the discrete category and valence of the sample x i . Here, N l , N u , and C denote the number of labeled samples, unlabeled samples, and discrete classes, respectively. In this paper, we aim to predict discrete and dimensional emotions from audiovisual recordings.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Data Preprocessing",
      "text": "For the visual modality, we first crop and align faces via OpenFace 8  [78] . Then, we use visual encoders to extract frame-level or segment-level features, followed by average pooling to compress them to the video level. For the audio modality, we use FFmpeg to separate the audio from the video and unify the audio format to 16kHz and mono. For the lexical modality, we first extract subtitles using WeNet 9  [79] , an open-source ASR toolkit. But the timestamps in video segmentation are not always accurate, causing the audio signal loss in some samples and resulting in ASR errors. Additionally, we observe that some videos also include subtitles. Therefore, we further use EasyOCR to extract subtitles from video frames. Then, we manually merge the ASR and OCR outputs to generate the final subtitles.\n\nAfter data preprocessing, for each sample x i , we extract acoustic features\n\nwhere {d m } m∈{a,l,v} is the feature dimension for each modality. Table  18  (see Appendix) presents the model cards of the main encoders involved in this paper. Since MER2023 is a Chinese dataset, we choose lexical encoders that support Chinese. Additionally, we evaluate the performance of acoustic encoders trained in different languages. More analysis on feature selection is available in the MERBench (see Section 5).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Model Structure",
      "text": "For unimodal features, we utilize the fully-connected layers to extract hidden representations, followed by a multi-task framework to predict both discrete and dimensional emotions simultaneously:\n\nwhere h m i ∈ R h is the hidden feature for each modality. vi ∈ R 1 and êi ∈ R C are the estimated valence and emotion probabilities.\n\n, and b e m ∈ R C are trainable parameters. For multimodal features, different modalities contribute differently to emotion recognition. Therefore, we compute importance scores α i ∈ R 3×1 for each modality and exploit weighted fusion to obtain multimodal features:  Here, W α ∈ R h×1 and b α ∈ R 3 are trainable parameters.\n\nDuring training, we choose the cross-entropy loss L e for classification and the mean squared error (MSE) L v for regression. We combine them into a joint objective and optimize all trainable parameters in an end-to-end manner.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Implementation Details",
      "text": "There is mainly one user-specific parameter in the baseline, i.e., the dimension of latent representations h. We select h from {64, 128, 256}. For the Train&Val subset, we employ five-fold cross-validation for hyperparameter tuning. During training, we use the Adam  [80]  optimizer and choose the learning rate from {10 -3 , 10 -4 }. We set the maximum number of epochs to 100 and the weight decay to 10 -5 . Dropout  [81]  is also employed, and we select the rate from {0.2, 0.3, 0.4, 0.5} to alleviate the over-fitting problem. To mitigate randomness, we run each experiment six times and report the average result along with the standard deviation.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results And Discussion",
      "text": "Tables 2∼3 present unimodal and multimodal results. In addition to five-fold cross-validation results on Train&Val, we also report test results on the MER-MULTI, MER-NOISE, TABLE  3  Multimodal results on MER2023. We select acoustic features from HUBERT-base (HB) and HUBERT-large (HL), lexical features from BLOOM-7B\n\n(BL) and Baichuan-13B (BA), and visual features from EVA-02-base (EB) and CLIP-large (CL). Here, \"A\", \"L\", and \"V\" represent the acoustic, lexical, and visual modalities, respectively. and MER-SEMI subsets. For discrete emotions, considering the inherent emotion class imbalance in the dataset (see Fig.  3 ), we choose the weighted average F-score (WAF) as the evaluation metric. For dimensional emotions, we use MSE as the evaluation metric, consistent with previous works  [82] . Higher WAF and lower MSE indicate better performance. In Tables 2∼3, the standard deviation is relatively small, which eliminates the influence of randomness and ensures the reliability of our conclusions. Table  2  summarizes the unimodal results. For the audio modality, deep features consistently outperform handcrafted features. Since emotion-related acoustic features vary across datasets, using a fixed set of handcrafted features may limit performance. In contrast, deep features can capture more universal acoustic representations for emotion recognition. For the visual modality, models trained on facial expression recognition (e.g., FER2013  [59]  and RAF-DB  [60] ) generally perform better than those trained on face recognition (e.g., MS-Celeb-1M  [58] ) and object recognition (e.g., ImageNet  [83] ). These results demonstrate the importance of task similarity in transfer learning. Meanwhile, the audio modality can achieve better performance than the visual and lexical modalities, which indicates that our dataset focuses more on audio to express emotions.\n\nIn Table  3 , we select several well-performing unimodal features and report their fusion results. Experimental results demonstrate that multimodal fusion consistently improves performance. The reason lies in the fact that emotions can be conveyed through multiple modalities. The integration of multimodal information allows the model to better comprehend the video content and accurately recognize emotions.\n\nIn Tables 2∼3, models that perform well on Train&Val generally exhibit good performance on other subsets. These results demonstrate that our system does not overfit training data and has a good generalization ability to unseen data. Meanwhile, MER-SEMI achieves better performance than the other two subsets. The reason lies in our confidencebased selection strategy (in Algorithm 1), which selects samples with well-defined emotions that can be easily recognized by emotion recognition systems. Meanwhile, we observe that models performing well on dimensional emotions generally achieve good performance on discrete emotions (see Tables  2∼3 ). These results reflect a high correlation between discrete and dimensional labels, consistent with the phenomenon in Fig.  4 . Compared with MER-MULTI, our system performs worse on MER-NOISE, and the acoustic modality degrades more than the visual modality. These results indicate that adding noise to audio has a larger impact than blurring video frames.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Merbench",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Corpus Description",
      "text": "In addition to MER2023, we also include mainstream datasets for multimodal emotion recognition. The statistical information for all corpora is presented in Table  4 .\n\nIEMOCAP  [13]  consists of five sessions, each with two actors performing improvised or scripted conversations. We adopt two popular label processing methods, resulting in four-class  [16] ,  [84]  and six-class  [17] ,  [85]  versions. CMU-MOSI  [22]  contains movie review videos from online websites. CMU-MOSEI  [23]  extends CMU-MOSI with more samples and a greater variety of topics. CH-SIMS  [18]  and CH-SIMS v2  [25]  are Chinese datasets collected from movies, TV series, and shows. MELD  [24]  extends Emotion-Lines  [86]  with multimodal information. For MER2023, we focus on the MER-MULTI subset in this benchmark. Since IEMOCAP does not provide official data splitting, we perform five-fold cross-validation using the leave-onesession-out strategy. For other datasets, they provide training/validation/test sets and we follow the default data splitting method to ensure a fair comparison. Among these datasets, IEMOCAP, MELD, and MER-MULTI provide discrete labels, while the remaining datasets utilize continuous sentiment scores. For the latter, we focus on the negative/positive classification task, where positive and negative classes are assigned for < 0 and > 0 scores, respectively. In this way, all datasets are transformed into categorical datasets, and we use WAF as the evaluation metric.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Unimodal Benchmark",
      "text": "In this section, we establish the unimodal benchmark for all datasets and report results in Table  5 . We hope this benchmark can provide guidance for feature selection and point the way to developing powerful feature extractors.\n\nVisual Modality. We evaluate some representative fully-, weakly-, and self-supervised visual encoders. A fullysupervised model relies on both the model structure and the training corpus. As shown in Table  5 , models trained for facial expression recognition tend to exhibit superior performance than object recognition and face recognition. These findings demonstrate the importance of domain compatibility between training corpora and downstream tasks.\n\nAlthough different datasets exhibit preferences for distinct visual encoders, their best features consistently come from weakly-or self-supervised models. These results indicate that weakly-or self-supervised models can learn universal visual representations, which are also useful for emotion recognition. Notably, the model cards in Table  18  (see Appendix) reveal that current encoders are often trained on action recognition datasets (e.g., Kinetics-400  [87] ) or web images (e.g., LVD-142M  [64] ). A heuristic idea is to further narrow the domain gap by training on humancentric videos. In Table  6 , we choose a self-supervised model, VideoMAE, and train it on human-centric corpora, MER-SEMI and VoxCeleb2  [88] . Consistent with previous findings  [89] , this approach significantly improves performance, which suggests future directions for visual encoders.\n\nIn emotion recognition, a good visual encoder should focus on weakly-or self-supervised learning and train on large amounts of human-centric videos. Audio Modality. In Table  5 , we observe that acoustic encoders trained on Chinese data (e.g., HUBERT-large) generally perform well on Chinese emotion corpora (e.g., MER-MULTI and CH-SIMS), while acoustic encoders trained on English data (e.g., WavLM-large) generally perform well on English emotion corpora (e.g., CMU-MOSI and CMU-MOSEI). These results suggest that acoustic encoders are language-sensitive. To uncover the underlying reasons behind this phenomenon, we conduct more experiments.\n\nAudio contains linguistic and paralinguistic information. Considering that linguistic information is languagesensitive, a heuristic conjecture is that the reason behind the language sensitivity of acoustic encoders is that they can implicitly capture linguistic information. To prove this guess, we use neutral voice to synthesize audio, eliminating the interference of emotion-related paralinguistic information. To generate audio in the target language, we use ChatGPT 3.5 for translation and the TTS system for audio generation. Experimental results are shown in Table  7 . In this table, we test the performance of acoustic encoders trained in different languages. We observe that language-matching encoders consistently achieve better performance. These results show that the acoustic encoder can implicitly capture linguistic information, leading to its language sensitivity. Fig.  5  further reveals the relationship between the major training language of the acoustic encoder and the input language. Although language-matching encoders generally result in better performance, there are some exceptions. The reason lies in that audio conveys emotions through linguistic and paralinguistic information. Although languagematching encoders can capture more linguistic information, good encoders can capture more emotion-related paralinguistic information and achieve better performance.\n\nIn summary, acoustic encoders can capture both linguistic and paralinguistic information. To capture linguistic information, it is best to use a language-matching encoder. To capture paralinguistic information, it is best to train the encoder with more expressive audio rather than just neutral audio. If you want a multilingual universal acoustic encoder, we recommend that the encoder should be trained on large amounts of expressive multilingual audio.\n\nLexical Modality. Different lexical encoders support distinct languages. In Table  5 , we focus on lexical encoders that support Chinese (LLMs are generally trained on multilingual corpora containing Chinese). For English emotion datasets, we use ChatGPT 3.5 to translate them into Chinese. We observe some interesting results in Table  5 . For emotion recognition, Baichuan-13B can achieve very promising results, but some powerful LLMs (such as Llama-13B and OPT-13B) perform poorly. To figure out the reason behind this phenomenon, we further investigate the impact of language matching on lexical encoders. Table  8  evaluates lexical encoders that support multiple languages, but each encoder has distinct major languages. In this table, we exploit ChatGPT 3.5 to translate the input text into the target language. For Baichuan-13B and BLOOM-7B, despite powerful translation systems like ChatGPT 3.5, emotion-related information is still lost during translation, resulting in performance degradation. Differently, Llama2-13B demonstrates superior results when translating MER-MULTI from Chinese to English. The reason lies in that although Llama2-13B supports Chinese, its primary training language is English, allowing it to better comprehend English. Therefore, we should focus on the primary language of lexical encoders, preferably aligning it with the input language and avoiding the use of translation systems. Fig.  5 . Impact of language matching for acoustic encoders. In this table, we reveal the relationship between the primary training language of the acoustic encoder and the input language.\n\nTABLE 8 Importance of language matching for lexical encoders. We choose encoders that support multiple languages but have different major languages.\n\nWe use ChatGPT 3.5 to translate the input into the target language. † denotes the result using translation and bold indicates the best result.",
      "page_start": 8,
      "page_end": 10
    },
    {
      "section_name": "Feature",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Multimodal Benchmark",
      "text": "In this section, we evaluate mainstream fusion strategies, including utterance-level algorithms (e.g., TFN  [22] , LMF  [67] , MISA  [68] , MMIM  [69] , Attention in Section 4.3) and sequence-level algorithms (MFN  [70] , GMFN  [23] , MCTN  [71] , MFM  [72] , MulT  [73] ). Since the choice of unimodal features seriously affects multimodal results, we compare the performance of different fusion algorithms under two feature sets: a medium-performance set (SENet-FER2013, wav2vec 2.0-base, DeBERTa-large) and a high-performance set (CLIP-large, HUBERT-large, Baichuan-13B).\n\nIn Table  9 , different combinations of features and datasets prefer distinct fusion algorithms. These results show the necessity of each algorithm and we should choose the best one for each combination. According to the overall results, MulT  [73]  is the best sequence-level algorithm. The reason lies in that MulT does not rely on hard alignment but uses Transformer to capture dynamic alignment, which can exploit long-term cross-modal interactions and achieve better performance. Furthermore, the attention mechanism achieves the best overall results, indicating that complex fusion algorithms can easily cause overfitting due to the small size of emotional datasets. The simple but effective attention mechanism can achieve relatively good performance among all algorithms. Therefore, we use the attention mechanism as the default fusion strategy in the following experiments.\n\nTable  10  illustrates the impact of feature number in multimodal fusion. Specifically, \"Top2\" denotes the selection of the top-2 features for each modality (i.e., MANet-RAFDB, CLIP-large, Whisper-large, HUBERT-large, RoBERTa-large, and Baichuan-13B). From Table  10 , we observe that increasing the number of features generally enhances the results. However, incorporating poorly-performing features can also have a detrimental effect. Therefore, it is necessary to adjust the feature number for each dataset.\n\nFig.  6  shows unimodal and multimodal results for each dataset. Interestingly, different datasets convey emotions in distinct ways. For example, MER-MULTI mainly conveys emotions through audio, while CMU-MOSI relies more on textual cues for emotional expression. In other words, although multimodal datasets provide information across all modalities, they may not be equally suitable for unimodal research. For example, it is not suitable to conduct visual emotion recognition on IEMOCAP as it conveys fewer emotions through video. Similarly, some datasets are not suitable for multimodal fusion research. For example, CMU-MOSI, CMU-MOSEI, and MELD heavily emphasize the lexical modality, making it challenging for advanced fusion algorithms to showcase their advantages.\n\nIn summary, the choice of unimodal features significantly impacts multimodal results. Hence, different fusion algorithms should be compared under the same feature  Fig.  6 . Modality preference analysis. Here, \"V\", \"A\", \"L\", and \"M\" represent visual, acoustic, lexical, and multimodal results, respectively.\n\nset. Although the attention mechanism may not consistently yield the optimal result, it can achieve relatively good performance among all algorithms. Additionally, not all multimodal emotion datasets are equally suitable for research on unimodal emotion recognition and multimodal fusion.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Cross-Corpus Benchmark",
      "text": "This section concentrates on cross-corpus performance.\n\nGiven that IEMOCAP(four), MELD, and MER-MULTI share four discrete labels (i.e., happiness, sadness, anger, neutral), cross-corpus experiments are conducted based on these labels. Due to sample overlap between IEMOCAP(four) and IEMOCAP(six), this section only focuses on IEMO-CAP(four). CMU-MOSI, CMU-MOSEI, CH-SIMS, and CH-SIMS v2 provide continuous sentiment scores, but the labels of the first two datasets are in the range [-3, 3], while the labels of the latter two datasets are in the range [-1, 1]. Therefore, we normalize these labels before conducting experiments. Tables 19∼20 (in Appendix) provide the crosscorpus results. For a clearer presentation, we further summarize these results in the following analysis.\n\nFig.  7  presents cross-corpus results using unimodal features. We select two features for each modality: one performing well and one performing poorly. Specifically, visual features (ResNet-MSCeleb and CLIP-large), acoustic features (VGGish and HUBERT-large), and lexical features (OPT-13B and Baichuan-13B) are evaluated. In Fig.  7 , good withincorpus features generally lead to good cross-corpus results. Therefore, a straightforward idea is to leverage multimodal fusion to enhance cross-corpus performance. (m) IEMOCAP→MELD (n) IEMOCAP→MER (o) MELD→IEMOCAP (p) MELD→MER (q) MER→IEMOCAP (r) MER→MELD Fig.  7 . Cross-corpus results using unimodal features. \"V\", \"A\", and \"L\" represent visual, acoustic, and lexical features. \"(-)\" and \"(+)\" denote poorlyand well-performing features in the within-corpus setup. We observe that good within-corpus features generally lead to good cross-corpus results. corpus setup but 4 labels in the cross-corpus setup, their results are excluded. Meanwhile, due to sample overlap, the cross-corpus results between CMU-MOSEI and CMU-MOSI (CH-SIMS and CH-SIMS v2) are also excluded. In general, multimodal fusion can improve both within-and crosscorpus results. However, its utility is relatively lower in the cross-corpus setting. The reason is that different datasets convey emotions in distinct ways (see Fig.  6 ). Multimodal fusion may cause the model to overfit the target dataset, resulting in limited improvements in the cross-corpus setup. Fig.  8  shows the performance gap between within-and cross-corpus results, highlighting the challenges associated with the cross-corpus setup. Meanwhile, we evaluate the emotion recognition performance of GPT-4V in Table  12 .  To reduce the cost of API calls, we only evaluate MER-MULTI, CMU-MOSI, and CH-SIMS  [90] . Although GPT-4V performs worse than within-corpus results, it performs close to or even better than cross-corpus results. This suggests that multimodal emotion recognition is a complex task that requires integrating diverse knowledge like GPT-4V, including unimodal emotion recognition and some background knowledge (e.g., dark environments may induce negative emotions). Additionally, as suggested in our previous work  [91] , it is better to use explainable reasoning processes as labels to enhance consistency and reduce subjectivity in emotion annotations. More uniform and reliable labels contribute to improved cross-corpus performance.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Robustness To Punctuation",
      "text": "Punctuation can also convey emotions. For example, the exclamation mark can express surprise or excitement, and the question mark can express confusion. In this section, we evaluate the punctuation robustness of different lexical encoders. We classify all punctuation into three categories: emotion-related punctuation (i.e., exclamation mark, question mark, ellipsis), pause-related punctuation (i.e., comma), and other punctuation. Table  13  reveals the impact of different punctuation on emotion recognition. Experimental results demonstrate that removing punctuation generally results in a performance drop, but this drop is often not noticeable (except for MELD). These results indicate that the lexical encoder is somewhat robust to missing punctuation.\n\nTo explain why MELD is sensitive to missing punctuation, we calculate the average number of emotion-related punctuation per sample for different datasets (see Fig.  9 ). We observe that MELD has more emotion-related punctuation than other datasets, possibly because the subtitles of this dataset are collected from original Friends TV scripts rather than ASR outputs. By further conducting case studies, we observe that many samples express emotions primarily through punctuation. For example, \"Hi Joey! What are you doing here?\" expresses joy, while \"Rachel!\" conveys angry  [24] ,  [86] . Therefore, although the lexical encoder is somewhat robust to missing punctuation, we still need to consider punctuation for some punctuation-rich datasets. From another perspective, if we can predict detailed punctuation using ASR, we can better understand emotional states.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Robustness To Additive Noise",
      "text": "This section assesses the noise robustness of acoustic encoders. In the experiments, we train the model on a clean corpus and evaluate its performance on a noisy dataset. We focus on additive noise and choose the SNR from {5dB, 10dB, 15dB}. Experimental results are shown in Table  14 . From these results, we observe that emotion recognition performance degrades as additive noise becomes more intense, highlighting the challenges in noisy conditions. Although eGeMAPS exhibits the smallest decrease among all features, its overall performance is relatively poor.\n\nMeanwhile, features performing well on clean corpora generally exhibit good results under noisy conditions (see Table  14 ). This suggests that noise robustness can be enhanced by using more powerful acoustic encoders. However, there are some exceptions to this trend. Although Whisper-large performs worse than HUBERT-large on clean corpora, it achieves better performance under noisy conditions. This is attributed to the fact that Whisper-large is trained on a highly diverse dataset, encompassing a broad distribution of audio from various environments, recording setups, speakers, and languages  [92] . These findings indicate a pathway to build more robust acoustic encoders by increasing the diversity of audio quality during training. Table  15  reveals the role of data augmentation under noisy conditions. We choose Whisper-large as the acoustic encoder due to its noise robustness, and report results under various training and test SNR combinations. In noisy conditions, data augmentation consistently enhances performance compared to models trained on clean data. Additionally, using the same training and test SNR generally results in improved performance, except when the training SNR is 5dB. This suggests that low training SNR may impair the model's ability to recognize emotions. Therefore, we should choose an appropriate SNR for data augmentation.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Necessity Of Fine-Tuning",
      "text": "The above experiments primarily utilize pre-trained models as feature extractors and then use shallow classifiers for emotion recognition. In other words, we freeze the weights of pre-trained models and only optimize the shallow classifiers. However, some recent studies have highlighted the effectiveness of jointly optimizing feature extractors and classifiers  [93] . Therefore, we further investigate the role of fine-tuning the pre-trained feature extractor.\n\nIn Table  16 , we select two representative feature encoders for each modality and study the effect of fine-tuning. Experimental results show that fine-tuning performs differently on distinct feature and dataset combinations. The reason may be twofold. On the one hand, fine-tuning improves performance when the encoder is incompatible with the emotion dataset. On the other hand, due to the small size of emotion datasets, fine-tuning increases the trainable parameters, causing the model to overfit training data and perform poorly on unseen data. Therefore, we should consider the trade-off between compatibility and overfitting.\n\nTable  17  provides a comprehensive comparison of computational complexity with and without fine-tuning. We observe that fine-tuning increases the MACs and trainable parameters, resulting in longer training times. In practice, fine-tuning is required for each downstream dataset, which further increases the computational cost. In contrast, pretraining only needs to be conducted once, and we can freeze the pre-trained weights and optimize the shallow classifier, which is more computationally efficient. Moreover, according to the results in Table  6 , the improvement of finetuning is smaller than that of pre-training. Therefore, for tasks with limited samples (such as emotion recognition), we recommend using pre-training instead of fine-tuning.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Conclusions",
      "text": "This paper builds MERBench, a unified evaluation benchmark for multimodal emotion recognition. We aim to reveal the role of some key techniques in this field and point out future research directions: (1) For visual encoders, we note the advantages of weakly-supervised and selfsupervised models. To obtain better results, it is necessary to further train them on human-centric videos to reduce the domain gap; (2) Lexical encoders are language-sensitive. Although we can translate the source language into the target language, this translation process often causes the loss of emotion-related information; (3) Acoustic encoders can implicitly capture language-sensitive linguistic information from audio, resulting in their language sensitivity. To obtain more powerful and universal acoustic encoders, we suggest training acoustic encoders on expressive multilingual audio; (4) For multimodal fusion, the attention mechanism can achieve relatively good performance among all fusion algorithms. Meanwhile, not all datasets are suitable for multimodal fusion research, as some datasets primarily convey emotions through a single modality; (5) For cross-corpus settings, more powerful within-corpus models generally achieve better cross-corpus results. But to truly solve the cross-corpus problem, we should combine knowledge from different tasks and use explainable reasoning processes to increase label consistency across different datasets; (6) Lexical encoders are somewhat robust to missing punctuation, but they cannot handle missing cases in punctuation-rich samples. From another perspective, it will help us to recognize emotions if we can predict detailed punctuation from audio;  (7)  To improve noise robustness of acoustic encoders, we should increase the diversity of audio quality during training. At the same time, data augmentation can effectively handle noisy data, but an appropriate SNR should be selected; (8) Fine-tuning requires more computational costs but achieves less improvement than pre-training. Therefore, we recommend that follow-up researchers pay more attention to pre-training, especially pre-training on datasets that are compatible with downstream tasks. Furthermore, this paper introduces a new dataset MER2023, aiming to provide a benchmark dataset for research on multi-label learning, noise robustness, and semi-supervised learning.\n\nBeyond multimodal emotion recognition, we plan to incorporate more emotion-related tasks into our benchmark, such as stress, humor, sarcasm, and depression detection. Additionally, we aim to broaden the evaluation scope by including more features and multimodal fusion strategies. We hope that MERBench can provide guidance for developing robust and powerful emotion recognition systems. For models without fine-tuning, we freeze the pre-trained feature encoders and only optimize shallow classifiers. Thus, we calculate the computational complexity on the classifiers. For models with fine-tuning, both pre-trained models and shallow classifiers are optimized. Thus, we calculate the computational complexity for all trainable parameters.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "Feature",
      "text": "",
      "page_start": 15,
      "page_end": 15
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows the entire annotation pipeline.",
      "page": 3
    },
    {
      "caption": "Figure 1: Pipeline of data annotation.",
      "page": 4
    },
    {
      "caption": "Figure 2: Empirical PDFs and estimated Gaussian models on sample",
      "page": 4
    },
    {
      "caption": "Figure 3: Distribution of discrete emotions for different subsets (neutral,",
      "page": 4
    },
    {
      "caption": "Figure 1: ). The reliable samples",
      "page": 4
    },
    {
      "caption": "Figure 2: and Fig. 3. We observe that most",
      "page": 4
    },
    {
      "caption": "Figure 4: Empirical PDF on the valence for different discrete emotions. We",
      "page": 4
    },
    {
      "caption": "Figure 4: Valence represents the degree",
      "page": 4
    },
    {
      "caption": "Figure 3: ), we choose the weighted average F-score (WAF) as the",
      "page": 7
    },
    {
      "caption": "Figure 4: Compared with",
      "page": 7
    },
    {
      "caption": "Figure 5: further reveals the relationship between the major",
      "page": 9
    },
    {
      "caption": "Figure 5: Impact of language matching for acoustic encoders. In this table, we reveal the relationship between the primary training language of the",
      "page": 10
    },
    {
      "caption": "Figure 6: shows unimodal and multimodal results for each",
      "page": 10
    },
    {
      "caption": "Figure 6: Modality preference analysis. Here, “V”, “A”, “L”, and “M” repre-",
      "page": 11
    },
    {
      "caption": "Figure 7: presents cross-corpus results using unimodal fea-",
      "page": 11
    },
    {
      "caption": "Figure 7: , good within-",
      "page": 11
    },
    {
      "caption": "Figure 7: Cross-corpus results using unimodal features. “V”, “A”, and “L” represent visual, acoustic, and lexical features. “(-)” and “(+)” denote poorly-",
      "page": 12
    },
    {
      "caption": "Figure 6: ). Multimodal",
      "page": 12
    },
    {
      "caption": "Figure 8: shows the performance gap between within- and",
      "page": 12
    },
    {
      "caption": "Figure 8: Performance gap between cross-corpus results (“c”) and within-",
      "page": 12
    },
    {
      "caption": "Figure 9: Average number of emotion-related punctuation per sample.",
      "page": 13
    }
  ],
  "tables": [
    {
      "caption": "Table 6: , we choose a self-supervised good encoders can capture more emotion-related paralin-",
      "data": [
        {
          "Encoder (Lang.)\nTTS Lang. MER-MULTI": "data2vec-base\nEN\n(EN)\nCH",
          "IEMOCAP\nIEMOCAP\nCMU-MOSI\nCMU-MOSEI\nCH-SIMS\nCH-SIMS v2\nMELD\n(four-class)\n(six-class)": "35.03±0.10\n69.53±0.44\n72.82±0.28\n65.23±0.68\n64.13±0.15\n46.14±0.27\n59.77±0.27\n45.67±0.13\n28.61±0.40\n60.54±0.71\n66.40±0.18\n62.24±0.52\n58.89±0.52\n42.01±0.53\n48.01±0.61\n36.33±0.15"
        },
        {
          "Encoder (Lang.)\nTTS Lang. MER-MULTI": "WavLM-large\nEN\n(MULTI, mainly EN)\nCH",
          "IEMOCAP\nIEMOCAP\nCMU-MOSI\nCMU-MOSEI\nCH-SIMS\nCH-SIMS v2\nMELD\n(four-class)\n(six-class)": "39.21±0.59\n71.52±0.39\n77.23±0.27\n67.78±0.28\n67.27±0.36\n47.60±0.08\n60.53±0.23\n47.25±0.14\n31.33±0.24\n61.99±0.37\n68.38±0.36\n63.84±0.46\n61.08±0.45\n43.03±0.28\n51.97±0.19\n39.85±0.24"
        },
        {
          "Encoder (Lang.)\nTTS Lang. MER-MULTI": "HUBERT-large\nEN\n(CH)\nCH",
          "IEMOCAP\nIEMOCAP\nCMU-MOSI\nCMU-MOSEI\nCH-SIMS\nCH-SIMS v2\nMELD\n(four-class)\n(six-class)": "29.52±0.35\n64.80±0.56\n68.99±0.14\n62.03±0.56\n55.89±0.41\n42.57±0.36\n50.77±0.48\n39.13±0.20\n42.82±0.38\n70.16±0.51\n77.22±0.29\n72.78±0.41\n73.47±0.23\n47.88±0.31\n58.70±0.31\n44.99±0.12"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 10: , we observe that in-",
      "data": [
        {
          "Major Lang.\nFeature\nTrain\nTest": "Baichuan-13B\nEN, CH\nEN\nBaichuan-13B\nEN, CH\nCH",
          "IEMOCAP\nIEMOCAP\nMER-MULTI\nCMU-MOSI\nCMU-MOSEI\nCH-SIMS\nCH-SIMS v2\nMELD\n(four-class)\n(six-class)": "82.31±0.58\n85.09±0.20\n62.02±0.16\n69.43±0.14\n54.45±0.21\n55.56±0.35†\n76.41±0.29†\n78.59±0.35†\n62.54±0.69\n81.99±0.50\n80.93±0.19\n79.37±0.39†\n84.42±0.05†\n58.27±0.08†\n67.01±0.14†\n51.70±0.13†"
        },
        {
          "Major Lang.\nFeature\nTrain\nTest": "BLOOM-7B\nMULTI\nEN\nBLOOM-7B\nMULTI\nCH",
          "IEMOCAP\nIEMOCAP\nMER-MULTI\nCMU-MOSI\nCMU-MOSEI\nCH-SIMS\nCH-SIMS v2\nMELD\n(four-class)\n(six-class)": "79.72±0.22\n84.37±0.14\n61.85±0.28\n69.32±0.06\n54.20±0.12\n53.35±0.80†\n78.13±0.47†\n79.23±0.24†\n59.23±0.37\n82.53±0.27\n80.64±0.12\n79.72±0.30†\n83.90±0.24†\n59.42±0.32†\n66.76±0.12†\n51.06±0.12†"
        },
        {
          "Major Lang.\nFeature\nTrain\nTest": "Llama2-13B\nEN\nEN\nLlama2-13B\nEN\nCH",
          "IEMOCAP\nIEMOCAP\nMER-MULTI\nCMU-MOSI\nCMU-MOSEI\nCH-SIMS\nCH-SIMS v2\nMELD\n(four-class)\n(six-class)": "53.77±0.29†\n81.81±0.30\n85.29±0.14\n77.75±0.32†\n78.42±0.23†\n62.23±0.29\n70.28±0.11\n54.64±0.16\n51.63±0.43\n78.62±0.16†\n83.62±0.09†\n75.46±0.49\n75.37±0.20\n58.09±0.12†\n65.74±0.14†\n50.75±0.06†"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 11: evaluates the impact of multimodal fusion on",
      "data": [
        {
          "# Top": "Top1\nTop2\nTop3\nTop4\nTop5\nTop6",
          "IEMOCAP\nIEMOCAP\nMER-MULTI\nCMU-MOSI\nCMU-MOSEI\nCH-SIMS\nCH-SIMS v2\nMELD\n(four-class)\n(six-class)": "83.03±0.16\n79.40±0.20\n85.05±0.23\n90.50±0.36\n88.48±0.32\n59.41±0.26\n78.20±0.12\n60.90±0.22\n84.46±0.68\n85.83±0.10\n91.57±0.43\n88.58±0.33\n82.84±0.27\n60.19±0.08\n78.58±0.04\n60.75±0.16\n84.75±0.26\n88.78±0.41\n79.14±0.15\n83.33±0.34\n85.46±0.16\n91.10±0.60\n60.23±0.29\n61.53±0.12\n84.65±0.47\n83.58±0.31\n91.75±0.29\n61.48±0.39\n61.59±0.27\n85.71±0.13\n88.23±0.36\n78.83±0.06\n85.96±0.25\n86.43±0.05\n91.17±0.51\n61.90±0.23\n79.11±0.07\n61.84±0.11\n83.53±0.35\n87.94±0.47\n85.94±0.29\n86.80±0.12\n88.57±0.32\n62.21±0.32\n79.15±0.06\n61.80±0.19\n83.85±0.12\n91.06±0.27",
          "Mean": "78.12\n79.10\n79.29\n79.48\n79.73\n79.92"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 12: contributetoimprovedcross-corpusperformance.",
      "data": [
        {
          "Source\nTarget": "MELD\nIEMOCAP(4)\nMER-MULTI\nIEMOCAP(4)\nIEMOCAP(4)\nIEMOCAP(4)",
          "∆\nUnimodal\nMultimodal": "46.62±0.98\n48.86±0.75\n↑2.24\n50.69±0.16\n59.58±1.10\n↑8.89\n69.67±0.15\n78.20±0.12\n↑8.53"
        },
        {
          "Source\nTarget": "CH-SIMS\nCMU-MOSI\nCH-SIMS v2\nCMU-MOSI\nCMU-MOSI\nCMU-MOSI",
          "∆\nUnimodal\nMultimodal": "70.15±0.71\n70.86±1.32\n↑0.71\n74.22±0.21\n73.85±1.17\n↓0.37\n79.37±0.39\n79.40±0.20\n↑0.03"
        },
        {
          "Source\nTarget": "CH-SIMS\nCMU-MOSEI\nCH-SIMS v2\nCMU-MOSEI\nCMU-MOSEI\nCMU-MOSEI",
          "∆\nUnimodal\nMultimodal": "73.02±0.69\n72.20±0.45\n↓0.82\n75.28±0.32\n75.19±0.90\n↓0.09\n84.42±0.05\n85.05±0.23\n↑0.63"
        },
        {
          "Source\nTarget": "CMU-MOSI\nCH-SIMS\nCMU-MOSEI\nCH-SIMS\nCH-SIMS\nCH-SIMS",
          "∆\nUnimodal\nMultimodal": "75.03±0.87\n76.55±0.83\n↑1.52\n75.57±0.73\n81.64±1.32\n↑6.07\n82.62±0.31\n90.50±0.36\n↑7.88"
        },
        {
          "Source\nTarget": "CMU-MOSI\nCH-SIMS v2\nCMU-MOSEI\nCH-SIMS v2\nCH-SIMS v2\nCH-SIMS v2",
          "∆\nUnimodal\nMultimodal": "75.80±0.32\n75.74±0.56\n↓0.06\n77.43±0.35\n81.18±1.03\n↑3.75\n80.93±0.19\n88.48±0.32\n↑7.55"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table 13: reveals the impact of dif-",
      "data": [
        {
          "Feature\nE\nP": "× × ×\nOPT-13B\n√\n× ×\nOPT-13B\n√ √\n×\nOPT-13B\n√ √ √\nOPT-13B",
          "IEMOCAP\nIEMOCAP\nO MER-MULTI\nCMU-MOSI\nCMU-MOSEI\nCH-SIMS\nCH-SIMS v2\nMELD\n(four-class)\n(six-class)": "46.33 (↑0.55)\n73.69 (↓0.87)\n79.75 (↑0.05)\n73.53 (↑0.98)\n69.94 (↓0.18)\n48.05 (↓9.44)\n61.71 (↑0.01)\n48.08 (↑0.03)\n45.63 (↓0.15)\n73.99 (↓0.57)\n79.62 (↓0.08)\n74.26 (↑1.71)\n70.90 (↑0.78)\n49.48 (↓8.01)\n62.07 (↑0.37)\n48.27 (↑0.22)\n45.12 (↓0.66)\n74.09 (↓0.47)\n80.11 (↑0.41)\n73.02 (↑0.47)\n70.19 (↑0.07)\n49.55 (↓7.94)\n61.80 (↑0.10)\n48.01 (↓0.04)\n45.78\n74.56\n79.70\n72.55\n70.12\n57.49\n61.70\n48.05"
        },
        {
          "Feature\nE\nP": "× × ×\nALBERT-small\n√\n× ×\nALBERT-small\n√ √\n×\nALBERT-small\n√ √ √\nALBERT-small",
          "IEMOCAP\nIEMOCAP\nO MER-MULTI\nCMU-MOSI\nCMU-MOSEI\nCH-SIMS\nCH-SIMS v2\nMELD\n(four-class)\n(six-class)": "45.90 (↓0.53)\n74.31 (↑0.15)\n79.44 (↓0.06)\n72.33 (↓0.94)\n71.64 (↓0.66)\n47.24 (↓9.18)\n61.15 (↑0.01)\n47.08 (↓0.35)\n46.46 (↑0.03)\n73.38 (↓0.78)\n79.67 (↑0.17)\n74.00 (↑0.73)\n71.65 (↓0.65)\n48.07 (↓8.35)\n61.00 (↓0.14)\n47.34 (↓0.09)\n45.74 (↓0.69)\n73.92 (↓0.24)\n79.75 (↑0.25)\n74.58 (↑1.31)\n72.14 (↓0.16)\n48.12 (↓8.30)\n61.47 (↑0.33)\n47.37 (↓0.06)\n46.43\n74.16\n79.50\n73.27\n72.30\n56.42\n61.14\n47.43"
        },
        {
          "Feature\nE\nP": "× × ×\nLlama2-13B\n√\n× ×\nLlama2-13B\n√ √\n×\nLlama2-13B\n√ √ √\nLlama2-13B",
          "IEMOCAP\nIEMOCAP\nO MER-MULTI\nCMU-MOSI\nCMU-MOSEI\nCH-SIMS\nCH-SIMS v2\nMELD\n(four-class)\n(six-class)": "50.39 (↓1.24)\n77.35 (↓1.27)\n82.86 (↓0.76)\n73.74 (↓1.72)\n73.73 (↓1.64)\n49.42 (↓8.67)\n64.58 (↓1.16)\n50.05 (↓0.70)\n51.05 (↓0.58)\n78.37 (↓0.25)\n83.23 (↓0.39)\n73.79 (↓1.67)\n74.36 (↓1.01)\n50.76 (↓7.33)\n65.14 (↓0.60)\n50.73 (↓0.02)\n51.70 (↑0.07)\n79.16 (↑0.54)\n83.52 (↓0.10)\n75.36 (↓0.10)\n75.33 (↓0.04)\n50.96 (↓7.13)\n65.48 (↓0.26)\n50.80 (↑0.05)\n51.63\n78.62\n83.62\n75.46\n75.37\n58.09\n65.74\n50.75"
        },
        {
          "Feature\nE\nP": "× × ×\nPERT-base\n√\n× ×\nPERT-base\n√ √\n×\nPERT-base\n√ √ √\nPERT-base",
          "IEMOCAP\nIEMOCAP\nO MER-MULTI\nCMU-MOSI\nCMU-MOSEI\nCH-SIMS\nCH-SIMS v2\nMELD\n(four-class)\n(six-class)": "53.61 (↑0.86)\n80.61 (↓0.58)\n83.05 (↓0.31)\n78.34 (↑0.32)\n78.01 (↑0.25)\n49.72 (↓7.48)\n64.42 (↓0.31)\n49.53 (↑0.09)\n53.29 (↑0.54)\n81.81 (↑0.62)\n83.29 (↓0.07)\n77.60 (↓0.42)\n77.59 (↓0.17)\n50.68 (↓6.52)\n64.48 (↓0.25)\n49.46 (↑0.02)\n52.76 (↑0.01)\n81.47 (↑0.28)\n83.29 (↓0.07)\n77.57 (↓0.45)\n77.41 (↓0.35)\n50.90 (↓6.30)\n64.62 (↓0.11)\n49.53 (↑0.09)\n52.75\n81.19\n83.36\n78.02\n77.76\n57.20\n64.73\n49.44"
        },
        {
          "Feature\nE\nP": "RoBERTa-large × × ×\n√\nRoBERTa-large × ×\n√ √\nRoBERTa-large ×\n√ √ √\nRoBERTa-large",
          "IEMOCAP\nIEMOCAP\nO MER-MULTI\nCMU-MOSI\nCMU-MOSEI\nCH-SIMS\nCH-SIMS v2\nMELD\n(four-class)\n(six-class)": "57.40 (↓0.82)\n82.87 (↓0.30)\n83.82 (↓0.29)\n81.83 (↓0.73)\n79.72 (↓0.32)\n50.80 (↓8.40)\n66.12 (↓0.09)\n51.14 (↑0.16)\n57.32 (↓0.90)\n83.04 (↓0.13)\n84.03 (↓0.08)\n81.60 (↓0.96)\n79.55 (↓0.49)\n51.10 (↓8.10)\n66.23 (↑0.02)\n51.17 (↑0.19)\n58.20 (↓0.02)\n83.30 (↑0.13)\n84.09 (↓0.02)\n82.26 (↓0.30)\n80.16 (↑0.12)\n51.20 (↓8.00)\n66.22 (↑0.01)\n51.00 (↑0.02)\n58.22\n83.17\n84.11\n82.56\n80.04\n59.20\n66.21\n50.98"
        },
        {
          "Feature\nE\nP": "× × ×\nBaichuan-13B\n√\n× ×\nBaichuan-13B\n√ √\n×\nBaichuan-13B\n√ √ √\nBaichuan-13B",
          "IEMOCAP\nIEMOCAP\nO MER-MULTI\nCMU-MOSI\nCMU-MOSEI\nCH-SIMS\nCH-SIMS v2\nMELD\n(four-class)\n(six-class)": "59.65 (↓2.89)\n78.20 (↓1.17)\n83.77 (↓0.65)\n79.95 (↓2.04)\n79.44 (↓1.49)\n50.32 (↓7.95)\n66.28 (↓0.73)\n51.39 (↓0.31)\n60.64 (↓1.90)\n79.30 (↓0.07)\n84.45 (↑0.03)\n81.53 (↓0.46)\n79.70 (↓1.23)\n51.47 (↓6.80)\n66.98 (↓0.03)\n52.03 (↑0.33)\n62.00 (↓0.54)\n79.36 (↓0.01)\n84.36 (↓0.06)\n81.65 (↓0.34)\n81.06 (↑0.13)\n51.45 (↓6.82)\n66.82 (↓0.19)\n51.73 (↑0.03)\n62.54\n79.37\n84.42\n81.99\n80.93\n58.27\n67.01\n51.70"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table 6: , the improvement of fine-",
      "data": [
        {
          "Feature": "eGeMAPS\ndata2vec-base\nwav2vec 2.0-large\nWavLM-large\nWhisper-large\nHUBERT-large",
          "Test SNR\n5dB\n10dB\n15dB\nClean": "44.20(↓1.65)\n44.39(↓1.46)\n45.16(↓0.69)\n45.85\n48.01(↓10.2)\n50.65(↓7.59)\n53.33(↓4.91)\n58.24\n51.96(↓7.02)\n54.53(↓4.45)\n56.23(↓2.75)\n58.98\n53.19(↓11.9)\n57.27(↓7.83)\n59.23(↓5.87)\n65.10\n57.92(↓7.27)\n61.03(↓4.16)\n62.90(↓2.29)\n65.19\n66.62\n58.02(↓8.60)\n60.79(↓5.83)\n61.76(↓4.86)"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Feature\nFinetune": "×\nVideoMAE-base\n√\nVideoMAE-base\n∆\nVideoMAE-base",
          "IEMOCAP\nIEMOCAP\nMER-MULTI\nCMU-MOSI\nCMU-MOSEI\nCH-SIMS\nCH-SIMS v2\nMELD\n(four-class)\n(six-class)": "46.72±0.75\n56.97±0.64\n70.40±0.05\n68.95±0.69\n70.11±0.26\n36.74±0.20\n39.15±0.18\n26.76±0.17\n51.20±0.01\n56.17±0.71\n69.79±0.07\n75.38±1.27\n72.31±0.59\n34.50±0.06\n41.45±0.23\n28.74±0.24\n↑4.48\n↓0.80\n↓0.61\n↑6.43\n↑2.20\n↓2.24\n↑2.30\n↑1.98"
        },
        {
          "Feature\nFinetune": "×\nCLIP-base\n√\nCLIP-base\n∆\nCLIP-base",
          "IEMOCAP\nIEMOCAP\nMER-MULTI\nCMU-MOSI\nCMU-MOSEI\nCH-SIMS\nCH-SIMS v2\nMELD\n(four-class)\n(six-class)": "60.12±0.85\n55.04±0.97\n70.24±0.07\n80.13±0.11\n77.97±0.19\n38.59±0.33\n44.52±0.21\n28.70±0.08\n57.07±0.09\n56.69±1.49\n69.80±1.27\n79.83±0.58\n78.52±0.16\n36.54±0.27\n43.41±0.56\n26.98±1.35\n↓3.05\n↑1.65\n↓0.44\n↓0.30\n↑0.55\n↓2.05\n↓1.11\n↓1.72"
        },
        {
          "Feature\nFinetune": "×\ndata2vec-base\n√\ndata2vec-base\n∆\ndata2vec-base",
          "IEMOCAP\nIEMOCAP\nMER-MULTI\nCMU-MOSI\nCMU-MOSEI\nCH-SIMS\nCH-SIMS v2\nMELD\n(four-class)\n(six-class)": "45.13±0.39\n66.24±0.45\n73.11±0.12\n65.00±0.30\n59.87±0.57\n44.30±0.34\n64.08±0.12\n48.22±0.28\n46.72±0.48\n62.30±0.33\n76.47±0.04\n68.37±0.08\n66.93±0.29\n43.86±0.02\n66.93±0.10\n51.27±0.35\n↑1.59\n↓3.94\n↑3.36\n↑3.37\n↑7.06\n↓0.44\n↑2.85\n↑3.05"
        },
        {
          "Feature\nFinetune": "×\nHUBERT-base\n√\nHUBERT-base\n∆\nHUBERT-base",
          "IEMOCAP\nIEMOCAP\nMER-MULTI\nCMU-MOSI\nCMU-MOSEI\nCH-SIMS\nCH-SIMS v2\nMELD\n(four-class)\n(six-class)": "67.48±0.18\n58.05±0.51\n73.50±0.11\n78.72±0.21\n75.36±0.31\n48.16±0.21\n67.23±0.11\n51.83±0.19\n65.58±0.67\n62.88±0.15\n73.33±0.12\n78.27±0.55\n77.75±0.27\n46.94±0.20\n67.36±0.50\n51.21±0.10\n↓1.90\n↑4.83\n↓0.17\n↓0.45\n↑2.39\n↓1.22\n↑0.13\n↓0.62"
        },
        {
          "Feature\nFinetune": "×\nALBERT-small\n√\nALBERT-small\n∆\nALBERT-small",
          "IEMOCAP\nIEMOCAP\nMER-MULTI\nCMU-MOSI\nCMU-MOSEI\nCH-SIMS\nCH-SIMS v2\nMELD\n(four-class)\n(six-class)": "46.43±0.29\n74.16±0.63\n79.50±0.24\n73.27±0.23\n72.30±0.22\n56.42±0.12\n61.14±0.14\n47.43±0.23\n44.52±1.28\n79.75±0.39\n81.92±0.17\n72.28±0.31\n72.02±0.11\n56.64±0.10\n62.73±0.19\n47.42±0.45\n↓1.91\n↑5.59\n↑2.42\n↓0.99\n↓0.28\n↑0.22\n↑1.59\n↓0.01"
        },
        {
          "Feature\nFinetune": "×\nMacBERT-base\n√\nMacBERT-base\n∆\nMacBERT-base",
          "IEMOCAP\nIEMOCAP\nMER-MULTI\nCMU-MOSI\nCMU-MOSEI\nCH-SIMS\nCH-SIMS v2\nMELD\n(four-class)\n(six-class)": "56.38±0.32\n83.89±0.19\n84.03±0.25\n80.24±0.26\n79.73±0.10\n57.04±0.14\n65.41±0.04\n50.18±0.08\n55.91±0.23\n84.47±0.16\n85.28±0.16\n80.22±0.13\n79.61±0.14\n58.34±0.31\n66.30±0.07\n51.09±0.08\n↓0.47\n↑0.58\n↑1.25\n↓0.02\n↓0.12\n↑1.30\n↑0.89\n↑0.91"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Feature\nFinetune": "×\nVideoMAE-base\n√\nVideoMAE-base",
          "MACs\n(G)": "0.02\n407.41",
          "# Params\n(M)": "0.48\n65.11",
          "Training Time\n(ms)": "0.03\n49.83"
        },
        {
          "Feature\nFinetune": "×\nCLIP-base\n√\nCLIP-base",
          "MACs\n(G)": "0.01\n1047.98",
          "# Params\n(M)": "0.38\n87.91",
          "Training Time\n(ms)": "0.02\n140.78"
        },
        {
          "Feature\nFinetune": "×\ndata2vec-base\n√\ndata2vec-base",
          "MACs\n(G)": "0.02\n878.92",
          "# Params\n(M)": "0.48\n93.30",
          "Training Time\n(ms)": "0.03\n26.52"
        },
        {
          "Feature\nFinetune": "×\nHUBERT-base\n√\nHUBERT-base",
          "MACs\n(G)": "0.02\n885.22",
          "# Params\n(M)": "0.48\n94.50",
          "Training Time\n(ms)": "0.03\n21.13"
        },
        {
          "Feature\nFinetune": "×\nALBERT-small\n√\nALBERT-small",
          "MACs\n(G)": "0.01\n21.55",
          "# Params\n(M)": "0.33\n2.06",
          "Training Time\n(ms)": "0.03\n1.12"
        },
        {
          "Feature\nFinetune": "×\nMacBERT-base\n√\nMacBERT-base",
          "MACs\n(G)": "0.02\n58.50",
          "# Params\n(M)": "0.48\n85.78",
          "Training Time\n(ms)": "0.02\n1.28"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Video": "",
          "ResNet-MSCeleb": "CLIP-large",
          "IEMOCAP(four)\nMELD\nMER-MULTI": "IEMOCAP(four)\nMELD\nMER-MULTI",
          "–\n41.87±0.36\n33.21±0.42\n14.91±0.42\n–\n13.12±0.64\n27.18±0.24\n11.18±0.39\n–": "–\n47.46±0.70\n54.96±0.35\n34.49±0.56\n–\n49.68±1.64\n37.22±0.79\n20.39±0.52\n–"
        }
      ],
      "page": 20
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Audio": "",
          "VGGish": "HUBERT-large",
          "IEMOCAP(four)\nMELD\nMER-MULTI": "IEMOCAP(four)\nMELD\nMER-MULTI",
          "–\n25.76±0.64\n39.81±0.74\n26.14±0.64\n–\n30.68±1.10\n40.66±0.09\n22.62±0.88\n–": "–\n19.16±4.03\n60.95±0.76\n46.62±0.98\n–\n43.62±1.12\n50.69±0.16\n47.83±0.19\n–"
        }
      ],
      "page": 20
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Text": "",
          "OPT-13B": "Baichuan-13B",
          "IEMOCAP(four)\nMELD\nMER-MULTI": "IEMOCAP(four)\nMELD\nMER-MULTI",
          "–\n47.40±0.16\n43.32±0.69\n35.27±0.40\n–\n42.30±0.50\n36.81±0.23\n44.65±0.75\n–": "–\n50.42±0.32\n54.05±0.54\n44.73±0.21\n–\n53.73±0.50\n43.37±0.55\n47.79±0.85\n–"
        }
      ],
      "page": 20
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Multi-modalities": "",
          "MISA": "MMIM",
          "IEMOCAP(four)\nMELD\nMER-MULTI": "IEMOCAP(four)\nMELD\nMER-MULTI",
          "–\n47.90±2.41\n69.60±0.93\n47.09±0.91\n–\n57.83±0.94\n58.00±0.36\n48.04±2.50\n–": "–\n42.40±2.44\n68.30±0.43\n47.77±0.78\n–\n60.16±2.09\n59.58±1.10\n48.15±1.13\n–"
        },
        {
          "Multi-modalities": "",
          "MISA": "LMF",
          "IEMOCAP(four)\nMELD\nMER-MULTI": "IEMOCAP(four)\nMELD\nMER-MULTI",
          "–\n47.90±2.41\n69.60±0.93\n47.09±0.91\n–\n57.83±0.94\n58.00±0.36\n48.04±2.50\n–": "–\n38.92±3.52\n70.15±0.44\n47.22±0.29\n–\n59.97±0.82\n57.57±0.54\n47.52±1.56\n–"
        },
        {
          "Multi-modalities": "",
          "MISA": "TFN",
          "IEMOCAP(four)\nMELD\nMER-MULTI": "IEMOCAP(four)\nMELD\nMER-MULTI",
          "–\n47.90±2.41\n69.60±0.93\n47.09±0.91\n–\n57.83±0.94\n58.00±0.36\n48.04±2.50\n–": "–\n37.34±1.52\n70.40±0.33\n48.57±0.43\n–\n61.86±0.29\n58.35±0.87\n45.90±2.15\n–"
        },
        {
          "Multi-modalities": "",
          "MISA": "Attention",
          "IEMOCAP(four)\nMELD\nMER-MULTI": "IEMOCAP(four)\nMELD\nMER-MULTI",
          "–\n47.90±2.41\n69.60±0.93\n47.09±0.91\n–\n57.83±0.94\n58.00±0.36\n48.04±2.50\n–": "–\n39.48±3.59\n69.09±0.26\n48.86±0.75\n–\n62.57±1.06\n58.59±0.43\n42.95±1.58\n–"
        }
      ],
      "page": 20
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Video": "",
          "ResNet-MSCeleb": "CLIP-large",
          "CMU-MOSI\nCMU-MOSEI\nCH-SIMS\nCH-SIMS v2": "CMU-MOSI\nCMU-MOSEI\nCH-SIMS\nCH-SIMS v2",
          "–\n56.64±0.28\n52.68±0.83\n50.72±0.21\n51.11±0.34\n–\n49.73±0.44\n53.62±1.69\n51.72±0.39\n58.02±0.14\n–\n64.31±0.42\n47.43±0.25\n57.97±0.14\n71.09±0.48\n–": "–\n58.80±0.30\n63.48±2.62\n60.44±2.05\n57.66±0.20\n–\n65.79±1.86\n67.38±1.32\n59.31±0.50\n53.86±1.14\n–\n78.74±0.09\n59.03±0.26\n51.73±2.73\n83.84±0.25\n–"
        }
      ],
      "page": 21
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Audio": "",
          "VGGish": "HUBERT-large",
          "CMU-MOSI\nCMU-MOSEI\nCH-SIMS\nCH-SIMS v2": "CMU-MOSI\nCMU-MOSEI\nCH-SIMS\nCH-SIMS v2",
          "–\n59.49±0.48\n58.13±0.26\n54.16±1.38\n52.65±0.22\n–\n22.49±0.35\n34.33±3.60\n51.20±0.17\n59.42±0.76\n–\n58.89±0.14\n55.79±0.49\n62.57±0.29\n62.67±0.32\n–": "–\n61.64±0.50\n56.49±1.97\n42.64±9.59\n61.17±0.50\n–\n59.25±1.83\n35.59±1.15\n54.17±1.24\n46.31±3.77\n–\n65.39±3.17\n54.53±1.91\n37.84±10.35\n78.90±0.27\n–"
        }
      ],
      "page": 21
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Text": "",
          "OPT-13B": "Baichuan-13B",
          "CMU-MOSI\nCMU-MOSEI\nCH-SIMS\nCH-SIMS v2": "CMU-MOSI\nCMU-MOSEI\nCH-SIMS\nCH-SIMS v2",
          "–\n68.11±0.55\n66.63±0.81\n65.62±0.45\n75.85±0.13\n–\n68.02±0.15\n68.57±0.36\n68.34±1.29\n67.11±0.53\n–\n68.75±0.34\n67.13±0.86\n68.61±0.17\n72.02±0.31\n–": "–\n74.87±0.44\n75.03±0.87\n75.80±0.32\n79.15±0.45\n–\n75.57±0.73\n77.43±0.35\n70.15±0.71\n73.02±0.69\n–\n79.80±0.29\n74.22±0.21\n75.28±0.32\n82.12±0.16\n–"
        }
      ],
      "page": 21
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Multi-modalities": "",
          "MISA": "MMIM",
          "CMU-MOSI\nCMU-MOSEI\nCH-SIMS\nCH-SIMS v2": "CMU-MOSI\nCMU-MOSEI\nCH-SIMS\nCH-SIMS v2",
          "–\n75.39±1.35\n75.43±1.14\n75.72±0.31\n79.14±0.17\n–\n79.17±1.31\n79.31±1.05\n70.05±1.18\n71.16±0.61\n–\n85.97±0.60\n72.40±0.80\n74.44±0.62\n89.72±0.45\n–": "–\n76.17±1.19\n75.56±0.53\n75.70±0.73\n79.03±0.41\n–\n81.64±1.32\n78.54±1.93\n70.86±1.32\n72.04±0.79\n–\n85.07±0.55\n72.17±0.86\n74.32±0.77\n89.73±1.08\n–"
        },
        {
          "Multi-modalities": "",
          "MISA": "LMF",
          "CMU-MOSI\nCMU-MOSEI\nCH-SIMS\nCH-SIMS v2": "CMU-MOSI\nCMU-MOSEI\nCH-SIMS\nCH-SIMS v2",
          "–\n75.39±1.35\n75.43±1.14\n75.72±0.31\n79.14±0.17\n–\n79.17±1.31\n79.31±1.05\n70.05±1.18\n71.16±0.61\n–\n85.97±0.60\n72.40±0.80\n74.44±0.62\n89.72±0.45\n–": "–\n76.07±0.71\n75.86±0.78\n75.52±0.49\n80.07±0.16\n–\n80.16±0.62\n81.00±1.03\n68.55±1.30\n72.20±0.45\n–\n86.91±0.51\n72.53±1.65\n74.64±0.41\n90.07±0.43\n–"
        },
        {
          "Multi-modalities": "",
          "MISA": "TFN",
          "CMU-MOSI\nCMU-MOSEI\nCH-SIMS\nCH-SIMS v2": "CMU-MOSI\nCMU-MOSEI\nCH-SIMS\nCH-SIMS v2",
          "–\n75.39±1.35\n75.43±1.14\n75.72±0.31\n79.14±0.17\n–\n79.17±1.31\n79.31±1.05\n70.05±1.18\n71.16±0.61\n–\n85.97±0.60\n72.40±0.80\n74.44±0.62\n89.72±0.45\n–": "–\n74.09±1.25\n76.55±0.83\n75.74±0.56\n80.19±0.55\n–\n81.44±0.76\n80.33±0.31\n70.60±0.76\n71.78±0.22\n–\n86.47±0.30\n73.85±1.17\n75.19±0.90\n90.84±0.56\n–"
        },
        {
          "Multi-modalities": "",
          "MISA": "Attention",
          "CMU-MOSI\nCMU-MOSEI\nCH-SIMS\nCH-SIMS v2": "CMU-MOSI\nCMU-MOSEI\nCH-SIMS\nCH-SIMS v2",
          "–\n75.39±1.35\n75.43±1.14\n75.72±0.31\n79.14±0.17\n–\n79.17±1.31\n79.31±1.05\n70.05±1.18\n71.16±0.61\n–\n85.97±0.60\n72.40±0.80\n74.44±0.62\n89.72±0.45\n–": "–\n74.13±1.72\n75.11±0.34\n75.41±0.20\n79.84±0.28\n–\n80.56±0.39\n81.18±1.03\n70.85±0.45\n70.94±1.12\n–\n86.66±0.94\n73.77±0.31\n74.91±0.33\n91.13±0.45\n–"
        }
      ],
      "page": 21
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "OPT-13B",
      "venue": "OPT-13B"
    },
    {
      "citation_id": "2",
      "title": "",
      "authors": [
        "Albert"
      ],
      "venue": ""
    },
    {
      "citation_id": "3",
      "title": "",
      "authors": [
        "Pert"
      ],
      "venue": ""
    },
    {
      "citation_id": "5",
      "title": "Affective computing",
      "authors": [
        "R Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "6",
      "title": "A survey of affect recognition methods: audio, visual and spontaneous expressions",
      "authors": [
        "Z Zeng",
        "M Pantic",
        "G Roisman",
        "T Huang"
      ],
      "year": "2007",
      "venue": "Proceedings of the 9th International Conference on Multimodal Interfaces"
    },
    {
      "citation_id": "7",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "8",
      "title": "A survey on empathetic dialogue systems",
      "authors": [
        "Y Ma",
        "K Nguyen",
        "F Xing",
        "E Cambria"
      ],
      "year": "2020",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "9",
      "title": "Emonets: Multimodal deep learning approaches for emotion recognition in video",
      "authors": [
        "S Kahou",
        "X Bouthillier",
        "P Lamblin",
        "C Gulcehre",
        "V Michalski",
        "K Konda",
        "S Jean",
        "P Froumenty",
        "Y Dauphin",
        "N Boulanger-Lewandowski"
      ],
      "year": "2016",
      "venue": "Journal on Multimodal User Interfaces"
    },
    {
      "citation_id": "10",
      "title": "Recent trends in deep learning based natural language processing",
      "authors": [
        "T Young",
        "D Hazarika",
        "S Poria",
        "E Cambria"
      ],
      "year": "2018",
      "venue": "IEEE Computational Intelligence Magazine"
    },
    {
      "citation_id": "11",
      "title": "The interspeech 2010 paralinguistic challenge",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner",
        "F Burkhardt",
        "L Devillers",
        "S Narayanan"
      ],
      "year": "2010",
      "venue": "Proceedings of the Interspeech"
    },
    {
      "citation_id": "12",
      "title": "Multi-attention recurrent network for human communication comprehension",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "P Vij",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "13",
      "title": "Smin: Semi-supervised multi-modal interaction network for conversational emotion recognition",
      "authors": [
        "Z Lian",
        "B Liu",
        "J Tao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "14",
      "title": "Muse 2023 challenge: Multimodal prediction of mimicked emotions, cross-cultural humour, and personalised recognition of affects",
      "authors": [
        "S Amiriparian",
        "L Christ",
        "A Önig",
        "A Cowen",
        "E.-M Meßner",
        "E Cambria",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "15",
      "title": "Multi-modal continuous dimensional emotion recognition using recurrent neural network and self-attention mechanism",
      "authors": [
        "L Sun",
        "Z Lian",
        "J Tao",
        "B Liu",
        "M Niu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 1st International on Multimodal Sentiment Analysis in Real-life Media Challenge and Workshop"
    },
    {
      "citation_id": "16",
      "title": "Investigation of multimodal features, classifiers and fusion methods for emotion recognition",
      "authors": [
        "Z Lian",
        "Y Li",
        "J Tao",
        "J Huang"
      ],
      "year": "2021",
      "venue": "Proceedings of the National Conference Man-Machine Speech Communication"
    },
    {
      "citation_id": "17",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "18",
      "title": "Superb: Speech processing universal performance benchmark",
      "authors": [
        "S Yang",
        "P Chi",
        "Y Chuang",
        "C Lai",
        "K Lakhotia",
        "Y Lin",
        "A Liu",
        "J Shi",
        "X Chang",
        "G Lin"
      ],
      "year": "2021",
      "venue": "Proceedings of the Interspeech"
    },
    {
      "citation_id": "19",
      "title": "Gcnet: Graph completion network for incomplete multimodal learning in conversation",
      "authors": [
        "Z Lian",
        "L Chen",
        "L Sun",
        "B Liu",
        "J Tao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "20",
      "title": "Context-dependent sentiment analysis in usergenerated videos",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "N Majumder",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "21",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "22",
      "title": "Ch-sims: A chinese multimodal sentiment analysis dataset with fine-grained annotation of modality",
      "authors": [
        "W Yu",
        "H Xu",
        "F Meng",
        "Y Zhu",
        "Y Ma",
        "J Wu",
        "J Zou",
        "K Yang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "23",
      "title": "M3ed: Multi-modal multi-scene multi-label emotional dialogue database",
      "authors": [
        "J Zhao",
        "T Zhang",
        "J Hu",
        "Y Liu",
        "Q Jin",
        "X Wang",
        "H Li"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "24",
      "title": "Multi-label, multi-task cnn approach for context-based emotion recognition",
      "authors": [
        "I Bendjoudi",
        "F Vanderhaegen",
        "D Hamad",
        "F Dornaika"
      ],
      "year": "2021",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "25",
      "title": "Multi-task semi-supervised adversarial autoencoding for speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Epps",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective computing"
    },
    {
      "citation_id": "26",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "A Zadeh",
        "M Chen",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "27",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "28",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Conference"
    },
    {
      "citation_id": "29",
      "title": "Make acoustic and visual cues matter: Ch-sims v2. 0 dataset and av-mixup consistent module",
      "authors": [
        "Y Liu",
        "Z Yuan",
        "H Mao",
        "Z Liang",
        "W Yang",
        "Y Qiu",
        "T Cheng",
        "X Li",
        "H Xu",
        "K Gao"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "30",
      "title": "Mapping discrete and dimensional emotions onto the brain: controversies and consensus",
      "authors": [
        "S Hamann"
      ],
      "year": "2012",
      "venue": "Trends in Cognitive Sciences"
    },
    {
      "citation_id": "31",
      "title": "An introduction to psychology",
      "authors": [
        "W Wundt"
      ],
      "year": "1912",
      "venue": "An introduction to psychology"
    },
    {
      "citation_id": "32",
      "title": "Discrete emotions or dimensions? the role of valence focus and arousal focus",
      "authors": [
        "L Barrett"
      ],
      "year": "1998",
      "venue": "Cognition & Emotion"
    },
    {
      "citation_id": "33",
      "title": "Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training",
      "authors": [
        "Z Tong",
        "Y Song",
        "J Wang",
        "L Wang"
      ],
      "year": "2022",
      "venue": "Proceedings of the Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "34",
      "title": "A survey of transfer learning",
      "authors": [
        "K Weiss",
        "T Khoshgoftaar",
        "D Wang"
      ],
      "year": "2016",
      "venue": "Journal of Big Data"
    },
    {
      "citation_id": "35",
      "title": "Training strategies to handle missing modalities for audio-visual expression recognition",
      "authors": [
        "S Parthasarathy",
        "S Sundaram"
      ],
      "year": "2020",
      "venue": "Companion Publication of the International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "36",
      "title": "Smil: Multimodal learning with severely missing modality",
      "authors": [
        "M Ma",
        "J Ren",
        "L Zhao",
        "S Tulyakov",
        "C Wu",
        "X Peng"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "37",
      "title": "Missing modality imagination network for emotion recognition with uncertain missing modalities",
      "authors": [
        "J Zhao",
        "R Li",
        "Q Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "38",
      "title": "Deep partial multi-view learning",
      "authors": [
        "C Zhang",
        "Y Cui",
        "Z Han",
        "J Zhou",
        "H Fu",
        "Q Hu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "39",
      "title": "Automatic speech emotion recognition: A survey",
      "authors": [
        "P Chandrasekar",
        "S Chapaneri"
      ],
      "year": "2014",
      "venue": "2014 International Conference on Circuits, Systems, Communication and Information Technology Applications (CSCITA)"
    },
    {
      "citation_id": "40",
      "title": "The interspeech 2009 emotion challenge",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner"
      ],
      "year": "2009",
      "venue": "Proceedings of the Interspeech"
    },
    {
      "citation_id": "41",
      "title": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis",
        "F Ringeval",
        "R Brueckner",
        "E Marchi",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2016",
      "venue": "Proceedings of the International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "42",
      "title": "Self-supervised speech representation learning: A review",
      "authors": [
        "A Mohamed",
        "H Lee",
        "L Borgholt",
        "J Havtorn",
        "J Edin",
        "C Igel",
        "K Kirchhoff",
        "S Li",
        "K Livescu",
        "L Maaløe"
      ],
      "year": "2022",
      "venue": "IEEE Journal on Selected Topics in Signal Processing"
    },
    {
      "citation_id": "43",
      "title": "wav2vec: Unsupervised pre-training for speech recognition",
      "authors": [
        "S Schneider",
        "A Baevski",
        "R Collobert",
        "M Auli"
      ],
      "year": "2019",
      "venue": "Proceedings of the Interspeech"
    },
    {
      "citation_id": "44",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Proceedings of the Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "45",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "46",
      "title": "Icon: interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "D Hazarika",
        "S Poria",
        "R Mihalcea",
        "E Cambria",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "47",
      "title": "Pre-trained models for natural language processing: A survey",
      "authors": [
        "X Qiu",
        "T Sun",
        "Y Xu",
        "Y Shao",
        "N Dai",
        "X Huang"
      ],
      "year": "2020",
      "venue": "Science China Technological Sciences"
    },
    {
      "citation_id": "48",
      "title": "Bert: Pretraining of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the Conference of the North American Chapter"
    },
    {
      "citation_id": "49",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "50",
      "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
      "authors": [
        "Z Yang",
        "Z Dai",
        "Y Yang",
        "J Carbonell",
        "R Salakhutdinov",
        "Q Le"
      ],
      "year": "2019",
      "venue": "Proceedings of the Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "51",
      "title": "Deberta: Decoding-enhanced bert with disentangled attention",
      "authors": [
        "P He",
        "X Liu",
        "J Gao",
        "W Chen"
      ],
      "year": "2020",
      "venue": "Proceedings of the 8th International Conference on Learning Representations"
    },
    {
      "citation_id": "52",
      "title": "Albert: A lite bert for self-supervised learning of language representations",
      "authors": [
        "Z Lan",
        "M Chen",
        "S Goodman",
        "K Gimpel",
        "P Sharma",
        "R Soricut"
      ],
      "year": "2020",
      "venue": "Proceedings of the International Conference on Learning Representations"
    },
    {
      "citation_id": "53",
      "title": "Electra: Pretraining text encoders as discriminators rather than generators",
      "authors": [
        "K Clark",
        "M.-T Luong",
        "Q Le",
        "C Manning"
      ],
      "year": "2020",
      "venue": "Proceedings of the International Conference on Learning Representations"
    },
    {
      "citation_id": "54",
      "title": "Llama: Open and efficient foundation language models",
      "authors": [
        "H Touvron",
        "T Lavril",
        "G Izacard",
        "X Martinet",
        "M.-A Lachaux",
        "T Lacroix",
        "B Rozière",
        "N Goyal",
        "E Hambro",
        "F Azhar"
      ],
      "year": "2023",
      "venue": "Llama: Open and efficient foundation language models",
      "arxiv": "arXiv:2302.13971"
    },
    {
      "citation_id": "55",
      "title": "The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only",
      "authors": [
        "G Penedo",
        "Q Malartic",
        "D Hesslow",
        "R Cojocaru",
        "A Cappelli",
        "H Alobeidli",
        "B Pannier",
        "E Almazrouei",
        "J Launay"
      ],
      "year": "2023",
      "venue": "The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only",
      "arxiv": "arXiv:2306.01116"
    },
    {
      "citation_id": "56",
      "title": "Bloom: A 176b-parameter open-access multilingual language model",
      "authors": [
        "B Workshop",
        "T Scao",
        "A Fan",
        "C Akiki",
        "E Pavlick",
        "S Ilić",
        "D Hesslow",
        "R Castagné",
        "A Luccioni",
        "F Yvon"
      ],
      "year": "2022",
      "venue": "Bloom: A 176b-parameter open-access multilingual language model",
      "arxiv": "arXiv:2211.05100"
    },
    {
      "citation_id": "57",
      "title": "Recognizing action units for facial expression analysis",
      "authors": [
        "Y.-I Tian",
        "T Kanade",
        "J Cohn"
      ],
      "year": "2001",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "58",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "59",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "60",
      "title": "Squeeze-and-excitation networks",
      "authors": [
        "J Hu",
        "L Shen",
        "G Sun"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "61",
      "title": "Learning deep global multi-scale and local attention features for facial expression recognition in the wild",
      "authors": [
        "Z Zhao",
        "Q Liu",
        "S Wang"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "62",
      "title": "Ms-celeb-1m: A dataset and benchmark for large-scale face recognition",
      "authors": [
        "Y Guo",
        "L Zhang",
        "Y Hu",
        "X He",
        "J Gao"
      ],
      "year": "2016",
      "venue": "Proceedings of the European Conference on Computer Vision"
    },
    {
      "citation_id": "63",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "I Goodfellow",
        "D Erhan",
        "P Carrier",
        "A Courville",
        "M Mirza",
        "B Hamner",
        "W Cukierski",
        "Y Tang",
        "D Thaler",
        "D.-H Lee"
      ],
      "year": "2013",
      "venue": "Proceedings of the 20th International Conference on Neural Information Processing"
    },
    {
      "citation_id": "64",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild",
      "authors": [
        "S Li",
        "W Deng",
        "J Du"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "65",
      "title": "Self-supervised visual feature learning with deep neural networks: A survey",
      "authors": [
        "L Jing",
        "Y Tian"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "66",
      "title": "Self-supervised learning for videos: A survey",
      "authors": [
        "M Schiappa",
        "Y Rawat",
        "M Shah"
      ],
      "year": "2023",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "67",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "C Hallacy",
        "A Ramesh",
        "G Goh",
        "S Agarwal",
        "G Sastry",
        "A Askell",
        "P Mishkin",
        "J Clark"
      ],
      "year": "2021",
      "venue": "Proceedings of the International Conference on Machine Learning"
    },
    {
      "citation_id": "68",
      "title": "Learning robust visual features without supervision",
      "authors": [
        "M Oquab",
        "T Darcet",
        "T Moutakanni",
        "H Vo",
        "M Szafraniec",
        "V Khalidov",
        "P Fernandez",
        "D Haziza",
        "F Massa",
        "A El-Nouby"
      ],
      "year": "2023",
      "venue": "Learning robust visual features without supervision",
      "arxiv": "arXiv:2304.07193"
    },
    {
      "citation_id": "69",
      "title": "Multimodal fusion for multimedia analysis: a survey",
      "authors": [
        "P Atrey",
        "M Hossain",
        "A Saddik",
        "M Kankanhalli"
      ],
      "year": "2010",
      "venue": "Multimedia Systems"
    },
    {
      "citation_id": "70",
      "title": "Multimodal sentiment analysis: A systematic review of history, datasets, multimodal fusion methods, applications, challenges and future directions",
      "authors": [
        "A Gandhi",
        "K Adhvaryu",
        "S Poria",
        "E Cambria",
        "A Hussain"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "71",
      "title": "Efficient low-rank multimodal fusion with modality-specific factors",
      "authors": [
        "Z Liu",
        "Y Shen",
        "V Lakshminarasimhan",
        "P Liang",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the"
    },
    {
      "citation_id": "72",
      "title": "Misa: Modalityinvariant and-specific representations for multimodal sentiment analysis",
      "authors": [
        "D Hazarika",
        "R Zimmermann",
        "S Poria"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "73",
      "title": "Improving multimodal fusion with hierarchical mutual information maximization for multimodal sentiment analysis",
      "authors": [
        "W Han",
        "H Chen",
        "S Poria"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "74",
      "title": "Memory fusion network for multi-view sequential learning",
      "authors": [
        "A Zadeh",
        "P Liang",
        "N Mazumder",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "75",
      "title": "Found in translation: Learning robust joint representations by cyclic translations between modalities",
      "authors": [
        "H Pham",
        "P Liang",
        "T Manzini",
        "L.-P Morency",
        "B Óczos"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "76",
      "title": "Learning factorized multimodal representations",
      "authors": [
        "Y.-H Tsai",
        "P Liang",
        "A Zadeh",
        "L.-P Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the 7th International Conference on Learning Representations"
    },
    {
      "citation_id": "77",
      "title": "Multimodal transformer for unaligned multi-modal language sequences",
      "authors": [
        "Y.-H Tsai",
        "S Bai",
        "P Liang",
        "J Kolter",
        "L.-P Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Conference"
    },
    {
      "citation_id": "78",
      "title": "Measuring emotion: the selfassessment manikin and the semantic differential",
      "authors": [
        "M Bradley",
        "P Lang"
      ],
      "year": "1994",
      "venue": "Journal of Behavior Therapy and Experimental Psychiatry"
    },
    {
      "citation_id": "79",
      "title": "Musan: A music, speech, and noise corpus",
      "authors": [
        "D Snyder",
        "G Chen",
        "D Povey"
      ],
      "year": "2015",
      "venue": "Musan: A music, speech, and noise corpus",
      "arxiv": "arXiv:1510.08484"
    },
    {
      "citation_id": "80",
      "title": "A baseline for detecting misclassified and out-of-distribution examples in neural networks",
      "authors": [
        "D Hendrycks",
        "K Gimpel"
      ],
      "year": "2017",
      "venue": "Proceedings of the International Conference on Learning Representations"
    },
    {
      "citation_id": "81",
      "title": "Unicon: Combating label noise through uniform selection and contrastive learning",
      "authors": [
        "N Karim",
        "M Rizve",
        "N Rahnavard",
        "A Mian",
        "M Shah"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "82",
      "title": "Openface: an open source facial behavior analysis toolkit",
      "authors": [
        "T Baltrušaitis",
        "P Robinson",
        "L.-P Morency"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "83",
      "title": "Wenet: Production oriented streaming and non-streaming end-to-end speech recognition toolkit",
      "authors": [
        "Z Yao",
        "D Wu",
        "X Wang",
        "B Zhang",
        "F Yu",
        "C Yang",
        "Z Peng",
        "X Chen",
        "L Xie",
        "X Lei"
      ],
      "year": "2021",
      "venue": "Proceedings of the Interspeech"
    },
    {
      "citation_id": "84",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2015",
      "venue": "Proceedings of the International Conference on Learning Representations"
    },
    {
      "citation_id": "85",
      "title": "Dropout: a simple way to prevent neural networks from overfitting",
      "authors": [
        "N Srivastava",
        "G Hinton",
        "A Krizhevsky",
        "I Sutskever",
        "R Salakhutdinov"
      ],
      "year": "2014",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "86",
      "title": "Analysing affective behavior in the first abaw 2020 competition",
      "authors": [
        "D Kollias",
        "A Schulc",
        "E Hajiyev",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "Proceedings of the 15th IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "87",
      "title": "Imagenet: A large-scale hierarchical image database",
      "authors": [
        "J Deng",
        "W Dong",
        "R Socher",
        "L.-J Li",
        "K Li",
        "L Fei-Fei"
      ],
      "year": "2009",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "88",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "D Hazarika",
        "S Poria",
        "A Zadeh",
        "E Cambria",
        "L.-P Morency",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the Conference of the North American Chapter"
    },
    {
      "citation_id": "89",
      "title": "Modality to modality translation: An adversarial representation learning and graph fusion network for multimodal fusion",
      "authors": [
        "S Mai",
        "H Hu",
        "S Xing"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "90",
      "title": "Emotionlines: An emotion corpus of multi-party conversations",
      "authors": [
        "C.-C Hsu",
        "S.-Y Chen",
        "C.-C Kuo",
        "T.-H Huang",
        "L.-W Ku"
      ],
      "year": "2018",
      "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation"
    },
    {
      "citation_id": "91",
      "title": "The kinetics human action video dataset",
      "authors": [
        "W Kay",
        "J Carreira",
        "K Simonyan",
        "B Zhang",
        "C Hillier",
        "S Vijayanarasimhan",
        "F Viola",
        "T Green",
        "T Back",
        "P Natsev"
      ],
      "year": "2017",
      "venue": "The kinetics human action video dataset",
      "arxiv": "arXiv:1705.06950"
    },
    {
      "citation_id": "92",
      "title": "Voxceleb2: Deep speaker recognition",
      "authors": [
        "J Chung",
        "A Nagrani",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "Proceedings of the Interspeech"
    },
    {
      "citation_id": "93",
      "title": "Mae-dfer: Efficient masked autoencoder for self-supervised dynamic facial expression recognition",
      "authors": [
        "L Sun",
        "Z Lian",
        "B Liu",
        "J Tao"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "94",
      "title": "Gpt-4v with emotion: A zero-shot benchmark for multimodal emotion understanding",
      "authors": [
        "Z Lian",
        "L Sun",
        "H Sun",
        "K Chen",
        "Z Wen",
        "H Gu",
        "S Chen",
        "B Liu",
        "J Tao"
      ],
      "year": "2023",
      "venue": "Gpt-4v with emotion: A zero-shot benchmark for multimodal emotion understanding",
      "arxiv": "arXiv:2312.04293"
    },
    {
      "citation_id": "95",
      "title": "Explainable multimodal emotion reasoning",
      "authors": [
        "Z Lian",
        "L Sun",
        "M Xu",
        "H Sun",
        "K Xu",
        "Z Wen",
        "S Chen",
        "B Liu",
        "J Tao"
      ],
      "year": "2023",
      "venue": "Explainable multimodal emotion reasoning",
      "arxiv": "arXiv:2306.15401"
    },
    {
      "citation_id": "96",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2023",
      "venue": "Proceedings of the International Conference on Machine Learning"
    },
    {
      "citation_id": "97",
      "title": "Jointly fine-tuning \"bert-like\" self supervised models to im-prove multimodal speech emotion recognition",
      "authors": [
        "S Siriwardhana",
        "A Reis",
        "R Weerasekera",
        "S Nanayakkara"
      ],
      "year": "2020",
      "venue": "Proceedings of the Interspeech"
    },
    {
      "citation_id": "98",
      "title": "Eva-02: A visual representation for neon genesis",
      "authors": [
        "Y Fang",
        "Q Sun",
        "X Wang",
        "T Huang",
        "X Wang",
        "Y Cao"
      ],
      "year": "2023",
      "venue": "Eva-02: A visual representation for neon genesis",
      "arxiv": "arXiv:2303.11331"
    },
    {
      "citation_id": "99",
      "title": "Data2vec: A general framework for self-supervised learning in speech, vision and language",
      "authors": [
        "A Baevski",
        "W.-N Hsu",
        "Q Xu",
        "A Babu",
        "J Gu",
        "M Auli"
      ],
      "year": "2022",
      "venue": "Proceedings of the International Conference on Machine Learning"
    },
    {
      "citation_id": "100",
      "title": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "101",
      "title": "Pert: pre-training bert with permuted language model",
      "authors": [
        "Y Cui",
        "Z Yang",
        "T Liu"
      ],
      "year": "2022",
      "venue": "Pert: pre-training bert with permuted language model",
      "arxiv": "arXiv:2203.06906"
    },
    {
      "citation_id": "102",
      "title": "Lert: A linguisticallymotivated pre-trained language model",
      "authors": [
        "Y Cui",
        "W Che",
        "S Wang",
        "T Liu"
      ],
      "year": "2022",
      "venue": "Lert: A linguisticallymotivated pre-trained language model",
      "arxiv": "arXiv:2211.05344"
    },
    {
      "citation_id": "103",
      "title": "Revisiting pre-trained models for chinese natural language processing",
      "authors": [
        "Y Cui",
        "W Che",
        "T Liu",
        "B Qin",
        "S Wang",
        "G Hu"
      ],
      "venue": "Revisiting pre-trained models for chinese natural language processing"
    },
    {
      "citation_id": "104",
      "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
      "authors": [
        "N Reimers",
        "I Gurevych"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "105",
      "title": "Llama 2: Open foundation and fine-tuned chat models",
      "authors": [
        "H Touvron",
        "L Martin",
        "K Stone",
        "P Albert",
        "A Almahairi",
        "Y Babaei",
        "N Bashlykov",
        "S Batra",
        "P Bhargava",
        "S Bhosale"
      ],
      "year": "2023",
      "venue": "Llama 2: Open foundation and fine-tuned chat models",
      "arxiv": "arXiv:2307.09288"
    },
    {
      "citation_id": "106",
      "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90% chatgpt quality",
      "authors": [
        "W.-L Chiang",
        "Z Li",
        "Z Lin",
        "Y Sheng",
        "Z Wu",
        "H Zhang",
        "L Zheng",
        "S Zhuang",
        "Y Zhuang",
        "J Gonzalez",
        "I Stoica",
        "E Xing"
      ],
      "year": "2023",
      "venue": "Vicuna: An open-source chatbot impressing gpt-4 with 90% chatgpt quality"
    },
    {
      "citation_id": "107",
      "title": "Stablelm alpha v2 models",
      "authors": [
        "J Tow"
      ],
      "year": "2023",
      "venue": "Stablelm alpha v2 models"
    },
    {
      "citation_id": "108",
      "title": "Opt: Open pre-trained transformer language models",
      "authors": [
        "S Zhang",
        "S Roller",
        "N Goyal",
        "M Artetxe",
        "M Chen",
        "S Chen",
        "C Dewan",
        "M Diab",
        "X Li",
        "X Lin"
      ],
      "year": "2022",
      "venue": "Opt: Open pre-trained transformer language models",
      "arxiv": "arXiv:2205.01068"
    },
    {
      "citation_id": "109",
      "title": "Glm: General language model pretraining with autoregressive blank infilling",
      "authors": [
        "Z Du",
        "Y Qian",
        "X Liu",
        "M Ding",
        "J Qiu",
        "Z Yang",
        "J Tang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "110",
      "title": "He is currently an Assistant Professor at State Key Laboratory of Multimodal Artificial Intelligence Systems",
      "authors": [
        "A Yang",
        "B Xiao",
        "B Wang",
        "B Zhang",
        "C Bian",
        "C Yin",
        "C Lv",
        "D Pan",
        "D Wang",
        "D Yan"
      ],
      "year": "2021",
      "venue": "2016, and the Ph.D degree from the Institute of Automation, Chinese Academy of Sciences",
      "arxiv": "arXiv:2309.10305"
    }
  ]
}