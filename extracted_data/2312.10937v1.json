{
  "paper_id": "2312.10937v1",
  "title": "An Extended Variational Mode Decomposition Algorithm Developed Speech Emotion Recognition Performance",
  "published": "2023-12-18T05:24:03Z",
  "authors": [
    "David Hason Rudd",
    "Huan Huo",
    "Guandong Xu"
  ],
  "keywords": [
    "Speech emotion recognition (SER)",
    "Variational mode decomposition (VMD)",
    "Convolutional neural network (CNN)",
    "Sound signal processing",
    "Acoustic features"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition (ER) from speech signals is a robust approach since it cannot be imitated like facial expression or text based sentiment analysis. Valuable information underlying the emotions are significant for human-computer interactions enabling intelligent machines to interact with sensitivity in the real world. Previous ER studies through speech signal processing have focused exclusively on associations between different signal mode decomposition methods and hidden informative features. However, improper decomposition parameter selections lead to informative signal component losses due to mode duplicating and mixing. In contrast, the current study proposes VGG-optiVMD, an empowered variational mode decomposition algorithm, to distinguish meaningful speech features and automatically select the number of decomposed modes and optimum balancing parameter for the data fidelity constraint by assessing their effects on the VGG16 flattening output layer. Various feature vectors were employed to train the VGG16 network on different databases and assess VGG-optiVMD reproducibility and reliability. One, two, and three-dimensional feature vectors were constructed by concatenating Mel-frequency cepstral coefficients, Chromagram, Mel spectrograms, Tonnetz diagrams, and spectral centroids. Results confirmed a synergistic relationship between the fine-tuning of the signal sample rate and decomposition parameters with classification accuracy, achieving state-of-the-art 96.09% accuracy in predicting seven emotions on the Berlin EMO-DB database.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Word meaning is often conveyed by the tone of voice  [30] , although human emotions are not solely conveyed through the words used, but also through by arXiv:2312.10937v1 [cs.SD] 18 Dec 2023 modifying facial expressions and vocal tone. Thus, changing voice characteristics is how most humans express different emotions  [36] . Consequently, considerable human-computer interaction research has considered emotion recognition (ER). Various applications detect serious state by analyzing caller emotion in emergency centers; and speech pathology, e-learning, voiceprints, security, and other smart-centric services commonly employ speech emotion recognition (SER). Other approaches have considered biosensing, Electroencephalography (EEG), and facial recognition, to detect emotions  [2, 20, 11] .\n\nSignal based ER employs various signals, including electrodermal activity, blood volume pulse, galvanic skin response, electrocardiogram (ECG), EEG, and speech, are commonly categorized into several decomposed modes due to the complexity and nonstationary nature of them, which allows latent factors and patterns to be extracted more easily. Several time series analysis approaches for SER have been over the previous two decades, extracting relevant speech features from nonstationary and instantaneous signals, including traditional short time Fourier transform (STFTs), empirical wavelet transforms (EWTs)  [13] , and variational mode decomposition (VMD)  [12] . Nonstationary signal properties and its components make mean STFTs are not always suitable, and previous studies have mostly considered these approaches in isolation  [8] . Huang et al.  [17]  proposed empirical mode decomposition (EMD), which decomposes the source signal into an unknown number of signal modes defined by frequency and amplitude modulated components. However, EMD has several limitations, including overlapping intrinsic mode functions (IMFs); and increased computational load when analyzing a large number of modes, particularly EEG and speech signals  [8] .\n\nEmpirical wave transforms employ an adaptive wavelet subdivision scheme, similar to EMD, to address EMD drawbacks by decomposing the signal into a predetermined number of IMFs or modes. Several studies have proposed an envelope weighted transformation to decompose and denoise EEG and speech signals for processing  [6, 35] . Variational mode decomposition employs non-recursive decomposition to deal with nonlinear and nonstationary signals. In contrast with EWT and EMD, few studies have considered VMD to analyze EEG signals. VMD decomposes signals into modes with a narrowband around a center frequency and can overcome EWT limitations, including shift and filter bank boundary sensitivity and EMD mode mixing effects. Therefore, we were motivated to apply VMD for speech signal processing. Acoustic feature selection is essential for SER to describe various voice signal aspects captured from different features  [5, 11, 20] . Acoustic features include time-frequency, time, and frequency domain representations. Extracted features from time-frequency domains carry more informative data than the other domains, and better capture latent emotion content from speech signals  [33] . Useful time-domain features include amplitude envelope, RMS energy, and zero-crossing rate; and are commonly employed as sequence evaluation ratios. In contrast, relevant frequency domain features include band energy ratio (BER), spectral centroid, and spectral flux. Several previous studies used VMD method to analyze signals, extracting features from the decomposed signals. However, we propose VGG-optiVMD, utilizing a VMD based feature augmentation method to enrich predictors and maximize emotion classification accuracy. Results from the proposed VGG-optiVMD approach on several common publicly available databases confirm significant ER improvement compared with previous approaches.\n\nThe main contributions from this study can be summarized as follows.\n\n-   [11]  decomposed the speech signal into three components sampling at 16000 Hz over 20 ms frames, then input various mode central frequency statistical parameters to a support vector machine (SVM) classifier. Optimum recognition rate achieved 85.81% and 69.13% accuracy for two and four emotion classes, respectively, and increased accuracy by 5% for eight emotion classes compared to previous studies on the RAVDESS database  [11] .\n\nLal et al.  [24]  empirically demonstrated VMD advantages to decompose speech signals in the correct central frequency and subsequently estimated epoch locations from noise degraded emotional speech signal.\n\nZhang et al.  [41]  proposed multidimensional feature extraction for EEG signal emotion recognition combining wavelet packet decomposition (WPD) with VMD to break down an EEG signals and extract wavelet packet entropy, modified multi-scale sample entropy, fractal dimension, and first difference of each emotional variational mode functions as feature components. They subsequently demonstrated robust results using a random forest (RF) classifier on the DEAP dataset  [21] .\n\nKhare et al.  [20]  reduced reconstruction error using meta-heuristic techniques to condensing from 16 to 1 dimension using eigenvector centrality method channel selection on EEG signals. They subsequently improved Optimized variational mode decomposition (O-VMD) accuracy by 5% compared with traditional VMD on the dataset of four emotions that built by themselves, with low computational load and model complexity. Furthermore, the SVM classifier significantly reducing average mean square error.\n\nGenerally, EEG signals can effectively analyze individual's emotion since they are subject dependent. Pandey  [29]  proposed subject-independent emotion recognition using VMD and deep neural networks (VMD-DNN) on the benchmark DEAP dataset. Two features, first difference and power-spectral-density used since were sufficient to recognize calm, happy, sad, and angry emotions. SVM and DNN classifier accuracy was improved by employing VMD based feature extraction compared with EMD, STFT, and differential entropy feature extraction, achieving 61.25% for arousal and 62.50% valence prediction accuracies.\n\nSeveral previous studies considered STFT signal decomposition techniques for SER. For example, Zhao et.  [42]  achieved robust 91.89% accuracy on the EMODB database  [7] . Few previous studies considered VMD to decompose speech signals as mostly employed EEG signal for ER. Dendukuri  [11]  achieved 69.13% accuracy to recognize four emotions on the RAVDESS database. However, to the best of our knowledge, the current study is the first to employ VMD to enrich multidimensional feature vectors to enhance VGG-16 network learning.",
      "page_start": 1,
      "page_end": 4
    },
    {
      "section_name": "Proposed Methodology",
      "text": "Speech signal processing involves decoding and encoding information within the speech signal. Glottal airflow from vocal folds, nasal cavity, and vocal tracts generate sounds and words that also convey emotions. Thus, human voice is a convolution of vocal tract frequency response with a glottal pulse. The glottal pulse itself does not contain emotion related informative, and hence is considered noise in this context. The main aim for decomposition based speech signal processing is to constrain noise and interference frequencies to enhance signal decoding.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Speech Feature Extraction",
      "text": "Essential and informative acoustic features in the time-frequency domain include the Mel spectrogram, chromograms, spectral contrasts, tonnetz, and Melfrequency cepstral coefficients (MFCCs)  [1, 15] . The above features are extracted and subsequently employed in various combinations to generate multidimensional feature vectors or maps.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Variational Mode Decomposition",
      "text": "Variational mode decomposition is a popular technique for decomposing nonstationary signals into sub-signals or modes, where mode contains a specific meaningful property from the original signal in a narrow bandwidth around the center frequency. Modes are obtained from Hilbert transform output, also called the intrinsic mode function (IMF). Furthermore, mode center frequency can be considered as a real component of the original signal for sufficiently narrow bandwidth  [24] . The VMD adaptive algorithm reduces the original signal complexity  [9, 12] .\n\nThe VMD algorithm applies the Wiener filter, Hilbert transform, analytical signals, and frequency mixing. Wiener filters are narrowband filters for noise reduction. The Hilbert transform is a time-invariant multiplier, convolving the original signal g(t) with the impulse response 1/πt  [22] . Therefore, it converts the real signal into the complex or imaginary part to extract magnitude and phase angle time series for frequencies with the most power at each specific time point. The VMD algorithm adds the Hilbert transform H[g(t)] to the original signal g(t), removing any negative frequencies present (due to Hermitian symmetry). The two main VMD objects are to constrain the bandwidth for each IMF center frequency and reconstruct the original signal from the sum of all modes. First, the Hilbert transform filters frequencies in the negative side of the spectrum, and then shifts the obtained bandwidth to the modes central frequency. Second, the obtained spectrum is shifted to the baseband region via a modulator function to obtain bandwidth around central frequency ω. Finally, H1 Gaussian smoothness for the demodulation signal is used to estimate the bandwidth. Thus, constraining the L2 norm squared gradient  [12]  defines the optimization problem (1),\n\n, subject to:\n\nwhere the partial derivative ∂ ∂t  [.]  minimizes variation in the obtained bandwidth; g(t) is the original speech signal frame; g k (t) is the kth mode for g(t); K is the total number of modes; ω k = {w1, . . . , wk} is the mode center frequency, and a convenient way to reference the center frequencies for the set of K modes; e -jω k t is a modulator function to shift the spectrum for each mode to the baseband.\n\nThe analytical signal generated by applying the Hilbert transform j πt and unit impulse function δ(t) as shown in equation  (1) . The δ(t) denotes to the Dirac delta distribution known as a unit impulse so that its value is zero everywhere and infinite at original signal. The original voice signal can be reproduced by solving the constraint optimization (1), which can be simplified using an augmented Lagrangian multiplier to transform it into an unconstrained problem (2),\n\nwhere, λ is a time dependent Lagrangian multiplier, and α is a bandwidth control parameter.\n\nThe unconstrained Lagrangian problem (2) can be solved to obtain the frequency and the modes using the alternate direction method of multipliers (ADMM)  [16, 32, 12]  optimization in spectral domain. However, optimization outcomes are the same for the frequency and time domains Hence mode g k (ω) can be updated in the spectral domain,\n\nUpdating is obtained using the Wiener filter for the current residual using the signal prior 1/(ω -ω k ) 2 to restrain variation across the central frequency minimum, providing the updated mode center frequency ω k as\n\nwhere Ĝk (ω) is the Fourier transform for g n+1 k (t). A better decomposed signal can be obtained by reconstructing the original signal as the sum of modes and estimating bandwidth using the Wiener filter. Details for the VMD algorithm are provided in  [12] .\n\nTo leverage VMD effectiveness, we propose the VGG-optiVMD algorithm for automatically selecting optimum α and K by analyzing different decomposition parameter effects on classification accuracy.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Proposed Vgg-Optivmd",
      "text": "Reconstruction error for a decomposed signal can be reduced by selecting optimum K and α. Improper decomposition parameter selection will create duplicate modes, causing signal information losses and hence reduced classifier performance. One drawback for VMD is that finding decomposition parameters K and α to provide optimum performance challenging. Several approaches have proposed for ER using ECG, EEG and vibrational signals. For example, the OVMD algorithm  [25]  uses a series of indicators, including permutation entropy, kurtosis criteria, extreme frequency domain value, and energy loss coefficients, to identify optimum K. Wang et al.  [38]  controlled power spectral and dynamic entropy features to find optimal K and α to decompose vibration signal and extract fault features.\n\nHowever, these approaches use IMF or mode characteristics to find the best decomposition parameters for specific low amplitude input signals with empirical threshold selection, which is not applicable for speech signal processing. Dendukuri et al.  [11]  decomposed speech signals using five modes to recognize eight emotions, achieving 61.2% accuracy on the RAVDESS database. They combined different features, including a 45-dimensional feature set including mode center frequency, statistical values for mode center frequency, MFCCs, and spectral statistical features to improve classifier performance.\n\nThe above methods evaluate optimum K value using statistical features and indicators for guidance. In particular, identified mode number correctness was not verified or fine-tuned practically by monitoring classification accuracy.\n\nIn contrast, the current study proposes to automate optimum VMD decomposition parameter selection using a feedback loop from the VGG16 flattening output layer. Algorithm 1 shows the proposed optimized VMD algorithm (VGG-optiVMD). The key strength for VGG-optiVMD is reliability, generality, and reproducibility across different speech databases for real-world applications, e.g. customer satisfaction analysis in call centers.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Feature Scaling, Data Augmentation, And Emotion Classification",
      "text": "Figure  1  shows the proposed framework to train CNN-VGG16  [34]  to extract enriched feature vectors and classify seven emotions: anger, boredom, happiness, neutral, disgust, sadness, and fear on two databases EMODB and RAVDESS. Figure  1  shows the model development proceeds as follows. 1. The voice signal is sampled at 88400 Hz and five well-known acoustic features extracted in the time-frequency domain: MFCCs, Mel spectrogram, Tonnetz, spectral contrast, and chromagram. 2. The Hann window function is applied with 2.9 s fixed length and 0.4 ms shifting time to sub-signal spectra assembled over a series of frames, extracted features are reshaped into a single (128 × 128 × 3) feature vector. 3. The SMOTE  [26]  oversampling strategy is applied to compensate minority classes and reduce model bias. Final testing and training features are randomly partitioned into 20% and 80% sets, respectively. 4. The proposed VGG-optiVMD algorithm is applied to decode frequency statistical properties at specific times that distinguish emotions within the feature vector. 5. The VGG network is trained on the augmented feature vector to classify emotions into seven classes.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Experiment Setup",
      "text": "This study followed the preprocessing system from  [33] . All acoustic features were extracted using the Librosa tool  [27]  using the Ryerson Audio Visual Database of Emotional Speech and Song (RAVDESS)  [26]  and the Berlin EMODB  [7]  databases. Voice data are preprocessed with frame size = 2048, HOP length = 256, and sampling rate = 88400 to avoid spectral leakage and enhance frequency resolution. Several experiments were performed on nine different feature vectors to identify the proposed VGG-optiVMD algorithm effectiveness using. The model was implemented on a Keras framework. The detail of network implementations are available in our GitHub repository 3  .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Modelling",
      "text": "The aim of modeling was to enhance informative data within the feature vectors and avoid overfitting. Therefore, we applied data augmentation by decomposing the feature vector data, i.e., g(t) is explained in proposed algorithm, into different modes. Augmentation effects on classification accuracy were assessed using diverse K and α sets. Optimal K and α was assessed iteratively until robust classification accuracy was achieved or the break loop condition reached. K and α were set to a wide range of 3-8 and 1000-6000, respectively, based on empirical experiments since there was no significant improvement in prediction accuracy outside those ranges. The VGG16 is selected to be trained from augmented feature maps as a trade-off between model runtime and classifier accuracy. The VGG16 architecture used the ADAM optimizer with learning rate = 0.0001; six fully connected hidden layers with ReLU, SELU, and TanH activation functions; epochs = 50, batch size = 4; and SoftMax function for the output layer.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Result And Discussion",
      "text": "To assess the effectiveness of our VMD-based feature augmentation method several evaluation metrics were employed including F1 score, test accuracy, and confusion matrix. Based on the experiment results shown in Table  1 , there is a correlation between the number of modes K, α and classification accuracy.\n\nThe different acoustic features are enriched with various sets of decomposition parameters. Results showed that higher accuracy was obtained for K (4 -6) and α (2000 -4000) in both datasets, although VGG-optiVMD is set to a limited range of α (1000-10000) and K (2-8) due to increasing a heavy computational load when K value is over 8 with sample rate 88400. This limitation can be considered a functional constraint of VGG-optiVMD. Nevertheless, a state-ofthe-art result was achieved with the accuracy of 96.09% with K=6 and α=2000 as demonstrated in Table ??.\n\nAnalyzing the results of the baseline model, which is built with the same framework simply without VMD-based feature vector augmentation, helps us to justify the power of the VGG-optiVMD in SER. Therefore, we attempted to evaluate the model performance through variation of sample rate, window size, K and α without using VMD (baseline model) and with VMD (proposed model). As shown in Figure  3 , unlike the baseline model, the proposed model performed better with a larger sampling rate and window size. Moreover, the highest test accuracy and F1 score were obtained via VGG-optiVMD, proving that our VMD-based feature augmentation method significantly improved the classification accuracy. The Figure  4  shows the efficient functionality of VGG-optiVMD on the feature vector 3D-Mel Spectrogram+MFCCs+Chromagram. Figure (a) represents the feature before applying VMD based data augmentation, and figure (b) clearly shows that the informative frequencies are distinguished on the feature vector after applying the data augmentation method. In addition, the image shows the feature vector acquired higher distinction energies in the timefrequency domain. Therefore, the implications of this finding can improve the learning process in VGG16 and result in better prediction accuracy. The confusion matrix in Table  1  demonstrates the high performance of the classification model with accuracy above 90% for all classes. Nevertheless, the model performs poorly when predicting happiness and anger emotions due to the similarity of signal attributes such as intensity, frequency and harmonic structure.  The VGG-optiVMD method is compared with the most recent works, shown in Table  2 , that our method outperforms previous models and achieves a stateof-the-art result in terms of accuracy. In accordance with the knowledge we have, this is the first work to employ VMD as a feature augmentation method in SER. Moreover, the main advantage of the VGG-optiVMD is its generality, which can be employed independently for other acoustic features and different databases.    [40]  13 MFCCs Tree Model 70 Popova et al.  [31]  Mel spectrograms VGG16 71 Hajarol. et al.  [14]  Mel spectrograms+MFCCs CNN 72.21 Wang et al.  [37]  Fourier Parameter+MFCCs SVM 73.3 Kown et al.  [23]  Spectrogram Deep SCNN 79.50 Badsha et al.  [4]  Spectrogram CNN 80.79 Huang et al.  [18]  Spectrogram CNN 85.2 Issa et al.  [19]  MFCCs+Chroma.+Mel spec.+Contrast+Tonnetz VGG16 86.10 Meng et al.  [28]  log Mel spec.+1st & 2nd delta(log Mel spec.) CNN-LSTM 90.78 Wu et al.  [39]  Modulation Spectral Features (MSFs) SVM 91.60 Rudd et al.  [33]  Harmonic-Percussive (HP)+log Mel spec. VGG16-MLP 92.79 Demircan et al.  [10]  LPC+MFCCs SVM 92.86 Zhao et al.  [42]  log Mel spectrogram CNN-LSTM 95.89 VGG-optiVMD 3D-Mel spectrogram+MFCCs+Chromagram VGG16-VMD 96.09%",
      "page_start": 8,
      "page_end": 11
    },
    {
      "section_name": "Conclusion",
      "text": "Speech signal processing is employed in some applications when we only have access to speech voice to detect emotions which is the first aim of this study, the second aim of this study is to introduce specific data augmentation techniques to enrich the extracted acoustic features by design of VGG-optiVMD, an extended VMD algorithm to improve SER performance.\n\nThe findings provide solid empirical confirmation of the key role of the sampling rate, the number of the decomposed mode, K and the balancing parameter of the data-fidelity constraint, α, in the performance of the emotion classifier. Taken together, these findings suggest that VMD decomposition parameters K (2-6) and α (2000-6000) are optimum values on both the RAVDESS and EMODB databases. The proposed VGG-optiVMD algorithm improved the emotion classification to a state-of-the-art result with a test accuracy of 96.09% in the Berlin EMO-DB and 86.21% in the RAVDESS datasets. Further work needs to be done to establish whether extracting acoustic features only from informative decomposed modes can reduce computational load constraints. Therefore, the study should be repeated using the VMD algorithm before acoustic feature extraction process.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows the proposed framework to train CNN-VGG16 [34] to extract",
      "page": 7
    },
    {
      "caption": "Figure 1: shows the model development proceeds as follows.",
      "page": 7
    },
    {
      "caption": "Figure 1: Proposed model development workflow: extracted features are enriched",
      "page": 7
    },
    {
      "caption": "Figure 2: Empirical results of emotion classification accuracy (%) are demonstrated",
      "page": 9
    },
    {
      "caption": "Figure 3: , unlike the baseline model, the proposed model per-",
      "page": 9
    },
    {
      "caption": "Figure 4: shows the efficient functionality of VGG-optiVMD",
      "page": 9
    },
    {
      "caption": "Figure 3: The model performance is assessed by different signal sampling rates and",
      "page": 10
    },
    {
      "caption": "Figure 4: The efficient functionality of VGG-optiVMD on the feature vector 3D-",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "∗corresponding authors": "Abstract. Emotion recognition (ER)\nfrom speech signals\nis a robust"
        },
        {
          "∗corresponding authors": "approach since it cannot be imitated like facial expression or text based"
        },
        {
          "∗corresponding authors": "sentiment analysis. Valuable information underlying the emotions are sig-"
        },
        {
          "∗corresponding authors": "nificant\nfor human-computer\ninteractions enabling intelligent machines"
        },
        {
          "∗corresponding authors": "to interact with sensitivity in the real world. Previous ER studies through"
        },
        {
          "∗corresponding authors": "speech signal processing have focused exclusively on associations between"
        },
        {
          "∗corresponding authors": "different\nsignal mode decomposition methods and hidden informative"
        },
        {
          "∗corresponding authors": "features. However,\nimproper decomposition parameter selections lead to"
        },
        {
          "∗corresponding authors": "informative signal component losses due to mode duplicating and mix-"
        },
        {
          "∗corresponding authors": "ing. In contrast, the current study proposes VGG-optiVMD, an empow-"
        },
        {
          "∗corresponding authors": "ered variational mode decomposition algorithm, to distinguish meaning-"
        },
        {
          "∗corresponding authors": "ful speech features and automatically select the number of decomposed"
        },
        {
          "∗corresponding authors": "modes and optimum balancing parameter for the data fidelity constraint"
        },
        {
          "∗corresponding authors": "by assessing their effects on the VGG16 flattening output layer. Various"
        },
        {
          "∗corresponding authors": "feature vectors were employed to train the VGG16 network on differ-"
        },
        {
          "∗corresponding authors": "ent databases and assess VGG-optiVMD reproducibility and reliability."
        },
        {
          "∗corresponding authors": "One, two, and three-dimensional feature vectors were constructed by con-"
        },
        {
          "∗corresponding authors": "catenating Mel-frequency cepstral coefficients, Chromagram, Mel\nspec-"
        },
        {
          "∗corresponding authors": "trograms, Tonnetz diagrams, and spectral centroids. Results confirmed"
        },
        {
          "∗corresponding authors": "a synergistic relationship between the fine-tuning of\nthe signal\nsample"
        },
        {
          "∗corresponding authors": "rate and decomposition parameters with classification accuracy, achiev-"
        },
        {
          "∗corresponding authors": "ing state-of-the-art 96.09% accuracy in predicting seven emotions on the"
        },
        {
          "∗corresponding authors": "Berlin EMO-DB database."
        },
        {
          "∗corresponding authors": "Keywords: Speech emotion recognition (SER) · Variational mode de-"
        },
        {
          "∗corresponding authors": "composition (VMD) · Convolutional neural network (CNN) · Sound sig-"
        },
        {
          "∗corresponding authors": "nal processing · Acoustic features"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2\nD. Hason Rudd et al.": "modifying facial\nexpressions and vocal\ntone. Thus,\nchanging voice\ncharacter-"
        },
        {
          "2\nD. Hason Rudd et al.": "istics\nis how most humans express different emotions\n[36]. Consequently, con-"
        },
        {
          "2\nD. Hason Rudd et al.": "siderable human-computer interaction research has considered emotion recogni-"
        },
        {
          "2\nD. Hason Rudd et al.": "tion (ER). Various applications detect serious state by analyzing caller emotion"
        },
        {
          "2\nD. Hason Rudd et al.": "in emergency centers; and speech pathology,\ne-learning, voiceprints,\nsecurity,"
        },
        {
          "2\nD. Hason Rudd et al.": "and other smart-centric services commonly employ speech emotion recognition"
        },
        {
          "2\nD. Hason Rudd et al.": "(SER). Other approaches have considered biosensing, Electroencephalography"
        },
        {
          "2\nD. Hason Rudd et al.": "(EEG), and facial recognition, to detect emotions [2,20,11]."
        },
        {
          "2\nD. Hason Rudd et al.": "Signal based ER employs various\nsignals,\nincluding electrodermal activity,"
        },
        {
          "2\nD. Hason Rudd et al.": "blood volume pulse, galvanic\nskin response,\nelectrocardiogram (ECG), EEG,"
        },
        {
          "2\nD. Hason Rudd et al.": "and speech, are commonly categorized into several decomposed modes due to"
        },
        {
          "2\nD. Hason Rudd et al.": "the complexity and nonstationary nature of\nthem, which allows\nlatent\nfactors"
        },
        {
          "2\nD. Hason Rudd et al.": "and patterns to be extracted more easily. Several time series analysis approaches"
        },
        {
          "2\nD. Hason Rudd et al.": "for SER have been over the previous two decades, extracting relevant speech fea-"
        },
        {
          "2\nD. Hason Rudd et al.": "tures from nonstationary and instantaneous signals,\nincluding traditional short"
        },
        {
          "2\nD. Hason Rudd et al.": "time Fourier\ntransform (STFTs),\nempirical wavelet\ntransforms\n(EWTs)\n[13],"
        },
        {
          "2\nD. Hason Rudd et al.": "and variational mode decomposition (VMD) [12]. Nonstationary signal proper-"
        },
        {
          "2\nD. Hason Rudd et al.": "ties and its components make mean STFTs are not always\nsuitable, and pre-"
        },
        {
          "2\nD. Hason Rudd et al.": "vious\nstudies have mostly considered these approaches\nin isolation [8]. Huang"
        },
        {
          "2\nD. Hason Rudd et al.": "et al.\n[17] proposed empirical mode decomposition (EMD), which decomposes"
        },
        {
          "2\nD. Hason Rudd et al.": "the source signal\ninto an unknown number of signal modes defined by frequency"
        },
        {
          "2\nD. Hason Rudd et al.": "and amplitude modulated components. However, EMD has several\nlimitations,"
        },
        {
          "2\nD. Hason Rudd et al.": "including overlapping intrinsic mode\nfunctions\n(IMFs); and increased compu-"
        },
        {
          "2\nD. Hason Rudd et al.": "tational\nload when analyzing a large number of modes, particularly EEG and"
        },
        {
          "2\nD. Hason Rudd et al.": "speech signals [8]."
        },
        {
          "2\nD. Hason Rudd et al.": "Empirical wave transforms employ an adaptive wavelet subdivision scheme,"
        },
        {
          "2\nD. Hason Rudd et al.": "similar to EMD, to address EMD drawbacks by decomposing the signal\ninto a"
        },
        {
          "2\nD. Hason Rudd et al.": "predetermined number of IMFs or modes. Several studies have proposed an enve-"
        },
        {
          "2\nD. Hason Rudd et al.": "lope weighted transformation to decompose and denoise EEG and speech signals"
        },
        {
          "2\nD. Hason Rudd et al.": "for processing [6,35]. Variational mode decomposition employs non-recursive de-"
        },
        {
          "2\nD. Hason Rudd et al.": "composition to deal with nonlinear and nonstationary signals. In contrast with"
        },
        {
          "2\nD. Hason Rudd et al.": "EWT and EMD, few studies have considered VMD to analyze EEG signals. VMD"
        },
        {
          "2\nD. Hason Rudd et al.": "decomposes signals into modes with a narrowband around a center frequency and"
        },
        {
          "2\nD. Hason Rudd et al.": "can overcome EWT limitations,\nincluding shift and filter bank boundary sensi-"
        },
        {
          "2\nD. Hason Rudd et al.": "tivity and EMD mode mixing effects. Therefore, we were motivated to apply"
        },
        {
          "2\nD. Hason Rudd et al.": "VMD for speech signal processing."
        },
        {
          "2\nD. Hason Rudd et al.": "Acoustic feature selection is essential\nfor SER to describe various voice sig-"
        },
        {
          "2\nD. Hason Rudd et al.": "nal aspects captured from different features [5,11,20]. Acoustic features include"
        },
        {
          "2\nD. Hason Rudd et al.": "time-frequency, time, and frequency domain representations. Extracted features"
        },
        {
          "2\nD. Hason Rudd et al.": "from time-frequency domains carry more informative data than the other do-"
        },
        {
          "2\nD. Hason Rudd et al.": "mains, and better capture latent emotion content from speech signals [33]. Useful"
        },
        {
          "2\nD. Hason Rudd et al.": "time-domain features include amplitude envelope, RMS energy, and zero-crossing"
        },
        {
          "2\nD. Hason Rudd et al.": "rate; and are commonly employed as sequence evaluation ratios. In contrast, rel-"
        },
        {
          "2\nD. Hason Rudd et al.": "evant frequency domain features include band energy ratio (BER), spectral cen-"
        },
        {
          "2\nD. Hason Rudd et al.": "troid, and spectral flux. Several previous studies used VMD method to analyze"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Title Suppressed Due to Excessive Length": "",
          "3": ""
        },
        {
          "Title Suppressed Due to Excessive Length": "",
          "3": ""
        },
        {
          "Title Suppressed Due to Excessive Length": "",
          "3": ""
        },
        {
          "Title Suppressed Due to Excessive Length": "",
          "3": ""
        },
        {
          "Title Suppressed Due to Excessive Length": "",
          "3": ""
        },
        {
          "Title Suppressed Due to Excessive Length": "The main contributions from this study can be summarized as follows.",
          "3": ""
        },
        {
          "Title Suppressed Due to Excessive Length": "– To our best knowledge, this study is the first to employ VMD as a dynamic",
          "3": ""
        },
        {
          "Title Suppressed Due to Excessive Length": "",
          "3": ""
        },
        {
          "Title Suppressed Due to Excessive Length": "– The proposed VGG-optiVMD algorithm automatically selects optimum de-",
          "3": ""
        },
        {
          "Title Suppressed Due to Excessive Length": "",
          "3": ""
        },
        {
          "Title Suppressed Due to Excessive Length": "– A reliable emotion classifier was achieved with state-of-art result 96.09% test",
          "3": ""
        },
        {
          "Title Suppressed Due to Excessive Length": "",
          "3": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4\nD. Hason Rudd et al.": "Generally, EEG signals can effectively analyze individual’s emotion since they"
        },
        {
          "4\nD. Hason Rudd et al.": "are subject dependent. Pandey [29] proposed subject-independent emotion recog-"
        },
        {
          "4\nD. Hason Rudd et al.": "nition using VMD and deep neural networks\n(VMD-DNN) on the benchmark"
        },
        {
          "4\nD. Hason Rudd et al.": "DEAP dataset. Two features, first difference and power-spectral-density used"
        },
        {
          "4\nD. Hason Rudd et al.": "since were sufficient\nto recognize calm, happy,\nsad, and angry emotions. SVM"
        },
        {
          "4\nD. Hason Rudd et al.": "and DNN classifier accuracy was improved by employing VMD based feature ex-"
        },
        {
          "4\nD. Hason Rudd et al.": "traction compared with EMD, STFT, and differential entropy feature extraction,"
        },
        {
          "4\nD. Hason Rudd et al.": "achieving 61.25% for arousal and 62.50% valence prediction accuracies."
        },
        {
          "4\nD. Hason Rudd et al.": "Several previous\nstudies considered STFT signal decomposition techniques"
        },
        {
          "4\nD. Hason Rudd et al.": "for SER. For example, Zhao et.\n[42] achieved robust 91.89% accuracy on the"
        },
        {
          "4\nD. Hason Rudd et al.": "EMODB database\n[7]. Few previous\nstudies\nconsidered VMD to decompose"
        },
        {
          "4\nD. Hason Rudd et al.": "speech signals as mostly employed EEG signal\nfor ER. Dendukuri\n[11] achieved"
        },
        {
          "4\nD. Hason Rudd et al.": "69.13% accuracy to recognize four emotions on the RAVDESS database. How-"
        },
        {
          "4\nD. Hason Rudd et al.": "ever, to the best of our knowledge, the current study is the first to employ VMD"
        },
        {
          "4\nD. Hason Rudd et al.": "to enrich multidimensional feature vectors to enhance VGG-16 network learning."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Title Suppressed Due to Excessive Length\n5": "The VMD algorithm applies the Wiener filter, Hilbert transform, analytical"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "signals, and frequency mixing. Wiener filters are narrowband filters\nfor noise"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "reduction. The Hilbert transform is a time-invariant multiplier, convolving the"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "original\nsignal g(t) with the impulse response 1/πt\n[22]. Therefore,\nit converts"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "the real\nsignal\ninto the complex or\nimaginary part\nto extract magnitude and"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "phase angle time series for frequencies with the most power at each specific time"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "point. The VMD algorithm adds\nthe Hilbert\ntransform H[g(t)]\nto the original"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "signal g(t), removing any negative frequencies present (due to Hermitian sym-"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "metry). The two main VMD objects are to constrain the bandwidth for each"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "IMF center\nfrequency and reconstruct\nthe original\nsignal\nfrom the sum of all"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "modes. First,\nthe Hilbert\ntransform filters\nfrequencies\nin the negative\nside of"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "the spectrum, and then shifts the obtained bandwidth to the modes central\nfre-"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "quency. Second, the obtained spectrum is shifted to the baseband region via a"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "modulator\nfunction to obtain bandwidth around central\nfrequency ω. Finally,"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "H1 Gaussian smoothness\nfor\nthe demodulation signal\nis used to estimate the"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "bandwidth. Thus, constraining the L2 norm squared gradient\n[12] defines\nthe"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "optimization problem (1),"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6\nD. Hason Rudd et al.": "be updated in the spectral domain,"
        },
        {
          "6\nD. Hason Rudd et al.": "λn(ω)"
        },
        {
          "6\nD. Hason Rudd et al.": "g(ω) − (cid:80)\n(ω) − (cid:80)"
        },
        {
          "6\nD. Hason Rudd et al.": "i (ω) +\ni<k ˆgn+1\ni>k ˆgn"
        },
        {
          "6\nD. Hason Rudd et al.": "2\n.\n(3)\ngn+1\n(ω) ←"
        },
        {
          "6\nD. Hason Rudd et al.": "k"
        },
        {
          "6\nD. Hason Rudd et al.": "1 + 2α (ω − ωn\nk )2"
        },
        {
          "6\nD. Hason Rudd et al.": "Updating is obtained using the Wiener filter\nfor\nthe current\nresidual using"
        },
        {
          "6\nD. Hason Rudd et al.": "to restrain variation across\nthe central\nfrequency\nthe signal prior 1/(ω − ωk)2"
        },
        {
          "6\nD. Hason Rudd et al.": "minimum, providing the updated mode center frequency ωk as"
        },
        {
          "6\nD. Hason Rudd et al.": "2"
        },
        {
          "6\nD. Hason Rudd et al.": "(cid:12)(cid:12)(cid:12)\n(cid:12)(cid:12)(cid:12)\n(cid:82) ∞\nˆ"
        },
        {
          "6\nD. Hason Rudd et al.": "ω\ndω\nGk(ω)"
        },
        {
          "6\nD. Hason Rudd et al.": "0"
        },
        {
          "6\nD. Hason Rudd et al.": ",\n(4)\nωn+1\n="
        },
        {
          "6\nD. Hason Rudd et al.": "k\n2"
        },
        {
          "6\nD. Hason Rudd et al.": "(cid:12)(cid:12)(cid:12)\n(cid:12)(cid:12)(cid:12)\n(cid:82) ∞\nˆ"
        },
        {
          "6\nD. Hason Rudd et al.": "dω\nGk(ω)"
        },
        {
          "6\nD. Hason Rudd et al.": "0"
        },
        {
          "6\nD. Hason Rudd et al.": "where\n(t).\nGk(ω) is the Fourier transform for gn+1"
        },
        {
          "6\nD. Hason Rudd et al.": "k"
        },
        {
          "6\nD. Hason Rudd et al.": "A better decomposed signal can be obtained by reconstructing the original"
        },
        {
          "6\nD. Hason Rudd et al.": "signal as the sum of modes and estimating bandwidth using the Wiener filter."
        },
        {
          "6\nD. Hason Rudd et al.": "Details for the VMD algorithm are provided in [12]."
        },
        {
          "6\nD. Hason Rudd et al.": "To leverage VMD effectiveness, we propose the VGG-optiVMD algorithm for"
        },
        {
          "6\nD. Hason Rudd et al.": "automatically selecting optimum α and K by analyzing different decomposition"
        },
        {
          "6\nD. Hason Rudd et al.": "parameter effects on classification accuracy."
        },
        {
          "6\nD. Hason Rudd et al.": "3.3\nProposed VGG-optiVMD"
        },
        {
          "6\nD. Hason Rudd et al.": "Reconstruction error\nfor a decomposed signal can be reduced by selecting op-"
        },
        {
          "6\nD. Hason Rudd et al.": "timum K and α.\nImproper decomposition parameter\nselection will create du-"
        },
        {
          "6\nD. Hason Rudd et al.": "plicate modes,\ncausing signal\ninformation losses and hence\nreduced classifier"
        },
        {
          "6\nD. Hason Rudd et al.": "performance. One drawback for VMD is\nthat finding decomposition parame-"
        },
        {
          "6\nD. Hason Rudd et al.": "ters K and α to provide optimum performance challenging. Several approaches"
        },
        {
          "6\nD. Hason Rudd et al.": "have proposed for ER using ECG, EEG and vibrational signals. For example, the"
        },
        {
          "6\nD. Hason Rudd et al.": "OVMD algorithm [25] uses a series of indicators, including permutation entropy,"
        },
        {
          "6\nD. Hason Rudd et al.": "kurtosis criteria, extreme frequency domain value, and energy loss coefficients,"
        },
        {
          "6\nD. Hason Rudd et al.": "to identify optimum K. Wang et al.\n[38] controlled power spectral and dynamic"
        },
        {
          "6\nD. Hason Rudd et al.": "entropy features\nto find optimal K and α to decompose vibration signal and"
        },
        {
          "6\nD. Hason Rudd et al.": "extract fault features."
        },
        {
          "6\nD. Hason Rudd et al.": "However, these approaches use IMF or mode characteristics to find the best"
        },
        {
          "6\nD. Hason Rudd et al.": "decomposition parameters for specific low amplitude input signals with empiri-"
        },
        {
          "6\nD. Hason Rudd et al.": "cal threshold selection, which is not applicable for speech signal processing. Den-"
        },
        {
          "6\nD. Hason Rudd et al.": "dukuri et al.\n[11] decomposed speech signals using five modes to recognize eight"
        },
        {
          "6\nD. Hason Rudd et al.": "emotions, achieving 61.2% accuracy on the RAVDESS database. They combined"
        },
        {
          "6\nD. Hason Rudd et al.": "different features,\nincluding a 45-dimensional\nfeature set including mode center"
        },
        {
          "6\nD. Hason Rudd et al.": "frequency,\nstatistical values\nfor mode\ncenter\nfrequency, MFCCs, and spectral"
        },
        {
          "6\nD. Hason Rudd et al.": "statistical\nfeatures to improve classifier performance."
        },
        {
          "6\nD. Hason Rudd et al.": "The above methods evaluate optimum K value using statistical\nfeatures and"
        },
        {
          "6\nD. Hason Rudd et al.": "indicators\nfor guidance.\nIn particular,\nidentified mode number correctness was"
        },
        {
          "6\nD. Hason Rudd et al.": "not verified or fine-tuned practically by monitoring classification accuracy."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Title Suppressed Due to Excessive Length\n7": "In contrast, the current study proposes to automate optimum VMD decom-"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "position parameter selection using a feedback loop from the VGG16 flattening"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "output layer. Algorithm 1 shows the proposed optimized VMD algorithm (VGG-"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "optiVMD). The key strength for VGG-optiVMD is\nreliability, generality, and"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "reproducibility across different speech databases for real-world applications, e.g."
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "customer satisfaction analysis in call centers."
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "3.4\nFeature scaling, data augmentation, and emotion classification"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "Figure 1 shows\nthe proposed framework to train CNN-VGG16 [34]\nto extract"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "enriched feature vectors and classify seven emotions: anger, boredom, happiness,"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "neutral, disgust,\nsadness, and fear on two databases EMODB and RAVDESS."
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "Figure 1 shows the model development proceeds as follows."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 1: Proposed model development workflow: extracted features are enriched": "using the VGG-optiVMD to automatically identify K and α."
        },
        {
          "Fig. 1: Proposed model development workflow: extracted features are enriched": "1. The voice signal is sampled at 88400 Hz and five well-known acoustic features"
        },
        {
          "Fig. 1: Proposed model development workflow: extracted features are enriched": "extracted in the time-frequency domain: MFCCs, Mel spectrogram, Tonnetz,"
        },
        {
          "Fig. 1: Proposed model development workflow: extracted features are enriched": "spectral contrast, and chromagram."
        },
        {
          "Fig. 1: Proposed model development workflow: extracted features are enriched": "2. The Hann window function is applied with 2.9 s fixed length and 0.4 ms shift-"
        },
        {
          "Fig. 1: Proposed model development workflow: extracted features are enriched": "ing time to sub-signal spectra assembled over a series of\nframes, extracted"
        },
        {
          "Fig. 1: Proposed model development workflow: extracted features are enriched": "features are reshaped into a single (128 × 128 × 3) feature vector."
        },
        {
          "Fig. 1: Proposed model development workflow: extracted features are enriched": "3. The SMOTE [26] oversampling strategy is applied to compensate minor-"
        },
        {
          "Fig. 1: Proposed model development workflow: extracted features are enriched": "ity classes and reduce model bias. Final\ntesting and training features are"
        },
        {
          "Fig. 1: Proposed model development workflow: extracted features are enriched": "randomly partitioned into 20% and 80% sets, respectively."
        },
        {
          "Fig. 1: Proposed model development workflow: extracted features are enriched": "4. The proposed VGG-optiVMD algorithm is applied to decode frequency sta-"
        },
        {
          "Fig. 1: Proposed model development workflow: extracted features are enriched": "tistical properties at specific times that distinguish emotions within the fea-"
        },
        {
          "Fig. 1: Proposed model development workflow: extracted features are enriched": "ture vector."
        },
        {
          "Fig. 1: Proposed model development workflow: extracted features are enriched": "5. The VGG network is\ntrained on the augmented feature vector\nto classify"
        },
        {
          "Fig. 1: Proposed model development workflow: extracted features are enriched": "emotions into seven classes."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 1: , there is",
      "data": [
        {
          "8\nD. Hason Rudd et al.": "4\nExperiment Setup"
        },
        {
          "8\nD. Hason Rudd et al.": "This study followed the preprocessing system from [33]. All acoustic features were"
        },
        {
          "8\nD. Hason Rudd et al.": "extracted using the Librosa tool\n[27] using the Ryerson Audio Visual Database"
        },
        {
          "8\nD. Hason Rudd et al.": "of Emotional Speech and Song (RAVDESS)\n[26] and the Berlin EMODB [7]"
        },
        {
          "8\nD. Hason Rudd et al.": "databases. Voice data are preprocessed with frame size = 2048, HOP length ="
        },
        {
          "8\nD. Hason Rudd et al.": "256, and sampling rate = 88400 to avoid spectral\nleakage and enhance frequency"
        },
        {
          "8\nD. Hason Rudd et al.": "resolution. Several\nexperiments were performed on nine different\nfeature vec-"
        },
        {
          "8\nD. Hason Rudd et al.": "tors to identify the proposed VGG-optiVMD algorithm effectiveness using. The"
        },
        {
          "8\nD. Hason Rudd et al.": "model was implemented on a Keras framework. The detail of network implemen-"
        },
        {
          "8\nD. Hason Rudd et al.": "tations are available in our GitHub repository3."
        },
        {
          "8\nD. Hason Rudd et al.": "4.1\nModelling"
        },
        {
          "8\nD. Hason Rudd et al.": "The aim of modeling was to enhance informative data within the feature vectors"
        },
        {
          "8\nD. Hason Rudd et al.": "and avoid overfitting. Therefore, we applied data augmentation by decomposing"
        },
        {
          "8\nD. Hason Rudd et al.": "the feature vector data,\ni.e., g(t) is explained in proposed algorithm,\ninto differ-"
        },
        {
          "8\nD. Hason Rudd et al.": "ent modes. Augmentation effects on classification accuracy were assessed using"
        },
        {
          "8\nD. Hason Rudd et al.": "diverse K and α sets. Optimal K and α was assessed iteratively until\nrobust"
        },
        {
          "8\nD. Hason Rudd et al.": "classification accuracy was achieved or the break loop condition reached. K and"
        },
        {
          "8\nD. Hason Rudd et al.": "α were set to a wide range of 3–8 and 1000–6000, respectively, based on empirical"
        },
        {
          "8\nD. Hason Rudd et al.": "experiments since there was no significant improvement in prediction accuracy"
        },
        {
          "8\nD. Hason Rudd et al.": "outside those ranges. The VGG16 is selected to be trained from augmented fea-"
        },
        {
          "8\nD. Hason Rudd et al.": "ture maps as a trade-off between model\nruntime and classifier accuracy. The"
        },
        {
          "8\nD. Hason Rudd et al.": "VGG16 architecture used the ADAM optimizer with learning rate = 0.0001; six"
        },
        {
          "8\nD. Hason Rudd et al.": "fully connected hidden layers with ReLU, SELU, and TanH activation functions;"
        },
        {
          "8\nD. Hason Rudd et al.": "epochs = 50, batch size = 4; and SoftMax function for the output layer."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 1: demonstrates the high performance of the classification",
      "data": [
        {
          "Fig. 2: Empirical results of emotion classification accuracy (%) are demonstrated": "through different sets of decomposition parameters α and K, that were selected"
        },
        {
          "Fig. 2: Empirical results of emotion classification accuracy (%) are demonstrated": "automatically by the VGG-optiVMD algorithm."
        },
        {
          "Fig. 2: Empirical results of emotion classification accuracy (%) are demonstrated": "to justify the power of\nthe VGG-optiVMD in SER. Therefore, we attempted"
        },
        {
          "Fig. 2: Empirical results of emotion classification accuracy (%) are demonstrated": "to evaluate the model performance through variation of\nsample rate, window"
        },
        {
          "Fig. 2: Empirical results of emotion classification accuracy (%) are demonstrated": "size, K and α without using VMD (baseline model) and with VMD (proposed"
        },
        {
          "Fig. 2: Empirical results of emotion classification accuracy (%) are demonstrated": "model). As shown in Figure 3, unlike the baseline model, the proposed model per-"
        },
        {
          "Fig. 2: Empirical results of emotion classification accuracy (%) are demonstrated": "formed better with a larger sampling rate and window size. Moreover, the highest"
        },
        {
          "Fig. 2: Empirical results of emotion classification accuracy (%) are demonstrated": "test accuracy and F1 score were obtained via VGG-optiVMD, proving that our"
        },
        {
          "Fig. 2: Empirical results of emotion classification accuracy (%) are demonstrated": "VMD-based feature augmentation method significantly improved the classifica-"
        },
        {
          "Fig. 2: Empirical results of emotion classification accuracy (%) are demonstrated": "tion accuracy. The Figure 4 shows the efficient functionality of VGG-optiVMD"
        },
        {
          "Fig. 2: Empirical results of emotion classification accuracy (%) are demonstrated": "on the feature vector 3D-Mel Spectrogram+MFCCs+Chromagram. Figure (a)"
        },
        {
          "Fig. 2: Empirical results of emotion classification accuracy (%) are demonstrated": "represents the feature before applying VMD based data augmentation, and fig-"
        },
        {
          "Fig. 2: Empirical results of emotion classification accuracy (%) are demonstrated": "ure (b) clearly shows that the informative frequencies are distinguished on the"
        },
        {
          "Fig. 2: Empirical results of emotion classification accuracy (%) are demonstrated": "feature vector after applying the data augmentation method.\nIn addition,\nthe"
        },
        {
          "Fig. 2: Empirical results of emotion classification accuracy (%) are demonstrated": "image shows the feature vector acquired higher distinction energies in the time-"
        },
        {
          "Fig. 2: Empirical results of emotion classification accuracy (%) are demonstrated": "frequency domain. Therefore,\nthe implications of\nthis finding can improve the"
        },
        {
          "Fig. 2: Empirical results of emotion classification accuracy (%) are demonstrated": "learning process in VGG16 and result in better prediction accuracy. The confu-"
        },
        {
          "Fig. 2: Empirical results of emotion classification accuracy (%) are demonstrated": "sion matrix in Table 1 demonstrates the high performance of the classification"
        },
        {
          "Fig. 2: Empirical results of emotion classification accuracy (%) are demonstrated": "model with accuracy above 90% for all classes. Nevertheless, the model performs"
        },
        {
          "Fig. 2: Empirical results of emotion classification accuracy (%) are demonstrated": "poorly when predicting happiness and anger emotions due to the similarity of"
        },
        {
          "Fig. 2: Empirical results of emotion classification accuracy (%) are demonstrated": "signal attributes such as intensity,\nfrequency and harmonic structure."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 1: Visualization of the model performance with confusion matrix (%) for",
      "data": [
        {
          "Table 1: Visualization of the model performance with confusion matrix (%) for": "the 3D-Mel Spectrogram+MFCCs+Chromagram with test accuracy = %96.09"
        },
        {
          "Table 1: Visualization of the model performance with confusion matrix (%) for": "on the Berlin EMO-DB dataset."
        },
        {
          "Table 1: Visualization of the model performance with confusion matrix (%) for": "Emotion:"
        },
        {
          "Table 1: Visualization of the model performance with confusion matrix (%) for": "Anger"
        },
        {
          "Table 1: Visualization of the model performance with confusion matrix (%) for": "Boredom"
        },
        {
          "Table 1: Visualization of the model performance with confusion matrix (%) for": "Disgust"
        },
        {
          "Table 1: Visualization of the model performance with confusion matrix (%) for": "Fear"
        },
        {
          "Table 1: Visualization of the model performance with confusion matrix (%) for": "Happiness"
        },
        {
          "Table 1: Visualization of the model performance with confusion matrix (%) for": "Neutral"
        },
        {
          "Table 1: Visualization of the model performance with confusion matrix (%) for": "Sadness"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 2: Comparison of the proposed method with previous works on the same",
      "data": [
        {
          "Fig. 4: The efficient\nfunctionality of VGG-optiVMD on the feature vector 3D-": "Mel Spectrogram+MFCCs+Chromagram clearly shows a higher distinction in"
        },
        {
          "Fig. 4: The efficient\nfunctionality of VGG-optiVMD on the feature vector 3D-": "the energy magnitudes of\nfrequencies in (b)."
        },
        {
          "Fig. 4: The efficient\nfunctionality of VGG-optiVMD on the feature vector 3D-": "Table 2: Comparison of the proposed method with previous works on the same"
        },
        {
          "Fig. 4: The efficient\nfunctionality of VGG-optiVMD on the feature vector 3D-": "databases."
        },
        {
          "Fig. 4: The efficient\nfunctionality of VGG-optiVMD on the feature vector 3D-": "Method proposed by Feature extraction strategy\nLearning Net.\nAcc(%)"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 2: Comparison of the proposed method with previous works on the same",
      "data": [
        {
          "Table 2: Comparison of the proposed method with previous works on the same": "databases."
        },
        {
          "Table 2: Comparison of the proposed method with previous works on the same": "Method proposed by Feature extraction strategy"
        },
        {
          "Table 2: Comparison of the proposed method with previous works on the same": "Badshah et al.\n[3]"
        },
        {
          "Table 2: Comparison of the proposed method with previous works on the same": "Dendukuri et al."
        },
        {
          "Table 2: Comparison of the proposed method with previous works on the same": "Zamil et al.\n[40]"
        },
        {
          "Table 2: Comparison of the proposed method with previous works on the same": "Popova et al.\n[31]"
        },
        {
          "Table 2: Comparison of the proposed method with previous works on the same": "Hajarol. et al.\n[14]"
        },
        {
          "Table 2: Comparison of the proposed method with previous works on the same": "Wang et al.\n[37]"
        },
        {
          "Table 2: Comparison of the proposed method with previous works on the same": "Kown et al.\n[23]"
        },
        {
          "Table 2: Comparison of the proposed method with previous works on the same": "Badsha et al.\n[4]"
        },
        {
          "Table 2: Comparison of the proposed method with previous works on the same": "Huang et al.\n[18]"
        },
        {
          "Table 2: Comparison of the proposed method with previous works on the same": "Issa et al.\n[19]"
        },
        {
          "Table 2: Comparison of the proposed method with previous works on the same": "Meng et al.\n[28]"
        },
        {
          "Table 2: Comparison of the proposed method with previous works on the same": "Wu et al.\n[39]"
        },
        {
          "Table 2: Comparison of the proposed method with previous works on the same": "Rudd et al.\n[33]"
        },
        {
          "Table 2: Comparison of the proposed method with previous works on the same": "Demircan et al.\n[10]"
        },
        {
          "Table 2: Comparison of the proposed method with previous works on the same": "Zhao et al.\n[42]"
        },
        {
          "Table 2: Comparison of the proposed method with previous works on the same": "VGG-optiVMD"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "12": "",
          "D. Hason Rudd et al.": "The findings provide solid empirical confirmation of the key role of the sam-"
        },
        {
          "12": "pling rate, the number of the decomposed mode, K and the balancing parameter",
          "D. Hason Rudd et al.": ""
        },
        {
          "12": "of",
          "D. Hason Rudd et al.": "the data-fidelity constraint, α,"
        },
        {
          "12": "",
          "D. Hason Rudd et al.": "Taken together, these findings suggest that VMD decomposition parameters K"
        },
        {
          "12": "",
          "D. Hason Rudd et al.": "(2-6) and α (2000-6000) are optimum values on both the RAVDESS and EMODB"
        },
        {
          "12": "",
          "D. Hason Rudd et al.": "databases. The proposed VGG-optiVMD algorithm improved the emotion clas-"
        },
        {
          "12": "",
          "D. Hason Rudd et al.": "sification to a state-of-the-art result with a test accuracy of 96.09% in the Berlin"
        },
        {
          "12": "",
          "D. Hason Rudd et al.": "EMO-DB and 86.21% in the RAVDESS datasets. Further work needs to be done"
        },
        {
          "12": "",
          "D. Hason Rudd et al.": "to establish whether extracting acoustic features only from informative decom-"
        },
        {
          "12": "",
          "D. Hason Rudd et al.": "posed modes can reduce computational"
        },
        {
          "12": "",
          "D. Hason Rudd et al.": "should be repeated using the VMD algorithm before acoustic feature extraction"
        },
        {
          "12": "process.",
          "D. Hason Rudd et al.": ""
        },
        {
          "12": "References",
          "D. Hason Rudd et al.": ""
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "process.": "References"
        },
        {
          "process.": "1. Aizawa, K., Nakamura, Y.,\nSatoh,\nS.: Advances\nin Multimedia\nInformation"
        },
        {
          "process.": "Processing-PCM 2004: 5th Pacific Rim Conference on Multimedia, Tokyo, Japan,"
        },
        {
          "process.": "November 30-December 3, 2004, Proceedings, Part II, vol. 3332. Springer (2004)"
        },
        {
          "process.": "2. Alshamsi, H., Kepuska, V., Alshamsi, H., Meng, H.: Automated facial expression"
        },
        {
          "process.": "and speech emotion recognition app development on smart phones using cloud"
        },
        {
          "process.": "computing.\nIn: 2018 IEEE 9th Annual\nInformation Technology, Electronics and"
        },
        {
          "process.": "Mobile Communication Conference (IEMCON). pp. 730–738. IEEE (2018)"
        },
        {
          "process.": "3. Badshah, A.M., Ahmad, J., Rahim, N., Baik, S.W.: Speech emotion recognition"
        },
        {
          "process.": "from spectrograms with deep convolutional neural network. In: 2017 international"
        },
        {
          "process.": "conference on platform technology and service (PlatCon). pp. 1–5 (2017)"
        },
        {
          "process.": "4. Badshah, A.M., Rahim, N., Ullah: Deep features-based speech emotion recognition"
        },
        {
          "process.": "for smart affective services. Multimedia Tools and Applications 78(5), 5571–5589"
        },
        {
          "process.": "(2019)"
        },
        {
          "process.": "5. Basharirad, B., Moradhaseli, M.: Speech emotion recognition methods: A literature"
        },
        {
          "process.": "review. In: AIP Conference Proceedings. vol. 1891, p. 020105. AIP Publishing LLC"
        },
        {
          "process.": "(2017)"
        },
        {
          "process.": "6. Bhattacharyya, A., Sharma, M., Pachori, R.B., Sircar, P., Acharya, U.R.: A novel"
        },
        {
          "process.": "approach for automated detection of focal eeg signals using empirical wavelet trans-"
        },
        {
          "process.": "form. Neural Computing and Applications 29(8), 47–57 (2018)"
        },
        {
          "process.": "7. Burkhardt, F., Paeschke, A., Rolfes, M., Sendlmeier, W.F., Weiss, B., et al.: A"
        },
        {
          "process.": "database of german emotional speech. In: Interspeech. vol. 5, pp. 1517–1520 (2005)"
        },
        {
          "process.": "8. Carvalho, V.R., Moraes, M.F., Braga, A.P., Mendes, E.M.: Evaluating five different"
        },
        {
          "process.": "adaptive decomposition methods for eeg signal seizure detection and classification."
        },
        {
          "process.": "Biomedical Signal Processing and Control 62, 102073 (2020)"
        },
        {
          "process.": "9. Deb, S., Dandapat, S.: Fourier model based features for analysis and classification"
        },
        {
          "process.": "of out-of-breath speech. Speech Communication 90, 1–14 (2017)"
        },
        {
          "process.": "10. Demircan, S., Kahramanli, H.: Application of\nfuzzy c-means clustering algorithm"
        },
        {
          "process.": "to spectral\nfeatures for emotion classification from speech. Neural Computing and"
        },
        {
          "process.": "Applications 29(8), 59–66 (2018)"
        },
        {
          "process.": "11. Dendukuri, L.S., Hussain, S.J.: Emotional speech analysis and classification using"
        },
        {
          "process.": "variational mode decomposition.\nInternational Journal of Speech Technology pp."
        },
        {
          "process.": "1–13 (2022)"
        },
        {
          "process.": "12. Dragomiretskiy, K., Zosso, D.: Variational mode decomposition. IEEE transactions"
        },
        {
          "process.": "on signal processing 62(3), 531–544 (2013)"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Title Suppressed Due to Excessive Length\n13": "13. Gilles, J.: Empirical wavelet\ntransform.\nIEEE transactions on signal processing"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "61(16), 3999–4010 (2013)"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "14. Hajarolasvadi, N., Demirel, H.: 3d cnn-based speech emotion recognition using"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "k-means clustering and spectrograms. Entropy 21(5), 479–495 (2019)"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "15. Harte, C., Sandler, M., Gasser, M.: Detecting harmonic change in musical audio. In:"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "Proceedings of the 1st ACM workshop on Audio and music computing multimedia."
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "pp. 21–26 (2006)"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "16. Hestenes, M.R.: Multiplier and gradient methods. Journal of optimization theory"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "and applications 4(5), 303–320 (1969)"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "17. Huang, N.E., Shen, Z., Long, S.R., Wu, M.C., Shih, H.H., Zheng, Q., Yen, N.C.,"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "Tung, C.C., Liu, H.H.: The empirical mode decomposition and the hilbert\nspec-"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "trum for nonlinear and non-stationary time\nseries analysis. Proceedings of\nthe"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "Royal Society of London. Series A: mathematical, physical and engineering sci-"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "ences 454(1971), 903–995 (1998)"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "18. Huang, Z., Dong, M., Mao, Q., Zhan, Y.: Speech emotion recognition using cnn. In:"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "Proceedings of the 22nd ACM international conference media. pp. 801–804 (2014)"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "19.\nIssa, D., Demirci, M.F., Yazici, A.: Speech emotion recognition with deep convo-"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "lutional neural networks. Biomedical Signal Processing and Control 59, 101894–"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "101904 (2020)"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "20. Khare, S.K., Bajaj, V.: An evolutionary optimized variational mode decomposition"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "for emotion recognition. IEEE Sensors Journal 21(2), 2035–2042 (2020)"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "21. Koelstra, S., Muhl, C., Soleymani, M., Lee, J.S., Yazdani, A., Ebrahimi, T., Pun,"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "T., Nijholt, A., Patras, I.: Deap: A database for emotion analysis; using physiolog-"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "ical signals. IEEE transactions on affective computing 3(1), 18–31 (2011)"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "22. Kschischang, F.R.: The hilbert transform, the edward s. Rogers Sr. Department of"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "Electrical and Computer Engineering, University of Toronto 12 (2006)"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "23. Kwon, S.: A cnn-assisted enhanced audio signal processing for\nspeech emotion"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "recognition. Sensors 20(1),\n183 (2019)"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "24. Lal, G.J., Gopalakrishnan, E., Govind, D.: Epoch estimation from emotional speech"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "signals using variational mode decomposition. Circuits, Systems, and Signal Pro-"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "cessing 37(8), 3245–3274 (2018)"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "25. Lian, J., Liu, Z., Wang, H., Dong, X.: Adaptive variational mode decomposition"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "method for\nsignal processing based on mode characteristic. Mechanical Systems"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "and Signal Processing 107, 53–77 (2018)"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "26. Livingstone, S.R., Russo, F.A.: The\nryerson audio-visual database of\nemotional"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "speech and song (ravdess): A dynamic, multimodal set of\nfacial and vocal expres-"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "sions in north american english. PloS one 13(5), e0196391 (2018)"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "27. McFee, B., Raffel, C., Liang, D., Ellis, D.P., McVicar, M., Battenberg, E., Nieto,"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "O.:\nlibrosa: Audio and music signal analysis in python. In: Proceedings of the 14th"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "python in science conference. vol. 8, pp. 18–25. Citeseer (2015)"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "28. Meng, H., Yan, T., Yuan, F., Wei, H.: Speech emotion recognition from 3d log-mel"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "spectrograms with deep learning network. IEEE access 7, 125868–125881 (2019)"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "29. Pandey, P., Seeja, K.: Subject independent emotion recognition from eeg using vmd"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "and deep learning. Journal of King Saud University-Computer and Information"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "Sciences (2019)"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "30. Pierre-Yves, O.: The production and recognition of emotions\nin speech:\nfeatures"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "and algorithms. International Journal of Human-Computer Studies 59(1-2), 157–"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "183 (2003)"
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "31. Popova, A.S., Rassadin, A.G., Ponomarenko, A.A.: Emotion recognition in sound."
        },
        {
          "Title Suppressed Due to Excessive Length\n13": "In: International Conference on Neuroinformatics. pp. 117–124 (2017)"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "14\nD. Hason Rudd et al.": "32. Rockafellar, R.T.: A dual approach to solving nonlinear programming problems by"
        },
        {
          "14\nD. Hason Rudd et al.": "unconstrained optimization. Mathematical programming 5(1), 354–373 (1973)"
        },
        {
          "14\nD. Hason Rudd et al.": "33. Rudd, D.H., Huo, H., Xu, G.: Leveraged mel\nspectrograms using harmonic and"
        },
        {
          "14\nD. Hason Rudd et al.": "percussive components in speech emotion recognition. In: Pacific-Asia Conference"
        },
        {
          "14\nD. Hason Rudd et al.": "on Knowledge Discovery and Data Mining. pp. 392–404. Springer (2022)"
        },
        {
          "14\nD. Hason Rudd et al.": "34. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,"
        },
        {
          "14\nD. Hason Rudd et al.": "Karpathy, A., Khosla, A., Bernstein, M., et al.: Imagenet large scale visual recog-"
        },
        {
          "14\nD. Hason Rudd et al.": "nition challenge. International\njournal of computer vision 115(3), 211–252 (2015)"
        },
        {
          "14\nD. Hason Rudd et al.": "35. Saxena, S., Hemanth, C., Sangeetha, R.: Classification of normal,\nseizure\nand"
        },
        {
          "14\nD. Hason Rudd et al.": "seizure-free eeg signals using emd and ewt.\nIn: 2017 International Conference on"
        },
        {
          "14\nD. Hason Rudd et al.": "Nextgen Electronic Technologies: Silicon to Software\n(ICNETS2). pp. 360–366."
        },
        {
          "14\nD. Hason Rudd et al.": "IEEE (2017)"
        },
        {
          "14\nD. Hason Rudd et al.": "36. Wang, J.Q., Nicol, T., Skoe, E., Sams, M., Kraus, N.: Emotion and the auditory"
        },
        {
          "14\nD. Hason Rudd et al.": "brainstem response to speech. Neuroscience letters 469(3), 319–323 (2010)"
        },
        {
          "14\nD. Hason Rudd et al.": "37. Wang, K., An, N., Li, B.N., Zhang, Y., Li, L.: Speech emotion recognition using"
        },
        {
          "14\nD. Hason Rudd et al.": "fourier parameters. IEEE Transactions on affective computing 6(1), 69–75 (2015)"
        },
        {
          "14\nD. Hason Rudd et al.": "38. Wang, Z., He, G., Du, W., Zhou, J., Han, X., Wang, J., He, H., Guo, X., Wang,"
        },
        {
          "14\nD. Hason Rudd et al.": "J., Kou, Y.: Application of parameter optimized variational mode decomposition"
        },
        {
          "14\nD. Hason Rudd et al.": "method in fault diagnosis of gearbox. Ieee Access 7, 44871–44882 (2019)"
        },
        {
          "14\nD. Hason Rudd et al.": "39. Wu, S., Falk, T.H., Chan, W.Y.: Automatic speech emotion recognition using mod-"
        },
        {
          "14\nD. Hason Rudd et al.": "ulation spectral\nfeatures. Speech communication 53(5), 768–785 (2011)"
        },
        {
          "14\nD. Hason Rudd et al.": "40. Zamil, A.A.A., Hasan, S., Baki, S.M.J., Adam, J.M., Zaman,\nI.: Emotion detec-"
        },
        {
          "14\nD. Hason Rudd et al.": "tion from speech signals using voting mechanism on classified frames.\nIn: 2019"
        },
        {
          "14\nD. Hason Rudd et al.": "International Conference on Robotics, Electrical and Signal Processing Techniques"
        },
        {
          "14\nD. Hason Rudd et al.": "(ICREST). pp. 281–285. IEEE (2019)"
        },
        {
          "14\nD. Hason Rudd et al.": "41. Zhang, M., Hu, B., Zheng, X., Li, T.: A novel multidimensional\nfeature extraction"
        },
        {
          "14\nD. Hason Rudd et al.": "method based on vmd and wpd for\nemotion recognition.\nIn: 2020 IEEE Inter-"
        },
        {
          "14\nD. Hason Rudd et al.": "national Conference on Bioinformatics and Biomedicine (BIBM). pp. 1216–1220."
        },
        {
          "14\nD. Hason Rudd et al.": "IEEE (2020)"
        },
        {
          "14\nD. Hason Rudd et al.": "42. Zhao, J., Mao, X., Chen, L.: Speech emotion recognition using deep 1d & 2d cnn"
        },
        {
          "14\nD. Hason Rudd et al.": "lstm networks. Biomedical Signal Processing and Control 47, 312–323 (2019)"
        }
      ],
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Advances in Multimedia Information Processing-PCM 2004: 5th Pacific Rim Conference on Multimedia",
      "authors": [
        "K Aizawa",
        "Y Nakamura",
        "S Satoh"
      ],
      "year": "2004",
      "venue": "Advances in Multimedia Information Processing-PCM 2004: 5th Pacific Rim Conference on Multimedia"
    },
    {
      "citation_id": "2",
      "title": "Automated facial expression and speech emotion recognition app development on smart phones using cloud computing",
      "authors": [
        "H Alshamsi",
        "V Kepuska",
        "H Alshamsi",
        "H Meng"
      ],
      "year": "2018",
      "venue": "IEEE 9th Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON)"
    },
    {
      "citation_id": "3",
      "title": "Speech emotion recognition from spectrograms with deep convolutional neural network",
      "authors": [
        "A Badshah",
        "J Ahmad",
        "N Rahim",
        "S Baik"
      ],
      "year": "2017",
      "venue": "2017 international conference on platform technology and service (PlatCon)"
    },
    {
      "citation_id": "4",
      "title": "Ullah: Deep features-based speech emotion recognition for smart affective services",
      "authors": [
        "A Badshah",
        "N Rahim"
      ],
      "year": "2019",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "5",
      "title": "Speech emotion recognition methods: A literature review",
      "authors": [
        "B Basharirad",
        "M Moradhaseli"
      ],
      "year": "2017",
      "venue": "AIP Conference Proceedings"
    },
    {
      "citation_id": "6",
      "title": "A novel approach for automated detection of focal eeg signals using empirical wavelet transform",
      "authors": [
        "A Bhattacharyya",
        "M Sharma",
        "R Pachori",
        "P Sircar",
        "U Acharya"
      ],
      "year": "2018",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "7",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Interspeech"
    },
    {
      "citation_id": "8",
      "title": "Evaluating five different adaptive decomposition methods for eeg signal seizure detection and classification",
      "authors": [
        "V Carvalho",
        "M Moraes",
        "A Braga",
        "E Mendes"
      ],
      "year": "2020",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "9",
      "title": "Fourier model based features for analysis and classification of out-of-breath speech",
      "authors": [
        "S Deb",
        "S Dandapat"
      ],
      "year": "2017",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "10",
      "title": "Application of fuzzy c-means clustering algorithm to spectral features for emotion classification from speech",
      "authors": [
        "S Demircan",
        "H Kahramanli"
      ],
      "year": "2018",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "11",
      "title": "Emotional speech analysis and classification using variational mode decomposition",
      "authors": [
        "L Dendukuri",
        "S Hussain"
      ],
      "year": "2022",
      "venue": "International Journal of Speech Technology"
    },
    {
      "citation_id": "12",
      "title": "Variational mode decomposition",
      "authors": [
        "K Dragomiretskiy",
        "D Zosso"
      ],
      "year": "2013",
      "venue": "IEEE transactions on signal processing"
    },
    {
      "citation_id": "13",
      "title": "Empirical wavelet transform",
      "authors": [
        "J Gilles"
      ],
      "year": "2013",
      "venue": "IEEE transactions on signal processing"
    },
    {
      "citation_id": "14",
      "title": "3d cnn-based speech emotion recognition using k-means clustering and spectrograms",
      "authors": [
        "N Hajarolasvadi",
        "H Demirel"
      ],
      "year": "2019",
      "venue": "Entropy"
    },
    {
      "citation_id": "15",
      "title": "Detecting harmonic change in musical audio",
      "authors": [
        "C Harte",
        "M Sandler",
        "M Gasser"
      ],
      "year": "2006",
      "venue": "Proceedings of the 1st ACM workshop on Audio and music computing multimedia"
    },
    {
      "citation_id": "16",
      "title": "Multiplier and gradient methods",
      "authors": [
        "M Hestenes"
      ],
      "year": "1969",
      "venue": "Journal of optimization theory and applications"
    },
    {
      "citation_id": "17",
      "title": "The empirical mode decomposition and the hilbert spectrum for nonlinear and non-stationary time series analysis",
      "authors": [
        "N Huang",
        "Z Shen",
        "S Long",
        "M Wu",
        "H Shih",
        "Q Zheng",
        "N Yen",
        "C Tung",
        "H Liu"
      ],
      "year": "1971",
      "venue": "Proceedings of the Royal Society of London. Series A: mathematical, physical and engineering sciences"
    },
    {
      "citation_id": "18",
      "title": "Speech emotion recognition using cnn",
      "authors": [
        "Z Huang",
        "M Dong",
        "Q Mao",
        "Y Zhan"
      ],
      "year": "2014",
      "venue": "Proceedings of the 22nd ACM international conference media"
    },
    {
      "citation_id": "19",
      "title": "Speech emotion recognition with deep convolutional neural networks",
      "authors": [
        "D Issa",
        "M Demirci",
        "A Yazici"
      ],
      "year": "2020",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "20",
      "title": "An evolutionary optimized variational mode decomposition for emotion recognition",
      "authors": [
        "S Khare",
        "V Bajaj"
      ],
      "year": "2020",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "21",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "22",
      "title": "The hilbert transform, the edward s. Rogers Sr",
      "authors": [
        "F Kschischang"
      ],
      "year": "2006",
      "venue": "The hilbert transform, the edward s. Rogers Sr"
    },
    {
      "citation_id": "23",
      "title": "A cnn-assisted enhanced audio signal processing for speech emotion recognition",
      "authors": [
        "S Kwon"
      ],
      "year": "2019",
      "venue": "Sensors"
    },
    {
      "citation_id": "24",
      "title": "Epoch estimation from emotional speech signals using variational mode decomposition",
      "authors": [
        "G Lal",
        "E Gopalakrishnan",
        "D Govind"
      ],
      "year": "2018",
      "venue": "Circuits, Systems, and Signal Processing"
    },
    {
      "citation_id": "25",
      "title": "Adaptive variational mode decomposition method for signal processing based on mode characteristic",
      "authors": [
        "J Lian",
        "Z Liu",
        "H Wang",
        "X Dong"
      ],
      "year": "2018",
      "venue": "Mechanical Systems and Signal Processing"
    },
    {
      "citation_id": "26",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "27",
      "title": "librosa: Audio and music signal analysis in python",
      "authors": [
        "B Mcfee",
        "C Raffel",
        "D Liang",
        "D Ellis",
        "M Mcvicar",
        "E Battenberg",
        "O Nieto"
      ],
      "year": "2015",
      "venue": "Proceedings of the 14th python in science conference"
    },
    {
      "citation_id": "28",
      "title": "Speech emotion recognition from 3d log-mel spectrograms with deep learning network",
      "authors": [
        "H Meng",
        "T Yan",
        "F Yuan",
        "H Wei"
      ],
      "year": "2019",
      "venue": "IEEE access"
    },
    {
      "citation_id": "29",
      "title": "Subject independent emotion recognition from eeg using vmd and deep learning",
      "authors": [
        "P Pandey",
        "K Seeja"
      ],
      "year": "2019",
      "venue": "Journal of King Saud University-Computer and Information Sciences"
    },
    {
      "citation_id": "30",
      "title": "The production and recognition of emotions in speech: features and algorithms",
      "authors": [
        "O Pierre-Yves"
      ],
      "year": "2003",
      "venue": "International Journal of Human-Computer Studies"
    },
    {
      "citation_id": "31",
      "title": "Emotion recognition in sound",
      "authors": [
        "A Popova",
        "A Rassadin",
        "A Ponomarenko"
      ],
      "year": "2017",
      "venue": "International Conference on Neuroinformatics"
    },
    {
      "citation_id": "32",
      "title": "A dual approach to solving nonlinear programming problems by unconstrained optimization",
      "authors": [
        "R Rockafellar"
      ],
      "year": "1973",
      "venue": "Mathematical programming"
    },
    {
      "citation_id": "33",
      "title": "Leveraged mel spectrograms using harmonic and percussive components in speech emotion recognition",
      "authors": [
        "D Rudd",
        "H Huo",
        "G Xu"
      ],
      "year": "2022",
      "venue": "Pacific-Asia Conference on Knowledge Discovery and Data Mining"
    },
    {
      "citation_id": "34",
      "title": "Imagenet large scale visual recognition challenge",
      "authors": [
        "O Russakovsky",
        "J Deng",
        "H Su",
        "J Krause",
        "S Satheesh",
        "S Ma",
        "Z Huang",
        "A Karpathy",
        "A Khosla",
        "M Bernstein"
      ],
      "year": "2015",
      "venue": "International journal of computer vision"
    },
    {
      "citation_id": "35",
      "title": "Classification of normal, seizure and seizure-free eeg signals using emd and ewt",
      "authors": [
        "S Saxena",
        "C Hemanth",
        "R Sangeetha"
      ],
      "year": "2017",
      "venue": "2017 International Conference on Nextgen Electronic Technologies: Silicon to Software (ICNETS2)"
    },
    {
      "citation_id": "36",
      "title": "Emotion and the auditory brainstem response to speech",
      "authors": [
        "J Wang",
        "T Nicol",
        "E Skoe",
        "M Sams",
        "N Kraus"
      ],
      "year": "2010",
      "venue": "Neuroscience letters"
    },
    {
      "citation_id": "37",
      "title": "Speech emotion recognition using fourier parameters",
      "authors": [
        "K Wang",
        "N An",
        "B Li",
        "Y Zhang",
        "L Li"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on affective computing"
    },
    {
      "citation_id": "38",
      "title": "Application of parameter optimized variational mode decomposition method in fault diagnosis of gearbox",
      "authors": [
        "Z Wang",
        "G He",
        "W Du",
        "J Zhou",
        "X Han",
        "J Wang",
        "H He",
        "X Guo",
        "J Wang",
        "Y Kou"
      ],
      "year": "2019",
      "venue": "Ieee Access"
    },
    {
      "citation_id": "39",
      "title": "Automatic speech emotion recognition using modulation spectral features",
      "authors": [
        "S Wu",
        "T Falk",
        "W Chan"
      ],
      "year": "2011",
      "venue": "Speech communication"
    },
    {
      "citation_id": "40",
      "title": "Emotion detection from speech signals using voting mechanism on classified frames",
      "authors": [
        "A Zamil",
        "S Hasan",
        "S Baki",
        "J Adam",
        "I Zaman"
      ],
      "year": "2019",
      "venue": "2019 International Conference on Robotics, Electrical and Signal Processing Techniques (ICREST)"
    },
    {
      "citation_id": "41",
      "title": "A novel multidimensional feature extraction method based on vmd and wpd for emotion recognition",
      "authors": [
        "M Zhang",
        "B Hu",
        "X Zheng",
        "T Li"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)"
    },
    {
      "citation_id": "42",
      "title": "Speech emotion recognition using deep 1d & 2d cnn lstm networks",
      "authors": [
        "J Zhao",
        "X Mao",
        "L Chen"
      ],
      "year": "2019",
      "venue": "Biomedical Signal Processing and Control"
    }
  ]
}