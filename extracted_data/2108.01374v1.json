{
  "paper_id": "2108.01374v1",
  "title": "Emopia: A Multi-Modal Pop Piano Dataset For Emotion Recognition And Emotion-Based Music Generation",
  "published": "2021-08-03T08:59:26Z",
  "authors": [
    "Hsiao-Tzu Hung",
    "Joann Ching",
    "Seungheon Doh",
    "Nabin Kim",
    "Juhan Nam",
    "Yi-Hsuan Yang"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "While there are many music datasets with emotion labels in the literature, they cannot be used for research on symbolic-domain music analysis or generation, as there are usually audio files only. In this paper, we present the EMOPIA (pronounced 'yee-mò-pi-uh') dataset, a shared multi-modal (audio and MIDI) database focusing on perceived emotion in pop piano music, to facilitate research on various tasks related to music emotion. The dataset contains 1,087 music clips from 387 songs and clip-level emotion labels annotated by four dedicated annotators. Since the clips are not restricted to one clip per song, they can also be used for song-level analysis. We present the procedure for building the dataset, covering the song list curation, clip selection, and emotion annotation processes. Moreover, we prototype use cases on clip-level music emotion classification and emotion-based symbolic music generation by training and evaluating corresponding models using the dataset. The result demonstrates the potential of EMOPIA for being used in future exploration on piano emotion-related MIR tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The affective aspect of music has been a major subject of research in the field of music information retrieval (MIR), not only for music analysis and labeling  [1] [2] [3] [4] [5] [6] [7] [8] [9] , but also for music generation or editing  [10] [11] [12] [13] [14] . Accordingly, there have been quite a few public music datasets with emotion, as listed in Table  1 . These datasets are different in many ways, including the musical genres considered, data modality and data size, and the way emotion is described.\n\nWith the growing interest in symbolic-domain music analysis and generation in recent years of ISMIR  [15] [16] [17] [18] [19] , it is desirable to have an emotion-labeled symbolic music dataset to add emotion-related elements to such research. However, among the datasets listed in Table  1 , only two provide MIDI data, and they are both small in size. Moreover, the majority of the audio-only datasets contain songs of multiple genres, making it hard to apply automatic music transcription algorithms, which currently work better for piano-only music  [20] [21] [22] [23] , to get MIDI-like data from the audio recordings.\n\nTo address this need, we propose a new emotion-labeled dataset comprising of three main nice properties:\n\n• Single-instrument. We collect audio recordings of piano covers and creations from YouTube, with fair to high audio and musical quality, and a diverse set of playing styles and perceived emotions. Focusing on only piano music allows for the use of piano transcription algorithms  [20, 21] , and facilitates disentanglement of musical composition from variations in timbre, arrangement, and other confounds seen in multi-instrument music.\n\n• Multi-modal. Both the audio and MIDI versions of the music pieces can be found from the Internet (see Section 3.5 for details). The MIDI files are automatically transcribed from the audio by a state-of-the-art model  [21] .\n\n• Clip-level annotation. The audio files downloaded from YouTube are full songs. As different parts of a song may convey different emotions, the first four authors of the paper manually and carefully pick emotion-consistent short clips from each song and label the emotion of these clips using a four-class taxonomy derived from the Russell's valence-arousal model  [32] . This leads to cliplevel emotion annotations for in total 1,087 clips from 387 songs (i.e., 2.78 clips per song on average), with the number of clips per emotion class fairly balanced.\n\nGiven these properties, EMOPIA has versatile use cases in MIR research. For music labeling, EMOPIA can be used for clip-level music emotion recognition or music emotion variation detection  [2] , in both the audio and symbolic domains. For music generation, EMOPIA can be used for emotion-conditioned piano music generation or style transfer, to create emotion-controllable new compositions, or variations of existing pieces, again in both domains.\n\nWe present details of the dataset and the way we com- Jamendo Moods  [24]  adjectives multiple genres 18,486 Audio DEAM  [25]  VA values (from FMA  [26] , Jamendo, MedleyDB  [27] ) 1,802 Audio EMO-Soundscapes  [28]  VA values (from FMA) 1,213 Audio CCMED-WCMED  [29]  VA values classical (both Western & Chinese) 800 Audio emoMusic  [30]  VA values pop, rock, classical, electronic 744 Audio EMusic  [31]  VA values experimental, 8 others 140 Audio MOODetector  [6]  adjectives multiple genres (AllMusic) 193 Audio+MIDI VGMIDI  [10]  valence video game 95 MIDI EMOPIA (ours) Russell's 4Q pop (piano covers) 1,078 Audio+MIDI Table  1 . Comparison of some existing public emotion-labeled music datasets and the proposed EMOPIA dataset.\n\npile it in Section 3, and report a computational analysis of the dataset in Section 4. Moreover, to demonstrate the potential of the new dataset, we use it to train and evaluate a few clip-level emotion classification models using both audio and MIDI data in Section 5, and emotion-conditioned symbolic music generation models in Section 6. The latter involves the use of a recurrent neural network (RNN) model proposed by Ferreira et al.  [10] , and our own modification of a Transformer-based model  [33] [34] [35]  that takes emotion as a conditioning signal for generation. We release the EMOPIA dataset at a Zenodo repo. 1  Source code implementing the generation and classification models can be found on GitHub. 2 3 Examples of generated pieces can be found at a demo webpage.  4",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Emotion Recognition in Symbolic Music. Symbolic music representations describe music with the note, key, tempo, structure, chords, instruments. To understand the relationship between music and emotion, various researchers have investigated machine learning approaches with handcrafted features. Grekow et al.  [36]  extract in total 63 harmony, rhythmic, dynamic features from 83 classic music MIDI files. Lin et al.  [37]  compare audio, lyric, and MIDI features for music emotion recognition, finding that MIDI features lead to higher performance in valence dimension. Panda et al.  [6]  also proposed multi-model approaches, combining audio and MIDI features for emotion recognition using a small dataset of 193 songs.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion-Conditioned Symbolic Music Generation.",
      "text": "Only few work has started to address this task recently. Ferreira et al.  [10]  compile a small dataset of video game MIDI tracks with manual annotations of valence values, named VGMIDI (cf. Table  1 ), and use it to train a long short term memory network (LSTM) in tandem with a genetic algorithm (GA) based elite selection mechanism to generate positive or negative music. Makris et al.  [12]  approach the same task by using designated chord progression sequence in a sequence-to-sequence architecture trained with the VGMIDI dataset. Zhao et al.  [38]  use LSTM to generate music with four different emotions. Madhok et al.  [13]  use human facial expressions as the condition to generate music. More recently, Tan & Herremans demonstrate that their FaderNets  [16]  can achieve arousal-conditioned symbolic music style transfer with a semi-supervised clustering method that learns the relation between high-level features and low-level representation. Their model modifies the emotion (specifically, only the arousal) of a music piece, instead of generating new pieces from scratch.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "The Emopia Dataset",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Song Selection And Segmentation",
      "text": "EMOPIA is a collection of 387 piano solo performances of popular music segmented manually into 1,087 clips for emotion annotation. Two authors of the paper curated the song list of the piano performances by scanning through playlists on Spotify for its consistently high quality, then downloading the recordings from YouTube. A song is included when it is played by a professional conveying a clear emotion, and the recording has not been heavily engineered during post-production. The genres of songs include Japanese anime, Korean and Western pop song covers, movie soundtracks, and personal compositions.\n\nIn an effort to extend the usefulness of the dataset for future research, at the best of our ability, the songs are intentionally segmented (with the help of the Sonic Visualizer  [40] ) only at cadential arrivals to make it an emotionallyconsistent clip and a valid musical phrase at the same time. Accordingly, EMOPIA contains information for full songs, extracted phrases, and emotion labels.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Emotion Annotation",
      "text": "Different emotion taxonomies have been adopted in the literature for emotion annotation, with no standard so far  [2] . For EMOPIA, we consider a simple four-class taxonomy corresponding to the four quadrants of the Russell's famous Circumplex model of affect  [32] , which conceptualizes emotions in a two-dimensional space defined by valence and arousal. The four classes are: HVHA (high valence high arousal); HVLA (high valence low arousal); Table  2 . The number of clips and their average length in seconds, or in the number of REMI tokens, for each quadrant of the Russell's model in the EMOPIA dataset.\n\nLVHA (low valence high arousal); and LVLA (low valence low arousal). We refer to this taxonomy as Russell's 4Q.\n\nAs pointed out in the literature  [2, 3, 41] , various factors affect the perceived emotion of music, including cultural background, musical training, gender, etc. Consensus on the perception of emotion is challenging accordingly. Therefore, the annotations were made only among the first four authors, all coming from similar cultural backgrounds and collaborating closely during the annotation campaign, to ensure mutual standards for high or low valence/arousal. As it is time-consuming and laborious to choose clips from a song and label the emotion, each song was taken care of by only one annotator. Yet, cross validation of the annotations among the annotators was made several times during the annotation campaign (which spans 2.5 months), to ensure all annotators work on the same standard.\n\nTable  2  shows the number of clips and the average length (in seconds) for each class. The clips amount to approximately 11 hours' worth of data.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Transcription",
      "text": "We transcribe the selected clips automatically with the help of the high-resolution piano transcription model proposed by Kong et al.  [21] , which is open source and represents the state-of-the-art for this task. We have manually checked the transcription result for a random set of clips and find the accuracy in note pitch, velocity, and duration satisfactory. The transcription might be fragmented and undesirable for cases such as when the audio recording is engineered to have unnatural ambient effects; we drop such songs from our collection. The model also transcribes pedal information, which we include to EMOPIA but do not use in our experiments.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Pre-Processing And Encoding",
      "text": "For building machine learning models that deal with symbolic data, we need a data representation that can be used as input to the models. For example, MusicVAE  [42]  adopts the event-based representation that encodes a symbolic music piece as a sequence of \"event tokens\" such as note-on and note-off, while MuseGAN  [43]  employs a timestep-based, piano roll-like representation. Since there is no standard on the symbolic representation thus far, we adopt the following event-based ones in our experiments. Specifically, we use MIDI-like and REMI in Section 5 and CP in Section 6. See Figure  1  for illustrations.\n\n• The MIDI-like representation  [39]  encodes information regarding a MIDI note with a \"note-on\" token, a \"noteoff\", and a \"velocity\" token. Moreover, the \"time shift\" token is used to indicate the relative time gap (in ms) between two tokens.\n\n• REMI  [34]  considers a beat-based representation that instead uses \"bar\" and \"subbeat\" tokens to represent the time information. A \"bar\" token signifies the beginning of a new bar, and \"subbeat\" points to one of the constant number of subbeat divisions in a bar. Additionally, REMI uses \"tempo\" tokens to control the pace of the music, and replaces \"note-off\" with \"note-duration\". Table  2  also shows the average number of REMI tokens for clips in each emotion class.\n\n• CP  [35] . Both MIDI-like and REMI view events as individual tokens. In CP, tokens belonging to the same family are grouped into a super token and placed on the same timestep (see Figure  1(c) ). CP considers by default three families: metrical, note, and end-of-sequence. We additionally consider the \"emotion\" tokens and make it a new family, as depicted in Figure  1(d) . The prepending approach is motivated by CTRL  [44] , a state-of-the-art controllable text generation model in natural language processing (NLP) that uses global tokens to affect some overall properties of a sequence.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Dataset Availability",
      "text": "In EMOPIA, each sample is accompanied with its corresponding metadata, segmentation annotations, emotion annotation, and transcribed MIDI, which are all available  Table  3 . Jensen-Shannon divergence between the key histograms of a few emotion quadrant pairs. in the Zenodo repository. Moreover, we have added the MIDI data to the MusPy library  [18]  to facilitate its usage. Due to copyright issues, however, we can only share audio through YouTube links instead of sharing the audio files directl; the availability of the songs are subject to the copyright licenses in different countries and whether the owners will remove them.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset Analysis",
      "text": "The emotion that listeners perceive is determined by a wide array of composed and performed features of music  [45] .\n\nTo observe the emotional correlate of the musical attributes in EMOPIA, we extract various MIDI-based features and examine the distributions over the four quadrants of emotion. We present here the most discriminative features among many choices we examined in our analysis.\n\nNote Density, Length, and Velocity. The arousal of music can be easily observed based on the frequency of note occurrences and their strength  [46] . We measure them by note density, length and velocity. The note density is defined as the number of notes per beat, and the note length is defined as the average note length in beat unit. The note velocity is obtained directly from MIDI. Figure  2  shows three violin plots of the three features. The general trend shows that the high-arousal group (Q1, Q2) and lowarousal group (Q3, Q4) are distinguished well in all three plots. The results of note density and velocity are expected considering the nature of arousal. However, it is quite interesting that the note lengths are generally longer in the low-arousal group (Q3, Q4). Between the same-arousal quadrants, the differences are subtle. In note density, Q2 has more dynamics than Q1, whereas Q3 is not distinguishable from Q4. In note length, Q1 has slightly longer notes than those of Q2, whereas Q3 is again not distinguishable from Q4. In velocity, Q2 have louder notes than those of Q1, and Q3 has slightly louder notes than Q4.\n\nKey Distribution. The valence of music is often found to be related to the major-minor tonality  [7] . For simplicity, we measure the tonality from the musical key. We extract the key information using the Krumhansl-Kessler algorithm  [47]  in the MIDI ToolBox  [48] . Figure  3  shows the key distributions on 12 major-minor pitch classes for the four emotion quadrants. They assure that the majorminor tonality is an important clue in distinguishing valence. In the high valence group (Q1, Q4), the distribution is skewed to the left (major), while, in the low valence group (Q2, Q3), the trend is the opposite. We also measured the distance between the key distributions using the Jensen-Shannon divergence, which has the property of symmetry. Table  3  summarizes the pair-wise distances, indicating that the valence difference group has a larger difference than the arousal difference group.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Music Emotion Recognition",
      "text": "We report our baseline research on both symbolic-and audio-domain emotion recognition using EMOPIA, which defines the task as classifying a song into four categories.\n\nThe clips in EMOPIA are divided into train-validation-test splits with the ratio of 7:2:1 in a stratified manner. length, velocity, and represent the key as a one-hot vector.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Symbolic-Domain",
      "text": "For the latter, we use two different symbolic note representation methods introduced in Section 3.4, MIDI-like  [39]  and REMI  [34] . For the learning model, we use the combination of bidirectional LSTM and a self-attention module, or LSTM-Attn for short, proposed originally for sentiment classification in NLP  [49] . The LSTM extracts temporal information from the MIDI note events, while the selfattention module calculates different weight vectors over the LSTM hidden states with multi-head attentions. The weighted hidden states are finally used for classification.\n\nAudio-domain Classification. We evaluate two audiodomain classification methods in a similar manner to the symbolic-domain ones. In the first method, we use an average of 20 dimensions of mel-frequency cepstral coefficient (MFCC) vectors and a logistic regression classifier. In the second method, we use the short-chunk ResNet following  [50] , which is composed of 7-layer CNNs with residual connections. The output is summarized as a fixed 128-dimensional vector through max pooling, which is followed by two fully connected layers with the ReLU activation for classification. The input audio to the shortchunk ResNet is 3-second excerpts represented as a logscaled mel-spectrogram with 128 bins with 1024-size FFT (Hanning window), and 512 size hop at 22,050 Hz sampling rate. We randomly sample three seconds of the audio chunk as an input size to the classifier.\n\nEvaluation. We calculate 4-way classification accuracy over the four different emotion quadrants and 2-way classification accuracy over either arousal and valence. Tables  4  and 5  show the results in the symbolic and audio domains, respectively. Except for arousal, we can see that the deep learning approaches generally outperform the logistic regression classifiers using hand-crafted features. In audio domain arousal classification, MFCC vectors averaging the entire song sequence showed better performance than the deep learning approach with 3-second input. It seems that wider input sequence has the strength in emotion recognition. In both domains, valence classification is a more difficult task compared to arousal classification. For valence classification, MIDI-domain classifiers yield better result than audio-domain classifiers (0.883 vs. 0.704). Among the two token representations, MIDI-like seems to outperform REMI for valence classification.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Emotion-Conditioned Generation",
      "text": "We build the Transformer and LSTM models for emotionconditioned symbolic music generation using EMPOIA.\n\nFor the former, we adopt the Compound Word Transformer  [35] , the state-of-the-art in unconditioned symbolic music generation. We employ the CP+emotion representation presented in Section 3.4 as the data representation.\n\nFor the LSTM model, we consider the approach proposed by Ferreira et al.  [10] , which represents the stateof-the-art in emotion-conditioned music generation. Our implementation follows that described in  [10] , with the following differences: 1) train on EMOPIA rather than VG-MIDI; 2) use 512 neurons instead of 4,096 due to our limited computational resource; 3) use the same linear logistic regression layer for classification but we classify four classes instead of two.\n\nAs the size of EMOPIA might not be big enough, we use additionally the AILabs1k7 dataset compiled by Hsiao et al.  [35]  to pre-train the Transformer. AILabs1k7 contains 1,748 samples and is also pop piano music, but it does not contain emotion labels. Most of the clips in AIL-abs1k7 are longer than EMOPIA, so to keep the consistency of the input sequence length, the length of the token sequence is set to be 1,024. We pre-train the Transformer with 1e-4 learning rate on AILabs1k7, take the checkpoint with negative log-likelihood loss 0.30, and then fine-tune it on EMOPIA with 1e-5 learning rate. During pre-training, the emotion token is always set to be \"ignore,\" while in fine-tuning it is set to the emotion of that sample.\n\nEvaluation. We use the following three sets of metrics.\n\n• Surface-level objective metrics. We use the following three metrics proposed by Dong et al.  [43]  to evaluate whether the generated samples fit the training data: pitch range (PR), number of unique pitch classes used (NPC), and number of notes being played concurrently (POLY). We use MusPy  [18]  to compute these metrics.\n\n• Emotion-related objective metrics. Since both REMI and CP adopt a beat-based representation of music, we employ the LSTM-Attn+REMI emotion classifier (cf. Section 5) here to quantify how well the generation result is influenced by the emotion condition. We first use the generative model to generate 100 samples per class, and use the assigned label as the target class of the sample. The trained classifier is then used to make prediction on the generated samples. Similar to the classification task, apart from 4Q classification, we also conduct 2-way classification of both Arousal and Valence aspect.\n\n• Subjective metrics. As the classifiers are not 100% accurate, we also resort to a user survey to evaluate the emotion-controllability of the models. Specifically, we deploy an online survey to collect responses to the music generated by different models. A subject has to lis-   ten to 12 random-generated samples, one for each of the three models and each of the four emotion classes, and rate them on a five-point Likert scale with respect to 1) Valence: is the audio negative or positive; 2) Arousal: is low or high in arousal; 3) Humanness: how well it sounds like a piece played by human; 4) Richness: is the content interesting; and, 5) Overall musical quality. In total 25 subjects participated in the survey.\n\nTable  6  tabulates some of the results. We see that the CP Transformer with ('w/') pre-training performs the best in most of the objective metrics and the three subjective metrics listed here. Nevertheless, the scores of the CP Transformer with pre-training in the three emotion-related objective metrics are much lower than that reported in Table 4, suggesting that either the generated pieces are not emotion-laden, or the generated pieces are too dissimilar to the real pieces to the classifier. Figure  4  shows the human assessment of the emotioncontrollability of the models. To our surprise, while the CP Transformer with pre-training does not score high in the emotion related objective metrics, the subjective test shows that it can actually control the emotion of the generated pieces to a certain extent, better than the two competing models. In particular, the valence of the samples generated by the Transformer with pre-training has a median rating of 4 when the goal is to generate positive-valence music (i.e., Figure  4 (a)), while the scores of the other two models are around 3. Moreover, the arousal of the samples generated by the Transformer with pre-training has a median rating of 4 when the goal is to generate high-arousal music (Figure 4(c)), which is higher than that of the non pre-trained Transformer. This suggests that the LSTM-Attn classifier employed for computing the emotion-related objective metrics may not be reliable enough to predict the emotion perceived by human, and that the Transformer with pretraining is actually effective in controlling the emotion of the music it generates to certain extent. But, the model seems not good enough for the cases of generating lowvalence (i.e., negative) music, as shown in Figure  4 (b).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we have proposed a new public dataset EMOPIA, a medium-scale emotion-labeled pop piano dataset. It is a multi-modal dataset that contains both the audio files and MIDI transcriptions of piano-only music, along with clip-level emotion annotations in four classes. We have also presented prototypes of models for clip-level music emotion classification and emotion-based symbolic music generation trained on this dataset, using a number of state-of-the-art models in respective tasks. The result shows that we are able to achieve high accuracy in both four-quadrant and valence-wise emotion classification, and that our Transformer-based model is capable of generating music with a given target emotion to a certain degree.\n\nIn the future, in-depth importance analysis can be conducted to figure out features that are important for emotion classification, and to seek ways to incorporate those features to the generation model. Many ideas can also be tried to further improve the performance of emotion conditioning, e.g., the Transformer-GAN approach  [51] .\n\nWe share not only the dataset itself but the code covering all our implemented models in a GitHub repo. We hope that researchers will find this contribution useful in future emotion-related MIR tasks.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Illustration of different token-based representation for symbolic music: (a) MIDI-like [39], (b) REMI [34], and",
      "page": 3
    },
    {
      "caption": "Figure 1: for illustrations.",
      "page": 3
    },
    {
      "caption": "Figure 1: (c)). CP considers by default",
      "page": 3
    },
    {
      "caption": "Figure 1: (d). The prepending",
      "page": 3
    },
    {
      "caption": "Figure 2: Violin plots of the distribution in (a) note density, (b) length, and (c) velocity for clips from different classes.",
      "page": 4
    },
    {
      "caption": "Figure 3: Histogram of the keys (left / right: major / minor",
      "page": 4
    },
    {
      "caption": "Figure 2: shows three violin plots of the three features. The general",
      "page": 4
    },
    {
      "caption": "Figure 4: The subjective emotional scores (in 1–5) for the",
      "page": 6
    },
    {
      "caption": "Figure 4: shows the human assessment of the emotion-",
      "page": 6
    },
    {
      "caption": "Figure 4: (a)), while the scores of the other two models are",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table 1: , only two",
      "data": [
        {
          "multi-modal (audio and MIDI) database focusing on per-": "ceived emotion in pop piano music,\nto facilitate research",
          "the audio recordings.": "To address this need, we propose a new emotion-labeled"
        },
        {
          "multi-modal (audio and MIDI) database focusing on per-": "on various tasks related to music emotion. The dataset con-",
          "the audio recordings.": "dataset comprising of three main nice properties:"
        },
        {
          "multi-modal (audio and MIDI) database focusing on per-": "tains 1,087 music clips from 387 songs and clip-level emo-",
          "the audio recordings.": ""
        },
        {
          "multi-modal (audio and MIDI) database focusing on per-": "",
          "the audio recordings.": "• Single-instrument. We collect audio recordings of pi-"
        },
        {
          "multi-modal (audio and MIDI) database focusing on per-": "tion labels annotated by four dedicated annotators. Since",
          "the audio recordings.": ""
        },
        {
          "multi-modal (audio and MIDI) database focusing on per-": "",
          "the audio recordings.": "ano covers and creations from YouTube, with fair to high"
        },
        {
          "multi-modal (audio and MIDI) database focusing on per-": "the clips are not\nrestricted to one clip per song,\nthey can",
          "the audio recordings.": ""
        },
        {
          "multi-modal (audio and MIDI) database focusing on per-": "",
          "the audio recordings.": "audio and musical quality, and a diverse set of playing"
        },
        {
          "multi-modal (audio and MIDI) database focusing on per-": "also be used for song-level analysis. We present\nthe pro-",
          "the audio recordings.": ""
        },
        {
          "multi-modal (audio and MIDI) database focusing on per-": "",
          "the audio recordings.": "styles and perceived emotions.\nFocusing on only pi-"
        },
        {
          "multi-modal (audio and MIDI) database focusing on per-": "cedure for building the dataset, covering the song list cu-",
          "the audio recordings.": ""
        },
        {
          "multi-modal (audio and MIDI) database focusing on per-": "",
          "the audio recordings.": "ano music allows for the use of piano transcription algo-"
        },
        {
          "multi-modal (audio and MIDI) database focusing on per-": "ration, clip selection, and emotion annotation processes.",
          "the audio recordings.": ""
        },
        {
          "multi-modal (audio and MIDI) database focusing on per-": "",
          "the audio recordings.": "rithms [20, 21], and facilitates disentanglement of musi-"
        },
        {
          "multi-modal (audio and MIDI) database focusing on per-": "Moreover, we prototype use cases on clip-level music emo-",
          "the audio recordings.": ""
        },
        {
          "multi-modal (audio and MIDI) database focusing on per-": "",
          "the audio recordings.": "cal composition from variations in timbre, arrangement,"
        },
        {
          "multi-modal (audio and MIDI) database focusing on per-": "tion classiﬁcation and emotion-based symbolic music gen-",
          "the audio recordings.": ""
        },
        {
          "multi-modal (audio and MIDI) database focusing on per-": "",
          "the audio recordings.": "and other confounds seen in multi-instrument music."
        },
        {
          "multi-modal (audio and MIDI) database focusing on per-": "eration by training and evaluating corresponding models",
          "the audio recordings.": ""
        },
        {
          "multi-modal (audio and MIDI) database focusing on per-": "using the dataset.\nThe result demonstrates\nthe potential",
          "the audio recordings.": "• Multi-modal. Both the audio and MIDI versions of the"
        },
        {
          "multi-modal (audio and MIDI) database focusing on per-": "of EMOPIA for being used in future exploration on piano",
          "the audio recordings.": "music pieces can be found from the Internet (see Section"
        },
        {
          "multi-modal (audio and MIDI) database focusing on per-": "emotion-related MIR tasks.",
          "the audio recordings.": "3.5 for details). The MIDI ﬁles are automatically tran-"
        },
        {
          "multi-modal (audio and MIDI) database focusing on per-": "",
          "the audio recordings.": "scribed from the audio by a state-of-the-art model [21]."
        },
        {
          "multi-modal (audio and MIDI) database focusing on per-": "1.\nINTRODUCTION",
          "the audio recordings.": ""
        },
        {
          "multi-modal (audio and MIDI) database focusing on per-": "",
          "the audio recordings.": "• Clip-level annotation. The audio ﬁles downloaded from"
        },
        {
          "multi-modal (audio and MIDI) database focusing on per-": "",
          "the audio recordings.": "YouTube are full songs. As different parts of a song may"
        },
        {
          "multi-modal (audio and MIDI) database focusing on per-": "The affective aspect of music has been a major subject of",
          "the audio recordings.": ""
        },
        {
          "multi-modal (audio and MIDI) database focusing on per-": "",
          "the audio recordings.": "convey different emotions,\nthe ﬁrst\nfour authors of\nthe"
        },
        {
          "multi-modal (audio and MIDI) database focusing on per-": "research in the ﬁeld of music information retrieval (MIR),",
          "the audio recordings.": ""
        },
        {
          "multi-modal (audio and MIDI) database focusing on per-": "",
          "the audio recordings.": "paper manually and carefully pick emotion-consistent"
        },
        {
          "multi-modal (audio and MIDI) database focusing on per-": "not only for music analysis and labeling [1–9], but also for",
          "the audio recordings.": ""
        },
        {
          "multi-modal (audio and MIDI) database focusing on per-": "",
          "the audio recordings.": "short clips from each song and label the emotion of these"
        },
        {
          "multi-modal (audio and MIDI) database focusing on per-": "music generation or editing [10–14]. Accordingly,\nthere",
          "the audio recordings.": ""
        },
        {
          "multi-modal (audio and MIDI) database focusing on per-": "",
          "the audio recordings.": "clips using a four-class taxonomy derived from the Rus-"
        },
        {
          "multi-modal (audio and MIDI) database focusing on per-": "have been quite a few public music datasets with emo-",
          "the audio recordings.": ""
        },
        {
          "multi-modal (audio and MIDI) database focusing on per-": "",
          "the audio recordings.": "sell’s valence-arousal model\n[32].\nThis\nleads\nto clip-"
        },
        {
          "multi-modal (audio and MIDI) database focusing on per-": "tion, as listed in Table 1.\nThese datasets are different\nin",
          "the audio recordings.": ""
        },
        {
          "multi-modal (audio and MIDI) database focusing on per-": "",
          "the audio recordings.": "level emotion annotations for\nin total 1,087 clips from"
        },
        {
          "multi-modal (audio and MIDI) database focusing on per-": "many ways, including the musical genres considered, data",
          "the audio recordings.": ""
        },
        {
          "multi-modal (audio and MIDI) database focusing on per-": "",
          "the audio recordings.": "387 songs (i.e., 2.78 clips per song on average), with the"
        },
        {
          "multi-modal (audio and MIDI) database focusing on per-": "modality and data size, and the way emotion is described.",
          "the audio recordings.": ""
        },
        {
          "multi-modal (audio and MIDI) database focusing on per-": "",
          "the audio recordings.": "number of clips per emotion class fairly balanced."
        },
        {
          "multi-modal (audio and MIDI) database focusing on per-": "With the growing interest\nin symbolic-domain music",
          "the audio recordings.": ""
        },
        {
          "multi-modal (audio and MIDI) database focusing on per-": "analysis and generation in recent years of ISMIR [15–19],",
          "the audio recordings.": "Given these properties, EMOPIA has versatile use cases"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table 1: , only two",
      "data": [
        {
          "r08922a20@csie.ntu.edu.tw,": "ABSTRACT",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": "dataset\nto add emotion-related elements to such research."
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": "However, among the datasets listed in Table 1, only two"
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "While\nthere\nare many music datasets with emotion la-",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": "provide MIDI data, and they are both small in size. More-"
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "bels in the literature,\nthey cannot be used for research on",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": "over, the majority of the audio-only datasets contain songs"
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "symbolic-domain music analysis or generation,\nas\nthere",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": "of multiple genres, making it hard to apply automatic mu-"
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "are usually audio ﬁles only.\nIn this paper, we present\nthe",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": "sic transcription algorithms, which currently work better"
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "EMOPIA (pronounced ‘yee-mò-pi-uh’) dataset, a shared",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": "for piano-only music [20–23],\nto get MIDI-like data from"
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "multi-modal (audio and MIDI) database focusing on per-",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": "the audio recordings."
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "ceived emotion in pop piano music,\nto facilitate research",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": "To address this need, we propose a new emotion-labeled"
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "on various tasks related to music emotion. The dataset con-",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": "dataset comprising of three main nice properties:"
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "tains 1,087 music clips from 387 songs and clip-level emo-",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": ""
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": "• Single-instrument. We collect audio recordings of pi-"
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "tion labels annotated by four dedicated annotators. Since",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": ""
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": "ano covers and creations from YouTube, with fair to high"
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "the clips are not\nrestricted to one clip per song,\nthey can",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": ""
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": "audio and musical quality, and a diverse set of playing"
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "also be used for song-level analysis. We present\nthe pro-",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": ""
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": "styles and perceived emotions.\nFocusing on only pi-"
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "cedure for building the dataset, covering the song list cu-",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": ""
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": "ano music allows for the use of piano transcription algo-"
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "ration, clip selection, and emotion annotation processes.",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": ""
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": "rithms [20, 21], and facilitates disentanglement of musi-"
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "Moreover, we prototype use cases on clip-level music emo-",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": ""
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": "cal composition from variations in timbre, arrangement,"
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "tion classiﬁcation and emotion-based symbolic music gen-",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": ""
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": "and other confounds seen in multi-instrument music."
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "eration by training and evaluating corresponding models",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": ""
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "using the dataset.\nThe result demonstrates\nthe potential",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": "• Multi-modal. Both the audio and MIDI versions of the"
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "of EMOPIA for being used in future exploration on piano",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": "music pieces can be found from the Internet (see Section"
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "emotion-related MIR tasks.",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": "3.5 for details). The MIDI ﬁles are automatically tran-"
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": "scribed from the audio by a state-of-the-art model [21]."
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "1.\nINTRODUCTION",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": ""
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": "• Clip-level annotation. The audio ﬁles downloaded from"
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": "YouTube are full songs. As different parts of a song may"
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "The affective aspect of music has been a major subject of",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": ""
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": "convey different emotions,\nthe ﬁrst\nfour authors of\nthe"
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "research in the ﬁeld of music information retrieval (MIR),",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": ""
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": "paper manually and carefully pick emotion-consistent"
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "not only for music analysis and labeling [1–9], but also for",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": ""
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": "short clips from each song and label the emotion of these"
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "music generation or editing [10–14]. Accordingly,\nthere",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": ""
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": "clips using a four-class taxonomy derived from the Rus-"
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "have been quite a few public music datasets with emo-",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": ""
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": "sell’s valence-arousal model\n[32].\nThis\nleads\nto clip-"
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "tion, as listed in Table 1.\nThese datasets are different\nin",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": ""
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": "level emotion annotations for\nin total 1,087 clips from"
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "many ways, including the musical genres considered, data",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": ""
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": "387 songs (i.e., 2.78 clips per song on average), with the"
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "modality and data size, and the way emotion is described.",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": ""
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": "number of clips per emotion class fairly balanced."
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "With the growing interest\nin symbolic-domain music",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": ""
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "analysis and generation in recent years of ISMIR [15–19],",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": "Given these properties, EMOPIA has versatile use cases"
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "it\nis desirable to have an emotion-labeled symbolic music",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": "in MIR research. For music labeling, EMOPIA can be used"
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": "for clip-level music emotion recognition or music emotion"
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": "variation detection [2], in both the audio and symbolic do-"
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "© H.T.Hung and J. Ching, and S.H Doh.\nLicensed under",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": ""
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": "mains.\nFor music generation, EMOPIA can be used for"
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "a Creative Commons Attribution 4.0 International License (CC BY 4.0).",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": ""
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": "emotion-conditioned piano music generation or style trans-"
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "Attribution: H.T.Hung and J. Ching, and S.H Doh, “EMOPIA: A Multi-",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": ""
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": "fer,\nto create emotion-controllable new compositions, or"
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "modal Pop Piano Dataset\nfor Emotion Recognition and Emotion-based",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": ""
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "Music Generation”, in Proc. of the 22nd Int. Society for Music Informa-",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": "variations of existing pieces, again in both domains."
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "tion Retrieval Conf., Online, 2021.",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": ""
        },
        {
          "r08922a20@csie.ntu.edu.tw,": "",
          "joann8512@citi.sinica.edu,\nseungheondoh@kaist.ac.kr": "We present details of the dataset and the way we com-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table 1: ), and use it to train a long",
      "data": [
        {
          "Name": "Jamendo Moods [24]",
          "Label type": "adjectives",
          "Genre (or data source)": "multiple genres",
          "Size": "18,486",
          "Modality": "Audio"
        },
        {
          "Name": "DEAM [25]",
          "Label type": "VA values",
          "Genre (or data source)": "(from FMA [26], Jamendo, MedleyDB [27])",
          "Size": "1,802",
          "Modality": "Audio"
        },
        {
          "Name": "EMO-Soundscapes [28]",
          "Label type": "VA values",
          "Genre (or data source)": "(from FMA)",
          "Size": "1,213",
          "Modality": "Audio"
        },
        {
          "Name": "CCMED-WCMED [29]",
          "Label type": "VA values",
          "Genre (or data source)": "classical (both Western & Chinese)",
          "Size": "800",
          "Modality": "Audio"
        },
        {
          "Name": "emoMusic [30]",
          "Label type": "VA values",
          "Genre (or data source)": "pop, rock, classical, electronic",
          "Size": "744",
          "Modality": "Audio"
        },
        {
          "Name": "EMusic [31]",
          "Label type": "VA values",
          "Genre (or data source)": "experimental, 8 others",
          "Size": "140",
          "Modality": "Audio"
        },
        {
          "Name": "MOODetector [6]",
          "Label type": "adjectives",
          "Genre (or data source)": "multiple genres (AllMusic)",
          "Size": "193",
          "Modality": "Audio+MIDI"
        },
        {
          "Name": "VGMIDI [10]",
          "Label type": "valence",
          "Genre (or data source)": "video game",
          "Size": "95",
          "Modality": "MIDI"
        },
        {
          "Name": "EMOPIA (ours)",
          "Label type": "Russell’s 4Q",
          "Genre (or data source)": "pop (piano covers)",
          "Size": "1,078",
          "Modality": "Audio+MIDI"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: ), and use it to train a long",
      "data": [
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "",
          "1,078\nAudio+MIDI": "Table 1. Comparison of some existing public emotion-labeled music datasets and the proposed EMOPIA dataset."
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "pile it in Section 3, and report a computational analysis of",
          "1,078\nAudio+MIDI": "trained with the VGMIDI dataset.\nZhao et al.\n[38] use"
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "the dataset in Section 4. Moreover, to demonstrate the po-",
          "1,078\nAudio+MIDI": "LSTM to generate music with four different\nemotions."
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "tential of the new dataset, we use it to train and evaluate a",
          "1,078\nAudio+MIDI": "Madhok et al.\n[13] use human facial expressions as\nthe"
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "few clip-level emotion classiﬁcation models using both au-",
          "1,078\nAudio+MIDI": "condition to generate music. More recently, Tan & Her-"
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "dio and MIDI data in Section 5, and emotion-conditioned",
          "1,078\nAudio+MIDI": "remans demonstrate that\ntheir FaderNets [16] can achieve"
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "symbolic music generation models in Section 6. The lat-",
          "1,078\nAudio+MIDI": "arousal-conditioned symbolic music style transfer with a"
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "ter involves the use of a recurrent neural network (RNN)",
          "1,078\nAudio+MIDI": "semi-supervised clustering method that\nlearns the relation"
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "model proposed by Ferreira et al. [10], and our own mod-",
          "1,078\nAudio+MIDI": "between high-level\nfeatures and low-level\nrepresentation."
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "iﬁcation of a Transformer-based model [33–35] that\ntakes",
          "1,078\nAudio+MIDI": "Their model modiﬁes the emotion (speciﬁcally, only the"
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "emotion as a conditioning signal for generation.",
          "1,078\nAudio+MIDI": "arousal) of a music piece, instead of generating new pieces"
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "We release the EMOPIA dataset at a Zenodo repo. 1",
          "1,078\nAudio+MIDI": "from scratch."
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "Source code implementing the generation and classiﬁca-",
          "1,078\nAudio+MIDI": ""
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "tion models can be found on GitHub. 2\n3 Examples of gen-",
          "1,078\nAudio+MIDI": ""
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "",
          "1,078\nAudio+MIDI": "3. THE EMOPIA DATASET"
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "erated pieces can be found at a demo webpage. 4",
          "1,078\nAudio+MIDI": ""
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "",
          "1,078\nAudio+MIDI": "3.1\nSong Selection and Segmentation"
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "2. RELATED WORK",
          "1,078\nAudio+MIDI": ""
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "",
          "1,078\nAudio+MIDI": "EMOPIA is a collection of 387 piano solo performances"
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "",
          "1,078\nAudio+MIDI": "of popular music segmented manually into 1,087 clips for"
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "Emotion Recognition in Symbolic Music.\nSymbolic",
          "1,078\nAudio+MIDI": ""
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "",
          "1,078\nAudio+MIDI": "emotion annotation. Two authors of the paper curated the"
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "music representations describe music with the note, key,",
          "1,078\nAudio+MIDI": ""
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "",
          "1,078\nAudio+MIDI": "song list of\nthe piano performances by scanning through"
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "tempo,\nstructure,\nchords,\ninstruments.\nTo\nunderstand",
          "1,078\nAudio+MIDI": ""
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "",
          "1,078\nAudio+MIDI": "playlists on Spotify for\nits consistently high quality,\nthen"
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "the relationship between music and emotion, various re-",
          "1,078\nAudio+MIDI": ""
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "",
          "1,078\nAudio+MIDI": "downloading the recordings from YouTube. A song is in-"
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "searchers have investigated machine learning approaches",
          "1,078\nAudio+MIDI": ""
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "",
          "1,078\nAudio+MIDI": "cluded when it\nis played by a professional conveying a"
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "with handcrafted features. Grekow et al. [36] extract in to-",
          "1,078\nAudio+MIDI": ""
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "",
          "1,078\nAudio+MIDI": "clear emotion, and the recording has not been heavily en-"
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "tal 63 harmony, rhythmic, dynamic features from 83 clas-",
          "1,078\nAudio+MIDI": ""
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "",
          "1,078\nAudio+MIDI": "gineered during post-production. The genres of songs in-"
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "sic music MIDI ﬁles. Lin et al. [37] compare audio,\nlyric,",
          "1,078\nAudio+MIDI": ""
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "",
          "1,078\nAudio+MIDI": "clude Japanese anime, Korean and Western pop song cov-"
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "and MIDI features for music emotion recognition, ﬁnding",
          "1,078\nAudio+MIDI": ""
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "",
          "1,078\nAudio+MIDI": "ers, movie soundtracks, and personal compositions."
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "that MIDI features lead to higher performance in valence",
          "1,078\nAudio+MIDI": ""
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "dimension. Panda et al. [6] also proposed multi-model ap-",
          "1,078\nAudio+MIDI": "In an effort to extend the usefulness of the dataset for fu-"
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "proaches, combining audio and MIDI features for emotion",
          "1,078\nAudio+MIDI": "ture research, at the best of our ability, the songs are inten-"
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "recognition using a small dataset of 193 songs.",
          "1,078\nAudio+MIDI": "tionally segmented (with the help of the Sonic Visualizer"
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "",
          "1,078\nAudio+MIDI": "[40]) only at cadential arrivals to make it an emotionally-"
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "Emotion-conditioned\nSymbolic Music\nGeneration.",
          "1,078\nAudio+MIDI": "consistent clip and a valid musical phrase at the same time."
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "Only few work has\nstarted to address\nthis\ntask recently.",
          "1,078\nAudio+MIDI": "Accordingly, EMOPIA contains information for full songs,"
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "Ferreira et al. [10] compile a small dataset of video game",
          "1,078\nAudio+MIDI": "extracted phrases, and emotion labels."
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "MIDI\ntracks with manual annotations of valence values,",
          "1,078\nAudio+MIDI": ""
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "named VGMIDI\n(cf.\nTable 1), and use it\nto train a long",
          "1,078\nAudio+MIDI": ""
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "",
          "1,078\nAudio+MIDI": "3.2 Emotion Annotation"
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "short term memory network (LSTM) in tandem with a ge-",
          "1,078\nAudio+MIDI": ""
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "netic algorithm (GA) based elite selection mechanism to",
          "1,078\nAudio+MIDI": "Different emotion taxonomies have been adopted in the lit-"
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "generate positive or negative music. Makris et al.\n[12]",
          "1,078\nAudio+MIDI": "erature for emotion annotation, with no standard so far [2]."
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "approach the same task by using designated chord pro-",
          "1,078\nAudio+MIDI": "For EMOPIA, we consider a simple four-class taxonomy"
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "gression sequence in a sequence-to-sequence architecture",
          "1,078\nAudio+MIDI": "corresponding to the four quadrants of\nthe Russell’s\nfa-"
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "",
          "1,078\nAudio+MIDI": "mous Circumplex model of affect [32], which conceptual-"
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "1 https://zenodo.org/record/5090631",
          "1,078\nAudio+MIDI": ""
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "",
          "1,078\nAudio+MIDI": "izes emotions in a two-dimensional space deﬁned by va-"
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "2 https://github.com/annahung31/EMOPIA",
          "1,078\nAudio+MIDI": ""
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "3 https://github.com/SeungHeonDoh/EMOPIA_cls",
          "1,078\nAudio+MIDI": "lence and arousal.\nThe four classes are: HVHA (high"
        },
        {
          "EMOPIA (ours)\nRussell’s 4Q\npop (piano covers)": "4 https://annahung31.github.io/EMOPIA/",
          "1,078\nAudio+MIDI": "valence high arousal); HVLA (high valence low arousal);"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 2: The number of clips and their average length in as note-on and note-off, while MuseGAN [43] employs a",
      "data": [
        {
          "Figure 1.\nIllustration of different token-based representation for symbolic music:": "(d) CP [35] plus emotion token. Sub-ﬁgure (c) is an intermediate representation of the CP one.",
          "(a) MIDI-like [39], (b) REMI [34], and": ""
        },
        {
          "Figure 1.\nIllustration of different token-based representation for symbolic music:": "",
          "(a) MIDI-like [39], (b) REMI [34], and": "3.4 Pre-processing and Encoding"
        },
        {
          "Figure 1.\nIllustration of different token-based representation for symbolic music:": "Quadrant\n# clips\nAvg.\nlength (in sec / #tokens)",
          "(a) MIDI-like [39], (b) REMI [34], and": ""
        },
        {
          "Figure 1.\nIllustration of different token-based representation for symbolic music:": "Q1\n250\n31.9 / 1,065",
          "(a) MIDI-like [39], (b) REMI [34], and": "For building machine learning models that deal with sym-"
        },
        {
          "Figure 1.\nIllustration of different token-based representation for symbolic music:": "Q2\n265\n35.6 / 1,368",
          "(a) MIDI-like [39], (b) REMI [34], and": "bolic data, we need a data representation that can be used"
        },
        {
          "Figure 1.\nIllustration of different token-based representation for symbolic music:": "Q3\n253\n40.6 /\n771",
          "(a) MIDI-like [39], (b) REMI [34], and": "as\ninput\nto the models.\nFor\nexample, MusicVAE [42]"
        },
        {
          "Figure 1.\nIllustration of different token-based representation for symbolic music:": "Q4\n310\n38.2 /\n729",
          "(a) MIDI-like [39], (b) REMI [34], and": "adopts the event-based representation that encodes a sym-"
        },
        {
          "Figure 1.\nIllustration of different token-based representation for symbolic music:": "",
          "(a) MIDI-like [39], (b) REMI [34], and": "bolic music piece as a sequence of “event\ntokens” such"
        },
        {
          "Figure 1.\nIllustration of different token-based representation for symbolic music:": "Table 2. The number of clips and their average length in",
          "(a) MIDI-like [39], (b) REMI [34], and": "as note-on and note-off, while MuseGAN [43] employs a"
        },
        {
          "Figure 1.\nIllustration of different token-based representation for symbolic music:": "seconds, or in the number of REMI tokens, for each quad-",
          "(a) MIDI-like [39], (b) REMI [34], and": "timestep-based, piano roll-like representation. Since there"
        },
        {
          "Figure 1.\nIllustration of different token-based representation for symbolic music:": "rant of the Russell’s model in the EMOPIA dataset.",
          "(a) MIDI-like [39], (b) REMI [34], and": "is no standard on the symbolic representation thus far, we"
        },
        {
          "Figure 1.\nIllustration of different token-based representation for symbolic music:": "",
          "(a) MIDI-like [39], (b) REMI [34], and": "adopt\nthe following event-based ones in our experiments."
        },
        {
          "Figure 1.\nIllustration of different token-based representation for symbolic music:": "",
          "(a) MIDI-like [39], (b) REMI [34], and": "Speciﬁcally, we use MIDI-like and REMI in Section 5 and"
        },
        {
          "Figure 1.\nIllustration of different token-based representation for symbolic music:": "LVHA (low valence high arousal); and LVLA (low valence",
          "(a) MIDI-like [39], (b) REMI [34], and": ""
        },
        {
          "Figure 1.\nIllustration of different token-based representation for symbolic music:": "",
          "(a) MIDI-like [39], (b) REMI [34], and": "CP in Section 6. See Figure 1 for illustrations."
        },
        {
          "Figure 1.\nIllustration of different token-based representation for symbolic music:": "low arousal). We refer to this taxonomy as Russell’s 4Q.",
          "(a) MIDI-like [39], (b) REMI [34], and": ""
        },
        {
          "Figure 1.\nIllustration of different token-based representation for symbolic music:": "As pointed out\nin the literature [2, 3, 41], various fac-",
          "(a) MIDI-like [39], (b) REMI [34], and": "• The MIDI-like representation [39] encodes information"
        },
        {
          "Figure 1.\nIllustration of different token-based representation for symbolic music:": "tors affect\nthe perceived emotion of music,\nincluding cul-",
          "(a) MIDI-like [39], (b) REMI [34], and": "regarding a MIDI note with a “note-on” token, a “note-"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: The number of clips and their average length in as note-on and note-off, while MuseGAN [43] employs a",
      "data": [
        {
          "adopt\nthe following event-based ones in our experiments.": "Speciﬁcally, we use MIDI-like and REMI in Section 5 and"
        },
        {
          "adopt\nthe following event-based ones in our experiments.": ""
        },
        {
          "adopt\nthe following event-based ones in our experiments.": "CP in Section 6. See Figure 1 for illustrations."
        },
        {
          "adopt\nthe following event-based ones in our experiments.": ""
        },
        {
          "adopt\nthe following event-based ones in our experiments.": "• The MIDI-like representation [39] encodes information"
        },
        {
          "adopt\nthe following event-based ones in our experiments.": "regarding a MIDI note with a “note-on” token, a “note-"
        },
        {
          "adopt\nthe following event-based ones in our experiments.": "off”, and a “velocity” token. Moreover,\nthe “time shift”"
        },
        {
          "adopt\nthe following event-based ones in our experiments.": "token is used to indicate the relative time gap (in ms)"
        },
        {
          "adopt\nthe following event-based ones in our experiments.": "between two tokens."
        },
        {
          "adopt\nthe following event-based ones in our experiments.": ""
        },
        {
          "adopt\nthe following event-based ones in our experiments.": "• REMI\n[34] considers a beat-based representation that"
        },
        {
          "adopt\nthe following event-based ones in our experiments.": ""
        },
        {
          "adopt\nthe following event-based ones in our experiments.": "instead uses “bar” and “subbeat” tokens to represent the"
        },
        {
          "adopt\nthe following event-based ones in our experiments.": ""
        },
        {
          "adopt\nthe following event-based ones in our experiments.": "time information. A “bar” token signiﬁes the beginning"
        },
        {
          "adopt\nthe following event-based ones in our experiments.": ""
        },
        {
          "adopt\nthe following event-based ones in our experiments.": "of a new bar, and “subbeat” points to one of\nthe con-"
        },
        {
          "adopt\nthe following event-based ones in our experiments.": ""
        },
        {
          "adopt\nthe following event-based ones in our experiments.": "stant number of subbeat divisions in a bar. Additionally,"
        },
        {
          "adopt\nthe following event-based ones in our experiments.": ""
        },
        {
          "adopt\nthe following event-based ones in our experiments.": "REMI uses “tempo” tokens to control the pace of the mu-"
        },
        {
          "adopt\nthe following event-based ones in our experiments.": ""
        },
        {
          "adopt\nthe following event-based ones in our experiments.": "sic, and replaces “note-off” with “note-duration”. Table"
        },
        {
          "adopt\nthe following event-based ones in our experiments.": ""
        },
        {
          "adopt\nthe following event-based ones in our experiments.": "2 also shows the average number of REMI\ntokens for"
        },
        {
          "adopt\nthe following event-based ones in our experiments.": ""
        },
        {
          "adopt\nthe following event-based ones in our experiments.": "clips in each emotion class."
        },
        {
          "adopt\nthe following event-based ones in our experiments.": ""
        },
        {
          "adopt\nthe following event-based ones in our experiments.": "• CP [35]. Both MIDI-like and REMI view events as in-"
        },
        {
          "adopt\nthe following event-based ones in our experiments.": ""
        },
        {
          "adopt\nthe following event-based ones in our experiments.": "dividual\ntokens.\nIn CP,\ntokens belonging to the same"
        },
        {
          "adopt\nthe following event-based ones in our experiments.": ""
        },
        {
          "adopt\nthe following event-based ones in our experiments.": "family are grouped into a super token and placed on the"
        },
        {
          "adopt\nthe following event-based ones in our experiments.": "same timestep (see Figure 1(c)). CP considers by default"
        },
        {
          "adopt\nthe following event-based ones in our experiments.": ""
        },
        {
          "adopt\nthe following event-based ones in our experiments.": "three families: metrical, note, and end-of-sequence. We"
        },
        {
          "adopt\nthe following event-based ones in our experiments.": ""
        },
        {
          "adopt\nthe following event-based ones in our experiments.": "additionally consider the “emotion” tokens and make it a"
        },
        {
          "adopt\nthe following event-based ones in our experiments.": ""
        },
        {
          "adopt\nthe following event-based ones in our experiments.": "new family, as depicted in Figure 1(d). The prepending"
        },
        {
          "adopt\nthe following event-based ones in our experiments.": ""
        },
        {
          "adopt\nthe following event-based ones in our experiments.": "approach is motivated by CTRL [44], a state-of-the-art"
        },
        {
          "adopt\nthe following event-based ones in our experiments.": ""
        },
        {
          "adopt\nthe following event-based ones in our experiments.": "controllable text generation model\nin natural\nlanguage"
        },
        {
          "adopt\nthe following event-based ones in our experiments.": ""
        },
        {
          "adopt\nthe following event-based ones in our experiments.": "processing (NLP) that uses global tokens to affect some"
        },
        {
          "adopt\nthe following event-based ones in our experiments.": ""
        },
        {
          "adopt\nthe following event-based ones in our experiments.": "overall properties of a sequence."
        },
        {
          "adopt\nthe following event-based ones in our experiments.": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 2. Violin plots of the distribution in (a) note density, (b) length, and (c) velocity for clips from different classes.": "length is deﬁned as the average note length in beat unit."
        },
        {
          "Figure 2. Violin plots of the distribution in (a) note density, (b) length, and (c) velocity for clips from different classes.": "The note velocity is obtained directly from MIDI. Figure 2"
        },
        {
          "Figure 2. Violin plots of the distribution in (a) note density, (b) length, and (c) velocity for clips from different classes.": "shows three violin plots of the three features. The general"
        },
        {
          "Figure 2. Violin plots of the distribution in (a) note density, (b) length, and (c) velocity for clips from different classes.": "trend shows that the high-arousal group (Q1, Q2) and low-"
        },
        {
          "Figure 2. Violin plots of the distribution in (a) note density, (b) length, and (c) velocity for clips from different classes.": "arousal group (Q3, Q4) are distinguished well\nin all\nthree"
        },
        {
          "Figure 2. Violin plots of the distribution in (a) note density, (b) length, and (c) velocity for clips from different classes.": "plots. The results of note density and velocity are expected"
        },
        {
          "Figure 2. Violin plots of the distribution in (a) note density, (b) length, and (c) velocity for clips from different classes.": "considering the nature of arousal. However,\nit\nis quite in-"
        },
        {
          "Figure 2. Violin plots of the distribution in (a) note density, (b) length, and (c) velocity for clips from different classes.": "teresting that\nthe note lengths are generally longer\nin the"
        },
        {
          "Figure 2. Violin plots of the distribution in (a) note density, (b) length, and (c) velocity for clips from different classes.": "low-arousal group (Q3, Q4).\nBetween the same-arousal"
        },
        {
          "Figure 2. Violin plots of the distribution in (a) note density, (b) length, and (c) velocity for clips from different classes.": "quadrants,\nthe differences are subtle.\nIn note density, Q2"
        },
        {
          "Figure 2. Violin plots of the distribution in (a) note density, (b) length, and (c) velocity for clips from different classes.": "has more dynamics than Q1, whereas Q3 is not distinguish-"
        },
        {
          "Figure 2. Violin plots of the distribution in (a) note density, (b) length, and (c) velocity for clips from different classes.": "able from Q4.\nIn note length, Q1 has slightly longer notes"
        },
        {
          "Figure 2. Violin plots of the distribution in (a) note density, (b) length, and (c) velocity for clips from different classes.": "than those of Q2, whereas Q3 is again not distinguishable"
        },
        {
          "Figure 2. Violin plots of the distribution in (a) note density, (b) length, and (c) velocity for clips from different classes.": "from Q4.\nIn velocity, Q2 have louder notes than those of"
        },
        {
          "Figure 2. Violin plots of the distribution in (a) note density, (b) length, and (c) velocity for clips from different classes.": "Q1, and Q3 has slightly louder notes than Q4."
        },
        {
          "Figure 2. Violin plots of the distribution in (a) note density, (b) length, and (c) velocity for clips from different classes.": "Key Distribution. The valence of music is often found to"
        },
        {
          "Figure 2. Violin plots of the distribution in (a) note density, (b) length, and (c) velocity for clips from different classes.": ""
        },
        {
          "Figure 2. Violin plots of the distribution in (a) note density, (b) length, and (c) velocity for clips from different classes.": ""
        },
        {
          "Figure 2. Violin plots of the distribution in (a) note density, (b) length, and (c) velocity for clips from different classes.": "be related to the major-minor\ntonality [7].\nFor\nsimplic-"
        },
        {
          "Figure 2. Violin plots of the distribution in (a) note density, (b) length, and (c) velocity for clips from different classes.": ""
        },
        {
          "Figure 2. Violin plots of the distribution in (a) note density, (b) length, and (c) velocity for clips from different classes.": "ity, we measure the tonality from the musical key. We"
        },
        {
          "Figure 2. Violin plots of the distribution in (a) note density, (b) length, and (c) velocity for clips from different classes.": ""
        },
        {
          "Figure 2. Violin plots of the distribution in (a) note density, (b) length, and (c) velocity for clips from different classes.": "extract\nthe key information using the Krumhansl-Kessler"
        },
        {
          "Figure 2. Violin plots of the distribution in (a) note density, (b) length, and (c) velocity for clips from different classes.": "algorithm [47] in the MIDI ToolBox [48]. Figure 3 shows"
        },
        {
          "Figure 2. Violin plots of the distribution in (a) note density, (b) length, and (c) velocity for clips from different classes.": ""
        },
        {
          "Figure 2. Violin plots of the distribution in (a) note density, (b) length, and (c) velocity for clips from different classes.": "the key distributions on 12 major-minor pitch classes for"
        },
        {
          "Figure 2. Violin plots of the distribution in (a) note density, (b) length, and (c) velocity for clips from different classes.": ""
        },
        {
          "Figure 2. Violin plots of the distribution in (a) note density, (b) length, and (c) velocity for clips from different classes.": "the four emotion quadrants.\nThey assure that\nthe major-"
        },
        {
          "Figure 2. Violin plots of the distribution in (a) note density, (b) length, and (c) velocity for clips from different classes.": "minor\ntonality is an important clue in distinguishing va-"
        },
        {
          "Figure 2. Violin plots of the distribution in (a) note density, (b) length, and (c) velocity for clips from different classes.": "lence.\nIn the high valence group (Q1, Q4),\nthe distribu-"
        },
        {
          "Figure 2. Violin plots of the distribution in (a) note density, (b) length, and (c) velocity for clips from different classes.": "tion is\nskewed to the left\n(major), while,\nin the low va-"
        },
        {
          "Figure 2. Violin plots of the distribution in (a) note density, (b) length, and (c) velocity for clips from different classes.": "lence group (Q2, Q3),\nthe trend is the opposite. We also"
        },
        {
          "Figure 2. Violin plots of the distribution in (a) note density, (b) length, and (c) velocity for clips from different classes.": "measured the distance between the key distributions using"
        },
        {
          "Figure 2. Violin plots of the distribution in (a) note density, (b) length, and (c) velocity for clips from different classes.": "the Jensen-Shannon divergence, which has the property of"
        },
        {
          "Figure 2. Violin plots of the distribution in (a) note density, (b) length, and (c) velocity for clips from different classes.": "symmetry. Table 3 summarizes the pair-wise distances, in-"
        },
        {
          "Figure 2. Violin plots of the distribution in (a) note density, (b) length, and (c) velocity for clips from different classes.": "dicating that the valence difference group has a larger dif-"
        },
        {
          "Figure 2. Violin plots of the distribution in (a) note density, (b) length, and (c) velocity for clips from different classes.": "ference than the arousal difference group."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ﬁer. In the second method, we use the short-chunk ResNet": "following [50], which is composed of 7-layer CNNs with",
          "ﬁne-tuning it is set to the emotion of that sample.": ""
        },
        {
          "ﬁer. In the second method, we use the short-chunk ResNet": "",
          "ﬁne-tuning it is set to the emotion of that sample.": "Evaluation. We use the following three sets of metrics."
        },
        {
          "ﬁer. In the second method, we use the short-chunk ResNet": "residual connections. The output is summarized as a ﬁxed",
          "ﬁne-tuning it is set to the emotion of that sample.": ""
        },
        {
          "ﬁer. In the second method, we use the short-chunk ResNet": "128-dimensional vector through max pooling, which is fol-",
          "ﬁne-tuning it is set to the emotion of that sample.": "• Surface-level objective metrics. We use the following"
        },
        {
          "ﬁer. In the second method, we use the short-chunk ResNet": "lowed by two fully connected layers with the ReLU ac-",
          "ﬁne-tuning it is set to the emotion of that sample.": "three metrics proposed by Dong et al.\n[43]\nto evaluate"
        },
        {
          "ﬁer. In the second method, we use the short-chunk ResNet": "tivation for classiﬁcation.\nThe input audio to the short-",
          "ﬁne-tuning it is set to the emotion of that sample.": "whether the generated samples ﬁt the training data: pitch"
        },
        {
          "ﬁer. In the second method, we use the short-chunk ResNet": "chunk ResNet\nis 3-second excerpts represented as a log-",
          "ﬁne-tuning it is set to the emotion of that sample.": "range (PR), number of unique pitch classes used (NPC),"
        },
        {
          "ﬁer. In the second method, we use the short-chunk ResNet": "scaled mel-spectrogram with 128 bins with 1024-size FFT",
          "ﬁne-tuning it is set to the emotion of that sample.": "and number of notes being played concurrently (POLY)."
        },
        {
          "ﬁer. In the second method, we use the short-chunk ResNet": "(Hanning window), and 512 size hop at 22,050 Hz sam-",
          "ﬁne-tuning it is set to the emotion of that sample.": "We use MusPy [18] to compute these metrics."
        },
        {
          "ﬁer. In the second method, we use the short-chunk ResNet": "pling rate. We randomly sample three seconds of the audio",
          "ﬁne-tuning it is set to the emotion of that sample.": ""
        },
        {
          "ﬁer. In the second method, we use the short-chunk ResNet": "",
          "ﬁne-tuning it is set to the emotion of that sample.": "• Emotion-related objective metrics. Since both REMI"
        },
        {
          "ﬁer. In the second method, we use the short-chunk ResNet": "chunk as an input size to the classiﬁer.",
          "ﬁne-tuning it is set to the emotion of that sample.": ""
        },
        {
          "ﬁer. In the second method, we use the short-chunk ResNet": "",
          "ﬁne-tuning it is set to the emotion of that sample.": "and CP adopt a beat-based representation of music, we"
        },
        {
          "ﬁer. In the second method, we use the short-chunk ResNet": "",
          "ﬁne-tuning it is set to the emotion of that sample.": "employ the LSTM-Attn+REMI emotion classiﬁer\n(cf."
        },
        {
          "ﬁer. In the second method, we use the short-chunk ResNet": "Evaluation. We calculate 4-way classiﬁcation accuracy",
          "ﬁne-tuning it is set to the emotion of that sample.": ""
        },
        {
          "ﬁer. In the second method, we use the short-chunk ResNet": "",
          "ﬁne-tuning it is set to the emotion of that sample.": "Section 5) here to quantify how well\nthe generation re-"
        },
        {
          "ﬁer. In the second method, we use the short-chunk ResNet": "over the four different emotion quadrants and 2-way clas-",
          "ﬁne-tuning it is set to the emotion of that sample.": ""
        },
        {
          "ﬁer. In the second method, we use the short-chunk ResNet": "",
          "ﬁne-tuning it is set to the emotion of that sample.": "sult is inﬂuenced by the emotion condition. We ﬁrst use"
        },
        {
          "ﬁer. In the second method, we use the short-chunk ResNet": "siﬁcation accuracy over either arousal and valence. Tables",
          "ﬁne-tuning it is set to the emotion of that sample.": ""
        },
        {
          "ﬁer. In the second method, we use the short-chunk ResNet": "",
          "ﬁne-tuning it is set to the emotion of that sample.": "the generative model\nto generate 100 samples per class,"
        },
        {
          "ﬁer. In the second method, we use the short-chunk ResNet": "4 and 5 show the results\nin the symbolic and audio do-",
          "ﬁne-tuning it is set to the emotion of that sample.": ""
        },
        {
          "ﬁer. In the second method, we use the short-chunk ResNet": "",
          "ﬁne-tuning it is set to the emotion of that sample.": "and use the assigned label as the target class of the sam-"
        },
        {
          "ﬁer. In the second method, we use the short-chunk ResNet": "mains, respectively. Except for arousal, we can see that the",
          "ﬁne-tuning it is set to the emotion of that sample.": ""
        },
        {
          "ﬁer. In the second method, we use the short-chunk ResNet": "",
          "ﬁne-tuning it is set to the emotion of that sample.": "ple. The trained classiﬁer is then used to make predic-"
        },
        {
          "ﬁer. In the second method, we use the short-chunk ResNet": "deep learning approaches generally outperform the logistic",
          "ﬁne-tuning it is set to the emotion of that sample.": ""
        },
        {
          "ﬁer. In the second method, we use the short-chunk ResNet": "",
          "ﬁne-tuning it is set to the emotion of that sample.": "tion on the generated samples. Similar to the classiﬁca-"
        },
        {
          "ﬁer. In the second method, we use the short-chunk ResNet": "regression classiﬁers using hand-crafted features. In audio",
          "ﬁne-tuning it is set to the emotion of that sample.": ""
        },
        {
          "ﬁer. In the second method, we use the short-chunk ResNet": "",
          "ﬁne-tuning it is set to the emotion of that sample.": "tion task, apart from 4Q classiﬁcation, we also conduct"
        },
        {
          "ﬁer. In the second method, we use the short-chunk ResNet": "domain arousal classiﬁcation, MFCC vectors averaging the",
          "ﬁne-tuning it is set to the emotion of that sample.": ""
        },
        {
          "ﬁer. In the second method, we use the short-chunk ResNet": "",
          "ﬁne-tuning it is set to the emotion of that sample.": "2-way classiﬁcation of both Arousal and Valence aspect."
        },
        {
          "ﬁer. In the second method, we use the short-chunk ResNet": "entire song sequence showed better performance than the",
          "ﬁne-tuning it is set to the emotion of that sample.": ""
        },
        {
          "ﬁer. In the second method, we use the short-chunk ResNet": "deep learning approach with 3-second input.\nIt seems that",
          "ﬁne-tuning it is set to the emotion of that sample.": "• Subjective metrics. As the classiﬁers are not 100% ac-"
        },
        {
          "ﬁer. In the second method, we use the short-chunk ResNet": "wider input sequence has the strength in emotion recogni-",
          "ﬁne-tuning it is set to the emotion of that sample.": "curate, we also resort\nto a user\nsurvey to evaluate the"
        },
        {
          "ﬁer. In the second method, we use the short-chunk ResNet": "tion.\nIn both domains, valence classiﬁcation is a more dif-",
          "ﬁne-tuning it is set to the emotion of that sample.": "emotion-controllability of\nthe models.\nSpeciﬁcally, we"
        },
        {
          "ﬁer. In the second method, we use the short-chunk ResNet": "ﬁcult\ntask compared to arousal classiﬁcation. For valence",
          "ﬁne-tuning it is set to the emotion of that sample.": "deploy an online survey to collect responses to the mu-"
        },
        {
          "ﬁer. In the second method, we use the short-chunk ResNet": "classiﬁcation, MIDI-domain classiﬁers yield better\nresult",
          "ﬁne-tuning it is set to the emotion of that sample.": "sic generated by different models. A subject has to lis-"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": ""
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "the two token representations, MIDI-like seems to outper-"
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": ""
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "form REMI for valence classiﬁcation."
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": ""
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": ""
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "6. EMOTION-CONDITIONED GENERATION"
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "We build the Transformer and LSTM models for emotion-"
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "conditioned symbolic music generation using EMPOIA."
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "For the former, we adopt the Compound Word Transformer"
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": ""
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "[35],\nthe state-of-the-art\nin unconditioned symbolic music"
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "generation. We employ the CP+emotion representation"
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "presented in Section 3.4 as the data representation."
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": ""
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "For\nthe LSTM model, we consider\nthe approach pro-"
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "posed by Ferreira et al.\n[10], which represents the state-"
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": ""
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "of-the-art\nin emotion-conditioned music generation. Our"
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "implementation follows that described in [10], with the fol-"
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "lowing differences: 1) train on EMOPIA rather than VG-"
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": ""
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "MIDI; 2) use 512 neurons instead of 4,096 due to our lim-"
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": ""
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "ited computational resource; 3) use the same linear logis-"
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": ""
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "tic regression layer\nfor classiﬁcation but we classify four"
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": ""
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "classes instead of two."
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": ""
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "As the size of EMOPIA might not be big enough, we"
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": ""
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "use additionally the AILabs1k7 dataset compiled by Hsiao"
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": ""
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "et al.\n[35]\nto pre-train the Transformer. AILabs1k7 con-"
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": ""
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "tains 1,748 samples and is also pop piano music, but\nit"
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": ""
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "does not contain emotion labels. Most of the clips in AIL-"
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": ""
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "abs1k7 are longer\nthan EMOPIA, so to keep the consis-"
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": ""
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "tency of the input sequence length, the length of the token"
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "sequence is set\nto be 1,024. We pre-train the Transformer"
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "with 1e–4 learning rate on AILabs1k7, take the checkpoint"
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "with negative log-likelihood loss 0.30, and then ﬁne-tune it"
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "on EMOPIA with 1e–5 learning rate. During pre-training,"
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "the emotion token is always set\nto be “ignore,” while in"
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "ﬁne-tuning it is set to the emotion of that sample."
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": ""
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "Evaluation. We use the following three sets of metrics."
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": ""
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "• Surface-level objective metrics. We use the following"
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "three metrics proposed by Dong et al.\n[43]\nto evaluate"
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "whether the generated samples ﬁt the training data: pitch"
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "range (PR), number of unique pitch classes used (NPC),"
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "and number of notes being played concurrently (POLY)."
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "We use MusPy [18] to compute these metrics."
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": ""
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "• Emotion-related objective metrics. Since both REMI"
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": ""
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "and CP adopt a beat-based representation of music, we"
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "employ the LSTM-Attn+REMI emotion classiﬁer\n(cf."
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": ""
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "Section 5) here to quantify how well\nthe generation re-"
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": ""
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "sult is inﬂuenced by the emotion condition. We ﬁrst use"
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": ""
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "the generative model\nto generate 100 samples per class,"
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": ""
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "and use the assigned label as the target class of the sam-"
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": ""
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "ple. The trained classiﬁer is then used to make predic-"
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": ""
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "tion on the generated samples. Similar to the classiﬁca-"
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": ""
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "tion task, apart from 4Q classiﬁcation, we also conduct"
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": ""
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "2-way classiﬁcation of both Arousal and Valence aspect."
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": ""
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "• Subjective metrics. As the classiﬁers are not 100% ac-"
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "curate, we also resort\nto a user\nsurvey to evaluate the"
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "emotion-controllability of\nthe models.\nSpeciﬁcally, we"
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "deploy an online survey to collect responses to the mu-"
        },
        {
          "than audio-domain classiﬁers (0.883 vs.\n0.704). Among": "sic generated by different models. A subject has to lis-"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 6: tabulates some of the results. We see that the CP musicwithagiventargetemotiontoacertaindegree.",
      "data": [
        {
          "Objective metrics": "",
          "Subjective metrics": ""
        },
        {
          "Objective metrics": "POLY",
          "Subjective metrics": "Richness"
        },
        {
          "Objective metrics": "5.90",
          "Subjective metrics": "—"
        },
        {
          "Objective metrics": "3.39",
          "Subjective metrics": "2.74±1.12"
        },
        {
          "Objective metrics": "3.48",
          "Subjective metrics": "2.81±1.03"
        },
        {
          "Objective metrics": "4.40",
          "Subjective metrics": "3.22±1.23"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 6: tabulates some of the results. We see that the CP musicwithagiventargetemotiontoacertaindegree.",
      "data": [
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "better), emotion-related objective metrics (4Q classiﬁcation, Arousal classiﬁcation, Valence classiﬁcation;\nthe higher the"
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "better), and subjective metrics (all in 1–5; the higher the better); bold font highlights the best result per metric."
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "(a)  High valence\n(b)  Low valence\nTransformer with pre-training does not score high in the"
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "emotion related objective metrics, the subjective test shows"
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "that\nit can actually control\nthe emotion of\nthe generated"
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "pieces to a certain extent, better\nthan the two competing"
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "models. In particular, the valence of the samples generated"
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "by the Transformer with pre-training has a median rating of"
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "4 when the goal is to generate positive-valence music (i.e.,"
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "Figure 4(a)), while the scores of the other two models are"
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "around 3. Moreover,\nthe arousal of the samples generated"
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "by the Transformer with pre-training has a median rating"
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "(c)  High arousal\n(d)  Low arousal\nof 4 when the goal is to generate high-arousal music (Fig-"
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "ure 4(c)), which is higher than that of the non pre-trained"
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "Transformer.\nThis\nsuggests\nthat\nthe LSTM-Attn classi-"
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "ﬁer employed for computing the emotion-related objective"
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "metrics may not be reliable enough to predict the emotion"
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "perceived by human, and that\nthe Transformer with pre-"
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "training is actually effective in controlling the emotion of"
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "the music it generates to certain extent.\nBut,\nthe model"
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "seems not good enough for\nthe cases of generating low-"
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "valence (i.e., negative) music, as shown in Figure 4(b)."
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "Figure 4. The subjective emotional scores (in 1–5) for the"
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "7. CONCLUSION"
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "generative results when the target emotion is (a) high va-"
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "In\nthis\npaper, we\nhave\nproposed\na\nnew public\ndataset"
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "lence, (b) low valence, (c) high arousal, (d) low arousal."
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "EMOPIA,\na medium-scale\nemotion-labeled\npop\npiano"
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "dataset.\nIt\nis a multi-modal dataset\nthat contains both the"
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "audio ﬁles and MIDI\ntranscriptions of piano-only music,\nten to 12 random-generated samples, one for each of"
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "along with clip-level emotion annotations in four classes.\nthe three models and each of\nthe four emotion classes,"
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "We have also presented prototypes of models for clip-level\nand rate them on a ﬁve-point Likert scale with respect to"
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "music emotion classiﬁcation and emotion-based symbolic\n1) Valence:\nis the audio negative or positive; 2) Arousal:"
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "music generation trained on this dataset, using a number\nis low or high in arousal; 3) Humanness:\nhow well\nit"
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "of state-of-the-art models in respective tasks.\nThe result\nsounds like a piece played by human; 4) Richness:\nis the"
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "shows that we are able to achieve high accuracy in both\ncontent\ninteresting; and, 5) Overall musical quality.\nIn"
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "four-quadrant and valence-wise emotion classiﬁcation, and\ntotal 25 subjects participated in the survey."
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "that our Transformer-based model is capable of generating"
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "Table 6 tabulates some of the results. We see that\nthe CP\nmusic with a given target emotion to a certain degree."
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "Transformer with (‘w/’) pre-training performs the best\nin\nIn the future,\nin-depth importance analysis can be con-"
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "most of the objective metrics and the three subjective met-\nducted to ﬁgure out features that are important for emotion"
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "rics listed here. Nevertheless,\nthe scores of the CP Trans-\nclassiﬁcation, and to seek ways to incorporate those fea-"
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "former with pre-training in the three emotion-related ob-\ntures to the generation model. Many ideas can also be tried"
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "jective metrics are much lower\nthan that\nreported in Ta-\nto further improve the performance of emotion condition-"
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "ble 4, suggesting that either\nthe generated pieces are not\ning, e.g., the Transformer-GAN approach [51]."
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "emotion-laden, or\nthe generated pieces are too dissimilar\nWe share not only the dataset\nitself but\nthe code cov-"
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "to the real pieces to the classiﬁer.\nering all our\nimplemented models in a GitHub repo. We"
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "hope that\nresearchers will ﬁnd this contribution useful\nin\nFigure 4 shows the human assessment of the emotion-"
        },
        {
          "level objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the": "future emotion-related MIR tasks.\ncontrollability of the models. To our surprise, while the CP"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "8. REFERENCES": "",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "sic generation based on emotions,” in Proc. Int. Conf."
        },
        {
          "8. REFERENCES": "[1] Y.\nKim,\nE.\nSchmidt,\nR. Migneco,\nB. Morton,",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": ""
        },
        {
          "8. REFERENCES": "",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "Agents and Artiﬁcial Intelligence, 2018."
        },
        {
          "8. REFERENCES": "P. Richardson, J. Scott, J. Speck, and D. Turnbull, “Mu-",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": ""
        },
        {
          "8. REFERENCES": "sic emotion recognition: A state of\nthe art\nreview,”",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": ""
        },
        {
          "8. REFERENCES": "",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "[14] A. M. Grimaud and T. Eerola, “EmoteControl: an in-"
        },
        {
          "8. REFERENCES": "Int. Society for Music Information Retrieval\nin Proc.",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": ""
        },
        {
          "8. REFERENCES": "",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "teractive system for real-time control of emotional ex-"
        },
        {
          "8. REFERENCES": "Conf., 2010.",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": ""
        },
        {
          "8. REFERENCES": "",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "pression in music,” Personal and Ubiquitous Comput-"
        },
        {
          "8. REFERENCES": "",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "ing, 2020."
        },
        {
          "8. REFERENCES": "[2] Y.-H. Yang and H. H. Chen, Music Emotion Recogni-",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": ""
        },
        {
          "8. REFERENCES": "tion.\nCRC Press, 2011.",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": ""
        },
        {
          "8. REFERENCES": "",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "[15] T.-P. Chen, S. Fukayama, M. Goto, and L. Su, “Chord"
        },
        {
          "8. REFERENCES": "",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "Jazziﬁcation:\nLearning Jazz interpretations of chord"
        },
        {
          "8. REFERENCES": "[3]\nJ.\nS.\nGómez-Cañón,\nE.\nCano,\nP.\nHerrera,\nand",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": ""
        },
        {
          "8. REFERENCES": "",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "symbols,” in Proc. Int. Society for Music Information"
        },
        {
          "8. REFERENCES": "E. Gómez, “Joyful\nfor you and tender\nfor us:\nthe in-",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": ""
        },
        {
          "8. REFERENCES": "",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "Retrieval Conf., 2020."
        },
        {
          "8. REFERENCES": "ﬂuence of\nindividual characteristics and language on",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": ""
        },
        {
          "8. REFERENCES": "emotion labeling and classiﬁcation,” in Proc. Int. Soci-",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": ""
        },
        {
          "8. REFERENCES": "",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "[16] H. H. TAN and D. Herremans, “Music Fadernets: Con-"
        },
        {
          "8. REFERENCES": "ety for Music Information Retrieval Conf., 2020.",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": ""
        },
        {
          "8. REFERENCES": "",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "trollable music generation based on high-level features"
        },
        {
          "8. REFERENCES": "[4]\nJ. S. Gómez-Cañón, E. Cano, Y.-H. Yang, P. Herrera,",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "via low-level feature modelling,” in Proc. Int. Society"
        },
        {
          "8. REFERENCES": "and E. Gómez, “Let’s agree to disagree: Consensus en-",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "for Music Information Retrieval Conf., 2020."
        },
        {
          "8. REFERENCES": "tropy active learning for personalized music emotion",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": ""
        },
        {
          "8. REFERENCES": "",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "[17] Z. Wang, K. Chen, J. Jiang, Y. Zhang, M. Xu, S. Dai,"
        },
        {
          "8. REFERENCES": "recognition,” in Proc. Int. Society for Music Informa-",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": ""
        },
        {
          "8. REFERENCES": "",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "X. Gu, and G. Xia, “POP909: A pop-song dataset for"
        },
        {
          "8. REFERENCES": "tion Retrieval Conf., 2021.",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": ""
        },
        {
          "8. REFERENCES": "",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "Int. Society\nmusic arrangement generation,” in Proc."
        },
        {
          "8. REFERENCES": "[5]\nJ. de Berardinis, A. Cangelosi, and E. Coutinho, “The",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "for Music Information Retrieval Conf., 2020."
        },
        {
          "8. REFERENCES": "multiple voices of musical emotions: Source separa-",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": ""
        },
        {
          "8. REFERENCES": "tion for improving music emotion recognition models",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "[18] H.-W. Dong, K. Chen,\nJ. McAuley,\nand T. Berg-"
        },
        {
          "8. REFERENCES": "and their interpretability,” in Proc. Int. Society for Mu-",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "Kirkpatrick,\n“MusPy: A toolkit\nfor\nsymbolic music"
        },
        {
          "8. REFERENCES": "sic Information Retrieval Conf., 2020.",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "Int. Society for Music Informa-\ngeneration,” in Proc."
        },
        {
          "8. REFERENCES": "",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "tion Retrieval Conf., 2020."
        },
        {
          "8. REFERENCES": "[6] R. Panda, R. Malheiro, B. Rocha, A. Oliveira,\nand",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": ""
        },
        {
          "8. REFERENCES": "R. P. Paiva, “Multi-modal music emotion recognition:",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "[19] A. McLeod,\nJ. Owers,\nand K. Yoshii,\n“The MIDI"
        },
        {
          "8. REFERENCES": "A new dataset, methodology and comparative analy-",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "Degradation Toolkit:\nSymbolic music\naugmentation"
        },
        {
          "8. REFERENCES": "sis,” in Proc. Int. Symposium on Computer Music Mul-",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "and correction,” in Proc. Int. Society for Music Infor-"
        },
        {
          "8. REFERENCES": "tidisciplinary Research, 2013.",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "mation Retrieval Conf., 2020."
        },
        {
          "8. REFERENCES": "[7] R. Panda, R. M. Malheiro, and R. P. Paiva, “Audio fea-",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": ""
        },
        {
          "8. REFERENCES": "",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "[20] C. Hawthorne, E. Elsen,\nJ. Song, A. Roberts,\nI. Si-"
        },
        {
          "8. REFERENCES": "tures for music emotion recognition: A survey,” IEEE",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": ""
        },
        {
          "8. REFERENCES": "",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "mon, C. Raffel, J. Engel, S. Oore, and D. Eck, “Onsets"
        },
        {
          "8. REFERENCES": "Trans. Affective Computing, pp. 1–20, 2020.",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": ""
        },
        {
          "8. REFERENCES": "",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "and Frames: Dual-objective piano transcription,” arXiv"
        },
        {
          "8. REFERENCES": "",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "preprint arXiv:1710.11153, 2018."
        },
        {
          "8. REFERENCES": "[8] S. Chowdhury, A. Vall, V. Haunschmid, and G. Wid-",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": ""
        },
        {
          "8. REFERENCES": "mer, “Towards explainable music emotion recognition:",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": ""
        },
        {
          "8. REFERENCES": "",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "[21] Q. Kong, B. Li, X. Song, Y. Wan,\nand Y. Wang,"
        },
        {
          "8. REFERENCES": "The route via mid-level features,” in Proc. Int. Society",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": ""
        },
        {
          "8. REFERENCES": "",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "“High-resolution piano transcription with pedals by"
        },
        {
          "8. REFERENCES": "for Music Information Retrieval Conf., 2019.",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": ""
        },
        {
          "8. REFERENCES": "",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "arXiv preprint\nregressing onsets\nand offsets\ntimes,”"
        },
        {
          "8. REFERENCES": "",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "arXiv:2010.01815, 2020."
        },
        {
          "8. REFERENCES": "[9] C. Cancino-Chacón, S. Peter, S. Chowdhury, A. Al-",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": ""
        },
        {
          "8. REFERENCES": "janaki, and G. Widmer, “On the characterization of ex-",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": ""
        },
        {
          "8. REFERENCES": "",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "[22] C. Hawthorne, A. Stasyuk, A. Roberts,\nI. Simon, C.-"
        },
        {
          "8. REFERENCES": "pressive performance in classical music: First\nresults",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": ""
        },
        {
          "8. REFERENCES": "",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "Z. A. Huang, S. Dieleman, E. Elsen,\nJ. Engel,\nand"
        },
        {
          "8. REFERENCES": "of the con espressione game,” in Proc. Int. Society for",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": ""
        },
        {
          "8. REFERENCES": "",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "D. Eck,\n“Enabling factorized piano music modeling"
        },
        {
          "8. REFERENCES": "Music Information Retrieval Conf., 2020.",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": ""
        },
        {
          "8. REFERENCES": "",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "and generation with the MAESTRO dataset,” in Proc."
        },
        {
          "8. REFERENCES": "[10] L. N. Ferreira and J. Whitehead, “Learning to generate",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "Int. Conf. Learning Representations, 2019."
        },
        {
          "8. REFERENCES": "music with sentiment,” in Proc. Int. Society for Music",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": ""
        },
        {
          "8. REFERENCES": "",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "[23] K. W. Cheuk, D. Herremans, and L. Su, “ReconVAT: A"
        },
        {
          "8. REFERENCES": "Information Retrieval Conf., 2019.",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": ""
        },
        {
          "8. REFERENCES": "",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "semi-supervised automatic music transcription frame-"
        },
        {
          "8. REFERENCES": "[11] K. Chen, C.-I. Wang, T. Berg-Kirkpatrick, and S. Dub-",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "work for low-resource real-world data,” in Proc. ACM"
        },
        {
          "8. REFERENCES": "nov, “Music SketchNet: Controllable music generation",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "Multimedia, 2021."
        },
        {
          "8. REFERENCES": "via\nfactorized representations of pitch and rhythm,”",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": ""
        },
        {
          "8. REFERENCES": "Int. Society for Music Information Retrieval\nin Proc.",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "[24] D. Bognadov, A. Porter, P. Tovstogan, and M. Won,"
        },
        {
          "8. REFERENCES": "Conf., 2020.",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "“Mediaeval 2019: Emotion and theme recognition in"
        },
        {
          "8. REFERENCES": "",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "music using Jamendo,” in MediaEval 2019 Workshop."
        },
        {
          "8. REFERENCES": "[12] D. Makris, K. R. Agres,\nand D. Herremans,\n“Gen-",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": ""
        },
        {
          "8. REFERENCES": "erating lead sheets with affect: A novel conditional",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "[25] A. Alajanki, Y.-H. Yang, and M. Soleymani, “Bench-"
        },
        {
          "8. REFERENCES": "Int. Joint Conf. Neural\nseq2seq framework,” in Proc.",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "marking music emotion recognition systems,” PLOS"
        },
        {
          "8. REFERENCES": "Networks, 2021.",
          "[13] R. Madhok, S. Goel, and S. Garg, “SentiMozart: Mu-": "ONE, 2016."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "X. Bresson,\n“FMA: A dataset\nfor music\nanalysis,”",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "monyan, “This time with feeling: Learning expressive"
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "Int. Society for Music Information Retrieval\nin Proc.",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "musical performance,” Neural Computing and Appli-"
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "Conf., 2017.",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "cations, 2018."
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "[40] C. Cannam, C. Landone, and M. Sandler, “Sonic Vi-"
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "[27] R. Bittner, J. Salamon, M. Tierney, M. Mauch, C. Can-",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": ""
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "sualiser:\nAn\nopen\nsource\napplication\nfor\nviewing,"
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "nam, and J. Bello, “MedleyDB: A multitrack dataset",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": ""
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "analysing, and annotating music audio ﬁles,” in Proc."
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "for annotation-intensive mir research,” in Proc. Int. So-",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": ""
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "ACM Multimedia, 2010, pp. 1467–1468, https://www."
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "ciety for Music Information Retrieval Conf., 2014.",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": ""
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "sonicvisualiser.org/."
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "[28]\nJ.\nFan, M.\nThorogood,\nand\nP.\nPasquier,\n“Emo-",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": ""
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "[41] L.-L. Balkwill\nand W. Thompson,\n“A cross-cultural"
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "Soundscapes: A dataset for soundscape emotion recog-",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": ""
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "investigation of\nthe perception of emotion in music:"
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "nition,” in Proc. Int. Conf. Affective Computing and In-",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": ""
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "Psychophysical and cultural cues,” Music Perception,"
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "telligent Interaction, 2017.",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": ""
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "vol. 17, pp. 43–64, October 1999."
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "[29]\nJ. Fan, Y.-H. Yang, K. Dong, and P. Pasquier, “A com-",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": ""
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "[42] A. Roberts,\nJ. Engel, C. Raffel, C. Hawthorne,\nand"
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "parative study of Western and Chinese classical mu-",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": ""
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "D. Eck, “A hierarchical\nlatent vector model for learn-"
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "Int. Conf.\nsic based on soundscape models,” in Proc.",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": ""
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "Int. Conf.\ning long-term structure in music,” in Proc."
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "Acoustics, Speech, and Signal Processing, 2020.",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": ""
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "Machine Learning, 2018."
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "[30] M. Soleymani, M. Caro, E. Schmidt, C.-Y. Sha, and",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": ""
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "[43] H.-W. Dong, W.-Y. Hsiao, L.-C. Yang, and Y.-H. Yang,"
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "Y\n.-H. Yang, “1000 songs for emotional analysis of mu-",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": ""
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "“MuseGAN: Multi-track sequential generative adver-"
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "sic,” in Proc. ACM Int. Workshop on Crowdsourcing",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": ""
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "sarial networks for symbolic music generation and ac-"
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "for Multimedia, 2013.",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": ""
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "Intelli-\ncompaniment,” in Proc. AAAI Conf. Artiﬁcial"
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "gence, 2018."
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "[31]\nJ. Fan, K. Tatar, M. Thorogood,\nand P. Pasquier,",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": ""
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "“Ranking-based emotion recognition for experimental",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "[44] N. S. Keskar\net al.,\n“CTRL -\na\nconditional Trans-"
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "music,” in Proc. Int. Society for Music Information Re-",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "former\nlanguage model\nfor controllable generation,”"
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "trieval Conf., 2017.",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "arXiv preprint arXiv:1909.05858, 2019."
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "[32]\nJ. Russell, “A circumplex model of affect,” Journal of",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "[45] P.\nJuslin\nand\nE.\nLindstorm,\n“Musical\nexpression"
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "Personality and Social Psychology, vol. 39, pp. 1161–",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "of\nemotions:\nModelling\nlisteners’\njudgements\nof"
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "1178, December 1980.",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "composed and performed features,” Music Analysis,"
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "vol. 29, pp. 334 – 364, 2011."
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "[33] A. Vaswani, N. Shazeer, N. Parmar,\nJ. Uszkoreit,",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": ""
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "[46] S. R. Livingstone, R. Muhlberger, A. R. Brown, and"
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polosukhin,",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": ""
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "W. F. Thompson, “Changing musical emotion: A com-"
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "“Attention is all you need,” in Proc. Advances in Neu-",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": ""
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "putational rule system for modifying score and perfor-"
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "ral Information Processing Systems, 2017.",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": ""
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "mance,” Computer Music Journal, vol. 34, no. 1, pp."
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "[34] Y.-S. Huang and Y.-H. Yang, “Pop Music Transformer:",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "41–64, 2010."
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "Beat-based modeling and generation of expressive pop",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": ""
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "foundations of musical\n[47] C. L. Krumhansl, Cognitive"
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "piano compositions,” in Proc. ACM Multimedia, 2020.",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": ""
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "pitch.\nOxford University Press, 2001."
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "[35] W.-Y. Hsiao,\nJ.-Y. Liu, Y.-C. Yeh,\nand Y.-H. Yang,",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": ""
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "[48] T. Eerola and P. Toiviainen, MIDI Toolbox: MATLAB"
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "“Compound Word Transformer: Learning to compose",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": ""
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "Tools\nfor Music\nResearch.\nJyväskylä,\nFinland:"
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "full-song music over dynamic directed hypergraphs,”",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": ""
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "University\nof\nJyväskylä,\n2004.\n[Online]. Available:"
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "in Proc. AAAI Conf. Artiﬁcial Intelligence, 2021.",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": ""
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "www.jyu.ﬁ/musica/miditoolbox/"
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "[36]\nJ. Grekow and Z. W. Ra´s, “Detecting emotions in clas-",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": ""
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "[49] Z. Lin, M. Feng, C. N. d. Santos, M. Yu, B. Xiang,"
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "sical music from MIDI ﬁles,” in Proc. Int. Symposium",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": ""
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "B. Zhou, and Y. Bengio, “A structured self-attentive"
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "on Methodologies for Intelligent Systems, 2009.",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": ""
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "Int. Conf. Learning\nsentence\nembedding,”\nin Proc."
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "Representations, 2017."
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "[37] Y. Lin, X. Chen, and D. Yang, “Exploration of music",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": ""
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "emotion recognition based on MIDI,” in Proc. Int. So-",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "[50] M. Won, A. Ferraro, D. Bogdanov, and X. Serra, “Eval-"
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "ciety for Music Information Retrieval Conf., 2013.",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "uation of CNN-based automatic music tagging mod-"
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "els,” in Proc. Sound and Music Conf., 2020."
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "[38] K. Zhao, S. Li,\nJ. Cai, H. Wang, and J. Wang, “An",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": ""
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "emotional symbolic music generation system based on",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "[51] A. Muhamed et al., “Symbolic music generation with"
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "LSTM networks,” in Proc. IEEE Information Technol-",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "Transformer-GANs,” in Proc. AAAI Conf. Artiﬁcial In-"
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "ogy, Networking, Electronic and Automation Control",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": "telligence, 2021."
        },
        {
          "[26] M. Defferrard,\nK.\nBenzi,\nP. Vandergheynst,\nand": "Conf., 2019.",
          "[39] S. Oore,\nI. Simon, S. Dieleman, D. Eck, and K. Si-": ""
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Music emotion recognition: A state of the art review",
      "authors": [
        "Y Kim",
        "E Schmidt",
        "R Migneco",
        "B Morton",
        "P Richardson",
        "J Scott",
        "J Speck",
        "D Turnbull"
      ],
      "year": "2010",
      "venue": "Proc. Int. Society for Music Information Retrieval Conf"
    },
    {
      "citation_id": "3",
      "title": "Music Emotion Recognition",
      "authors": [
        "Y.-H Yang",
        "H Chen"
      ],
      "year": "2011",
      "venue": "Music Emotion Recognition"
    },
    {
      "citation_id": "4",
      "title": "Joyful for you and tender for us: the influence of individual characteristics and language on emotion labeling and classification",
      "authors": [
        "J Gómez-Cañón",
        "E Cano",
        "P Herrera",
        "E Gómez"
      ],
      "year": "2020",
      "venue": "Proc. Int. Society for Music Information Retrieval Conf"
    },
    {
      "citation_id": "5",
      "title": "Let's agree to disagree: Consensus entropy active learning for personalized music emotion recognition",
      "authors": [
        "J Gómez-Cañón",
        "E Cano",
        "Y.-H Yang",
        "P Herrera",
        "E Gómez"
      ],
      "year": "2021",
      "venue": "Proc. Int. Society for Music Information Retrieval Conf"
    },
    {
      "citation_id": "6",
      "title": "The multiple voices of musical emotions: Source separation for improving music emotion recognition models and their interpretability",
      "authors": [
        "J De Berardinis",
        "A Cangelosi",
        "E Coutinho"
      ],
      "year": "2020",
      "venue": "Proc. Int. Society for Music Information Retrieval Conf"
    },
    {
      "citation_id": "7",
      "title": "Multi-modal music emotion recognition: A new dataset, methodology and comparative analysis",
      "authors": [
        "R Panda",
        "R Malheiro",
        "B Rocha",
        "A Oliveira",
        "R Paiva"
      ],
      "year": "2013",
      "venue": "Proc. Int. Symposium on Computer Music Multidisciplinary Research"
    },
    {
      "citation_id": "8",
      "title": "Audio features for music emotion recognition: A survey",
      "authors": [
        "R Panda",
        "R Malheiro",
        "R Paiva"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Affective Computing"
    },
    {
      "citation_id": "9",
      "title": "Towards explainable music emotion recognition: The route via mid-level features",
      "authors": [
        "S Chowdhury",
        "A Vall",
        "V Haunschmid",
        "G Widmer"
      ],
      "year": "2019",
      "venue": "Proc. Int. Society for Music Information Retrieval Conf"
    },
    {
      "citation_id": "10",
      "title": "On the characterization of expressive performance in classical music: First results of the con espressione game",
      "authors": [
        "C Cancino-Chacón",
        "S Peter",
        "S Chowdhury",
        "A Aljanaki",
        "G Widmer"
      ],
      "year": "2020",
      "venue": "Proc. Int. Society for Music Information Retrieval Conf"
    },
    {
      "citation_id": "11",
      "title": "Learning to generate music with sentiment",
      "authors": [
        "L Ferreira",
        "J Whitehead"
      ],
      "year": "2019",
      "venue": "Proc. Int. Society for Music Information Retrieval Conf"
    },
    {
      "citation_id": "12",
      "title": "Music SketchNet: Controllable music generation via factorized representations of pitch and rhythm",
      "authors": [
        "K Chen",
        "C.-I Wang",
        "T Berg-Kirkpatrick",
        "S Dubnov"
      ],
      "year": "2020",
      "venue": "Proc. Int. Society for Music Information Retrieval Conf"
    },
    {
      "citation_id": "13",
      "title": "Generating lead sheets with affect: A novel conditional seq2seq framework",
      "authors": [
        "D Makris",
        "K Agres",
        "D Herremans"
      ],
      "year": "2021",
      "venue": "Proc. Int. Joint Conf. Neural Networks"
    },
    {
      "citation_id": "14",
      "title": "SentiMozart: Music generation based on emotions",
      "authors": [
        "R Madhok",
        "S Goel",
        "S Garg"
      ],
      "year": "2018",
      "venue": "Proc. Int. Conf. Agents and Artificial Intelligence"
    },
    {
      "citation_id": "15",
      "title": "EmoteControl: an interactive system for real-time control of emotional expression in music",
      "authors": [
        "A Grimaud",
        "T Eerola"
      ],
      "year": "2020",
      "venue": "Personal and Ubiquitous Computing"
    },
    {
      "citation_id": "16",
      "title": "Chord Jazzification: Learning Jazz interpretations of chord symbols",
      "authors": [
        "T.-P Chen",
        "S Fukayama",
        "M Goto",
        "L Su"
      ],
      "year": "2020",
      "venue": "Proc. Int. Society for Music Information Retrieval Conf"
    },
    {
      "citation_id": "17",
      "title": "Music Fadernets: Controllable music generation based on high-level features via low-level feature modelling",
      "authors": [
        "H Tan",
        "D Herremans"
      ],
      "year": "2020",
      "venue": "Proc. Int. Society for Music Information Retrieval Conf"
    },
    {
      "citation_id": "18",
      "title": "POP909: A pop-song dataset for music arrangement generation",
      "authors": [
        "Z Wang",
        "K Chen",
        "J Jiang",
        "Y Zhang",
        "M Xu",
        "S Dai",
        "X Gu",
        "G Xia"
      ],
      "year": "2020",
      "venue": "Proc. Int. Society for Music Information Retrieval Conf"
    },
    {
      "citation_id": "19",
      "title": "MusPy: A toolkit for symbolic music generation",
      "authors": [
        "H.-W Dong",
        "K Chen",
        "J Mcauley",
        "T Berg-Kirkpatrick"
      ],
      "year": "2020",
      "venue": "Proc. Int. Society for Music Information Retrieval Conf"
    },
    {
      "citation_id": "20",
      "title": "The MIDI Degradation Toolkit: Symbolic music augmentation and correction",
      "authors": [
        "A Mcleod",
        "J Owers",
        "K Yoshii"
      ],
      "year": "2020",
      "venue": "Proc. Int. Society for Music Information Retrieval Conf"
    },
    {
      "citation_id": "21",
      "title": "Onsets and Frames: Dual-objective piano transcription",
      "authors": [
        "C Hawthorne",
        "E Elsen",
        "J Song",
        "A Roberts",
        "I Simon",
        "C Raffel",
        "J Engel",
        "S Oore",
        "D Eck"
      ],
      "year": "2018",
      "venue": "Onsets and Frames: Dual-objective piano transcription",
      "arxiv": "arXiv:1710.11153"
    },
    {
      "citation_id": "22",
      "title": "High-resolution piano transcription with pedals by regressing onsets and offsets times",
      "authors": [
        "Q Kong",
        "B Li",
        "X Song",
        "Y Wan",
        "Y Wang"
      ],
      "year": "2020",
      "venue": "High-resolution piano transcription with pedals by regressing onsets and offsets times",
      "arxiv": "arXiv:2010.01815"
    },
    {
      "citation_id": "23",
      "title": "Enabling factorized piano music modeling and generation with the MAESTRO dataset",
      "authors": [
        "C Hawthorne",
        "A Stasyuk",
        "A Roberts",
        "I Simon",
        "C.-Z Huang",
        "S Dieleman",
        "E Elsen",
        "J Engel",
        "D Eck"
      ],
      "year": "2019",
      "venue": "Proc. Int. Conf. Learning Representations"
    },
    {
      "citation_id": "24",
      "title": "ReconVAT: A semi-supervised automatic music transcription framework for low-resource real-world data",
      "authors": [
        "K Cheuk",
        "D Herremans",
        "L Su"
      ],
      "year": "2021",
      "venue": "Proc. ACM Multimedia"
    },
    {
      "citation_id": "25",
      "title": "Mediaeval 2019: Emotion and theme recognition in music using Jamendo",
      "authors": [
        "D Bognadov",
        "A Porter",
        "P Tovstogan",
        "M Won"
      ],
      "venue": "Mediaeval 2019: Emotion and theme recognition in music using Jamendo"
    },
    {
      "citation_id": "26",
      "title": "Benchmarking music emotion recognition systems",
      "authors": [
        "A Alajanki",
        "Y.-H Yang",
        "M Soleymani"
      ],
      "year": "2016",
      "venue": "PLOS ONE"
    },
    {
      "citation_id": "27",
      "title": "FMA: A dataset for music analysis",
      "authors": [
        "M Defferrard",
        "K Benzi",
        "P Vandergheynst",
        "X Bresson"
      ],
      "year": "2017",
      "venue": "Proc. Int. Society for Music Information Retrieval Conf"
    },
    {
      "citation_id": "28",
      "title": "MedleyDB: A multitrack dataset for annotation-intensive mir research",
      "authors": [
        "R Bittner",
        "J Salamon",
        "M Tierney",
        "M Mauch",
        "C Cannam",
        "J Bello"
      ],
      "year": "2014",
      "venue": "Proc. Int. Society for Music Information Retrieval Conf"
    },
    {
      "citation_id": "29",
      "title": "Emo-Soundscapes: A dataset for soundscape emotion recognition",
      "authors": [
        "J Fan",
        "M Thorogood",
        "P Pasquier"
      ],
      "year": "2017",
      "venue": "Proc. Int. Conf. Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "30",
      "title": "A comparative study of Western and Chinese classical music based on soundscape models",
      "authors": [
        "J Fan",
        "Y.-H Yang",
        "K Dong",
        "P Pasquier"
      ],
      "year": "2020",
      "venue": "Proc. Int. Conf. Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "31",
      "title": "1000 songs for emotional analysis of music",
      "authors": [
        "M Soleymani",
        "M Caro",
        "E Schmidt",
        "C.-Y Sha",
        "Y.-H Yang"
      ],
      "year": "2013",
      "venue": "Proc. ACM Int. Workshop on Crowdsourcing for Multimedia"
    },
    {
      "citation_id": "32",
      "title": "Ranking-based emotion recognition for experimental music",
      "authors": [
        "J Fan",
        "K Tatar",
        "M Thorogood",
        "P Pasquier"
      ],
      "year": "2017",
      "venue": "Proc. Int. Society for Music Information Retrieval Conf"
    },
    {
      "citation_id": "33",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "34",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Proc. Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "35",
      "title": "Pop Music Transformer: Beat-based modeling and generation of expressive pop piano compositions",
      "authors": [
        "Y.-S Huang",
        "Y.-H Yang"
      ],
      "year": "2020",
      "venue": "Proc. ACM Multimedia"
    },
    {
      "citation_id": "36",
      "title": "Compound Word Transformer: Learning to compose full-song music over dynamic directed hypergraphs",
      "authors": [
        "W.-Y Hsiao",
        "J.-Y Liu",
        "Y.-C Yeh",
        "Y.-H Yang"
      ],
      "year": "2021",
      "venue": "Proc. AAAI Conf. Artificial Intelligence"
    },
    {
      "citation_id": "37",
      "title": "Detecting emotions in classical music from MIDI files",
      "authors": [
        "J Grekow",
        "Z Raś"
      ],
      "year": "2009",
      "venue": "Proc. Int. Symposium on Methodologies for Intelligent Systems"
    },
    {
      "citation_id": "38",
      "title": "Exploration of music emotion recognition based on MIDI",
      "authors": [
        "Y Lin",
        "X Chen",
        "D Yang"
      ],
      "year": "2013",
      "venue": "Proc. Int. Society for Music Information Retrieval Conf"
    },
    {
      "citation_id": "39",
      "title": "An emotional symbolic music generation system based on LSTM networks",
      "authors": [
        "K Zhao",
        "S Li",
        "J Cai",
        "H Wang",
        "J Wang"
      ],
      "year": "2019",
      "venue": "Proc. IEEE Information Technology, Networking, Electronic and Automation Control Conf"
    },
    {
      "citation_id": "40",
      "title": "This time with feeling: Learning expressive musical performance",
      "authors": [
        "S Oore",
        "I Simon",
        "S Dieleman",
        "D Eck",
        "K Simonyan"
      ],
      "year": "2018",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "41",
      "title": "Sonic Visualiser: An open source application for viewing, analysing, and annotating music audio files",
      "authors": [
        "C Cannam",
        "C Landone",
        "M Sandler"
      ],
      "year": "2010",
      "venue": "Proc. ACM Multimedia"
    },
    {
      "citation_id": "42",
      "title": "A cross-cultural investigation of the perception of emotion in music: Psychophysical and cultural cues",
      "authors": [
        "L.-L Balkwill",
        "W Thompson"
      ],
      "year": "1999",
      "venue": "Music Perception"
    },
    {
      "citation_id": "43",
      "title": "A hierarchical latent vector model for learning long-term structure in music",
      "authors": [
        "A Roberts",
        "J Engel",
        "C Raffel",
        "C Hawthorne",
        "D Eck"
      ],
      "year": "2018",
      "venue": "Proc. Int. Conf. Machine Learning"
    },
    {
      "citation_id": "44",
      "title": "MuseGAN: Multi-track sequential generative adversarial networks for symbolic music generation and accompaniment",
      "authors": [
        "H.-W Dong",
        "W.-Y Hsiao",
        "L.-C Yang",
        "Y.-H Yang"
      ],
      "year": "2018",
      "venue": "Proc. AAAI Conf. Artificial Intelligence"
    },
    {
      "citation_id": "45",
      "title": "CTRL -a conditional Transformer language model for controllable generation",
      "authors": [
        "N Keskar"
      ],
      "year": "2019",
      "venue": "CTRL -a conditional Transformer language model for controllable generation",
      "arxiv": "arXiv:1909.05858"
    },
    {
      "citation_id": "46",
      "title": "Musical expression of emotions: Modelling listeners' judgements of composed and performed features",
      "authors": [
        "P Juslin",
        "E Lindstorm"
      ],
      "year": "2011",
      "venue": "Music Analysis"
    },
    {
      "citation_id": "47",
      "title": "Changing musical emotion: A computational rule system for modifying score and performance",
      "authors": [
        "S Livingstone",
        "R Muhlberger",
        "A Brown",
        "W Thompson"
      ],
      "year": "2010",
      "venue": "Computer Music Journal"
    },
    {
      "citation_id": "48",
      "title": "Cognitive foundations of musical pitch",
      "authors": [
        "C Krumhansl"
      ],
      "year": "2001",
      "venue": "Cognitive foundations of musical pitch"
    },
    {
      "citation_id": "49",
      "title": "",
      "authors": [
        "T Eerola",
        "P Toiviainen",
        "Toolbox"
      ],
      "year": "2004",
      "venue": ""
    },
    {
      "citation_id": "50",
      "title": "A structured self-attentive sentence embedding",
      "authors": [
        "Z Lin",
        "M Feng",
        "C Santos",
        "M Yu",
        "B Xiang",
        "B Zhou",
        "Y Bengio"
      ],
      "year": "2017",
      "venue": "Proc. Int. Conf. Learning Representations"
    },
    {
      "citation_id": "51",
      "title": "Evaluation of CNN-based automatic music tagging models",
      "authors": [
        "M Won",
        "A Ferraro",
        "D Bogdanov",
        "X Serra"
      ],
      "year": "2020",
      "venue": "Proc. Sound and Music Conf"
    },
    {
      "citation_id": "52",
      "title": "Symbolic music generation with Transformer-GANs",
      "authors": [
        "A Muhamed"
      ],
      "year": "2021",
      "venue": "Proc. AAAI Conf. Artificial Intelligence"
    }
  ]
}