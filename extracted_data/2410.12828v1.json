{
  "paper_id": "2410.12828v1",
  "title": "Gcm-Net: Graph-Enhanced Cross-Modal Infusion With A Metaheuristic-Driven Network For Video Sentiment And Emotion Analysis",
  "published": "2024-10-02T10:07:48Z",
  "authors": [
    "Prasad Chaudhari",
    "Aman Kumar",
    "Chandravardhan Singh Raghaw",
    "Mohammad Zia Ur Rehman",
    "Nagendra Kumar"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Sentiment analysis and emotion recognition in videos are challenging tasks, given the diversity and complexity of the information conveyed in different modalities. Developing a highly competent framework that effectively addresses the distinct characteristics across various modalities is a primary concern in this domain. Previous studies on combined multimodal sentiment and emotion analysis often overlooked effective fusion for modality integration, intermodalcontextual congruity, optimizing concatenated feature spaces, leading to suboptimal architecture. This paper presents a novel framework that leverages the multi-modal contextual information from utterances and applies metaheuristic algorithms to learn the contributing features for utterance-level sentiment and emotion prediction. Our Graph-enhanced Cross-Modal Infusion with a Metaheuristic-Driven Network (GCM-Net) integrates graph sampling and aggregation to recalibrate the modality features for video sentiment and emotion prediction. GCM-Net includes a cross-modal attention module determining intermodal interactions and utterance relevance. A harmonic optimization module employing a metaheuristic algorithm combines attended features, allowing for handling both single and multi-utterance inputs. To show the effectiveness of our approach, we have conducted extensive evaluations on three prominent multi-modal benchmark datasets, CMU MOSI, CMU MOSEI, and IEMOCAP. The experimental results demonstrate the efficacy of our proposed approach, showcasing accuracies of 91.56% and 86.95% for sentiment analysis on MOSI and MOSEI datasets. We have performed emotion analysis for the IEMOCAP dataset procuring an accuracy of 85.66% which signifies substantial performance enhancements over existing methods.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "The social media evolution driven by the widespread adoption of mobile and networking technology, has undergone a remarkable transformation marked by an unprecedented surge in multimodal content and a significant increase in the volume of expressions observed on these platforms. Users effortlessly utilize videos  (Poria, Cambria, Hazarika, Majumder, Zadeh and Morency, 2017)  to convey a diverse set of expressions, reflecting their sentiments and emotions through the integration of text, audio, and visual data. This shift towards multimodal content  (Shi, Wu, Guo, Hu, Chen, Zheng and He, 2022)  underscores the need for refined methods and data analysis techniques to navigate the intricacies of human emotions embedded within multimedia content. Recognizing this evolving landscape, our research introduces a novel Video Sentiment Analysis and Emotion Recognition framework, as a solution to figure out the detailed reciprocation of human sentiments and emotions encapsulated in videos.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Advancing From Uni-Modality To Multi-Modality",
      "text": "Traditionally, sentiment analysis and emotion recognition tasks were predominantly centered around unimodal approaches  (Zhang, Wang and Liu, 2018) , with a primary focus on textual content where the inter-relationships among words and phrases were considered. As social media evolved to include more multimodal content, the shortcomings of unimodal approaches  (Devlin, Chang, Lee and Toutanova, 2019 ) became more evident. Consequently, depending solely on textual content  (Gong, Teng, Teng, Zhang, Du, Chen, Bhuiyan, Li, Liu and Ma, 2020)  proves inadequate for extracting human sentiments, as the interpretation of speaker expressions often evolves dynamically, influenced   (Zhu, Zhu, Zhang, Xu and Kong, 2023) . This unimodal approach relying on text also posed challenges in understanding the intricate details conveyed through visual and audio data  (Yang, Xu and Gao, 2020) , as well as need of efficient techniques for fusion of these modalities.\n\nThe Video Sentiment Analysis and Emotion Recognition (VSAER) task aligns with the paradigm shift towards multimodality. It contrasts the traditional text-based approaches  (Zadeh, Chen, Poria, Cambria and Morency, 2017)  by incorporating information from various modalities, such as visuals and acoustics. This procedure is illustrated in a generalized way in Figure  1 . However, this integration presents a significant challenge due to the inherent heterogeneity of sentiment and emotional information across these modalities. Unlike unimodal analysis, where each modality carries consistent semantic meaning, multimodal signals in VSAER are often disparate. Text, for example, is composed of discrete words with specific meanings, while visuals and audio consist of continuous digital signals. This disparity necessitates a robust fusion framework to effectively integrate these heterogeneous sentiment sources which turns out as a crucial aspect in VSAER research domain.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Research Trajectory In Video Sentiment Analysis And Emotion Recognition",
      "text": "Prior research works have emphasized the development of comprehensive sentimental and emotional representations, capturing intricate intra-modal and inter-modal interactions through fusion techniques and yielded notable advancements in VSAER tasks. As shown in Figure  1 , generic architectures utilize multi-input from a video and generate emotion or sentiment as output, where different fusion mechanisms are employed for cross-modal information extraction.\n\nConsidering these approaches, more concentration was on diverse fusion architectures to facilitate interactions within and between modalities. These approaches can be broadly considered feature-fusion-based, graph-based, and deep learning-based models. We investigated feature-fusion-based models such as THMM  (Tri-modal Hidden Markov Model)  by  (Morency, Mihalcea and Doshi, 2011)  that extracts features from all modalities, concatenates them, and passes through a tri-modal HMM classifier. The TFN (Tensor Fusion Network) proposed by  (Zadeh et al., 2017)  is based on the concept that learns both the intra-modality and inter-modality dynamics end-to-end using tensor fusion. The LMF (Low-rank Multimodal Fusion)  (Liu, Shen, Lakshminarasimhan, Liang, Zadeh and Morency, 2018)  model works by capturing the low-rank tensors for efficient multimodal representation. We then explored graph-based models  (Gong et al., 2020)  such as Adversarial Representation Graph Fusion (ARGF)  (Mai, Hu and Xing, 2014)  uses adversarial learning to refine representations with graph fusion. Multi-channel Attentive Graph Convolutional Network (MAGCN)  (Cheng, Wang, Tao, Xie and Gao, 2020)  combines multi-channel attention with graph convolutions for joint learning. Deep Learning model, COSMIC  (Ghosal, Majumder, Gelbukh, Mihalcea and Poria, 2020)  combines cross-modal similarities using a shared attention mechanism. The SIMR  (Wang, Wang, Lin, Xu and Guo, 2023)  learns shared multimodal representations while preserving modality-specific details. The AOBERT  (Kim and Park, 2023)  extends BERT for joint encoding of text and visual information.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Gaps In Existing Research",
      "text": "Multimodal sentiment analysis models both intra-modal dynamics and inter-modal dynamics. Intra-modality dynamics represent interactions within a specific modality, such as interactions between words in a sentence. Intermodality dynamics denotes the interactions between different modalities. Current multimodal sentiment analysis methods struggle to concurrently quantify both intra-modal and inter-modal dynamics. To address this, we propose a novel technique that explicitly harnesses both types of dynamics. However, capturing these dynamics often presents a challenge with traditional fusion approaches.\n\nFurther, previous approaches, while applying attention solely to the contextual utterance for classification did not fully account for the correlations among the modalities of the target utterance and the context utterances, which in turn hindered the accurate distinction of the most relevant modalities for sentiment and emotion  (Liu, Gao, Li, Fu and Ding, 2023)  prediction of the target utterance. Consequently, this approach resulted in suboptimal multimodal feature representation when combining modalities from the context with those of the target utterance  (Huang, Pu, Zhou, Cao, Gu, Zhao and Xu, 2024) . We therefore extend our research, by building upon the strengths and constructively focusing on the limitations identified in earlier studies.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Technical Insights Of Proposed Framework And Contributions",
      "text": "This paper introduces Graph-enhanced Cross-Modal Infusion with a Metaheuristic-Driven Network (GCM-Net) for VSAER tasks. GCM-Net employs four key modules for enhanced sentiment and emotion analysis: Graphbased Feature Recalibration and Enrichment (FRE), Intermodal Contextual Interaction Module (ICIM), Harmonic Optimization Algorithm (HOA), and a classifier module. FRE analyzes each utterance by building a network. It leverages modality-specific graphs to capture both temporal context and feature relationships with nearby features within each modality. This graph-based feature enrichment surpasses individual features alone, enabling more accurate cross-modal representation. Furthermore, FRE reconstructs features by graphically sampling and combining feature  (Zhao, Yang, Zhang and Wang, 2022)  relativity within the modality-specific graph network. By reconstructing these features, FRE encompasses a broader range of features surpassing the individual features that assist in improved inter-modality feature reconstruction. Further, we incorporate ICIM, which creates an amplified and more informative cross-modal feature representation. It computes pairwise attention scores between modalities, learning each modality's contribution to the overall sentiment and emotion. This cross-modal attention mechanism allows ICIM to capture interactions that might be missed by analyzing modalities independently. Finally, HOA is a metaheuristic approach that actively explores the solution space and selects an optimal subset of features. This effectively addresses data redundancy, which is a known challenge in late fusion approaches. HOA reduces dimensionality by selectively discarding irrelevant features, resulting in a compact and informative feature set. These optimized features are subsequently fed into a classifier, leading to demonstrably improved classification performance in comparison to the existing approaches.\n\nWe evaluate our Model on two subtasks: Multimodal Sentiment Analysis (MSA) and Multimodal Emotion Recognition (MER). Three public datasets are used: CMU-MOSI  (Zadeh, Zellers, Pincus and Morency, 2016) ,  CMU-MOSEI (Bagher Zadeh, Liang, Poria, Cambria and Morency, 2018) , and IEMOCAP  (Busso, Bulut, Lee, Kazemzadeh, Provost, Kim, Chang, Lee and Narayanan, 2008) . The experimental results demonstrate that our model outperforms the existing techniques. Furthermore, the ablation study and further analysis prove the effectiveness of different components of our proposed model. Our main contributions to this paper are summarized as follows:\n\nâ€¢ We propose a unified video sentiment and emotion analysis framework named Graph-enhanced Cross-Modal Infusion with a Metaheuristic Driven Network for Video Sentiment and Emotion Analysis (GCM-Net).\n\nâ€¢ This novel framework integrates a graph-based modality-specific feature recalibration approach to capture intricate details often unexamined by approaches based on early fusion techniques.\n\nâ€¢ We incorporate a Intermodal Contextual Interaction Module to dynamically assign weights to each modality's representation based on its significance in the fusion process. This ensures each modality contributes most effectively in the model training phase.\n\nâ€¢ We employ a harmonic optimization algorithm, to efficiently identify the optimal feature subset using a population-based metaheuristic approach. This solves the data redundancy challenge in late fusion models.\n\nâ€¢ Extensive experimental analysis on three benchmark datasets demonstrates that our proposed mode, GCM-Net model effectively addresses limitations of previous work and exhibits improved efficiency and generalizability compared to existing approaches.\n\nThe organization of this paper is as follows: Section 2 provides an overview of the related work, discussing the current state-of-the-art research on multimodal sentiment and emotion prediction. The problem formulation is explained in Section 3. Furthermore, in Section 4 we illustrate our proposed approach and implementation details to cater the problem definition. The are provided in Section 5 presents experimental results conducted on public datasets to show the performance and robustness of the proposed architecture. Finally, our proposed work is summarized with the conclusion in Section 6.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Related Works",
      "text": "This section provides a comprehensive review of existing research on VSAER. Recent studies have proposed various VSAER models, which can be broadly categorized into two main architectural approaches: Deep Learningbased Methods and Modality Fusion-based Methods with Graph-based models. We delve into each of these approaches in detail within the following subsections.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A) Deep Learning-Based Methods",
      "text": "This section explores deep learning approaches for sentiment or emotion analysis, particularly in the context of dialogue scenarios where sentiment interactions are often more complex. Capturing the subtle sentiment associations between participants in a conversation remains a significant challenge.  Hazarika et al. (Hazarika, Poria, Mihalcea, Cambria and Zimmermann, 2018)  proposed an interactive conversational memory network (ICMN) that leverages global memories to generate contextual summaries, facilitating multimodal sentiment detection. Zhang et al.  (Zhang, Li, Song, Zhang1 and Wang, 2019)  introduced a novel quantum-inspired interactive network (QLM) that combines elements of quantum theory with Long Short-Term Memory (LSTM) networks  (Rajagopalan, Morency, Baltrusaitis and Goecke, 2016)  to capture both intra-utterance and inter-utterance interaction dynamics. Additionally, Ghosal et al.  (Ghosal et al., 2020)  presented the COSMIC framework, which incorporates commonsense reasoning to learn the interrelationships between speakers in a conversation.\n\nThe emergence of deep learning technologies has significantly impacted various research fields due to their impressive performance. Among these, the Transformer model, renowned for its application in machine translation, has garnered considerable attention. This sequence-to-sequence architecture leverages solely attention mechanisms, eschewing recurrent and convolutional structures. Notably, the Transformer establishes associations between each element within a sequence during sequential data modeling and context mining, leading to superior accuracy, stability, and speed. Consequently, researchers are actively exploring its potential in diverse domains beyond machine translation.\n\nThe Transformer model has been applied to unimodal representation correlation in a multiple research works. To record the interactions between multimodal sequences at various time steps, Tsai et al.  (Tsai, Bai, Liang, Kolter, Morency and Salakhutdinov, 2019) used a Multimodal Transformer (MulT) . The Transformer has additionally shown promise in merging unimodal features for emotion analysis.  Rahman et al. (Rahman, Hasan, Lee, Zadeh, Mao, Morency and Hoque, 2020)  used huge pre-trained Transformers for multimodal information integration, while Delbrouck et al.  (Delbrouck, Tits, Brousmiche and Dupont, 2020 ) used a Transformer framework to combine several unimodal representations for multimodal emotion analysis(MER).\n\nRecent advancements include the work of  (Wang et al., 2023)  which suggested a Transformer-based multimodal encoding-decoding translation network that prioritizes textual data using a combined encoding-decoding strategy, is one example of recent advances. The Speaker-Independent Multimodal Representation (SIMR) framework was established by  (Wang et al., 2023)  to minimize the impact of customized speech and visual elements. This approach employs a Cross-modal Transformer module to concurrently identify compatible and incompatible cross-modal interactions, splitting nonverbal inputs into style encoding and content representation. Additionally, All-modalities-inone Bidirectional Encoder Representations from Transformers (AOBERT), a single-stream Transformer pre-trained on two tasks concurrently, was presented by  (Kim and Park, 2023)  to capture linkages and dependencies between modalities. When taken as a whole, this research demonstrates how promising the Transformer approach is as a basis for multimodal sentiment analysis.\n\nFurthermore, to address challenges in multimodal emotion recognition (MER),  Dai et al. (Dai, Liu, Yu and Fung, 2020 ) present the EmoEmbs model, which introduces a modality-transferable approach using emotion embeddings. This model learns mapping functions to translate pre-trained word embeddings into visual and auditory spaces, hence representing emotion categories for textual input. The model determines the representation distance for each modality between the goal emotions and the input sequence and then uses this distance to forecast outcomes. This approach's reliance on pre-trained word embeddings may result in suboptimal performance for non-textual modalities.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "B) Modality Fusion-Based Methods",
      "text": "Modality fusion-based methods  (Dai, Yan, Cheng, Duan and Wang, 2023)  for VSAER involves combining information from multiple modalities, such as audio, visual, and textual data, to achieve improved sentiment understanding. These methods can be broadly categorized into three main approaches: tensor-based fusion, translationbased fusion, and attention-based fusion.\n\nTensor-based fusion leverages tensors to represent and combine multimodal features. Representative models include the Tensor Fusion Network (TFN)  (Zadeh et al., 2017)  and Low-rank Multimodal Fusion (LMF)  (Liu et al., 2018) . TFN employs a three-fold Cartesian product for multimodal representation fusion, while LMF focuses on optimizing fusion efficiency and temporal modeling. However, these methods may prioritize low-level features over contextual information, potentially hindering their performance in complex scenarios like dialogue analysis.\n\nTranslation-based fusion approaches multimodal sentiment analysis by translating representations from one modality to another. The Multimodal Cyclic Translation Network (MCTN)  (Pham, Liang, Manzini, Morency and PÃ³czos, 2018)  exemplifies this approach, where representations are iteratively translated between modalities to learn increasingly discriminative joint representations. While MCTN enhances robustness to missing or corrupted modalities, the cyclic translation process can be computationally expensive and time-consuming.\n\nAttention-based fusion utilizes attention mechanisms to selectively focus on relevant information from each modality during the fusion process. Models like the Multi-attention Recurrent Network (MARN)  (Li, Wang, Tan, Zeng, Ou and Zheng, 2020) , Recurrent Attended Variation Embedding Network (RAVEN) Huang, Dong, Wang, Hao, Singhal, Ma, Lv, Cui, Mohammed, Liu, Aggarwal, Chi, Bjorck, Chaudhary, Som, Song and Wei, and modalutterance-temporal attention (MUTA)  (Tang, Xiao, Zhou, Li, Chen and Li, 2023)  fall under this category. Attentionbased methods generally outperform other fusion approaches in sentiment analysis and emotion recognition tasks. However, their parallel structure might neglect the inherent coherence of human emotions, and simple concatenation techniques commonly employed may overlook modality-specific information.\n\nMoreover,  (Dai, Cahyawijaya, Liu and Fung, 2021a)  proposed a MESM that builds a fully end-to-end model that connects and jointly optimizes the two phases for the Multimodal emotion recognition (MER) task. Here they rearrange the current datasets to make end-to-end training easier. The feature extraction process made use of a sparse cross-modal attention mechanism in order to minimize the computing overhead caused by the end-to-end model. However, one shortcoming of this approach is that the rearrangement of datasets might limit its generalizability to other datasets or real-world scenarios.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C) Graph-Based Methods",
      "text": "In VSAER, graph neural networks (GNNs) have become an effective means for modeling intricate interactions between all three modalities. In contrast to conventional models, GNNs depict data as expressive graphs, in which nodes stand for distinct modalities and edges show how they interact. GNNs can discover hidden dependencies and acquire richer representations thanks to this graph-based method, which eventually improves sentiment prediction accuracy.\n\nSeveral recent studies have exhibited the efficacy of GNNs by using a hierarchical graph neural network to build the encoded multimodal representation fusion, the Adversarial Representation Graph Fusion model (ARGF)  (Mai et al., 2014)  uses adversarial training to overcome the modality distribution gap. For learning in-depth intra and intermodal temporal relationships, the multimodal graph network turns unaligned sequences into a graph and uses cuttingedge graph convolution and pooling methods. Moreover, the Multi-channel Attentive Graph Convolutional Network (MAGCN)  (Cheng et al., 2020)  integrates sentiment-related knowledge into inter-modality feature representations by utilizing multi-head self-attention and densely connected graph convolutional networks to capture inter-modality dynamics. Further, in addition to graph neural network and attention, our model effectively capitalizes the modality fusion, intermodal contextual congruity, and suboptimal feature space optimization that facilitates enhanced video understanding and classification.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Definition And Formulations",
      "text": "This study investigates the problem of automatically analyzing sentiment and emotion in video data. We consider a corpus of ğ‘€ videos, denoted as ğ‘‰ = {ğ‘‰ ğ‘– } ğ‘€ ğ‘–=1 . Each video is segmented into a sequence of ğ‘ utterances, represented as ğ‘ˆ = {ğ‘ˆ ğ‘– } ğ‘ ğ‘–=1 . For a given video ğ‘‰ ğ‘— âˆˆ ğ‘‰ , its corresponding utterance sequence is denoted as ğ‘ˆ ğ‘— = {ğ‘ˆ ğ‘—,ğ‘– } ğ‘ ğ‘–=1 , where, ğ‘ˆ ğ‘—,ğ‘– refers to the ğ‘– ğ‘¡â„ utterance within that video. Therefore, each utterance is represented as a multimodal sequence ğ‘ˆ ğ‘š ğ‘—,ğ‘– , where, ğ‘ˆ ğ‘š ğ‘—,ğ‘– âˆˆ {ğ‘ˆ ğ‘¡ ğ‘—,ğ‘– , ğ‘ˆ ğ‘ ğ‘—,ğ‘– , ğ‘ˆ ğ‘£ ğ‘—,ğ‘– } corresponds to a raw unimodal sequence extracted from the ğ‘– ğ‘¡â„ utterance of the ğ‘— ğ‘¡â„ video. Our research is focused towards achieving two primary objectives, each addressing key aspects of sentiment and emotion analysis within multimodal data. We aim to create a combined architecture that possesses capability to accurately predict both emotion and sentiment from the input data.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Problem 1: Sentiment Analysis",
      "text": "The sentiment analysis task involves predicting the sentiment label ğ‘† ğ‘  ğ‘– âˆˆ {0, 1} for a given multimodal utterance ğ‘ˆ ğ‘š ğ‘—,ğ‘– . Here, 1 represents positive sentiment and 0 represents negative sentiment. We aim to learn a function ğ‘“ ğ‘† âˆ¶ ğ‘ˆ ğ‘š ğ‘—,ğ‘– â†¦ ğ‘† ğ‘  ğ‘– that effectively maps multimodal utterances to their corresponding sentiment categories.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Problem 2: Emotion Recognition",
      "text": "We explore emotion prediction in addition to sentiment to extract more meaningful information from the data. Mathematically, this task seeks to predict the emotion category ğ‘† ğ‘’ ğ‘– âˆˆ {1, 2, â€¦ , ğ¶} for a given multimodal utterance ğ‘ˆ ğ‘š ğ‘—,ğ‘– , where, ğ¶ denotes the total number of emotion categories. We aim to learn a function ğ‘“ ğ¸ âˆ¶ ğ‘ˆ ğ‘š ğ‘—,ğ‘– â†¦ ğ‘† ğ‘’ ğ‘– that efficiently maps multimodal utterances to their corresponding emotion categories.\n\nOur proposed model, denoted as GCM-Net, intricately fuses information from text (ğ‘ˆ ğ‘¡ ğ‘– ), acoustic (ğ‘ˆ ğ‘ ğ‘– ), and visual (ğ‘ˆ ğ‘£ ğ‘– ) modalities to derive these predictions as illustrated in Equation  1 :\n\nThis study presents an innovative and methodical approach that contributes significantly to the understanding of the intricate sentiment and emotion themes inherent in multimodal video data.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Methodology",
      "text": "In this section, we elaborate on our architecture and the modality fusion approaches adopted for precise sentiment and emotion classification. Our proposed approach, as illustrated in Figure  2 , employs distinct submodules to achieve multimodal sentiment analysis. First, we project the input multimodal video data into feature adjacency matrices. The Feature Recalibration and Enrichment (FRE) module then refines these matrices by leveraging graph sampling, aggregation, and Bi-GRU layers. Subsequently, the Intermodal Contextual Interaction Module (ICIM) analyzes the interactions between all three modalities, fusing the enriched multisource representations. To address data redundancy, the Harmonic Optimization module selects an optimal feature subset based on calculated fitness. Finally, the classification module utilizes this refined feature subset to generate sentiment and emotion predictions.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Feature Recalibration And Enrichment",
      "text": "We take input videos from benchmark datasets, segment them into utterances, and obtain embeddings of textual, audio, and video data, respectively. Each video consists of a sequence of these elements, where, ğ‘ is the number of segments in the utterance. As for every existing video, we have ğ‘‰ = {ğ‘ˆ 1 , ğ‘ˆ 2 , â€¦ , ğ‘ˆ ğ‘ }, where each utterance is comprised of different elements as illustrated in Equation  2 .\n\nHere, ğ‘ˆ ğ‘¡ ğ‘– , ğ‘ˆ ğ‘ ğ‘– , ğ‘ˆ ğ‘£ ğ‘– denote the textual, acoustic and visual segments of utterance ğ‘ˆ ğ‘– .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Feature Recalibration Using Graph Sampling",
      "text": "For multimodal feature enrichment, the features extracted from each modality undergo a feature reconstruction process through a graph-based method. This process takes into account their temporal context and interrelation with neighboring features, measured by cosine similarity. This feature recalibration is performed independently for each modality as shown in Figure  3 .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "(A) Graph-Based Sampling And Aggregation:",
      "text": "We use Graph Sampling and Aggregation (GraphSAGE), a variation of the Graph Convolutional Neural Network (GCN), for feature enrichment illustrated on lines 1-16 in Algoirthm 1. This method works by carefully selecting and compiling characteristics from a node's immediate vicinity inside the graph. GraphSAGE efficiently captures contextual information while enhancing computing efficiency by concentrating on the near neighborhood of each node, which is especially useful for large-scale graphs.\n\nWe initiate the process by calculating the cosine similarity-based adjacency matrix ğ´ to capture relationships between feature embeddings and create an updated matrix as illustrated in Equation  3 :\n\nIn the above loss function, ğ‘£ is a node co-occurring near ğ‘¢ in a fixed-length random walk. The element ğ¸ ğ‘›âˆ¼ğ‘ƒ ğ‘› (ğ‘£) represents expectation over negative samples, where, ğ‘› is sampled from the negative sampling distribution ğ‘ƒ ğ‘› (ğ‘£). Furthermore, ğ‘„ defines the number of negative samples. Finally, log(ğœ(-ğ‘§ ğ‘‡ ğ‘¢ ğ‘§ ğ‘£ğ‘› )), measures the dissimilarity between the representation of node ğ‘¢ and the representations of the negatively sampled nodes ğ‘£ğ‘›.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "(C) Aggregator Function:",
      "text": "The model incorporates an aggregator function to combine the contextual information extracted from the Bi-GRU layer for each segment or utterance. While various options exist, such as mean, LSTM, and pooling aggregators, extensive evaluations revealed that the LSTM aggregator consistently outperformed the others. Consequently, the LSTM aggregator is employed within the model.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Lstm Aggregator:",
      "text": "The LSTM aggregator is based on an LSTM architecture, capturing long-term dependencies and intricate temporal patterns. It is defined in line 4 in Algorithm 1:\n\nAs shown in Equation  5 , the element ğ¡ ğ‘ ğ‘˜-1 , can be explained as,\n\n) and ğ‘ ğ‘˜ is a bias term.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "(D) Graph-Based Recalibrated Feature:",
      "text": "The final enriched features for each segment ğ‘£ are obtained using a transformation function as illustrated in Equation  6 :\n\nwhere, the variable ğ– ğ‘‡ donotes the learnable weight matrix and ğ‘ ğ‘‡ signifies the corresponding bias term.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Bi-Directional And Contextual Feature Enrichment",
      "text": "To enhance the procured multimodal features, we integrate a Bidirectional Gated Recurrent Unit (Bi-GRU)  (Miao, Ji and Peng, 2020 ) layer into our model architecture. This critical addition of Bi-GRU layer empowers our model to capture temporal dependencies inherent in the data, enabling a profound understanding of sequential patterns within multimodal features. For each segment or utterance ğ‘£, this integrated Bi-GRU layer conscientiously computes both forward ( âƒ– âƒ— ğ¡ ğ‘£ ) and backward ( âƒ–âƒ– ğ¡ ğ‘£ ) hidden states. These hidden states further assist in the forward pass propagation.\n\n(a) Forward Pass (Forward Hidden States): In the Bi-GRU layer, the forward hidden states âƒ– âƒ— ğ‘ ğ‘¡ are iteratively computed for each time step (t). This calculation involves applying an activation function ğ‘” 1 to a weighted combination of the previous forward hidden state âƒ– âƒ— ğ‘ ğ‘¡+1 , the current input element ğ‘¥ ğ‘¡ , and learnable weight matrices (ğ‘Š ğ‘ğ‘ and ğ‘Š ğ‘ğ‘¥ ).",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "(B) Backward Pass (Backward Hidden States):",
      "text": "Similarly, the backward hidden states (âƒ–âƒ– ğ‘ ğ‘¡ ) are calculated for each time step in the backward direction. The process mirrors the forward pass, using the same activation function ğ‘” 1 but considering the previous backward hidden state (âƒ–âƒ– ğ‘ ğ‘¡-1 ) instead of the forward hidden state.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "(C) Final Hidden States (Combining Forward And Backward States):",
      "text": "Once both forward and backward hidden states are obtained, they are concatenated to form the final hidden states (ğ‘ ğ‘¡ ) for each segment ğ‘£. This final representation incorporates contextual information from both directions, enabling the Bi-GRU to capture a more comprehensive context for downstream tasks.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Dimension Consistency Via Multimodal Feature Projection To Dense Layers",
      "text": "Three distinct Bidirectional Gated Recurrent Unit (Bi-GRU) layers are successively processed through the output from the graph recalibration process (î‰ ğ‘£ ), using forward and backward state concatenation. As a result, fully connected dense layers get the outputs from the Bi-GRU layers, reducing the dimensionality of the features to a common size. Three matrices are produced as a result: ğ‘‡ ğ‘ğ‘–ğ‘”ğ‘Ÿğ‘¢ âˆˆ â„ ğ‘¢Ã—ğ‘‘ (text), ğ‘‰ ğ‘ğ‘–ğ‘”ğ‘Ÿğ‘¢ âˆˆ â„ ğ‘¢Ã—ğ‘‘ (visual), and ğ´ ğ‘ğ‘–ğ‘”ğ‘Ÿğ‘¢ âˆˆ â„ ğ‘¢Ã—ğ‘‘ (acoustic), where, ğ‘¢ represents the number of utterances and ğ‘‘ is the number of neurons in the dense layer.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Intermodal Contextual Interaction Module (Icim)",
      "text": "The modality-specific features are procured from the dense layer channels for each audio, video, and textual data with uniform dimensionality. They are fed forward to the ICIM which is based on a cross-modal attention mechanism aiming to capture the interactions between different modalities illustrated in Figure  4 . We compute pairwise attentions for each pair of modalities such as (ğ‘‰ ğ‘ğ‘–ğ‘”ğ‘Ÿğ‘¢ & ğ‘‡ ğ‘ğ‘–ğ‘”ğ‘Ÿğ‘¢ ), ğ‘‡ ğ‘ğ‘–ğ‘”ğ‘Ÿğ‘¢ & ğ´ ğ‘ğ‘–ğ‘”ğ‘Ÿğ‘¢ ), and (ğ´ ğ‘ğ‘–ğ‘”ğ‘Ÿğ‘¢ & ğ‘‰ ğ‘ğ‘–ğ‘”ğ‘Ÿğ‘¢ ). As shown in Equation  7 , we can calculate the attention scores between queries and keys :\n\nwhere, ğ‘ ğ‘–ğ‘— is the attention score between the ğ‘–-th token in the query and the ğ‘—-th token in the key, ğ‘… is the query matrix, ğ¾ is the key matrix and ğ‘‘ ğ‘˜ is the dimensionality of the keys.\n\nIn particular, for (ğ´ ğ‘ğ‘–ğ‘”ğ‘Ÿğ‘¢ & ğ‘‰ ğ‘ğ‘–ğ‘”ğ‘Ÿğ‘¢ ), we obtain the modality representations of ğ´ ğ‘ğ‘–ğ‘”ğ‘Ÿğ‘¢ and ğ‘‰ ğ‘ğ‘–ğ‘”ğ‘Ÿğ‘¢ from the Bi-GRU network, which encodes the contextual information of the utterances for each modality coming from the graph enrichment process. We then compute a pair of matching matrices ğ‘€ 1 , ğ‘€ 2 âˆˆ â„ ğ‘¢Ã—ğ‘¢ over the two representations, which measure the cross-modal similarity between the utterances as illustrated in Equation  8 .\n\nThe probability distribution scores ğ‘ 1 and ğ‘ 2 are computed over ğ‘€ 1 and ğ‘€ 2 using the softmax function as shown in Equation  9 to compute modality-wise attentive representations (ğ‘Œ 1 and ğ‘Œ 2 ):\n\nFinally, the attention matrices ğ´ 1 and ğ´ 2 is obtained by taking dot product again with V and A respectively.\n\nThese attention matrices ğ´ 1 and ğ´ 2 computed in Equation 10 are then concatenated to original refined values to increase feature size and the final refined embeddings are mentioned to be ğ» ğ‘£ .",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Feature Optimization Techniques",
      "text": "This section explains the functioning of the population-based metaheuristic algorithm for feature selection and optimization in the proposed model. Population-based optimization techniques, known for their randomized search for optimization problems, lack certainty in discovering a solution in a single execution. However, with increased Building upon these embeddings, advanced optimization techniques were employed to enhance feature subsets. The Harmonic Optimization Algorithm (HOA) systematically explored the solution space, ensuring the selection of informative and contextually relevant features. This enriched feature set undergoes further refinement through the (Ağ›½HC) local search strategy, which assists in fine-tuning the selected features for optimal precision. The classification phase involves the utilization of the K-Nearest Neighbors (KNN) classifier  (Cunningham and Delany, 2020) , providing distinctive insights into the complexities of the incorporated multimodal data.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Harmonic Optimization Algorithm (Hoa) For Feature Selection",
      "text": "The Harmonic Optimization Algorithm (HOA) is a population-based metaheuristic algorithm inspired by sine and cosine trigonometric functions as shown in Figure  2 . In the context of multimodal sentiment analysis, HOA is applied for feature selection, efficiently exploring the solution space to choose an optimal subset of features for enhanced classification performance. Apply BiGRU to capture temporal dependencies 10:\n\nfor each pair of modalities ğ‘‹ and ğ‘Œ do 11:\n\nCalculate attention scores ğ´ XY and update modality ğ‘‹ using ğ‘Œ and vice versa using it 12:\n\nCompute matching matrices ğ‘€ 1 and ğ‘€ 2 using Eq. (  8 ) Update solution position using SCA equation:\n\nRandomly select a feature subset using ğ‘¥ rand 24:\n\nApply îˆ­ğ›½îˆ´îˆ¯ local search for fine-tuning as shown in Eq. (  14 ), Eq. (  15 )\n\n25:\n\nend for 26: end for 27: return Optimized features ğ… ğ’— For feature selection and optimization, we use the population-based metaheuristic Harmonic Optimization Algorithm (HOA). HOA explores the solution space and identifies the ideal destination space by using populationbased, iterative stochastic techniques that resemble the harmonic behavior of sine and cosine functions. The two main phases of HOA are usually exploration and exploitation illustrated in Algorithm 1. In the exploration phase, regions of the search space that show promise are found by combining random solutions with a higher degree of randomization. To lessen the randomness in the variations, on the other hand, the exploitation phase gradually modifies the answers.\n\nTo initiate the optimization procedure using HOA, the search element adjusts its self-position based on the sine and cosine functions, as given in Equation  11 .\n\nHere, îˆ¼ ğ‘¡ ğ‘–,ğ‘— denotes the position of the current solution in the ğ‘— ğ‘¡â„ dimension of the ğ‘– ğ‘¡â„ search element at the ğ‘¡ ğ‘¡â„ iteration. ğ‘Ÿ ğ‘¡ 2,ğ‘— , ğ‘Ÿ ğ‘¡ 3,ğ‘— , and ğ‘Ÿ ğ‘¡ 4,ğ‘— are uniformly distributed random numbers and ğ· ğ‘¡ ğ‘— represents the position of the ğ‘— ğ‘¡â„ dimension of the destination point (best solution) at the ğ‘¡ ğ‘¡â„ iteration. A random number ğ‘Ÿ ğ‘¡ 1,ğ‘— facilitates the transition from exploration to exploitation of the search space, determined by Equation  12 .\n\nHere, ğ›¼, ğ‘¡, and ğ‘‡ represent the constant number, the ğ‘¡ ğ‘¡â„ iteration, and the total number of iterations, respectively. The value of ğ‘Ÿ ğ‘¡ 1,ğ‘— decides whether the search area is for exploitation (destination solution region)\n\n) is mentioned on line 21 in Algorithm 1. The stochastic variable (ğ‘Ÿ ğ‘¡ 2,ğ‘— defines the search agent's movement toward or away from the destination point, bounded within [0, 2ğœ‹], in sync with a complete cycle of sine and cosine functions. (ğ‘Ÿ ğ‘¡ 3,ğ‘— balances the exploration and exploitation rates by introducing a random weight between (0, 2). Furthermore, ğ‘Ÿ ğ‘¡ 3,ğ‘— introduces a stochastic step size for the destination point, emphasizing (ğ‘Ÿ ğ‘¡ 3,ğ‘— >1) or not emphasizing (ğ‘Ÿ ğ‘¡ 3,ğ‘— <1) its impact. Finally, the parameter ğ‘Ÿ ğ‘¡ 4,ğ‘— evenly transitions between the sine and cosine components, as given in Equation  11 .\n\nAs shown in lines 17-26 of Algorithm 1, HOA initializes a set of random solutions representing feature subsets. Through iterative evaluations using the objective function, HOA refines these solutions by smoothly transitioning between the exploration and exploitation phases. The algorithm employs sine and cosine functions to update solution positions and efficiently explore the search space.",
      "page_start": 10,
      "page_end": 12
    },
    {
      "section_name": "Solution Update Procedure:",
      "text": "The positions of destination points ğ‘¥ ğ‘¡+1 ğ‘– are updated using the following equations, where, ğ‘Ÿ 1 , ğ‘Ÿ 2 , ğ‘Ÿ 3 , and ğ‘Ÿ 4 are random numbers in the range [0, 1] as illsutrated in Equation  13 :\n\nHere, ğ‘¥ rand is a random binary vector, indicating whether a feature is selected (1) or not (0).",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Local Search: Adaptive Beta Hill Climbing",
      "text": "After identifying the most efficient features through the metaheuristic algorithm Harmonic Optimization Algorithm (HOA), further enhancement of exploitation ability can be achieved by integrating the local search technique named Adaptive ğ›½-Hill Climbing (Ağ›½HC). Ağ›½HC is a feature optimization algorithm utilizing local search-based techniques. These search techniques are guided by a pair of control parameters îˆº HC and ğ›½ HC , respectively. By adjusting these parameters, the search technique finds the optimal trade-off between exploitation and exploration. Fine-tuning these parameters plays a significant role in optimization because it helps enhance the convergence rate. The parameter îˆº HC is initially set to a value close to 1, but it gradually decreases as the search process iterates. This allows the algorithm to dynamically adjust îˆº HC to improve search performance, as given in Equation  14 .\n\nHere, îˆº ğ‘¡ HC represents the value of îˆº HC at time ğ‘¡, ğ‘ƒ is a constant used to linearly decrease the value of îˆº HC to a value close to 0, and ğ‘‡ ğ‘šğ‘ğ‘¥ represents the upper limit of iterations for Ağ›½HC algorithm.\n\nMoreover, the ğ›½ parameter undergoes deterministic adaptation within a defined range âˆˆ [ğ›½ ğ‘šğ‘–ğ‘› HC , ğ›½ ğ‘šğ‘ğ‘¥ HC ], mathematically expressed in Equation  15 .\n\nHere, ğ›½ ğ‘¡ HC denotes the rate of ğ›½ HC at time ğ‘¡, ğ›½ ğ‘šğ‘–ğ‘› HC and ğ›½ ğ‘šğ‘ğ‘¥ HC represent the minimum and maximum values of ğ›½ HC respectively, ğ‘‡ ğ‘šğ‘ğ‘¥ is the total number of iterations, and ğ‘¡ signifies the current time.\n\nAfter applying Ağ›½HC based HOA to the graph enriched feature vector î‰ ğ‘£ this function returns a masking array stating the selected features for final feature leaning and sentiment prediction given by ğ‘­ ğ‘£ .",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Feature Learning With Bidirectional Convolutional Processing",
      "text": "The final feature learning is a bidirectional approach for sentiment prediction as mentioned in Algorithm 2 having multiple stages to it. The first stage is the input layer. This is followed by convolutional layers, responsible for feature learning by applying convolution and bias to input features. Next, the reshape layer is integrated, and finally, the class prediction layer. One portion of each layer can be used for feature learning, and the other part can be used for class prediction shown in Figure  2 . Further, we explore the convolutional layers responsible for feature extraction in the following sections.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "A) Input Representation And Initial Processing:",
      "text": "The optimized embeddings, ğ¹ ğ‘£ obtained after applying Ağ›½HC based HOA, serve as input to the model, as shown in line 1 of Algorithm 1. This layer will directly pass onto the next convolutional layers for further feature extraction.\n\nb) Convolutional Feature Extraction: This layer employs convolution and pooling operations to extract hierarchical features, capturing both simple textures and complex structures crucial for understanding multimodal interactions. We denote these convolutional network parameters as Î˜ conv , and perform the following operations as illustrated below.\n\nConvolutional Operations: This operation applies filters of size ğ¾ to the input sequences given on line 8 in Algorithm 1, sliding them across the sequence and capturing local patterns. Mathematically, this can be expressed in Equation  16 :\n\nwhere, ğ¶ ğ‘£1,ğ‘– denotes the ğ‘–-th feature map at the first convolutional layer for utterance ğ‘£, ğ¹ ğ‘£ represents the optimized embedding for utterance ğ‘£, Î˜ conv,ğ‘˜ is the ğ‘˜-th convolutional filter and ğ¾ is the filter size.\n\nPooling Operations: This layer performs downsampling on the feature maps by selecting the maximum value within a predefined window size. This operation emphasizes the most prominent features within the local receptive field, reducing the spatial dimensionality of the data while potentially preserving essential information as illustrated in line 7 of the Algorithm 2. The specific implementation involves selecting the maximum value from each element within the window, resulting in a compressed representation of the input.\n\nThe last layer in the CNN stack is the reshape layer. Its input is the output of the pooling layer that came before it, usually flattened into a single-column vector illustrated in line 8 in Algorithm 2. The retrieved features are compressed and forwarded to the reshape layer when using this vector for classification or regression tasks. The reshape layer creates the network's prediction by learning intricate correlations between the characteristics and the intended output using a set of weights and biases.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Classification",
      "text": "The features extracted from the prior CNN layer are first forwarded to a reshape layer and are further passed to the classification stage where we integrate the ConvXGB (Thongsuwan, Jaiyen, Padcharoen and Agarwal, 2021) classifier. It leverages the XGBoost algorithm for sentiment and emotion classification as illustrated in Algorithm 2. This procedure is sequentially elaborated as follows. Apply convolution to extract features:\n\nApply ReLU activation for non-linearity: ğ‚ ğ‘£1,ğ‘– = max(0, ğ‚ ğ‘£1,ğ‘– )\n\n7:\n\nApply max pooling for down-sampling:\n\nReshape the feature maps for XGBoost input:\n\nend for 10: end for 11: Initialize the regularization and tree parameters. 12: for each iteration e = 1 to E do 13:\n\nfor each tree ğ‘˜ in range(K) do b) Classification Layer: The functioning of the classification layer is mathematically explained on lines 11-21 in Algorithm 2. This layer serves primarily for class prediction and leverages the XGBoost algorithm. XGBoost is a tree-based machine learning model that utilizes gradient boosting to sequentially construct an ensemble of decision trees. The number of trees in the ensemble directly influences the model's performance and complexity.\n\nIn every cycle, a collection of K trees is used, each tree having ğ¾ ğ‘– ğ¸ | ğ‘– âˆˆ 1..ğ¾ nodes. The total of the various prediction scores produced by each tree is the final prediction for a particular instance as illustrated in Equation  17 :\n\nwhere, the training set members are denoted by ğ‘¥ ğ‘– , the associated class labels are denoted by ğ‘¦ ğ‘– , the leaf score for the ğ‘˜ ğ‘¡â„ tree is represented by ğ‘“ ğ‘˜ , and the set of all ğ¾ scores for all trees of classification and regression is represented by ğ¹ .\n\nNow we define the complexity penalizer function as mathematically illustrated in Equation  18 .\n\nwhere, ğ‘‡ is the number of leaves in the tree, ğœ” is the weight of each leaf, and ğ›¿, ğœ are constants governing the regularization degree. Further, regularization is applied to improve the final result as given in line 15 of Algorithm 2:\n\nEquation 19 mentions the regularization term ğ¿(ğœ™) comprising two parts: the complexity penalizer function, which tries to avoid overfitting, and the differentiable loss function ğ“, which calculates the difference between the ground truth label ğ‘¦ ğ‘– and the prediction Å·ğ‘– .\n\nSeveral regression and classification problems can be handled by gradient boosting. At each phase, the gradient boost loss function is simplified using an extended second-order Taylor expansion to yield a more achievable goal, as follows:\n\nFurther, Equation  20 , on substitution is further transformed into below form as illustrated in Equation  21 : ) 2 are the loss functions of the first and second-order statistics.\n\nFurther, Equation 22 determines the weight ğœ” (ğ‘¡) ğ‘— for leaf ğ‘— at iteration ğ‘¡ based on the ratio of the sum of gradients for the loss function over all samples in the node Î“ ğ‘— to the sum of second-order gradients and a regularization term ğœ . The weight reflects the leaf node's effectiveness in correcting errors, considering both the loss and regularization.",
      "page_start": 13,
      "page_end": 15
    },
    {
      "section_name": "ğœ” (ğ‘¡)",
      "text": "ğ‘— = -\n\nThe new leaf weight is updated using Equation  22 iteratively, mentioned in line 16 in Algorithm 2, ensuring that each new tree focuses on correcting the errors of the previous ones, leading to a progressively more accurate ensemble of decision trees for better feature classification.\n\nPrediction: The trained XGB model is then employed for sentiment and emotion prediction. Given a set of features for a new input, denoted as ğ‚ â€² ğ‘£ , the model predicts the sentiment label ğ’ ğ‘£ using the XGBoost prediction function as illustrated on lines 20-21 in Algorithm 2. The resulting sentiment predictions provide valuable insights into the sentiment conveyed by the multimodal input. Similarly, for Emotion prediction the XGB model will be given the feature tensor as input denoted by ğ‚ â€² ğ‘£ , the model predicts the emotion label ğ„ ğ‘£ using the XGB predict function.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Experimental Evaluations",
      "text": "This section exhibits the validity of our technique by first introducing the experimental setup and then demonstrating the results of the experiment.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Experimental Setup",
      "text": "The following section summarizes the extensive datasets that have been employed for the experimentations. We next go over preliminary techniques for comparison, followed by the adopted evaluation metrics.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Datasets",
      "text": "In the process of evaluating the accuracy of our proposed approach for multimodal sentiment and emotion analysis, we selected and employed three benchmark datasets. To evaluate the efficiency of the sentiment analysis task, we leveraged the well-regarded CMU-MOSI and CMU-MOSEI datasets. Furthermore, we utilize the IEMOCAP dataset for emotion prediction. The subsequent sections provide an in-depth explanation of the datasets shown in Table  1 .",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "(A) Cmu-Mosi:",
      "text": "We incorporate the CMU-MOSI dataset in our research. The aforementioned data set was first published by Amir Zadeh et al  (Zadeh et al., 2016) . It stands as a prominent benchmark, particularly well-suited for evaluating the performance of fusion networks in the challenging task of sentiment intensity prediction. Comprising an array of YouTube video blogs (vlogs), this dataset captures the diverse expressions of speakers articulating their opinions across various topics. In its entirety, it consists of 2,199 curated utterance-video segments sourced from 93 videos, each featuring a unique narrator. This dataset is distinctive due to its rigorous manual annotation process, where each segment is assigned a real-number score in a range from -3 to +3. This score is a measure of the relative strength of emotions-negative sentiments have values below zero and positive sentiments have values more than zero.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "(B) Cmu-Mosei:",
      "text": "Building upon the foundation of CMU-MOSI (Bagher  Zadeh et al., 2018) , the CMU-MOSEI dataset emerges as an enriched counterpart, both in terms of sample size and speaker diversity. With an expanded set of samples totaling 23,453 video segments, CMU-MOSEI captures a broader spectrum of human experiences and opinions. These segments undergo manual annotation, maintaining the real-number score convention for sentiment intensity. This expansive dataset spans 5,000 videos, engaging 1,000 distinct speakers and spanning 250 unique topics. CMU-MOSEI thus provides a comprehensive and diverse set of multimedia instances for the exploration and evaluation of multimodal sentiment analysis.\n\n(c) IEMOCAP: For multimodal emotion recognition, the Interactive Emotional Dyadic Motion Capture (IEMO-CAP)  (Busso et al., 2008)  dataset offers a unique and rich resource. Comprising a total of 12 hours of audio-visual data, IEMOCAP captures dialogues between 10 actors engaged in both scripted and improvised conversations. Following data collection, the audio-visual content is segmented into smaller utterances, each lasting between 3 to 15 seconds. The uniqueness of IEMOCAP lies in its detailed labeling process. Each utterance undergoes evaluation by 3-4 assessors, using a 10-option scale encompassing a wide range of emotions. For our analysis, we focus on four emotions-anger, excitement (happiness), neutrality, and sadness, keeping consistent with prior research and representing emotions where at least 2 experts were in agreement. This stringent labeling ensures a robust and reliable dataset, aligning with established practices in emotion research.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Compared Methods",
      "text": "To assess the efficiency of our proposed technique, we compared it to other recent sentiment and emotion analysis methods as discussed below.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "(A) Multimodal Fusion Network (Mfn):",
      "text": "The Multimodal Fusion Network (MFN)  (Gan, Fu, Feng, Zhu, Cao and Zhu, 2024)  established the feasibility of early fusion in multimodal sentiment analysis. Its key innovation was the direct concatenation of feature vectors extracted from disparate modalities (e.g., text, audio, video) into a single representation. This approach bypassed intermediate feature-level fusion and fed the combined vector directly into a sentiment classifier. While seemingly simplistic, MFN demonstrated remarkable effectiveness in capturing crossmodal correlations and identifying basic sentiment patterns across diverse information sources. Its success laid to the foundation of early fusion techniques in multimodal learning tasks.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "(B) Tensor Fusion Network (Tfn):",
      "text": "The Tensor Fusion Network (TFN)  (Zadeh et al., 2017)  builds upon the success of Multimodal Fusion Network(MFN) by refining its early fusion approach. Unlike MFN's simple concatenation, TFN leverages the expressive power of tensor products. For each modality, TFN employs separate subnetworks to extract modality-specific feature vectors. These vectors are then combined through an outer product, which generates a higher-order tensor capturing both inter-and intramodal interactions. This multimodal tensor undergoes subsequent transformations to learn complex fusion patterns beyond basic correlations. Notably, the tensor product operation is parameter-free, reducing overfitting risks and potentially facilitating the interpretability of the learned multimodal representation. In essence, TFN elevates early fusion beyond mere feature aggregation, leading to a richer and more specific understanding of sentiment across modalities.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "(C) Multi-Attention Recurrent Network (Marn):",
      "text": "The MARN  (Li et al., 2020)  tackles multimodal sentiment and emotion analysis by first capturing the essence of each modality (text, audio, video) through separate subnetworks. These subnetworks extract features specific to each modality, like word meanings, vocal tone, and facial expressions. A clever \"multi-attention\" mechanism then analyzes these features within each modality and across modalities over time, pinpointing which aspects are most relevant to the overall sentiment or emotion being conveyed. This dynamic attention dynamically weights the features, creating a systematic understanding of how different modalities interact and contribute to the emotional message. Finally, a \"Long-short Term Hybrid Memory\" component carefully stores and integrates this rich information, capturing both fleeting emotions and deeper sentiments across the entire communication sequence. The resulting comprehensive representation is then fed into a classifier to accurately pinpoint the overall sentiment or emotion expressed.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "(D) Modality-Invariant And Specific Subspaces (Misa):",
      "text": "Complex fusion methods in multimodal tasks can struggle with morphological gaps between different modalities. To address this challenge, Hazarika et al.  (Hazarika, Zimmermann and Poria, 2020) , proposed MISA, a novel framework that leverages modal subspaces to enhance the fusion process. The core contribution of MISA lies in its modal representation learning stage, which precedes the actual fusion step. Following feature extraction for each modality (audio, visual, and text), MISA projects each modality into two distinct subspaces. The first subspace is modality-invariant, aiming to capture the commonalities between modalities by minimizing the heterogeneity gap through a distribution similarity constraint. Conversely, the second subspace is modality-specific, focusing on learning unique feature information specific to each modality. After subspace projection, a transformer-based self-attention mechanism is employed to concatenate all six transformed modal vectors. This combined representation is then fed into simple feed-forward layers for prediction. Notably, by exploring the feature space through subspace learning, MISA reduces the reliance on complex fusion mechanisms, potentially leading to improved performance.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "(E) Speaker-Independent Multimodal Representation:",
      "text": "The SIMR framework, as proposed by  (Wang et al., 2023) , strategically partitions nonverbal data into distinct components, namely style encoding and content representation. This deliberate separation serves to mitigate the impact of personalized acoustic and visual features. Simultaneously, the framework adeptly uncovers both compatible and incompatible cross-modal interactions through the integration of an enhanced Transformer module. By dissecting nonverbal inputs into style encoding and content representation, the framework leverages informative cross-modal correlations. Unlike conventional transformer-based approaches that primarily focus on discovering compatible cross-modal interactions, our methodology goes a step further. It not only identifies compatible interactions but also pays due attention to incompatible ones, achieved through the incorporation of an enhanced cross-modal transformer module. This systematic approach ensures a more comprehensive understanding of the interplay between modalities, enhancing the model's ability to handle speaker-independent multimodal representation effectively.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "(F) Multi-Level Correlation Mining Framework (Mcmf):",
      "text": "This work introduces a novel approach to multimodal sentiment analysis, addressing challenges related to feature fusion and co-learning. Their proposed method  (Li, Guo, Pan, Ding, Yu, Zhang, Liu, Chen, Wang and Xie, 2023)  incorporates a multilevel correlation mining framework and a self-supervised label generation module. Leveraging unimodal features fusion and a linguistics-guided transformer, the model effectively integrates low and high-level correlation information. A multi-task learning framework facilitates co-learning, while the self-supervised label generation module overcomes the lack of unimodal labels. The study's key contributions include enhancing fusion through unimodal features fusion, addressing multimodal complexity with linguistics-guided transformers, and providing a comprehensive solution to co-learning challenges. The results demonstrate the model's effectiveness in multimodal sentiment analysis.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "(G) Multimodal Transformer (Mult):",
      "text": "The Multimodal Transformer (MulT)  (Tsai et al., 2019) , recognizes emotions by processing multimodal data, including language, facial movements, and audio behaviors, without explicit alignment. capture associated crossmodal information, it employs a directional pairwise crossmodal attention mechanism to handle interactions across several modalities and time steps. Our method varies from MulT in that it includes an Intermodal Contextual Interaction Module that dynamically assigns weights to each modality's representation and a harmonic optimization algorithm to address data redundancy. MulT focuses on handling non-alignment of data and long-range dependencies. This difference emphasizes how we optimize feature contributions and fusion efficiency, while MulT focuses on attention processes.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "(H) Multimodal End-To-End Sparse Model (Mesm):",
      "text": "In order to improve emotion recognition, the Multimodal End-to-End Sparse Model (MESM)  (Dai et al., 2021a) , combines feature extraction and model training into a single end-to-end procedure. The standard two-phase pipeline has drawbacks that MESM solves. Specifically, its fixed features cannot be adjusted to suit varied workloads. Whereas, with MESM, the performance is maintained at a lower computational overhead with the introduction of a sparse cross-modal attention mechanism and the reorganization of datasets for endto-end training. Tests reveal that MESM outperforms cutting-edge models built on the two-phase pipeline. In contrast to MESM, GCM-Net emphasizes feature contribution optimization by dynamic weighting and effective feature selection, demonstrating distinct approaches to multimodal emotion identification problems.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "Our proposed model GCM-Net, aims to extract sentiment and emotions from user videos by analyzing multiple modalities, including visual, textual, and audio modalities. To gauge the effectiveness of this method, it's crucial to evaluate their performance using appropriate metrics. This article focuses on four widely employed metrics: accuracy, precision, recall, and F1-score.\n\nThe precision parameter measures the proportion of correctly predicted instances for a specific label within a binary classification task, where, ğ‘™ğ‘ğ‘ğ‘’ğ‘™ âˆˆ {ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’, ğ‘›ğ‘’ğ‘”ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’}. It is denoted as the ratio of the number of correctly predicted instances of the specific label (ğ‘™ğ‘ğ‘ğ‘’ğ‘™ âˆˆ ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’, ğ‘›ğ‘’ğ‘”ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’) to the total number of instances predicted with that label, as shown in Equation  23 .\n\nRecall, also termed sensitivity, measures the proportion of true positives within the total number of actual positive cases. It reflects the model's ability to correctly identify all relevant instances belonging to a specific class. Mathematically, recall is calculated as: Equation  24 .\n\nThe F1-score is a widely used metric that combines precision and recall into a single, harmonic mean value. This metric aims to provide a balanced evaluation of a model's performance by considering both its ability to correctly identify positive instances (precision) and its ability to capture all relevant positive instances (recall). F1-score can be mathematically calculated as shown in Equation  25 .",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Experimental Results",
      "text": "This section summarizes the experimental outcomes derived from existing methods against our proposed method. We evaluate the proposed technique by comparing its effectiveness with existing methods on varied datasets, visualizing the recommendations through qualitative analysis, analyzing performance gain, and identifying the sensitivity of various parameters.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Effectiveness Comparisons:",
      "text": "We compare the performance of GCM-Net with the existing methods on three benchmark datasets. The performance over sentiment analysis is evaluated on CMU-MOSI and CMU-MOSEI datasets, whereas the IEMOCAP dataset is leveraged for the emotion analysis task.\n\n(a) Performance on CMU-MOSI and CMU-MOSEI for Sentiment Analysis: In assessing the efficacy of our proposed model for multimodal sentiment analysis, we conducted a comparative analysis with existing methods, as per prior research practices.   (Zadeh et al., 2017)  0.7460 0.7450 0.7560 0.7550 MARN  (Li et al., 2020)  0.7710 0.7700 0.7930 0.7780 MFN  (Gan et al., 2024)  0.7740 0.7740 0.7990 0.7910 MISA  (Hazarika et al., 2020)  0.8180 0.8187 0.8360 0.8380 SIMR  (Wang et al., 2023)  0.8610 0.8610 0.8320 0.8320 MCMF  (Li et al., 2023)  0.8843 0.8843 0.8616 0.8588 GCM-Net 0.9266 0.9444 0.8657 0.8923 The performance evaluation encompasses metrics such as accuracy and F1-score on the CMU-MOSI dataset, and the results are presented in Table  2 . The tabulated data clearly illustrates that our model surpasses previous stateof-the-art approaches on the specified dataset by a significant margin. Specifically, GCM-Net exhibits a noteworthy improvement of 18.06% and 19.94% in accuracy and F1-score, respectively, over TFN. This enhancement is attributed to the utilization of graph-based feature reconstruction, which considers temporal context and interrelations with neighboring features.\n\nIn contrast, TFN relies solely on late fusion, limiting its capacity to learn intricate inter-modality associativity. Furthermore, in comparison to early fusion techniques like MFN, our model demonstrates a performance boost of 15.26% and 17.04% in accuracy and F1-score. Overcoming the challenge of feature redundancy in MFN, our model employs a metaheuristic algorithm for feature selection, focusing only on crucial features for sentiment prediction.\n\nConsidering the MCMF model, which leverages correlation information between modalities at various levels, our model showcases superiority with improvements of 4.23% and 6.01% in accuracy and F1-score. Additionally, our model exhibits advancements of 10.86% and 12.57% in accuracy and F1-score, respectively, over MISA. The increasing accuracy trend is illustrated in Figure  5 . These significant improvements in both metrics across various existing methods underscore the competitive edge and effectiveness of our proposed technique.   2 , reveal the consistent superiority of our proposed model over existing benchmarks.\n\nNotably, GCM-Net achieves an impressive improvement of 11.37% in accuracy and 13.73% in F1-score over TFN. This notable advancement can be attributed to our model's adept utilization of graph-based feature recalibration, capturing temporal context and intermodal relationships effectively. Compared with early fusion techniques such as MFN, our model demonstrates a remarkable boost of 5.97% in accuracy and 10.13% in the F1-score. Addressing the challenge of feature redundancy, our model employs a metaheuristic algorithm for feature selection, enhancing its discernment of critical features for sentiment prediction. Considering the SIMR model, which exploits correlation information between modalities, our model showcases a substantial lead with improvements of 2.67% in accuracy and 6.03% in the F1-score. Additionally, our model exhibits advancements of 1.97% in accuracy and 5.43% in F1-score over MISA. As shown in Figure  6 , consistent improvements over these datasets underscore the robustness and effectiveness of our proposed model in handling diverse multimodal sentiment analysis tasks.",
      "page_start": 19,
      "page_end": 20
    },
    {
      "section_name": "(C) Performance On Iemocap For Emotion Analysis:",
      "text": "To justify the effectiveness of our suggested approach, we examine its results on the IEMOCAP dataset, which is well-known as a standard dataset for complex emotion identification problems.   (Dai et al., 2020)  0.7720 0.4980 MulT  (Tsai et al., 2019)  0.7760 0.5690 CMHA  (Zheng, Zhang, Wang, Wang and Zeng, 2023)  0.8420 0.5610 MESM  (Dai et al., 2021a)  0.8440 0.5740 FE2E  (Dai, Cahyawijaya, Liu and Fung, 2021b)  0.8450 0.5880 GCM-Net 0.8566 0.7269\n\nAs illustrated our model GCM-Net stands out, achieving remarkable advancements over prior methods. Compared to LF-LSTM, the prevailing benchmark, GCM-Net exhibits a significant stride of 13.86% in average accuracy and 23.19% in F1-score. This dominance extends to other competitive models like LF-TRANS, EmoEmbs, and MulT, demonstrating improvements of 6.86%, 8.46%, and 8.06% in accuracy respectively, alongside notable F1-score gains. Also, the popular approaches like CMHA, MESM, and FE2E, are outperformed by GCM-Net by 1.46%, 1.26%, and 1.16% in accuracy respectively, establishing a remarkable 15.29% lead in F1-score over FE2E. This comparison of performances is tabulated in Table  3 . The emotion prediction results highlight the effectiveness of our multimodal approach in identifying subtle inherent emotions in video data. Our chosen benchmark dataset, IEMOCAP  (Busso et al., 2008) , exhibits a class imbalance. While the dataset encompasses utterances categorized into four emotions, the distribution is uneven. Specifically, the number of utterances labeled as \"Happy\" significantly outnumbers those labeled as \"Angry\". Figure  7  provides recall, F1-score, and precision for each emotion category: Happy, Angry, Sad, and Neutral, all presented in decimal form for clarity.\n\nWe emphasize the tri-modal combination (A + T + V), which integrates acoustic, textual, and visual modalities. This comprehensive approach facilitates a deeper understanding of emotional expressions, leveraging the synergies between audio, textual, and visual modalities as shown in Figure  7 . This strategic integration enhances our model's ability to interpret and decode the inherent emotion of the utterances, showcasing the precision and depth of our analysis.",
      "page_start": 21,
      "page_end": 22
    },
    {
      "section_name": "Performance Gain Analysis On Cmu-Mosi For Sentiment Analysis:",
      "text": "In this section, we analyze the performance gain of the proposed method. We first examine the performance of GCM-Net with different modalities combinations followed by different attention mechanisms. Ultimately, we conduct experiments involving tri-modal inputs and gain a notably enhanced performance. This observation underscores the significance of employing a combination that integrates all three modalities, emphasizing its superiority in achieving improved results.\n\n(b) Feature Recaliberation using Graph Neural Network: We initiate by presenting the results of incorporating the Graph Neural Network in Table  5  that illustrate the impact of Feature Recalibration using Graph Neural Network in our architecture compared to a scenario without its implementation and different combinations of all the modules for drawing out better conclusions. The performance gain is expressed as the difference between the two approaches. Feature recaliberation significantly enhances accuracy, demonstrating its effectiveness in capturing complex relationships in multimodal data.   5 . We study the attention values to better understand the proposed architecture's behavior while learning. The outcomes that were achieved show that by applying distinct weights across separate modalities and contextual utterances, the model accurately predicts the labels of the experimented utterances. The utilization of Cross-Modal Attention contributes to a notable improvement in accuracy, emphasizing its role in capturing relevant features across diverse modalities.\n\n(d) Feature Selection using Ağ›½HC Integrated HOA: Finally, we investigate the performance gain resulting from the integration of Ağ›½HC encased HOA (Semantic Correspondence Attention) as illustrated in Table  5 . The integration of Ağ›½HC encased HOA results in a substantial accuracy improvement, highlighting its effectiveness in capturing semantic correspondences and enhancing feature representations.\n\nIn the upcoming section, we will discuss about qualitative analysis carried out for our proposed model.",
      "page_start": 22,
      "page_end": 23
    },
    {
      "section_name": "Qualitative Analysis",
      "text": "Multimodal sentiment analysis and emotion analysis approaches typically evaluate effectiveness by accurately classifying utterance data into corresponding sentiment or emotion classes. This section presents a qualitative analysis to assess our proposed model's performance in classifying data points within both sentiment and emotion categories. We showcase the qualitative results on the CMU-MOSI dataset for the sentiment analysis task and on the IEMOCAP dataset for the emotion analysis task. In Figure  8 , we first show the individual modality elements from the utterances illustrated by the audio plot, associated video frames, and its corresponding textual transcript. We perform a binary classification of sentiments through positive and negative labels and compare the inherent and the predicted sentiment class to evaluate the distinctive capability of GCM-Net. For emotion classification, we assess the emotions by portraying in a similar manner by classifying them into one among four emotion classes namely-Happy, Angry, Sad, and Neutral. The exhibited utterances were chosen from the test data, with yellow blocks representing accurately predicted data labels and red showing uncertainty in prediction. Here, accurately refers to the predicted sentiments that match the ground-truth labels of the utterances, and ambiguous refers to those that do not align with ground-truth labels. As illustrated in Figure  8 , GCM-Net performs excellently in the binary sentiment classification of the stated utterances. Although, in the IEMOCAP dataset, we encountered an instance where an \"Angry\" emotion was expressed through text conveying dullness and facial expressions appearing upset, where our model categorized it as \"Sad\". This case highlights the potential of complexities in multimodal emotion analysis  (Fu, Zhang, Yang and Yao, 2024) , where the model can sometimes deviate from the ground-truth class labels leading to ambiguity in prediction. Ultimately, in an overall sense, we see that GCM-Net shows better results with respect to considered tasks in comparison to the existing methods. Its ability to grasp inter-modal contextual information and optimal feature selection makes it ideal for combined sentiment and emotion classification.",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "Parameter Study",
      "text": "This section explores various parameters in our proposed model. We examine the impact of four key parameters: the threshold for creating the similarity matrix (ğ¾ ğ‘¡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘ ), the dropouts in Bi-GRU layers, the number of agents and iterations in the Harmonic optimization Algorithm (HOA) for feature selection (represented as ğ‘€ ğ‘ğ‘”ğ‘’ğ‘›ğ‘¡ğ‘  and ğ‘€ ğ‘–ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘  respectively), and specific parameters in ConvXGB. For ğ¾ ğ‘¡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘ , the optimal value yielding the best results was found to be 0.7. Values below 0.7 were disregarded as they provided insufficient similarity for relevant details, resulting in reduced data redundancy. Higher values led to a sparse matrix with limited utility. We integrated modality-specific Bi-GRU with 300 neurons each, followed by dense layers of 100 neurons (MOSI) and 128 neurons (MOSEI and IEMOCAP) to standardize the input dimensions across modalities. Dropout regularization was optimally set to 0.7 (MOSI & IEMOCAP) & 0.5 (MOSEI) for overall regularization, and 0.5 for Bi-GRU layers.\n\nFurther, the value of ğ‘€ ğ‘ğ‘”ğ‘’ğ‘›ğ‘¡ğ‘  was set to 4, and value of ğ‘€ ğ‘–ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘  to 100 to leverage the local search algorithm extensively for optimal feature selection. Lower values for ğ‘€ ğ‘ğ‘”ğ‘’ğ‘›ğ‘¡ğ‘  and the ğ‘€ ğ‘–ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘  parameters were deemed insufficient for meaningful comparisons and obtaining an optimized feature set. Finally, Extreme Gradient Boost parameters, particularly alpha and gamma, values of 0.6 and 0.5 respectively were chosen for the emotion classification task. This decision was influenced by the class imbalance in the dataset.",
      "page_start": 23,
      "page_end": 24
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Generic Methodology Structure for Multimodal Sentiment Analysis and Emotion Prediction",
      "page": 2
    },
    {
      "caption": "Figure 1: However, this integration presents a significant challenge due to the inherent heterogeneity",
      "page": 2
    },
    {
      "caption": "Figure 1: , generic architectures utilize multi-input from a video and",
      "page": 2
    },
    {
      "caption": "Figure 2: , employs distinct submodules to",
      "page": 6
    },
    {
      "caption": "Figure 2: This figure illustrates our Graph-enhanced Cross-Modal Infusion (GCM-Net) architecture for video sentiment and",
      "page": 7
    },
    {
      "caption": "Figure 3: (a) Graph-based Sampling and Aggregation: We use Graph Sampling and Aggregation (GraphSAGE), a variation of",
      "page": 7
    },
    {
      "caption": "Figure 3: Illustration of Graph-based Feature Recalibration and Enrichment (FRE) which begins by constructing an",
      "page": 8
    },
    {
      "caption": "Figure 4: We compute pairwise attentions",
      "page": 9
    },
    {
      "caption": "Figure 4: Intermodal Contextual Interaction Module (ICIM). This module facilitates cross-modal interaction by computing",
      "page": 10
    },
    {
      "caption": "Figure 2: In the context of multimodal sentiment analysis, HOA is applied",
      "page": 10
    },
    {
      "caption": "Figure 2: Further, we explore the convolutional layers responsible for feature extraction in the",
      "page": 13
    },
    {
      "caption": "Figure 2: ) transforms the tensors output from the convolution layers into a vector format, as shown on line 8 in",
      "page": 14
    },
    {
      "caption": "Figure 5: Performance of Considered Models on CMU-MOSI for Sentiment Analysis",
      "page": 19
    },
    {
      "caption": "Figure 5: These significant improvements in both metrics across various",
      "page": 20
    },
    {
      "caption": "Figure 6: Performance of Considered Models on CMU-MOSEI for Sentiment Analysis",
      "page": 20
    },
    {
      "caption": "Figure 6: , consistent improvements over these datasets underscore the robustness and effectiveness",
      "page": 20
    },
    {
      "caption": "Figure 7: Label-specific Prediction Results on IEMOCAP",
      "page": 21
    },
    {
      "caption": "Figure 7: provides recall, F1-score, and precision for each emotion category: Happy, Angry, Sad,",
      "page": 21
    },
    {
      "caption": "Figure 7: This strategic integration enhances our modelâ€™s",
      "page": 21
    },
    {
      "caption": "Figure 8: , we first show the individual modality elements from the utterances",
      "page": 23
    },
    {
      "caption": "Figure 8: , GCM-Net performs excellently in the binary sentiment classification of the stated",
      "page": 23
    },
    {
      "caption": "Figure 8: Qualitative Results on CMU-MOSI and IEMOCAP:This figure illustrates qualitative results on corresponding",
      "page": 23
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Comparative Analysis of Incorporated Datasets: We assess our proposed technique on three publicly available multimodal",
      "page": 16
    },
    {
      "caption": "Table 2: Comparison of our GCM-Net with Existing Models on Two-Class Accuracy",
      "page": 19
    },
    {
      "caption": "Table 2: The tabulated data clearly illustrates that our model surpasses previous state-",
      "page": 20
    },
    {
      "caption": "Table 2: , reveal the consistent superiority of our proposed model over existing benchmarks.",
      "page": 20
    },
    {
      "caption": "Table 3: Comparison of GCM-Net with Existing Models for IEMOCAP Dataset",
      "page": 21
    },
    {
      "caption": "Table 3: The emotion prediction results highlight the effectiveness of our multimodal",
      "page": 21
    },
    {
      "caption": "Table 4: Ablation Study of GCM-Net on CMU-MOSI for Sentiment Analysis for different",
      "page": 22
    },
    {
      "caption": "Table 4: , we explain the ablation studies done based on the combinations",
      "page": 22
    },
    {
      "caption": "Table 5: that illustrate the impact of Feature Recalibration using Graph Neural",
      "page": 22
    },
    {
      "caption": "Table 5: Ablation Study of GCM-Net on CMU-MOSI for Sentiment Analysis with Combination",
      "page": 22
    },
    {
      "caption": "Table 5: We study the attention values",
      "page": 22
    },
    {
      "caption": "Table 5: The integration of",
      "page": 22
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph",
      "authors": [
        "A Bagher Zadeh",
        "P Liang",
        "S Poria",
        "E Cambria",
        "L Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P18-1208"
    },
    {
      "citation_id": "2",
      "title": "Deep feature selection using adaptive beta-hill climbing aided whale optimization algorithm for lung and colon cancer detection",
      "authors": [
        "A Bhattacharya",
        "B Saha",
        "S Chattopadhyay",
        "R Sarkar"
      ],
      "year": "2023",
      "venue": "Biomedical Signal Processing and Control",
      "doi": "10.1016/j.bspc.2023.104692"
    },
    {
      "citation_id": "3",
      "title": "Iemocap: interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C Lee",
        "E Kazemzadeh",
        "E Provost",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "4",
      "title": "Multi-view attribute graph convolution networks for clustering",
      "authors": [
        "J Cheng",
        "Q Wang",
        "Z Tao",
        "D Xie",
        "Q Gao"
      ],
      "year": "2020",
      "venue": "Multi-view attribute graph convolution networks for clustering",
      "doi": "10.24963/ijcai.2020/411"
    },
    {
      "citation_id": "5",
      "title": "",
      "authors": [
        "P Cunningham",
        "S Delany"
      ],
      "year": "2020",
      "venue": "",
      "arxiv": "arXiv:2004.04523"
    },
    {
      "citation_id": "6",
      "title": "Multimodal end-to-end sparse model for emotion recognition",
      "authors": [
        "W Dai",
        "S Cahyawijaya",
        "Z Liu",
        "P Fung"
      ],
      "year": "2021",
      "venue": "Multimodal end-to-end sparse model for emotion recognition",
      "arxiv": "arXiv:2103.09666"
    },
    {
      "citation_id": "7",
      "title": "Multimodal end-to-end sparse model for emotion recognition",
      "authors": [
        "W Dai",
        "S Cahyawijaya",
        "Z Liu",
        "P Fung",
        "K Toutanova",
        "A Rumshisky",
        "L Zettlemoyer",
        "D Hakkani-Tur",
        "I Beltagy",
        "S Bethard",
        "R Cotterell",
        "T Chakraborty"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/2021.naacl-main.417"
    },
    {
      "citation_id": "8",
      "title": "Modality-transferable emotion embeddings for low-resource multimodal emotion recognition",
      "authors": [
        "W Dai",
        "Z Liu",
        "T Yu",
        "P Fung"
      ],
      "year": "2020",
      "venue": "Modality-transferable emotion embeddings for low-resource multimodal emotion recognition",
      "doi": "10.18653/v1/2021.naacl-main.417",
      "arxiv": "arXiv:2009.09629"
    },
    {
      "citation_id": "9",
      "title": "Analysis of multimodal data fusion from an information theory perspective",
      "authors": [
        "Y Dai",
        "Z Yan",
        "J Cheng",
        "X Duan",
        "G Wang"
      ],
      "year": "2023",
      "venue": "Information Sciences",
      "doi": "10.1016/j.ins.2022.12.014"
    },
    {
      "citation_id": "10",
      "title": "A transformer-based joint-encoding for emotion recognition and sentiment analysis",
      "authors": [
        "J Delbrouck",
        "N Tits",
        "M Brousmiche",
        "S Dupont"
      ],
      "year": "2020",
      "venue": "A transformer-based joint-encoding for emotion recognition and sentiment analysis",
      "arxiv": "arXiv:2006.15955"
    },
    {
      "citation_id": "11",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "12",
      "title": "Hybrid cross-modal interaction learning for multimodal sentiment analysis",
      "authors": [
        "Y Fu",
        "Z Zhang",
        "R Yang",
        "C Yao"
      ],
      "year": "2024",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "13",
      "title": "A multimodal fusion network with attention mechanisms for visual-textual sentiment analysis",
      "authors": [
        "C Gan",
        "X Fu",
        "Q Feng",
        "Q Zhu",
        "Y Cao",
        "Y Zhu"
      ],
      "year": "2024",
      "venue": "Expert Systems with Applications",
      "doi": "10.1016/j.eswa.2023.122731"
    },
    {
      "citation_id": "14",
      "title": "COSMIC: COmmonSense knowledge for eMotion identification in conversations",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "A Gelbukh",
        "R Mihalcea",
        "S Poria"
      ],
      "year": "2020",
      "venue": "COSMIC: COmmonSense knowledge for eMotion identification in conversations",
      "doi": "10.18653/v1/2020.findings-emnlp.224"
    },
    {
      "citation_id": "15",
      "title": "Hierarchical graph transformer-based deep learning model for large-scale multi-label text classification",
      "authors": [
        "J Gong",
        "Z Teng",
        "Q Teng",
        "H Zhang",
        "L Du",
        "S Chen",
        "M Bhuiyan",
        "J Li",
        "M Liu",
        "H Ma"
      ],
      "year": "2020",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2020.2972751"
    },
    {
      "citation_id": "16",
      "title": "Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "D Hazarika",
        "S Poria",
        "R Mihalcea",
        "E Cambria",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Interactive conversational memory network for multimodal emotion detection",
      "doi": "10.18653/v1/D18-1280"
    },
    {
      "citation_id": "17",
      "title": "Misa: Modality-invariant and-specific representations for multimodal sentiment analysis",
      "authors": [
        "D Hazarika",
        "R Zimmermann",
        "S Poria"
      ],
      "year": "2020",
      "venue": "Misa: Modality-invariant and-specific representations for multimodal sentiment analysis",
      "arxiv": "arXiv:2005.03545"
    },
    {
      "citation_id": "18",
      "title": "Dynamic hypergraph convolutional network for multimodal sentiment analysis",
      "authors": [
        "J Huang",
        "Y Pu",
        "D Zhou",
        "J Cao",
        "J Gu",
        "Z Zhao",
        "D Xu"
      ],
      "year": "2024",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "19",
      "title": "Language is not all you need: Aligning perception with language models",
      "authors": [
        "S Huang",
        "L Dong",
        "W Wang",
        "Y Hao",
        "S Singhal",
        "S Ma",
        "T Lv",
        "L Cui",
        "O Mohammed",
        "Q Liu",
        "K Aggarwal",
        "Z Chi",
        "J Bjorck",
        "V Chaudhary",
        "S Som",
        "X Song",
        "F Wei"
      ],
      "year": "2023",
      "venue": "Language is not all you need: Aligning perception with language models",
      "doi": "10.48550/arXiv.2302.14045"
    },
    {
      "citation_id": "20",
      "title": "All-modalities-in-one bert for multimodal sentiment analysis",
      "authors": [
        "K Kim",
        "S Park"
      ],
      "year": "2023",
      "venue": "All-modalities-in-one bert for multimodal sentiment analysis",
      "doi": "10.1016/j.inffus.2019.07.005"
    },
    {
      "citation_id": "21",
      "title": "Adversarial multimodal representation learning for click-through rate prediction",
      "authors": [
        "X Li",
        "C Wang",
        "J Tan",
        "X Zeng",
        "D Ou",
        "B Zheng"
      ],
      "year": "2020",
      "venue": "Proceedings of The Web Conference",
      "doi": "10.1145/3366423.3380163"
    },
    {
      "citation_id": "22",
      "title": "Multi-level correlation mining framework with self-supervised label generation for multimodal sentiment analysis",
      "authors": [
        "Z Li",
        "Q Guo",
        "Y Pan",
        "W Ding",
        "J Yu",
        "Y Zhang",
        "W Liu",
        "H Chen",
        "H Wang",
        "Y Xie"
      ],
      "year": "2023",
      "venue": "Information Fusion",
      "doi": "10.1016/j.inffus.2023.01.116"
    },
    {
      "citation_id": "23",
      "title": "Multi-modal fusion network with complementarity and importance for emotion recognition",
      "authors": [
        "S Liu",
        "P Gao",
        "Y Li",
        "W Fu",
        "W Ding"
      ],
      "year": "2023",
      "venue": "Information Sciences",
      "doi": "10.1016/j.ins.2022.11.076"
    },
    {
      "citation_id": "24",
      "title": "Efficient low-rank multimodal fusion with modality-specific factors",
      "authors": [
        "Z Liu",
        "Y Shen",
        "V Lakshminarasimhan",
        "P Liang",
        "A Zadeh",
        "L Morency"
      ],
      "year": "2018",
      "venue": "ACL -Annu. Meet. Assoc. Comput. Linguist"
    },
    {
      "citation_id": "25",
      "title": "Modality to modality translation: An adversarial representation learning and graph fusion network for multimodal fusion",
      "authors": [
        "S Mai",
        "H Hu",
        "S Xing"
      ],
      "year": "2014",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": "10.1609/aaai.v34i01.5347"
    },
    {
      "citation_id": "26",
      "title": "Application of cnn-bigru model in chinese short text sentiment analysis",
      "authors": [
        "Y Miao",
        "Y Ji",
        "E Peng"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2019 2nd International Conference on Algorithms, Computing and Artificial Intelligence",
      "doi": "10.1145/3377713.3377804"
    },
    {
      "citation_id": "27",
      "title": "Towards multimodal sentiment analysis: harvesting opinions from the web",
      "authors": [
        "L Morency",
        "R Mihalcea",
        "P Doshi"
      ],
      "year": "2011",
      "venue": "Proceedings of the 13th International Conference on Multimodal Interfaces",
      "doi": "10.1145/2070481.2070509"
    },
    {
      "citation_id": "28",
      "title": "Found in translation: Learning robust joint representations by cyclic translations between modalities",
      "authors": [
        "H Pham",
        "P Liang",
        "T Manzini",
        "L Morency",
        "B PÃ³czos"
      ],
      "year": "2018",
      "venue": "Found in translation: Learning robust joint representations by cyclic translations between modalities",
      "arxiv": "arXiv:1812.07809"
    },
    {
      "citation_id": "29",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "N Majumder",
        "A Zadeh",
        "L Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P17-1081"
    },
    {
      "citation_id": "30",
      "title": "Integrating multimodal information in large pretrained transformers",
      "authors": [
        "W Rahman",
        "M Hasan",
        "S Lee",
        "A Zadeh",
        "C Mao",
        "L Morency",
        "E Hoque"
      ],
      "year": "2020",
      "venue": "Proceedings of the conference"
    },
    {
      "citation_id": "31",
      "title": "Extending long short-term memory for multi-view structured learning",
      "authors": [
        "S Rajagopalan",
        "L Morency",
        "T Baltrusaitis",
        "R Goecke"
      ],
      "year": "2016",
      "venue": "Computer Vision-ECCV 2016: 14th European Conference"
    },
    {
      "citation_id": "32",
      "title": "Sengr: Sentiment-enhanced neural graph recommender",
      "authors": [
        "L Shi",
        "W Wu",
        "W Guo",
        "W Hu",
        "J Chen",
        "W Zheng",
        "L He"
      ],
      "year": "2022",
      "venue": "Information Sciences",
      "doi": "10.1016/j.ins.2021.12.120"
    },
    {
      "citation_id": "33",
      "title": "Learning discriminative multi-relation representations for multimodal sentiment analysis",
      "authors": [
        "Z Tang",
        "Q Xiao",
        "X Zhou",
        "Y Li",
        "C Chen",
        "K Li"
      ],
      "year": "2023",
      "venue": "Information Sciences",
      "doi": "10.1016/j.ins.2023.119125"
    },
    {
      "citation_id": "34",
      "title": "Convxgb: A new deep learning model for classification problems based on cnn and xgboost",
      "authors": [
        "S Thongsuwan",
        "S Jaiyen",
        "A Padcharoen",
        "P Agarwal"
      ],
      "year": "2021",
      "venue": "Nuclear Engineering and Technology",
      "doi": "10.1016/j.net.2020.04.008"
    },
    {
      "citation_id": "35",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Y Tsai",
        "S Bai",
        "P Liang",
        "J Kolter",
        "L Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P19-1656"
    },
    {
      "citation_id": "36",
      "title": "Learning speaker-independent multimodal representation for sentiment analysis",
      "authors": [
        "J Wang",
        "S Wang",
        "M Lin",
        "Z Xu",
        "W Guo"
      ],
      "year": "2023",
      "venue": "Information Sciences",
      "doi": "10.1016/j.ins.2023.01.116"
    },
    {
      "citation_id": "37",
      "title": "Cm-bert: Cross-modal bert for text-audio sentiment analysis",
      "authors": [
        "K Yang",
        "H Xu",
        "K Gao"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia",
      "doi": "10.1145/3394171.3413690"
    },
    {
      "citation_id": "38",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "A Zadeh",
        "M Chen",
        "S Poria",
        "E Cambria",
        "L Morency"
      ],
      "year": "2017",
      "venue": "Tensor fusion network for multimodal sentiment analysis",
      "arxiv": "arXiv:1707.07250"
    },
    {
      "citation_id": "39",
      "title": "Mosi: Multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "authors": [
        "A Zadeh",
        "R Zellers",
        "E Pincus",
        "L Morency"
      ],
      "year": "2016",
      "venue": "Mosi: Multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "arxiv": "arXiv:1606.06259"
    },
    {
      "citation_id": "40",
      "title": "Deep learning for sentiment analysis: a survey",
      "authors": [
        "L Zhang",
        "S Wang",
        "B Liu"
      ],
      "year": "2018",
      "venue": "WIREs Data Mining and Knowledge Discovery",
      "doi": "10.1002/widm.1253"
    },
    {
      "citation_id": "41",
      "title": "Quantum-inspired interactive networks for conversational sentiment analysis",
      "authors": [
        "Y Zhang",
        "Q Li",
        "D Song",
        "P Zhang1",
        "P Wang"
      ],
      "year": "2019",
      "venue": "Quantum-inspired interactive networks for conversational sentiment analysis",
      "doi": "10.1016/j.procs.2018.12.613"
    },
    {
      "citation_id": "42",
      "title": "Aggregated graph convolutional networks for aspect-based sentiment classification",
      "authors": [
        "M Zhao",
        "J Yang",
        "J Zhang",
        "S Wang"
      ],
      "year": "2022",
      "venue": "Information Sciences",
      "doi": "10.1016/j.ins.2022.03.082"
    },
    {
      "citation_id": "43",
      "title": "Multi-channel weight-sharing autoencoder based on cascade multi-head attention for multimodal emotion recognition",
      "authors": [
        "J Zheng",
        "S Zhang",
        "Z Wang",
        "X Wang",
        "Z Zeng"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia",
      "doi": "10.1109/TMM.2022.3144885"
    },
    {
      "citation_id": "44",
      "title": "Multimodal sentiment analysis based on fusion methods: A survey",
      "authors": [
        "L Zhu",
        "Z Zhu",
        "C Zhang",
        "Y Xu",
        "X Kong"
      ],
      "year": "2023",
      "venue": "Information Fusion",
      "doi": "10.1016/j.inffus.2023.02.028"
    }
  ]
}