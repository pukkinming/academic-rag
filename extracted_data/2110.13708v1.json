{
  "paper_id": "2110.13708v1",
  "title": "Tntc: Two-Stream Network With Transformer-Based Complementarity For Gait-Based Emotion Recognition",
  "published": "2021-10-26T13:55:31Z",
  "authors": [
    "Chuanfei Hu",
    "Weijie Sheng",
    "Bo Dong",
    "Xinde Li"
  ],
  "keywords": [
    "Gait-based emotion recognition",
    "complementarity",
    "convolutional neural network",
    "transformer"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recognizing the human emotion automatically from visual characteristics plays a vital role in many intelligent applications. Recently, gait-based emotion recognition, especially gait skeletons-based characteristic, has attracted much attention, while many available methods have been proposed gradually. The popular pipeline is to first extract affective features from joint skeletons, and then aggregate the skeleton joint and affective features as the feature vector for classifying the emotion. However, the aggregation procedure of these emerged methods might be rigid, resulting in insufficiently exploiting the complementary relationship between skeleton joint and affective features. Meanwhile, the long range dependencies in both spatial and temporal domains of the gait sequence are scarcely considered. To address these issues, we propose a novel two-stream network with transformer-based complementarity, termed as TNTC. Skeleton joint and affective features are encoded into two individual images as the inputs of two streams, respectively. A new transformer-based complementarity module (TCM) is proposed to bridge the complementarity between two streams hierarchically via capturing long range dependencies. Experimental results demonstrate TNTC outperforms state-of-the-art methods on the latest dataset in terms of accuracy.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Human emotion recognition, based on visual cues, has been widely applied in various intelligence applications, such as video surveillance  [1] , behavior prediction  [2, 3] , robot navigation  [4]  and human-machine interaction  [5] . Facial expression is one of the most predominant visual cues  [6]  to be used for recognizing the human emotions including anger, disgust, happiness, sadness, fear and other combinations. However, facial expressions may be unreliable in complex situations, for instance, imitation expressions  [7]  and concealed expressions  [8] . Therefore, recent researches gradually focus on the Corresponding author: Xinde Li (xindeli@seu.edu.cn).\n\nother visual cues of human to perceive the emotions, such as a gait of human in a walking  [9, 10] .\n\nRecent efforts have been made towards improving gaitbased emotion recognition  [11, 12, 13, 14, 15] , which can be categorized as sequence-based, graph-based and imagebased methods. The paradigm of sequence-based methods is to construct a sequence deep model, such as Gate Recurrent Unit (GRU) and Long Short-term Memory (LSTM), based on skeleton sequences to predict the emotion  [11, 12] . The insight of graph based-methods is to utilize Spatial Temporal Graph Convolutional Network (ST-GCN) to represent the inherent relationship between joints, since the skeleton is naturally structured as a graph in non-Euclidean geometric space  [13, 14] . The image-based methods cast the sequence classification as an image classification via encoding the skeleton sequences, while Convolutional Neural Network (CNN) is constructed to extract hierarchical features for recognizing the emotions  [15] . Although these methods achieve promising results in human emotion recognition, there are two major drawbacks. Firstly, the aggregation of joints and affective features might be rigid, resulting in exploiting the complementary information insufficiently. Furthermore, long range dependencies in both spatial and temporal domains are ignored, which are important to depict the implicit relationships between skeleton joints for human poses  [16] .\n\nTo address the above issues, we focus on the image-based method, and a novel two-stream network with transformerbased complementarity, termed as TNTC, is proposed for recognizing the human emotions. We argue that spatial and temporal information of the skeleton sequences can be effectively extracted via CNN in the image domain. Meanwhile, transformers are utilized to handle the problem of capturing long range dependencies via the self-attention mechanism whose effectiveness has been verified in many computer vision tasks  [17, 18]  • We propose a novel method for gait-based human emotion recognition, learning the deep features from images encoded by skeleton joints and affective features.\n\nTo the best of our knowledge, we are among the first to represent affective features as an image for gait-based human emotion recognition.\n\n• Transformer-based complementarity module (TCM) is exploited to complement the information between skeleton joints and affective features effectively via capturing long range dependencies.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "The overall framework of our proposed method consists of skeleton sequence encoding and two-stream emotion perception, as illustrated in Fig.  1 . Skeleton joints and affective features are first constructed via a skeleton sequence of gait, and encoded into skeleton joint image (SJI) and affective feature image (AFI), respectively. Next, TNTC based on CNNs is modeled to extract hierarchical features whose complementary information are replenished cross TCMs. MCM is conducted at the end of the network to identify the emotion category.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Skeleton Sequence Encoding",
      "text": "Skeleton joint image (SJI). The motivation of encoding the skeleton sequence as an image is to take full advantage of CNN to compactly extract the local spatial-temporal features.\n\nInspired by  [15] , we encode the 3D coordinates of joints as three channel of the image. Specifically, a skeleton sequence is given as follows: where P t presents coordinates of skeleton joints in the t-th frame. D s denotes the dimension of coordinates, and the number of joints is denoted as N s .\n\nThen, we arrange P t according to the order of frames, and JFI is encoded as follows:\n\nwhere M J ∈ R T ×Ns×Ds , and [•] is an operation to concatenate the coordinates of joints in the temporal dimension.\n\nAffective feature image (AFI). Besides the skeleton joints, affective features, such as posture and movement, convey the emotion information in the gaits  [9]  which are essential to involve the prediction for affective state of the subject. Here, we merely consider the joint angles as the affective features, and discard other characteristics, such as distances and velocities of joints. The reason is that the formulations of such characteristics can be approximated as special cases of a low-order combination which are implicit in the convolution operations of joint stream.\n\nTo construct the reliable AFI, we utilize the projection angles of joints on three planes to avoid the inconsistency caused by various viewpoints. As shown in Fig.  2 , the projection angles of joints on three planes are invariant with arbitrary viewpoints obviously. Formally, the projection angle of the n-th joint P t n ∈ R Ds on the projected planes can be computed as follows:\n\nwhere φ o1 (P\n\nwhere e 1 , e 2 , e 3 ∈ R 3 are unit vectors. For a joint p ∈ R 3 , the function φ oi (p) can be formulated as follows:\n\nwhere p oi(1) represents the 1-st value of the coordinate on the projected plane O-i, and is a small positive infinitesimal quantity to avoid the invalid denominator. Finally, the angles of the skeleton joints P t in the t-th frame can be formulated as follows:\n\nwhere Q t ∈ R Ns×Ds . AFI constructed via the projection angles can be denoted as follows:\n\nwhere M A ∈ R T ×Ns×Ds , and [•] is an operation to concatenate the projection angles of joints in the temporal dimension.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Tntc",
      "text": "Backbone. The backbone of TNTC is ResNet-34 whose capability of feature representations has been proved in many vision tasks  [19, 20] . We discard the fully connected layer of ResNet-34, and retain the four level residual blocks to extract hierarchical features, as shown in Fig.  1 . Assuming an input encoded map with a size of 224 × 224, the size and channel of output feature extracted via four level residual blocks are 7 × 7 and 512, respectively. TCM. After each residual block, TCM is modeled based on a vanilla transformer architecture  [17]  and a series of operations, as shown in Fig.  3 . The transformer architecture takes as input a sequence consisting of discrete tokens, each represented by a feature vector. The long range dependencies between feature vectors are captured, and the feature vectors supplemented by positional encoding to incorporate positional inductive biases. Specifically, given joints feature F J and affective feature F A extracted via residual blocks in a level, the size of F J and F A with H × W × C are first reduced via average pooling operations to H × W × C, where H = H/l, W = W/l, and scale parameter l is set to different values to modify the features with a fixed size in each level, since processing features with transformer at high spatial resolutions is computationally expensive. Then, two reduced features are concatenated to F AJ ∈ R H ×W ×2×C as input of transformer, which is an encoder structure completely described in  [17]  and its implementation details are discussed in Section 3. After capturing long range dependencies between F J and F A via transformer, F AJ is arranged to H × W × 2 × C and divided into two complementary information F c J and F c A with dimensions of H × W × C for joint stream and affective stream, respectively. Bilinear interpolation is utilized as upsampling operation to resize the complementary information to the same size of F J and F A . Finally, elementwise summation is leveraged to aggregate the complementary information and corresponding stream, respectively.\n\nMCM. At the end of joint and affective streams, joint and affective features are reduced to a size of 1 × 1 × 512 via average pooling, and are combined by element-wise addition as the final feature vector with 512-dimension. The feature vector is fed to MLP with two hidden layers and softmax function to classify the emotion of the skeleton sequence.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Setup",
      "text": "Dataset. We evaluate the proposed method on Emotion-Gait  [13]  dataset, which consists of 2177 real gait sequences separately annotated into one of four emotion categories including happy, sad, angry, or neutral. The gait is defined as the 16 point pose model, and the steps of gait sequences are maintained via duplication to 240 which is the maximum length of gait sequence in the dataset.\n\nEvaluation protocol. We employ 5-fold cross-validation to evaluate the proposed method, where the sample numbers of each category are divided in the same ratio among the folds. Accuracy is adopted as the metric defined as follows:\n\nwhere T denotes to the number of successfully classified gait sequences with corresponding categories, and S denotes the number of test samples. The average accuracy of 5-fold crossvalidation is recorded along its standard deviation. Implementation Details. The experiments are conducted on a work station with an NVIDIA RTX 2080Ti GPU. The Table  1 . Comparison of our method with the state-of-the-art on Emotion-Gait. The best result of accuracy is highlighted in bold.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Method",
      "text": "Accuracy % STEP  [13]  77.65(0.87) Graph-based G-GCSN  [14]  80.31(0.92) LSTM (Vanilla)  [12]  75.38(0.98) Sequence-based TEW  [11]  81.89(0.69) ProxEmo  [15]  80.33(0.85) Image-based TNTC (Ours) 85.97(0.75) proposed method is implemented based on PyTorch deep learning framework. The main hyperparameters consist of the network and training stage. For the hyperparameters of TCMs in the network, we stack 2 transformers and 4 attention heads for each TCM. The dimensions of feature embedding are 64, 128, 256, and 512 for corresponding TCMs, while the values of each level scale parameter l are 8, 4, 2, and 1. In the training stage, stochastic gradient descent (SGD) is used to optimize the learnable parameters with a momentum of 0.9 and a weight decay of 5e-4. The total epochs are 300 and initial learning rate is 1e-3, where the decay ratio is 0.1 every 75 epochs. JFI and AFI are resized into 224×224 via bilinear interpolation, and the size of mini-batches is 64.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Comparisons With The State-Of-The-Art",
      "text": "We compare our method with 5 state-of-the-art methods lately reported on Emotion-Gait including sequence-based  [11, 12] , graph-based  [13, 14] , and image-based  [15]  methods. To come up with a fair comparison, we reproduce these methods by public codes and report the experimental results with the same evaluation protocol, as listed in Tab. 1. We can observe that the proposed method achieves a superior performance than all the other methods.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Ablation Studies",
      "text": "Effectiveness of two-stream architecture. To clarify the effectiveness of two-stream architecture, we separately train joint stream and affective stream as two independent networks. Meanwhile, we construct TNTC without TCMs as a Effectiveness of TCMs. To confirm the effectiveness of TCMs, we gradually insert TCM at each level based on the baseline, and report the performances of the networks in Tab. 2, respectively. It can be observed that the accuracy of the baseline is improved as the number of TCM increases. Furthermore, we plot the attention maps of TCMs to reveal the capability of representing complementary information. As shown in Fig.  4 , the high scores of the attention maps focus on the cross regions between skeleton joints and affective features, which interpret visually the complementary information between two streams represented via TCMs.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose a novel method for gait-based human emotion recognition, modeled via a two-stream network with transformer-based complementarity (TNTC). Skeleton joints and affective features of gait sequence are encoded into images as the inputs of the two-stream architecture. Meanwhile, the importance of complementary information between two streams is revealed, which can be represented effectively via the proposed transformer-based complementarity module (TCM). Experimental results demonstrate that the proposed method achieves the superior performance over state-of-theart methods on the latest gait-based emotion dataset.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The overall framework of the proposed method. “RB id” denotes the “id”-th residual block of ResNet-34.",
      "page": 2
    },
    {
      "caption": "Figure 1: Skeleton joints and affective fea-",
      "page": 2
    },
    {
      "caption": "Figure 2: The visualization of invariant φoi(p) under arbitrary",
      "page": 2
    },
    {
      "caption": "Figure 2: , the projection an-",
      "page": 2
    },
    {
      "caption": "Figure 3: The details of TCM in TNTC. “P”, “C”, “S”, and “U”",
      "page": 3
    },
    {
      "caption": "Figure 1: Assuming an input",
      "page": 3
    },
    {
      "caption": "Figure 3: The transformer architecture takes",
      "page": 3
    },
    {
      "caption": "Figure 4: The visual interpretation of complementary informa-",
      "page": 4
    },
    {
      "caption": "Figure 4: , the high scores of the attention maps",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JJooiinntt SSttrreeaamm\nCCCooommm CCCooommm CCCooommm CCCooommm\nRRRBBB___111 RRRBBB___222 RRRBBB___333 RRRBBB___444\nMMM MMM MMM MMM": "",
          "Column_2": "CCCooommm\nMMM",
          "Column_3": "",
          "Column_4": "CCCooommm\nMMM",
          "Column_5": "",
          "Column_6": "CCCooommm\nMMM",
          "Column_7": "",
          "Column_8": "CCCooommm\nMMM",
          "Column_9": ""
        },
        {
          "JJooiinntt SSttrreeaamm\nCCCooommm CCCooommm CCCooommm CCCooommm\nRRRBBB___111 RRRBBB___222 RRRBBB___333 RRRBBB___444\nMMM MMM MMM MMM": "",
          "Column_2": "TTTCCCMMM",
          "Column_3": "",
          "Column_4": "TTTCCCMMM",
          "Column_5": "",
          "Column_6": "TTTCCCMMM",
          "Column_7": "",
          "Column_8": "TTTCCCMMM",
          "Column_9": ""
        },
        {
          "JJooiinntt SSttrreeaamm\nCCCooommm CCCooommm CCCooommm CCCooommm\nRRRBBB___111 RRRBBB___222 RRRBBB___333 RRRBBB___444\nMMM MMM MMM MMM": "CCCooommm CCCooommm CCCooommm CCCooommm\nRRRBBB___111 RRRBBB___222 RRRBBB___333 RRRBBB___444\nMMM MMM MMM MMM\nAAffffeeccttiivvee SSttrreeaamm",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": ""
        },
        {
          "JJooiinntt SSttrreeaamm\nCCCooommm CCCooommm CCCooommm CCCooommm\nRRRBBB___111 RRRBBB___222 RRRBBB___333 RRRBBB___444\nMMM MMM MMM MMM": "",
          "Column_2": "CCCooommm",
          "Column_3": "",
          "Column_4": "CCCooommm",
          "Column_5": "",
          "Column_6": "CCCooommm",
          "Column_7": "",
          "Column_8": "CCCooommm",
          "Column_9": ""
        },
        {
          "JJooiinntt SSttrreeaamm\nCCCooommm CCCooommm CCCooommm CCCooommm\nRRRBBB___111 RRRBBB___222 RRRBBB___333 RRRBBB___444\nMMM MMM MMM MMM": "",
          "Column_2": "MMM",
          "Column_3": "",
          "Column_4": "MMM",
          "Column_5": "",
          "Column_6": "MMM",
          "Column_7": "",
          "Column_8": "MMM",
          "Column_9": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TCM": "++\nHH××WW××CC HH××WW××CC\nHH''××WW''××CC T HH''××WW''××CC PP UU\nar\nHH''××WW''××22××CC s n HH''××WW''××22××CC of CC SS\nr F F\nm AJ AJ\ne r PP UU"
        },
        {
          "TCM": "++"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Graph-based": "",
          "STEP[13]": "G-GCSN[14]",
          "77.65(0.87)": "80.31(0.92)"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Image-based": "",
          "ProxEmo[15]": "TNTC(Ours)",
          "80.33(0.85)": "85.97(0.75)"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "BaselinewithTCMs",
          "(cid:88)": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "83.27(0.79)": ""
        },
        {
          "Column_1": "",
          "(cid:88)": "(cid:88)",
          "Column_3": "(cid:88)",
          "Column_4": "(cid:88)",
          "Column_5": "",
          "83.27(0.79)": "84.13(0.85)"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "A wearable pedestrian localization and gait identification system using kalman filtered inertial data",
      "authors": [
        "Nasim Hajati",
        "Amin Rezaeizadeh"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "3",
      "title": "Review and challenges of technologies for real-time human behavior monitoring",
      "authors": [
        "Sylmarie Dávila-Montero",
        "Alisa Dana-Lê",
        "Gary Bente",
        "Angela Hall",
        "Andrew Mason"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Biomedical Circuits and Systems"
    },
    {
      "citation_id": "4",
      "title": "The fusion of electroencephalography and facial expression for continuous emotion recognition",
      "authors": [
        "Dahua Li",
        "Zhe Wang",
        "Chuhan Wang",
        "Shuang Liu",
        "Wenhao Chi",
        "Enzeng Dong",
        "Xiaolin Song",
        "Qiang Gao",
        "Yu Song"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "5",
      "title": "Modelling multi-channel emotions using facial expression and trajectory cues for improving socially-aware robot navigation",
      "authors": [
        "Aniket Bera",
        "Tanmay Randhavane",
        "Dinesh Manocha"
      ],
      "year": "2019",
      "venue": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "6",
      "title": "Predicting emotion reactions for human-computer conversation: A variational approach",
      "authors": [
        "Rui Zhang",
        "Zhenyu Wang",
        "Zhenhua Huang",
        "Li Li",
        "Mengdan Zheng"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Human-Machine Systems"
    },
    {
      "citation_id": "7",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "Shan Li",
        "Weihong Deng"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "8",
      "title": "Imitation of in-group versus out-group members' facial expressions of anger: A test with a time perception task",
      "authors": [
        "Laurie Mondillon",
        "Paula Niedenthal",
        "Sandrine Gil",
        "Sylvie Droit- Volet"
      ],
      "year": "2007",
      "venue": "Social neuroscience"
    },
    {
      "citation_id": "9",
      "title": "Reading between the lies: Identifying concealed and falsified emotions in universal facial expressions",
      "authors": [
        "Stephen Porter",
        "Leanne Brinke"
      ],
      "year": "2008",
      "venue": "Psychological science"
    },
    {
      "citation_id": "10",
      "title": "Emotion recognition from gait analyses: Current research and future directions",
      "authors": [
        "Shihao Xu",
        "Jing Fang",
        "Xiping Hu",
        "Edith Ngai",
        "Yi Guo",
        "Victor Leung",
        "Jun Cheng",
        "Bin Hu"
      ],
      "year": "2020",
      "venue": "Emotion recognition from gait analyses: Current research and future directions",
      "arxiv": "arXiv:2003.11461"
    },
    {
      "citation_id": "11",
      "title": "Multi-task learning for gait-based identity recognition and emotion recognition using attention enhanced temporal graph convolutional network",
      "authors": [
        "Weijie Sheng",
        "Xinde Li"
      ],
      "year": "2021",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "12",
      "title": "Take an emotion walk: Perceiving emotions from gaits using hierarchical attention pooling and affective mapping",
      "authors": [
        "Uttaran Bhattacharya",
        "Christian Roncal",
        "Trisha Mittal",
        "Rohan Chandra",
        "Kyra Kapsaskis",
        "Kurt Gray",
        "Aniket Bera",
        "Dinesh Manocha"
      ],
      "year": "2020",
      "venue": "Computer Vision -ECCV 2020"
    },
    {
      "citation_id": "13",
      "title": "Identifying emotions from walking using affective and deep features",
      "authors": [
        "Tanmay Randhavane",
        "Uttaran Bhattacharya",
        "Kyra Kapsaskis",
        "Kurt Gray",
        "Aniket Bera",
        "Dinesh Manocha"
      ],
      "year": "2019",
      "venue": "Identifying emotions from walking using affective and deep features",
      "arxiv": "arXiv:1906.11884"
    },
    {
      "citation_id": "14",
      "title": "Step: Spatial temporal graph convolutional networks for emotion perception from gaits",
      "authors": [
        "Uttaran Bhattacharya",
        "Trisha Mittal",
        "Rohan Chandra",
        "Tanmay Randhavane",
        "Aniket Bera",
        "Dinesh Manocha"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "15",
      "title": "G-gcsn: Global graph convolution shrinkage network for emotion perception from gait",
      "authors": [
        "Yuan Zhuang",
        "Lanfen Lin",
        "Ruofeng Tong",
        "Jiaqing Liu",
        "Yutaro Iwamoto",
        "Yen-Wei Chen"
      ],
      "year": "2020",
      "venue": "ACCV Workshops"
    },
    {
      "citation_id": "16",
      "title": "Proxemo: Gait-based emotion learning and multi-view proxemic fusion for socially-aware robot navigation",
      "authors": [
        "Venkatraman Narayanan",
        "Bala Murali Manoghar",
        "Vishnu Sashank Dorbala",
        "Dinesh Manocha",
        "Aniket Bera"
      ],
      "year": "2020",
      "venue": "2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"
    },
    {
      "citation_id": "17",
      "title": "Learning trajectory dependencies for human motion prediction",
      "authors": [
        "Wei Mao",
        "Miaomiao Liu",
        "Mathieu Salzmann",
        "Hongdong Li"
      ],
      "year": "2019",
      "venue": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "18",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer",
        "Alexander Kolesnikov",
        "Dirk Weissenborn",
        "Xiaohua Zhai",
        "Thomas Unterthiner",
        "Mostafa Dehghani",
        "Matthias Minderer",
        "Georg Heigold",
        "Sylvain Gelly"
      ],
      "year": "2020",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "19",
      "title": "Transreid: Transformer-based object re-identification",
      "authors": [
        "Shuting He",
        "Hao Luo",
        "Pichao Wang",
        "Fan Wang",
        "Hao Li",
        "Wei Jiang"
      ],
      "year": "2021",
      "venue": "Transreid: Transformer-based object re-identification",
      "arxiv": "arXiv:2102.04378"
    },
    {
      "citation_id": "20",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "21",
      "title": "An efficient convolutional neural network model based on object-level attention mechanism for casting defect detection on radiography images",
      "authors": [
        "C Hu",
        "Y Wang"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Industrial Electronics"
    }
  ]
}