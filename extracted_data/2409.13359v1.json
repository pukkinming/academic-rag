{
  "paper_id": "2409.13359v1",
  "title": "Emotionqueen: A Benchmark For Evaluating Empathy Of Large Language Models",
  "published": "2024-09-20T09:44:51Z",
  "authors": [
    "Yuyan Chen",
    "Hao Wang",
    "Songzhou Yan",
    "Sijia Liu",
    "Yueze Li",
    "Yi Zhao",
    "Yanghua Xiao"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotional intelligence in large language models (LLMs) is of great importance in Natural Language Processing. However, the previous research mainly focus on basic sentiment analysis tasks, such as emotion recognition, which is not enough to evaluate LLMs' overall emotional intelligence. Therefore, this paper presents a novel framework named Emotion-Queen for evaluating the emotional intelligence of LLMs. The framework includes four distinctive tasks: Key Event Recognition, Mixed Event Recognition, Implicit Emotional Recognition, and Intention Recognition. LLMs are requested to recognize important event or implicit emotions and generate empathetic response. We also design two metrics to evaluate LLMs' capabilities in recognition and response for emotion-related statements. Experiments yield significant conclusions about LLMs' capabilities and limitations in emotion intelligence. 1 https://chat.openai.com/ I completed the project today and also received a gift from a friend.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Introduction",
      "text": "Emotional intelligence in humans has long been a topic of interest in psychological research  (Chen and Xiao, 2024) . Emotionally intelligent individuals possess the capability to perceive, use, understand, and manage emotions effectively  (Colman, 2015; Chen et al., 2023e, 2024e) . The concept of emotional intelligence has been widely recognized, and many researchers argue that it can be considered as a form of intelligence. Various tests and assessments have been developed to evaluate different aspects of emotional intelligence, which provide scores for each branch of emotional intelligence as well as an overall score  (Mayer et al., 2003; Chen et al., 2024d,b) .\n\nIn recent years, there has been a growing interest in evaluating the emotional intelligence of large language models (LLMs). Early efforts primarily fo-  cus on atomic tasks in emotion recognition, such as aspect-based sentiment analysis, target-dependent sentiment classification  (Li et al., 2023e; Tang et al., 2015; Chen et al., 2023c; Li et al., 2022b Li et al., , 2023d Li et al., ,c, 2024)) , etc. As the capabilities of dialogue models advanced, evaluations expand to include sentiment analysis in a conversational setting  (Li et al., 2023a; Chen et al., 2022) , such as whether an LLM generate empathetic responses in a dialogue  (Zhao et al., 2023; Chen et al., 2024f) . However, the overall assessment of emotional intelligence in LLMs is still lack of investigation  (Schaaff et al., 2023) , and existing evaluations often suffer from subjectivity issues  (Elyoseph et al., 2023; Chen et al., 2023b) . Therefore, it's necessary to construct a unified and objective benchmark for evaluating the emotional intelligence of LLMs, thereby enhancing their capabilities in handling real-world emotional interactions. For example, Fig.  1  shows two types of responses in different scenarios in the real world: blue for good responses and orange for bad ones. Good responses demonstrate empathy, concern or positive reinforcement, like asking about someone's health outside a hospital or acknowledging hard work. Poor responses either miss the emotional context, like commenting on food when arXiv:2409.13359v1 [cs.CL] 20 Sep 2024 the user comes back from a hospital, or provide a possibly stressful choice, like asking someone to balance excitement and worry about a promotion. These scenarios pose a higher-level emotion requirements which are more discriminative for evaluate the emotion intelligence of LLMs.\n\nInspired by the scenarios in the real world, we propose an evaluation benchmark named Emotion-Queen to evaluate LLMs' emotion intelligence in our work. Specifically, we construct 10,000 users' statement including five categories and introduce four distinct tasks including Key Event Recognition, Mixed Event Recognition, Implicit Emotional Recognition, and Intention Recognition. LLMs are requested to recognize the most significant event or multiple events of similar importance expressed in the users' statement, identifying underlying users' deep emotions or intentions. They are also asked to make empathetic responses aiming at key events, or provide emotional support. Moreover, we introduce two metrics, including the PASS rate and the WIN rate to quantify LLMs' capabilities in recognition and response for emotion-related statements. Experiments demonstrate that Claude2 and LLaMA-70B have great performance in Emotion-Queen. In summary, our study makes three pivotal contributions:\n\n• We propose a benchmark named Emotion-Queen including four distinct tasks for evaluating LLMs' emotion intelligence.\n\n• Based on this benchmark, we introduce PASS rate and the WIN rate to quantify LLMs' capabilities in recognition and response for emotion-related statements.\n\n• We conduct comprehensive experiments for powerful LLMs, yielding significant conclusions about their capabilities and limitations in emotion intelligence.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Datasets And Task Setups",
      "text": "We categorize the empathy benchmark into four distinct tasks: Key Event Recognition, where LLMs are expected to identify the more significant event in a user's statement that includes both a significant event and a routine event. Mixed Event Recognition, where LLMs are tasked with simultaneously responding to both aspects when the user's statement contains two events of similar importance. Implicit Emotional Recognition, focusing on LLMs identifying underlying deep emotions in the user's statements.\n\nIntent Recognition, where LLMs should comprehend the user's real purpose and provide specific suggestions, going beyond just offering comfort. Based on the aforementioned four dimensions, we concentrate on generating 10,000 statements across five primary life scenarios using GPT-4 1 , with an equal number of statements for each scenario. These scenarios are achievements, family and friends, health status, economic status, and accidents. Some statement examples are displayed in Table  14  (positive statement), and figures from Fig.  8  to Fig.  21  (negative statement). The portion of positive and negative statements are 3:7 in our generated statements, which caters for the real situation that positive statements often require less complex emotional expressions compared to negative ones. The tasks' overview is depicted in Fig.  2 , and the user statement generation prompts are detailed in Table  13 .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Task Setups",
      "text": "Key Event Recognition focuses on identifying and understanding significant events expressed by users and their emotional impact. Based on the Emotion-Focused Theory in psychology developed by  Greenberg (2004) , this approach encourages awareness and expression of emotions, including those neglected or not fully understood, and supports emotional transformation through exploring and processing emotional experiences. For instance, when a user's statement contains multiple points of information (usually a significant event and a routine event), the LLM should identify the most significant event and ask questions based on it. For example, if a user says, \"I ran into an old middle school classmate on my way to buy medicine,\" the LLM should inquire about the reason for buying medicine rather than the meeting with the classmate. The LLM does not need to address both aspects simultaneously, meaning it should not respond to both the medicine purchase and the classmate encounter.\n\nMixed Event Recognition focuses on responding to both aspects simultaneously when the user's statement includes two events of similar importance, differing from Key Event Recognition, which deals with a single important event. Based on the concept of emotional complexity developed by  Lindquist and Barrett (2008) , a statement usually conveys multiple events with multi-aspect emotion at similar importance, and an empathetic response is expected to pay attention to all involved events. Given a sample statement \"I got promoted, but this means I have to work further away from home,\" a more empathetic response is \"Are you excited about the promotion, and are you also worried about being far from home?\" instead of ignoring any one event.\n\nImplicit Emotional Recognition is about identifying underlying deep emotions. In some scenarios, although the user's statement includes only one event, the emotion is implicit rather than directly expressed through language. The theory of Emotion Understanding, proposed by  Mayer and Salovey (1993)  and popularized by  Cherniss and Goleman (2000) , emphasizes recognizing both direct and implicit emotions expressed by others and providing appropriate responses. Therefore, it's empathetic to identify implicit emotions behind users' statements and provide targeted emotional support. For example, the response \"Do you feel overwhelmed? Have you try some ways to relax?\" is more suitable for emotional support than a simple reply \"Hard work!\" which seems perfunctory for responding to the statement \"I've been busy with work all day.\"\n\nIntention Recognition aims to understand the potential purpose or needs behind users' statements and provide specific help or solutions. Based on the pragmatics theory proposed by  Grice (1978) , especially the concept of Implicature, this approach suggests that language communication involves understanding not only the literal meaning but also the speaker's intent and purpose. For a sample statement like \"I've been busy with work all day,\" a response that recognizes intention is \"Have you eaten? I can order some takeout for you.\" instead of a simple emotional support like \"Remember to eat.\"",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "We propose employing PASS rate and WIN rate to evaluate four tasks using GPT-4, where each LLM response is rated as either 0 or 1, disregarding ambiguous middle results. Because it can be quite challenging to measure the difference between intermediate levels such as 3 or 4 objectively in 1-5 scale. The PASS rate assesses an LLM's accuracy in recognizing emotion-related events, while the WIN rate evaluates its ability to provide an empathetic response.\n\nIn the Key Event Recognition task, if an LLM correctly identifies a key event, it earns 1 point in PASS, otherwise 0; if it also presents an empathetic response for that event, it scores 1 in WIN, otherwise 0. For instance, if a user says, \"I visited my sick mother in the hospital today, then went to the supermarket,\" and the LLM correctly recognizes \"visiting the sick mother in the hospital\" as the key event, it scores 1 in PASS. If the LLM responds with \"Is your mother okay?\", it is deemed appropriate and scores 1 in WIN. Responses like \"It's troublesome that your mother is sick, hope she doesn't keep you too busy\" or \"Everyone's mother gets sick eventually\" score 0.\n\nIn the Mixed Event Detection task, if an LLM recognizes both mentioned events, it scores 1 in PASS; if it provides an empathetic response to both, it scores 1 in WIN. For example, if a user mentions, \"I got promoted but also need to move to a new city,\" and the LLM identifies both \"got promoted\" and \"move to a new city\" correctly, it scores 1 in PASS. An LLM response like \"Congratulations on the promotion! Is moving to a new city a challenge for you?\" would score 1 in WIN, whereas focusing only on one aspect, like \"Great, you got promoted! That's a huge achievement,\" scores 0.\n\nFor the Implicit Emotion Recognition task, if an LLM accurately identifies the core emotion in a user's statement, it earns 1 in PASS; a corresponding empathetic response earns 1 in WIN. For instance, if a user says, \"I've been feeling a bit anxious lately,\" and the LLM correctly identifies \"anxiety,\" it scores 1 in PASS. A response like \"Would you like to talk about what is making you anxious?\" would be appropriate, scoring 1 in WIN, whereas a generic response like \"Everyone gets anxious\" scores 0.\n\nIn the Intent Detection task, if an LLM accurately discerns the core intent of a user's statement, it scores 1 in PASS; offering specific advice or help earns 1 in WIN. For example, if a user states, \"I am considering changing jobs recently,\" and the LLM correctly identifies the intent, such as \"offering career advice\" or \"helping find new job opportunities,\" it scores 1 in PASS. An LLM response like \"I can help you analyze the pros and cons of changing jobs, or offer some advice\" would be appropriate, scoring 1 in WIN, while a response like \"You should find suitable job opportunities yourself \" scores 0.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiments",
      "text": "In this section, we conduct extensive experiments to evaluate different LLMs' performance in the proposed EmotionQueen.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Experimental Setups",
      "text": "Our experiments are conducted on 8 Nvidia A100 GPUs, each with 80GB of memory, and we use Py-Torch 2 in Python. We set the maximum sequence length for both input and output sequences to maximum 100 tokens, ensuring the responses not over lengthy. We also conduct an analysis of the average response length from LLMs and find it to be 52.3 tokens, demonstrating the reasonableness of the set sequence length.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Datasets, Baselines And Metrics",
      "text": "The baseline LLMs for this evaluation are BLOOM-7B  (Workshop et al., 2023)   et al., 2020), GPT4 (OpenAI, 2023), LLaMA2-7B  (Touvron et al., 2023) , LLaMA2-70B  (Touvron et al., 2023) , Vicuna-7B  (Chiang et al., 2023), and Vicuna-33B (Zheng et al., 2023b) . We randomly select 1000 questions each task generated by each LLM and enroll three volunteers to manually evaluate generated responses with the same metrics as GPT4. Our annotators are selected from a pool of female graduate students in psychology. While gender may not necessarily impact the scoring, there's a common perception that women tend to be more detail-oriented in their thinking, especially regarding emotional intelligence. We first inform the annotators about the intent of each task and the rules for scoring. Then, we ask them to score the responses. To ensure the reliability and confidence of human ratings, we also calculate Inter-rater agreement of Krippendorff's Alpha (IRA) to ensure the confidence of human ratings. For the controversial ratings which have low agreements (less than 0.7), we discard this statement and introduce another one.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Main Results",
      "text": "Question 1:Which LLM is the winner of the Emo-tionQueen? Answer 1: LLaMA-70B! But Claude2 is slightly fall behind! Performance of different LLMs across four tasks is shown in Table  1, Table 2, Table 3  and Ta  rate and WIN rate, and compare the their comprehensive empathy capabilities as shown in Table  5  and Fig.  3 . In the aspect of key event recognition (denoted as \"KER\"), Claude2, BLOOM-176B, and LLaMA2-70B show nearly perfect PASS rate, demonstrating their strong capability in capturing the core events of user statements. In WIN rate, Claude2 maintains a lead, other LLMs such as Vicuna-33b and LLaMA2-70B fall slightly short.\n\nIn the aspect of mixed event recognition (denoted as \"MER\"), Claude2 also excels in identifying multiple events, showing its capability in handling complex scenarios. But the WIN rate generally decreases for all LLMs, suggesting potential room for improvement in understanding and balanced responses to mixed events. Regarding implicit emotion recognition (denoted as \"IER\"), LLaMA2-70B, Vicuna-33B, and GPT4 perform well. However, the WIN rate analysis shows that, except for LLaMA2-70B, other LLMs like Vicuna-33B and GPT4 have gaps in response appropriateness. In intent recognition (denoted as \"IR\"), most LLMs, especially Claude2 and LLaMA2-70B, show a high PASS rate, but they still show potential in providing effective guidance. Overall, although most LLMs excel in identifying the key content of user statements, they still have room for improvement in providing empathetic responses.\n\nWe also explore the emotional intelligence of these LLMs on the traditional emotion datasets, including MELD  (Poria et al., 2018) , Emotion-Lines (Chen et al., 2018), and DailyDialog  (Li et al., 2017) , which are all about emotion categorization. We randomly select 100 samples from each for assessment with accuracy as the metric. We find that the results as shown in Question 2: What's the relationship between PASS and WIN rate? Answer 2: There is almost no correlation!\n\nWe also analyze the relationship between PASS and WIN rate in four tasks as shown in Fig.  4 . It suggests that there is little correlation between the PASS and WIN rate across different LLMs for the various tasks. In the Key Event Recognition task, most LLMs cluster around high PASS scores and high WIN scores, indicating their strong capabilities in this task. In the Mixed Events Recognition task, when observing a low WIN rate (below 30%), there appears to be a positive correlation between PASS and WIN rate, with a wide dispersion in WIN rates among the LLMs. However, Claude2 still stands out, achieving high PASS rate and WIN rate compared to others. In Implicit Emotion Recognition and Intent Recognition tasks, there's virtually no correlation between PASS and WIN rates, with PASS rates being generally high across the board. For Implicit Emotion Recognition, WIN rates are distributed across various scores, whereas for Intent Recognition, there is a concentration of higher WIN rates. Overall, LLaMA-70B and Claude2 show consistent and superior performance across all tasks and metrics, while other models have their strengths in specific areas.\n\nQuestion 3: Which LLM wins in each domain? Answer 3: LLaMA-70B in economic status and accidents, as well as Claude2 in achievements, family and friends, and health status!\n\nWe also compare the performance of different models in different categories of events as shown in Table  8 , Table  9 , Table  10  and Table 11 . We also rank them based on the average of PASS rate and WIN rate. In the category of the achievement (denoted as \"Ach\"), Claude2, LLaMA2-70B, and BLOOM-176B all show a 100% pass rate, but Claude2 slightly leads in win rate. In the category of the health status (denoted as \"H\"), Claude2 continues to hold the highest average score, with LLaMA2-70B and BLOOM-176B also performing well. In the category of the family and friends (denoted as \"F\"), Claude2 and LLaMA2-70B continue to lead, showing their ability to handle complex emotional issues related to close relationships.\n\nIn the category of the economic status (denoted as \"E\"), LLaMA2-70B achieves the highest score.\n\nClaude2 and GPT4 also show strong performance, particularly in PASS rate. Finally, in the category of accidents (denoted as \"Acc\"), LLaMA2-70B leads significantly in WIN rate, indicating its ability to provide more accurate and effective responses in sudden and bad situations. We also shown the relationship between PASS rate and WIN rate in each scenario of event in Fig.  5 . The trend is similar as that overall relationship as shown in Fig.  4  Question 4: Is the automatic scores generated by GPT4 agree with human perspectives? Answer 4: Yes, and the correlation coefficient reaches 0.991!\n\nWe select 1000 statements with an equal number in each task to make human evaluation. We compare the consistency between automatic scores and human ratings as shown Fig.  6 . The Pearson correlation coefficient achieves 0.991, indicating a strong alignment. We also recruit three volunteers to respond to these statements, selecting the one with the highest average score for WIN and PASS as the human-written performance. We find that some LLMs, such as LLaMA2-70B and Claude2, have already surpassed the real human in empathy capabilities, at least in the tasks in EmotionQueen. This indicates that some LLMs are capable of generating responses that can exceed the emotional value brought by human interactions.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Case Study",
      "text": "We shown some cases in Fig.  7  and more cases are shown in figures from Fig.  8  to Fig.  21 . For example, in key event recognition, Claude2 not only accurately captures the significant event of user getting their dream job but also further inquires about the user's favorite aspects of the new job. Similarly, in mixed event detection, Claude2 can simultaneously focus on the sadness of friends moving away and the content of the package. This demonstrates its capability to deeply understand user's situation and effectively communicate. Looking further, in implicit emotion recognition, LLaMA2-70B not only identifies the underlying stress but also delves into the user's feelings about the exams, providing emotional support. In intention recognition, addressing real-life problems of users, such as \"My phone keeps dying...\", Claude2 offers practical advice, like reducing screen brightness, showing its capability to discern user needs and provide practical help.\n\nThere are also some bad cases from several LLMs as shown in Table  15 . For example, GPT3.5 echoes the original statement without adding empathy in the key event recognition. GPT4 acknowledges the implied emotion but behaves more like questioning than offering support. Claude2 shows After landing my dream job, the simple act of packing my lunch with excitement and anticipation has become a daily ritual.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Statement:",
      "text": "That's wonderful that you landed your dream job! I can understand why you'd be excited to pack your lunch each day. What is it about this new job that makes it your dream job? I'd love to hear more about what you enjoy most about your new role and responsibilities.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Claude2:",
      "text": "Your friend moving away made you very sad. However, It must have been a surprise to receive a package from your friend so soon after they moved away. What was inside the package? I've taken two exams recently, and I feel like both of them had quite challenging questions.\n\nYou've been quite stressed recently. Even though the exam questions were a bit tough, how do you think you did on these exams? My phone keeps dying in the middle of the day. Too bad...You can reduce screen brightness or find external power sources.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Statement:",
      "text": "",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Claude2:",
      "text": "LLaMA2-70B:\n\nStatement:",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Key Event Recognition Mixed Event Recognition",
      "text": "The day after a close friend moved away, I received a package addressed to me from him.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Statement:",
      "text": "",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Intention Recognition",
      "text": "Claude2: interest and asks for more details without acknowledging the difficulties mentioned. LLaMA-70B overlooks the potential concern about volcanic activity and focuses only on the travel inspiration aspect. Vicuna-33B's response is positive, asking for details but missing the frustration of waiting.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Implicit Emotion Recognition",
      "text": "Falcon-180B fails to address any sympathy for the experience at the police station, focusing instead on the choice of bread. BLOOM-176B invites further\n\ndiscussion with an open-ended question but doesn't convey understanding for the difficulties. BLOOM-7B's casual greeting seems to ignore the gravity of the sadness. LLaMA2-7B acknowledges the friend's kindness but might need more emotional resonance. Vicuna-7B's response could lead to anxiety by focusing on negative outcomes. Falcon-7B's could be perceived as accusatory, questioning the user's intent without offering consolation. In the future, enhancements could be made through methods like supervised fine-tuning or reinforcement learning to improve LLMs' performance in this benchmark.\n\n4 Related Work",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Emotion Recognition",
      "text": "LLMs has seen significant advancements in different scenarios  (Chen et al., 2023a; Ren et al., 2024; Tao et al., 2024; Chen et al., 2024c,a) .   2023a ) provide a investigation of LLMs' capabilities in various sentiment analysis tasks. However, even when emotions are accurately recognized, these tend to be explicit emotions. In the real world, implicit emotions are prevalent.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Empathetic Dialogue",
      "text": "",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Limitations",
      "text": "The limitations of our study pertain to two main areas. First, LLMs may generate harmful or inappropriate utterances, which may not be discovered by our evaluation metrics. Despite efforts to mitigate such outputs, the complex nature of language and context can lead to scenarios where the LLMs' responses are offensive or ethically dubious. Second, while our evaluation framework is designed to assess the emotional intelligence of LLMs, it is not immune to subjectivity. The interpretation of emotional events and the judgment of the quality of responses can be influenced by the personal biases of the assessors. In light of these limitations, future research must focus on enhancing the ethical safeguards of responses of LLMs and on refining the objectivity of emotional intelligence evaluations.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Prompt Key Event Recognition",
      "text": "Statement Generate statements following these rules: 1. A statement should include a significant event and a trivial event, the significant event most likely negative, showing how the significant event has an impact on the trivial event. 2. Both events must not be vague.Here are some examples:After a difficult breakup, even the act of cooking for one feels lonely and unfulfilling.When I successfully overcame social anxiety, last year's Christmas party became more enjoyable and fun.Avoid expressions like these:My daily nature walks provide solace and healing as I cope with the loss of my beloved pet.(Not showing the impact of significant event on daily routine)After experiencing a major job loss, my daily routines felt even heavier, as if the world had lost its color. Below is a statement, containing a significant event and some trivial event, and a reply generated by a language model. You need to score the reply. First, decide whether the reply succeeds in recognizing both events. Second, decide whether the reply gives a balanced response, which means the length and intensity of emotion for each event should not fall too apart. If the reply is a balanced one, give a 2. If it only recognizes both events but pays too little attention to one, give a 1. If it completely fails, 0.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Mixed Event Recognition",
      "text": "Statement Follow these steps to generate statements that meet the criteria.First, provide statements that contain both a significant negative event and a routine event, with no causal relationship between them. The negative event should have a profound or long-lasting impact, such as personal or family illness, an encounter with a crime or disaster, economic pressure, failure in an important exam or interview, etc. Examples: 'My mother got sick, and I saw a puppy; I was in a car accident, and I have many PPTs to do.'Second, replace the significant event with a phrase that can evoke association, making it less direct. Examples: 'My mother got sick' becomes 'I went to the pharmacy to buy medicine'; 'I was in a car accident' becomes 'I lay in the hospital bed.'Third, link the events through non-causal relationships such as time or space. Examples: 'On my way to the pharmacy to buy medicine, I saw a puppy; lying in the hospital bed, I still have to work on the PPT for work.'Finally, grade the significant event from 1 to 3, where 3 is the most significant. Examples: departure of a close relative: 3; a disaster or accident: 2; a therapy session: 1.More examples of finished statements:'2 After spending hours at the police station, I was hungry so I stopped by the grocery store to pick up some bread.\"3 On my way back from the funeral home, I conveniently dropped off some books at the library since it was on my route.\"1 Following a long session with the therapist, I decided to watch a movie to relax.",
      "page_start": 22,
      "page_end": 22
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Responses with and without empathy in four",
      "page": 1
    },
    {
      "caption": "Figure 1: shows two",
      "page": 1
    },
    {
      "caption": "Figure 8: to Fig. 21 (negative statement). The por-",
      "page": 2
    },
    {
      "caption": "Figure 2: , and the user statement generation prompts",
      "page": 2
    },
    {
      "caption": "Figure 2: The overview of the proposed EmotionQueen benchmark, including four tasks.",
      "page": 3
    },
    {
      "caption": "Figure 3: In the aspect of key event recogni-",
      "page": 5
    },
    {
      "caption": "Figure 3: The overall performance of different LLMs in",
      "page": 5
    },
    {
      "caption": "Figure 5: The trend is similar as",
      "page": 6
    },
    {
      "caption": "Figure 4: Question 4: Is the automatic scores generated by",
      "page": 6
    },
    {
      "caption": "Figure 4: The relationship between PASS rate and WIN rate of different LLMs in four tasks, respectively.",
      "page": 7
    },
    {
      "caption": "Figure 5: The relationship between PASS rate and WIN rate of different LLMs in five categories of events,",
      "page": 7
    },
    {
      "caption": "Figure 6: The Pearson correlation coefficient that nor-",
      "page": 7
    },
    {
      "caption": "Figure 6: The Pearson",
      "page": 7
    },
    {
      "caption": "Figure 7: and more cases are",
      "page": 7
    },
    {
      "caption": "Figure 8: to Fig. 21.",
      "page": 7
    },
    {
      "caption": "Figure 7: Responses of top LLMs, i.e. Claude2 and LLaMA2-70B, in four tasks for given statements.",
      "page": 8
    },
    {
      "caption": "Figure 8: Responses of different LLMs in key event recognition for given statements.",
      "page": 16
    },
    {
      "caption": "Figure 9: Responses of different LLMs in key event recognition for given statements.",
      "page": 17
    },
    {
      "caption": "Figure 10: Responses of different LLMs in key event recognition for given statements.",
      "page": 18
    },
    {
      "caption": "Figure 11: Responses of different LLMs in key event recognition for given statements.",
      "page": 19
    },
    {
      "caption": "Figure 12: Responses of different LLMs in mixed events recognition for given statements.",
      "page": 20
    },
    {
      "caption": "Figure 13: Responses of different LLMs in mixed events recognition for given statements.",
      "page": 21
    },
    {
      "caption": "Figure 14: Responses of different LLMs in mixed events recognition for given statements.",
      "page": 22
    },
    {
      "caption": "Figure 15: Responses of different LLMs in mixed events recognition for given statements.",
      "page": 23
    },
    {
      "caption": "Figure 16: Responses of different LLMs in implicit emotion recognition for given statements.",
      "page": 24
    },
    {
      "caption": "Figure 17: Responses of different LLMs in implicit emotion recognition for given statements.",
      "page": 25
    },
    {
      "caption": "Figure 18: Responses of different LLMs in implicit emotion recognition for given statements.",
      "page": 26
    },
    {
      "caption": "Figure 19: Responses of different LLMs in Intention Recognition for given statements.",
      "page": 27
    },
    {
      "caption": "Figure 20: Responses of different LLMs in Intention Recognition for given statements.",
      "page": 28
    },
    {
      "caption": "Figure 21: Responses of different LLMs in Intention Recognition for given statements.",
      "page": 29
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Scenario 1": "I saw a newly opened fast food\nrestaurant on my way out of\nthe hospital this morning.\nAre you sick?\nIs the food in fast food\nrestaurants delicious?",
          "Scenario 3": "I have been busy with work all day\ntoday.\nDo you feel a lot of pressure?\nYou work hard."
        },
        {
          "Scenario 1": "Scenario 2",
          "Scenario 3": "Scenario 4"
        },
        {
          "Scenario 1": "Igotpromoted,butitmeansIhaveto\nworkfurtherawayfromhome.\nAre you excited about promotion,\nor are you worried about being far\nfrom home?\nAre you excited about promotion?",
          "Scenario 3": "I have been busy with work all day\ntoday.\nCan I order you some takeout?\nYou work hard."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table 12: Performance of different LLMs in the task",
      "data": [
        {
          "92.5": "93.1",
          "92.4": "93.3",
          "89.8": "89.0",
          "81.1": "85.7",
          "80.5": "83.8",
          "79.8": "80.1",
          "78.7": "77.5",
          "76.4": "71.9",
          "74.5": "72.2",
          "71.2": "71.7",
          "66.0": "60.4",
          "28.7": "23.3"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Key Event Recognition": "Statement:\nAfter landing my dream job, the simple\nact of packing my lunch with excitement\nand anticipation has become a daily\nritual.\nClaude2:\nThat's wonderful that you landed your\ndream job! I can understand why you'd\nbe excited to pack your lunch each day.\nWhat is it about this new job that makes\nit your dream job? I'd love to hear more\nabout what you enjoy most about your\nnew role and responsibilities.",
          "Column_2": "Statement:\nThe day after a close friend\nmoved away, I received a\npackage addressed to me\nfrom him.\nClaude2:\nYour friend moving away\nmade you very sad. However,\nIt must have been a surprise\nto receive a package from\nyour friend so soon after they\nmoved away. What was inside\nthe package?",
          "Column_3": "Statement:\nI've taken two exams recently,\nand I feel like both of them had\nquite challenging questions.\nLLaMA2-70B:\nYou've been quite stressed\nrecently. Even though the\nexam questions were a bit\ntough, how do you think you\ndid on these exams?",
          "Intention Recognition": "Statement:\nMy phone keeps dying in the\nmiddle of the day.\nClaude2:\nToo bad...You can reduce\nscreen brightness or find\nexternal power sources."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "Statement\nResponse\nEvaluation"
        },
        {
          "Column_1": "Statement\nResponse\nEvaluation"
        },
        {
          "Column_1": "Statement\nResponse\nEvaluation"
        },
        {
          "Column_1": "Statement\nResponse\nEvaluation"
        }
      ],
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
      "authors": [
        "Wei-Lin Chiang",
        "Zhuohan Li",
        "Zi Lin",
        "Ying Sheng",
        "Zhanghao Wu",
        "Hao Zhang",
        "Lianmin Zheng",
        "Siyuan Zhuang",
        "Yonghao Zhuang",
        "Joseph Gonzalez"
      ],
      "year": "2023",
      "venue": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality"
    },
    {
      "citation_id": "2",
      "title": "A dictionary of psychology",
      "authors": [
        "Andrew M Colman"
      ],
      "year": "2015",
      "venue": "A dictionary of psychology"
    },
    {
      "citation_id": "3",
      "title": "Chatgpt outperforms humans in emotional awareness evaluations",
      "authors": [
        "Zohar Elyoseph",
        "Dorit Hadar-Shoval",
        "Kfir Asraf",
        "Maya Lvovsky"
      ],
      "year": "2023",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "4",
      "title": "Affect recognition in conversations using large language models",
      "authors": [
        "Shutong Feng",
        "Guangzhi Sun",
        "Nurul Lubis",
        "Chao Zhang",
        "Milica Gašić"
      ],
      "year": "2023",
      "venue": "Affect recognition in conversations using large language models"
    },
    {
      "citation_id": "5",
      "title": "Emotion-focused therapy",
      "authors": [
        "S Leslie",
        "Greenberg"
      ],
      "year": "2004",
      "venue": "Clinical Psychology & Psychotherapy: An International Journal of Theory & Practice"
    },
    {
      "citation_id": "6",
      "title": "Further notes on logic and conversation",
      "authors": [
        "Grice Paul"
      ],
      "year": "1978",
      "venue": "Pragmatics"
    },
    {
      "citation_id": "7",
      "title": "Language is not all you need: Aligning perception with language models",
      "authors": [
        "Shaohan Huang",
        "Li Dong",
        "Wenhui Wang",
        "Yaru Hao",
        "Saksham Singhal",
        "Shuming Ma",
        "Tengchao Lv",
        "Lei Cui",
        "Owais Khan Mohammed",
        "Barun Patra",
        "Qiang Liu",
        "Kriti Aggarwal",
        "Zewen Chi",
        "Johan Bjorck",
        "Vishrav Chaudhary",
        "Subhojit Som",
        "Xia Song",
        "Furu Wei"
      ],
      "year": "2023",
      "venue": "Language is not all you need: Aligning perception with language models"
    },
    {
      "citation_id": "8",
      "title": "Does gpt-3 generate empathetic dialogues? a novel in-context example selection method and automatic evaluation metric for empathetic dialogue generation",
      "authors": [
        "Young-Jun Lee",
        "Chae-Gyun Lim",
        "Ho-Jin Choi"
      ],
      "year": "2022",
      "venue": "Proceedings of the 29th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "9",
      "title": "Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework",
      "authors": [
        "Shanglin Lei",
        "Guanting Dong",
        "Xiaoping Wang",
        "Keheng Wang",
        "Sirui Wang"
      ],
      "year": "2023",
      "venue": "Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework"
    },
    {
      "citation_id": "10",
      "title": "Tat-Seng Chua, and Donghong Ji. 2023a. Diaasq : A benchmark of conversational aspect-based sentiment quadruple analysis",
      "authors": [
        "Bobo Li",
        "Hao Fei",
        "Fei Li",
        "Yuhan Wu",
        "Jinsong Zhang",
        "Shengqiong Wu",
        "Jingye Li",
        "Yijiang Liu",
        "Lizi Liao"
      ],
      "venue": "Tat-Seng Chua, and Donghong Ji. 2023a. Diaasq : A benchmark of conversational aspect-based sentiment quadruple analysis"
    },
    {
      "citation_id": "11",
      "title": "Fang Luo, Qiang Yang, and Xing Xie. 2023b. Large language models understand and can be enhanced by emotional stimuli",
      "authors": [
        "Cheng Li",
        "Jindong Wang",
        "Yixuan Zhang",
        "Kaijie Zhu",
        "Wenxin Hou",
        "Jianxun Lian"
      ],
      "venue": "Fang Luo, Qiang Yang, and Xing Xie. 2023b. Large language models understand and can be enhanced by emotional stimuli"
    },
    {
      "citation_id": "12",
      "title": "Shaoxiong Ji, and Erik Cambria. 2022a. Bieru: Bidirectional emotional recurrent unit for conversational sentiment analysis",
      "authors": [
        "Wei Li",
        "Wei Shao"
      ],
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "13",
      "title": "Dailydialog: A manually labelled multi-turn dialogue dataset",
      "authors": [
        "Yanran Li",
        "Hui Su",
        "Xiaoyu Shen",
        "Wenjie Li",
        "Ziqiang Cao",
        "Shuzi Niu"
      ],
      "year": "2017",
      "venue": "Dailydialog: A manually labelled multi-turn dialogue dataset",
      "arxiv": "arXiv:1710.03957"
    },
    {
      "citation_id": "14",
      "title": "Gs2p: a generative pre-trained learning to rank model with over-parameterization for web-scale search",
      "authors": [
        "Yuchen Li",
        "Haoyi Xiong",
        "Linghe Kong",
        "Jiang Bian",
        "Shuaiqiang Wang",
        "Guihai Chen",
        "Dawei Yin"
      ],
      "year": "2024",
      "venue": "Machine Learning"
    },
    {
      "citation_id": "15",
      "title": "2023c. Ltrgcn: Large-scale graph convolutional networks-based learning to rank for web search",
      "authors": [
        "Yuchen Li",
        "Haoyi Xiong",
        "Linghe Kong",
        "Shuaiqiang Wang",
        "Zeyi Sun",
        "Hongyang Chen",
        "Guihai Chen",
        "Dawei Yin"
      ],
      "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases"
    },
    {
      "citation_id": "16",
      "title": "Dejing Dou, and Guihai Chen. 2022b. Meta hierarchical reinforced learning to rank for recommendation: a comprehensive study in moocs",
      "authors": [
        "Yuchen Li",
        "Haoyi Xiong",
        "Linghe Kong",
        "Rui Zhang"
      ],
      "venue": "Joint European conference on machine learning and knowledge discovery in databases"
    },
    {
      "citation_id": "17",
      "title": "Mhrr: Moocs recommender service with meta hierarchical reinforced ranking",
      "authors": [
        "Yuchen Li",
        "Haoyi Xiong",
        "Linghe Kong",
        "Rui Zhang",
        "Fanqin Xu",
        "Guihai Chen",
        "Minglu Li"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Services Computing"
    },
    {
      "citation_id": "18",
      "title": "2023e. Coltr: Semi-supervised learning to rank with co-training and over-parameterization for web search",
      "authors": [
        "Yuchen Li",
        "Haoyi Xiong",
        "Qingzhong Wang",
        "Linghe Kong",
        "Hao Liu",
        "Haifang Li",
        "Jiang Bian",
        "Shuaiqiang Wang",
        "Guihai Chen",
        "Dejing Dou"
      ],
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "19",
      "title": "Emotional complexity. Handbook of emotions",
      "authors": [
        "Kristen Lindquist",
        "Lisa Feldman"
      ],
      "year": "2008",
      "venue": "Emotional complexity. Handbook of emotions"
    },
    {
      "citation_id": "20",
      "title": "The biases of pre-trained language models: An empirical study on prompt-based sentiment analysis and emotion detection",
      "authors": [
        "Rui Mao",
        "Qian Liu",
        "Kai He",
        "Wei Li",
        "Erik Cambria"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2022.3204972"
    },
    {
      "citation_id": "21",
      "title": "The intelligence of emotional intelligence",
      "authors": [
        "D John",
        "Peter Mayer",
        "Salovey"
      ],
      "year": "1993",
      "venue": "The intelligence of emotional intelligence"
    },
    {
      "citation_id": "22",
      "title": "Measuring emotional intelligence with the msceit v2. 0",
      "authors": [
        "Peter John D Mayer",
        "David Salovey",
        "Gill Caruso",
        "Sitarenios"
      ],
      "year": "2003",
      "venue": "Emotion"
    },
    {
      "citation_id": "23",
      "title": "Gpt-4 technical report",
      "authors": [
        "Openai"
      ],
      "year": "2023",
      "venue": "Gpt-4 technical report"
    },
    {
      "citation_id": "24",
      "title": "Navonil Majumder, Gautam Naik, Erik Cambria, and Rada Mihalcea",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "25",
      "title": "Harnessing the power of large language models for empathetic response generation: Empirical investigations and improvements",
      "authors": [
        "Yushan Qian",
        "Wei-Nan Zhang",
        "Ting Liu"
      ],
      "year": "2023",
      "venue": "Harnessing the power of large language models for empathetic response generation: Empirical investigations and improvements"
    },
    {
      "citation_id": "26",
      "title": "A survey on fairness of large language models in e-commerce: progress, application, and challenge",
      "authors": [
        "Qingyang Ren",
        "Zilin Jiang",
        "Jinghan Cao",
        "Sijia Li",
        "Chiqu Li",
        "Yiyang Liu",
        "Shuning Huo",
        "Tiange He"
      ],
      "year": "2024",
      "venue": "A survey on fairness of large language models in e-commerce: progress, application, and challenge",
      "arxiv": "arXiv:2405.13025"
    },
    {
      "citation_id": "27",
      "title": "Exploring chatgpt's empathic abilities",
      "authors": [
        "Kristina Schaaff",
        "Caroline Reinig",
        "Tim Schlippe"
      ],
      "year": "2023",
      "venue": "Exploring chatgpt's empathic abilities"
    },
    {
      "citation_id": "28",
      "title": "",
      "authors": [
        "Xiaofei Sun",
        "Xiaoya Li",
        "Shengyu Zhang",
        "Shuhe Wang",
        "Fei Wu",
        "Jiwei Li",
        "Tianwei Zhang",
        "Guoyin Wang"
      ],
      "year": "2023",
      "venue": ""
    },
    {
      "citation_id": "29",
      "title": "Effective lstms for target-dependent sentiment classification",
      "authors": [
        "Duyu Tang",
        "Bing Qin",
        "Xiaocheng Feng",
        "Ting Liu"
      ],
      "year": "2015",
      "venue": "Effective lstms for target-dependent sentiment classification",
      "arxiv": "arXiv:1512.01100"
    },
    {
      "citation_id": "30",
      "title": "Nevlp: Noise-robust framework for efficient vision-language pre-training",
      "authors": [
        "Yiyi Tao",
        "Zhuoyue Wang",
        "Hang Zhang",
        "Lun Wang"
      ],
      "year": "2024",
      "venue": "Nevlp: Noise-robust framework for efficient vision-language pre-training",
      "arxiv": "arXiv:2409.09582"
    },
    {
      "citation_id": "31",
      "title": "Chatplug: Open-domain generative dialogue system with internet-augmented instruction tuning for digital human",
      "authors": [
        "Junfeng Tian",
        "Hehong Chen",
        "Guohai Xu",
        "Ming Yan",
        "Xing Gao",
        "Jianhai Zhang",
        "Chenliang Li",
        "Jiayi Liu",
        "Wenshen Xu",
        "Haiyang Xu",
        "Qi Qian",
        "Wei Wang",
        "Qinghao Ye",
        "Jiejing Zhang",
        "Ji Zhang",
        "Fei Huang",
        "Jingren Zhou"
      ],
      "year": "2023",
      "venue": "Chatplug: Open-domain generative dialogue system with internet-augmented instruction tuning for digital human"
    },
    {
      "citation_id": "32",
      "title": "Igor Molybog",
      "authors": [
        "Hugo Touvron",
        "Louis Martin",
        "Kevin Stone",
        "Peter Albert",
        "Amjad Almahairi",
        "Yasmine Babaei",
        "Nikolay Bashlykov",
        "Soumya Batra",
        "Prajjwal Bhargava",
        "Shruti Bhosale",
        "Dan Bikel",
        "Lukas Blecher",
        "Cristian Canton Ferrer",
        "Moya Chen",
        "Guillem Cucurull",
        "David Esiobu",
        "Jude Fernandes",
        "Jeremy Fu",
        "Wenyin Fu",
        "Brian Fuller",
        "Cynthia Gao",
        "Vedanuj Goswami",
        "Naman Goyal",
        "Anthony Hartshorn",
        "Saghar Hosseini",
        "Rui Hou",
        "Hakan Inan",
        "Marcin Kardas",
        "Viktor Kerkez",
        "Madian Khabsa",
        "Isabel Kloumann",
        "Artem Korenev",
        "Punit Singh Koura",
        "Marie-Anne Lachaux",
        "Thibaut Lavril",
        "Jenya Lee",
        "Diana Liskovich",
        "Yinghai Lu",
        "Yuning Mao",
        "Xavier Martinet",
        "Todor Mihaylov",
        "Pushkar Mishra"
      ],
      "venue": "Igor Molybog"
    },
    {
      "citation_id": "33",
      "title": "Emotionally numb or empathetic? evaluating how llms feel using emotionbench",
      "authors": [
        "Jen Tse Huang",
        "Man Lam",
        "Eric Li",
        "Shujie Ren",
        "Wenxuan Wang",
        "Wenxiang Jiao",
        "Zhaopeng Tu",
        "Michael Lyu"
      ],
      "year": "2023",
      "venue": "Emotionally numb or empathetic? evaluating how llms feel using emotionbench"
    },
    {
      "citation_id": "34",
      "title": "Bias in emotion recognition with chatgpt",
      "authors": [
        "Naoki Wake",
        "Atsushi Kanehira",
        "Kazuhiro Sasabuchi",
        "Jun Takamatsu",
        "Katsushi Ikeuchi"
      ],
      "year": "2023",
      "venue": "Bias in emotion recognition with chatgpt"
    },
    {
      "citation_id": "35",
      "title": "",
      "authors": [
        "Bigscience Workshop",
        "Teven Le Scao",
        "Angela Fan",
        "Christopher Akiki",
        "Ellie Pavlick"
      ],
      "venue": ""
    }
  ]
}