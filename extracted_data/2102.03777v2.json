{
  "paper_id": "2102.03777v2",
  "title": "Eegfusenet: Hybrid Unsupervised Deep Feature Characterization And Fusion For High-Dimensional Eeg With An Application To Emotion Recognition",
  "published": "2021-02-07T11:09:16Z",
  "authors": [
    "Zhen Liang",
    "Rushuang Zhou",
    "Li Zhang",
    "Linling Li",
    "Gan Huang",
    "Zhiguo Zhang",
    "Shin Ishii"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "I. Introduction",
      "text": "E LECTROENCEPHALOGRAPHY (EEG) is a vital mea- surement of brain activity that could reflect the activities of neuron dynamics originated from the central nervous system and respond rapidly to different brain states  [1] . Recently, EEG based emotion recognition has become an increasingly important topic for human emotion understanding, regulation, and management  [2] . However, due to the microvolt-range amplitude of EEG, the collected EEG data is easily contaminated with noises (e.g., physiological artifacts or nonphysiological artifacts). In spite of the large number of studies working on EEG-based emotion decoding, how to effectively and efficiently extract valid and useful EEG features from the collected data is still a big challenging. For example, how to fuse the EEG signals collected at different brain locations and at different time points in an efficient approach remains unclear. In general, there are three types of information fusion in the processing of EEG signals.\n\n(1) Fusion of spatial information: based on the given time point(s), fuse the relationship dynamics (e.g. correlation) of the EEG signals at different brain regions. The commonly used methods include connectivity  [3] , microstate  [4] , or other topographic analysis  [5] ,  [6] , that interpret cortical region communication behavior by assessing the interaction functions between cortical areas and measuring the direction and strength of the interactions.\n\n(2) Fusion of temporal information: at the specific brain regions or EEG electrodes, truncate continuous time-series EEG signals into short data segments and fuse the EEG samples at different time points by calculating time-domain features (such as statistical patterns and shape information), frequency-domain features (such as power spectral features), or time-frequency features. Two well-known examples are short-time Fourier transform (STFT)  [7]  and wavelet analysis  [8] , both of which compute time-frequency dynamics of the data by performing successive calculations and measuring the data interaction along the time. This type of fusion applies to commonly used EEG features in the time-, frequency-, or time-frequency domain at one specific electrode. (3) Fusion of both spatial and temporal information: not only assess cortical region interaction but also estimate dynamic cortical involvement in a serial reaction time. Currently, a number of studies try to characterize both temporal and spatial information in a sequential approach, in which the temporal information is characterized from EEG data at each electrode in the first step and the spatial information is characterized in the second step by measuring the relationship between any two electrodes or among a set of electrodes in terms of the characterized temporal information at each EEG electrode in the first step. Although it is possible to estimate spatial features and temporal features, such an approach would be limited by the pre-defined sequential relationship between the spatial and temporal information and fail to effectively extract and fuse useful but latent information from a joint temporal-spatial domain. Thus, it is still a substantial challenge in current feature extraction methods for EEG signals considering the factors of validity and reliability, which needs to be tackled urgently.\n\nDeep learning in neural networks provides a good solution to characterize and fuse deep semantic features from the input data and has achieved tremendous success in solving EEG based emotion decoding problems  [9] -  [13] . For example, Jirayucharoensak et al.  [9]  introduced a deep learning network with a stack of three autoencoders and two softmax classifiers to perform EEG-based emotion classification, where an improvement of a three-level emotion classification (arousal: 46.03%; valence: 49.52%) was demonstrated under a comparison with two baseline methods (support vector machine (SVM) and naïve Bayes). Zheng and Lu  [10]  constructed deep belief networks (DBNs) to investigate the critical frequency bands and channels in EEG signals and select the optimal ones by considering the weight distribution learnt from the trained DBNs. Song et al.  [12]  presented a novel dynamic graph convolutional neural network (DGCNN) to solve a multichannel EEG emotion recognition problem, where the discriminant EEG features as well as the intrinsic relationship were learnt. This model manifested that a non-linear deep neural network (DNN) is a powerful tool in solving EEG signals which are highly non-linear in nature. Cimtay and Ekmekcioglu  [13]  adopted a pretrained state-of-the-art CNN, InceptionResnetV2, to extract useful and hidden features from the raw EEG signals. In these studies, the effectiveness of deep feature extraction and representation on EEG signals has been well demonstrated. However, all the above-mentioned studies were supervised learning based, where a great size of training samples with emotion labels was highly required, especially for deep networks. For example, a smaller size of training samples would make the deep network fail to generalize well due to the overfitting problem. Not only the network design but also the sample size would significantly affect the network performance. Unlike multimedia sources which can be easily obtained from social media platforms like YouTube, it is unrealistic to collect a huge number of EEG signals from different participants and manually annotate each sample with emotional labels in the real-world application scenarios. Also, there is a risk to induce \"label noise\" during the sample annotation process  [14] . Unsupervised learning would provide a more natural approach to decode EEG signals and is more aligned with human learning mechanism that requires useful information from the available samples without any associated teachers  [15] . How to appropriately characterize EEG signals is one of the most important part in an unsupervised EEG decoding model, which should be able to explore an optimal feature set and achieve a good unsupervised learning performance even in the absence of label guidance. An improper feature representation would lead to a wrong estimation of relationship structure among the samples. Recently, Liang et al.  [16]  introduced a novel hypergraphbased unsupervised EEG decoding model for human emotion recognition using traditional and shallow EEG features such as statistical features, Hjorth features, frequency bandpowers, energy and entropy properties. Unfortunately, traditional and shallow features mostly rely on heuristics, prior knowledge and experience, and the modelling performance would be limited. Also, traditional features may fail to efficiently elicit the complicated and non-linear patterns from the raw EEG data.\n\nThere is now a need for valid and reliable deep feature extraction method for time-series high-dimensional EEG signals (a number of electrodes placed along the scalp × sampling points at a high sampling rate) under an information fusion of spatial and temporal cortical dynamics, and meet this need in an unsupervised manner. Emerging progress in unsupervised based encoder-decoder networks (the basic ideas of a deep encoder-decoder architecture are introduced in Appendix A of Supplementary Materials 1  ) has offered a huge success in feature characterization and representation for images  [17] , videos  [18] , and audios  [19] . The encoder-decoder structures perform excellently in the aspects of highly non-linear feature extraction. Wen and Zhang  [20]  proposed a deep autoencoder based DNN to learn low-dimensional features from highdimensional EEG data in an unsupervised manner and adopted several commonly used supervised classifiers to demonstrate an improvement of the detection accuracy could be achieved. Similarly, considering the non-stationary and chaotic behavior of the high-dimensional EEG signals, Shoeibi et al.  [21]  developed a convolutional autoencoder for EEG feature learning and showed an accurate and reliable performance in a computeraided diagnosis system. Instead of directly using EEG raw signals, Tabar and Halici  [22]  converted high-dimensional EEG data to two-dimensional images by STFT and fed into a stack autoencoder network to solve a classification problem. In this study, we propose a novel unsupervised EEG feature extraction method (termed as EEGFuseNet below) and solve the emotion recognition problem using hypergraph theory. The proposed EEGFuseNet based hypergraph decoding framework includes two parts. (1) EEGFuseNet. An efficient hybrid deep encoder-decoder network architecture is proposed to characterize non-stationary time-series EEG signals. The jointinformation of spatial and temporal dynamics is fused in an effective manner and the useful but latent spatial-temporal dynamic information are characterized. The proposed hybrid network incorporates different sources of feature information through integrating CNN, recurrent neural network (RNN) and generative adversarial network (GAN) in a smart hybrid manner. Specifically, based on the features extracted by CNN from raw EEG signals, RNN is adopted to enhance the feature representation by exploring the potential feature relationships at temporal adjacencies. To improve the training performance, GAN is incorporated to improve the training process of the CNN-RNN network through dynamic updates in an unsupervised manner, which is potentially beneficial to highquality feature generation. The extracted deep features could represent the spatial relationship among the channels and the dependencies of the signals collected at adjacent time points.\n\n(2) Hypergraph decoding model. An effective hypergraph decoding model is developed to classify emotions based on the characterized deep features, where the complex relations of brain dynamics under various emotion statuses are measured and the EEG-based emotion classification problem is solved. Specifically, we measure the sample relationships in terms of the characterized deep features in the hypergraph construction, where the EEG samples that share similar properties are connected by hyperedges (the hyperedges are more flexible to describe group relationships). Following the hypergraph partitioning rule, the hypergraph Laplacian is then computed and optimized, where the connections among the hyperedges that share similar properties are grouped into one cluster while the connections among the hyperedges that share different properties are grouped into different clusters.\n\nWe evaluate the performance of the proposed EEGFuseNet based hypergraph decoding framework with an emotion recognition application on three well-known public databases and compare to the other state-of-the-art methods. The results show the generalizability of the proposed unsupervised framework is established and the individual difference is well solved in the cross-subject task. Regardless of the existing deep unsupervised EEG networks, to our knowledge, there is no example of studies where a solid and thorough exploration on hybrid deep configuration for converting high-dimensional EEG signals to low-dimensional valid and reliable feature representation in a fusion and unsupervised manner has been conducted. The proposed EEGFuseNet together with hypergraph decoding would be beneficial to brain decoding applications and offer a pure unsupervised framework for EEG feature extraction, fusion and classification for other use-cases. The major novelties of this work are as follows. (1) A hybrid unsupervised deep EEGFuseNet is proposed, which serves as a fundamental framework for high-dimensional EEG feature characterization and fusion. A valid and reliable deep EEG feature representation is formed to cover both spatial and temporal dynamics in brain activities, under a consideration of cortical region interactions and cortical involvement in a serial reaction time. (2) A unified unsupervised EEGFuseNet based hypergraph decoding framework is established, and its feasibility and effectiveness in solving brain decoding applications are demonstrated. (3) A cross-individual task of EEG-based emotion recognition is employed to validate the generality of the proposed unified unsupervised framework on three famous affective databases for the individual difference problem which is common in brain studies. On all three databases, the proposed method outperforms the existing unsupervised methods and achieves a comparable performance comparing to the existing supervised methods without transfer learning strategy.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Methodology",
      "text": "In this section, we introduce the proposed EEGFuseNet with the corresponding design and configuration and explain how to efficiently characterize non-stationary high-dimensional EEG signals in an unsupervised manner. An overall EEG preprocessing is first conducted on the collected raw EEG signals to remove noises such as physiological artifacts (e.g. ocular activity and muscle activity) and non-physiological artifacts (e.g. AC electrical and electromagnetic inferences). A full explanation of EEG preprocessing steps is provided in Appendix B of Supplementary Materials. After preprocessing, the EEG data at each trial is further partitioned into a number of segments with a fixed length. A segment-based EEG data is denoted as X ∈ R C×T , representing the signals collected from the electrode channels (C) at a period of time points (T ). Next, X is treated as the input to the proposed hybrid EEGFuseNet and the corresponding deep features are characterized and fused based on unsupervised learning. The proposed EEGFuseNet mitigates the limitations of the existing state-of-the-art feature extraction and fusion methods and provides a number of practical benefits, for example, easy modification and simple training, for EEG signals collected under different environment variables in various applications. Next, the proposed hybrid deep encoder-decoder network architecture will be illustrated in details. More precisely, we will introduce (1) how to construct the basic architecture of the proposed EEGFuseNet from the classical CNN, (2) how to incorporate GAN into the CNN-based network to generate high-quality features, (3) how to incorporate RNN into the CNN-GAN based network to better fuse both temporal and spatial information and develop the final architecture of EEGFuseNet.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "A. Cnn Based",
      "text": "In a typical CNN based deep encoder-decoder network, the encoder consists of convolution layers for extracting useful information from the given input (X) and converting it into a single dimensional vector (hidden vector), and the decoder consists of deconvolutional layers for upscaling the encoder feature maps and transferring the hidden vector to the generated output (Y )  [23] . Through maximizing the similarity between X and Y (in other words, minimizing the loss function given as\n\n2 ), the autoencoder structure is jointly trained and the learnt hidden vector is considered as an informative latent feature representation of X and used for further data analysis and modelling. Noteworthy, the autoencoder architecture is a self-learning paradigm, which does not require any labeling information during training process and is significantly easier to train comparing to the other common feature extraction architectures  [24] ,  [25] . Thus, it would be suitable to solve the small size problem of EEG data with label missing.\n\nFor time-series EEG signals, both spatial and temporal information are important which represents the relationships of the brain activities at different brain locations and the changing dynamics of brain patterns along the time. Inspired from EEGNet structure  [26] , a CNN based deep encoder-decoder network is developed as shown in Fig.  1 . A sequential twodimensional convolutional layers are implemented to generate feature maps covering EEG spatial information at different frequency bands, where the filter length is the half of the sampling rate of input data. Batch normalization (BN) is adopted to normalize each training mini-batch and speed up network training process by reducing internal covariate shift. The activation function, exponential linear units (ELU), is added in convolutional and deconvolutional layers for model fitting improvement. Notably, as the input EEG signals consist of channels and time points (X ∈ R C×T ), twodimensional convolution functions are adopted here, instead of one-dimensional convolution function. In the architecture, the encoder performs convolution and down-sampling, while the decoder performs deconvolution and up-sampling to reconstruct the input EEG signals. The main possible benefits of the multiply convolution layers include: (1) compact, comprehensive and complete EEG pattern characterization from different dimensions; (2) relationship explorations within and between the extracted feature maps and feature fusion in an optimal approach; (3) less parameters to fit with the implementation of subsampling layers. Thus, the design of the convolution layers could be capable of providing an efficient way to learn spatial-temporal dynamics from time-series EEG signals collected at different brain locations and integrate the sample points to a compact and deep feature representation vector which has been demonstrated to be useful for accurate and efficient data description  [27] ,  [28] . This network could offer a baseline for unsupervised deep feature characterization and fusion. Specifically, the encoder network consists of 4 convolution layers. The weights in the training process are initialized randomly. In the design of an encoder-decoder architecture, each encoder layer would have a corresponding decoder layer. Thus, there are also 4 deconvolution layers in decoder part. The final decoder output is to reconstruct the input EEG signals with minimized difference. In the model training process, we use the mean squared error (MSE) as the objective function to measure the difference between the input EEG signals X ∈ R C×T and the reconstructed EEG signals Y ∈ R C×T from the estimated deep features by the network, given as loss = X -Y 2 2 . A perfect model would have a loss of 0. The specific architecture details are presented in Appendix C of Supplementary Materials (Table  S1 ).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Hybrid Cnn-Gan Based",
      "text": "A traditional encoder-decoder network is easy to train, but the generated features would be with low quality  [29] . Many researches have proven GAN could be capable of generating features with high quality from sequential data  [30] -  [32] . A general encoder-decoder pipeline with GAN includes a generator (encoder-decoder network) and a discriminator, where the generator is response to reconstruct EEG signals from the extracted deep features and the discriminator is to distinguish whether the input EEG signals is a fake one generated by the generator or a real one collected from human brain. To further learn the complex structures of the non-stationary time-series EEG data, a hybrid encoder-decoder architecture incorporating  CNN and GAN is developed. On the basis of the CNN-based encoder-decoder network presented in Section II-A, we further develop a hybrid CNN-GAN based deep encoder-decoder network. In the construction of the hybrid CNN-GAN based network, the generator is CNN-based encoder-decoder network as shown in Fig.  1 . The discriminator architecture is the same as the discriminator used in the final proposed EEGFuseNet (Fig.  2 ). More specific configurations about the designed discriminator are reported in Appendix C of Supplementary Materials (Table  S2 ).\n\nIn the training process, the generator G characterizes the latent feature representation (o) of the sequential EEG signals X and the discriminator D(X, G(X)) ∈ [0, 1] measures the probability that the input (real training sample X or synthesized fake sample G(X) produced by the generator) is real or fake. The objective function in the training process is to build a good D that is capable of discriminating the real sample from the generated fake samples and at the same time develop a good G that can produce a fake sample which is as similar as possible to the real ones (two-player minimax game). In the training process, the discriminator inputs are pairs of X and G(X). The objective function is given as\n\n(1) where the first part log D(X) is the discriminator output for real sample X and the second part D(G(X)) is the discriminator output for the generated fake sample based on the estimated o. Together with the objective function of G,\n\nthe overall objective function of the hybrid CNN-GAN network is given as",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "C. Hybrid Cnn-Rnn-Gan Based (Eegfusenet)",
      "text": "According to the nature of EEG signals, there should possess a hierarchical structure with complex dependencies between the extracted features at different time points. The extracted feature at each time point should not be considered as an independent and isolated point. In the existing works, encoder-decoder models based on RNNs, LSTM and gated recurrent neural networks (GRUs) have recently demonstrated impressive feature characterization performance on sequential data  [33] -  [35] . To enhance the feature representation of timeseries EEG signals, we extend the CNN-GAN based network to a hybrid architecture and extract EEG features by exploiting the advantages of both recurrent and convolutional networks. As shown in Fig.  2 , the encoder consists of convolutional layers to extract features from EEG signals at every time point (shallow feature extraction) and the recurrent layers encode the extracted features at every time point to an entire feature representation of the whole input EEG signals (deep feature extraction). The decoder consists of recurrent layers to predict the features at each time point from the output of encoder and deconvolutional layers to reconstruct the features to the original EEG signals. During the EEG processing, the informative features covering both spatial and temporal dynamics are characterized in an effective fusion approach from a joint temporal-spatial domain.\n\nIn the hybrid CNN-RNN-GAN based network, the convolution and deconvolution layers in the shallow feature extraction are the same as the designed CNN based network (Fig.  1 ). Based on the generated feature maps (the rows and columns refer to the features from channels and time points) in the shallow feature extraction part, the sequential features are characterized in the deep feature extraction part. In the recurrent layers (RNN network), the basic building modules for learning spatial dependencies between neighbors are the LSTM units. Due to the sophisticated training of LSTM, a GRU was proposed  [36] , which is similar to LSTM that modulates the flow of intimation inside the gating unit without separate memory cell. It has been evident that GRU has shown comparable performance as LSTM on machine learning tasks, with less parameters required  [37] . To achieve a higher computation efficiency, we employ a bidirectional GRU in the implementation which is defined as\n\nwhere z) , U (r) , and U (h) are weight matrices and b (z) , b (r) , b (h) are biases, which are learnt in the training process. z t , r t and ĥt are update gate vector, reset gate vector and hidden state vector, respectively. σ and φ are sigmoid and tangent function. is an element-wise multiplication. In the implementation, the forward and backward recurrent layers iteratively work on the time point based feature vectors in a sequence and compute the corresponding forward and backward sequences of hidden state vectors. Specifically, the data {x 1 , x 2 , . . . , x T } is input to the bidirectional GRU in a forward and backward sequences, respectively. Here, the hidden layer of the forward and backward GRU are denoted as\n\nThe outputs of forward and backward GRU at the time point t are given as\n\nThe output of the bidirection GRU at t point is given as a t = h f t ⊕ h b t , where ⊕ indicates vector concatenation. Finally, the generated deep feature representation vector (o) is denoted as o = (a 1 , . . . , a t , . . . , a T ) and the sequential feature information is captured. For the purpose of EEG feature characterization and fusion, the input EEG signals are first characterized as a sequence of feature vectors at each time point t after the convolutional layers (considered as spatial dynamic characterization) and then sequential features are learnt by recurrent layers to synthesize the past and future dynamic information of time-series EEG signals (considered as temporal dynamic characterization). Thus, the extracted o, which is treated as the deep EEG features to be used in the following unsupervised EEG decoding, can represent the entire input EEG signals cross timepoints covering not only the EEG characteristics but also the EEG characteristics in the sequential information. To improve the implementation efficiency, we update the input of GRU from batch to batch in the training process, where one batch includes a continuous EEG signals at a certain time gap. The specific configurations about the generator in the hybrid CNN-RNN-GAN based encoder-decoder network are presented in Appendix C of Supplementary Materials (Table  S3 ). This architecture successfully fuses the extracted feature representations at different deep levels, at different brain locations, and at different time points, which would be beneficial to the representation of spatial and temporal dynamics in the non-stationary timeseries EEG signals.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Unsupervised Based Hypergraph Decoding Model",
      "text": "To solve a pure unsupervised learning based cross-subject EEG-based emotion decoding problem, we introduce the hypergraph theory  [38]  to realize emotion classification. The EEG samples are treated as vertices and a hypergraph is constructed based on the relationship among these vertices in terms of EEG characteristics. Here, the similarity among the EEG samples is calculated based on the characterized features by EEGFuseNet and the hyperedges are formed to connect a number of EEG samples based on the calculated similarity distributions. Different from a simple graph, a hypergraph is capable of connecting a couple of vertices (more than two) that share similar properties, presenting more general types of relations, and revealing more complex hidden structures than single connections. The emotion classification is realized by partitioning the constructed hypergraph into a specific number of classes, through computing the hypergraph Laplacian and solving it with an optimal eigenspace. The constructed hypergraph is then divided into a number of classes and each class indicates one emotion status. For more details, please refer to Appendix D of Supplementary Materials.  Here, {p 1 , p 2 , ..., p T } and {q 1 , q 2 , ..., q T } are the CNN and reconstructed CNN features extracted from all the channels at each single time point.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Real",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Iii. Experimental Results",
      "text": "The simultaneously recorded EEG signals are used to recognize the corresponding emotion statuses, which has been proved to be effective in tackling with the great deal of complexity and variability in emotions. In this section, we fully evaluate the ability of the proposed EEGFuseNet based hypergraph decoding framework on the application of emotion recognition.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Benchmarking",
      "text": "We conduct extensive experiments on three EEG databases, including DEAP  [39] , MAHNOB-HCI  [40] , and SEED  [10] , all of which are commonly used for EEG-based emotion recognition evaluation. In all three databases, the emotions are defined by the dimensional emotion model, i.e. valence, arousal, dominance, liking and predictability. The validity and reliability of the proposed unsupervised framework are fully evaluated, and the performance of unsupervised emotion recognition on different emotion dimensions is carefully quantified and compared with the literature.\n\nThe DEAP database was composed of 32 subjects' EEG emotion data. For each subject, different specific and strong emotions were evoked by 40 selected music videos, each having a duration of 60s, and the corresponding subjective feedbacks on different emotion dimensions (valence, arousal, dominance, and liking) were given for each music video. EEG signals were recorded at a sampling rate of 512Hz from 32 active AgCl electrode sites according to the international 10-20 system placement. To cross-compare with the other studies, we use a fixed threshold of 5 for each emotion dimension to discretize the subjective feedbacks into two classes (low and high).\n\nThe MAHNOB-HCI database included a total of 30 subjects, whose EEG data were recorded using Biosemi active II system with 32 Ag/AgCl electrodes at a sampling rate of 256Hz. Twenty film clips were selected to evoke emotions and the subjective feedback was given using a score in the range of 1 to 9. In the model evaluation, a fixed threshold of 5 is used to discretize the subjective feedback into binaries for each emotion dimension (valence, arousal, dominance, and predictability).\n\nThe SEED database included 15 subjects' 62-channel EEG data collected when they were viewing 15 film clips with an average duration around 4 min. The data sampling rate was downsampled to 200Hz. Three emotions were elicited, including negative, neural and positive.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "B. Experiment Protocols",
      "text": "To avoid information leaking in the evaluation process, we conduct a leave-one-subject-out cross-validation (LOOCV) subject-independent evaluation protocol on the three databases, where the training and test data are from different subjects and no information overlap exists. Take the DEAP database as an example. We use 31 subjects' data for training and the remaining 1 subject's data for testing and repeat the validation process until each subject is treated as the test data for once. In other words, we repeat 32 times and calculate the final crossvalidation performance as an average of all the obtained testing results. This validation method provides a fair evaluation of the cross-subject model performance that could accurately estimate the possible recognition accuracies of the newly coming data from new subjects. For comparison with other studies, the decoding performance is evaluated using recognition accuracy P acc and F1-Score P f . Considering the emotion dimensions in the dimensional emotion model are independent to each other, the leave-one-subject-out cross-validation subject-independent evaluation process is separately conducted on each single emotion dimension and the corresponding results are analyzed.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Network Training",
      "text": "Not like the media data used in deep learning studies, EEG database is relatively small. The challenge of network training is to well train the network to extract sufficient EEG features and avoid the over-fitting problem. To increase the sample size, each trial is further segmented into a number of segments with a fixed length of 1s. Take DEAP database as an example. Each trial length is 60s, so the number of segments of one trial is equal to 60. Thus, the total sample size is increased from 1280 (32 subjects × 40 videos) to 76800 (32 subjects × 40 trials × 60 segments). In the training process, to avoid information leaking, the segments from one video would all be considered as training data or test data. The weight parameters in convolution layers are initialized with the uniform distribution based on Glorot initialization  [41] . We run 100 training epochs and perform validation stopping. The model weights that generated the lowest validation set loss are saved as the final parameters. The Adam optimizer is with a momentum of 0.9. The mini-batch stochastic gradient descent (SGD) method with a fixed learning rate of 0.001 for generator and of 0.0002 for discriminator is used. Here, the mini-batch size is equal to 128. All the models are trained on an NVIDIA GeForce RTX 2080 GPU, with CUDA 10.0 using the Pytorch API.\n\nOn the other hand, due to the computation complexity of the hypergraph construction process, it is very time-consuming to measure the similarity relationships among all the available samples. To improve the computation efficiency in model implementation, we introduce a learning strategy for tackling the computation complexity issue. Specifically, in one round of cross-validation for DEAP database, all the samples from 1 subject are treated as test data (in total 1 subject × 40 trials × 60 segments=2400 samples) and the samples from the other 31 subjects are used as training data candidates. Then, η% samples are randomly selected from the training data candidates (31 subjects × 40 trials × 60 segments=74400 samples) and are then used to construct a hypergraph with the test data (2400 samples). Through this approach, the computation efficiency is largely improved. More details about the implementation process are presented in Appendix E of Supplementary Materials. Under a consideration of computation efficiency and performance stability, in the implementation the feature size and hyperedge size κ in hypergraph construction and partitioning are set to 64 and 5, while the selected rate η in model learning is given to 10.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "D. Evaluation On Deap Database",
      "text": "It is well known that, even evaluating on the same database, different evaluation protocols would lead to a great difference in the results. Generally speaking, the validation methods affect the obtained performance as: (1) supervised vs. unsupervised: supervised methods would have a better result than unsupervised methods, as label information is used for model training in the supervised methods; (2) subject-dependent vs. subject-independent: subject-dependent evaluation methods would have a better performance than subject-independent evaluation methods, as individual difference is not considered",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Performance Comparison With Different Decoding Models",
      "text": "We evaluate the robustness of the proposed unsupervised hypergraph decoding model by comparing to the state-of-the-art decoding methods. For example, simple graph based method, principal component analysis (PCA) and K-means clustering method (PCA+Kmeans), K-nearest neighbors (KNN) algorithm, robust continuous clustering method (RCC)  [63] , and directed graph based agglomerative algorithm (AGDL)  [64] . PCA+Kmeans and KNN are two baseline methods. Simple graph works with pair-wise relationship measurement. RCC is a clustering algorithm by optimizing a continuous objective based on robust estimation. AGDL is an agglomerative clustering method based on a direct graph, where the product of average indegree and average outdegree was measured to guarantee the stability of cluster results. Another recently popular unsupervised method presented by Yang et al.  [65]  was not included in this comparison, because Yang et al. 's work was an image-based end-to-end learning framework to learn effective features and implement clusters jointly, which is hard to separate the clustering part from the framework and directly extend to EEG tasks. To make the results comparable, the used EEG features are extracted from the proposed EEGFuseNet with same parameter settings. The performance comparisons are conducted on all three databases, and the corresponding results are reported in Table VII, Table  VIII , and Table  IX . Through comparing the emotion recognition performance on different emotion dimensions and different databases, the results show that our proposed unsupervised hypergraph decoding model achieves the most robust results across different subjects, different trials and different experimental environments. Comparing to pair-wise relationship measurement in simple graph, hypergraph construction and partitioning could be more beneficial to describe the complex hidden relationships of EEG data in decoding problems. For the other decoding methods, the recognition results are similar to simple graph's. These results verify a simple unsupervised method is not suitable for solving the complex and difficult decoding problems using high-dimensional EEG signals.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "C. Ablation Study",
      "text": "The effectiveness of each component in our proposed EEG-FuseNet is fully validated in an ablation study based on three different databases. EEGFuseNet is built upon CNN-based encoder-decoder with two additional modules: GAN for model performance enhancement and RNN for temporal feature dynamic measurement. In the ablation study, we compare our proposed EEGFuseNet with three variant models:    the single model (CNN based). These results show hybrid networks are more flexible and stable to handle the data diversity issue and are more beneficial to high-quality EEG feature characterization and information fusion across spatial and temporal dynamics.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "D. Hyperparameter Effect",
      "text": "The effect of the hyperparameters in the proposed EEG-FuseNet based hypergraph decoding framework on the emotion recognition performance is also validated. For EEG-FuseNet, we test input size effect on model performance. Take DEAP database as an example. We adjust the input data size of EEGFuseNet from 32×128 to 32×512 and summarize the corresponding emotion recognition performance in Fig.  3 . According to the network design, the kernel size in the first convolution layer and the depthwise separable convolution layer would be adaptively adjusted according to the input data size. The results show a relative smaller input size of 32×384 perform the best, which is evident to cover almost of the important information in the collected data and improve the computation efficiency as well. If the input data size is further reduced, a loss of information would lead to a significant decrease in the recognition performance. For the original data size (32×512), the corresponding kernel size and the number of parameters in the network greatly increase compared to the other input data sizes, which would also lead to a higher chance of overfitting especially when the sample size is not large enough. Furthermore, we verify the corresponding computational time under different input sizes and report the results in Table XIII. Here, the computational time is separately measured under EEGFuseNet training, EEGFuseNet testing, and hypergraph decoding. The overall computational time is also given. It is found that EEGFuseNet training time is not much affected by the input size, but the computational time of EEGFuseNet testing and hypergraph decoding increase along with an increase of the input size. Note that after model learning, the computational time reduces to seconds level (11.36s ∼ 44.51s) which would be acceptable in real applications. On the other hand, for hypergraph decoding model, the effect of hyperedge size (κ) on emotion recognition performance is also examined. We adjust κ value from 5 to 35 with a step of 5 and present the corresponding emotion recognition performance in Fig.  4 (a) . The results show the performance is relative stable and less sensitive to the change of κ value.\n\nAs present in Section III-C, we introduce a speed up theorem with a hyperparameter η% to reduce the computation complexity of hypergraph decoding. To evaluate the effect of η on the model learning performance, we adjust the value to 1, 2, 3, 4, 5, 10, and 15, where the corresponding training data size is 744, 1488, 2232, 2976, 3720, 7440 and 11160 samples (the total training data candidates are 74400 samples). The emotion recognition performance under different η values are shown in Fig.  4 (b) . It reveals that an increase of η value could generally lead to a greater emotion recognition accuracy. For the case of η = 2 achieving better performance than η = 3, it could be the randomly selected training data of η = 2 probably share similar patterns to the test data and less individual difference are involved. The average computational time under different η values is shown in Fig.  5 . An increase of η value leads to an exponential growth in the computational time, where the cost time of hypergraph decoding is 1496s for η = 1 and 62466s    for η = 15. For the η value of 10, the computational time is 11718s. There is a trade-off between decoding performance and computational time.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "E. Conclusion",
      "text": "The aim of this paper is to present a theoretical and practical method for valid and reliable feature characterization and fusion of high-dimensional EEG signals in an TABLE XIII: The computational time (in seconds) with an adjusted input size using leave-one-subject-out cross-validation subject-independent protocol on DEAP database (η = 10).",
      "page_start": 11,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: A sequential two-",
      "page": 4
    },
    {
      "caption": "Figure 1: An illustration of the CNN based encoder-decoder",
      "page": 4
    },
    {
      "caption": "Figure 1: The discriminator architecture is the same",
      "page": 4
    },
    {
      "caption": "Figure 2: ). More speciﬁc conﬁgurations about the designed",
      "page": 4
    },
    {
      "caption": "Figure 2: , the encoder consists of convolutional",
      "page": 5
    },
    {
      "caption": "Figure 1: ). Based on the generated feature maps (the rows and",
      "page": 5
    },
    {
      "caption": "Figure 2: The architecture design of the proposed EEGFuseNet. Here, {p1, p2, ..., pT } and {q1, q2, ..., qT } are the CNN and",
      "page": 6
    },
    {
      "caption": "Figure 3: According to the network design, the kernel size in the",
      "page": 10
    },
    {
      "caption": "Figure 4: (a). The results show the",
      "page": 10
    },
    {
      "caption": "Figure 4: (b). It reveals that an increase of η value could generally",
      "page": 10
    },
    {
      "caption": "Figure 5: An increase of η value leads to an",
      "page": 10
    },
    {
      "caption": "Figure 3: A comparison of emotion recognition performance with",
      "page": 11
    },
    {
      "caption": "Figure 4: A comparison of emotion recognition performance",
      "page": 11
    },
    {
      "caption": "Figure 5: The corresponding computational time (in seconds)",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "Encoder",
          "Column_2": "FFCCEELLUU FFCC pp11pp22\n...... FFCCEELLUU ...... GRU ...... FFCC\n...... ...... ppTT\nFeature Map\nFFCCEELLUU FFCC",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "xtracted\nDeep\nFeatures\n.........\n.........": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "xtracted\nDeep\nFeatures\n.........\n.........": ".........\n........."
        },
        {
          "Column_1": "Decoder",
          "Column_2": "EELLUUFFCC FFCC qq11qq22\n......\nEELLUUFFCC ...... GRU FFCC ......\n...... ......\nqqTT\nFeature Map\nEELLUUFFCC FFCC",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "xtracted\nDeep\nFeatures\n.........\n.........": ""
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotions recognition using eeg signals: A survey",
      "authors": [
        "S Alarcao",
        "M Fonseca"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "2",
      "title": "Videotriggered eeg-emotion public databases and current methods: A survey",
      "authors": [
        "W Hu",
        "G Huang",
        "L Li",
        "L Zhang",
        "Z Zhang",
        "Z Liang"
      ],
      "year": "2020",
      "venue": "Brain Science Advances"
    },
    {
      "citation_id": "3",
      "title": "A critical assessment of connectivity measures for eeg data: a simulation study",
      "authors": [
        "S Haufe",
        "V Nikulin",
        "K.-R Müller",
        "G Nolte"
      ],
      "year": "2013",
      "venue": "Neuroimage"
    },
    {
      "citation_id": "4",
      "title": "The functional significance of eeg microstates-associations with modalities of thinking",
      "authors": [
        "P Milz",
        "P Faber",
        "D Lehmann",
        "T Koenig",
        "K Kochi",
        "R Pascual-Marqui"
      ],
      "year": "2016",
      "venue": "Neuroimage"
    },
    {
      "citation_id": "5",
      "title": "Scalp and source power topography in sleepwalking and sleep terrors: a high-density eeg study",
      "authors": [
        "A Castelnovo",
        "B Riedner",
        "R Smith",
        "G Tononi",
        "M Boly",
        "R Benca"
      ],
      "year": "2016",
      "venue": "Sleep"
    },
    {
      "citation_id": "6",
      "title": "Eeg based topography analysis in string recognition task",
      "authors": [
        "X Ma",
        "X Huang",
        "Y Shen",
        "Z Qin",
        "Y Ge",
        "Y Chen",
        "X Ning"
      ],
      "year": "2017",
      "venue": "Physica A: Statistical Mechanics and its Applications"
    },
    {
      "citation_id": "7",
      "title": "Feature extraction from eeg spectrograms for epileptic seizure detection",
      "authors": [
        "R Ramos-Aguilar",
        "J Olvera-López",
        "I Olmos-Pineda",
        "S Sánchez-Urrieta"
      ],
      "year": "2020",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "8",
      "title": "Wavelet analysis based classification of emotion from eeg signal",
      "authors": [
        "M Islam",
        "M Ahmad"
      ],
      "year": "2019",
      "venue": "2019 International Conference on Electrical, Computer and Communication Engineering (ECCE)"
    },
    {
      "citation_id": "9",
      "title": "Eeg-based emotion recognition using deep learning network with principal component based covariate shift adaptation",
      "authors": [
        "S Jirayucharoensak",
        "S Pan-Ngum",
        "P Israsena"
      ],
      "year": "2014",
      "venue": "The Scientific World Journal"
    },
    {
      "citation_id": "10",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    },
    {
      "citation_id": "11",
      "title": "Deep learning with convolutional neural networks for eeg decoding and visualization",
      "authors": [
        "R Schirrmeister",
        "J Springenberg",
        "L Fiederer",
        "M Glasstetter",
        "K Eggensperger",
        "M Tangermann",
        "F Hutter",
        "W Burgard",
        "T Ball"
      ],
      "year": "2017",
      "venue": "Human brain mapping"
    },
    {
      "citation_id": "12",
      "title": "Eeg emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "Investigating the use of pretrained convolutional neural network on cross-subject and cross-dataset eeg emotion recognition",
      "authors": [
        "Y Cimtay",
        "E Ekmekcioglu"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "14",
      "title": "Unsupervised learning of long-term motion dynamics for videos",
      "authors": [
        "Z Luo",
        "B Peng",
        "D.-A Huang",
        "A Alahi",
        "L Fei-Fei"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "15",
      "title": "Unsupervised learning",
      "authors": [
        "H Barlow"
      ],
      "year": "1989",
      "venue": "Neural computation"
    },
    {
      "citation_id": "16",
      "title": "An unsupervised eeg decoding system for human emotion recognition",
      "authors": [
        "Z Liang",
        "S Oba",
        "S Ishii"
      ],
      "year": "2019",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "17",
      "title": "Unsupervised spectral-spatial feature learning with stacked sparse autoencoder for hyperspectral imagery classification",
      "authors": [
        "C Tao",
        "H Pan",
        "Y Li",
        "Z Zou"
      ],
      "year": "2015",
      "venue": "IEEE Geoscience and remote sensing letters"
    },
    {
      "citation_id": "18",
      "title": "An overview of deep learning based methods for unsupervised and semi-supervised anomaly detection in videos",
      "authors": [
        "B Kiran",
        "D Thomas",
        "R Parakkal"
      ],
      "year": "2018",
      "venue": "Journal of Imaging"
    },
    {
      "citation_id": "19",
      "title": "Autoencoder-based unsupervised domain adaptation for speech emotion recognition",
      "authors": [
        "J Deng",
        "Z Zhang",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2014",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "20",
      "title": "Deep convolution neural network and autoencoders-based unsupervised feature learning of eeg signals",
      "authors": [
        "T Wen",
        "Z Zhang"
      ],
      "year": "2018",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "21",
      "title": "A comprehensive comparison of handcrafted features and convolutional autoencoders for epileptic seizures detection in eeg signals",
      "authors": [
        "A Shoeibi",
        "N Ghassemi",
        "R Alizadehsani",
        "M Rouhani",
        "H Hosseini-Nejad",
        "A Khosravi",
        "M Panahiazar",
        "S Nahavandi"
      ],
      "year": "2021",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "22",
      "title": "A novel deep learning approach for classification of eeg motor imagery signals",
      "authors": [
        "Y Tabar",
        "U Halici"
      ],
      "year": "2016",
      "venue": "Journal of neural engineering"
    },
    {
      "citation_id": "23",
      "title": "Understanding geometry of encoder-decoder cnns",
      "authors": [
        "J Ye",
        "W Sung"
      ],
      "year": "2019",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "24",
      "title": "Deep convolutional neural networks for mental load classification based on eeg data",
      "authors": [
        "Z Jiao",
        "X Gao",
        "Y Wang",
        "J Li",
        "H Xu"
      ],
      "year": "2018",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "25",
      "title": "A deep learning framework for identifying children with adhd using an eeg-based brain network",
      "authors": [
        "H Chen",
        "Y Song",
        "X Li"
      ],
      "year": "2019",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "26",
      "title": "Eegnet: a compact convolutional neural network for eeg-based brain-computer interfaces",
      "authors": [
        "V Lawhern",
        "A Solon",
        "N Waytowich",
        "S Gordon",
        "C Hung",
        "B Lance"
      ],
      "year": "2018",
      "venue": "Journal of neural engineering"
    },
    {
      "citation_id": "27",
      "title": "Contractive auto-encoders: Explicit invariance during feature extraction",
      "authors": [
        "S Rifai",
        "P Vincent",
        "X Muller",
        "X Glorot",
        "Y Bengio"
      ],
      "year": "2011",
      "venue": "Contractive auto-encoders: Explicit invariance during feature extraction"
    },
    {
      "citation_id": "28",
      "title": "Extracting deep bottleneck features using stacked auto-encoders",
      "authors": [
        "J Gehring",
        "Y Miao",
        "F Metze",
        "A Waibel"
      ],
      "year": "2013",
      "venue": "2013 IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "29",
      "title": "Semi-recurrent cnn-based vae-gan for sequential data generation",
      "authors": [
        "M Akbari",
        "J Liang"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "30",
      "title": "Adversarial autoencoders",
      "authors": [
        "A Makhzani",
        "J Shlens",
        "N Jaitly",
        "I Goodfellow",
        "B Frey"
      ],
      "year": "2015",
      "venue": "Adversarial autoencoders",
      "arxiv": "arXiv:1511.05644"
    },
    {
      "citation_id": "31",
      "title": "Unsupervised detection of lesions in brain mri using constrained adversarial auto-encoders",
      "authors": [
        "X Chen",
        "E Konukoglu"
      ],
      "year": "2018",
      "venue": "Unsupervised detection of lesions in brain mri using constrained adversarial auto-encoders",
      "arxiv": "arXiv:1806.04972"
    },
    {
      "citation_id": "32",
      "title": "Adversarial auto-encoders for speech based emotion recognition",
      "authors": [
        "S Sahu",
        "R Gupta",
        "G Sivaraman",
        "W Abdalmageed",
        "C Espy-Wilson"
      ],
      "year": "2018",
      "venue": "Adversarial auto-encoders for speech based emotion recognition",
      "arxiv": "arXiv:1806.02146"
    },
    {
      "citation_id": "33",
      "title": "A hierarchical latent variable encoder-decoder model for generating dialogues",
      "authors": [
        "I Serban",
        "A Sordoni",
        "R Lowe",
        "L Charlin",
        "J Pineau",
        "A Courville",
        "Y Bengio"
      ],
      "year": "2017",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "34",
      "title": "Tweet2vec: Learning tweet embeddings using character-level cnn-lstm encoder-decoder",
      "authors": [
        "S Vosoughi",
        "P Vijayaraghavan",
        "D Roy"
      ],
      "year": "2016",
      "venue": "Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval"
    },
    {
      "citation_id": "35",
      "title": "Deep recurrent generative decoder for abstractive text summarization",
      "authors": [
        "P Li",
        "W Lam",
        "L Bing",
        "Z Wang"
      ],
      "year": "2017",
      "venue": "Deep recurrent generative decoder for abstractive text summarization",
      "arxiv": "arXiv:1708.00625"
    },
    {
      "citation_id": "36",
      "title": "On the properties of neural machine translation: Encoder-decoder approaches",
      "authors": [
        "K Cho",
        "B Van Merriënboer",
        "D Bahdanau",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "On the properties of neural machine translation: Encoder-decoder approaches",
      "arxiv": "arXiv:1409.1259"
    },
    {
      "citation_id": "37",
      "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "authors": [
        "J Chung",
        "C Gulcehre",
        "K Cho",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "arxiv": "arXiv:1412.3555"
    },
    {
      "citation_id": "38",
      "title": "Learning with hypergraphs: Clustering, classification, and embedding",
      "authors": [
        "D Zhou",
        "J Huang",
        "B Schölkopf"
      ],
      "year": "2006",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "39",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "40",
      "title": "A multimodal database for affect recognition and implicit tagging",
      "authors": [
        "M Soleymani",
        "J Lichtenauer",
        "T Pun",
        "M Pantic"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "41",
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "authors": [
        "X Glorot",
        "Y Bengio"
      ],
      "year": "2010",
      "venue": "Proceedings of the thirteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings"
    },
    {
      "citation_id": "42",
      "title": "Eeg-based valence level recognition for realtime applications",
      "authors": [
        "Y Liu",
        "O Sourina"
      ],
      "year": "2012",
      "venue": "2012 International Conference on Cyberworlds"
    },
    {
      "citation_id": "43",
      "title": "Eeg based emotion identification using unsupervised deep feature learning",
      "authors": [
        "X Li",
        "P Zhang",
        "D Song",
        "G Yu",
        "Y Hou",
        "B Hu"
      ],
      "year": "2015",
      "venue": "Eeg based emotion identification using unsupervised deep feature learning"
    },
    {
      "citation_id": "44",
      "title": "Electroencephalogrambased emotion assessment system using ontology and data mining techniques",
      "authors": [
        "J Chen",
        "B Hu",
        "P Moore",
        "X Zhang",
        "X Ma"
      ],
      "year": "2015",
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "45",
      "title": "Eeg-based emotion recognition using recurrence plot analysis and k nearest neighbor classifier",
      "authors": [
        "F Bahari",
        "A Janghorbani"
      ],
      "year": "2013",
      "venue": "2013 20th Iranian Conference on Biomedical Engineering (ICBME)"
    },
    {
      "citation_id": "46",
      "title": "Recognition of emotions induced by music videos using dt-cwpt",
      "authors": [
        "D Naser",
        "G Saha"
      ],
      "year": "2013",
      "venue": "2013 Indian Conference on Medical Informatics and Telemedicine (ICMIT)"
    },
    {
      "citation_id": "47",
      "title": "Emotion recognition from eeg signals using multidimensional information in emd domain",
      "authors": [
        "N Zhuang",
        "Y Zeng",
        "L Tong",
        "C Zhang",
        "H Zhang",
        "B Yan"
      ],
      "year": "2017",
      "venue": "BioMed research international"
    },
    {
      "citation_id": "48",
      "title": "Comparative analysis of physiological signals and electroencephalogram (eeg) for multimodal emotion recognition using generative models",
      "authors": [
        "C Torres-Valencia",
        "H Garcia-Arias",
        "M Lopez",
        "A Orozco-Gutiérrez"
      ],
      "year": "2014",
      "venue": "2014 XIX Symposium on Image, Signal Processing and Artificial Vision"
    },
    {
      "citation_id": "49",
      "title": "Improving bci-based emotion recognition by combining eeg feature selection and kernel classifiers",
      "authors": [
        "J Atkinson",
        "D Campos"
      ],
      "year": "2016",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "50",
      "title": "Emotion detection from eeg recordings",
      "authors": [
        "J Liu",
        "H Meng",
        "A Nandi",
        "M Li"
      ],
      "year": "2016",
      "venue": "2016 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD"
    },
    {
      "citation_id": "51",
      "title": "Emotion recognition based on wavelet analysis of empirical mode decomposed eeg signals responsive to music videos",
      "authors": [
        "C Shahnaz",
        "S Hasan"
      ],
      "year": "2016",
      "venue": "2016 IEEE Region 10 Conference (TENCON)"
    },
    {
      "citation_id": "52",
      "title": "A hierarchical bidirectional gru model with attention for eeg-based emotion classification",
      "authors": [
        "J Chen",
        "D Jiang",
        "Y Zhang"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "53",
      "title": "Cross-subject emotion recognition from eeg using convolutional neural networks",
      "authors": [
        "X Zhong",
        "Z Yin",
        "J Zhang"
      ],
      "year": "2020",
      "venue": "\" in 2020 39th Chinese Control Conference (CCC)"
    },
    {
      "citation_id": "54",
      "title": "An efficient lstm network for emotion recognition from multichannel eeg signals",
      "authors": [
        "X Du",
        "C Ma",
        "G Zhang",
        "J Li",
        "Y.-K Lai",
        "G Zhao",
        "X Deng",
        "Y.-J Liu",
        "H Wang"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "55",
      "title": "Emotion recognition from users' eeg signals with the help of stimulus videos",
      "authors": [
        "Y Zhu",
        "S Wang",
        "Q Ji"
      ],
      "year": "2014",
      "venue": "2014 IEEE international conference on multimedia and expo (ICME)"
    },
    {
      "citation_id": "56",
      "title": "Multi-modal emotion analysis from facial expressions and electroencephalogram",
      "authors": [
        "X Huang",
        "J Kortelainen",
        "G Zhao",
        "X Li",
        "A Moilanen",
        "T Seppänen",
        "M Pietikäinen"
      ],
      "year": "2016",
      "venue": "Computer Vision and Image Understanding"
    },
    {
      "citation_id": "57",
      "title": "Locally robust eeg feature selection for individual-independent emotion recognition",
      "authors": [
        "Z Yin",
        "L Liu",
        "J Chen",
        "B Zhao",
        "Y Wang"
      ],
      "year": "2020",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "58",
      "title": "Domain adaptation for eeg emotion recognition based on latent representation similarity",
      "authors": [
        "J Li",
        "S Qiu",
        "C Du",
        "Y Wang",
        "H He"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "59",
      "title": "Multichannel eeg-based emotion recognition via group sparse canonical correlation analysis",
      "authors": [
        "W Zheng"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "60",
      "title": "A novel neural network model based on cerebral hemispheric asymmetry for eeg emotion recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "Z Cui",
        "T Zhang",
        "Y Zong"
      ],
      "year": "2018",
      "venue": "IJCAI"
    },
    {
      "citation_id": "61",
      "title": "From regional to global brain: A novel hierarchical spatial-temporal neural network model for eeg emotion recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "L Wang",
        "Y Zong",
        "Z Cui"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "62",
      "title": "Domain adaptation via transfer component analysis",
      "authors": [
        "S Pan",
        "I Tsang",
        "J Kwok",
        "Q Yang"
      ],
      "year": "2010",
      "venue": "IEEE transactions on neural networks"
    },
    {
      "citation_id": "63",
      "title": "Robust continuous clustering",
      "authors": [
        "S Shah",
        "V Koltun"
      ],
      "year": "2017",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "64",
      "title": "Graph degree linkage: Agglomerative clustering on a directed graph",
      "authors": [
        "W Zhang",
        "X Wang",
        "D Zhao",
        "X Tang"
      ],
      "year": "2012",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "65",
      "title": "Joint unsupervised learning of deep representations and image clusters",
      "authors": [
        "J Yang",
        "D Parikh",
        "D Batra"
      ],
      "venue": "Proceedings of the IEEE"
    }
  ]
}