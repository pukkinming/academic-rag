{
  "paper_id": "2102.01754v1",
  "title": "Lssed: A Large-Scale Dataset And Benchmark For Speech Emotion Recognition",
  "published": "2021-01-30T11:15:32Z",
  "authors": [
    "Weiquan Fan",
    "Xiangmin Xu",
    "Xiaofen Xing",
    "Weidong Chen",
    "Dongyan Huang"
  ],
  "keywords": [
    "speech emotion recognition",
    "dataset",
    "pretrained model",
    "deep learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition is a vital contributor to the next generation of human-computer interaction (HCI). However, current existing small-scale databases have limited the development of related research. In this paper, we present LSSED, a challenging large-scale english speech emotion dataset, which has data collected from 820 subjects to simulate realworld distribution. In addition, we release some pre-trained models based on LSSED, which can not only promote the development of speech emotion recognition, but can also be transferred to related downstream tasks such as mental health analysis where data is extremely difficult to collect. Finally, our experiments show the necessity of large-scale datasets and the effectiveness of pre-trained models. The dateset will be released on https://github.com/tobefans/LSSED.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) is a necessary part of the human-computer interaction system. Although emotion itself is very abstract, it still has some obvious intonation characteristics. Intuitively, sad voices are generally low-pitched and slow while happy voices are usually the opposite. Up to now, many algorithms have emerged for existing dataset.\n\nThere are lots of researches carried out on SER. In  [1] , Schuller et al. applies continuous Hidden Markov Models (HMM) to introduce SER with a self-collected speech corpus. Since 2004, some standardized speech emotion databases have been released. AIBO  [2] , EMODB  [3] , ENTERFACE  [4] , RML  [5] , IEMOCAP  [6] , AFEW  [7] , and MELD  [8] . Among them, IEMOCAP  [6]  and MELD  [8]  are the databases with the most data. IEMOCAP  [6]  collects 7,433 sentences (13 hours and 40 minutes in total) spoken by 10 people. MELD  [8]  contains 13,708 sentences (about 12 hours) from 407 people. In  [9] , decision tree is utilized to mitigate error propagation on AIBO  [2]  and IEMOCAP  [6] . In  [10] , RBM is applied to learn discriminatory features on EMODB  [3]  and ENTERFACE  [4] .\n\nXiaofen Xing is the corresponding author. Thanks to Datatang for support.\n\nWith the rapid development of deep learning, Zhang et al.  [11]  utilizes DCNN to bridge the affective gap in speech signals on EMODB  [3] , RML  [5] , ENTERFACE  [4] . At the same time, Satt et al.  [12]  presents a system based on an end-toend LSTM-CNN with raw spectrograms on IEMOCAP  [6] . Recently, Yeh et al.  [13]  proposes a dialogical emotion decoding algorithm to consecutively decode the emotion states of each utterance on IEMOCAP  [6]  and MELD  [8] .\n\nAlthough there have been certain level of progression on SER, there is still a potentially serious overfitting problem, which may limit the development of SER. As shown in  [14] [15] [16] [17] , even if a high accuracy is achieved on a certain database, their performance may be poor when transferring to another database. This is because the existing databases are generally small in scale, resulting in insufficient diversity, which is far from the real-world scenarios thus leading to the tendency of model overfitting. Therefore, a large-scale emotion dataset that can more comprehensively represent the real distribution is urgently needed to improve the generalization of existing algorithms.\n\nGenerally speaking, transfer learning can to a certain extent improve the performance of an algorithm. Boigne et al.  [18]  points out task-related transfer learning of recognizing emotions on small datasets. For emotion recognition related task, a good pre-trained model is urgent since data collection is very difficult. Taking the depression detection task as an example, there are only about a hundred subjects at most till date. In our opinion, the pre-trained model from the SER task is more suitable for detecting depression, since it is more inclined to obtain acoustic features while the model from ASR task is prone to extract linguistic features.\n\nIn this paper, we present LSSED, a challenging largescale english dataset for speech emotion recognition. It contains 147,025 sentences (206 hours and 25 minutes in total) spoken by 820 people. Based on our dataset, we can simulate a more comprehensive and rich data distribution of realworld scenarios so that deep neural networks can better model their distribution. Furthermore, since there is currently no non-semantic large-scale pre-training model, we release some pre-trained models with speech emotion recognition task.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Lssed",
      "text": "In this section, we introduce our dataset, LSSED in details. LSSED collects a total of 147,025 utterances from 820 subjects, with an average duration of 5.05s. As shown in Table  1 , the data volume of LSSED is very large, and its total duration (over 200 hours) can reach dozens of times than existing databases.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Collection And Labeling",
      "text": "The subjects that participate in the experiment are widely distributed with representations from both genders and variety of age groups. Each subject would be recorded in one or several emotional videos sessions in an indoor lab environment with a camera pointing at him or her. In the video, the subject is induced by random questions as their utterances are associated with an emotional label. The total length of a video is about 10-20 minutes.\n\nThe utterances in each video dialogue are annotated by a professional annotation team. Each utterance is annotated with the corresponding emotion label, including anger, happiness, sadness, disappointment, boredom, disgust, excitement, fear, surprise, normal, and other. Note that some utterances in the video contain two or more emotions. In addition, each utterance is also annotated with auxiliary information, including the gender and age of the subject.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Data Distribution",
      "text": "As mentioned above, our database covers various groups of people. Table  2  shows the conditional and joint distribution of the ages and genders. In LSSED, the gender distribution is relatively balanced. The age distribution however has fewer elderly people.  Since the subjects speak in a spontaneous environment, the more common neutral samples accounted for a larger proportion. Next is happy, sad, disappointed, excited, and angry samples respectively. The samples of these six categories account for 81% of the total sample. Then, the samples of boring, disgusting, fearful, and surprised are fewer, accounting for only 6%. In addition, 13% of other uncommon samples can be used for tasks to distinguish whether they are common emotions.\n\nIn order to standardize future training benchmarks, we divided our LSSED dataset into training and test sets. Specifically, we first shuffle the order of all samples, then set 20% of the samples as the test set, and the rest as the training set. It should be noted that we ensure the distribution of each emotion category in the training set and test set are the same or at least similar. Table  3  shows the specific distribution of data for emotion labels in the training set and test set respectively.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Preprocessing And Feature Extraction",
      "text": "After obtaining the videos, we then convert them into audio signals at a sampling rate of 16kHz. According to the start time and end time of each utterance, we cut out 147,025 audio utterances. For each sentence, we use spectral subtraction algorithm  [19]  to perform audio denoising. It subtracts noise on the short-time spectrum and then restores the audio. Next, we increase the audio volume by a factor of 2 to make the sound louder.\n\nAfter preprocessing, we perform STFT with Hann window length of 1024 points and the window shift of 512 points. A square operation follows to obtain the power spectrum. The power spectrum is then passed through a triangular filter bank with 128 Mel-scales to simulate the human auditory perception system.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Pre-Trained Models",
      "text": "We firstly select VGG  [20]  and ResNet  [21]  for pre-training, which are useful in many scenarios. VGG builds a unified and simple structure to deepen the network, while ResNet proposes residual learning to ease the training procedure.\n\nIn order to better adapt to the specificity of speech, we propose PyResNet, an improved model of ResNet  [21] . Due to the sufficient amount of data, PyResNet is based on ResNet50, ResNet101 or ResNet152.\n\nSpecifically, the second convolution layer in each layer of ResNet is replaced with a pyramid convolution  [22]  that can capture multi-scale information to solve the problem of uncertain time position of valid speech information. In addition, we replaced the GAP layer with average pooling layer only in the time dimension to make the model insensitive to time and preserve the frequency information.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Effectiveness Of Lssed",
      "text": "Although the current algorithms have achieved good results on many small-scale datasets, pre-trained models often cannot be well generalized to other datasets. This triggered our thinking about the scale of databases resulting in the collection and building of a large amount of database, that can be informative enough to train a model with good generalization.\n\nIn order to verify the effectiveness of different-scale datasets, we calculate the performance degradation based on ResNet152 as shown in Table  4 . As indicated, the performance degradation is very large when the model trained from small-scale IEMOCAP  [6]  is tested on large-scale LSSED, while it is less when tested from large-scale to small-scale. This demonstrates the effectiveness of LSSED, since it simulates the real-world distribution. We investigate some papers  [23] [24] [25]  with open source code from recent SER papers. Also, we carry out a series of contrast experiments based on commonly used backbone models, including VGG and ResNet. In addition, we also test our PyResNet model mentioned in Section 2.4.\n\nIn the experiments, all algorithms use the training set and test set from LSSED. The models of existing algorithms are based on the configuration in the original papers. Our PyRes-Net and the backbone models are iterated for 60 epochs with batch size of 256 through the SGD optimizer with a weight decay of 0.001. The learning rate (initialized to 0.01) drops to 10% of the original every 20 epochs. Consistent with the current mainstream SER experiments, we use four emotion categories, including angry, neutral, happy and sad.\n\nThe results are shown in table  5 . This shows that the performance of existing algorithms on large-scale LSSED is not satisfactory. More importantly, the accuracy (weighted and unweighted) of these algorithms is even lower than that of the basic VGG and ResNet models.\n\nIn addition, it is worth noting that our PyResNet achieves better results than the basic backbone models. This demonstrates that the improvement based on pyramid convolution is effective on large-scale database. Since these algorithms are not excellent in overall performance on large-scale databases, it should be indicated that LSSED still has great challenges which means that speech emotion recognition is still a long way from being perfectly widely applicable.\n\nConfusion matrices of both MTS-3 branches and PyRes-Net that use ResNet152 as the backbone is shown below. Although they all use multi-scale convolution kernels, the former uses multi-scale kernels derived from one kernel, while the latter directly uses multiple different kernels with more powerful modeling capabilities. As shown in Figure  2 , we can observe that neutral samples have a high probability of being correctly predicted, which is also the most common Table  5 . The performance of different methods on LSSED.\n\nAlgorithm Backbone WA UA FCN-Attention  [23]  ALEXNet 0.570 0.250 MTS-3branches  [24]  ALEXNet 0.570 0.250 MTS-5branches  [24]  ALEXNet 0.570 0.250 MTS-3branches  [24]  ResNet152 0.585 0.296 MTS-5branches  [24]  ResNet152 0.582 0.311 ADV-Real  [25]  VGG16 0.570 0.250 ADV-Fake  [25]  VGG16 0.570 0.250 ADV-Real  [25]  ResNet152 0.548 0.381 ADV-Fake  [25]  ResNet152 0.453 0.339 VGG  [20]  VGG11 0.595 0.337 VGG  [20]  VGG13 0.604 0.393 VGG  [20]  VGG16 0.585 0.313 VGG  [20]  VGG19 0.585 0.370 ResNet  [21]  ResNet18 0.594 0.382 ResNet  [21]  ResNet34 0.598 0.355 ResNet  [21]  ResNet50 0.587 0.377 ResNet  [21]  ResNet101 0.592 0.332 ResNet  [21]  ResNet152 The confusion matrix of MTS-3 branches\n\nThe confusion matrix of our PyResNet (b) (a) Fig.  2 . Confusion matrices of different algorithms.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Pre-Trained Model In Downstream Task",
      "text": "With the above pre-trained models, we want to further explore its applicability to downstream tasks. We choose speechbased depression detection as our downstream task. Due to the high professional requirements, it is very difficult to collect data on patients with depression. This leads to the current unsatisfactory effect of automatic depression detection. It is therefore a natural idea to use a pre-trained model with suffi-cient prior knowledge to improve the detection accuracy.\n\nThese series of experiments are carried out on the DAIC-WOZ depression database, which is a subset of the Distress Analysis Interview Corpus (DAIC)  [26] . There are 107 subjects in the training set, 35 in the development set, and 47 in the test set. Each subject will be interviewed by an animated virtual interviewer and recorded with video and audio equipments. The data will be annotated with the start time, end time and depression (or not) of each sentence.\n\nWe choose SER task and ASR task for transfer. Firstly, we need to get the pre-trained models. For SER, we use the pretrained PyResNet with ResNet152 as a backbone. For ASR, we use ESPNet  [27] , which is an end-to-end encoder-decoder structure network. The results of the experiment are shown in Table  6 . The performance of transfer based on SER is better than that based on ASR. This is because the features extracted by ASR are bias towards semantics while the features extracted by SER are bias towards acoustics. Depression detection pays more attention to acoustic features which has larger gaps with ASR tasks. Therefore, the pre-trained model on SER with a smaller gap has better performance.\n\nMoreover, we also considered the differences in bandwidth between SER and ASR when framing. ASR generally uses a narrow window length of about 25ms. This means that it pays more attention to changes in time and has a higher time resolution. For SER, we use a wide window length of about 65ms, which means that the frequency information in each frame is richer and the frequency resolution is higher. In general, a high time resolution is conducive to extracting semantic features from frame by frame and a high frequency resolution is conducive to extracting acoustic features. Therefore, for downstream tasks such as depression detection, the SER pre-trained model with high frequency resolution and smaller gap may be a better choice.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we present LSSED, a challenging large-scale english database for speech emotion recognition that can simulate real distribution. We point out that existing algorithms tend to overfit small-scale databases and thus cannot be well generalized to real scenes. Furthermore, we release some pretrained models based on LSSED. These models can not only promote the development of SER, but can also be transferred to similar downstream tasks like mental health analysis where data is extremely difficult to collect.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Distribution of data for each emotion labels",
      "page": 2
    },
    {
      "caption": "Figure 1: Since the subjects speak in a spontaneous",
      "page": 2
    },
    {
      "caption": "Figure 2: Confusion matrices of different algorithms.",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "† UBTECH Robotics Corp, China": "ABSTRACT"
        },
        {
          "† UBTECH Robotics Corp, China": ""
        },
        {
          "† UBTECH Robotics Corp, China": "Speech emotion recognition is a vital contributor to the next"
        },
        {
          "† UBTECH Robotics Corp, China": ""
        },
        {
          "† UBTECH Robotics Corp, China": "generation of human-computer\ninteraction (HCI). However,"
        },
        {
          "† UBTECH Robotics Corp, China": ""
        },
        {
          "† UBTECH Robotics Corp, China": "current existing small-scale databases have limited the devel-"
        },
        {
          "† UBTECH Robotics Corp, China": ""
        },
        {
          "† UBTECH Robotics Corp, China": "opment of related research. In this paper, we present LSSED,"
        },
        {
          "† UBTECH Robotics Corp, China": ""
        },
        {
          "† UBTECH Robotics Corp, China": "a\nchallenging\nlarge-scale\nenglish\nspeech\nemotion\ndataset,"
        },
        {
          "† UBTECH Robotics Corp, China": ""
        },
        {
          "† UBTECH Robotics Corp, China": "which has data collected from 820 subjects to simulate real-"
        },
        {
          "† UBTECH Robotics Corp, China": ""
        },
        {
          "† UBTECH Robotics Corp, China": "world distribution.\nIn addition, we release some pre-trained"
        },
        {
          "† UBTECH Robotics Corp, China": "models based on LSSED, which can not only promote the"
        },
        {
          "† UBTECH Robotics Corp, China": ""
        },
        {
          "† UBTECH Robotics Corp, China": "development of speech emotion recognition, but can also be"
        },
        {
          "† UBTECH Robotics Corp, China": ""
        },
        {
          "† UBTECH Robotics Corp, China": "transferred to related downstream tasks such as mental health"
        },
        {
          "† UBTECH Robotics Corp, China": ""
        },
        {
          "† UBTECH Robotics Corp, China": "analysis where data is extremely difﬁcult\nto collect. Finally,"
        },
        {
          "† UBTECH Robotics Corp, China": ""
        },
        {
          "† UBTECH Robotics Corp, China": "our experiments\nshow the necessity of\nlarge-scale datasets"
        },
        {
          "† UBTECH Robotics Corp, China": ""
        },
        {
          "† UBTECH Robotics Corp, China": "and the effectiveness of pre-trained models. The dateset will"
        },
        {
          "† UBTECH Robotics Corp, China": ""
        },
        {
          "† UBTECH Robotics Corp, China": "be released on https://github.com/tobefans/LSSED."
        },
        {
          "† UBTECH Robotics Corp, China": ""
        },
        {
          "† UBTECH Robotics Corp, China": "Index Terms— speech emotion recognition, dataset, pre-"
        },
        {
          "† UBTECH Robotics Corp, China": "trained model, deep learning"
        },
        {
          "† UBTECH Robotics Corp, China": ""
        },
        {
          "† UBTECH Robotics Corp, China": ""
        },
        {
          "† UBTECH Robotics Corp, China": "1.\nINTRODUCTION"
        },
        {
          "† UBTECH Robotics Corp, China": ""
        },
        {
          "† UBTECH Robotics Corp, China": "Speech emotion recognition (SER) is a necessary part of the"
        },
        {
          "† UBTECH Robotics Corp, China": ""
        },
        {
          "† UBTECH Robotics Corp, China": "human-computer interaction system. Although emotion itself"
        },
        {
          "† UBTECH Robotics Corp, China": ""
        },
        {
          "† UBTECH Robotics Corp, China": "is very abstract,\nit still has some obvious intonation charac-"
        },
        {
          "† UBTECH Robotics Corp, China": ""
        },
        {
          "† UBTECH Robotics Corp, China": "teristics. Intuitively, sad voices are generally low-pitched and"
        },
        {
          "† UBTECH Robotics Corp, China": ""
        },
        {
          "† UBTECH Robotics Corp, China": "slow while happy voices are usually the opposite. Up to now,"
        },
        {
          "† UBTECH Robotics Corp, China": ""
        },
        {
          "† UBTECH Robotics Corp, China": "many algorithms have emerged for existing dataset."
        },
        {
          "† UBTECH Robotics Corp, China": ""
        },
        {
          "† UBTECH Robotics Corp, China": "There are lots of\nresearches carried out on SER.\nIn [1],"
        },
        {
          "† UBTECH Robotics Corp, China": ""
        },
        {
          "† UBTECH Robotics Corp, China": "Schuller et al.\napplies continuous Hidden Markov Models"
        },
        {
          "† UBTECH Robotics Corp, China": ""
        },
        {
          "† UBTECH Robotics Corp, China": "(HMM) to introduce SER with a self-collected speech corpus."
        },
        {
          "† UBTECH Robotics Corp, China": ""
        },
        {
          "† UBTECH Robotics Corp, China": "Since 2004,\nsome\nstandardized speech emotion databases"
        },
        {
          "† UBTECH Robotics Corp, China": ""
        },
        {
          "† UBTECH Robotics Corp, China": "have been released. AIBO[2], EMODB[3], ENTERFACE[4],"
        },
        {
          "† UBTECH Robotics Corp, China": ""
        },
        {
          "† UBTECH Robotics Corp, China": "RML[5],\nIEMOCAP[6], AFEW[7], and MELD[8]. Among"
        },
        {
          "† UBTECH Robotics Corp, China": "them, IEMOCAP[6] and MELD[8] are the databases with the"
        },
        {
          "† UBTECH Robotics Corp, China": ""
        },
        {
          "† UBTECH Robotics Corp, China": "most data.\nIEMOCAP[6] collects 7,433 sentences (13 hours"
        },
        {
          "† UBTECH Robotics Corp, China": ""
        },
        {
          "† UBTECH Robotics Corp, China": "and 40 minutes in total) spoken by 10 people. MELD[8] con-"
        },
        {
          "† UBTECH Robotics Corp, China": ""
        },
        {
          "† UBTECH Robotics Corp, China": "tains 13,708 sentences (about 12 hours) from 407 people.\nIn"
        },
        {
          "† UBTECH Robotics Corp, China": ""
        },
        {
          "† UBTECH Robotics Corp, China": "[9], decision tree is utilized to mitigate error propagation on"
        },
        {
          "† UBTECH Robotics Corp, China": ""
        },
        {
          "† UBTECH Robotics Corp, China": "AIBO[2] and IEMOCAP[6]. In [10], RBM is applied to learn"
        },
        {
          "† UBTECH Robotics Corp, China": ""
        },
        {
          "† UBTECH Robotics Corp, China": "discriminatory features on EMODB[3] and ENTERFACE[4]."
        },
        {
          "† UBTECH Robotics Corp, China": ""
        },
        {
          "† UBTECH Robotics Corp, China": ""
        },
        {
          "† UBTECH Robotics Corp, China": "Xiaofen Xing is the corresponding author."
        },
        {
          "† UBTECH Robotics Corp, China": "Thanks to Datatang for support."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table 1: ,thedatavolumeofLSSEDisverylarge,anditstotaldura-",
      "data": [
        {
          "Table 1. Comparison to existing public speech emotion datasets.": "Language"
        },
        {
          "Table 1. Comparison to existing public speech emotion datasets.": "Multiple"
        },
        {
          "Table 1. Comparison to existing public speech emotion datasets.": "German"
        },
        {
          "Table 1. Comparison to existing public speech emotion datasets.": "English"
        },
        {
          "Table 1. Comparison to existing public speech emotion datasets.": "Multiple"
        },
        {
          "Table 1. Comparison to existing public speech emotion datasets.": "English"
        },
        {
          "Table 1. Comparison to existing public speech emotion datasets.": "Multiple"
        },
        {
          "Table 1. Comparison to existing public speech emotion datasets.": "English"
        },
        {
          "Table 1. Comparison to existing public speech emotion datasets.": "English"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: ,thedatavolumeofLSSEDisverylarge,anditstotaldura-",
      "data": [
        {
          "2. LSSED": "In this section, we introduce our dataset, LSSED in details."
        },
        {
          "2. LSSED": "LSSED collects a total of 147,025 utterances from 820 sub-"
        },
        {
          "2. LSSED": "jects, with an average duration of 5.05s. As shown in Table"
        },
        {
          "2. LSSED": "1, the data volume of LSSED is very large, and its total dura-"
        },
        {
          "2. LSSED": "tion (over 200 hours) can reach dozens of times than existing"
        },
        {
          "2. LSSED": "databases."
        },
        {
          "2. LSSED": "2.1. Collection and Labeling"
        },
        {
          "2. LSSED": "The subjects that participate in the experiment are widely dis-"
        },
        {
          "2. LSSED": "tributed with representations from both genders and variety of"
        },
        {
          "2. LSSED": ""
        },
        {
          "2. LSSED": "age groups. Each subject would be recorded in one or several"
        },
        {
          "2. LSSED": "emotional videos sessions in an indoor lab environment with"
        },
        {
          "2. LSSED": "a camera pointing at him or her. In the video, the subject is in-"
        },
        {
          "2. LSSED": "duced by random questions as their utterances are associated"
        },
        {
          "2. LSSED": "with an emotional\nlabel. The total\nlength of a video is about"
        },
        {
          "2. LSSED": "10-20 minutes."
        },
        {
          "2. LSSED": "The utterances in each video dialogue are annotated by"
        },
        {
          "2. LSSED": "a professional annotation team. Each utterance is annotated"
        },
        {
          "2. LSSED": "with the corresponding emotion label, including anger, happi-"
        },
        {
          "2. LSSED": "ness, sadness, disappointment, boredom, disgust, excitement,"
        },
        {
          "2. LSSED": "fear, surprise, normal, and other. Note that some utterances"
        },
        {
          "2. LSSED": "in the video contain two or more emotions.\nIn addition, each"
        },
        {
          "2. LSSED": "utterance is also annotated with auxiliary information, includ-"
        },
        {
          "2. LSSED": "ing the gender and age of the subject."
        },
        {
          "2. LSSED": ""
        },
        {
          "2. LSSED": ""
        },
        {
          "2. LSSED": "2.2. Data Distribution"
        },
        {
          "2. LSSED": ""
        },
        {
          "2. LSSED": "As mentioned above, our database covers various groups of"
        },
        {
          "2. LSSED": "people. Table 2 shows the conditional and joint distribution"
        },
        {
          "2. LSSED": "of the ages and genders. In LSSED, the gender distribution is"
        },
        {
          "2. LSSED": "relatively balanced. The age distribution however has fewer"
        },
        {
          "2. LSSED": "elderly people."
        },
        {
          "2. LSSED": ""
        },
        {
          "2. LSSED": ""
        },
        {
          "2. LSSED": "Table 2. Data distribution for gender and age."
        },
        {
          "2. LSSED": ""
        },
        {
          "2. LSSED": "Young\nMiddle-aged\nOld\nTotal"
        },
        {
          "2. LSSED": ""
        },
        {
          "2. LSSED": "Female\n253\n167\n65\n485"
        },
        {
          "2. LSSED": ""
        },
        {
          "2. LSSED": "Male\n155\n141\n39\n335"
        },
        {
          "2. LSSED": ""
        },
        {
          "2. LSSED": "Total\n408\n308\n104\n820"
        },
        {
          "2. LSSED": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "In\norder\nto\nverify\nthe\neffectiveness\nof\ndifferent-scale": ""
        },
        {
          "In\norder\nto\nverify\nthe\neffectiveness\nof\ndifferent-scale": "datasets, we\ncalculate\nthe\nperformance\ndegradation\nbased"
        },
        {
          "In\norder\nto\nverify\nthe\neffectiveness\nof\ndifferent-scale": ""
        },
        {
          "In\norder\nto\nverify\nthe\neffectiveness\nof\ndifferent-scale": "on ResNet152 as shown in Table 4. As indicated,\nthe perfor-"
        },
        {
          "In\norder\nto\nverify\nthe\neffectiveness\nof\ndifferent-scale": ""
        },
        {
          "In\norder\nto\nverify\nthe\neffectiveness\nof\ndifferent-scale": "mance degradation is very large when the model trained from"
        },
        {
          "In\norder\nto\nverify\nthe\neffectiveness\nof\ndifferent-scale": ""
        },
        {
          "In\norder\nto\nverify\nthe\neffectiveness\nof\ndifferent-scale": "small-scale\nIEMOCAP[6]\nis\ntested on large-scale LSSED,"
        },
        {
          "In\norder\nto\nverify\nthe\neffectiveness\nof\ndifferent-scale": ""
        },
        {
          "In\norder\nto\nverify\nthe\neffectiveness\nof\ndifferent-scale": "while it\nis less when tested from large-scale to small-scale."
        },
        {
          "In\norder\nto\nverify\nthe\neffectiveness\nof\ndifferent-scale": ""
        },
        {
          "In\norder\nto\nverify\nthe\neffectiveness\nof\ndifferent-scale": "This demonstrates the effectiveness of LSSED, since it simu-"
        },
        {
          "In\norder\nto\nverify\nthe\neffectiveness\nof\ndifferent-scale": ""
        },
        {
          "In\norder\nto\nverify\nthe\neffectiveness\nof\ndifferent-scale": "lates the real-world distribution."
        },
        {
          "In\norder\nto\nverify\nthe\neffectiveness\nof\ndifferent-scale": ""
        },
        {
          "In\norder\nto\nverify\nthe\neffectiveness\nof\ndifferent-scale": ""
        },
        {
          "In\norder\nto\nverify\nthe\neffectiveness\nof\ndifferent-scale": "Table 4. Performance degradation when testing in the target"
        },
        {
          "In\norder\nto\nverify\nthe\neffectiveness\nof\ndifferent-scale": "database compared with the source database (training in the"
        },
        {
          "In\norder\nto\nverify\nthe\neffectiveness\nof\ndifferent-scale": "source database)."
        },
        {
          "In\norder\nto\nverify\nthe\neffectiveness\nof\ndifferent-scale": "Source\nTarget\n-WA\n-UA"
        },
        {
          "In\norder\nto\nverify\nthe\neffectiveness\nof\ndifferent-scale": "IEMOCAP\nLSSED\n0.596\n0.342"
        },
        {
          "In\norder\nto\nverify\nthe\neffectiveness\nof\ndifferent-scale": "0.119\n0.071\nLSSED\nIEMOCAP"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "cient prior knowledge to improve the detection accuracy.": ""
        },
        {
          "cient prior knowledge to improve the detection accuracy.": "These series of experiments are carried out on the DAIC-"
        },
        {
          "cient prior knowledge to improve the detection accuracy.": ""
        },
        {
          "cient prior knowledge to improve the detection accuracy.": "WOZ depression database, which is a subset of the Distress"
        },
        {
          "cient prior knowledge to improve the detection accuracy.": ""
        },
        {
          "cient prior knowledge to improve the detection accuracy.": "Analysis Interview Corpus (DAIC) [26]. There are 107 sub-"
        },
        {
          "cient prior knowledge to improve the detection accuracy.": ""
        },
        {
          "cient prior knowledge to improve the detection accuracy.": "jects in the training set, 35 in the development set, and 47 in"
        },
        {
          "cient prior knowledge to improve the detection accuracy.": ""
        },
        {
          "cient prior knowledge to improve the detection accuracy.": "the test set. Each subject will be interviewed by an animated"
        },
        {
          "cient prior knowledge to improve the detection accuracy.": ""
        },
        {
          "cient prior knowledge to improve the detection accuracy.": "virtual interviewer and recorded with video and audio equip-"
        },
        {
          "cient prior knowledge to improve the detection accuracy.": ""
        },
        {
          "cient prior knowledge to improve the detection accuracy.": "The data will be annotated with the start"
        },
        {
          "cient prior knowledge to improve the detection accuracy.": ""
        },
        {
          "cient prior knowledge to improve the detection accuracy.": "time and depression (or not) of each sentence."
        },
        {
          "cient prior knowledge to improve the detection accuracy.": ""
        },
        {
          "cient prior knowledge to improve the detection accuracy.": "We choose SER task and ASR task for transfer. Firstly, we"
        },
        {
          "cient prior knowledge to improve the detection accuracy.": ""
        },
        {
          "cient prior knowledge to improve the detection accuracy.": "need to get the pre-trained models. For SER, we use the pre-"
        },
        {
          "cient prior knowledge to improve the detection accuracy.": ""
        },
        {
          "cient prior knowledge to improve the detection accuracy.": "trained PyResNet with ResNet152 as a backbone. For ASR,"
        },
        {
          "cient prior knowledge to improve the detection accuracy.": ""
        },
        {
          "cient prior knowledge to improve the detection accuracy.": "we use ESPNet [27], which is an end-to-end encoder-decoder"
        },
        {
          "cient prior knowledge to improve the detection accuracy.": ""
        },
        {
          "cient prior knowledge to improve the detection accuracy.": ""
        },
        {
          "cient prior knowledge to improve the detection accuracy.": ""
        },
        {
          "cient prior knowledge to improve the detection accuracy.": ""
        },
        {
          "cient prior knowledge to improve the detection accuracy.": "Table 6. The performance of different pre-trained models on"
        },
        {
          "cient prior knowledge to improve the detection accuracy.": ""
        },
        {
          "cient prior knowledge to improve the detection accuracy.": ""
        },
        {
          "cient prior knowledge to improve the detection accuracy.": ""
        },
        {
          "cient prior knowledge to improve the detection accuracy.": "Algorithm\nWA\nUA"
        },
        {
          "cient prior knowledge to improve the detection accuracy.": ""
        },
        {
          "cient prior knowledge to improve the detection accuracy.": "ESPNet (ASR)\n0.657\n0.500"
        },
        {
          "cient prior knowledge to improve the detection accuracy.": ""
        },
        {
          "cient prior knowledge to improve the detection accuracy.": "0.714\n0.583\nPyResNet (SER)"
        },
        {
          "cient prior knowledge to improve the detection accuracy.": ""
        },
        {
          "cient prior knowledge to improve the detection accuracy.": ""
        },
        {
          "cient prior knowledge to improve the detection accuracy.": "The results of the experiment are shown in Table 6. The"
        },
        {
          "cient prior knowledge to improve the detection accuracy.": ""
        },
        {
          "cient prior knowledge to improve the detection accuracy.": "performance of transfer based on SER is better than that based"
        },
        {
          "cient prior knowledge to improve the detection accuracy.": ""
        },
        {
          "cient prior knowledge to improve the detection accuracy.": "on ASR. This is because the features extracted by ASR are"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "The results of the experiment are shown in Table 6. The": ""
        },
        {
          "The results of the experiment are shown in Table 6. The": "performance of transfer based on SER is better than that based"
        },
        {
          "The results of the experiment are shown in Table 6. The": ""
        },
        {
          "The results of the experiment are shown in Table 6. The": "on ASR. This is because the features extracted by ASR are"
        },
        {
          "The results of the experiment are shown in Table 6. The": "bias towards semantics while the features extracted by SER"
        },
        {
          "The results of the experiment are shown in Table 6. The": "are bias towards acoustics. Depression detection pays more"
        },
        {
          "The results of the experiment are shown in Table 6. The": ""
        },
        {
          "The results of the experiment are shown in Table 6. The": "attention to acoustic features which has larger gaps with ASR"
        },
        {
          "The results of the experiment are shown in Table 6. The": ""
        },
        {
          "The results of the experiment are shown in Table 6. The": "tasks. Therefore, the pre-trained model on SER with a smaller"
        },
        {
          "The results of the experiment are shown in Table 6. The": ""
        },
        {
          "The results of the experiment are shown in Table 6. The": ""
        },
        {
          "The results of the experiment are shown in Table 6. The": ""
        },
        {
          "The results of the experiment are shown in Table 6. The": "in band-"
        },
        {
          "The results of the experiment are shown in Table 6. The": ""
        },
        {
          "The results of the experiment are shown in Table 6. The": "width between SER and ASR when framing. ASR generally"
        },
        {
          "The results of the experiment are shown in Table 6. The": ""
        },
        {
          "The results of the experiment are shown in Table 6. The": "uses a narrow window length of about 25ms. This means that"
        },
        {
          "The results of the experiment are shown in Table 6. The": ""
        },
        {
          "The results of the experiment are shown in Table 6. The": "it pays more attention to changes in time and has a higher"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "width between SER and ASR when framing. ASR generally": ""
        },
        {
          "width between SER and ASR when framing. ASR generally": "uses a narrow window length of about 25ms. This means that"
        },
        {
          "width between SER and ASR when framing. ASR generally": ""
        },
        {
          "width between SER and ASR when framing. ASR generally": "it pays more attention to changes in time and has a higher"
        },
        {
          "width between SER and ASR when framing. ASR generally": "time resolution.\nFor SER, we use a wide window length of"
        },
        {
          "width between SER and ASR when framing. ASR generally": ""
        },
        {
          "width between SER and ASR when framing. ASR generally": "about 65ms, which means that\nthe frequency information in"
        },
        {
          "width between SER and ASR when framing. ASR generally": "each frame is richer and the frequency resolution is higher."
        },
        {
          "width between SER and ASR when framing. ASR generally": "In general, a high time resolution is conducive to extracting"
        },
        {
          "width between SER and ASR when framing. ASR generally": "semantic features from frame by frame and a high frequency"
        },
        {
          "width between SER and ASR when framing. ASR generally": "resolution is conducive to extracting acoustic features. There-"
        },
        {
          "width between SER and ASR when framing. ASR generally": "fore, for downstream tasks such as depression detection,\nthe"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "",
          "no. 5, pp. 585–589, 2016.": "[15] M. Abdelwahab and C. Busso, “Domain adversarial for acous-"
        },
        {
          "References": "[1] B. Schuller, G. Rigoll, and M. Lang, “Hidden markov model-",
          "no. 5, pp. 585–589, 2016.": "IEEE/ACM Transactions on Audio,\ntic emotion recognition,”"
        },
        {
          "References": "based speech emotion recognition,”\nin 2003 IEEE Interna-",
          "no. 5, pp. 585–589, 2016.": "Speech, and Language Processing, vol. 26, no. 12, pp. 2423–"
        },
        {
          "References": "tional Conference on Acoustics, Speech, and Signal Process-",
          "no. 5, pp. 585–589, 2016.": "2435, 2018."
        },
        {
          "References": "ing, 2003. Proceedings.(ICASSP’03). IEEE, 2003, vol. 2, pp.",
          "no. 5, pp. 585–589, 2016.": "[16] N. Liu, Y. Zong, B. Zhang, L. Liu,\nJ. Chen, G. Zhao,\nand"
        },
        {
          "References": "II–1.",
          "no. 5, pp. 585–589, 2016.": "J. Zhu,\n“Unsupervised cross-corpus speech emotion recogni-"
        },
        {
          "References": "[2] A. Batliner, C. Hacker, S. Steidl, E. N¨oth, S. D’Arcy, M. J.",
          "no. 5, pp. 585–589, 2016.": "tion using domain-adaptive subspace learning,”\nin 2018 IEEE"
        },
        {
          "References": "Russell, and M. Wong,\n“” you stupid tin box”-children inter-",
          "no. 5, pp. 585–589, 2016.": "International Conference on Acoustics, Speech and Signal Pro-"
        },
        {
          "References": "acting with the aibo robot: A cross-linguistic emotional speech",
          "no. 5, pp. 585–589, 2016.": "cessing (ICASSP). IEEE, 2018, pp. 5144–5148."
        },
        {
          "References": "corpus.,” in Lrec, 2004.",
          "no. 5, pp. 585–589, 2016.": "[17] H. Luo and J. Han,\n“Nonnegative matrix factorization based"
        },
        {
          "References": "[3]\nF. Burkhardt, A. Paeschke, M. Rolfes, W. F. Sendlmeier, and",
          "no. 5, pp. 585–589, 2016.": "transfer\nsubspace\nlearning for\ncross-corpus\nspeech emotion"
        },
        {
          "References": "B. Weiss, “A database of german emotional speech,” in Ninth",
          "no. 5, pp. 585–589, 2016.": "IEEE/ACM Transactions on Audio, Speech, and\nrecognition,”"
        },
        {
          "References": "European Conference on Speech Communication and Technol-",
          "no. 5, pp. 585–589, 2016.": "Language Processing, vol. 28, pp. 2047–2060, 2020."
        },
        {
          "References": "ogy, 2005.",
          "no. 5, pp. 585–589, 2016.": "¨"
        },
        {
          "References": "",
          "no. 5, pp. 585–589, 2016.": "[18]\nJ. Boigne, B. Liyanage, and T.\nOstrem,\n“Recognizing more"
        },
        {
          "References": "[4] O. Martin, I. Kotsia, B. Macq, and I. Pitas,\n“The enterface’05",
          "no. 5, pp. 585–589, 2016.": "emotions with less data using self-supervised transfer\nlearn-"
        },
        {
          "References": "audio-visual emotion database,”\nin 22nd International Con-",
          "no. 5, pp. 585–589, 2016.": "ing,” 2020."
        },
        {
          "References": "ference on Data Engineering Workshops (ICDEW’06). IEEE,",
          "no. 5, pp. 585–589, 2016.": "[19] N. Upadhyay and A. Karmakar,\n“Speech enhancement using"
        },
        {
          "References": "2006, pp. 8–8.",
          "no. 5, pp. 585–589, 2016.": "spectral subtraction-type algorithms: A comparison and simu-"
        },
        {
          "References": "[5] Y. Wang and L. Guan,\n“Recognizing human emotional state",
          "no. 5, pp. 585–589, 2016.": "lation study,”\nProcedia Computer Science, vol. 54, pp. 574–"
        },
        {
          "References": "from audiovisual signals,”\nIEEE transactions on multimedia,",
          "no. 5, pp. 585–589, 2016.": "584, 2015."
        },
        {
          "References": "vol. 10, no. 5, pp. 936–946, 2008.",
          "no. 5, pp. 585–589, 2016.": "[20] K. Simonyan and A. Zisserman,\n“Very deep convolutional"
        },
        {
          "References": "[6] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower,",
          "no. 5, pp. 585–589, 2016.": "arXiv preprint\nnetworks for\nlarge-scale image recognition,”"
        },
        {
          "References": "S. Kim, J. N. Chang, S. Lee, and S. S. Narayanan,\n“Iemocap:",
          "no. 5, pp. 585–589, 2016.": "arXiv:1409.1556, 2014."
        },
        {
          "References": "Lan-\nInteractive emotional dyadic motion capture database,”",
          "no. 5, pp. 585–589, 2016.": "[21] K. He, X. Zhang, S. Ren, and J. Sun,\n“Deep residual learning"
        },
        {
          "References": "guage resources and evaluation, vol. 42, no. 4, pp. 335, 2008.",
          "no. 5, pp. 585–589, 2016.": "for image recognition,” in Proceedings of the IEEE conference"
        },
        {
          "References": "[7] A. Dhall, R. Goecke, S. Lucey,\nand T. Gedeon,\n“Collect-",
          "no. 5, pp. 585–589, 2016.": "on computer vision and pattern recognition, 2016, pp. 770–"
        },
        {
          "References": "ing large,\nrichly annotated facial-expression databases\nfrom",
          "no. 5, pp. 585–589, 2016.": "778."
        },
        {
          "References": "movies,” IEEE multimedia,\n, no. 3, pp. 34–41, 2012.",
          "no. 5, pp. 585–589, 2016.": "[22]\nI. C. Duta, L. Liu, F. Zhu, and L. Shao,\n“Pyramidal convo-"
        },
        {
          "References": "[8]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,",
          "no. 5, pp. 585–589, 2016.": "lution: Rethinking convolutional neural networks\nfor visual"
        },
        {
          "References": "and R. Mihalcea,\n“Meld: A multimodal multi-party dataset",
          "no. 5, pp. 585–589, 2016.": "recognition,” arXiv preprint arXiv:2006.11538, 2020."
        },
        {
          "References": "arXiv preprint\nfor\nemotion recognition in conversations,”",
          "no. 5, pp. 585–589, 2016.": "[23] Y. Zhang,\nJ. Du, Z. Wang,\nJ. Zhang, and Y. Tu,\n“Attention"
        },
        {
          "References": "arXiv:1810.02508, 2018.",
          "no. 5, pp. 585–589, 2016.": "based fully convolutional network for speech emotion recogni-"
        },
        {
          "References": "[9] C.-C. Lee, E. Mower, C. Busso, S. Lee, and S. Narayanan,",
          "no. 5, pp. 585–589, 2016.": "tion,” in 2018 Asia-Paciﬁc Signal and Information Processing"
        },
        {
          "References": "“Emotion recognition using a hierarchical binary decision tree",
          "no. 5, pp. 585–589, 2016.": "Association Annual Summit and Conference (APSIPA ASC)."
        },
        {
          "References": "approach,”\nSpeech Communication, vol. 53, no. 9-10, pp.",
          "no. 5, pp. 585–589, 2016.": "IEEE, 2018, pp. 1771–1775."
        },
        {
          "References": "1162–1171, 2011.",
          "no. 5, pp. 585–589, 2016.": "[24] E. Guizzo, T. Weyde, and J. B. Leveson,\n“Multi-time-scale"
        },
        {
          "References": "[10] A. Stuhlsatz, C. Meyer, F. Eyben, T. Zielke, G. Meier, and",
          "no. 5, pp. 585–589, 2016.": "convolution for emotion recognition from speech audio sig-"
        },
        {
          "References": "B. Schuller,\n“Deep neural networks\nfor\nacoustic\nemotion",
          "no. 5, pp. 585–589, 2016.": "nals,”\nin ICASSP 2020-2020 IEEE International Conference"
        },
        {
          "References": "recognition: Raising the benchmarks,”\nin 2011 IEEE interna-",
          "no. 5, pp. 585–589, 2016.": "on Acoustics, Speech and Signal Processing (ICASSP). IEEE,"
        },
        {
          "References": "tional conference on acoustics, speech and signal processing",
          "no. 5, pp. 585–589, 2016.": "2020, pp. 6489–6493."
        },
        {
          "References": "(ICASSP). IEEE, 2011, pp. 5688–5691.",
          "no. 5, pp. 585–589, 2016.": "[25] Z. Ren, A. Baird, J. Han, Z. Zhang, and B. Schuller, “Generat-"
        },
        {
          "References": "[11]\nS. Zhang, S. Zhang, T. Huang, and W. Gao,\n“Speech emotion",
          "no. 5, pp. 585–589, 2016.": "ing and protecting against adversarial attacks for deep speech-"
        },
        {
          "References": "recognition using deep convolutional neural network and dis-",
          "no. 5, pp. 585–589, 2016.": "based emotion recognition models,”\nin ICASSP 2020-2020"
        },
        {
          "References": "criminant temporal pyramid matching,” IEEE Transactions on",
          "no. 5, pp. 585–589, 2016.": "IEEE International Conference on Acoustics, Speech and Sig-"
        },
        {
          "References": "Multimedia, vol. 20, no. 6, pp. 1576–1590, 2017.",
          "no. 5, pp. 585–589, 2016.": "nal Processing (ICASSP). IEEE, 2020, pp. 7184–7188."
        },
        {
          "References": "[12] A. Satt, S. Rozenberg, and R. Hoory, “Efﬁcient emotion recog-",
          "no. 5, pp. 585–589, 2016.": "[26]\nJ. Gratch, R. Artstein, G. M. Lucas, G. Stratou, S. Scherer,"
        },
        {
          "References": "nition from speech using deep learning on spectrograms.,”\nin",
          "no. 5, pp. 585–589, 2016.": "A. Nazarian, R. Wood,\nJ. Boberg, D. DeVault, S. Marsella,"
        },
        {
          "References": "Interspeech, 2017, pp. 1089–1093.",
          "no. 5, pp. 585–589, 2016.": "et al.,\n“The distress analysis interview corpus of human and"
        },
        {
          "References": "",
          "no. 5, pp. 585–589, 2016.": "computer interviews.,” in LREC, 2014, pp. 3123–3128."
        },
        {
          "References": "[13]\nS.-L. Yeh, Y.-S. Lin, and C.-C. Lee,\n“A dialogical emotion",
          "no. 5, pp. 585–589, 2016.": ""
        },
        {
          "References": "decoder\nfor speech motion recognition in spoken dialog,”\nin",
          "no. 5, pp. 585–589, 2016.": "[27]\nS. Watanabe, T. Hori, S. Karita, T. Hayashi,\nJ. Nishitoba,"
        },
        {
          "References": "ICASSP 2020-2020 IEEE International Conference on Acous-",
          "no. 5, pp. 585–589, 2016.": "Y\n. Unno, N. E. Y. Soplin, J. Heymann, M. Wiesner, N. Chen,"
        },
        {
          "References": "tics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp.",
          "no. 5, pp. 585–589, 2016.": "arXiv\net al.,\n“Espnet: End-to-end speech processing toolkit,”"
        },
        {
          "References": "6479–6483.",
          "no. 5, pp. 585–589, 2016.": "preprint arXiv:1804.00015, 2018."
        },
        {
          "References": "[14] Y. Zong, W. Zheng, T. Zhang, and X. Huang,\n“Cross-corpus",
          "no. 5, pp. 585–589, 2016.": ""
        },
        {
          "References": "speech emotion recognition based on domain-adaptive least-",
          "no. 5, pp. 585–589, 2016.": ""
        },
        {
          "References": "squares regression,”\nIEEE signal processing letters, vol. 23,",
          "no. 5, pp. 585–589, 2016.": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Hidden markov modelbased speech emotion recognition",
      "authors": [
        "B Schuller",
        "G Rigoll",
        "M Lang"
      ],
      "year": "2003",
      "venue": "2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings.(ICASSP'03"
    },
    {
      "citation_id": "2",
      "title": "you stupid tin box\"-children interacting with the aibo robot: A cross-linguistic emotional speech corpus",
      "authors": [
        "A Batliner",
        "C Hacker",
        "S Steidl",
        "E Nöth",
        "S D'arcy",
        "M Russell",
        "M Wong"
      ],
      "year": "2004",
      "venue": "Lrec"
    },
    {
      "citation_id": "3",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Ninth European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "4",
      "title": "The enterface'05 audio-visual emotion database",
      "authors": [
        "O Martin",
        "I Kotsia",
        "B Macq",
        "I Pitas"
      ],
      "year": "2006",
      "venue": "22nd International Conference on Data Engineering Workshops (ICDEW'06)"
    },
    {
      "citation_id": "5",
      "title": "Recognizing human emotional state from audiovisual signals",
      "authors": [
        "Y Wang",
        "L Guan"
      ],
      "year": "2008",
      "venue": "IEEE transactions on multimedia"
    },
    {
      "citation_id": "6",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "7",
      "title": "Collecting large, richly annotated facial-expression databases from movies",
      "authors": [
        "A Dhall",
        "R Goecke",
        "S Lucey",
        "T Gedeon"
      ],
      "year": "2012",
      "venue": "IEEE multimedia"
    },
    {
      "citation_id": "8",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "9",
      "title": "Emotion recognition using a hierarchical binary decision tree approach",
      "authors": [
        "C.-C Lee",
        "E Mower",
        "C Busso",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2011",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "10",
      "title": "Deep neural networks for acoustic emotion recognition: Raising the benchmarks,\" in 2011 IEEE international conference on acoustics, speech and signal processing (ICASSP)",
      "authors": [
        "A Stuhlsatz",
        "C Meyer",
        "F Eyben",
        "T Zielke",
        "G Meier",
        "B Schuller"
      ],
      "year": "2011",
      "venue": "Deep neural networks for acoustic emotion recognition: Raising the benchmarks,\" in 2011 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "11",
      "title": "Speech emotion recognition using deep convolutional neural network and discriminant temporal pyramid matching",
      "authors": [
        "S Zhang",
        "S Zhang",
        "T Huang",
        "W Gao"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "12",
      "title": "Efficient emotion recognition from speech using deep learning on spectrograms",
      "authors": [
        "A Satt",
        "S Rozenberg",
        "R Hoory"
      ],
      "year": "2017",
      "venue": "Efficient emotion recognition from speech using deep learning on spectrograms"
    },
    {
      "citation_id": "13",
      "title": "A dialogical emotion decoder for speech motion recognition in spoken dialog",
      "authors": [
        "S.-L Yeh",
        "Y.-S Lin",
        "C.-C Lee"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Cross-corpus speech emotion recognition based on domain-adaptive leastsquares regression",
      "authors": [
        "Y Zong",
        "W Zheng",
        "T Zhang",
        "X Huang"
      ],
      "year": "2016",
      "venue": "IEEE signal processing letters"
    },
    {
      "citation_id": "15",
      "title": "Domain adversarial for acoustic emotion recognition",
      "authors": [
        "M Abdelwahab",
        "C Busso"
      ],
      "year": "2018",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "16",
      "title": "Unsupervised cross-corpus speech emotion recognition using domain-adaptive subspace learning",
      "authors": [
        "N Liu",
        "Y Zong",
        "B Zhang",
        "L Liu",
        "J Chen",
        "G Zhao",
        "J Zhu"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "17",
      "title": "Nonnegative matrix factorization based transfer subspace learning for cross-corpus speech emotion recognition",
      "authors": [
        "H Luo",
        "J Han"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "18",
      "title": "Recognizing more emotions with less data using self-supervised transfer learning",
      "authors": [
        "J Boigne",
        "B Liyanage",
        "T Östrem"
      ],
      "year": "2020",
      "venue": "Recognizing more emotions with less data using self-supervised transfer learning"
    },
    {
      "citation_id": "19",
      "title": "Speech enhancement using spectral subtraction-type algorithms: A comparison and simulation study",
      "authors": [
        "N Upadhyay",
        "A Karmakar"
      ],
      "year": "2015",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "20",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "21",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "22",
      "title": "Pyramidal convolution: Rethinking convolutional neural networks for visual recognition",
      "authors": [
        "I Duta",
        "L Liu",
        "F Zhu",
        "L Shao"
      ],
      "year": "2020",
      "venue": "Pyramidal convolution: Rethinking convolutional neural networks for visual recognition",
      "arxiv": "arXiv:2006.11538"
    },
    {
      "citation_id": "23",
      "title": "Attention based fully convolutional network for speech emotion recognition",
      "authors": [
        "Y Zhang",
        "J Du",
        "Z Wang",
        "J Zhang",
        "Y Tu"
      ],
      "year": "2018",
      "venue": "2018 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "24",
      "title": "Multi-time-scale convolution for emotion recognition from speech audio signals",
      "authors": [
        "E Guizzo",
        "T Weyde",
        "J Leveson"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "25",
      "title": "Generating and protecting against adversarial attacks for deep speechbased emotion recognition models",
      "authors": [
        "Z Ren",
        "A Baird",
        "J Han",
        "Z Zhang",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "26",
      "title": "The distress analysis interview corpus of human and computer interviews",
      "authors": [
        "J Gratch",
        "R Artstein",
        "G Lucas",
        "G Stratou",
        "S Scherer",
        "A Nazarian",
        "R Wood",
        "J Boberg",
        "D Devault",
        "S Marsella"
      ],
      "year": "2014",
      "venue": "LREC"
    },
    {
      "citation_id": "27",
      "title": "Espnet: End-to-end speech processing toolkit",
      "authors": [
        "S Watanabe",
        "T Hori",
        "S Karita",
        "T Hayashi",
        "J Nishitoba",
        "Y Unno",
        "N Soplin",
        "J Heymann",
        "M Wiesner",
        "N Chen"
      ],
      "year": "2018",
      "venue": "Espnet: End-to-end speech processing toolkit",
      "arxiv": "arXiv:1804.00015"
    }
  ]
}