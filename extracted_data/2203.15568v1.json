{
  "paper_id": "2203.15568v1",
  "title": "A Dataset For Speech Emotion Recognition In Greek Theatrical Plays",
  "published": "2022-03-27T21:55:59Z",
  "authors": [
    "Maria Moutti",
    "Sofia Eleftheriou",
    "Panagiotis Koromilas",
    "Theodoros Giannakopoulos"
  ],
  "keywords": [
    "GreThE dataset",
    "speech emotion recognition",
    "Greek theatrical plays",
    "valence",
    "arousal"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Machine learning methodologies can be adopted in cultural applications and propose new ways to distribute or even present the cultural content to the public. For instance, speech analytics can be adopted to automatically generate subtitles in theatrical plays, in order to (among other purposes) help people with hearing loss. Apart from a typical speech-to-text transcription with Automatic Speech Recognition (ASR), Speech Emotion Recognition (SER) can be used to automatically predict the underlying emotional content of speech dialogues in theatrical plays, and thus to provide a deeper understanding how the actors utter their lines. However, real-world datasets from theatrical plays are not available in the literature. In this work we present GreThE, the Greek Theatrical Emotion dataset, a new publicly available data collection for speech emotion recognition in Greek theatrical plays. The dataset contains utterances from various actors and plays, along with respective valence and arousal annotations. Towards this end, multiple annotators have been asked to provide their input for each speech recording and inter-annotator agreement is taken into account in the final ground truth generation. In addition, we discuss the results of some indicative experiments that have been conducted with machine and deep learning frameworks, using the dataset, along with some widely used databases in the field of speech emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The task of recognizing the underlying emotion from speech, irrespective of its semantic content, is rather important in various applications. However, it is hard to notate even by human beings, mostly due to the subjectiveness of the emotional content. The ability to automatically conduct it, is a demanding task and still an ongoing subject of research. Several opensource databases exist in the field of Speech Emotion Recognition (SER) which may contain audio-only or multimodal information and are usually annotated on two categories: categorical attributes (distinct classes of emotions)  (Ortony and Turner, 1990; Ekman, 1992)  and dimensional attributes (continuous values of valence, arousal and intensity)  (Wundt and Judd, 2011; Russell, 1980) . The emerge of the fields of Machine and Deep Learning in the past decade, gave the chance to researchers to apply research outputs on real-world problems. However, despite the fact that SER is a task that has gained great attention in the literature  (Koromilas and Giannakopoulos, 2021) , the industrial applications of the proposed works are either centered around web content (online video or podcast analysis) or have been applied on actual conversations to enrich understanding (eg. empathetic dialogue  (Ma et al., 2020) ). At the same time, cultural events are a base factor for every human civilization and, as such, they can also benefit from modern Machine Learning (ML) applications. Automatic review mining, automatic summary generation of movies, content-based movie recommendation systems, music and movie retrieval, production of subtitles/transcriptions in guided tours, movies or theatrical performances are just some examples. These applications of ML on cultural content changes the way the content is generated and distributed. Moreover, it provides solutions to increase inclusion of particular groups of the population: e.g. automatic generation of subtitles, enriched with paralinguistic attributes such as emotional arousal could help people with hearing loss to actually understand and \"feel\" a theatrical play. The widely used SER approaches have not properly been used to address challenges that are set from cultural content data, with the best example being the theatrical plays. This is mostly due to the fact that methods that are trained on SER datasets cannot be properly applied on theatrical content. This claim is based on the following facts:\n\n1. actors that perform in theatrical plays are more expressive and thus the emotional levels are aroused. That is, the arousal classes are shifted towards more energetic emotions (e.g. the weak class is expressed in a more \"aroused\" -energetic way). As far as the valence classes are concerned, they also differ to the respective valence classes in other SER datasets. For example, depending on play type (e.g. dramas) the neutral class itself can also include negative or positive emotions, compared to SER datasets that try to capture a real-world (non-theatrical) context.\n\n2. interaction with the audience make the actors express their actions in different ways so as to be better perceived by the attendants. That is, actors use emotional states that are not common in reallife conversations and thus are not included in the arXiv:2203.15568v1 [cs.SD] 27 Mar 2022\n\ngeneral SER datasets 3. the recording setups and conditions of theatrical plays differ from that of the datasets found in the literature, as the former may include complex microphone systems and fine postediting procedures.\n\nIn this work, we propose the Greek Theatrical Emotion (GreThE) dataset with the aim of filling the existing gap in the literature. GreThE is a collection of speech utterances from 23 Greek theatrical plays annotated with regards to the respective levels of emotional valence and arousal. We also provide a baseline evaluation for the presented dataset and we examine whether the domain knowledge of general SER can be used to achieve robust performance on GreThE.\n\nThe paper is organized as follows: in section 2 we report the related works on SER datasets, section 3 describes the GreThE dataset, section 4 contains the used classification methods, section 5 reports the experimental results, section 6 comment on the availability of the dataset and section 7 concludes the paper.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Speech Emotion Recognition Datasets",
      "text": "Remarkable effort has been given in the literature, to create emotion-based datasets that accurately represent the basic human emotions and reactions in speech signals. The existing datasets can be classified into four categories according to the recording procedure that is followed through the data collection process  (Koromilas and Giannakopoulos, 2021) . Specifically, these methods include one of the following: (i) spontaneous speech: the participants are unaware of the recording while their speech and reactions are recorded with hidden mechanisms in a real environment  (Cao et al., 2015) ; (ii) acted speech: the emotional condition of the speakers is acted; (iii) elicited speech: where the speaker is placed in a situation which evokes a specific emotional state  (Basu et al., 2017) ; and (iv) annotated public speech: data from public sources, such as YouTube, are annotated to associate them with a range of emotional states. Some of the most commonly used datasets in that field are: IEMOCAP  (Busso et al., 2008) , a multimodal database which includes recordings from 10 actors annotated in categorical and dimensional attributes, Emo-DB  (Burkhardt et al., 2005) , an emotional speech database containing recordings of 10 speakers that simulates 7 emotional states, MSP-podcast (Martinez-Lucas et al., 2020) that contains speech segments from podcast recordings which are annotated with emotional labels using attribute-based descriptors and categorical labels, EMOVO  (Costantini et al., 2014)  an Italian emotional speech database created by 6 actors that simulates 7 emotional state, SAVEE  (Jackson and ul haq, 2011)  database which describes the emotion in 6 distinct categories and RAVDESS  (Livingstone and Russo, 2018 ) that provides speeches of 24 actors and songs in audio and video format and includes 7 emotional expressions among with two levels of emotional intensity.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Greek Emotion Recognition",
      "text": "The respective work on Greek-based speech emotion recognition databases is limited. In particular, one of the first approaches has been introduced in the AESDD dataset  (Vryzas et al., 2018) , which is a publicly available SER database that contains utterances of acted emotional speech in the Greek language created by 5 actors and annotated with five emotional states (without containing the neutral state). Furthermore, SEWA  (Kossaifi et al., 2021 ) is a multi-lingual database for audio-visual emotion and sentiment research in the wild containing more than 2000 minutes of data of 398 people coming from 6 cultures (including Greek), annotated among others in terms of continuously valued valence and arousal.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Emotion Datasets For Cultural Content",
      "text": "As discussed in 2.1, the participation of actors in the recording of emotional databases in order to perform acted emotional speech has been a popular approach in the study of emotions. There is a range of databases that are based on actors, such as CREMA-D  (Cao et al., 2014) , CaFE  (Gournay et al., 2018) , IEMOCAP  (Busso et al., 2008) , EMOVO  (Costantini et al., 2014)  and RAVDESS  (Livingstone and Russo, 2018) .\n\nThe MSP-IMPROV corpus  (Busso et al., 2017)  is an example of the elicited speech (category (iii)) datasets.\n\nIt proposes an alternative, approach according to which the authors define hypothetical scenarios for each sentence that are carefully designed to elicit a particular emotion. Two actors improvise these emotion-specific situations, leading them to utter contextualized, nonread renditions of sentences that have fixed lexical content and convey different emotions. In this way, they manage to produce more natural behaviors. However, neither the recording conditions or the emotional reactions can be considered to be close to these of an actual theatrical play which is more expressive and differs from real-life reactions.\n\nRegarding the task of recognizing emotions in actual theatrical plays, to our knowledge, the only study in the literature is presented in  (Gloor et al., 2019) , where the authors developed a system to measure both audience and actor satisfaction during a public performance. They used smartwatches to gather physiological signals from the actors, as well as video cameras to capture facial expressions from the audience and finally speech signals from the actors to be used in SER. Then, predictions of emotions were extracted on the three channels of information using pretrained models from existing external datasets and they presented results of correlation metrics between the emotions predicted from the individual channels.\n\nTherefore, the particular work does not present a new speech emotion recognition dataset rather than it examines relationships between predicted emotions of both the audience and the actors from different channels of information. Cinematic films is a type of content with limited representation in the literature of emotion recognition. Specifically, the EMOVIE  (Cui et al., 2021)  dataset that includes 9,724 samples from seven movies with audio files annotated in the emotion polarity and the AVE  (Kadiri et al., 2014)  dataset which is based on an Indonesian Movie study  (Muljono et al., 2019)  are two of the few examples.\n\nOur proposed dataset, GreThE, aims to fulfill the gap in the literature of language resources, by proposing a publicly available non-english speech emotion recognition dataset for real-world theatrical recording conditions.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Dataset",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Audio Data Collection",
      "text": "We have selected to adopt the Audacity open source audio editing tool to manually segment at least 20 single speaker utterances from each theatrical play. This process led to 95 single-speaker utterances (90 unique speakers) from 23 Greek discrete theatrical plays, resulted to a total of 500 recordings/speeches. The total duration of speech is 46 minutes and their average duration is 5.5 seconds (the shortest utterance is 2.1 seconds and the longest utterance is 10.9 seconds).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Utterance Annotation Process",
      "text": "Each speech utterance that has been collected from the various theatrical plays, as described in Section 3.1, has been annotated with respect to its emotional content. Towards this end, we have selected to use the dimensional emotional representation of Valence and Arousal. Our goal was to adopt the standard in SER 3-class approach  (Metallinou et al., 2012) , according to which the classes for Valence are: negative, neutral and positive and the classes for Arousal are weak, neutral and strong. However, in the initial annotation process we asked the individual annotators to provide their feedback in a 5valued scale for both tasks. Then we used aggregates on these 5-scale estimates to map them to the three distinct final ground truth classes as described in Section 3. Each individual annotated the whole dataset of 500 utterances and the final ground truth has been generated by an annotation aggregation procedure described in Section 3.3.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Annotations Aggregation",
      "text": "After the previously described data annotation process, we applied an aggregation step to extract the final 3-class ground-truth label for each data point (utterance) from the initial individual 5-scale annotations. Furthermore, for each data point we calculated the agreement between the individual annotators so as to quantify the level of complexity of the adopted classification tasks (valence and arousal) for the particular dataset  (Eleftheriou et al., 2021) . Final ground truths were obtained by first calculating an average annotation rating (in the 1-5 scale for both classification tasks, as described above) and comparing it against some predefined thresholds, so as to conclude a final 3-class classification for the respective sample.\n\nIn particular, two thresholds are required to map the aggregated annotations to a 3-class taxonomy. An obvious selection would be to uniformly select the thresholds in the 1-5 range: in that case the thresholds would be T 1 = 2.33 and T 2 = 3.66, i.e. any utterance with an average annotation in the [T 1, T 2] range would be finally assigned to class \"neutral\". However, we have made some slight modifications from that uniform selection of thresholds, to satisfy the aforementioned (Section 1) shift of the theatrical utterances to more aroused emotional states. For the valence task, the T 1 was defined to be equal to 2.33 and T 2 equal to 3.33. Concerning the task of arousal, we used slightly different thresholds: the lower threshold was defined to 2.66 and the higher threshold to 3.66. This minor differentiation between the thresholds for the two tasks stems from the fact that the theatrical data are characterized by a high dominance of negative and strong emotions. So, for instance, the fact that \"weak\" arousal would be rather rare, setting the lower threshold T 1 = 2.66 instead of the \"default\" uniform value (2.33) makes it possible to decide in favor of the weak class even in borderline cases such as the following: suppose the 4 annotators have given the ratings \"weak\" (value 2), \"weak\" (value 2), \"neutral\" (value 3), \"neutral\" (value 3), the average value of these ratings is 2+2+3+3 4 = 2.5. In that case, having the uniform T 1 would make that sample be finally classified as \"neutral\", while the threshold T 1 = 2.66 finally classifies that sample as \"weak\". To sum up, the allocation of the final labels followed the below thresholding rules:\n\nwhere A i,α is the arousal annotated value for a sample i of the annotator α (in the 1-5 range) and E[A i ] is the mean annotation of sample i (similarly for valence, let V i,α be the annotation value for valence in the 1-5 range) In addition to the mean values of valence and arousal annotations that has been used to extract the final ground truths, through the thresholding steps described above, a deviation threshold is also considered, in order to filter out controversial annotations. To this end, the mean absolute deviation (M AD) of the annotations of each sample is calculated for each annotation X for both tasks:\n\nThis is obviously a metric of inter-annotator agreement for the given sample, therefore samples with high M AD values should be excluded from the dataset. This rule is used as a safety net step to exclude possible examples with extreme inter-annotator disagreements. Inter-annotator agreement: Apart from using M AD to filter out possible highly questionable utterances, we also used it to compute the overall interannotator (dis)agreement for each of the classification tasks. High values of average disagreement would have indicated that annotators had different points of view on the ratings. On top of the total disagreement for all data, the average disagreement for each annotator was also calculated, to evaluate each human annotator compared to the overall ground truth. To this end, we first calculated the disagreement of the annotation of annotator a for the i-th sample. Let A i,a be the value of that annotation (in the 1-5 range), and M be the total number of annotators. Then the deviation of A i,a from the average value of all other annotators for the same sample is:\n\nThe results of the filtering procedure and the calculated agreement metrics for the arousal and valence task are listed in Table  1 . It has to be noted that the average disagreement of both tasks is below 0.5 (0.48 and 0.49 respectively), which is half the size of the neutral class.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Baseline Classification Methods",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Traditional Machine-Learning-Based Approach",
      "text": "As a baseline audio classification technique, we have selected to use a set of handcrafted audio features from the time, spectral and cepstral domains, such as Zero Crossing Rate, Spectral Centroid and Mel Frequency Cepstral Coefficients (MFCCs), along with Support Vector Machines as classifiers. In particular, each speech utterance is first split into a sequence of nonoverlapping 50 msec short-term windows (frames), and for each frame a set of 68 audio features is computed. At this stage, each speech utterance is represented by a sequence of short-term feature vectors (short-term representation).\n\nThen, the mean and standard deviation of these features are extracted in a long-term segment of 3 seconds, using 1 second step. According to that, each utterance is represented by a sequence of (68 x 2 = 136) feature statistics. Finally, we apply a long-term averaging step, which results in a single-vector 168-D representation for the whole utterance (long-term representation). Note that both short-term (matrix) and long-term (vector) representations are provided in the repository of the dataset.\n\nAs a baseline classification method we have experimented with training and evaluating an SVM classifier with an RBF kernel, using the long-term vector representation described above. All respective experiments and feature extraction procedures have been carried out with the pyAudioAnalysis library  (Giannakopoulos, 2015) .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Deep-Learning-Based Approach",
      "text": "Using audio spectrograms (or mel-spectrograms) as image inputs to Convolutional Neural Networks (CNNs) is a widely adopted approach in the literature of emotion recognition  (Koromilas and Giannakopoulos, 2021) . In this work we incorporate this methodology as a baseline from the field of Deep Learning. While this method is expected to underperform when applied to small datasets, it is perfectly suited for testing whether knowledge from large amount of emotion data can be applied to our proposed dataset. For that reason we use the deep audio features library to extract mel-spectrograms and train a CNN for our two speech emotion recognition tasks. The traditional feature extraction and SVM approach described in Section 4.1, has been evaluated using a repeated random shuffling train/validation split. The \"session\" ID used to split the data was based on the ID of the theatrical play. In this way, we guarantee that the evaluation results we report in the Results Section are not assuming dependence on the theatrical plays and are therefore realistic. Note, that this ID is provided with the dataset, as well. Moreover, we have used the aforementioned validation strategy to experiment against different values of the C parameter. Finally, since the datasets is imbalanced in both classification tasks (valence and arousal), we have adopted a data balancing step using either a basic random subsampling (of the dominant classes) or SMOTE oversampling (synthetic minority over-sampling technique,  (Chawla et al., 2002) ).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Grethe",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Cross-Domain Validation",
      "text": "Intending to examine whether the domain knowledge of Speech Emotion Recognition can be transferred on our dataset, we train a CNN in the way described in 4.2, on two widely used datasets, the MSP-podcast  (Martinez-Lucas et al., 2020)  and IEMOCAP  (Busso et al., 2008) . Following  (Metallinou et al., 2012)  we define two three-class problems on these datasets, namely arousal (or valence) with classes That is, none of the GreThE instances are used for the CNNs' training, and thus the used models are completely unaware of the recording conditions and acted speech of a theatrical play.\n\nOur CNN architecture consists of 4 convolutional and 3 linear layers, including batch normalization  (Ioffe and Szegedy, 2015)  and the LeakyRelu activation function.\n\nWe trained the model using the Cross-Entropy loss as loss function, while Adam was chosen to be the optimizer with initial learning rate of 0.002 and a reduce-on-plateau learning rate scheduler.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Baseline Models",
      "text": "In order to compare our methods with the threshold of a random classifier, one randomized classifier is defined for each of the two approaches. Specifically, since the traditional machine-learning-based approach (section 4.1) is trained on the GreThE dataset and thus is aware of the sample distribution, it can only be compared to a prior-aware randomized (prior-aware baseline) classifier, ie. a classifier that randomly predicts class a based on the prior probability of class a in the dataset distribution. On the other hand, the crossdomain trained models, as defined in section 5.1.2, are not familiar with GreThE and thus their predictions can only be compared with a completely randomized (baseline) classifier.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results",
      "text": "In table 2 we report the f1 metrics that emerged from our evaluation.\n\nAs can be clearly inferred from the provided results, easily verified by the fact that the actors expressiveness in theatrical plays usually result in increased arousal, shifting (in terms of energy and frequency) the weak and neutral classes towards the strong one.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Dataset Availability",
      "text": "The dataset is public available at https:// github.com/magcil/GreThE. Each utterance is represented by either (a) a sequence of shortterm feature vectors or (b) a spectrogram. Groundtruth is provided in a simple tabular CSV format for both classification tasks (valence and arousal).\n\nFinally, the repository also contains Python scripts that demonstrate (a) the aforementioned SVM experimental setup and (b) the adopted feature extraction methods (so that reproducability can also be made possible and combined with other sources of data).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper we presented the Greek Theatrical dataset GreThE, a new publicly available data collection for speech emotion recognition in Greek theatrical plays. The dataset contains 500 utterances that have been annotated in terms of their emotional content (valence and arousal). Multiple-annotator data have been used to assure annotation quality. To our knowledge, this is the first dataset with real-world speech data from theatrical plays annotated in terms of the underlying emotion. Also, we have presented classification performance results for a baseline machine learning classification approach cross-validated on GreThE, along with results using GreThE as a test dataset for cross-domain deep emotional models, trained on popular datasets of the English language. Results have proven that (a) the task of recognising emotion -and mostly valenceis rather challenging in theatrical data when training from scratch (b) using state-of-the-art datasets from generic SER on cross-language theatrical data is not effective. This indicates that future works in the field of recognizing emotions in theatrical data should probably consider robust domain adaptation techniques using few-shot learning strategies.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Screen shot from the LabelStudio project",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table 2: we report the f1 metrics that emerged from",
      "data": [
        {
          "GreThE": "Arousal"
        },
        {
          "GreThE": "Strong"
        },
        {
          "GreThE": "µ >= 3.66"
        },
        {
          "GreThE": "227"
        },
        {
          "GreThE": "σ < 1.3"
        },
        {
          "GreThE": "227"
        },
        {
          "GreThE": "0.48"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Experiment": "Baseline\nPrior-aware Baseline",
          "Arousal F1": "27%\n31%",
          "Valence F1": "26%\n30%"
        },
        {
          "Experiment": "SVM\nSVM - Oversampling\nSVM - Undersampling",
          "Arousal F1": "53%\n55%\n54%",
          "Valence F1": "38%\n40%\n39%"
        },
        {
          "Experiment": "CNN iemocap\nCNN msp\nCNN merged",
          "Arousal F1": "40%\n36%\n41%",
          "Valence F1": "37%\n34%\n34%"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Bibliographical References",
      "venue": "Bibliographical References"
    },
    {
      "citation_id": "2",
      "title": "A review on emotion recognition using speech",
      "authors": [
        "S Basu",
        "J Chakraborty",
        "A Bag",
        "M Aftabuddin"
      ],
      "year": "2017",
      "venue": "2017 International conference on inventive communication and computational technologies (ICICCT)"
    },
    {
      "citation_id": "3",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "A database of german emotional speech"
    },
    {
      "citation_id": "4",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower Provost",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "5",
      "title": "Msp-improv: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "C Busso",
        "S Parthasarathy",
        "A Burmania",
        "M Abdelwahab",
        "N Sadoughi",
        "E Provost"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "Crema-d: Crowdsourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "7",
      "title": "Speakersensitive emotion recognition via ranking: Studies on acted and spontaneous speech",
      "authors": [
        "H Cao",
        "R Verma",
        "A Nenkova"
      ],
      "year": "2015",
      "venue": "Computer speech & language"
    },
    {
      "citation_id": "8",
      "title": "Smote: synthetic minority over-sampling technique",
      "authors": [
        "N Chawla",
        "K Bowyer",
        "L Hall",
        "W Kegelmeyer"
      ],
      "year": "2002",
      "venue": "Journal of artificial intelligence research"
    },
    {
      "citation_id": "9",
      "title": "Emovo corpus: an italian emotional speech database",
      "authors": [
        "G Costantini",
        "I Iaderola",
        "A Paoloni",
        "M Todisco"
      ],
      "year": "2014",
      "venue": "LREC"
    },
    {
      "citation_id": "10",
      "title": "Emovie: A mandarin emotion speech dataset with a simple emotional textto-speech model",
      "authors": [
        "C Cui",
        "Y Ren",
        "J Liu",
        "F Chen",
        "R Huang",
        "M Lei",
        "Z Zhao"
      ],
      "year": "2021",
      "venue": "Emovie: A mandarin emotion speech dataset with a simple emotional textto-speech model"
    },
    {
      "citation_id": "11",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & Emotion"
    },
    {
      "citation_id": "12",
      "title": "Automatic assessment of speaking skills using aural and textual information",
      "authors": [
        "S Eleftheriou",
        "P Koromilas",
        "T Giannakopoulos"
      ],
      "year": "2021",
      "venue": "Proceedings of The Fourth International Conference on Natural Language and Speech Processing (ICNLSP 2021)"
    },
    {
      "citation_id": "13",
      "title": "pyaudioanalysis: An open-source python library for audio signal analysis",
      "authors": [
        "T Giannakopoulos"
      ],
      "year": "2015",
      "venue": "PloS one"
    },
    {
      "citation_id": "14",
      "title": "Measuring audience and actor emotions at a theater play through automatic emotion recognition from face, speech, and body sensors",
      "authors": [
        "P Gloor",
        "K Araño",
        "E Guerrazzi"
      ],
      "year": "2019",
      "venue": "Measuring audience and actor emotions at a theater play through automatic emotion recognition from face, speech, and body sensors"
    },
    {
      "citation_id": "15",
      "title": "A canadian french emotional speech dataset",
      "authors": [
        "P Gournay",
        "O Lahaie",
        "R Lefebvre"
      ],
      "year": "2018",
      "venue": "Proceedings of the 9th ACM Multimedia Systems Conference, MMSys '18"
    },
    {
      "citation_id": "16",
      "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "authors": [
        "S Ioffe",
        "C Szegedy"
      ],
      "year": "2015",
      "venue": "Proceedings of the 32nd International Conference on International Conference on Machine Learning"
    },
    {
      "citation_id": "17",
      "title": "Surrey audio-visual expressed emotion (savee) database",
      "authors": [
        "P Jackson"
      ],
      "year": "2011",
      "venue": "Surrey audio-visual expressed emotion (savee) database"
    },
    {
      "citation_id": "18",
      "title": "Naturalistic audiovisual emotion database",
      "authors": [
        "S Kadiri",
        "P Gangamohan",
        "V Mittal",
        "B Yegnanarayana"
      ],
      "year": "2014",
      "venue": "ICON"
    },
    {
      "citation_id": "19",
      "title": "Deep multimodal emotion recognition on human speech: A review",
      "authors": [
        "P Koromilas",
        "T Giannakopoulos"
      ],
      "year": "2021",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "20",
      "title": "Sewa db: A rich database for audio-visual emotion and sentiment research in the wild",
      "authors": [
        "J Kossaifi",
        "R Walecki",
        "Y Panagakis",
        "J Shen",
        "M Schmitt",
        "F Ringeval",
        "J Han",
        "V Pandit",
        "A Toisoul",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "21",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PLOS ONE"
    },
    {
      "citation_id": "22",
      "title": "A survey on empathetic dialogue systems",
      "authors": [
        "Y Ma",
        "K Nguyen",
        "F Xing",
        "E Cambria"
      ],
      "year": "2020",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "23",
      "title": "The msp-conversation corpus",
      "authors": [
        "L Martinez-Lucas",
        "M Abdelwahab",
        "C Busso"
      ],
      "year": "2020",
      "venue": "The msp-conversation corpus"
    },
    {
      "citation_id": "24",
      "title": "Contextsensitive learning for enhanced audiovisual emotion classification",
      "authors": [
        "A Metallinou",
        "M Wollmer",
        "A Katsamanis",
        "F Eyben",
        "B Schuller",
        "S Narayanan"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "25",
      "title": "Speech emotion recognition of indonesian movie audio tracks based on mfcc and svm",
      "authors": [
        "Prasetya Muljono",
        "M Harjoko",
        "A Supriyanto"
      ],
      "year": "2019",
      "venue": "2019 International Conference on contemporary Computing and Informatics (IC3I)"
    },
    {
      "citation_id": "26",
      "title": "What's basic about basic emotions?",
      "authors": [
        "A Ortony",
        "T Turner"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "27",
      "title": "Speech emotion recognition for performance interaction",
      "authors": [
        "N Vryzas",
        "R Kotsakis",
        "A Liatsou",
        "C Dimoulas",
        "G Kalliris"
      ],
      "year": "2018",
      "venue": "Journal of the Audio Engineering Society. Audio Engineering Society"
    },
    {
      "citation_id": "28",
      "title": "Outlines of Psychology",
      "authors": [
        "W Wundt",
        "C Judd"
      ],
      "year": "2011",
      "venue": "Outlines of Psychology"
    }
  ]
}