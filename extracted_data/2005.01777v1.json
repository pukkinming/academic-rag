{
  "paper_id": "2005.01777v1",
  "title": "Adviser: A Toolkit For Developing Multi-Modal, Multi-Domain And Socially-Engaged Conversational Agents",
  "published": "2020-05-04T18:27:58Z",
  "authors": [
    "Chia-Yu Li",
    "Daniel Ortega",
    "Dirk Väth",
    "Florian Lux",
    "Lindsey Vanderlyn",
    "Maximilian Schmidt",
    "Michael Neumann",
    "Moritz Völkel",
    "Pavel Denisov",
    "Sabrina Jenne",
    "Zorica Kacarevic",
    "Ngoc Thang Vu"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We present ADVISER 1 -an open-source, multi-domain dialog system toolkit that enables the development of multi-modal (incorporating speech, text and vision), sociallyengaged (e.g. emotion recognition, engagement level prediction and backchanneling) conversational agents. The final Python-based implementation of our toolkit is flexible, easy to use, and easy to extend not only for technically experienced users, such as machine learning researchers, but also for less technically experienced users, such as linguists or cognitive scientists, thereby providing a flexible platform for collaborative research.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Dialog systems or chatbots, both text-based and multi-modal, have received much attention in recent years, with an increasing number of dialog systems in both industrial contexts such as Amazon Alexa, Apple Siri, Microsoft Cortana, Google Duplex, XiaoIce  (Zhou et al., 2018)  and Furhat 2  , as well as academia such as MuMMER  (Foster et al., 2016)  and Alana  (Curry et al., 2018) . However, open-source toolkits and frameworks for developing such systems are rare, especially for developing multi-modal systems comprised of speech, text, and vision. Most of the existing toolkits are designed for developing dialog systems focused only on core dialog components, with or without the option to access external speech processing services  (Bohus and Rudnicky, 2009; Baumann and Schlangen, 2012; Lison and Kennington, 2016; Ultes et al., 2017; Ortega et al., 2019; Lee et al., 2019) .\n\nTo the best of our knowledge, there are only two toolkits, proposed in  (Foster et al., 2016)  and  (Bohus et al., 2017) , that support developing dialog agents using multi-modal processing and social signals  (Wagner et al., 2013) . Both provide a decent platform for building systems, however, to the best of our knowledge, the former is not open-source, and the latter is based on the .NET platform, which could be less convenient for non-technical users such as linguists and cognitive scientists, who play an important role in dialog research.\n\nIn this paper, we introduce a new version of ADVISER -previously a text-based, multi-domain dialog system toolkit  (Ortega et al., 2019)  -that supports multi-modal dialogs, including speech, text and vision information processing. This provides a new option for building dialog systems that is open-source and Python-based for easy use and fast prototyping. The toolkit is designed in such a way that it is modular, flexible, transparent, and user-friendly for both technically experienced and less technically experienced users.\n\nFurthermore, we add novel features to AD-VISER, allowing it to process social signals and to incorporate them into the dialog flow. We believe that these features will be key to developing humanlike dialog systems because it is well-known that social signals, such as emotional states and engagement levels, play an important role in human computer interaction  (McTear et al., 2016) . However in contrast to open-ended dialog systems  (Weizenbaum, 1966) , our toolkit focuses on task-oriented applications  (Bobrow et al., 1977) , such as searching for a lecturer at the university  (Ortega et al., 2019) . The purpose we envision for dialog systems developed using our toolkit is not the same as the objective of a social chatbot such as XiaoIce  (Zhou et al., 2018) . Rather than promoting \"an AI companion with an emotional connection to satisfy the human need for communication, affection, and social belonging\"  (Zhou et al., 2018) , ADVISER helps develop dialog systems that support users in efficiently fulfilling concrete goals, while at the same time considering social signals such as emotional states and engagement levels so as to remain friendly and likeable.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Objectives",
      "text": "The main objective of this work is to develop a multi-domain dialog system toolkit that allows for multi-modal information processing and that provides different modules for extracting social signals such as emotional states and for integrating them into the decision making process. The toolkit should be easy to use and extend for users of all levels of technical experience, providing a flexible collaborative research platform.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Toolkit Design",
      "text": "We extend and substantially modify our previous, text-based dialog system toolkit  (Ortega et al., 2019)  while following the same design choices. This means that our toolkit is meant to optimize the following four criteria: Modularity, Flexibility, Transparency and User-friendliness at different levels. This is accomplished by decomposing the dialog system into independent modules (services), which in turn are either rule-based, machine learning-based or both. These services can easily be combined in different orders/architectures, providing users with flexible options to design new dialog architectures.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Challenges & Proposed Solutions",
      "text": "Multi-modality The main challenges in handling multi-modality are a) the design of a synchronization infrastructure and b) the large range of different latencies from different modalities. To alleviate the former, we use the publisher/subscriber software pattern presented in section 4 to synchronize signals coming from different sources. This software pattern also allows for services to run in a distributed manner. By assigning computationally heavy tasks such as speech recognition and speech synthesis to a more powerful computing node, it is possible to reduce differences in latency when processing different modalities, therefore achieving more natural interactions.\n\nSocially-Engaged Systems Determining the ideal scope of a socially-engaged dialog system is a complex issue, that is which information should be extracted from users and how the system can best react to these signals. Here we focus on two major social signals: emotional states and engagement levels (see section 3.1), and maintain an internal user state to track them over the course of a dialog. Note that the toolkit is designed in such a way that any social signal could be extracted and leveraged in the dialog manager. In order to react to social signals extracted from the user, we provide an initial affective policy module (see section 3.5) and an initial affective NLG module (see section 3.7), which could be easily extended to more sophisticated behavior. Furthermore, we provide a backchanneling module that enables the dialog system to give feedback to users during conversations. Utilizing these features could lead to increased trust and enhance the impression of an empathetic system.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Functionalities",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Social Signal Processing",
      "text": "We present the three modules of ADVISER for processing social signals: (a) emotion recognition, (b) engagement level prediction, and (c) backchanneling. Figure  1  illustrates an example of our system tracking emotion states and engagement levels.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multi-Modal Emotion Recognition",
      "text": "For recognizing a user's emotional state, all three available modalities -text, audio, and vision -can potentially be exploited, as they can deliver complementary information  (Zeng et al., 2009) . Therefore, the emotion recognition module can subscribe to the particular input streams of interest (see section 4 for details) and apply emotion prediction either in a time-continuous fashion or discretely per turn.\n\nIn our example implementation in the toolkit, we integrate speech emotion recognition, i.e. using the acoustic signal as features. Based on the work presented in  (Neumann and Vu, 2017)  we use log Mel filterbank coefficients as input to convo-lutional neural networks (CNNs). For the sake of modularity, three separate models are employed for predicting different types of labels: (a) basic emotions {angry, happy, neutral, sad}, (b) arousal levels {low, medium, high}, and (c) valence levels {negative, neutral, positive}. The models are trained on the IEMOCAP dataset  (Busso et al., 2008) . The output of the emotion recognition module consists of three predictions per user turn, which can then be used by the user state tracker (see section 3.4). For future releases, we plan to incorporate multiple training datasets as well as visual features.\n\nEngagement Level Prediction User engagement is closely related to states such as boredom and level of interest, with implications for user satisfaction and task success  (Forbes-Riley et al., 2012; Schuller et al., 2009) . In ADVISER, we assume that eye activity serves as an indicator of various mental states  (Schuller et al., 2009; Niu et al., 2018)  and implement a gaze tracker that monitors the user's direction of focus via webcam.\n\nUsing OpenFace 2.2.0, a toolkit for facial behavior analysis  (Baltrusaitis et al., 2018) , we extract the features gaze angle x and gaze angle y, which capture left-right and up-down eye movement, for each frame and compute the deviation from the central point of the screen. If the deviation exceeds a certain threshold for a certain number of seconds, the user is assumed to look away from the screen, thereby disengaging. Thus, the output of our engagement level prediction module is the binary decision {looking, not looking}. Both the spatial and temporal sensitivity can be adjusted, such that developers have the option to decide how far and how long the user's gaze can stray from the central point until they are considered to be disengaged. In an adaptive system, this information could be used to select re-engagement strategies, e.g. using an affective template (see section 3.7).\n\nBackchanneling In a conversation, a backchannel (BC) is a soft interjection from the listener to the speaker, with the purpose of signaling acknowledgment or reacting to what was just uttered. Backchannels contribute to a successful conversation flow  (Clark and Krych, 2004) . Therefore, we add an acoustic backchannel module to create a more human-like dialog experience. For backchannel prediction, we extract 13 Mel-frequency-cepstral coefficients from the user's speech signal, which form the input to the convolutional neural network based on  Ortega et al. (2020) . The model assigns one of three categories from the proactive backchanneling theory  (Goodwin, 1986)  to each user utterance {no-backchannel, backchannel-continuer and backchannel-assessment}. The predicted category is used to add the backchannel realization, such as Right or Uh-huh, to the next system response.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Speech Processing",
      "text": "Automatic Speech Recognition (ASR) The speech recognition module receives a speech signal as input, which can come from an internal or external microphone, and outputs decoded text. The specific realization of ASR can be interchanged or adapted, for example for new languages or different ASR methods. We provide an end-to-end ASR model for English based on the Transformer neural network architecture. We use the end-to-end speech processing toolkit ESPnet  (Watanabe et al., 2018)  and the IMS-speech English multi-dataset recipe  (Denisov and Vu, 2019) , updated to match the LibriSpeech Transformer-based system in ESPnet  (Karita et al., 2019)  and to include more training data. Training data comprises the LibriSpeech, Switchboard, TED-LIUM 3, AMI, WSJ, Common Voice 3, SWC, VoxForge and M-AILABS datasets with a total amount of 3249 hours. As input features, 80-dimensional log Mel filterbank coefficients are used. Output of the ASR model is a sequence of subword units, which include single characters as well as combinations of several characters, making the model lexicon independent.\n\nSpeech Synthesis For ADVISER's voice output, we use the ESPnet-TTS toolkit  (Hayashi et al., 2019) , which is an extension of the ESPnet toolkit mentioned above. We use FastSpeech as the synthesis model speeding up mel-spectrogram generation by a factor of 270 and voice generation by a factor of 38 compared to autoregressive Transformer TTS  (Ren et al., 2019) . We use a Parallel Wave-GAN  (Yamamoto et al., 2020)  to generate waveforms that is computationally efficient and achieves a high mean opinion score of 4.16. The FastSpeech and WaveGAN models were trained with 24 hours of the LJSpeech dataset from a single speaker  (Ito, 2017)  and are capable of generating voice output in real-time when using a GPU. The synthesis can run on any device in a distributed system. Additionally, we optimize the synthesizer for abbreviations, such as Prof., Univ., IMS, NLP, ECTS and PhD, as well as for German proper names, such as street names. These optimizations can be easily extended.\n\nTurn Taking To make interacting with the system more natural, we use a naive end-of-utterance detection. Users indicate the start of their turn by pressing a hotkey, so they can choose to pause the interaction. The highest absolute peak of each recording chunk is then compared with a predefined threshold. If a certain number of sequential chunks do not peak above the threshold, the recording stops. We are currenlty in the process of planning more sophisticated turn taking models, such as  Skantze et al. (2015) .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Natural Language Understanding",
      "text": "The natural language understanding (NLU) unit parses the textual user input (De  Mori et al., 2008)  -or the output from the speech recognition systemand extracts the user action type, generally referred to as intent in goal-oriented dialog systems (e.g. Inform and Request), as well as the corresponding slots and values. The domain-independent, rulebased NLU presented in Ortega et al. (  2019 ) is integrated into ADVISER and adapted to the new domains presented in section 5.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "State Tracking",
      "text": "Belief State Tracking (BST): The BST tracks the history of user informs and the user action types, requests, with one BST entry per turn. This information is stored in a dictionary structure that is built up, as the user provides more details and the system has a better understanding of user intent.\n\nUser State Tracking (UST): Similar to the BST, the UST tracks the history of the user's state over the course of a dialog, with one entry per turn. In the current implementation, the user state consists of the user's engagement level, valence, arousal, and emotion category (details in section 3.1).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Dialog Policies",
      "text": "Policies To determine the correct system action, we provide three types of policy services: a handcrafted and a reinforcement learning policy for finding entities from a database  (Ortega et al., 2019) , as well as a handcrafted policy for looking up information through an API call. Both handcrafted policies use a series of rules to help the user find a single entity or, once an entity has been found (or directly provided by the user), find information about that entity. The reinforcement learning (RL) policy's action-value function is approximated by a neural network which outputs a value for each possible system action, given the vectorized representation of a turn's belief state as input. The neural network is constructed as proposed in  Väth and Vu (2019)  following a duelling architecture  (Wang et al., 2016) . It consists of two separate calculation streams, each with its own layers, where the final layer yields the action-value function. For off-policy batch-training, we make use of prioritized experience replay  (Schaul et al., 2015) .\n\nAffective Policy In addition, we have also implemented a rule-based affective policy service that can be used to determine the system's emotional response. As this policy is domain-agnostic, predicting the next system emotion output rather than the next system action, it can be used alongside any of the previously mentioned policies.\n\nUser Simulator To support automatic evaluation and to train the RL policy, we provide a user simulator service outputting at the user acts level. As we are concerned with task-oriented dialogs here, the user simulator has an agenda-based  (Schatzmann et al., 2007)  architecture and is randomly assigned a goal at the beginning of the dialog. Each turn, it then works to first respond to the system utterance, and then after to fulfill its own goal. When the system utterance also works toward fulfilling the user goal, the RL policy is rewarded by achieving a shorter total dialog turn count  (Ortega et al., 2019) .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "External Information Resources",
      "text": "ADVISER supports three options to access information from external information sources. In addition to being able to query information from SQL-based databases, we add two new options that includes querying information via APIs and from knowledge bases (e.g. Wikidata  (Vrandečić and Krötzsch, 2014) ). For example, when a user asks a simple question -Where was Dirk Nowitzki born?, our pretrained neural network predicts the topic entity -Dirk Nowitzki -and the relation -place of birth.\n\nThen, the answer is automatically looked up using Wikidata's SPARQL endpoint.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Natural Language Generation (Nlg)",
      "text": "In the NLG service, the semantic representation of the system act is transformed into natural language. ADVISER currently uses a template-based approach to NLG in which each possible system act is mapped to exactly one utterance. A special syntax using placeholders reduces the number of templates needed and accounts for correct morphological inflections  (Ortega et al., 2019) . Additionally, we developed an affective NLG service, which allows for different templates to be used depending on the user's emotional state. This enables a more sensitive/adaptive system. For example, if the user is sad and the system does not understand the user's input, it might try to establish common ground to prevent their mood from getting worse due to the bad news. An example response would be \"As much as I would love to help, I am a bit confused\" rather than the more neutral \"Sorry I am a bit confused\". One set of NLG templates can be specified for each possible emotional state. At runtime, the utterance is then generated from the template associated with the current system emotion and system action.\n\n4 Software Architecture",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Dialog As A Collection Of Services",
      "text": "To allow for maximum flexibility in combining and reusing components, we consider a dialog system as a group of services which communicate asynchronously by publishing/subscribing to certain topics. A service is called as soon as at least one message for all its subscribed topics is received and may additionally publish to one or more top-ics. Services can elect to receive the most recent message for a topic (e.g. up-to-date belief state) or a list of all messages for that topic since the last service call (e.g. a list of video frames). Constructing a dialog system in this way allows us to break free from a pipeline architecture. Each step in the dialog process is represented by one or more services which can operate in parallel or sequentially. For example, tasks like video and speech capture may be performed and processed in parallel before being synchronized by a user state tracking module subscribing to input from both sources. Figure  2  illustrates the system architecture. For debugging purposes, we provide a utility to draw the dialog graph, showing the information flow between services, including remote services, and any inconsistencies in publish/subscribe connections.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Support For Distributed Systems",
      "text": "Services are location-transparent and may thus be distributed across multiple machines. A central dialog system discovers local and remote services and provides synchronization guarantees for dialog initialization and termination. Distribution of services enables, for instance, a more powerful computer to handle tasks such as real-time text-to-speech generation (see Figure  2 ). This is particularly helpful when multiple resource-heavy tasks are combined into a single dialog system.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Support For Multi-Domain Systems",
      "text": "In addition to providing multi-modal support, the publish/subscribe framework also allows for multidomain support by providing a structure which enables arbitrary branching and rejoining of graph structures. When a service is created, users simply specify which domain(s) it should publish/subscribe to. This, in combination with a domain tracking service, allows for seamless integration of domain-agnostic services (such as speech input/output) and domain-specific services (such as NLU/NLG for the lecturers domain).\n\n5 Example Use Cases",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Example Domains",
      "text": "We provide several example domains to demonstrate ADVISER's functionalities. Databases for lecturers and courses at the Institute for Natural Language Processing (IMS), which we used in the previous version of ADVISER, were adapted to the new system architecture. As example APIs, we implemented a weather domain that makes calls to the OpenWeatherMap API 3  and a mensa domain for gathering information from the dining hall at the university of Stuttgart. Note that affective templates were only added to the lecturers and mensa domain. All domains can be used within the same dialog, simply by switching the topic.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "An Example Implementation",
      "text": "Our toolkit allows for easy creation of a dialog system within a few lines of code as follows.\n\n# As a first step, a dialog system object is initialized, which is responsible for coordinating the initialization and graceful termination of all dialog services. Talking about multiple domains in one dialog is enabled by creating a simple keywordbased domain tracker which is introduced as the first argument to the dialog system. To make the dialog multi-modal, speech and vision modules are introduced next, along with modules to extract engagement and emotion. So far, all of these modules are domain-agnostic and can be used as shared resources between all domains. Next, domaindependent services such as NLUs, BSTs and NLGs for weather and mensa, are added. The following shows an example dialog. SYSTEM: Hello, please let me know how I can help you, I can discuss the following domains: Mensa Food and Weather. USER: I could have something to eat. What does the mensa offer today? SYSTEM: What type of dish are you looking for? USER: I would like a main dish. SYSTEM: Should the meal be vegan? USER: Yes. SYSTEM: The meal mediterranean Ebly wheat is served today, is a main dish and is vegan. USER: Okay, cool, I will go there now! What is the weather like?",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Other tools for building dialog systems include ConvLab  (Lee et al., 2019) , an open-source, textbased dialog system platform that supports both pipelined architectures and an end-to-end neural architecture. ConvLab also provides reusable components and supports multi-domain settings. Other systems are largely text-based, but offer the incorporation of external speech components. In-proTK  (Baumann and Schlangen, 2012) , for instance, in which modules communicate by networks via configuration files, uses ASR based on Sphinx-4 and synthesis based on MaryTTS. Similarly, RavenClaw  (Bohus and Rudnicky, 2009)  provides a framework for creating dialog managers; ASR and synthesis components can be supplied, for example, by connecting to Sphinx and Kalliope.\n\nOpenDial  (Lison and Kennington, 2016)  relies on probabilistic rules and provides options to connect to speech components such as Sphinx. Multidomain dialog toolkit -PyDial  (Ultes et al., 2017)  supports connection to DialPort. As mentioned in the introduction, Microsoft Research's \\psi is an open and extensible platform that supports the development of multi-modal AI systems  (Bohus et al., 2017) . It further offers audio and visual processing, such as speech recognition and face tracking, as well as output, such as synthesis and avatar rendering. And the MuMMER (multimodal Mall Entertainment Robot) project  (Foster et al., 2016)  is based on the SoftBank Robotics Pepper platform, and thereby comprises processing of audio-, visual-and social signals, with the aim to develop a socially engaging robot that can be deployed in public spaces.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Conclusions",
      "text": "We introduce ADVISER -an open-source, multidomain dialog system toolkit that allows users to easily develop multi-modal and socially-engaged conversational agents. We provide a large variety of functionalities, ranging from speech processing to core dialog system capabilities and social signal processing. With this toolkit, we hope to provide a flexible platform for collaborative research in multi-domain, multi-modal, socially-engaged conversational agents.",
      "page_start": 1,
      "page_end": 1
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Tracking emotion states and engagement lev-",
      "page": 2
    },
    {
      "caption": "Figure 1: illustrates an example of our system",
      "page": 2
    },
    {
      "caption": "Figure 2: Example ADVISER toolkit conﬁguration:",
      "page": 5
    },
    {
      "caption": "Figure 2: illustrates the system architecture. For debugging",
      "page": 5
    },
    {
      "caption": "Figure 2: ). This is particularly helpful",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "dialog system toolkit (Ortega et al., 2019) - that": ""
        },
        {
          "dialog system toolkit (Ortega et al., 2019) - that": "supports multi-modal dialogs,\nincluding speech,"
        },
        {
          "dialog system toolkit (Ortega et al., 2019) - that": "text and vision information processing. This pro-"
        },
        {
          "dialog system toolkit (Ortega et al., 2019) - that": "vides a new option for building dialog systems that"
        },
        {
          "dialog system toolkit (Ortega et al., 2019) - that": "is open-source and Python-based for easy use and"
        },
        {
          "dialog system toolkit (Ortega et al., 2019) - that": ""
        },
        {
          "dialog system toolkit (Ortega et al., 2019) - that": "fast prototyping. The toolkit\nis designed in such"
        },
        {
          "dialog system toolkit (Ortega et al., 2019) - that": ""
        },
        {
          "dialog system toolkit (Ortega et al., 2019) - that": "a way that it is modular, ﬂexible, transparent, and"
        },
        {
          "dialog system toolkit (Ortega et al., 2019) - that": ""
        },
        {
          "dialog system toolkit (Ortega et al., 2019) - that": "user-friendly for both technically experienced and"
        },
        {
          "dialog system toolkit (Ortega et al., 2019) - that": ""
        },
        {
          "dialog system toolkit (Ortega et al., 2019) - that": "less technically experienced users."
        },
        {
          "dialog system toolkit (Ortega et al., 2019) - that": ""
        },
        {
          "dialog system toolkit (Ortega et al., 2019) - that": "Furthermore, we\nadd novel\nfeatures\nto AD-"
        },
        {
          "dialog system toolkit (Ortega et al., 2019) - that": ""
        },
        {
          "dialog system toolkit (Ortega et al., 2019) - that": "VISER, allowing it to process social signals and to"
        },
        {
          "dialog system toolkit (Ortega et al., 2019) - that": ""
        },
        {
          "dialog system toolkit (Ortega et al., 2019) - that": "incorporate them into the dialog ﬂow. We believe"
        },
        {
          "dialog system toolkit (Ortega et al., 2019) - that": ""
        },
        {
          "dialog system toolkit (Ortega et al., 2019) - that": "that these features will be key to developing human-"
        },
        {
          "dialog system toolkit (Ortega et al., 2019) - that": ""
        },
        {
          "dialog system toolkit (Ortega et al., 2019) - that": "like dialog systems because it is well-known that"
        },
        {
          "dialog system toolkit (Ortega et al., 2019) - that": ""
        },
        {
          "dialog system toolkit (Ortega et al., 2019) - that": "social signals, such as emotional states and engage-"
        },
        {
          "dialog system toolkit (Ortega et al., 2019) - that": ""
        },
        {
          "dialog system toolkit (Ortega et al., 2019) - that": "ment levels, play an important role in human com-"
        },
        {
          "dialog system toolkit (Ortega et al., 2019) - that": ""
        },
        {
          "dialog system toolkit (Ortega et al., 2019) - that": "puter interaction (McTear et al., 2016). However"
        },
        {
          "dialog system toolkit (Ortega et al., 2019) - that": ""
        },
        {
          "dialog system toolkit (Ortega et al., 2019) - that": "in contrast to open-ended dialog systems (Weizen-"
        },
        {
          "dialog system toolkit (Ortega et al., 2019) - that": ""
        },
        {
          "dialog system toolkit (Ortega et al., 2019) - that": "baum, 1966), our toolkit focuses on task-oriented"
        },
        {
          "dialog system toolkit (Ortega et al., 2019) - that": ""
        },
        {
          "dialog system toolkit (Ortega et al., 2019) - that": "applications (Bobrow et al., 1977), such as search-"
        },
        {
          "dialog system toolkit (Ortega et al., 2019) - that": ""
        },
        {
          "dialog system toolkit (Ortega et al., 2019) - that": "ing for a lecturer at\nthe university (Ortega et al.,"
        },
        {
          "dialog system toolkit (Ortega et al., 2019) - that": ""
        },
        {
          "dialog system toolkit (Ortega et al., 2019) - that": "2019). The purpose we envision for dialog sys-"
        },
        {
          "dialog system toolkit (Ortega et al., 2019) - that": ""
        },
        {
          "dialog system toolkit (Ortega et al., 2019) - that": "tems developed using our toolkit\nis not\nthe same"
        },
        {
          "dialog system toolkit (Ortega et al., 2019) - that": ""
        },
        {
          "dialog system toolkit (Ortega et al., 2019) - that": "as the objective of a social chatbot such as XiaoIce"
        },
        {
          "dialog system toolkit (Ortega et al., 2019) - that": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": "the"
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        },
        {
          "thangvu@ims.uni-stuttgart.de": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "helps develop dialog systems that support users in": "efﬁciently fulﬁlling concrete goals, while at\nthe"
        },
        {
          "helps develop dialog systems that support users in": "same time considering social signals such as emo-"
        },
        {
          "helps develop dialog systems that support users in": "tional states and engagement levels so as to remain"
        },
        {
          "helps develop dialog systems that support users in": "friendly and likeable."
        },
        {
          "helps develop dialog systems that support users in": "2\nObjectives"
        },
        {
          "helps develop dialog systems that support users in": "The main objective of\nthis work is to develop a"
        },
        {
          "helps develop dialog systems that support users in": "multi-domain dialog system toolkit that allows for"
        },
        {
          "helps develop dialog systems that support users in": ""
        },
        {
          "helps develop dialog systems that support users in": "multi-modal information processing and that pro-"
        },
        {
          "helps develop dialog systems that support users in": ""
        },
        {
          "helps develop dialog systems that support users in": "vides different modules for extracting social sig-"
        },
        {
          "helps develop dialog systems that support users in": "nals such as emotional states and for integrating"
        },
        {
          "helps develop dialog systems that support users in": "them into the decision making process. The toolkit"
        },
        {
          "helps develop dialog systems that support users in": "should be easy to use and extend for users of all"
        },
        {
          "helps develop dialog systems that support users in": "levels of technical experience, providing a ﬂexible"
        },
        {
          "helps develop dialog systems that support users in": "collaborative research platform."
        },
        {
          "helps develop dialog systems that support users in": ""
        },
        {
          "helps develop dialog systems that support users in": "2.1\nToolkit Design"
        },
        {
          "helps develop dialog systems that support users in": ""
        },
        {
          "helps develop dialog systems that support users in": "We extend and substantially modify our previous,"
        },
        {
          "helps develop dialog systems that support users in": "text-based dialog system toolkit\n(Ortega\net\nal.,"
        },
        {
          "helps develop dialog systems that support users in": "2019) while following the same design choices."
        },
        {
          "helps develop dialog systems that support users in": "This means that our toolkit\nis meant\nto optimize"
        },
        {
          "helps develop dialog systems that support users in": "the following four criteria: Modularity, Flexibil-"
        },
        {
          "helps develop dialog systems that support users in": "ity, Transparency and User-friendliness at differ-"
        },
        {
          "helps develop dialog systems that support users in": "ent levels. This is accomplished by decomposing"
        },
        {
          "helps develop dialog systems that support users in": "the dialog system into independent modules (ser-"
        },
        {
          "helps develop dialog systems that support users in": "vices), which in turn are either rule-based, machine"
        },
        {
          "helps develop dialog systems that support users in": "learning-based or both. These services can easily"
        },
        {
          "helps develop dialog systems that support users in": "be combined in different orders/architectures, pro-"
        },
        {
          "helps develop dialog systems that support users in": ""
        },
        {
          "helps develop dialog systems that support users in": "viding users with ﬂexible options to design new"
        },
        {
          "helps develop dialog systems that support users in": "dialog architectures."
        },
        {
          "helps develop dialog systems that support users in": ""
        },
        {
          "helps develop dialog systems that support users in": "2.2\nChallenges & Proposed Solutions"
        },
        {
          "helps develop dialog systems that support users in": ""
        },
        {
          "helps develop dialog systems that support users in": "Multi-modality\nThe main\nchallenges\nin\nhan-"
        },
        {
          "helps develop dialog systems that support users in": "dling multi-modality are a) the design of a synchro-"
        },
        {
          "helps develop dialog systems that support users in": "nization infrastructure and b)\nthe large range of"
        },
        {
          "helps develop dialog systems that support users in": "different latencies from different modalities. To al-"
        },
        {
          "helps develop dialog systems that support users in": "leviate the former, we use the publisher/subscriber"
        },
        {
          "helps develop dialog systems that support users in": ""
        },
        {
          "helps develop dialog systems that support users in": "software pattern presented in section 4 to synchro-"
        },
        {
          "helps develop dialog systems that support users in": ""
        },
        {
          "helps develop dialog systems that support users in": "nize signals coming from different sources. This"
        },
        {
          "helps develop dialog systems that support users in": ""
        },
        {
          "helps develop dialog systems that support users in": "software pattern also allows for services to run in a"
        },
        {
          "helps develop dialog systems that support users in": ""
        },
        {
          "helps develop dialog systems that support users in": "distributed manner. By assigning computationally"
        },
        {
          "helps develop dialog systems that support users in": ""
        },
        {
          "helps develop dialog systems that support users in": "heavy tasks such as speech recognition and speech"
        },
        {
          "helps develop dialog systems that support users in": ""
        },
        {
          "helps develop dialog systems that support users in": "synthesis to a more powerful computing node, it is"
        },
        {
          "helps develop dialog systems that support users in": ""
        },
        {
          "helps develop dialog systems that support users in": "possible to reduce differences in latency when pro-"
        },
        {
          "helps develop dialog systems that support users in": ""
        },
        {
          "helps develop dialog systems that support users in": "cessing different modalities,\ntherefore achieving"
        },
        {
          "helps develop dialog systems that support users in": ""
        },
        {
          "helps develop dialog systems that support users in": "more natural interactions."
        },
        {
          "helps develop dialog systems that support users in": ""
        },
        {
          "helps develop dialog systems that support users in": "Socially-Engaged\nSystems\nDetermining\nthe"
        },
        {
          "helps develop dialog systems that support users in": "ideal scope of a socially-engaged dialog system is a"
        },
        {
          "helps develop dialog systems that support users in": "complex issue, that is which information should be"
        },
        {
          "helps develop dialog systems that support users in": "extracted from users and how the system can best"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "lutional neural networks (CNNs). For the sake of": "modularity,\nthree separate models are employed",
          "user’s\nspeech signal, which form the\ninput\nto": "the\nconvolutional neural network based on Or-"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "for predicting different\ntypes of labels:\n(a) basic",
          "user’s\nspeech signal, which form the\ninput\nto": "tega\net\nal.\n(2020).\nThe model\nassigns one of"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "emotions {angry, happy, neutral, sad}, (b) arousal",
          "user’s\nspeech signal, which form the\ninput\nto": "three categories from the proactive backchannel-"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "levels {low, medium, high}, and (c) valence lev-",
          "user’s\nspeech signal, which form the\ninput\nto": "ing theory (Goodwin, 1986)\nto each user utter-"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "els {negative, neutral, positive}. The models are",
          "user’s\nspeech signal, which form the\ninput\nto": "ance {no-backchannel, backchannel-continuer and"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "trained on the IEMOCAP dataset\n(Busso et al.,",
          "user’s\nspeech signal, which form the\ninput\nto": "backchannel-assessment}. The predicted category"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "2008).\nThe output of\nthe emotion recognition",
          "user’s\nspeech signal, which form the\ninput\nto": "is used to add the backchannel realization, such as"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "module consists of three predictions per user turn,",
          "user’s\nspeech signal, which form the\ninput\nto": "Right or Uh-huh, to the next system response."
        },
        {
          "lutional neural networks (CNNs). For the sake of": "which can then be used by the user state tracker",
          "user’s\nspeech signal, which form the\ninput\nto": ""
        },
        {
          "lutional neural networks (CNNs). For the sake of": "",
          "user’s\nspeech signal, which form the\ninput\nto": "3.2\nSpeech Processing"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "(see section 3.4). For future releases, we plan to",
          "user’s\nspeech signal, which form the\ninput\nto": ""
        },
        {
          "lutional neural networks (CNNs). For the sake of": "incorporate multiple training datasets as well as",
          "user’s\nspeech signal, which form the\ninput\nto": ""
        },
        {
          "lutional neural networks (CNNs). For the sake of": "",
          "user’s\nspeech signal, which form the\ninput\nto": "Automatic\nSpeech\nRecognition\n(ASR)\nThe"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "visual features.",
          "user’s\nspeech signal, which form the\ninput\nto": ""
        },
        {
          "lutional neural networks (CNNs). For the sake of": "",
          "user’s\nspeech signal, which form the\ninput\nto": "speech recognition module receives a speech signal"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "",
          "user’s\nspeech signal, which form the\ninput\nto": "as input, which can come from an internal or ex-"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "Engagement Level\nPrediction\nUser\nengage-",
          "user’s\nspeech signal, which form the\ninput\nto": ""
        },
        {
          "lutional neural networks (CNNs). For the sake of": "",
          "user’s\nspeech signal, which form the\ninput\nto": "ternal microphone, and outputs decoded text. The"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "ment is closely related to states such as boredom",
          "user’s\nspeech signal, which form the\ninput\nto": ""
        },
        {
          "lutional neural networks (CNNs). For the sake of": "",
          "user’s\nspeech signal, which form the\ninput\nto": "speciﬁc realization of ASR can be interchanged"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "and level of\ninterest, with implications for user",
          "user’s\nspeech signal, which form the\ninput\nto": ""
        },
        {
          "lutional neural networks (CNNs). For the sake of": "",
          "user’s\nspeech signal, which form the\ninput\nto": "or adapted, for example for new languages or dif-"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "satisfaction and task success (Forbes-Riley et al.,",
          "user’s\nspeech signal, which form the\ninput\nto": ""
        },
        {
          "lutional neural networks (CNNs). For the sake of": "",
          "user’s\nspeech signal, which form the\ninput\nto": "ferent ASR methods. We provide an end-to-end"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "2012; Schuller et al., 2009).\nIn ADVISER, we",
          "user’s\nspeech signal, which form the\ninput\nto": ""
        },
        {
          "lutional neural networks (CNNs). For the sake of": "",
          "user’s\nspeech signal, which form the\ninput\nto": "ASR model for English based on the Transformer"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "assume that eye activity serves as an indicator of",
          "user’s\nspeech signal, which form the\ninput\nto": ""
        },
        {
          "lutional neural networks (CNNs). For the sake of": "",
          "user’s\nspeech signal, which form the\ninput\nto": "neural network architecture. We use the end-to-end"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "various mental states (Schuller et al., 2009; Niu",
          "user’s\nspeech signal, which form the\ninput\nto": ""
        },
        {
          "lutional neural networks (CNNs). For the sake of": "",
          "user’s\nspeech signal, which form the\ninput\nto": "speech processing toolkit ESPnet (Watanabe et al.,"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "et al., 2018) and implement a gaze tracker\nthat",
          "user’s\nspeech signal, which form the\ninput\nto": ""
        },
        {
          "lutional neural networks (CNNs). For the sake of": "",
          "user’s\nspeech signal, which form the\ninput\nto": "2018) and the IMS-speech English multi-dataset"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "monitors the user’s direction of focus via webcam.",
          "user’s\nspeech signal, which form the\ninput\nto": ""
        },
        {
          "lutional neural networks (CNNs). For the sake of": "",
          "user’s\nspeech signal, which form the\ninput\nto": "recipe (Denisov and Vu, 2019), updated to match"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "Using OpenFace 2.2.0, a toolkit for facial behav-",
          "user’s\nspeech signal, which form the\ninput\nto": ""
        },
        {
          "lutional neural networks (CNNs). For the sake of": "",
          "user’s\nspeech signal, which form the\ninput\nto": "the LibriSpeech Transformer-based system in ESP-"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "ior analysis (Baltrusaitis et al., 2018), we extract",
          "user’s\nspeech signal, which form the\ninput\nto": ""
        },
        {
          "lutional neural networks (CNNs). For the sake of": "",
          "user’s\nspeech signal, which form the\ninput\nto": "net (Karita et al., 2019) and to include more train-"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "the features gaze angle x and gaze angle y, which",
          "user’s\nspeech signal, which form the\ninput\nto": ""
        },
        {
          "lutional neural networks (CNNs). For the sake of": "",
          "user’s\nspeech signal, which form the\ninput\nto": "ing data. Training data comprises the LibriSpeech,"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "capture left-right and up-down eye movement, for",
          "user’s\nspeech signal, which form the\ninput\nto": ""
        },
        {
          "lutional neural networks (CNNs). For the sake of": "",
          "user’s\nspeech signal, which form the\ninput\nto": "Switchboard, TED-LIUM 3, AMI, WSJ, Com-"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "each frame and compute the deviation from the",
          "user’s\nspeech signal, which form the\ninput\nto": ""
        },
        {
          "lutional neural networks (CNNs). For the sake of": "",
          "user’s\nspeech signal, which form the\ninput\nto": "mon Voice 3, SWC, VoxForge and M-AILABS"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "central point of\nthe screen.\nIf\nthe deviation ex-",
          "user’s\nspeech signal, which form the\ninput\nto": ""
        },
        {
          "lutional neural networks (CNNs). For the sake of": "",
          "user’s\nspeech signal, which form the\ninput\nto": "datasets with a total amount of 3249 hours. As"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "ceeds a certain threshold for a certain number of",
          "user’s\nspeech signal, which form the\ninput\nto": ""
        },
        {
          "lutional neural networks (CNNs). For the sake of": "",
          "user’s\nspeech signal, which form the\ninput\nto": "input features, 80-dimensional log Mel ﬁlterbank"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "seconds,\nthe user is assumed to look away from",
          "user’s\nspeech signal, which form the\ninput\nto": ""
        },
        {
          "lutional neural networks (CNNs). For the sake of": "",
          "user’s\nspeech signal, which form the\ninput\nto": "coefﬁcients are used. Output of the ASR model"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "the screen, thereby disengaging. Thus, the output",
          "user’s\nspeech signal, which form the\ninput\nto": ""
        },
        {
          "lutional neural networks (CNNs). For the sake of": "",
          "user’s\nspeech signal, which form the\ninput\nto": "is a sequence of subword units, which include sin-"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "of our engagement level prediction module is the",
          "user’s\nspeech signal, which form the\ninput\nto": ""
        },
        {
          "lutional neural networks (CNNs). For the sake of": "",
          "user’s\nspeech signal, which form the\ninput\nto": "gle characters as well as combinations of several"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "binary decision {looking, not looking}. Both the",
          "user’s\nspeech signal, which form the\ninput\nto": ""
        },
        {
          "lutional neural networks (CNNs). For the sake of": "",
          "user’s\nspeech signal, which form the\ninput\nto": "characters, making the model lexicon independent."
        },
        {
          "lutional neural networks (CNNs). For the sake of": "spatial and temporal sensitivity can be adjusted,",
          "user’s\nspeech signal, which form the\ninput\nto": ""
        },
        {
          "lutional neural networks (CNNs). For the sake of": "such that developers have the option to decide how",
          "user’s\nspeech signal, which form the\ninput\nto": "Speech Synthesis\nFor ADVISER’s voice output,"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "far and how long the user’s gaze can stray from",
          "user’s\nspeech signal, which form the\ninput\nto": "we use the ESPnet-TTS toolkit\n(Hayashi et al.,"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "the central point until they are considered to be dis-",
          "user’s\nspeech signal, which form the\ninput\nto": "2019), which is an extension of the ESPnet toolkit"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "engaged.\nIn an adaptive system, this information",
          "user’s\nspeech signal, which form the\ninput\nto": "mentioned above. We use FastSpeech as the synthe-"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "could be used to select re-engagement strategies,",
          "user’s\nspeech signal, which form the\ninput\nto": "sis model speeding up mel-spectrogram generation"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "e.g. using an affective template (see section 3.7).",
          "user’s\nspeech signal, which form the\ninput\nto": "by a factor of 270 and voice generation by a fac-"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "",
          "user’s\nspeech signal, which form the\ninput\nto": "tor of 38 compared to autoregressive Transformer"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "Backchanneling\nIn a conversation, a backchan-",
          "user’s\nspeech signal, which form the\ninput\nto": "TTS (Ren et al., 2019). We use a Parallel Wave-"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "nel\n(BC)\nis a soft\ninterjection from the listener",
          "user’s\nspeech signal, which form the\ninput\nto": "GAN (Yamamoto et al., 2020) to generate wave-"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "to\nthe\nspeaker, with\nthe\npurpose\nof\nsignaling",
          "user’s\nspeech signal, which form the\ninput\nto": "forms that is computationally efﬁcient and achieves"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "acknowledgment\nor\nreacting\nto what was\njust",
          "user’s\nspeech signal, which form the\ninput\nto": "a high mean opinion score of 4.16. The FastSpeech"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "uttered.\nBackchannels contribute to a success-",
          "user’s\nspeech signal, which form the\ninput\nto": "and WaveGAN models were trained with 24 hours"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "ful conversation ﬂow (Clark and Krych, 2004).",
          "user’s\nspeech signal, which form the\ninput\nto": "of the LJSpeech dataset from a single speaker\n(Ito,"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "Therefore, we add an acoustic backchannel mod-",
          "user’s\nspeech signal, which form the\ninput\nto": "2017) and are capable of generating voice output in"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "ule to create a more human-like dialog experi-",
          "user’s\nspeech signal, which form the\ninput\nto": "real-time when using a GPU. The synthesis can run"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "ence.\nFor backchannel prediction, we\nextract",
          "user’s\nspeech signal, which form the\ninput\nto": "on any device in a distributed system. Additionally,"
        },
        {
          "lutional neural networks (CNNs). For the sake of": "13 Mel-frequency-cepstral coefﬁcients\nfrom the",
          "user’s\nspeech signal, which form the\ninput\nto": "we optimize the synthesizer for abbreviations, such"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "as for German proper names, such as street names.",
          "about that entity. The reinforcement learning (RL)": "policy’s action-value function is approximated by"
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "These optimizations can be easily extended.",
          "about that entity. The reinforcement learning (RL)": "a neural network which outputs a value for each"
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "",
          "about that entity. The reinforcement learning (RL)": "possible system action, given the vectorized rep-"
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "Turn Taking\nTo make interacting with the sys-",
          "about that entity. The reinforcement learning (RL)": ""
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "",
          "about that entity. The reinforcement learning (RL)": "resentation of a turn’s belief state as input. The"
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "tem more natural, we use a naive end-of-utterance",
          "about that entity. The reinforcement learning (RL)": ""
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "",
          "about that entity. The reinforcement learning (RL)": "neural network is constructed as proposed in V¨ath"
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "detection.\nUsers indicate the start of\ntheir\nturn",
          "about that entity. The reinforcement learning (RL)": ""
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "",
          "about that entity. The reinforcement learning (RL)": "and Vu (2019)\nfollowing a duelling architecture"
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "by pressing a hotkey, so they can choose to pause",
          "about that entity. The reinforcement learning (RL)": ""
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "",
          "about that entity. The reinforcement learning (RL)": "(Wang et al., 2016). It consists of two separate cal-"
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "the interaction. The highest absolute peak of each",
          "about that entity. The reinforcement learning (RL)": ""
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "",
          "about that entity. The reinforcement learning (RL)": "culation streams, each with its own layers, where"
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "recording chunk is then compared with a prede-",
          "about that entity. The reinforcement learning (RL)": ""
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "",
          "about that entity. The reinforcement learning (RL)": "the ﬁnal layer yields the action-value function. For"
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "ﬁned threshold.\nIf a certain number of sequential",
          "about that entity. The reinforcement learning (RL)": ""
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "",
          "about that entity. The reinforcement learning (RL)": "off-policy batch-training, we make use of priori-"
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "chunks do not peak above the threshold, the record-",
          "about that entity. The reinforcement learning (RL)": ""
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "",
          "about that entity. The reinforcement learning (RL)": "tized experience replay (Schaul et al., 2015)."
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "ing stops. We are currenlty in the process of plan-",
          "about that entity. The reinforcement learning (RL)": ""
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "ning more sophisticated turn taking models, such",
          "about that entity. The reinforcement learning (RL)": "Affective Policy\nIn addition, we have also imple-"
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "as Skantze et al. (2015).",
          "about that entity. The reinforcement learning (RL)": "mented a rule-based affective policy service that"
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "",
          "about that entity. The reinforcement learning (RL)": "can be used to determine the system’s emotional"
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "3.3\nNatural Language Understanding",
          "about that entity. The reinforcement learning (RL)": ""
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "",
          "about that entity. The reinforcement learning (RL)": "response. As this policy is domain-agnostic, pre-"
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "The natural\nlanguage understanding (NLU) unit",
          "about that entity. The reinforcement learning (RL)": "dicting the next system emotion output rather than"
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "parses the textual user input (De Mori et al., 2008)",
          "about that entity. The reinforcement learning (RL)": "the next system action, it can be used alongside any"
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "- or the output from the speech recognition system -",
          "about that entity. The reinforcement learning (RL)": "of the previously mentioned policies."
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "and extracts the user action type, generally referred",
          "about that entity. The reinforcement learning (RL)": ""
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "",
          "about that entity. The reinforcement learning (RL)": "User Simulator\nTo support automatic evaluation"
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "to as intent\nin goal-oriented dialog systems (e.g.",
          "about that entity. The reinforcement learning (RL)": ""
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "",
          "about that entity. The reinforcement learning (RL)": "and to train the RL policy, we provide a user simu-"
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "Inform and Request), as well as the corresponding",
          "about that entity. The reinforcement learning (RL)": ""
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "",
          "about that entity. The reinforcement learning (RL)": "lator service outputting at the user acts level. As we"
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "slots and values. The domain-independent,\nrule-",
          "about that entity. The reinforcement learning (RL)": ""
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "",
          "about that entity. The reinforcement learning (RL)": "are concerned with task-oriented dialogs here, the"
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "based NLU presented in Ortega et al.\n(2019)\nis",
          "about that entity. The reinforcement learning (RL)": ""
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "",
          "about that entity. The reinforcement learning (RL)": "user simulator has an agenda-based (Schatzmann"
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "integrated into ADVISER and adapted to the new",
          "about that entity. The reinforcement learning (RL)": ""
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "",
          "about that entity. The reinforcement learning (RL)": "et al., 2007) architecture and is randomly assigned"
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "domains presented in section 5.",
          "about that entity. The reinforcement learning (RL)": ""
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "",
          "about that entity. The reinforcement learning (RL)": "a goal at the beginning of the dialog. Each turn, it"
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "3.4\nState Tracking",
          "about that entity. The reinforcement learning (RL)": "then works to ﬁrst respond to the system utterance,"
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "",
          "about that entity. The reinforcement learning (RL)": "and then after\nto fulﬁll\nits own goal. When the"
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "Belief State Tracking (BST):\nThe BST tracks",
          "about that entity. The reinforcement learning (RL)": ""
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "",
          "about that entity. The reinforcement learning (RL)": "system utterance also works toward fulﬁlling the"
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "the history of user informs and the user action types,",
          "about that entity. The reinforcement learning (RL)": ""
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "",
          "about that entity. The reinforcement learning (RL)": "user goal, the RL policy is rewarded by achieving a"
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "requests, with one BST entry per turn. This infor-",
          "about that entity. The reinforcement learning (RL)": ""
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "",
          "about that entity. The reinforcement learning (RL)": "shorter total dialog turn count (Ortega et al., 2019)."
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "mation is stored in a dictionary structure that\nis",
          "about that entity. The reinforcement learning (RL)": ""
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "built up, as the user provides more details and the",
          "about that entity. The reinforcement learning (RL)": ""
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "",
          "about that entity. The reinforcement learning (RL)": "3.6\nExternal Information Resources"
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "system has a better understanding of user intent.",
          "about that entity. The reinforcement learning (RL)": ""
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "",
          "about that entity. The reinforcement learning (RL)": "ADVISER supports three options to access informa-"
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "User State Tracking (UST):\nSimilar to the BST,",
          "about that entity. The reinforcement learning (RL)": "tion from external information sources. In addition"
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "the UST tracks the history of the user’s state over",
          "about that entity. The reinforcement learning (RL)": "to being able to query information from SQL-based"
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "the course of a dialog, with one entry per turn. In",
          "about that entity. The reinforcement learning (RL)": "databases, we add two new options that\nincludes"
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "the current implementation, the user state consists",
          "about that entity. The reinforcement learning (RL)": "querying information via APIs and from knowl-"
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "of the user’s engagement\nlevel, valence, arousal,",
          "about that entity. The reinforcement learning (RL)": "edge bases (e.g. Wikidata (Vrandeˇci´c and Kr¨otzsch,"
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "and emotion category (details in section 3.1).",
          "about that entity. The reinforcement learning (RL)": "2014)). For example, when a user asks a simple"
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "",
          "about that entity. The reinforcement learning (RL)": "question - Where was Dirk Nowitzki born?, our"
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "3.5\nDialog Policies",
          "about that entity. The reinforcement learning (RL)": ""
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "",
          "about that entity. The reinforcement learning (RL)": "pretrained neural network predicts the topic entity"
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "Policies\nTo determine the correct system action,",
          "about that entity. The reinforcement learning (RL)": "- Dirk Nowitzki - and the relation - place of birth."
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "we provide three types of policy services: a hand-",
          "about that entity. The reinforcement learning (RL)": "Then, the answer is automatically looked up using"
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "crafted and a reinforcement learning policy for ﬁnd-",
          "about that entity. The reinforcement learning (RL)": "Wikidata’s SPARQL endpoint."
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "ing entities from a database (Ortega et al., 2019),",
          "about that entity. The reinforcement learning (RL)": ""
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "",
          "about that entity. The reinforcement learning (RL)": "3.7\nNatural Language Generation (NLG)"
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "as well as a handcrafted policy for looking up in-",
          "about that entity. The reinforcement learning (RL)": ""
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "formation through an API call. Both handcrafted",
          "about that entity. The reinforcement learning (RL)": "In the NLG service,\nthe semantic representation"
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "policies use a series of rules to help the user ﬁnd",
          "about that entity. The reinforcement learning (RL)": "of the system act\nis transformed into natural\nlan-"
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "a single entity or, once an entity has been found",
          "about that entity. The reinforcement learning (RL)": "guage. ADVISER currently uses a template-based"
        },
        {
          "as Prof., Univ., IMS, NLP, ECTS and PhD, as well": "(or directly provided by the user), ﬁnd information",
          "about that entity. The reinforcement learning (RL)": "approach to NLG in which each possible system"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "of audio-, visual- and social signals, with the aim": ""
        },
        {
          "of audio-, visual- and social signals, with the aim": "that can be"
        },
        {
          "of audio-, visual- and social signals, with the aim": ""
        },
        {
          "of audio-, visual- and social signals, with the aim": ""
        },
        {
          "of audio-, visual- and social signals, with the aim": ""
        },
        {
          "of audio-, visual- and social signals, with the aim": ""
        },
        {
          "of audio-, visual- and social signals, with the aim": ""
        },
        {
          "of audio-, visual- and social signals, with the aim": ""
        },
        {
          "of audio-, visual- and social signals, with the aim": "We introduce ADVISER – an open-source, multi-"
        },
        {
          "of audio-, visual- and social signals, with the aim": ""
        },
        {
          "of audio-, visual- and social signals, with the aim": "domain dialog system toolkit that allows users to"
        },
        {
          "of audio-, visual- and social signals, with the aim": ""
        },
        {
          "of audio-, visual- and social signals, with the aim": "easily develop multi-modal and socially-engaged"
        },
        {
          "of audio-, visual- and social signals, with the aim": ""
        },
        {
          "of audio-, visual- and social signals, with the aim": "conversational agents. We provide a large variety"
        },
        {
          "of audio-, visual- and social signals, with the aim": ""
        },
        {
          "of audio-, visual- and social signals, with the aim": "of functionalities, ranging from speech processing"
        },
        {
          "of audio-, visual- and social signals, with the aim": ""
        },
        {
          "of audio-, visual- and social signals, with the aim": "to core dialog system capabilities and social signal"
        },
        {
          "of audio-, visual- and social signals, with the aim": ""
        },
        {
          "of audio-, visual- and social signals, with the aim": "processing. With this toolkit, we hope to provide"
        },
        {
          "of audio-, visual- and social signals, with the aim": ""
        },
        {
          "of audio-, visual- and social signals, with the aim": "a ﬂexible platform for collaborative research in"
        },
        {
          "of audio-, visual- and social signals, with the aim": ""
        },
        {
          "of audio-, visual- and social signals, with the aim": "multi-domain, multi-modal, socially-engaged con-"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "implemented a weather domain that makes calls to": "",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "PM is 3 degrees celsius with light snow."
        },
        {
          "implemented a weather domain that makes calls to": "the OpenWeatherMap API3 and a mensa domain",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": ""
        },
        {
          "implemented a weather domain that makes calls to": "",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "USER: Thank you, ADVISER, good bye!"
        },
        {
          "implemented a weather domain that makes calls to": "for gathering information from the dining hall at",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": ""
        },
        {
          "implemented a weather domain that makes calls to": "",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "SYSTEM: Thank you, good bye."
        },
        {
          "implemented a weather domain that makes calls to": "the university of Stuttgart. Note that affective tem-",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": ""
        },
        {
          "implemented a weather domain that makes calls to": "plates were only added to the lecturers and mensa",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "6\nRelated Work"
        },
        {
          "implemented a weather domain that makes calls to": "domain. All domains can be used within the same",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": ""
        },
        {
          "implemented a weather domain that makes calls to": "",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "Other\ntools for building dialog systems include"
        },
        {
          "implemented a weather domain that makes calls to": "dialog, simply by switching the topic.",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": ""
        },
        {
          "implemented a weather domain that makes calls to": "",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "ConvLab (Lee et al., 2019), an open-source,\ntext-"
        },
        {
          "implemented a weather domain that makes calls to": "5.2\nAn Example Implementation",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "based dialog system platform that supports both"
        },
        {
          "implemented a weather domain that makes calls to": "",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "pipelined architectures and an end-to-end neural"
        },
        {
          "implemented a weather domain that makes calls to": "Our\ntoolkit allows for easy creation of a dialog",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": ""
        },
        {
          "implemented a weather domain that makes calls to": "",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "architecture. ConvLab also provides reusable com-"
        },
        {
          "implemented a weather domain that makes calls to": "system within a few lines of code as follows.",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": ""
        },
        {
          "implemented a weather domain that makes calls to": "",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "ponents and supports multi-domain settings. Other"
        },
        {
          "implemented a weather domain that makes calls to": "#\ndomains",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": ""
        },
        {
          "implemented a weather domain that makes calls to": "",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "systems are largely text-based, but offer\nthe in-"
        },
        {
          "implemented a weather domain that makes calls to": "weather\n=\nWeatherDomain()",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": ""
        },
        {
          "implemented a weather domain that makes calls to": "mensa\n=\nMensaDomain()",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "In-\ncorporation of external speech components."
        },
        {
          "implemented a weather domain that makes calls to": "#\ncreate\nsystem\nfrom\ndesired\nservices",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "proTK (Baumann and Schlangen, 2012),\nfor\nin-"
        },
        {
          "implemented a weather domain that makes calls to": "ds\n=\nDialogSystem(services=[",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": ""
        },
        {
          "implemented a weather domain that makes calls to": "",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "stance,\nin which modules communicate by net-"
        },
        {
          "implemented a weather domain that makes calls to": "DomainTracker(domains=[mensa,",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": ""
        },
        {
          "implemented a weather domain that makes calls to": "weather]),",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "works via conﬁguration ﬁles, uses ASR based on"
        },
        {
          "implemented a weather domain that makes calls to": "SpeechRecorder(),",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": ""
        },
        {
          "implemented a weather domain that makes calls to": "",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "Sphinx-4 and synthesis based on MaryTTS. Simi-"
        },
        {
          "implemented a weather domain that makes calls to": "VideoInput(),",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": ""
        },
        {
          "implemented a weather domain that makes calls to": "",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "larly, RavenClaw (Bohus and Rudnicky, 2009) pro-"
        },
        {
          "implemented a weather domain that makes calls to": "EngagementTracker(),",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": ""
        },
        {
          "implemented a weather domain that makes calls to": "EmotionTracker(),",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "vides a framework for creating dialog managers;"
        },
        {
          "implemented a weather domain that makes calls to": "WeatherNLU(domain=weather),",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": ""
        },
        {
          "implemented a weather domain that makes calls to": "",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "ASR and synthesis components can be supplied,"
        },
        {
          "implemented a weather domain that makes calls to": "MensaNLU(domain=mensa),",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": ""
        },
        {
          "implemented a weather domain that makes calls to": "",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "for example, by connecting to Sphinx and Kalliope."
        },
        {
          "implemented a weather domain that makes calls to": "...,",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": ""
        },
        {
          "implemented a weather domain that makes calls to": "SpeechOutputPlayer()])",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "OpenDial (Lison and Kennington, 2016) relies on"
        },
        {
          "implemented a weather domain that makes calls to": "",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "probabilistic rules and provides options\nto con-"
        },
        {
          "implemented a weather domain that makes calls to": "ds.run_dialog(...)",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": ""
        },
        {
          "implemented a weather domain that makes calls to": "",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "nect to speech components such as Sphinx. Multi-"
        },
        {
          "implemented a weather domain that makes calls to": "",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "domain dialog toolkit - PyDial (Ultes et al., 2017)"
        },
        {
          "implemented a weather domain that makes calls to": "As a ﬁrst step, a dialog system object is initial-",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": ""
        },
        {
          "implemented a weather domain that makes calls to": "",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "supports connection to DialPort."
        },
        {
          "implemented a weather domain that makes calls to": "ized, which is responsible for coordinating the ini-",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": ""
        },
        {
          "implemented a weather domain that makes calls to": "",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "As mentioned in the introduction, Microsoft Re-"
        },
        {
          "implemented a weather domain that makes calls to": "tialization and graceful\ntermination of all dialog",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": ""
        },
        {
          "implemented a weather domain that makes calls to": "",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "search’s \\psi\nis an open and extensible platform"
        },
        {
          "implemented a weather domain that makes calls to": "services. Talking about multiple domains in one",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": ""
        },
        {
          "implemented a weather domain that makes calls to": "",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "that supports the development of multi-modal AI"
        },
        {
          "implemented a weather domain that makes calls to": "dialog is enabled by creating a simple keyword-",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": ""
        },
        {
          "implemented a weather domain that makes calls to": "",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "systems (Bohus et al., 2017). It further offers audio"
        },
        {
          "implemented a weather domain that makes calls to": "based domain tracker which is introduced as the",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": ""
        },
        {
          "implemented a weather domain that makes calls to": "",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "and visual processing, such as speech recognition"
        },
        {
          "implemented a weather domain that makes calls to": "ﬁrst argument\nto the dialog system. To make the",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": ""
        },
        {
          "implemented a weather domain that makes calls to": "",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "and face tracking, as well as output, such as synthe-"
        },
        {
          "implemented a weather domain that makes calls to": "dialog multi-modal, speech and vision modules are",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": ""
        },
        {
          "implemented a weather domain that makes calls to": "",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "sis and avatar rendering. And the MuMMER (multi-"
        },
        {
          "implemented a weather domain that makes calls to": "introduced next, along with modules to extract en-",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": ""
        },
        {
          "implemented a weather domain that makes calls to": "",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "modal Mall Entertainment Robot) project (Foster"
        },
        {
          "implemented a weather domain that makes calls to": "gagement and emotion. So far, all of these modules",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": ""
        },
        {
          "implemented a weather domain that makes calls to": "",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "et al., 2016)\nis based on the SoftBank Robotics"
        },
        {
          "implemented a weather domain that makes calls to": "are domain-agnostic and can be used as shared",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": ""
        },
        {
          "implemented a weather domain that makes calls to": "",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "Pepper platform, and thereby comprises processing"
        },
        {
          "implemented a weather domain that makes calls to": "resources between all domains.\nNext, domain-",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": ""
        },
        {
          "implemented a weather domain that makes calls to": "",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "of audio-, visual- and social signals, with the aim"
        },
        {
          "implemented a weather domain that makes calls to": "dependent services such as NLUs, BSTs and NLGs",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": ""
        },
        {
          "implemented a weather domain that makes calls to": "",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "to develop a socially engaging robot\nthat can be"
        },
        {
          "implemented a weather domain that makes calls to": "for weather and mensa, are added. The following",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": ""
        },
        {
          "implemented a weather domain that makes calls to": "",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "deployed in public spaces."
        },
        {
          "implemented a weather domain that makes calls to": "shows an example dialog.",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": ""
        },
        {
          "implemented a weather domain that makes calls to": "",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "7\nConclusions"
        },
        {
          "implemented a weather domain that makes calls to": "SYSTEM: Hello, please let me know how I can help",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": ""
        },
        {
          "implemented a weather domain that makes calls to": "you, I can discuss the following domains: Mensa",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": ""
        },
        {
          "implemented a weather domain that makes calls to": "",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "We introduce ADVISER – an open-source, multi-"
        },
        {
          "implemented a weather domain that makes calls to": "Food and Weather.",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": ""
        },
        {
          "implemented a weather domain that makes calls to": "USER:\nI could have something to eat. What does the",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "domain dialog system toolkit that allows users to"
        },
        {
          "implemented a weather domain that makes calls to": "mensa offer today?",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": ""
        },
        {
          "implemented a weather domain that makes calls to": "",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "easily develop multi-modal and socially-engaged"
        },
        {
          "implemented a weather domain that makes calls to": "SYSTEM: What type of dish are you looking for?",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": ""
        },
        {
          "implemented a weather domain that makes calls to": "",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "conversational agents. We provide a large variety"
        },
        {
          "implemented a weather domain that makes calls to": "USER:\nI would like a main dish.",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": ""
        },
        {
          "implemented a weather domain that makes calls to": "SYSTEM:\nShould the meal be vegan?",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "of functionalities, ranging from speech processing"
        },
        {
          "implemented a weather domain that makes calls to": "USER: Yes.",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": ""
        },
        {
          "implemented a weather domain that makes calls to": "",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "to core dialog system capabilities and social signal"
        },
        {
          "implemented a weather domain that makes calls to": "SYSTEM: The meal mediterranean Ebly wheat\nis",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": ""
        },
        {
          "implemented a weather domain that makes calls to": "",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "processing. With this toolkit, we hope to provide"
        },
        {
          "implemented a weather domain that makes calls to": "served today, is a main dish and is vegan.",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": ""
        },
        {
          "implemented a weather domain that makes calls to": "USER: Okay, cool,\nI will go there now! What\nis the",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "a ﬂexible platform for collaborative research in"
        },
        {
          "implemented a weather domain that makes calls to": "weather like?",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": ""
        },
        {
          "implemented a weather domain that makes calls to": "",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "multi-domain, multi-modal, socially-engaged con-"
        },
        {
          "implemented a weather domain that makes calls to": "3https://openweathermap.org/api",
          "SYSTEM: The weather in Stuttgart on January 28 at 3": "versational agents."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "Oliver\nLemon,\nMarketta\nNiemel¨a,\nJean Marc"
        },
        {
          "References": "Tadas Baltrusaitis, Amir Zadeh, Yao Chong Lim, and",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": ""
        },
        {
          "References": "",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "Odobez, and Amit Kumar Pandey. 2016. The MuM-"
        },
        {
          "References": "Louis-Philippe Morency. 2018. OpenFace 2.0: Fa-",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": ""
        },
        {
          "References": "",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "MER project:\nEngaging\nhuman-robot\ninteraction"
        },
        {
          "References": "cial Behavior Analysis Toolkit.",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": ""
        },
        {
          "References": "",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "the\nin real-world public spaces.\nIn Processings of"
        },
        {
          "References": "",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "Eighth International Conference on Social Robotics"
        },
        {
          "References": "Timo Baumann and David Schlangen. 2012.\nThe In-",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": ""
        },
        {
          "References": "",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "(ICSR 2016), pages 753–763. Springer."
        },
        {
          "References": "proTK 2012 Release.\nIn NAACL-HLT Workshop on",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": ""
        },
        {
          "References": "Future Directions and Needs in the Spoken Dialog",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": ""
        },
        {
          "References": "",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "Charles Goodwin. 1986. Between and within: Alterna-"
        },
        {
          "References": "Community: Tools and Data.",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": ""
        },
        {
          "References": "",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "tive sequential\ntreatments of continuers and assess-"
        },
        {
          "References": "",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "ments. Journal of Human Studies."
        },
        {
          "References": "Daniel G. Bobrow, Ronald M. Kaplan, Martin Kay,",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": ""
        },
        {
          "References": "Donald A. Norman, Henry Thompson,\nand Terry",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": ""
        },
        {
          "References": "",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "Tomoki Hayashi, Ryuichi Yamamoto, Katsuki\nInoue,"
        },
        {
          "References": "Winograd. 1977. Gus, a frame-driven dialog system.",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": ""
        },
        {
          "References": "",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "Takenori Yoshimura, Shinji Watanabe, Tomoki Toda,"
        },
        {
          "References": "Artiﬁcial Intelligence.",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": ""
        },
        {
          "References": "",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "Kazuya Takeda, Yu Zhang,\nand Xu Tan.\n2019."
        },
        {
          "References": "",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "Espnet-tts: Uniﬁed,\nreproducible, and integratable"
        },
        {
          "References": "Dan Bohus, Sean Andrist, and Mihai Jalobeanu. 2017.",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": ""
        },
        {
          "References": "",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "open source end-to-end text-to-speech toolkit."
        },
        {
          "References": "Rapid Development of Multimodal\nInteractive Sys-",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": ""
        },
        {
          "References": "tems: A Demonstration of Platform for Situated In-",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": ""
        },
        {
          "References": "",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "https://\nKeith Ito. 2017.\nThe lj\nspeech dataset."
        },
        {
          "References": "’17: Proceedings of\nthe 19th\ntelligence.\nIn ICMI",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": ""
        },
        {
          "References": "ACM International Conference on Multimodal Inter-",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "keithito.com/LJ-Speech-Dataset/."
        },
        {
          "References": "action, pages 493–494.",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": ""
        },
        {
          "References": "",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "Shigeki\nKarita,\nNanxin\nChen,\nTomoki\nHayashi,"
        },
        {
          "References": "Dan Bohus\nand Alexander\nI Rudnicky. 2009.\nThe",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "Takaaki Hori,\nHirofumi\nInaguma,\nZiyan\nJiang,"
        },
        {
          "References": "ravenclaw dialog management framework: Architec-",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "Masao\nSomeki,\nNelson\nEnrique\nYalta\nSoplin,"
        },
        {
          "References": "ture and systems.\nComputer Speech & Language,",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "Ryuichi Yamamoto, Xiaofei Wang, et al. 2019. A"
        },
        {
          "References": "23(3):332–361.",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "comparative study on transformer vs rnn in speech"
        },
        {
          "References": "",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "applications.\nIn IEEE Automatic Speech Recogni-"
        },
        {
          "References": "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "tion and Understanding Workshop (ASRU)."
        },
        {
          "References": "Kazemzadeh,\nEmily Mower,\nSamuel Kim,\nJean-",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": ""
        },
        {
          "References": "nette N Chang,\nSungbok Lee,\nand\nShrikanth\nS",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "Sungjin Lee, Qi Zhu, Ryuichi Takanobu, Zheng Zhang,"
        },
        {
          "References": "Narayanan. 2008.\nIemocap:\nInteractive emotional",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "Yaoqin Zhang, Xiang Li,\nJinchao Li, Baolin Peng,"
        },
        {
          "References": "Language\nre-\ndyadic motion\ncapture\ndatabase.",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": ""
        },
        {
          "References": "",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "Xiujun Li, Minlie Huang, and Jianfeng Gao. 2019."
        },
        {
          "References": "sources and evaluation, 42(4).",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "ConvLab: Multi-Domain End-to-End Dialog Sys-"
        },
        {
          "References": "",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "the 57th Annual\ntem Platform.\nIn Proceedings of"
        },
        {
          "References": "H. H Clark and M. A Krych. 2004.\nSpeaking while",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": ""
        },
        {
          "References": "",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "Meeting of\nthe Association for Computational Lin-"
        },
        {
          "References": "Journal\nmonitoring addressees for understanding.",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": ""
        },
        {
          "References": "",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "guistics: System Demonstrations, pages 64–69."
        },
        {
          "References": "of Memory and Language.",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": ""
        },
        {
          "References": "",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "Pierre Lison and Casey Kennington. 2016. Opendial:"
        },
        {
          "References": "Amanda Cercas Curry, Ioannis Papaioannou, Alessan-",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": ""
        },
        {
          "References": "",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "A toolkit\nfor developing spoken dialogue systems"
        },
        {
          "References": "dro Suglia,\nShubham Agarwal,\nIgor Shalyminov,",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": ""
        },
        {
          "References": "",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "with probabilistic\nrules.\nIn Proceedings of ACL-"
        },
        {
          "References": "Xinnuo Xu, Ond˘rej Du˘sek, Arash Eshghi,\nIoan-",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": ""
        },
        {
          "References": "",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "2016 system demonstrations, pages 67–72."
        },
        {
          "References": "nis Konstas, Verena Rieser,\nand Oliver Lemon.",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": ""
        },
        {
          "References": "2018. Alana v2: Entertaining and Informative Open-",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": ""
        },
        {
          "References": "",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "Michael\nFrederick McTear,\nZoraida Callejas,\nand"
        },
        {
          "References": "domain Social Dialogue using Ontologies and Entity",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": ""
        },
        {
          "References": "",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "The conversational\nDavid Griol. 2016.\ninterface,"
        },
        {
          "References": "Linking.\nIn 1st Proceedings of Alexa Prize.",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": ""
        },
        {
          "References": "",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "volume 6. Springer."
        },
        {
          "References": "Renato De Mori, Fr´ed´eric Bechet, Dilek Hakkani-Tur,",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": ""
        },
        {
          "References": "",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "Michael Neumann and Ngoc Thang Vu. 2017. Atten-"
        },
        {
          "References": "Michael McTear, Giuseppe Riccardi,\nand Gokhan",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": ""
        },
        {
          "References": "",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "tive convolutional neural network based speech emo-"
        },
        {
          "References": "IEEE\nTur. 2008.\nSpoken language understanding.",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": ""
        },
        {
          "References": "",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "tion recognition: A study on the impact of input fea-"
        },
        {
          "References": "Signal Processing Magazine.",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": ""
        },
        {
          "References": "",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "tures, signal\nlength, and acted speech.\nIn Proceed-"
        },
        {
          "References": "",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "ings of Interspeech."
        },
        {
          "References": "Pavel Denisov\nand Ngoc Thang Vu.\n2019.\nIms-",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": ""
        },
        {
          "References": "Studientexte\nspeech:\nA speech\nto\ntext\ntool.",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": ""
        },
        {
          "References": "zur Sprachkommunikation: Elektronische Sprachsig-",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "Xuesong Niu, Hu Han,\nJiabei\nZeng, Xuran\nSun,"
        },
        {
          "References": "nalverarbeitung 2019, pages 170–177.",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "Shiguang Shan, Yan Huang,\nSongfan Yang,\nand"
        },
        {
          "References": "",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "Xilin Chen. 2018.\nAutomatic Engagement Predic-"
        },
        {
          "References": "Kate Forbes-Riley, Diane Litman, Heather Friedberg,",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "tion with GAP Feature.\nIn ICMI, pages 599–603,"
        },
        {
          "References": "and Joanna Drummond. 2012.\nIntrinsic and Extrin-",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "Boulder. Association for Computing Machinery."
        },
        {
          "References": "sic Evaluation of an Automatic User Disengagement",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": ""
        },
        {
          "References": "Detector\nfor an Uncertainty-Adaptive Spoken Dia-",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "Daniel Ortega, Chia-Yu Li, and Thang Vu. 2020. Oh,"
        },
        {
          "References": "the North Ameri-\nlogue System.\nIn Conference of",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "Jeez!\nor\nuh-huh?\nA listener-aware Backchan-"
        },
        {
          "References": "can Chapter of\nthe Association for Computational",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "nel predictor on ASR transcriptions.\nIn ICASSP"
        },
        {
          "References": "Linguistics: Human Language Technologies, pages",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "2020 - IEEE International Conference on Acoustics,"
        },
        {
          "References": "91–102, Montr´eal.",
          "Mary Ellen Foster, Rachid Alami, Olli Gestranius,": "Speech and Signal Processing, pages 8064–8068."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Daniel Ortega, Dirk V¨ath, Gianna Weber, Lindsey Van-": "derlyn, Maximilian Schmidt, Moritz V¨olkel, Zorica",
          "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki": "Hayashi,\nJiro Nishitoba, Yuya Unno, Nelson En-"
        },
        {
          "Daniel Ortega, Dirk V¨ath, Gianna Weber, Lindsey Van-": "Karacevic, and Ngoc Thang Vu. 2019. Adviser: A",
          "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki": "rique Yalta Soplin, Jahn Heymann, Matthew Wies-"
        },
        {
          "Daniel Ortega, Dirk V¨ath, Gianna Weber, Lindsey Van-": "dialog system framework for education & research.",
          "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki": "ner, Nanxin Chen, Adithya Renduchintala,\nand"
        },
        {
          "Daniel Ortega, Dirk V¨ath, Gianna Weber, Lindsey Van-": "the 57th Annual Meeting of\nthe\nIn Proceedings of",
          "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki": "Tsubasa Ochiai. 2018.\nEspnet: End-to-end speech"
        },
        {
          "Daniel Ortega, Dirk V¨ath, Gianna Weber, Lindsey Van-": "Association for Computational Linguistics:\nSystem",
          "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki": "processing\ntoolkit.\nIn\nInterspeech,\npages\n2207–"
        },
        {
          "Daniel Ortega, Dirk V¨ath, Gianna Weber, Lindsey Van-": "Demonstrations, pages 93–98.",
          "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki": "2211."
        },
        {
          "Daniel Ortega, Dirk V¨ath, Gianna Weber, Lindsey Van-": "Yi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao,",
          "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki": "Joseph Weizenbaum. 1966. ELIZA: A Computer Pro-"
        },
        {
          "Daniel Ortega, Dirk V¨ath, Gianna Weber, Lindsey Van-": "Zhou Zhao,\nand Tie-Yan Liu. 2019.\nFastspeech:",
          "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki": "gram for\nthe Study of Natural Language Commu-"
        },
        {
          "Daniel Ortega, Dirk V¨ath, Gianna Weber, Lindsey Van-": "",
          "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki": "Communica-\nnication Between Man and Machine."
        },
        {
          "Daniel Ortega, Dirk V¨ath, Gianna Weber, Lindsey Van-": "Fast,\nrobust and controllable text\nto speech.\nIn Ad-",
          "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki": ""
        },
        {
          "Daniel Ortega, Dirk V¨ath, Gianna Weber, Lindsey Van-": "vances\nin Neural\nInformation Processing Systems,",
          "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki": "tions of the ACM, 9(1)."
        },
        {
          "Daniel Ortega, Dirk V¨ath, Gianna Weber, Lindsey Van-": "pages 3165–3174.",
          "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki": ""
        },
        {
          "Daniel Ortega, Dirk V¨ath, Gianna Weber, Lindsey Van-": "",
          "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki": "Ryuichi Yamamoto, Eunwoo Song, and Jae-Min Kim."
        },
        {
          "Daniel Ortega, Dirk V¨ath, Gianna Weber, Lindsey Van-": "",
          "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki": "2020.\nParallel wavegan: A fast waveform genera-"
        },
        {
          "Daniel Ortega, Dirk V¨ath, Gianna Weber, Lindsey Van-": "Jost Schatzmann, Blaise Thomson, Karl Weilhammer,",
          "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki": ""
        },
        {
          "Daniel Ortega, Dirk V¨ath, Gianna Weber, Lindsey Van-": "",
          "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki": "tion model based on generative adversarial networks"
        },
        {
          "Daniel Ortega, Dirk V¨ath, Gianna Weber, Lindsey Van-": "Hui Ye, and Steve Young. 2007. Agenda-based user",
          "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki": ""
        },
        {
          "Daniel Ortega, Dirk V¨ath, Gianna Weber, Lindsey Van-": "",
          "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki": "ICASSP\nwith multi-resolution\nspectrogram.\nIn"
        },
        {
          "Daniel Ortega, Dirk V¨ath, Gianna Weber, Lindsey Van-": "simulation for bootstrapping a pomdp dialogue sys-",
          "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki": ""
        },
        {
          "Daniel Ortega, Dirk V¨ath, Gianna Weber, Lindsey Van-": "",
          "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki": "2020 - IEEE International Conference on Acoustics,"
        },
        {
          "Daniel Ortega, Dirk V¨ath, Gianna Weber, Lindsey Van-": "tem.\nIn Proceedings of NAACL.",
          "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki": ""
        },
        {
          "Daniel Ortega, Dirk V¨ath, Gianna Weber, Lindsey Van-": "",
          "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki": "Speech and Signal Processing, pages 6199–6203."
        },
        {
          "Daniel Ortega, Dirk V¨ath, Gianna Weber, Lindsey Van-": "Tom Schaul,\nJohn Quan,\nIoannis Antonoglou,\nand",
          "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki": ""
        },
        {
          "Daniel Ortega, Dirk V¨ath, Gianna Weber, Lindsey Van-": "",
          "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki": "Zhihong Zeng, Maja Pantic, Glenn I Roisman,\nand"
        },
        {
          "Daniel Ortega, Dirk V¨ath, Gianna Weber, Lindsey Van-": "David Silver. 2015. Prioritized experience replay.\nIn",
          "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki": ""
        },
        {
          "Daniel Ortega, Dirk V¨ath, Gianna Weber, Lindsey Van-": "",
          "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki": "Thomas S Huang. 2009. A survey of affect\nrecog-"
        },
        {
          "Daniel Ortega, Dirk V¨ath, Gianna Weber, Lindsey Van-": "Proceedings of ICLR.",
          "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki": ""
        },
        {
          "Daniel Ortega, Dirk V¨ath, Gianna Weber, Lindsey Van-": "",
          "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki": "nition methods: Audio, visual, and spontaneous ex-"
        },
        {
          "Daniel Ortega, Dirk V¨ath, Gianna Weber, Lindsey Van-": "",
          "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki": "IEEE transactions on pattern analysis\npressions."
        },
        {
          "Daniel Ortega, Dirk V¨ath, Gianna Weber, Lindsey Van-": "Bj¨orn Schuller, Ronald M¨uller, Florian Eyben, J¨urgen",
          "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki": ""
        },
        {
          "Daniel Ortega, Dirk V¨ath, Gianna Weber, Lindsey Van-": "",
          "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki": "and machine intelligence, 31(1):39–58."
        },
        {
          "Daniel Ortega, Dirk V¨ath, Gianna Weber, Lindsey Van-": "Gast, Benedikt H¨ornler, Martin W¨ollmer, Gerhard",
          "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki": ""
        },
        {
          "Daniel Ortega, Dirk V¨ath, Gianna Weber, Lindsey Van-": "Rigoll, Anja H¨othker,\nand Hitoshi Konosu. 2009.",
          "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki": ""
        },
        {
          "Daniel Ortega, Dirk V¨ath, Gianna Weber, Lindsey Van-": "",
          "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki": "Li Zhou, Jianfeng Gao, Di Li, and Heung-Yeung Shum."
        },
        {
          "Daniel Ortega, Dirk V¨ath, Gianna Weber, Lindsey Van-": "Being bored? Recognising natural interest by exten-",
          "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki": ""
        },
        {
          "Daniel Ortega, Dirk V¨ath, Gianna Weber, Lindsey Van-": "",
          "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki": "2018.\nThe Design and Implementation of XiaoIce,"
        },
        {
          "Daniel Ortega, Dirk V¨ath, Gianna Weber, Lindsey Van-": "sive audiovisual\nintegration for real-life application.",
          "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki": ""
        },
        {
          "Daniel Ortega, Dirk V¨ath, Gianna Weber, Lindsey Van-": "",
          "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki": "an Empathetic Social Chatbot. Computational Lin-"
        },
        {
          "Daniel Ortega, Dirk V¨ath, Gianna Weber, Lindsey Van-": "Image and Vision Computing, 27:1760–1774.",
          "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki": ""
        },
        {
          "Daniel Ortega, Dirk V¨ath, Gianna Weber, Lindsey Van-": "",
          "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki": "guistics, pages 1–62."
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "OpenFace 2.0: Facial Behavior Analysis Toolkit",
      "authors": [
        "Tadas Baltrusaitis",
        "Amir Zadeh",
        "Chong Lim",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "OpenFace 2.0: Facial Behavior Analysis Toolkit"
    },
    {
      "citation_id": "2",
      "title": "The In-proTK 2012 Release",
      "authors": [
        "Timo Baumann",
        "David Schlangen"
      ],
      "year": "2012",
      "venue": "NAACL-HLT Workshop on Future Directions and Needs in the Spoken Dialog Community: Tools and Data"
    },
    {
      "citation_id": "3",
      "title": "Gus, a frame-driven dialog system",
      "authors": [
        "G Daniel",
        "Ronald Bobrow",
        "Martin Kaplan",
        "Donald Kay",
        "Henry Norman",
        "Terry Thompson",
        "Winograd"
      ],
      "year": "1977",
      "venue": "Artificial Intelligence"
    },
    {
      "citation_id": "4",
      "title": "Rapid Development of Multimodal Interactive Systems: A Demonstration of Platform for Situated Intelligence",
      "authors": [
        "Dan Bohus",
        "Sean Andrist",
        "Mihai Jalobeanu"
      ],
      "year": "2017",
      "venue": "ICMI '17: Proceedings of the 19th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "5",
      "title": "The ravenclaw dialog management framework: Architecture and systems",
      "authors": [
        "Dan Bohus",
        "Alexander Rudnicky"
      ],
      "year": "2009",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "6",
      "title": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation"
    },
    {
      "citation_id": "7",
      "title": "Speaking while monitoring addressees for understanding",
      "authors": [
        "H Clark",
        "M Krych"
      ],
      "year": "2004",
      "venue": "Journal of Memory and Language"
    },
    {
      "citation_id": "8",
      "title": "Alana v2: Entertaining and Informative Opendomain Social Dialogue using Ontologies and Entity Linking",
      "authors": [
        "Amanda Cercas Curry",
        "Ioannis Papaioannou",
        "Alessandro Suglia",
        "Shubham Agarwal",
        "Igor Shalyminov",
        "Xinnuo Xu",
        "Ondȓej Dusek",
        "Arash Eshghi",
        "Ioannis Konstas",
        "Verena Rieser",
        "Oliver Lemon"
      ],
      "year": "2018",
      "venue": "1st Proceedings of Alexa Prize"
    },
    {
      "citation_id": "9",
      "title": "Spoken language understanding",
      "authors": [
        "Renato Mori",
        "Frédéric Bechet",
        "Dilek Hakkani-Tur",
        "Michael Mctear",
        "Giuseppe Riccardi",
        "Gokhan Tur"
      ],
      "year": "2008",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "10",
      "title": "Imsspeech: A speech to text tool",
      "authors": [
        "Pavel Denisov",
        "Ngoc Vu"
      ],
      "year": "2019",
      "venue": "Studientexte zur Sprachkommunikation: Elektronische Sprachsignalverarbeitung 2019"
    },
    {
      "citation_id": "11",
      "title": "Intrinsic and Extrinsic Evaluation of an Automatic User Disengagement Detector for an Uncertainty-Adaptive Spoken Dialogue System",
      "authors": [
        "Kate Forbes-Riley",
        "Diane Litman",
        "Heather Friedberg",
        "Joanna Drummond"
      ],
      "year": "2012",
      "venue": "Conference of the North American Chapter"
    },
    {
      "citation_id": "12",
      "title": "The MuM-MER project: Engaging human-robot interaction in real-world public spaces",
      "authors": [
        "Ellen Mary",
        "Rachid Foster",
        "Olli Alami",
        "Oliver Gestranius",
        "Marketta Lemon",
        "Jean Niemelä",
        "Amit Marc Odobez",
        "Pandey"
      ],
      "year": "2016",
      "venue": "Processings of the Eighth International Conference on Social Robotics (ICSR 2016)"
    },
    {
      "citation_id": "13",
      "title": "Between and within: Alternative sequential treatments of continuers and assessments",
      "authors": [
        "Charles Goodwin"
      ],
      "year": "1986",
      "venue": "Journal of Human Studies"
    },
    {
      "citation_id": "14",
      "title": "Espnet-tts: Unified, reproducible, and integratable open source end-to-end text-to-speech toolkit",
      "authors": [
        "Tomoki Hayashi",
        "Ryuichi Yamamoto",
        "Katsuki Inoue",
        "Takenori Yoshimura",
        "Shinji Watanabe",
        "Tomoki Toda",
        "Kazuya Takeda",
        "Yu Zhang",
        "Xu Tan"
      ],
      "year": "2019",
      "venue": "Espnet-tts: Unified, reproducible, and integratable open source end-to-end text-to-speech toolkit"
    },
    {
      "citation_id": "15",
      "title": "The lj speech dataset",
      "authors": [
        "Keith Ito"
      ],
      "year": "2017",
      "venue": "The lj speech dataset"
    },
    {
      "citation_id": "16",
      "title": "A comparative study on transformer vs rnn in speech applications",
      "authors": [
        "Shigeki Karita",
        "Nanxin Chen",
        "Tomoki Hayashi",
        "Takaaki Hori",
        "Hirofumi Inaguma",
        "Ziyan Jiang",
        "Masao Someki",
        "Nelson Enrique",
        "Yalta Soplin",
        "Ryuichi Yamamoto",
        "Xiaofei Wang"
      ],
      "year": "2019",
      "venue": "IEEE Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "17",
      "title": "ConvLab: Multi-Domain End-to-End Dialog System Platform",
      "authors": [
        "Sungjin Lee",
        "Qi Zhu",
        "Ryuichi Takanobu",
        "Zheng Zhang",
        "Yaoqin Zhang",
        "Xiang Li",
        "Jinchao Li",
        "Baolin Peng",
        "Xiujun Li",
        "Minlie Huang",
        "Jianfeng Gao"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations"
    },
    {
      "citation_id": "18",
      "title": "Opendial: A toolkit for developing spoken dialogue systems with probabilistic rules",
      "authors": [
        "Pierre Lison",
        "Casey Kennington"
      ],
      "year": "2016",
      "venue": "Proceedings of ACL-2016 system demonstrations"
    },
    {
      "citation_id": "19",
      "title": "The conversational interface",
      "authors": [
        "Michael Frederick Mctear",
        "Zoraida Callejas",
        "David Griol"
      ],
      "year": "2016",
      "venue": "The conversational interface"
    },
    {
      "citation_id": "20",
      "title": "Attentive convolutional neural network based speech emotion recognition: A study on the impact of input features, signal length, and acted speech",
      "authors": [
        "Michael Neumann",
        "Ngoc Vu"
      ],
      "year": "2017",
      "venue": "Proceedings of Interspeech"
    },
    {
      "citation_id": "21",
      "title": "Automatic Engagement Prediction with GAP Feature",
      "authors": [
        "Xuesong Niu",
        "Hu Han",
        "Jiabei Zeng",
        "Xuran Sun",
        "Shiguang Shan",
        "Yan Huang",
        "Songfan Yang",
        "Xilin Chen"
      ],
      "year": "2018",
      "venue": "ICMI"
    },
    {
      "citation_id": "22",
      "title": "Oh, Jeez! or uh-huh? A listener-aware Backchannel predictor on ASR transcriptions",
      "authors": [
        "Daniel Ortega",
        "Chia-Yu Li",
        "Thang Vu"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "23",
      "title": "Adviser: A dialog system framework for education & research",
      "authors": [
        "Daniel Ortega",
        "Dirk Väth",
        "Gianna Weber",
        "Lindsey Vanderlyn",
        "Maximilian Schmidt",
        "Moritz Völkel",
        "Zorica Karacevic",
        "Ngoc Vu"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations"
    },
    {
      "citation_id": "24",
      "title": "Fastspeech: Fast, robust and controllable text to speech",
      "authors": [
        "Yi Ren",
        "Yangjun Ruan",
        "Xu Tan",
        "Tao Qin",
        "Sheng Zhao",
        "Zhou Zhao",
        "Tie-Yan Liu"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "25",
      "title": "Agenda-based user simulation for bootstrapping a pomdp dialogue system",
      "authors": [
        "Jost Schatzmann",
        "Blaise Thomson",
        "Karl Weilhammer",
        "Hui Ye",
        "Steve Young"
      ],
      "year": "2007",
      "venue": "Proceedings of NAACL"
    },
    {
      "citation_id": "26",
      "title": "Prioritized experience replay",
      "authors": [
        "Tom Schaul",
        "John Quan",
        "Ioannis Antonoglou",
        "David Silver"
      ],
      "year": "2015",
      "venue": "Proceedings of ICLR"
    },
    {
      "citation_id": "27",
      "title": "Being bored? Recognising natural interest by extensive audiovisual integration for real-life application",
      "authors": [
        "Björn Schuller",
        "Ronald Müller",
        "Florian Eyben",
        "Jürgen Gast",
        "Benedikt Hörnler",
        "Martin Wöllmer",
        "Gerhard Rigoll",
        "Anja Höthker",
        "Hitoshi Konosu"
      ],
      "year": "2009",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "28",
      "title": "Exploring turn-taking cues in multi-party human-robot discussions about objects",
      "authors": [
        "Gabriel Skantze",
        "Martin Johansson",
        "Jonas Beskow"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 ACM on International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "29",
      "title": "PyDial: A Multi-domain Statistical Dialogue System Toolkit",
      "authors": [
        "Stefan Ultes",
        "Lina Barahona",
        "Pei-Hao Su",
        "David Vandyke",
        "Dongho Kim",
        "Iñigo Casanueva",
        "Paweł Budzianowski",
        "Nikola Mrkšić",
        "Tsung-Hsien Wen",
        "Milica Gasic",
        "Steve Young"
      ],
      "year": "2017",
      "venue": "Proceedings of ACL"
    },
    {
      "citation_id": "30",
      "title": "To combine or not to combine? a rainbow deep reinforcement learning agent for dialog policies",
      "authors": [
        "Dirk Väth",
        "Ngoc Vu"
      ],
      "year": "2019",
      "venue": "Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue"
    },
    {
      "citation_id": "31",
      "title": "Wikidata: a free collaborative knowledgebase",
      "authors": [
        "Denny Vrandečić",
        "Markus Krötzsch"
      ],
      "year": "2014",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "32",
      "title": "The social signal interpretation (ssi) framework: multimodal signal processing and recognition in real-time",
      "authors": [
        "Johannes Wagner",
        "Florian Lingenfelser",
        "Tobias Baur",
        "Ionut Damian",
        "Felix Kistler",
        "Elisabeth André"
      ],
      "year": "2013",
      "venue": "Proceedings of the 21st ACM international conference on Multimedia"
    },
    {
      "citation_id": "33",
      "title": "Dueling network architectures for deep reinforcement learning",
      "authors": [
        "Ziyu Wang",
        "Tom Schaul",
        "Matteo Hessel",
        "Hado Van Hasselt",
        "Marc Lanctot",
        "Nando Freitas"
      ],
      "year": "2016",
      "venue": "Proceedings of ICML"
    },
    {
      "citation_id": "34",
      "title": "Espnet: End-to-end speech processing toolkit",
      "authors": [
        "Shinji Watanabe",
        "Takaaki Hori",
        "Shigeki Karita",
        "Tomoki Hayashi",
        "Jiro Nishitoba",
        "Yuya Unno",
        "Nelson Enrique",
        "Yalta Soplin",
        "Jahn Heymann",
        "Matthew Wiesner",
        "Nanxin Chen",
        "Adithya Renduchintala",
        "Tsubasa Ochiai"
      ],
      "year": "2018",
      "venue": "Espnet: End-to-end speech processing toolkit",
      "doi": "10.21437/Interspeech.2018-1456"
    },
    {
      "citation_id": "35",
      "title": "ELIZA: A Computer Program for the Study of Natural Language Communication Between Man and Machine",
      "authors": [
        "Joseph Weizenbaum"
      ],
      "year": "1966",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "36",
      "title": "Parallel wavegan: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram",
      "authors": [
        "Ryuichi Yamamoto",
        "Eunwoo Song",
        "Jae-Min Kim"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "37",
      "title": "A survey of affect recognition methods: Audio, visual, and spontaneous expressions",
      "authors": [
        "Zhihong Zeng",
        "Maja Pantic",
        "Thomas Glenn I Roisman",
        "Huang"
      ],
      "year": "2009",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "38",
      "title": "The Design and Implementation of XiaoIce, an Empathetic Social Chatbot",
      "authors": [
        "Li Zhou",
        "Jianfeng Gao",
        "Di Li",
        "Heung-Yeung Shum"
      ],
      "year": "2018",
      "venue": "Computational Linguistics"
    }
  ]
}