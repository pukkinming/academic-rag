{
  "paper_id": "2307.07961v2",
  "title": "Emoset: A Large-Scale Visual Emotion Dataset With Rich Attributes",
  "published": "2023-07-16T06:42:46Z",
  "authors": [
    "Jingyuan Yang",
    "Qirui Huang",
    "Tingting Ding",
    "Dani Lischinski",
    "Daniel Cohen-Or",
    "Hui Huang"
  ],
  "keywords": [
    "based on Mikels model  [29]",
    "yielding the EmoSet-118K dataset. Compared with existing datasets",
    "EmoSet contains diverse images covering both social and artistic types. Furthermore",
    "EmoSet-118K is well balanced between the eight emotion categories",
    "each of which is represented with 10",
    "660 to 19",
    "828 images",
    "as reported in Table  2 . We f"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "EmoSet images are annotated with eight emotion categories (blue) and six emotion attributes (orange), where different attributes may evoke different emotions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotions are different ways to think that our mind uses to increase our intelligence  [31] . Much of the research in Artificial Intelligence (AI) has focused on designing human-like machines, while neglecting emotional intelligence. Since emotions are innate to human beings, AI systems should aim to better understand emotions, in order to succeed in mimicking human behavior. Affective computing  [35]  is an emerging field that aims to identify, understand, and respond to human emotions. This field has seen significant progress in recent years, and has potential applications in areas such as education  [49] , healthcare  [56] , advertising  [42] , and safety  [9] .\n\nVisual Emotion Analysis (VEA) is a promising, yet challenging, task in affective computing, aiming to predict emotional responses to visual stimuli. For instance, when viewing the images in Figure  1 , one not only recognizes the visual elements therein, but also may experience emotional reactions. Furthermore, even though emotions are subjective, people tend to share similar reactions to the same external stimuli. With the prevalence of social networks, users often choose to convey feelings via images shared on the in-ternet. Thus, VEA is an increasingly popular research topic within the computer vision field  [51, 63] . Advances in VEA may benefit high-level vision tasks (e.g., image aesthetic assessment  [26] , stylized image captioning  [12] , and image understanding  [47] ), as well as human-centered applications (e.g., opinion mining  [27] , mental health  [46] , smart advertisement  [41] , and hate detection  [1] ).\n\nMost of the work in VEA focused on feature design, covering hand-crafted features  [28, 60, 3]  and, more recently, learned ones  [37, 53, 51] . Based on art and psychological theories, hand-crafted features fail to cover all important factors in human emotions. Although deep learning methods boost recognition performance significantly, the results are still unsatisfying. In particular, while supervised deep learning methods often require large-scale labeled datasets, little attention has been paid to dataset construction. Existing VEA datasets are usually unlabeled on a large scale or labeled on a relatively small scale  [24, 58, 34] . Besides, only emotion labels are provided in most datasets. Since emotions are abstract, a key problem is how to bridge the affective gap  [13]  between images and emotions with auxiliary information. We believe that a new and rich dataset is needed for further research and improvement in VEA.\n\nTo tackle the above issues, we introduce EmoSet, a largescale visual emotion dataset, annotated with rich attributes. EmoSet is superior to existing datasets in four aspects: scale, annotation richness, diversity, and data balance. The full (EmoSet-3.3M) dataset comprises 3.3 million machine retrieved and annotated images, among which there are 118,102 human-annotated ones (EmoSet-118K). The latter is five times larger than the widely-used FI dataset  [58] , as reported in Table  1 . Apart from emotions, our dataset is annotated with emotion attributes. Inspired by psychological studies  [22, 4, 7] , we propose a set of describable visual attributes to facilitate understanding why an image evokes a certain emotion. Considering the complexity of emotions, the attributes are designed to cover different levels of visual information, including brightness, colorfulness, scene type, object class, facial expression, and human action. With these rich attribute annotations, we hope EmoSet will improve not only the recognition of visual emotions, but also their understanding.\n\nBy querying 810 emotion keywords based on Mikels model  [29] , we collect 3.3 million candidate images from four different sources to form the EmoSet-3.3M dataset. A subset of EmoSet-3.3M is then labeled by human annotators, yielding the EmoSet-118K dataset. Compared with existing datasets, EmoSet contains diverse images covering both social and artistic types. Furthermore, EmoSet-118K is well balanced between the eight emotion categories, each of which is represented with 10,660 to 19,828 images, as reported in Table  2 . We further analyze the correlations between our attributes and emotion categories, and demon-strate that some attributes are indeed strongly relevant to emotions. In addition, to mine the emotion-related information from each attribute, we design an attribute module for visual emotion recognition, and validate it using several CNN backbones.\n\nIn summary, our contributions are:\n\n• EmoSet, the first large-scale visual emotion dataset with rich attributes, exceeding existing VEA datasets in terms of scale, annotation richness, diversity and data balance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Visual Emotion Datasets",
      "text": "In psychology, emotion models can be grouped into two types: Categorical Emotion States (CES), where emotions are described with discrete categories, and Dimensional Emotion Space (DES), where a continuous space is used to represent emotions  [62] . CES models are more popular in VEA for their simplicity and interpretability, including 2-category sentiment model, 6-category Ekman model  [7] , and 8-category Mikels model  [29] .\n\nNumerous datasets have been constructed to study people's different emotional reactions toward images  [29, 34, 58] . In Table  1  we report various statistics of widely-used VEA datasets, as well as our new dataset, EmoSet. Considering the diversity in image types, emotion models and dataset scales, below we discuss five of the existing datasets.\n\nIAPSa. IAPS  [24]  aims to investigate the possible relationships between emotions and visual stimuli. IAPSa  [29]  is a subset of IAPS, built on the Mikels model with eight emotion categories covering amusement, awe, contentment, excitement, anger, disgust, fear, and sadness. IAPSa comprises 395 affective images, and is the first visual emotion dataset with discrete categories.\n\nArtPhoto. There are a total of 806 artistic images in Art-Photo  [28] , which is collected from an art sharing website by using emotion categories as the search keywords.\n\nEmotion6. Emotion6  [34]     [17]  are crawled from the internet by searching emotional categories. After labeling by crowd-sourced human annotation, 60,745 and 42,856 affective images are preserved with sentiment labels (i.e., positive or negative). Specifically, each image is labeled by 3 workers, where the ground-truth is determined by a majority vote.\n\nFI. Another dataset based on Flickr and Instagram is FI  [58] . It is one of the largest scale visual emotion datasets to date, with a total of 23,308 labeled images. Using eight emotion words in Mikels model as queries, FI collects candidate images from Flickr and Instagram. Each collected image is then labeled by 5 Amazon Mechanical Turk (AMT) workers, and images having more than 3 votes are kept in the final dataset. FI serves as one of the most widely-used datasets in VEA.\n\nIn this work, we introduce EmoSet, which is five times larger than the FI dataset. In addition to the Mikels emotion category, each image is annotated with six comprehensively designed emotion attributes, including brightness, colorfulness, scene type, object class, facial expression and human action. The images in our dataset come from more diverse sources and the dataset is more balanced across emotion categories, as reported in Tables  1  and 2 .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Visual Emotion Recognition",
      "text": "Researchers have been engaged in VEA for two decades, with approaches ranging from the early traditional ones to the recent deep learning ones. Machajdik et al.  [28]  extracts specific image features to predict emotions, i.e., color, texture, composition, and content inspired by psychology and art theory. Adjective Noun Pairs (ANPs) are introduced by Borth et al.  [3]  to help learn visual emotions from a semantic level. By extracting a set of principle-of-art-based emotional features like balance, emphasis, harmony, vari-ety, gradation, and movement, Zhao et al.  [60]  proposes a method to deal with both classification and regression tasks. Traditional methods fail to cover all important factors related to human emotions, leading to sub-optimal results.\n\nBased on deep learning techniques, You et al.  [57]  propose a progressive CNN (PCNN), and Rao et al.  [37]  build a multi-level deep representation network (MldrNet). Early attempts usually focus on extracting holistic image features, while neglecting the importance of local regions. Yang et al.  [54]  leverages object detection, as well as attention mechanism  [53]  to help emotion recognition. With specially designed emotional features, Yang et al.  [52]  construct a network to learn emotions from different visual stimuli and to mine the correlations between them  [51] . However, development in VEA is still unsatisfying due to low classification accuracy and the use of generic network design. Considering the abstract nature of emotion, it is necessary to introduce auxiliary information to assist visual emotion recognition. We believe that a large-scale dataset with rich annotations can help to mitigate the affective gap  [13]  between images and emotion labels.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Construction Of Emoset",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Data Collection",
      "text": "To build EmoSet, we need to retrieve a large number of images from the Internet. Since not all images are likely to arouse significant emotion, we construct a list of emotion keywords to help filter candidate images for our dataset. Following previous work  [28, 58] , EmoSet is built on the widely-used Mikels model  [29]  with eight categories, i.e., amusement, awe, contentment, excitement, anger, disgust, fear, sadness, where the former four are positive emotions and the latter four are negative ones. Each of the eight emotion categories is first synonymized according to three widely-used English dictionaries: WordNet  [30] , Collins  [14]  and Roget's  [39] . For instance, \"sadness\" is synonymized to words like \"depression, sorrow, mourn, despair, grieve\". Since the number of retrievable images is often limited for each query, we combine the synonyms and further augment them with different parts of speech, aiming at retrieving a large amount of data. For example, \"amusement\" is augmented with other word forms like \"amuse, amuses, amused, amusing, amusingly\". The final list contains 810 keywords, serving as queries to retrieve candidate images from the Internet. For more details, please refer to the supplementary material.\n\nIn view of the fact that in most image-text pairs, the two modalities are in agreement, i.e., the textual tag or description indeed reflects the emotion that the image conveys, eqach retrieved image is automatically labeled with one of the eight emotions from which the query used to retrieve that image was derived. For larger scale and richer diversity, EmoSet is collected from four different sources including openverse, pexels, pixabay and rawpixels. In total, 4.3 million images are collected, followed by voting in order to determine the labels of images with more than one emotion tag and removal of duplicates considering both file names and pixel-wise similarities, leaving 3.3 million images. With such a large amount of images tagged with text descriptions, EmoSet-3.3M has great potential for weakly supervised learning  [65] , vision-language modeling  [36]  and multi-modal emotion analysis  [44] .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Emotion Attributes Design",
      "text": "Aiming at figuring out some possible visual cues related to human emotions, we propose a set of describable emotion attributes inspired by psychological studies. Since emotions are abstract and complex, emotion attributes are designed to cover different levels of visual information: lowlevel (i.e., brightness, colorfulness), mid-level (i.e., scene type, object class), and high-level (i.e., facial expression, human action). We leverage both classic traditional methods and well-trained deep models to automatically predict attributes, which then constitute part of the automatic annotation, along with the emotion labels.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Brightness:",
      "text": "The overall lighting level of an image has been proven to be essential in perceptual processing and is closely related to human emotions  [22] . We use discrete numerical values ranging from 0 (darkest) to 1 (brightest), with increments of 0.1, to quantify the brightness of an image  [5] . For more details, please refer to the supplementary material. Colorfulness: Psychological studies  [38]  suggest that there are correlations between the perceived colorfulness of an image and affect. Similarly to brightness, colorfulness is calculated, normalized and discretized to the range of 0 to 1  [32] . Specifically, 0 corresponds to black-and-white images, while 1 refers to the most colorful ones. Scene type: The scene depicted in an image is often considered as an important emotional stimulus  [4] . We use a scene recognition model trained with Places365  [64] , a well-known benchmark for scene recognition. Out of 365 scene categories (e.g., sky, mountain, balcony, plaza, and church), we choose the top prediction as the scene type label for each image in our dataset. Object class: Psychologists have long investigated the relationships between objects and emotions  [10] . Thus, we associate object labels with each image in our dataset. Our object detection model is built on the OpenImagesV4 dataset  [23] . Considering that multiple objects may appear in an image and jointly evoke emotions, we associate with each image the three object classes predicted with the highest confidence. Facial expression: Facial expression can undoubtedly influence visual emotion experience  [7] , where people tend to empathize with the one in image. In Ekman model, there are six basic facial expressions: happy, angry, disgust, fear, sad, and surprise. We crop the largest face in the image and apply a model pre-trained on FER2013  [11]  to obtain the facial expression label. Human action: Some human actions stem from emotion and can also arouse emotion in an observer  [66] . Kinetics 400 is a large video dataset for human action recognition  [18] , which includes various actions like dining, water sliding, playing piano, barbecuing, and training a dog. Since Kinetics 400 is based on the video modality, we convert the image into a singleframe video as input and feed it into the UniformerV2 model  [25]  to predict the human action label.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Human Annotation",
      "text": "EmoSet-3.3M is automatically labeled by queries (i.e., emotions) and machines (i.e., attributes) without human participation. To build a more carefully annotated dataset, we invite humans to help annotate and ask them to take the qualification tests first. We ask the participants to take the empathy quotient test  [2]  to verify that they are sensitive to emotions, where annotators are qualified with a score greater than 30. Subsequently, we randomly select 100 emotion-labeled images from the FI dataset to evaluate the classification accuracy of the participants, with a passing rate at 85%. We hired 60 annotators who passed all the above tests, thereby meeting our criteria.\n\nThere are three main challenges in visual emotion analysis: abstractness, ambiguity and subjectivity. For abstractness, we introduce a set of attributes to help understand emotion in a more precise and interpretable way. The annotation tool is presented in Figure  2 , where annotators are  required to answer several questions on emotion (Q1) and attributes (Q2-Q9). For example, annotators are asked \"Do you feel excitement when you see this picture?\" (emotion) or \"Is this a picture of formal garden?\" (attribute). Since emotions are ambiguous, it is much easier for annotators to indicate whether an image evokes a specific emotion, rather than asking them to decide which emotion a given image evokes. Fewer choices may lead to more accurate results. Thus, we ask the annotators to verify both the emotion and the attribute labels for each image by answering \"yes\" or \"no\", instead of selecting a specific category, following previous work  [6, 58] . To mitigate the subjectivity in emotion annotations, in EmoSet, each image is labeled by 10 annotators. For each image, annotation results that reached a consensus of more than 7 out of 10 annotators are regarded as the final label. In particular, images with more than 7 votes for \"yes\" in emotion label are preserved while others are deleted. For more details, please refer to the supplementary material. By the end, EmoSet-118K is carefully labeled with human annotations, where both emotion labels and attribute labels are provided. Note that the analysis and evaluation in this paper are reported on EmoSet-118K.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Analysis Of Emoset",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Properties Of Emoset",
      "text": "EmoSet aims at constructing a comprehensive and interpretable dataset, which can help researchers to dive deep into visual emotions. To our knowledge, this is the first large-scale VEA dataset that is also annotated with rich attributes, as shown in Table  1 . In general, EmoSet has advantages in four aspects compared with the existing datasets: scale, annotation richness, diversity, and data balance.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Emotion-Attribute Analysis",
      "text": "Emotion attributes are designed to help visual emotion recognition as well as understanding. To validate their effectiveness, we conduct several numerical experiments and visualizations on EmoSet to examine the relationships between attributes and emotions.\n\nIn Mikels model, amusement, awe, contentment, and excitement are considered positive emotions, while anger, disgust, fear, and sadness are negative ones. The two polarities (i.e., positive, negative) can be seen as more basic emotional elements, than the eight more specific categories. One might hypothesize that brighter and/or more colorful images are more likely to evoke a positive emotion. We verify this hypothesis in Figure  4 , where we plot the break-  down of each brightness and colorfulness level into negative (blue) and positive (orange) emotions. Indeed, it may be seen that the proportion of images with a positive emotion label increases from left to right.\n\nOur facial expression attribute is built upon Ekman model, where happy is positive, surprise is neutral, and other four (i.e., angry, disgust, fear, and sad) are negative. In this experiment, we would like to see how facial expressions influence visual emotions. In Figure  4 , we show the breakdown of facial expressions for different visual emotions. Unsurprisingly, all of the positive emotions exhibit a high correlation with a happy facial expression, while anger, disgust, and sadness are highly correlated with their corresponding facial expressions. Interestingly, for fear, the top facial expression is happy, probably because the image contains a sinister or a spooky smile. The above experiment indicates that people are easily affected by the facial expressions present in the image, a manifestation of empathy  [8] .\n\nEach of the attributes scene type, object class, and human action, may have many different values, as in Figure  3 . Obviously, some attribute values are strongly related to emotions (e.g., amusement park, cemetery, laughing, or crying), while others are not, such as sky, plant, tree, or window. To discover emotion-related attribute values, we calculate the co-occurrence between each emotion-attribute pair and adopt the TF-IDF technique  [40] , where the importance of a value increases when it appears in a specific emotion and decreases with its appearance in the whole dataset. Figure  5  presents the correlation matrices between each emotion and its top-1 attribute value, where the large number on the diagonal suggests a strong relationship between them, with an average on 0.85 (scene type), 0.86 (object class) and 0.83 (human action). The statistics in Figure  5  are highly consistent with human cognition, indicating that some attribute values are indeed strongly related to emotions. Once a certain attribute value appears, the image is much more likely to evoke the corresponding emotion.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Evaluation Of Emoset",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Datasets Comparison",
      "text": "Development in VEA has been greatly limited by the lack of large-scale, high-quality datasets, leading to unsatisfying results in visual emotion recognition, i.e., 59.60% in Emotion6, 70.07% in FI, as shown in Table  3 . Consisting of 118,102 carefully annotated images, EmoSet aims at serving as an important dataset in VEA. To verify the quality of EmoSet, we compare it with other datasets, where results are reported in top-1 accuracy (%). We conduct experiments by leveraging both classic convolutional neural networks, i.e., AlexNet  [21] , VGG-16  [43] , ResNet-50  [15]  and DenseNet-121  [16] , as well as VEA-oriented methods, i.e., WSCNet  [53] , StyleNet  [59] , PDANet  [61] , Stimuliaware  [52]  and MDAN  [48] . The classic convolutional  neural networks are first pretrained on ImageNet  [6] , then fine-tuned and tested on each dataset respectively, while the VEA-oriented methods are trained and tested following their specific settings. For EmoSet, we split the data into 80% training set, 5% validation set and 15% test set, in accordance with that of FI. Our network is trained by the adaptive optimizer Adam  [19] , where the learning rate is set as the default one in each method. Our experiments are implemented based on PyTorch  [33]  and performed on an NVIDIA GTX 3090 GPU. Notably, some datasets are labeled with two (e.g., Twitter I-2) or six (e.g., Emotion6-6) emotion categories, making it unfair to compare them with the eight-category EmoSet. Therefore, we degenerate eight emotions to two sentiments, denoted as EmoSet-2, where amusement, awe, contentment and excitement are positive and anger, disgust, fear and sadness are negative. In Table  3 , EmoSet reaches the best performance in both 2-sentiment and 8-category recognition tasks, compared with other VEA datasets.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Attribute-Aware Visual Emotion Recognition",
      "text": "We propose an attribute module to facilitate visual emotion recognition, as shown in Figure  6 , which can be easily attached to a backbone network. Attribute module is built with three branches, namely low-level, mid-level and high-level, to extract different visual information from a given image. The extracted features are then sent to several lightweight convolutional layers, yielding attribute features. Take backbone as main branch, features from other branches are fused with it to jointly predict visual emotions. According to Section 3.2, there are many options for selecting attributes. In our experiments, we choose brightness, scene type and facial expression as representatives, which results are shown in Table  4 . Each attribute branch is supervised by its ground-truth label, while the main branch is supervised by the emotion label. The whole network is trained on EmoSet in an end-to-end manner. To verify how attributes assist emotion recognition, we further visualize the attribute features extracted from different branches, i.e., brightness, scene type and facial expression, as shown in Figure  7 . Our visualizations are based on test set. Each 2048-dimensional attribute feature is projected to a 2-dimensional vector by using t-Distributed Stochastic Neighbor Embedding (t-SNE)  [45] , shown as a  In scene type, \"ruin\" and \"playroom\" are separated with a large distance, locating in sadness and amusement respectively. Different attribute values fall in different emotional areas, which suggests that attribute features have been trained to distinguish both emotions and attributes jointly. Visualization results in Figure  7  further proved the effectiveness of emotion attributes on assisting visual emotion recognition and understanding, which is also consistent with our human cognition.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Cross-Dataset Generalization",
      "text": "To demonstrate the generalization ability of EmoSet, we conduct a cross-dataset validation between EmoSet and the large-scale FI in Table  5 . We choose ResNet-50 as our backbone. To validate the performance on a broader sense, we conduct two experiment settings: with or without pretrained on ImageNet, denoted as w/ pretrained, w/o pretrained. Trained on FI, the backbone meets a performance drop of 26.98% (w/o pretrained) and 20.09% (w/ pretrained), compared to the baseline of EmoSet. Conversely, trained on EmoSet, the backbone meets a performance drop of 5.36% and 11.86%, correspondingly. The above results illustrate that EmoSet is more capable to generalize to FI, compare with the opposite. We further conduct validations by introducing a third-party test dataset, Artphoto, which consists of artistic images. Since Artphoto is a small-scale dataset with 806 images, we only use it for test purpose. In each setting, model trained on EmoSet performs better than that of FI, resulting from the diverse image types in EmoSet, i.e., social and artistic. Consisting of high-quality and diverse images, EmoSet is robust to generalize to other VEA datasets with a good visual emotion representation, which may bring new opportunities to visual emotion recognition.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "EmoSet is built with three main goals. The first one is to provide a large-scale, diverse and balanced VEA dataset, which may offer new opportunities for visual emotion recognition. Second, we believe that the rich annotated attributes can serve as auxiliary information to boost recognition performance. Most importantly, we hope that the comprehensively designed emotion attributes will encourage VEA researchers to turn their eyes from recognition to understanding, and to dive deep into visual emotion.\n\nThe underlying premise of our work is that each image evokes a single type of emotion. In reality, different emotions may be evoked at the same time, considering the subjectivity of human emotions. Besides, our work is built upon Mikels emotion model with eight categories, following previous work. It is obvious that emotions are complex, and it is hard to precisely classify them into only a few discrete types. We will explore more in these directions.\n\nWith 3.3 million images tagged with emotions and texts, EmoSet has the potential for weakly supervised learning, vision-language modeling and multi-modal emotion analysis. With rich attribute annotations, EmoSet also holds promise for visual emotion generation and editing.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: EmoSet images are annotated with eight emotion categories (blue) and six emotion attributes (orange), where",
      "page": 1
    },
    {
      "caption": "Figure 1: , one not only recognizes the vi-",
      "page": 1
    },
    {
      "caption": "Figure 2: Graphical interface of annotation tool.",
      "page": 4
    },
    {
      "caption": "Figure 2: , where annotators are",
      "page": 4
    },
    {
      "caption": "Figure 3: Word cloud distributions of scene type, object",
      "page": 5
    },
    {
      "caption": "Figure 3: , we present the word cloud dis-",
      "page": 5
    },
    {
      "caption": "Figure 4: , where we plot the break-",
      "page": 5
    },
    {
      "caption": "Figure 4: Histogram of brightness, colorfulness and facial expression, where different colors suggest different categories.",
      "page": 6
    },
    {
      "caption": "Figure 5: Correlation matrices of scene type, object class and human action, where numbers on the diagonal indicate the",
      "page": 6
    },
    {
      "caption": "Figure 4: , we show the",
      "page": 6
    },
    {
      "caption": "Figure 5: presents the correlation matrices between each emotion and",
      "page": 6
    },
    {
      "caption": "Figure 5: are highly con-",
      "page": 6
    },
    {
      "caption": "Figure 6: , which can be eas-",
      "page": 7
    },
    {
      "caption": "Figure 6: The proposed attribute module.",
      "page": 7
    },
    {
      "caption": "Figure 7: Our visualizations are",
      "page": 7
    },
    {
      "caption": "Figure 7: Scatter diagram of brightness, scene type and facial expression, where different colors represent attribute features",
      "page": 8
    },
    {
      "caption": "Figure 7: further proved the",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table 3: . Consist-",
      "data": [
        {
          "Column_1": "",
          "Column_2": "Happ\nSurpr\nSad\nDisgu",
          "Column_3": "y\nise\nst",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": ""
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Angrybert: Joint learning target and emotion for hate speech detection",
      "authors": [
        "Md Rabiul Awal",
        "Rui Cao",
        "Ka-Wei Lee",
        "Sandra Mitrović"
      ],
      "year": "2021",
      "venue": "Pacific-Asia conference on knowledge discovery and data mining"
    },
    {
      "citation_id": "2",
      "title": "The empathy quotient: an investigation of adults with asperger syndrome or high functioning autism, and normal sex differences",
      "authors": [
        "Simon Baron-Cohen",
        "Sally Wheelwright"
      ],
      "year": "2004",
      "venue": "Journal of autism and developmental disorders"
    },
    {
      "citation_id": "3",
      "title": "Large-scale visual sentiment ontology and detectors using adjective noun pairs",
      "authors": [
        "Damian Borth",
        "Rongrong Ji",
        "Tao Chen",
        "Thomas Breuel",
        "Shih-Fu Chang"
      ],
      "year": "2013",
      "venue": "Proceedings of the 21st ACM international conference on Multimedia"
    },
    {
      "citation_id": "4",
      "title": "The perception and categorisation of emotional stimuli: A review",
      "authors": [
        "Tobias Brosch",
        "Gilles Pourtois",
        "David Sander"
      ],
      "year": "2010",
      "venue": "The perception and categorisation of emotional stimuli: A review"
    },
    {
      "citation_id": "5",
      "title": "Studying aesthetics in photographic images using a computational approach",
      "authors": [
        "Ritendra Datta",
        "Dhiraj Joshi",
        "Jia Li",
        "James Wang"
      ],
      "year": "2006",
      "venue": "European conference on computer vision"
    },
    {
      "citation_id": "6",
      "title": "Imagenet: A large-scale hierarchical image database",
      "authors": [
        "Jia Deng",
        "Wei Dong",
        "Richard Socher",
        "Li-Jia Li",
        "Kai Li",
        "Li Fei-Fei"
      ],
      "year": "2009",
      "venue": "2009 IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "7",
      "title": "Facial expression and emotion",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1993",
      "venue": "American psychologist"
    },
    {
      "citation_id": "8",
      "title": "Psychotherapy",
      "authors": [
        "Robert Elliott",
        "Arthur Bohart",
        "Jeanne Watson",
        "Leslie Greenberg",
        "Empathy"
      ],
      "year": "2011",
      "venue": "Psychotherapy"
    },
    {
      "citation_id": "9",
      "title": "Emotion on the road-necessity, acceptance, and feasibility of affective computing in the car",
      "authors": [
        "Florian Eyben",
        "Martin Wöllmer",
        "Tony Poitschke",
        "Björn Schuller",
        "Christoph Blaschke",
        "Berthold Färber",
        "Nhu Nguyen-Thien"
      ],
      "year": "2010",
      "venue": "Advances in human-computer interaction"
    },
    {
      "citation_id": "10",
      "title": "Emotion experience and its varieties",
      "authors": [
        "H Nico",
        "Frijda"
      ],
      "year": "2009",
      "venue": "Emotion Review"
    },
    {
      "citation_id": "11",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "Ian Goodfellow",
        "Dumitru Erhan",
        "Pierre Carrier",
        "Aaron Courville",
        "Mehdi Mirza",
        "Ben Hamner",
        "Will Cukierski",
        "Yichuan Tang",
        "David Thaler",
        "Dong-Hyun Lee"
      ],
      "year": "2013",
      "venue": "International conference on neural information processing"
    },
    {
      "citation_id": "12",
      "title": "Mscap: Multi-style image captioning with unpaired stylized text",
      "authors": [
        "Longteng Guo",
        "Jing Liu",
        "Peng Yao",
        "Jiangwei Li",
        "Hanqing Lu"
      ],
      "year": "2019",
      "venue": "CVPR"
    },
    {
      "citation_id": "13",
      "title": "Extracting moods from pictures and sounds: Towards truly personalized tv",
      "authors": [
        "Alan Hanjalic"
      ],
      "year": "2006",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "14",
      "title": "Collins dictionary of the english language",
      "authors": [
        "Patrick Hanks"
      ],
      "year": "1979",
      "venue": "Collins dictionary of the english language"
    },
    {
      "citation_id": "15",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "CVPR"
    },
    {
      "citation_id": "16",
      "title": "Densely connected convolutional networks",
      "authors": [
        "Gao Huang",
        "Zhuang Liu",
        "Laurens Van Der Maaten",
        "Kilian Weinberger"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "17",
      "title": "Image sentiment analysis using latent correlations among visual, textual, and sentiment views",
      "authors": [
        "Marie Katsurai",
        "Shin'ichi Satoh"
      ],
      "year": "2016",
      "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "18",
      "title": "The kinetics human action video dataset",
      "authors": [
        "Will Kay",
        "Joao Carreira",
        "Karen Simonyan",
        "Brian Zhang",
        "Chloe Hillier",
        "Sudheendra Vijayanarasimhan",
        "Fabio Viola",
        "Tim Green",
        "Trevor Back",
        "Paul Natsev"
      ],
      "year": "2017",
      "venue": "The kinetics human action video dataset",
      "arxiv": "arXiv:1705.06950"
    },
    {
      "citation_id": "19",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "20",
      "title": "Emotic: Emotions in context dataset",
      "authors": [
        "Ronak Kosti",
        "Jose Alvarez",
        "Adria Recasens",
        "Agata Lapedriza"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "21",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "Alex Krizhevsky",
        "Ilya Sutskever",
        "Geoffrey Hinton"
      ],
      "year": "2012",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "22",
      "title": "Tubanur Bayram Kuzgun, and Bahar Güntekin. The modulation of delta responses in the interaction of brightness and emotion",
      "authors": [
        "Pınar Kurt",
        "Kübra Eroglu"
      ],
      "year": "2017",
      "venue": "International Journal of Psychophysiology"
    },
    {
      "citation_id": "23",
      "title": "The open images dataset v4",
      "authors": [
        "Alina Kuznetsova",
        "Hassan Rom",
        "Neil Alldrin",
        "Jasper Uijlings",
        "Ivan Krasin",
        "Jordi Pont-Tuset",
        "Shahab Kamali",
        "Stefan Popov",
        "Matteo Malloci",
        "Alexander Kolesnikov"
      ],
      "year": "2020",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "24",
      "title": "International affective picture system (iaps): Instruction manual and affective ratings. The center for research in psychophysiology",
      "authors": [
        "Margaret Peter J Lang",
        "Bruce Bradley",
        "Cuthbert"
      ],
      "year": "1999",
      "venue": "International affective picture system (iaps): Instruction manual and affective ratings. The center for research in psychophysiology"
    },
    {
      "citation_id": "25",
      "title": "Spatiotemporal learning by arming image vits with video uniformer",
      "authors": [
        "Kunchang Li",
        "Yali Wang",
        "Yinan He",
        "Yizhuo Li",
        "Yi Wang",
        "Limin Wang",
        "Yu Qiao"
      ],
      "year": "2022",
      "venue": "Spatiotemporal learning by arming image vits with video uniformer",
      "arxiv": "arXiv:2211.09552"
    },
    {
      "citation_id": "26",
      "title": "Personality-assisted multi-task learning for generic and personalized image aesthetics assessment",
      "authors": [
        "Leida Li",
        "Hancheng Zhu",
        "Sicheng Zhao",
        "Guiguang Ding",
        "Weisi Lin"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "27",
      "title": "A survey on sentiment analysis and opinion mining for social multimedia",
      "authors": [
        "Zuhe Li",
        "Yangyu Fan",
        "Bin Jiang",
        "Tao Lei",
        "Weihua Liu"
      ],
      "year": "2019",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "28",
      "title": "Affective image classification using features inspired by psychology and art theory",
      "authors": [
        "Jana Machajdik",
        "Allan Hanbury"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "29",
      "title": "Emotional category data on images from the international affective picture system",
      "authors": [
        "Barbara Joseph A Mikels",
        "Fredrickson",
        "Casey Gregory R Larkin",
        "Sam Lindberg",
        "Patricia Maglio",
        "Reuter-Lorenz"
      ],
      "year": "2005",
      "venue": "Behavior research methods"
    },
    {
      "citation_id": "30",
      "title": "Wordnet: a lexical database for english",
      "authors": [
        "George Miller"
      ],
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "31",
      "title": "The emotion machine: Commonsense thinking, artificial intelligence, and the future of the human mind. Simon and Schuster",
      "authors": [
        "Marvin Minsky"
      ],
      "year": "2007",
      "venue": "The emotion machine: Commonsense thinking, artificial intelligence, and the future of the human mind. Simon and Schuster"
    },
    {
      "citation_id": "32",
      "title": "No reference color image contrast and quality measures",
      "authors": [
        "Karen Panetta",
        "Chen Gao",
        "Sos Agaian"
      ],
      "year": "2013",
      "venue": "IEEE transactions on Consumer Electronics"
    },
    {
      "citation_id": "33",
      "title": "Automatic differentiation in pytorch",
      "authors": [
        "Adam Paszke",
        "Sam Gross",
        "Soumith Chintala",
        "Gregory Chanan",
        "Edward Yang",
        "Zachary Devito",
        "Zeming Lin",
        "Alban Desmaison",
        "Luca Antiga",
        "Adam Lerer"
      ],
      "year": "2017",
      "venue": "Automatic differentiation in pytorch"
    },
    {
      "citation_id": "34",
      "title": "A mixed bag of emotions: Model, predict, and transfer emotion distributions",
      "authors": [
        "Kuan-Chuan",
        "Tsuhan Peng",
        "Amir Chen",
        "Andrew Sadovnik",
        "Gallagher"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "35",
      "title": "Affective computing",
      "authors": [
        "Rosalind Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "36",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark"
      ],
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "37",
      "title": "Learning multi-level deep representations for image emotion classification",
      "authors": [
        "Tianrong Rao",
        "Xiaoxu Li",
        "Min Xu"
      ],
      "year": "2016",
      "venue": "Neural Processing Letters"
    },
    {
      "citation_id": "38",
      "title": "Perceived changes in ordinary autobiographical events' affect and visual imagery colorfulness",
      "authors": [
        "D Timothy",
        "Tamzin J Ritchie",
        "Batteson"
      ],
      "year": "2013",
      "venue": "Consciousness and cognition"
    },
    {
      "citation_id": "39",
      "title": "Roget's Thesaurus of English Words and Phrases",
      "authors": [
        "Peter Mark"
      ],
      "venue": "Roget's Thesaurus of English Words and Phrases"
    },
    {
      "citation_id": "40",
      "title": "On the construction of effective vocabularies for information retrieval",
      "authors": [
        "Gerard Salton",
        "Clement Yu"
      ],
      "year": "1973",
      "venue": "Acm Sigplan Notices"
    },
    {
      "citation_id": "41",
      "title": "Opinion mining, sentiment analysis and emotion understanding in advertising: a bibliometric analysis",
      "authors": [
        "Pablo Sánchez-Núñez",
        "Manuel Cobo",
        "Carlos De",
        "Las Heras-Pedrosa",
        "Ignacio Peláez",
        "Enrique Herrera-Viedma"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "42",
      "title": "Recognition of advertisement emotions with application to computational advertising",
      "authors": [
        "Abhinav Shukla",
        "Shruti Shriya Gullapuram",
        "Harish Katti",
        "Mohan Kankanhalli",
        "Stefan Winkler",
        "Ramanathan Subramanian"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "43",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "Karen Simonyan",
        "Andrew Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "44",
      "title": "Multi-modal emotion recognition on iemocap dataset using deep learning",
      "authors": [
        "Samarth Tripathi",
        "Sarthak Tripathi",
        "Homayoon Beigi"
      ],
      "year": "2018",
      "venue": "Multi-modal emotion recognition on iemocap dataset using deep learning",
      "arxiv": "arXiv:1804.05788"
    },
    {
      "citation_id": "45",
      "title": "Visualizing data using t-sne",
      "authors": [
        "Laurens Van Der Maaten",
        "Geoffrey Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "46",
      "title": "Reduced early visual emotion discrimination as an index of diminished emotion processing in parkinson's disease?-evidence from event-related brain potentials",
      "authors": [
        "Matthias Wieser",
        "Elisabeth Klupp",
        "Peter Weyers",
        "Paul Pauli",
        "David Weise",
        "Daniel Zeller",
        "Joseph Classen",
        "Andreas Mühlberger"
      ],
      "year": "2012",
      "venue": "Cortex"
    },
    {
      "citation_id": "47",
      "title": "Large-scale datasets for going deeper in image understanding",
      "authors": [
        "Jiahong Wu",
        "He Zheng",
        "Bo Zhao",
        "Yixin Li",
        "Baoming Yan",
        "Rui Liang",
        "Wenjia Wang",
        "Shipei Zhou",
        "Guosen Lin",
        "Yanwei Fu"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "48",
      "title": "Mdan: Multi-level dependent attention network for visual emotion analysis",
      "authors": [
        "Liwen Xu",
        "Zhengtao Wang"
      ],
      "year": "2022",
      "venue": "CVPR"
    },
    {
      "citation_id": "49",
      "title": "Nurul Fazmidar Binti Mohd Noor, Mohamad Nizam Bin Ayub, Hannyzzura Binti Affal, and Nornazlita Binti Hussin",
      "authors": [
        "Elaheh Yadegaridehkordi"
      ],
      "year": "2019",
      "venue": "Computers & Education"
    },
    {
      "citation_id": "50",
      "title": "Emotion recognition for multiple context awareness",
      "authors": [
        "Dingkang Yang",
        "Shuai Huang"
      ],
      "year": "2022",
      "venue": "ECCV"
    },
    {
      "citation_id": "51",
      "title": "Solver: Scene-object interrelated visual emotion reasoning network",
      "authors": [
        "Jingyuan Yang",
        "Xinbo Gao",
        "Leida Li",
        "Xiumei Wang",
        "Jinshan Ding"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "52",
      "title": "Stimuli-aware visual emotion analysis",
      "authors": [
        "Jingyuan Yang",
        "Jie Li",
        "Xiumei Wang",
        "Yuxuan Ding",
        "Xinbo Gao"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "53",
      "title": "Weakly supervised coupled networks for visual sentiment analysis",
      "authors": [
        "Jufeng Yang",
        "Dongyu She",
        "Yu-Kun Lai",
        "Paul Rosin",
        "Ming-Hsuan Yang"
      ],
      "year": "2007",
      "venue": "CVPR"
    },
    {
      "citation_id": "54",
      "title": "Visual sentiment prediction based on automatic discovery of affective regions",
      "authors": [
        "Jufeng Yang",
        "Dongyu She",
        "Ming Sun",
        "Ming-Ming Cheng",
        "Paul Rosin",
        "Liang Wang"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "55",
      "title": "Learning visual sentiment distributions via augmented conditional probability neural network",
      "authors": [
        "Jufeng Yang",
        "Ming Sun"
      ],
      "year": "2017",
      "venue": "AAAI"
    },
    {
      "citation_id": "56",
      "title": "Enhancing health care via affective computing",
      "authors": [
        "N Georgios",
        "Yannakakis"
      ],
      "year": "2018",
      "venue": "Enhancing health care via affective computing"
    },
    {
      "citation_id": "57",
      "title": "Robust image sentiment analysis using progressively trained and domain transferred deep networks",
      "authors": [
        "Quanzeng You",
        "Jiebo Luo",
        "Jin Hailin",
        "Jianchao Yang"
      ],
      "year": "2015",
      "venue": "AAAI"
    },
    {
      "citation_id": "58",
      "title": "Building a large scale dataset for image emotion recognition: The fine print and the benchmark",
      "authors": [
        "Quanzeng You",
        "Jiebo Luo",
        "Jin Hailin",
        "Jianchao Yang"
      ],
      "year": "2016",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "59",
      "title": "Exploring discriminative representations for image emotion recognition with cnns",
      "authors": [
        "Wei Zhang",
        "Xuanyu He",
        "Weizhi Lu"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "60",
      "title": "Exploring principles-of-art features for image emotion recognition",
      "authors": [
        "Sicheng Zhao",
        "Yue Gao",
        "Xiaolei Jiang",
        "Hongxun Yao",
        "Tat-Seng Chua",
        "Xiaoshuai Sun"
      ],
      "year": "2014",
      "venue": "ACMMM"
    },
    {
      "citation_id": "61",
      "title": "Pdanet: Polarity-consistent deep attention network for fine-grained visual emotion regression",
      "authors": [
        "Sicheng Zhao",
        "Zizhou Jia",
        "Hui Chen",
        "Leida Li",
        "Guiguang Ding",
        "Kurt Keutzer"
      ],
      "year": "2019",
      "venue": "Proceedings of the 27th ACM international conference on multimedia"
    },
    {
      "citation_id": "62",
      "title": "Guiguang Ding, and Tat-Seng Chua. Predicting personalized image emotion perceptions in social networks",
      "authors": [
        "Sicheng Zhao",
        "Hongxun Yao",
        "Yue Gao"
      ],
      "year": "2016",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "63",
      "title": "Affective image content analysis: Two decades review and new perspectives",
      "authors": [
        "Sicheng Zhao",
        "Xingxu Yao"
      ],
      "year": "2021",
      "venue": "IEEE TPAMI"
    },
    {
      "citation_id": "64",
      "title": "Places: A 10 million image database for scene recognition",
      "authors": [
        "Bolei Zhou",
        "Agata Lapedriza",
        "Aditya Khosla",
        "Aude Oliva",
        "Antonio Torralba"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "65",
      "title": "A brief introduction to weakly supervised learning",
      "authors": [
        "Zhi-Hua Zhou"
      ],
      "year": "2018",
      "venue": "National science review"
    },
    {
      "citation_id": "66",
      "title": "Emotion and action",
      "authors": [
        "Jing Zhu",
        "Paul Thagard"
      ],
      "year": "2002",
      "venue": "Philosophical psychology"
    }
  ]
}