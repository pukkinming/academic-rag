{
  "paper_id": "2201.11826v1",
  "title": "Sentiment-Aware Automatic Speech Recognition Pre-Training For Enhanced Speech Emotion Recognition",
  "published": "2022-01-27T22:20:28Z",
  "authors": [
    "Ayoub Ghriss",
    "Bo Yang",
    "Viktor Rozgic",
    "Elizabeth Shriberg",
    "Chao Wang"
  ],
  "keywords": [
    "Speech emotion recognition",
    "automatic speech recognition",
    "sentiment analysis",
    "pre-training"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We propose a novel multi-task pre-training method for Speech Emotion Recognition (SER). We pre-train SER model simultaneously on Automatic Speech Recognition (ASR) and sentiment classification tasks to make the acoustic ASR model more \"emotion aware\". We generate targets for the sentiment classification using text-to-sentiment model trained on publicly available data. Finally, we fine-tune the acoustic ASR on emotion annotated speech data. We evaluated the proposed approach on MSP-Podcast dataset, where we achieved the best reported concordance correlation coefficient (CCC) of 0.41 for valence prediction.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The origins of Speech Emotion Recognition (SER) date back to Blanton's work  [1]  when he wrote that \"the language of the tones is the oldest and most universal of all our means of communication.\", and culminated in studies on word-free voice samples with the goal of isolating qualitative features of the voice from those related to the articulated sounds patterns  [2] . Emotion researchers define emotion either as a discrete construct or using emotion dimensions. The discrete construction theory credited to Ekman  [3]  isolates six basic emotion categories: anger, disgust, fear, happiness, sadness, surprise, but more comprehensive emotion category sets have been proposed  [4] . The dimensional perspective, on the other hand, defines emotion as a point in a space defined by emotion dimensions, most commonly Activation, Valence, and Dominance (AVD).\n\nA century later, the challenge of SER remains but new computational tools have emerged permitting more complex modelling. The motivations have also changed, it is no longer destined to understanding the emotion from a psychological perspective alone. It is further fueled by the ubiquitous speech-based interactions. Moreover, building smart systems capable of detecting the user emotional state has the poten-tial of enhancing the interactive experience with different devices.\n\n(Deep) neural network-based models have been a popular choice for SER in recent years  [5, 6, 7] . However, training a Neural Network for SER requires a large training corpus, and non-neutral emotions are rare in speech. Recent work has addressed this limitation by pre-training on speech tasks, such as ASR  [5, 8]  or Speaker Identification  [9, 10] . In this work, we propose a novel pre-training approach for SER. The proposed pre-training consists of building a \"Sentimentaware ASR\" (SA2SR). The training of SA2SR objective is a combination of ASR and text-based sentiment classification, where the text sentiment labels are generated from a trained text sentiment model. This approach allows us to amass a large amount of text sentiment labels for speech data, and the effectiveness of these labels on improving SER is validated in our experiments.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "A suitable choice of input representation is crucial for Speech Emotion Recognition (SER). Traditional approaches used classical signal processing methods (pitch, filter banks) or statistical measures (mean, variance, quantiles...) of the acoustic signal to train on classification/regression (Ekman categories vs AVD). Recent work on SER attempted to infer the emotion based on acoustic and textual cues, either simultaneously or separately.\n\nIn End-to-End SER with ASR  [11] , an acoustic-to-word ASR model is first trained then fine-tuned on a multi-task learning to jointly optimize ASR and SER objective. The emotion prediction block has two input fields: acoustic features similar to those used in ASR and the states of the ASR decoder. The authors also show that using this combination (raw acoustic inputs & ASR decoder features) outperforms an SER based on any single element of this combination.\n\nCombining ASR and SER also outperforms a variant in which the SER block model takes the transcript of the ASR (word embeddings) instead of the decoder states. This result is expected since using the ASR transcript propagates the transcription inaccuracies to the SER block. The same reasoning can applied to text-based SER to point out their limitations, such as the one used in Sentiment Analysis based on speaker data  [12] . In this speech-based Sentiment Analysis a pretrained ASR is used for transcription of the utterance. The text is then fed to a feature extractor in parallel to a Speaker Identification feature block to provide an input to the emotion predictor.\n\nA different approach that decouples text and acoustic features was introduced in Multi-modal SER  [13] , where the SER model encodes information from audio and text using two encoders. For the text encoder, the text input is the transcription from an ASR. This approach leverages the ability of text encoders to capture long term semantics better than the acoustic ones. However, this multi-modal approach assumes that the transcript is provided (via ASR), which limits its applicability when only the utterance (audio) is accessible.\n\nTo the extent of our knowledge, the only previous work that leveraged text-based sentiment labels was published recently  [14]  with three major differences: 1) we start with analyzing the correlation between text sentiment and speech emotion, thereby establishing a strong motivation for the proposed method, and an explanation for the observed performance boost, 2) the focus of this work is on SER with the widely used dimensional emotion representation (activation, valence and dominance), while that of  [14]  is on three-way speech sentiment classification, and 3) The approach in  [14]  uses out of the box encodings (ASR followed by Bert) and is oblivious to the feedback from sentiment labels. Our approach, on the other hand, leverages the proxy sentiment labels to induce the ASR embedding to incorporate emotional knowledge and yields better performance.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Proposed Method",
      "text": "We propose building a SER model that is pre-trained on a sentiment-aware ASR task. The sentiment-awareness is implemented by transferring sentiment knowledge from a text domain to the acoustic domain.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Correlation Between Text Sentiment And Speech Emotion",
      "text": "We conjecture that text sentiment correlates with the valence dimension of speech emotion. Indeed, when a human listener tries to determine the emotion from a speech segment, the words in the speech also plays a role -the obvious examples are the speech segments that contain cursing words and phrases, or strong praising adjectives (e.g. excellent, beautiful, etc).\n\nWe test this correspondence between text-based sentiment and valence on the IEMOCAP dataset  [15]  and use a pretrained text sentiment analysis (Roberta  [16] ) to get the sentiment labels: negative, positive, neutral. Table  1  shows the confusion matrix between text sentiment classes and speech emotion, with the dominant speech emotion highlighted in red in each sentiment class and the second dominant ones highlighted in blue.\n\nAs can be seen from Table  1 , the negative text-sentiment utterances are mostly associated with negative speech emotion labels (Sad, Anger and Frustrated); while the positive text-sentiment utterances correspond to positive speech emotion labels (Happy). More interestingly, by investigating elements of the cell {N eutral, f rustrated} we can find transcripts such as : \"Nothing\", \"A vacation.\", \"I'm just saying.\"}, while the inferred sentiment is neutralthis means the emotion in this case is likely conveyed through speech style and tone. Furthermore, by grouping {Sad, F rustrated, Anger} into one class, we get a Spearman correlation of 0.22 between text sentiment and speech emotion. These observations motivate us to employ readily available text sentiment model to generate sentiment labels, which will serve as weak signal to train speech emotion models. In addition to the ASR model, a sentiment classifier (proxy classifier) is trained jointly on the acoustic encoder states. The architecture logic is similar to the one in combined ASR-SER  [11] . The model takes as input the log filterbank energy (LFBE) features and contains two classifiers (Figure  1 ) that take the encoded acoustic sequence as input:",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Text Sentiment",
      "text": "• Sequence to sequence classifier: A softmax layer for token classification\n\n• Sentiment classifier: A sequence summarizer (recurrent neural network) followed by a softmax layer over the sentiment classes (negative, neutral, positive).\n\nThe proposed architecture is trained using a loss that is a linear combination (Equation  1 ) of (a) Connectionist Temporal Classification (CTC) loss  [17]  between the target sequence and the sequence of output probabilities over the token set, and (b) cross entropy loss between the predicted and proxy sentiment targets, i.e., sentiment obtained from the pretrained text-to-sentiment model. The global loss is defined as:\n\nwhere λ ≥ 0 is a hyper-parameter reflecting how the importance of sentiment classification vis-à-vis the ASR task.  CCC(y, ŷ) = 2Cov(y, ŷ)\n\nwhere Cov denotes the co-variance, σ and µ denote the sample variance and mean, respectively. During model training, these statistics are computed on mini-batches of data.\n\nTo enable easy comparison to previous works, the CCC objectives for each of the activation, valence and dominance are simply combined as:",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiment Results",
      "text": "Datasets: We use the full Librispeech dataset  [18]  for pretraining a character based SA2SR. The dataset contains audios and transcripts for around 960 hours of audio, and we generate the proxy sentiment labels using text-to-sentiment RoBerta model trained on Twitter sentiment data  [19] . We extract LFBE features with 40 frequencies using 25ms window and 10ms steps. Hence, for SA2SR pre-training, an input tuple consists of (LFBE Features, transcript, proxy label).\n\nModel fine-tuning and evaluation are conducted on MSP-Podcast  [20]  dataset. The MSP-Podcast contains 60K speech segments from podcast recordings which are perceptually annotated using crowd-sourcing. The ground truth emotion dimension scores are obtained averaging scores selected by individual annotators on seven-point Likert scale. The dataset was split following the same partitions provided in the official MSP corpus.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Networks Architecture",
      "text": "The Acoustic Encoder consists of 5 Bidirectional LSTM (Bi-LSTM) layers. All LSTM layers have 192 units with tanh activation and sigmoid recurrent activation.\n\nThe 1-D convolutional block in Emotion Regressor (Figure  2 ) is a stack of 2 one-dimensional masked convolutions with filter sizes (6,3) and strides (3,2) respectively. The convolutions accept input masks and process them accordingly to match the output of the convolution. The convolutions are followed by sample level normalization and a LeakyRELU α=0,3 is applied to the convolutional block output.\n\nThe multi-head attention uses 4 attention heads and 64 dimensions for all encoding spaces and feed-forward outputs. The output of the attention mechanism is pooled by computing the variance and the mean of the sequences yielding 128 features that are linearly projected into the AVD space.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Pre-Training",
      "text": "For the global loss weight, we chose λ = 200. To optimize it, we use Adam optimizer with the parameters: lr = 5 × 10 -5 , β 1 = 0.9, β 2 = 0.999. The λ was chosen such that the ASR and sentiment classification losses achieve similar value on the validation set.\n\nThe LFBE features are normalized on a sample level. The training set is then augmented using speed augmentation of factors 0.9 and 1.1. This augmentation step triples the training set size. The features are then masked on time and frequency dimensions with probability p = 0.5, as in the SpecAugment method  [21] . We finally stack every adjacent three frames and skip every other two time steps, which leads to shorter sequences and improved model training time. The ASR training uses a token set of 29 characters.\n\nAfter each epoch, we use the validation set to compute the Character Error Rate (CER) from the ASR task and Area Under the Curve (AUC) of the sentiment classifier. The pretraining terminates when the metric M = CER -AU C does not improve for 25 epochs and the model that has the lowest M is evaluated. For the baseline model, we pre-train on the ASR task only and use the model from the epoch with the best CER on the validation set.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Effectiveness Of Asr Features For Ser",
      "text": "To start, we test the effectiveness of ASR trained features for SER. Some previous work, for example  [5, 8] , reported limited transferability between ASR and SER. In our experiments, however, we found ASR features to be quite effective. It is possible that the differences are due to the large pre-training dataset (full Librispeech) and modern model architectures such as bidirectional LSTM and transformers.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Model",
      "text": "Activation We report these results in Table  2 . The \"ASR features\" are pre-trained on Librispeech and fine-tuned on MSP-Podcast for SER. The \"No pre-training\" baseline is directly trained with the MSP-Podcast. We observe large performance boost when ASR pre-training is employed.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Effectiveness Of Sa2Sr Pre-Training",
      "text": "In this experiment, we examine whether the additional proxy sentiment labels enhance SER performance. In particular, we expect that valence recognition to be improved, as we have seen correlation between text sentiment and valence in Section 3.1. The results are reported in Table  3 . As a comparison, we also included the results from  [7]   We can see from Table  3  that recognition of valence, arguably the most important dimension of emotion, is further improved compared to the strong \"ASR features\" baseline. However, in this equal-weight multi-task emotion training setting, we see that activation and dominance dimension performs relatively weak compare to that of  [7] . We view this as an encouraging result as in many applications, valence (positive v.s. negative) is of most interest.\n\nAdditionally, during the pre-training of the SA2SR model, we observed that the sentiment classification part achieves weighted-average-recall of 0.71 and 0.81 AUC. This indicates that indeed the model is trained to recognize these proxy sentiment labels. Therefore, we expect the learned representation to be suitable for the final SER task.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Importance Of Fine-Tuning",
      "text": "Lastly, we are interested in examining whether we should freeze the learned representations during SER training. For both ASR and SA2SR features, we train models with and without freezing the acoustic encoder. The results are reported in Table  4  As can be seen from Table  4 , fine-tuning the encoder gives better SER performance compared to the methods with frozen encoders. This result proves that using out of the box ASR models for transcription without any fine-tuning would be outperformed when the gradient propagates to the ASR network weights. It also proves the point we made earlier about the importance of sentiment awareness in the SA2SR. Even without fine-tuning, SA2SR clearly outperforms the ASR embedding on 2 out of the 3 emotion AVD dimensions.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Architecture of SA2SR pre-training network",
      "page": 3
    },
    {
      "caption": "Figure 2: ) and add an emotion regression transformer block",
      "page": 3
    },
    {
      "caption": "Figure 2: Architecture of the ﬁne-tuned SA2SR",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "∗ University of Colorado Boulder, ayoub.ghriss@colorado.edu, † Ellipsis Health,": ""
        },
        {
          "∗ University of Colorado Boulder, ayoub.ghriss@colorado.edu, † Ellipsis Health,": "tial of\nenhancing the\ninteractive\nexperience with different"
        },
        {
          "∗ University of Colorado Boulder, ayoub.ghriss@colorado.edu, † Ellipsis Health,": "devices."
        },
        {
          "∗ University of Colorado Boulder, ayoub.ghriss@colorado.edu, † Ellipsis Health,": ""
        },
        {
          "∗ University of Colorado Boulder, ayoub.ghriss@colorado.edu, † Ellipsis Health,": "(Deep) neural network-based models have been a popu-"
        },
        {
          "∗ University of Colorado Boulder, ayoub.ghriss@colorado.edu, † Ellipsis Health,": ""
        },
        {
          "∗ University of Colorado Boulder, ayoub.ghriss@colorado.edu, † Ellipsis Health,": "lar choice for SER in recent years [5, 6, 7]. However,\ntrain-"
        },
        {
          "∗ University of Colorado Boulder, ayoub.ghriss@colorado.edu, † Ellipsis Health,": ""
        },
        {
          "∗ University of Colorado Boulder, ayoub.ghriss@colorado.edu, † Ellipsis Health,": "ing a Neural Network for SER requires a large training cor-"
        },
        {
          "∗ University of Colorado Boulder, ayoub.ghriss@colorado.edu, † Ellipsis Health,": ""
        },
        {
          "∗ University of Colorado Boulder, ayoub.ghriss@colorado.edu, † Ellipsis Health,": "pus,\nand non-neutral emotions are rare in speech.\nRecent"
        },
        {
          "∗ University of Colorado Boulder, ayoub.ghriss@colorado.edu, † Ellipsis Health,": ""
        },
        {
          "∗ University of Colorado Boulder, ayoub.ghriss@colorado.edu, † Ellipsis Health,": "work has addressed this limitation by pre-training on speech"
        },
        {
          "∗ University of Colorado Boulder, ayoub.ghriss@colorado.edu, † Ellipsis Health,": ""
        },
        {
          "∗ University of Colorado Boulder, ayoub.ghriss@colorado.edu, † Ellipsis Health,": "tasks, such as ASR [5, 8] or Speaker Identiﬁcation [9, 10]. In"
        },
        {
          "∗ University of Colorado Boulder, ayoub.ghriss@colorado.edu, † Ellipsis Health,": ""
        },
        {
          "∗ University of Colorado Boulder, ayoub.ghriss@colorado.edu, † Ellipsis Health,": "this work, we propose a novel pre-training approach for SER."
        },
        {
          "∗ University of Colorado Boulder, ayoub.ghriss@colorado.edu, † Ellipsis Health,": ""
        },
        {
          "∗ University of Colorado Boulder, ayoub.ghriss@colorado.edu, † Ellipsis Health,": "The proposed pre-training consists of building a “Sentiment-"
        },
        {
          "∗ University of Colorado Boulder, ayoub.ghriss@colorado.edu, † Ellipsis Health,": ""
        },
        {
          "∗ University of Colorado Boulder, ayoub.ghriss@colorado.edu, † Ellipsis Health,": "aware ASR” (SA2SR). The training of SA2SR objective is a"
        },
        {
          "∗ University of Colorado Boulder, ayoub.ghriss@colorado.edu, † Ellipsis Health,": ""
        },
        {
          "∗ University of Colorado Boulder, ayoub.ghriss@colorado.edu, † Ellipsis Health,": "combination of ASR and text-based sentiment classiﬁcation,"
        },
        {
          "∗ University of Colorado Boulder, ayoub.ghriss@colorado.edu, † Ellipsis Health,": ""
        },
        {
          "∗ University of Colorado Boulder, ayoub.ghriss@colorado.edu, † Ellipsis Health,": "where the text sentiment\nlabels are generated from a trained"
        },
        {
          "∗ University of Colorado Boulder, ayoub.ghriss@colorado.edu, † Ellipsis Health,": "text sentiment model.\nThis approach allows us to amass a"
        },
        {
          "∗ University of Colorado Boulder, ayoub.ghriss@colorado.edu, † Ellipsis Health,": "large amount of text sentiment labels for speech data, and the"
        },
        {
          "∗ University of Colorado Boulder, ayoub.ghriss@colorado.edu, † Ellipsis Health,": "effectiveness of these labels on improving SER is validated in"
        },
        {
          "∗ University of Colorado Boulder, ayoub.ghriss@colorado.edu, † Ellipsis Health,": "our experiments."
        },
        {
          "∗ University of Colorado Boulder, ayoub.ghriss@colorado.edu, † Ellipsis Health,": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "analyzing the correlation between text sentiment and speech": "emotion,\nthereby establishing a\nstrong motivation for\nthe",
          "models.": ""
        },
        {
          "analyzing the correlation between text sentiment and speech": "proposed method, and an explanation for\nthe observed per-",
          "models.": ""
        },
        {
          "analyzing the correlation between text sentiment and speech": "",
          "models.": ""
        },
        {
          "analyzing the correlation between text sentiment and speech": "formance boost, 2) the focus of this work is on SER with the",
          "models.": ""
        },
        {
          "analyzing the correlation between text sentiment and speech": "widely used dimensional emotion representation (activation,",
          "models.": ""
        },
        {
          "analyzing the correlation between text sentiment and speech": "valence and dominance), while that of\n[14]\nis on three-way",
          "models.": ""
        },
        {
          "analyzing the correlation between text sentiment and speech": "speech sentiment classiﬁcation, and 3) The approach in [14]",
          "models.": "Speech emotion"
        },
        {
          "analyzing the correlation between text sentiment and speech": "uses out of\nthe box encodings (ASR followed by Bert) and",
          "models.": ""
        },
        {
          "analyzing the correlation between text sentiment and speech": "is oblivious\nto the\nfeedback from sentiment\nlabels.\nOur",
          "models.": ""
        },
        {
          "analyzing the correlation between text sentiment and speech": "approach, on the other hand,\nleverages the proxy sentiment",
          "models.": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "data [12].\nIn this\nspeech-based Sentiment Analysis a pre-": "trained ASR is used for\ntranscription of\nthe utterance.\nThe",
          "in each sentiment class and the second dominant ones high-": "lighted in blue."
        },
        {
          "data [12].\nIn this\nspeech-based Sentiment Analysis a pre-": "text\nis then fed to a feature extractor in parallel\nto a Speaker",
          "in each sentiment class and the second dominant ones high-": "As can be seen from Table 1,\nthe negative text-sentiment"
        },
        {
          "data [12].\nIn this\nspeech-based Sentiment Analysis a pre-": "Identiﬁcation feature block to provide an input to the emotion",
          "in each sentiment class and the second dominant ones high-": "utterances are mostly associated with negative speech emo-"
        },
        {
          "data [12].\nIn this\nspeech-based Sentiment Analysis a pre-": "predictor.",
          "in each sentiment class and the second dominant ones high-": "tion\nlabels\n(Sad, Anger\nand Frustrated); while\nthe\nposi-"
        },
        {
          "data [12].\nIn this\nspeech-based Sentiment Analysis a pre-": "A different approach that decouples text and acoustic fea-",
          "in each sentiment class and the second dominant ones high-": "tive text-sentiment utterances correspond to positive speech"
        },
        {
          "data [12].\nIn this\nspeech-based Sentiment Analysis a pre-": "tures was\nintroduced in Multi-modal SER [13], where the",
          "in each sentiment class and the second dominant ones high-": "emotion labels\n(Happy). More interestingly, by investigat-"
        },
        {
          "data [12].\nIn this\nspeech-based Sentiment Analysis a pre-": "SER model encodes information from audio and text using",
          "in each sentiment class and the second dominant ones high-": "ing\nelements\nof\nthe\ncell\ncan\n{N eutral, f rustrated} we"
        },
        {
          "data [12].\nIn this\nspeech-based Sentiment Analysis a pre-": "two encoders. For the text encoder, the text input is the tran-",
          "in each sentiment class and the second dominant ones high-": "“Nothing”, “A vacation.”, “I’m\nﬁnd transcripts\nsuch as\n:"
        },
        {
          "data [12].\nIn this\nspeech-based Sentiment Analysis a pre-": "scription from an ASR. This approach leverages the ability of",
          "in each sentiment class and the second dominant ones high-": "just\nthe\ninferred\nsentiment\nis\nneutral\n–\nsaying.”}, while"
        },
        {
          "data [12].\nIn this\nspeech-based Sentiment Analysis a pre-": "text encoders to capture long term semantics better than the",
          "in each sentiment class and the second dominant ones high-": "this means\nthe\nemotion\nin\nthis\ncase\nis\nlikely\nconveyed"
        },
        {
          "data [12].\nIn this\nspeech-based Sentiment Analysis a pre-": "acoustic ones. However,\nthis multi-modal approach assumes",
          "in each sentiment class and the second dominant ones high-": "through speech style and tone.\nFurthermore, by grouping"
        },
        {
          "data [12].\nIn this\nspeech-based Sentiment Analysis a pre-": "that the transcript is provided (via ASR), which limits its ap-",
          "in each sentiment class and the second dominant ones high-": "{Sad, F rustrated, Anger} into one class, we get a Spear-"
        },
        {
          "data [12].\nIn this\nspeech-based Sentiment Analysis a pre-": "plicability when only the utterance (audio) is accessible.",
          "in each sentiment class and the second dominant ones high-": "man correlation of 0.22 between text sentiment and speech"
        },
        {
          "data [12].\nIn this\nspeech-based Sentiment Analysis a pre-": "To the extent of our knowledge,\nthe only previous work",
          "in each sentiment class and the second dominant ones high-": "emotion. These observations motivate us to employ readily"
        },
        {
          "data [12].\nIn this\nspeech-based Sentiment Analysis a pre-": "that\nleveraged\ntext-based\nsentiment\nlabels was\npublished",
          "in each sentiment class and the second dominant ones high-": "available text sentiment model\nto generate sentiment\nlabels,"
        },
        {
          "data [12].\nIn this\nspeech-based Sentiment Analysis a pre-": "recently[14] with three major differences:\n1) we start with",
          "in each sentiment class and the second dominant ones high-": "which will\nserve\nas weak signal\nto train speech emotion"
        },
        {
          "data [12].\nIn this\nspeech-based Sentiment Analysis a pre-": "analyzing the correlation between text sentiment and speech",
          "in each sentiment class and the second dominant ones high-": "models."
        },
        {
          "data [12].\nIn this\nspeech-based Sentiment Analysis a pre-": "emotion,\nthereby establishing a\nstrong motivation for\nthe",
          "in each sentiment class and the second dominant ones high-": ""
        },
        {
          "data [12].\nIn this\nspeech-based Sentiment Analysis a pre-": "proposed method, and an explanation for\nthe observed per-",
          "in each sentiment class and the second dominant ones high-": "Text sentiment"
        },
        {
          "data [12].\nIn this\nspeech-based Sentiment Analysis a pre-": "",
          "in each sentiment class and the second dominant ones high-": "Negative\nNeutral\nPositive"
        },
        {
          "data [12].\nIn this\nspeech-based Sentiment Analysis a pre-": "formance boost, 2) the focus of this work is on SER with the",
          "in each sentiment class and the second dominant ones high-": ""
        },
        {
          "data [12].\nIn this\nspeech-based Sentiment Analysis a pre-": "widely used dimensional emotion representation (activation,",
          "in each sentiment class and the second dominant ones high-": "Sad\n339\n604\n137"
        },
        {
          "data [12].\nIn this\nspeech-based Sentiment Analysis a pre-": "valence and dominance), while that of\n[14]\nis on three-way",
          "in each sentiment class and the second dominant ones high-": "490\nAnger\n518\n94"
        },
        {
          "data [12].\nIn this\nspeech-based Sentiment Analysis a pre-": "speech sentiment classiﬁcation, and 3) The approach in [14]",
          "in each sentiment class and the second dominant ones high-": "658\n1049\nSpeech emotion\n141\nFrustrated"
        },
        {
          "data [12].\nIn this\nspeech-based Sentiment Analysis a pre-": "uses out of\nthe box encodings (ASR followed by Bert) and",
          "in each sentiment class and the second dominant ones high-": "1251\n204\nNeutral\n253"
        },
        {
          "data [12].\nIn this\nspeech-based Sentiment Analysis a pre-": "is oblivious\nto the\nfeedback from sentiment\nlabels.\nOur",
          "in each sentiment class and the second dominant ones high-": "533\nHappy\n252\n848"
        },
        {
          "data [12].\nIn this\nspeech-based Sentiment Analysis a pre-": "approach, on the other hand,\nleverages the proxy sentiment",
          "in each sentiment class and the second dominant ones high-": ""
        },
        {
          "data [12].\nIn this\nspeech-based Sentiment Analysis a pre-": "labels to induce the ASR embedding to incorporate emotional",
          "in each sentiment class and the second dominant ones high-": "Table 1. Confusion matrix between text-based sentiment [16]"
        },
        {
          "data [12].\nIn this\nspeech-based Sentiment Analysis a pre-": "knowledge and yields better performance.",
          "in each sentiment class and the second dominant ones high-": "and speech emotion (IEMOCAP)"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4. EXPERIMENT RESULTS": ""
        },
        {
          "4. EXPERIMENT RESULTS": "Datasets: We use the full Librispeech dataset\n[18]\nfor pre-"
        },
        {
          "4. EXPERIMENT RESULTS": "training a character based SA2SR. The dataset contains au-"
        },
        {
          "4. EXPERIMENT RESULTS": "dios and transcripts for around 960 hours of audio, and we"
        },
        {
          "4. EXPERIMENT RESULTS": "generate the proxy sentiment\nlabels using text-to-sentiment"
        },
        {
          "4. EXPERIMENT RESULTS": "RoBerta model trained on Twitter sentiment data [19]. We ex-"
        },
        {
          "4. EXPERIMENT RESULTS": "tract LFBE features with 40 frequencies using 25ms window"
        },
        {
          "4. EXPERIMENT RESULTS": "for SA2SR pre-training, an input\nand 10ms steps. Hence,"
        },
        {
          "4. EXPERIMENT RESULTS": "tuple consists of (LFBE Features, transcript, proxy label)."
        },
        {
          "4. EXPERIMENT RESULTS": "Model ﬁne-tuning and evaluation are conducted on MSP-"
        },
        {
          "4. EXPERIMENT RESULTS": "Podcast [20] dataset. The MSP-Podcast contains 60K speech"
        },
        {
          "4. EXPERIMENT RESULTS": "segments from podcast recordings which are perceptually an-"
        },
        {
          "4. EXPERIMENT RESULTS": ""
        },
        {
          "4. EXPERIMENT RESULTS": "notated using crowd-sourcing. The ground truth emotion di-"
        },
        {
          "4. EXPERIMENT RESULTS": "mension scores are obtained averaging scores selected by in-"
        },
        {
          "4. EXPERIMENT RESULTS": "dividual annotators on seven-point Likert scale. The dataset"
        },
        {
          "4. EXPERIMENT RESULTS": "was split following the same partitions provided in the ofﬁcial"
        },
        {
          "4. EXPERIMENT RESULTS": ""
        },
        {
          "4. EXPERIMENT RESULTS": "MSP corpus."
        },
        {
          "4. EXPERIMENT RESULTS": ""
        },
        {
          "4. EXPERIMENT RESULTS": ""
        },
        {
          "4. EXPERIMENT RESULTS": ""
        },
        {
          "4. EXPERIMENT RESULTS": "4.1. Networks Architecture"
        },
        {
          "4. EXPERIMENT RESULTS": ""
        },
        {
          "4. EXPERIMENT RESULTS": ""
        },
        {
          "4. EXPERIMENT RESULTS": "The Acoustic Encoder consists of 5 Bidirectional LSTM (Bi-"
        },
        {
          "4. EXPERIMENT RESULTS": ""
        },
        {
          "4. EXPERIMENT RESULTS": "LSTM) layers. All LSTM layers have 192 units with tanh"
        },
        {
          "4. EXPERIMENT RESULTS": "activation and sigmoid recurrent activation."
        },
        {
          "4. EXPERIMENT RESULTS": ""
        },
        {
          "4. EXPERIMENT RESULTS": ""
        },
        {
          "4. EXPERIMENT RESULTS": "The 1-D convolutional block in Emotion Regressor (Fig-"
        },
        {
          "4. EXPERIMENT RESULTS": ""
        },
        {
          "4. EXPERIMENT RESULTS": "ure\n2) is a stack of 2 one-dimensional masked convolutions"
        },
        {
          "4. EXPERIMENT RESULTS": "with ﬁlter sizes (6,3) and strides (3,2) respectively. The con-"
        },
        {
          "4. EXPERIMENT RESULTS": ""
        },
        {
          "4. EXPERIMENT RESULTS": "volutions accept input masks and process them accordingly to"
        },
        {
          "4. EXPERIMENT RESULTS": ""
        },
        {
          "4. EXPERIMENT RESULTS": "match the output of the convolution. The convolutions are fol-"
        },
        {
          "4. EXPERIMENT RESULTS": ""
        },
        {
          "4. EXPERIMENT RESULTS": "lowed by sample level normalization and a LeakyRELUα=0,3"
        },
        {
          "4. EXPERIMENT RESULTS": "is applied to the convolutional block output."
        },
        {
          "4. EXPERIMENT RESULTS": ""
        },
        {
          "4. EXPERIMENT RESULTS": "The multi-head attention uses 4 attention heads and 64 di-"
        },
        {
          "4. EXPERIMENT RESULTS": "mensions for all encoding spaces and feed-forward outputs."
        },
        {
          "4. EXPERIMENT RESULTS": "The output of the attention mechanism is pooled by comput-"
        },
        {
          "4. EXPERIMENT RESULTS": ""
        },
        {
          "4. EXPERIMENT RESULTS": "ing the variance and the mean of the sequences yielding 128"
        },
        {
          "4. EXPERIMENT RESULTS": "features that are linearly projected into the AVD space."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 3: SA2SR produces good features for enhancing the",
      "data": [
        {
          "4.2. Pre-training": "",
          "we also included the results from [7] that uses self-supervised": "pre-training as a strong contender."
        },
        {
          "4.2. Pre-training": "For the global loss weight, we chose λ = 200. To optimize it,",
          "we also included the results from [7] that uses self-supervised": ""
        },
        {
          "4.2. Pre-training": "we use Adam optimizer with the parameters:\nlr = 5 × 10−5,",
          "we also included the results from [7] that uses self-supervised": ""
        },
        {
          "4.2. Pre-training": "",
          "we also included the results from [7] that uses self-supervised": "Model\nActivation\nValence\nDominance"
        },
        {
          "4.2. Pre-training": "β1 = 0.9, β2 = 0.999. The λ was chosen such that the ASR",
          "we also included the results from [7] that uses self-supervised": ""
        },
        {
          "4.2. Pre-training": "and sentiment classiﬁcation losses achieve similar value on",
          "we also included the results from [7] that uses self-supervised": "ASR features\n0.649\n0.393\n0.544"
        },
        {
          "4.2. Pre-training": "the validation set.",
          "we also included the results from [7] that uses self-supervised": "0.706\n0.639\nCPC-based pre-traing [7]\n0.377"
        },
        {
          "4.2. Pre-training": "The LFBE features are normalized on a sample level. The",
          "we also included the results from [7] that uses self-supervised": "0.412\nSA2SR features\n0.679\n0.564"
        },
        {
          "4.2. Pre-training": "training set\nis then augmented using speed augmentation of",
          "we also included the results from [7] that uses self-supervised": ""
        },
        {
          "4.2. Pre-training": "factors 0.9 and 1.1. This augmentation step triples the training",
          "we also included the results from [7] that uses self-supervised": "Table 3.\nSA2SR produces good features for enhancing the"
        },
        {
          "4.2. Pre-training": "set size. The features are then masked on time and frequency",
          "we also included the results from [7] that uses self-supervised": "CCC metric"
        },
        {
          "4.2. Pre-training": "dimensions with probability p = 0.5, as in the SpecAugment",
          "we also included the results from [7] that uses self-supervised": ""
        },
        {
          "4.2. Pre-training": "method [21]. We ﬁnally stack every adjacent\nthree frames",
          "we also included the results from [7] that uses self-supervised": "We can see from Table 3 that recognition of valence, ar-"
        },
        {
          "4.2. Pre-training": "and skip every other two time steps, which leads to shorter se-",
          "we also included the results from [7] that uses self-supervised": "guably the most\nimportant dimension of emotion,\nis further"
        },
        {
          "4.2. Pre-training": "quences and improved model training time. The ASR training",
          "we also included the results from [7] that uses self-supervised": "improved compared to the strong “ASR features” baseline."
        },
        {
          "4.2. Pre-training": "uses a token set of 29 characters.",
          "we also included the results from [7] that uses self-supervised": "However, in this equal-weight multi-task emotion training set-"
        },
        {
          "4.2. Pre-training": "After each epoch, we use the validation set\nto compute",
          "we also included the results from [7] that uses self-supervised": "ting, we see that activation and dominance dimension per-"
        },
        {
          "4.2. Pre-training": "the Character Error Rate (CER) from the ASR task and Area",
          "we also included the results from [7] that uses self-supervised": "forms relatively weak compare to that of [7]. We view this as"
        },
        {
          "4.2. Pre-training": "Under the Curve (AUC) of the sentiment classiﬁer. The pre-",
          "we also included the results from [7] that uses self-supervised": "an encouraging result as in many applications, valence (posi-"
        },
        {
          "4.2. Pre-training": "training terminates when the metric M = CER − AU C does",
          "we also included the results from [7] that uses self-supervised": "tive v.s. negative) is of most interest."
        },
        {
          "4.2. Pre-training": "not improve for 25 epochs and the model that has the lowest",
          "we also included the results from [7] that uses self-supervised": ""
        },
        {
          "4.2. Pre-training": "",
          "we also included the results from [7] that uses self-supervised": "Additionally, during the pre-training of the SA2SR model,"
        },
        {
          "4.2. Pre-training": "M is evaluated. For the baseline model, we pre-train on the",
          "we also included the results from [7] that uses self-supervised": ""
        },
        {
          "4.2. Pre-training": "",
          "we also included the results from [7] that uses self-supervised": "we observed that\nthe sentiment classiﬁcation part achieves"
        },
        {
          "4.2. Pre-training": "ASR task only and use the model from the epoch with the best",
          "we also included the results from [7] that uses self-supervised": ""
        },
        {
          "4.2. Pre-training": "",
          "we also included the results from [7] that uses self-supervised": "weighted-average-recall of 0.71 and 0.81 AUC. This indicates"
        },
        {
          "4.2. Pre-training": "CER on the validation set.",
          "we also included the results from [7] that uses self-supervised": ""
        },
        {
          "4.2. Pre-training": "",
          "we also included the results from [7] that uses self-supervised": "that indeed the model is trained to recognize these proxy sen-"
        },
        {
          "4.2. Pre-training": "",
          "we also included the results from [7] that uses self-supervised": "timent labels. Therefore, we expect the learned representation"
        },
        {
          "4.2. Pre-training": "4.2.1. Effectiveness of ASR features for SER",
          "we also included the results from [7] that uses self-supervised": "to be suitable for the ﬁnal SER task."
        },
        {
          "4.2. Pre-training": "To start, we test\nthe effectiveness of ASR trained features",
          "we also included the results from [7] that uses self-supervised": ""
        },
        {
          "4.2. Pre-training": "for SER. Some previous work,\nfor example [5, 8],\nreported",
          "we also included the results from [7] that uses self-supervised": ""
        },
        {
          "4.2. Pre-training": "",
          "we also included the results from [7] that uses self-supervised": "4.2.3.\nImportance of ﬁne-tuning"
        },
        {
          "4.2. Pre-training": "limited transferability between ASR and SER.\nIn our exper-",
          "we also included the results from [7] that uses self-supervised": ""
        },
        {
          "4.2. Pre-training": "iments, however, we found ASR features to be quite effec-",
          "we also included the results from [7] that uses self-supervised": ""
        },
        {
          "4.2. Pre-training": "",
          "we also included the results from [7] that uses self-supervised": "Lastly, we are interested in examining whether we should"
        },
        {
          "4.2. Pre-training": "tive.\nIt\nis possible that\nthe differences are due to the large",
          "we also included the results from [7] that uses self-supervised": ""
        },
        {
          "4.2. Pre-training": "",
          "we also included the results from [7] that uses self-supervised": "freeze the learned representations during SER training.\nFor"
        },
        {
          "4.2. Pre-training": "pre-training dataset (full Librispeech) and modern model ar-",
          "we also included the results from [7] that uses self-supervised": ""
        },
        {
          "4.2. Pre-training": "",
          "we also included the results from [7] that uses self-supervised": "both ASR and SA2SR features, we train models with and"
        },
        {
          "4.2. Pre-training": "chitectures such as bidirectional LSTM and transformers.",
          "we also included the results from [7] that uses self-supervised": ""
        },
        {
          "4.2. Pre-training": "",
          "we also included the results from [7] that uses self-supervised": "without\nfreezing the acoustic encoder.\nThe results are re-"
        },
        {
          "4.2. Pre-training": "",
          "we also included the results from [7] that uses self-supervised": "ported in Table 4."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5. CONCLUSION": "We proposed a novel pre-training method that utilizes proxy",
          "6. REFERENCES": "[1] Smiley Blanton,\n“The voice and the emotions,” Quar-"
        },
        {
          "5. CONCLUSION": "sentiment\nlabels\nto aid ASR pre-training for SER. As\ntext-",
          "6. REFERENCES": "terly Journal of Speech, vol. 1, no. 2, pp. 154–172, 1915."
        },
        {
          "5. CONCLUSION": "sentiment and speech-emotion are correlated,\nthis way we",
          "6. REFERENCES": ""
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "[2] William F. Soskin and Paul E. Kauffman,\n“Judgment"
        },
        {
          "5. CONCLUSION": "train\nspeech\nrepresentations\ncapturing\nboth\nphonetic\nand",
          "6. REFERENCES": ""
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "Journal of\nof Emotion in Word-Free Voice Samples,”"
        },
        {
          "5. CONCLUSION": "emotion-relevant\ninfo. We evaluated the proposed method",
          "6. REFERENCES": ""
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "Communication, vol. 11, no. 2, pp. 73–80, 02 2006."
        },
        {
          "5. CONCLUSION": "on the MSP-Podcast dataset achieving state of the art perfor-",
          "6. REFERENCES": ""
        },
        {
          "5. CONCLUSION": "mance on the challenging valence dimension.",
          "6. REFERENCES": ""
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "[3] Paul Ekman, “An argument for basic emotions,” Cogni-"
        },
        {
          "5. CONCLUSION": "Albeit we focused on ASR-based pre-training,\nthe proxy",
          "6. REFERENCES": ""
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "tion and Emotion, vol. 6, no. 3-4, pp. 169–200, 1992."
        },
        {
          "5. CONCLUSION": "sentiment classiﬁcation task can be combined with other pre-",
          "6. REFERENCES": ""
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "[4] Alan S. Cowen and Dacher Keltner,\n“Self-report cap-"
        },
        {
          "5. CONCLUSION": "training techniques, such as APC [22], CPC [23], which we",
          "6. REFERENCES": ""
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "tures 27 distinct categories of emotion bridged by con-"
        },
        {
          "5. CONCLUSION": "will address in the future work.",
          "6. REFERENCES": ""
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "tinuous gradients,” PNAS, published online September"
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "5, 2017."
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "[5] Haytham M Fayek, Margaret Lech, and Lawrence Cave-"
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "don,\n“On the correlation and transferability of features"
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "between automatic speech recognition and speech emo-"
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "tion recognition.,” in Interspeech, 2016, pp. 3618–3622."
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "[6] Vasudha Kowtha, Vikramjit Mitra, Chris Bartels, Erik"
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "Marchi, Sue Booker, William Caruso, Sachin Kajarekar,"
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "and Devang Naik,\n“Detecting emotion primitives from"
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "speech and their use in discerning categorical emotions,”"
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "in ICASSP 2020. IEEE, 2020, pp. 7164–7168."
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "[7] Mao Li, Bo Yang, Joshua Levy, Andreas Stolcke, Vik-"
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "tor Rozgic, Spyros Matsoukas, Constantinos Papayian-"
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "nis, Daniel Bone, and Chao Wang,\n“Contrastive unsu-"
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "pervised learning for speech emotion recognition,”\nin"
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "ICASSP 2021. IEEE, 2021, pp. 6329–6333."
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "[8] Egor Lakomkin, Cornelius Weber,\nSven Magg,\nand"
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "Stefan Wermter,\n“Reusing neural\nspeech representa-"
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "tions for auditory emotion recognition,” arXiv preprint"
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "arXiv:1803.11508, 2018."
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "[9] Michelle Bancroft, Reza Lotﬁan, John Hansen, and Car-"
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "los Busso, “Exploring the intersection between speaker"
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "veriﬁcation and emotion recognition,”\nin ACIIW 2019."
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "IEEE, 2019, pp. 337–342."
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "[10] Raghavendra Pappagari, Tianzi Wang,\nJesus Villalba,"
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "Nanxin Chen, and Najim Dehak, “X-vectors meet emo-"
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "tions: A study on dependencies between emotion and"
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "speaker recognition,” in ICASSP 2020. IEEE, 2020, pp."
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "7169–7173."
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-"
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "to-End Speech Emotion Recognition Combined with"
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "Interspeech\nAcoustic-to-Word ASR Model,”\nin Proc."
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "2020, 2020, pp. 501–505."
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "[12] S Maghilnan and M Rajesh Kumar,\n“Sentiment anal-"
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "ysis on speaker speciﬁc speech data,”\nin 2017 Interna-"
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "tional Conference on Intelligent Computing and Control"
        },
        {
          "5. CONCLUSION": "",
          "6. REFERENCES": "(I2C2), 2017, pp. 1–5."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "“Multimodal\nspeech emotion recognition using audio"
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "and text,” CoRR, vol. abs/1810.04635, 2018."
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "[14] Suwon Shon,\nPablo Brusco,\nJing Pan, Kyu\nJ. Han,"
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "and Shinji Watanabe,\n“Leveraging\npre-trained\nlan-"
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "guage model for speech sentiment analysis,” CoRR, vol."
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "abs/2106.06598, 2021."
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "[15] Carlos Busso, Murtaza Bulut,\nChi-Chun Lee, Abe"
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "Kazemzadeh, Emily Mower, Samuel Kim, Jeannette N"
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "Chang,\nSungbok Lee,\nand\nShrikanth\nS Narayanan,"
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "“Iemocap:\nInteractive emotional dyadic motion capture"
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "database,” Language Resources and Evaluation, vol. 42,"
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "no. 4, pp. 335–359, 2008."
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "[16] Francesco Barbieri, Jos´e Camacho-Collados, Leonardo"
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "Neves, and Luis Espinosa Anke,\n“Tweeteval: Uniﬁed"
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "benchmark and comparative evaluation for tweet classi-"
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "ﬁcation,” CoRR, vol. abs/2010.12421, 2020."
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "[17] Alex Graves, Santiago Fern´andez, Faustino Gomez, and"
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "J¨urgen Schmidhuber,\n“Connectionist\ntemporal classiﬁ-"
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "cation:\nlabelling unsegmented sequence data with recur-"
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "the 23rd In-\nrent neural networks,”\nin Proceedings of"
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "ternational Conference on Machine Learning, 2006, pp."
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "369–376."
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "[18] Vassil Panayotov, Guoguo Chen, Daniel Povey, and San-"
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "jeev Khudanpur,\n“Librispeech: An ASR corpus based"
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "on public domain audio books,”\nICASSP 2015,\npp."
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "5206–5210, 2015."
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "[19] Mark Heitmann, Christian Siebert,\nJochen Hartmann,"
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "and Christina Schamp,\n“More than a feeling: Bench-"
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "marks for sentiment analysis accuracy,” Communication"
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "& Computational Methods eJournal, 2020."
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "[20] R. Lotﬁan and C. Busso,\n“Building naturalistic emo-"
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "tionally balanced speech corpus by retrieving emotional"
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "IEEE Trans-\nspeech from existing podcast recordings,”"
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "actions on Affective Computing, vol. 10, no. 4, pp. 471–"
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "483, October-December 2019."
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "[21] Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng"
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "Chiu, Barret Zoph, Ekin D. Cubuk, and Quoc V. Le,"
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "“Specaugment: A simple data augmentation method for"
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "automatic speech recognition,”\nInterspeech 2019, Sep"
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "2019."
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "[22] Yu-An Chung\nand\nJames Glass,\n“Generative\npre-"
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "training for speech with autoregressive predictive cod-"
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "ing,” in ICASSP 2020. IEEE, 2020, pp. 3497–3501."
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "[23] Aaron van den Oord, Yazhe Li, and Oriol Vinyals, “Rep-"
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "resentation learning with contrastive predictive coding,”"
        },
        {
          "[13] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,": "arXiv preprint arXiv:1807.03748, 2018."
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "CONCLUSION We proposed a novel pre-training method that utilizes proxy sentiment labels to aid ASR pre-training for SER. As textsentiment and speech-emotion are correlated, this way we train speech representations capturing both phonetic and emotion-relevant info. We evaluated the proposed method on the MSP-Podcast dataset achieving state of the art performance on the challenging valence dimension. Albeit we focused on ASR-based pre-training, the proxy sentiment classification task can be combined with other pretraining techniques",
      "venue": "CONCLUSION We proposed a novel pre-training method that utilizes proxy sentiment labels to aid ASR pre-training for SER. As textsentiment and speech-emotion are correlated, this way we train speech representations capturing both phonetic and emotion-relevant info. We evaluated the proposed method on the MSP-Podcast dataset achieving state of the art performance on the challenging valence dimension. Albeit we focused on ASR-based pre-training, the proxy sentiment classification task can be combined with other pretraining techniques"
    },
    {
      "citation_id": "2",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "3",
      "title": "The voice and the emotions",
      "authors": [
        "Smiley Blanton"
      ],
      "year": "1915",
      "venue": "Quarterly Journal of Speech"
    },
    {
      "citation_id": "4",
      "title": "Judgment of Emotion in Word-Free Voice Samples",
      "authors": [
        "William Soskin",
        "Paul Kauffman"
      ],
      "year": "2006",
      "venue": "Journal of Communication"
    },
    {
      "citation_id": "5",
      "title": "An argument for basic emotions",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1992",
      "venue": "Cognition and Emotion"
    },
    {
      "citation_id": "6",
      "title": "Self-report captures 27 distinct categories of emotion bridged by continuous gradients",
      "authors": [
        "Alan Cowen",
        "Dacher Keltner"
      ],
      "year": "2017",
      "venue": "PNAS"
    },
    {
      "citation_id": "7",
      "title": "On the correlation and transferability of features between automatic speech recognition and speech emotion recognition",
      "authors": [
        "Margaret Haytham M Fayek",
        "Lawrence Lech",
        "Cavedon"
      ],
      "year": "2016",
      "venue": "On the correlation and transferability of features between automatic speech recognition and speech emotion recognition"
    },
    {
      "citation_id": "8",
      "title": "Detecting emotion primitives from speech and their use in discerning categorical emotions",
      "authors": [
        "Vasudha Kowtha",
        "Vikramjit Mitra",
        "Chris Bartels",
        "Erik Marchi",
        "Sue Booker",
        "William Caruso",
        "Sachin Kajarekar",
        "Devang Naik"
      ],
      "venue": "Detecting emotion primitives from speech and their use in discerning categorical emotions"
    },
    {
      "citation_id": "9",
      "title": "Contrastive unsupervised learning for speech emotion recognition",
      "authors": [
        "Mao Li",
        "Bo Yang",
        "Joshua Levy",
        "Andreas Stolcke",
        "Viktor Rozgic",
        "Spyros Matsoukas",
        "Constantinos Papayiannis",
        "Daniel Bone",
        "Chao Wang"
      ],
      "venue": "Contrastive unsupervised learning for speech emotion recognition"
    },
    {
      "citation_id": "10",
      "title": "Reusing neural speech representations for auditory emotion recognition",
      "authors": [
        "Egor Lakomkin",
        "Cornelius Weber",
        "Sven Magg",
        "Stefan Wermter"
      ],
      "year": "2018",
      "venue": "Reusing neural speech representations for auditory emotion recognition",
      "arxiv": "arXiv:1803.11508"
    },
    {
      "citation_id": "11",
      "title": "Exploring the intersection between speaker verification and emotion recognition",
      "authors": [
        "Michelle Bancroft",
        "Reza Lotfian",
        "John Hansen",
        "Carlos Busso"
      ],
      "year": "2019",
      "venue": "Exploring the intersection between speaker verification and emotion recognition"
    },
    {
      "citation_id": "12",
      "title": "X-vectors meet emotions: A study on dependencies between emotion and speaker recognition",
      "authors": [
        "Raghavendra Pappagari",
        "Tianzi Wang",
        "Jesus Villalba",
        "Nanxin Chen",
        "Najim Dehak"
      ],
      "venue": "X-vectors meet emotions: A study on dependencies between emotion and speaker recognition"
    },
    {
      "citation_id": "13",
      "title": "Endto-End Speech Emotion Recognition Combined with Acoustic-to-Word ASR Model",
      "authors": [
        "Han Feng",
        "Sei Ueno",
        "Tatsuya Kawahara"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "14",
      "title": "Sentiment analysis on speaker specific speech data",
      "authors": [
        "S Maghilnan",
        "M Rajesh Kumar"
      ],
      "year": "2017",
      "venue": "2017 International Conference on Intelligent Computing and Control"
    },
    {
      "citation_id": "15",
      "title": "Multimodal speech emotion recognition using audio and text",
      "authors": [
        "Seunghyun Yoon",
        "Seokhyun Byun",
        "Kyomin Jung"
      ],
      "year": "2018",
      "venue": "CoRR"
    },
    {
      "citation_id": "16",
      "title": "Leveraging pre-trained language model for speech sentiment analysis",
      "authors": [
        "Suwon Shon",
        "Pablo Brusco",
        "Jing Pan",
        "J Kyu",
        "Shinji Han",
        "Watanabe"
      ],
      "year": "2021",
      "venue": "CoRR"
    },
    {
      "citation_id": "17",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "18",
      "title": "Tweeteval: Unified benchmark and comparative evaluation for tweet classification",
      "authors": [
        "Francesco Barbieri",
        "José Camacho-Collados",
        "Leonardo Neves",
        "Luis Espinosa"
      ],
      "year": "2020",
      "venue": "CoRR"
    },
    {
      "citation_id": "19",
      "title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
      "authors": [
        "Alex Graves",
        "Santiago Fernández",
        "Faustino Gomez",
        "Jürgen Schmidhuber"
      ],
      "year": "2006",
      "venue": "Proceedings of the 23rd International Conference on Machine Learning"
    },
    {
      "citation_id": "20",
      "title": "Librispeech: An ASR corpus based on public domain audio books",
      "authors": [
        "Vassil Panayotov",
        "Guoguo Chen",
        "Daniel Povey",
        "Sanjeev Khudanpur"
      ],
      "year": "2015",
      "venue": "Librispeech: An ASR corpus based on public domain audio books"
    },
    {
      "citation_id": "21",
      "title": "More than a feeling: Benchmarks for sentiment analysis accuracy",
      "authors": [
        "Mark Heitmann",
        "Christian Siebert",
        "Jochen Hartmann",
        "Christina Schamp"
      ],
      "year": "2020",
      "venue": "Communication & Computational Methods eJournal"
    },
    {
      "citation_id": "22",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "23",
      "title": "Specaugment: A simple data augmentation method for automatic speech recognition",
      "authors": [
        "Daniel Park",
        "William Chan",
        "Yu Zhang",
        "Chung-Cheng Chiu",
        "Barret Zoph",
        "Ekin Cubuk",
        "Quoc Le"
      ],
      "year": "2019",
      "venue": "Specaugment: A simple data augmentation method for automatic speech recognition"
    },
    {
      "citation_id": "24",
      "title": "Generative pretraining for speech with autoregressive predictive coding",
      "authors": [
        "Yu-An Chung",
        "James Glass"
      ],
      "venue": "Generative pretraining for speech with autoregressive predictive coding"
    },
    {
      "citation_id": "25",
      "title": "Representation learning with contrastive predictive coding",
      "authors": [
        "Aaron Van Den Oord",
        "Yazhe Li",
        "Oriol Vinyals"
      ],
      "year": "2018",
      "venue": "Representation learning with contrastive predictive coding",
      "arxiv": "arXiv:1807.03748"
    }
  ]
}