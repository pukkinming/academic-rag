{
  "paper_id": "2410.21897v3",
  "title": "Semi-Supervised Self-Learning Enhanced Music Emotion Recognition",
  "published": "2024-10-29T09:42:07Z",
  "authors": [
    "Yifu Sun",
    "Xulong Zhang",
    "Monan Zhou",
    "Wei Li"
  ],
  "keywords": [
    "Music emotion recognition",
    "Learning with label noise",
    "Semi-supervised learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Music emotion recognition (MER) aims to identify the emotions conveyed in a given musical piece. However, currently, in the field of MER, the available public datasets have limited sample sizes. Recently, segment-based methods for emotion-related tasks have been proposed, which train backbone networks on shorter segments instead of entire audio clips, thereby naturally augmenting training samples without requiring additional resources. Then, the predicted segment-level results are aggregated to obtain the entire song prediction. The most commonly used method is that the segment inherits the label of the clip containing it, but music emotion is not constant during the whole clip. Doing so will introduce label noise and make the training easy to overfit. To handle the noisy label issue, we propose a semi-supervised self-learning (SSSL) method, which can differentiate between samples with correct and incorrect labels in a self-learning manner, thus effectively utilizing the augmented segmentlevel data. Experiments on three public emotional datasets demonstrate that the proposed method can achieve better or comparable performance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The music emotion recognition (MER) task aims to recognize the emotion expressed in a given music clip automatically. MER can be widely used in many fields, such as dynamically generating music to adapt to the emotion of scenes in movies or games  [1] , music-assisted psychological or physical therapy, personalized recommendation in stream media, humanmachine interaction, music retrieval, and so on, which has broad application prospects. In recent years, as the amount of data grows, data-driven deep learning methods have become the mainstream method in the music information retrieval (MIR) field  [2, 3] .\n\nAt present, the duration of audio clips in public music emotion datasets is 30 ∼ 45 seconds. Although the longer the duration is, the more helpful it is to distinguish emotions, according to the study of music psychology, it is found that the duration of about one second of music is sufficient to evoke an emotional reaction  [4] . To address the issue of limited annotated data in emotion recognition tasks, some segment-based methods  [5] [6] [7] [8]  have been proposed recently, which naturally increase the amount of training data and can make full use of every audio sample in the dataset.\n\nAfter the audio clip is divided into segments, Sarkar et al.  [7]  make each segment inherit the label of the clip containing it, which is also the simplest method, then majority vote and maximum run length are used to obtain clip-level results. However, the emotion of the music is not constant. Therefore, each segment may actually carry different emotions, which also introduces the problem of a noisy label. He et al.  [8]  used an unsupervised method, i.e. using autoencoder to reconstruct the masked mel spectrogram of the segment to obtain audio segment embedding. Then a supervised learning structure using Bi-directional Long Short-Term Memory (BiLSTM) is employed to capture temporal music information and perform emotion classification. However, it is unknown how many emotion-related features are included in the embedding. In the field of speech emotion recognition, Mao et al.\n\n[9] leveraged a self-learning framework to update model parameters and segment labels iteratively in the training process and used soft labels instead of hard labels, which to some extent solved the problem of noisy labels. However, only using the output probability distribution of the model as the soft label of the next epoch will excessively rely on the prediction ability of the model. Once the model makes a prediction error, this error will deepen with the training, which is called confirmation bias.\n\nInspired by  [10]  when the mixture of correct and incorrect labels are fed to the deep neural networks (DNNs), networks have a tendency to learn the latter after the former.\n\nTherefore, we propose a semi-supervised self-learning (SSSL) framework, to model each training sample's loss value, and to distinguish the training samples most likely to be clean from those most likely to be noisy. Then we use the mixup  [11]  data augmentation algorithm and consistency regularization to prevent the confirmation bias of the model's prediction. After obtaining a label noise-robust segment-level emotion predictor, we can use it to generate the predicted probabilities of each segment in the song-level data as a structured feature representation. Finally, a second machine learning algorithm is employed to predict the overall emotion for each song. Our main contributions are:\n\n1. Instead of directly inheriting clip-level labels for each segment or unsupervised methods that do not use labels at all, we use semi-supervised learning to deal with noisy labels.\n\n2. Combining noisy label processing with semi-supervised learning, to mitigate the confirmation bias issue associated with self-training, which can lead to the accumulation of model errors.\n\n3. Compared with baseline models, the effect is improved.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Method",
      "text": "Our method is divided into two steps. The first step is to train a segment-level classifier robust to label noise on the expanded segment-level dataset. The second step uses the original song-level dataset to predict each segment in each song, obtain the statistical value of the probability distribution of each segment, and aggregate them as the feature representation of the song. Then, a machine learning method is used to complete the emotion prediction of the song. Figure  1  illustrates the overall framework of our proposed algorithm.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Sssl Framework",
      "text": "We propose an SSSL on the extended segment dataset, aiming to obtain a label-noise robust segment-level classifier. Unlike conventional semi-supervised learning methods, our labeled data and unlabeled data generation are dynamic. Moreover, we do not assign a fixed pseudo-label to the unlabeled data.\n\nAt the beginning of each epoch, the training set is partitioned into a clean set and a labeled noisy set using a two-component Gaussian mixture model (GMM) by the crossentropy (CE) loss value for each training sample. Then the expectation-maximization (EM) algorithms are leveraged to fit the GMM to the observation. Then, the semi-supervised learning method is used to treat the clean set as the labeled set X while the noisy set is the unlabeled set U. We erase the label of the U set, use the predicted value of the model as the soft label, and then use a sharpening algorithm to get the pseudo soft label. Then we obtain the total loss using mixup and consistency regularization.\n\nThe CE loss function in a classification problem is defined as Eq. (  2 ), where f denotes the output of the neural network classifier, i.e. the softmax layer.\n\nBut when we use the above formulas (  1 ) and (  2 ) to train on the noisy label dataset, severe overfitting will occur.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Training Samples Partition",
      "text": "DNNs have been observed to prioritize learning from simple and logically consistent samples in the presence of noisy labels, resulting in reduced loss for these samples, particularly in the early stages of training  [12] . This phenomenon suggests that the loss distributions of clean and noisy samples during training can be approximated by two Gaussian distributions, with the clean samples having a smaller mean loss. Leveraging this training characteristic, we employ a GMM to differentiate between noisy and clean samples by utilizing the per-sample loss as input. The probability density function of the loss K-component mixture model is defined as Eq. (  3 ), where λ k indicates the mixing coefficients for the combination of each probability density function p(l|k).\n\nRegarding this case, we can utilize a two-component GMM to model the distribution of clean and noisy samples. We utilize a two-component GMM and then feed the CE loss L of every training data to the GMM. Then, to estimate the parameters of the GMM we apply the EM algorithm. We define the posterior probability ω i as the probability that the i th data sample belongs to the Gaussian component with a smaller mean, given its loss l i .\n\nBy setting a global threshold τ for the probability ω of all training samples' labels to be clean, we can split the expanded segment-level dataset D into two parts: set X and set U.\n\nThe labels in set X have more possibilities to be correct, and thus, they will be used as the labeled set. On the other hand, the labels in set U are more likely to be incorrect, and will be erased in the subsequent steps. Therefore, set U will be used as the unlabeled set.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Modified Semi-Supervised Learning",
      "text": "We first preprocess the set U. For the set U, the initial labels are likely to be incorrect and have been erased. As a result, we generate pseudo soft labels ŷ by sharpening the predicted distribution of the model by Eq. (  4 ), where S(•) indicates the temperature sharpening function often used in pseudo labeling, and T represents the temperature coefficient.\n\nAnd the Û will be generated then as Eq. (  5 ).\n\nAs  [13]  has shown the mixup technology can eliminate confirmation bias to a certain extent. This technique involves training on convex combinations of pairs of samples, denoted as x p and x q , along with their corresponding labels y p and y q shown as Eq. (  6 ), where δ is drawn from a beta distribution randomly.\n\nThis integration introduces regularization to encourage the network to exhibit linear behavior between samples, thereby reducing fluctuations in distant regions. In terms of label noise, mixup offers a strategy to merge clean and noisy samples, resulting in a more representative loss that guides the training procedure.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Loss Function",
      "text": "Eq. (  6 ) can be regarded in the loss as l = δl p + (1 -δ)l q , and the standard CE loss is leveraged for semi-supervised learning part by Eq. (  7 ):\n\nConfirmation bias, resulting from the accumulation of errors, is a common issue in selftraining. Model ensembling is a widely adopted approach to mitigate this bias. Dropout, which can be viewed as an implicit form of model ensembling, is commonly used during network training. However, when it comes to sample selection or inference, dropout is typically disabled to ensure consistency. In the presence of label noise, the decision boundaries between classes become blurred, leading to significant inconsistencies among sub-models.\n\nTo address this, we incorporate the R-Drop loss  [14] , a straightforward yet effective dropout regularization method shown as Eq. (  8 ), to promote consistency among the sub-models.\n\nTherefore, the total loss is as Eq. (  9 ), where λ is the hyperparameter to control the weight.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Song-Level Decisions",
      "text": "The final goal is to get song-level prediction results. Given a sequence of probability distributions of emotional states generated by a segment-level classifier for a song, we can make decisions based on the information.\n\nA reliable segment-level classifier serves as a prerequisite for clip-level classification.\n\nThis allows us to utilize machine learning algorithms to handle structured features obtained from statistical properties of the segment probability distributions. The probability of the k th emotion for segment s is defined as P s (E k ) in Eq.  (10) , where C represents the set of all segments in a single song.\n\nFeatures f k 1 , f k 2 and f k 3 correspond to the maximum value of the segment-level probability of the k th emotion in the song, minimum, and average values, respectively. The features f k 3 ∼ f k 5 correspond to the k th emotion's three quartiles in the song respectively. The feature f k 7 represents the percentage of segments with a high probability of sentiment k. The feature exhibits low sensitivity to a threshold, allowing for empirical selection. This aggregation step produces a feature representation of dimension 7 × K for each song. With this set of song-level feature representations, we can train a secondary classifier that is relatively simple in nature to make decisions at the song level.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Experiments",
      "text": "We tested our proposed algorithm on three public datasets: PMEmo dataset  [15] , Emotion in Music (EiM)  [16]  and 4Q dataset  [17] . Since deep learning is data hungry, these data volumes are difficult to support models with strong training generalization ability and are easy to overfit.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiment Setup",
      "text": "We convert each segment unit into a mel spectrogram, with the Hanning window length of 1,024 and window hop size of 512 using the librosa  [18] . We utilize 128 mel bins. Once the audio clip is segmented into smaller parts, the amount of training samples is dozens of times larger than the original. Our segment classifier uses VGG16  [19] , we only modify the input channel and output number of classification. The support vector machine (SVM) is used for song-level decisions.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Datasets",
      "text": "PMEmo: This dataset contains 794 pieces of music, all of which are pop music in mp3 format. The audio duration is mainly around 45 seconds. We utilize 767 of these clips with static valence/arousal (V/A) annotations in our work.\n\n4Q: There are 900 music clips in this dataset. All music clips are divided into four parts according to Russell's V/A quadrant, with 225 clips in each part. Most audio clips are approximately 30 seconds.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Eim:",
      "text": "The dataset consists of 1,000 music samples, each 45 seconds long, obtained from sources like Jamendo, with copyrighted music. The labels for this dataset were obtained through crowdsourcing platforms. Just like in previous works, we utilized a set of 744 audio samples after removing duplicates.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Audio Pre-Processing",
      "text": "In the previous works, in the methods of taking the entire clip as input  [20, 21] , 30 seconds of audio are generally reserved, and the part less than 30 seconds is padded with zeros. In  [8] , for clips less than 30 seconds, they padded them to 30 seconds by repeating themselves continuously from the beginning to the end.\n\nSince the variance of sample duration in different data sets is large, when the method of filling zeros is used, many blank segments will be generated, which is invalid training data. Using the circular padding method will generate too many repeated training samples. Therefore, we did not do any padding.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Results",
      "text": "In this section, we present the performance results for different segment durations and compare them with the results obtained from other MER methods.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Performance At Different Segment Duration",
      "text": "According to the previous music psychology research  [4, 22] , people can react to and make judgments about the emotions in music within 1s. The longer the segment, the more helpful it is for emotion recognition, but too long segments will reduce the data volume of the segment dataset, so a compromise is required. Thus, we experimented with integer segment duration from 1s to 5s, the overlap of adjacent segments is 1s less than the segment duration. After segmentation, see Table  1  for the sample number comparison between the expanded segment-level dataset and the original song-level dataset. The experimental results of different segment durations on binary classification are shown in Table  2 .\n\nThe findings also indicate that shorter segment durations exhibit better performance in capturing the arousal dimension, whereas longer segment durations are advantageous for recognizing valence. For example, in the PMEmo dataset, the 1s segment showed the best arousal result with an accuracy of 85.42% and an F1-score of 86.61%, while the 3s segment showed a better valence accuracy of 83.19% and F1-score: 82.31%. The results on the 4Q dataset show a similar trend.\n\nFor such results, our analysis may be that arousal and intensity are more correlated, and intensity is relatively easier to be preferentially recognized by the auditory system. The identification of valence requires a longer period, this dimension involves more psychological knowledge, and the perception process of psychology is much more complicated, so it needs a longer time period  [4] . Moreover, we found it is not that the longer the segment duration, the better the experimental results. It may be related to the overlap value used by different segment durations. To ensure the amount of data, we leverage a larger overlap value in long segments, which will lead to data redundancy and make the model overfit the data.\n\nLeading to poor generalization ability to new data.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Validation Experiment",
      "text": "Figure  2  shows an example of emotion predictions for each segment of the test sample MT0010465830 in the 4Q dataset, which is labeled as happy (Q1). The segment classifier has four outputs corresponding to four different emotional states: Q1, Q2, Q3, and Q4.\n\nAs shown in the figure, the probabilities for each emotion change throughout the entire song. The true emotion of the whole song is Q1, and as seen in the figure, the Q1 probability is highest in most segments, but there are also other emotions dominating some parts of the song. While not all songs exhibit distinctive segment-level outputs, we can employ the songlevel classifier to discern them. The high-pitched and cheerful singing of the male singer at the beginning of the song kept the Q1 prediction probability the highest for the first few seconds. Then the vocals quickly changed to a low tone, and Q3 (sadness) took over. At 7s, the trumpet and drums come in and shift the emotion back to happiness. In the 20s, although the vocals turned to a low tone again, the background drumbeats and trumpet sounds continued, so the Q1 probability only slightly decreased, still dominant overall.\n\nThe audio content confirms that the emotional expression in music changes dynamically throughout the song, rather than being consistent throughout.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Experimental Results Compared With Other Models",
      "text": "In order to make a fair comparison, our experimental results are all the average values obtained under the cross-validation of ten fold. Among them, the proposed* is the ablation  In comparison to the models listed in Table  3 , our model, which utilizes segment-level data, demonstrates superior performance when compared to other models that directly use entire music clips. Emotional states in long music fragments may have changed or be in transition between different emotional states  [22] , which may confuse the learning model and make it difficult to extract unified musical features specific to one emotion. In addition, the emotional value of music may be influenced by the processing of Western harmony, particularly the extensive use of minor keys. This processing requires more time to fully comprehend compared to the arousal dimension, which is primarily associated with the dynamic aspects of music stimulation  [4] . The best results of the same task have been achieved in most of the evaluation indicators of the binary classification problem. Segmentbased method alleviates this problem, as emotions tend to be more constant over shorter",
      "page_start": 8,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: illustrates the overall framework of our proposed algorithm.",
      "page": 3
    },
    {
      "caption": "Figure 1: Flow chart of the proposed method: blue arrows indicate the clean set (Gaussian",
      "page": 3
    },
    {
      "caption": "Figure 2: shows an example of emotion predictions for each segment of the test sample",
      "page": 8
    },
    {
      "caption": "Figure 2: Segment-level classifier output on test sample MT0010465830",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3 Department of Music AI and Information Technology, Central Conservatory of Music, Beijing, China": "4 Shanghai Key Laboratory of Intelligent Information Processing, Fudan University, Shanghai, China"
        },
        {
          "3 Department of Music AI and Information Technology, Central Conservatory of Music, Beijing, China": ""
        },
        {
          "3 Department of Music AI and Information Technology, Central Conservatory of Music, Beijing, China": "the available public datasets have\nlimited sample"
        },
        {
          "3 Department of Music AI and Information Technology, Central Conservatory of Music, Beijing, China": "for\nemotion-related tasks have been proposed, which train"
        },
        {
          "3 Department of Music AI and Information Technology, Central Conservatory of Music, Beijing, China": "instead of\nentire audio clips,\nthereby naturally augmenting"
        },
        {
          "3 Department of Music AI and Information Technology, Central Conservatory of Music, Beijing, China": ""
        },
        {
          "3 Department of Music AI and Information Technology, Central Conservatory of Music, Beijing, China": ""
        },
        {
          "3 Department of Music AI and Information Technology, Central Conservatory of Music, Beijing, China": "the clip containing it, but music emotion is not constant during the whole clip."
        },
        {
          "3 Department of Music AI and Information Technology, Central Conservatory of Music, Beijing, China": ""
        },
        {
          "3 Department of Music AI and Information Technology, Central Conservatory of Music, Beijing, China": ""
        },
        {
          "3 Department of Music AI and Information Technology, Central Conservatory of Music, Beijing, China": ""
        },
        {
          "3 Department of Music AI and Information Technology, Central Conservatory of Music, Beijing, China": ""
        },
        {
          "3 Department of Music AI and Information Technology, Central Conservatory of Music, Beijing, China": ""
        },
        {
          "3 Department of Music AI and Information Technology, Central Conservatory of Music, Beijing, China": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "make full use of every audio sample in the dataset.": "After the audio clip is divided into segments, Sarkar et al.\n[7] make each segment inherit"
        },
        {
          "make full use of every audio sample in the dataset.": "the label of\nthe clip containing it, which is also the simplest method,\nthen majority vote"
        },
        {
          "make full use of every audio sample in the dataset.": "and maximum run length are used to obtain clip-level results. However, the emotion of the"
        },
        {
          "make full use of every audio sample in the dataset.": "music is not constant. Therefore, each segment may actually carry different emotions, which"
        },
        {
          "make full use of every audio sample in the dataset.": "also introduces the problem of a noisy label. He et al.\n[8] used an unsupervised method,"
        },
        {
          "make full use of every audio sample in the dataset.": "i.e. using autoencoder to reconstruct the masked mel spectrogram of the segment to obtain"
        },
        {
          "make full use of every audio sample in the dataset.": "audio segment embedding. Then a supervised learning structure using Bi-directional Long"
        },
        {
          "make full use of every audio sample in the dataset.": "Short-Term Memory (BiLSTM)\nis employed to capture temporal music information and"
        },
        {
          "make full use of every audio sample in the dataset.": "perform emotion classification. However,\nit is unknown how many emotion-related features"
        },
        {
          "make full use of every audio sample in the dataset.": "are\nincluded in the\nembedding.\nIn the field of\nspeech emotion recognition, Mao et al."
        },
        {
          "make full use of every audio sample in the dataset.": "[9]\nleveraged a self-learning framework to update model parameters and segment\nlabels"
        },
        {
          "make full use of every audio sample in the dataset.": "iteratively in the\ntraining process and used soft\nlabels\ninstead of hard labels, which to"
        },
        {
          "make full use of every audio sample in the dataset.": "some extent solved the problem of noisy labels. However, only using the output probability"
        },
        {
          "make full use of every audio sample in the dataset.": "distribution of\nthe model as\nthe soft\nlabel of\nthe next epoch will excessively rely on the"
        },
        {
          "make full use of every audio sample in the dataset.": "prediction ability of\nthe model. Once the model makes a prediction error,\nthis error will"
        },
        {
          "make full use of every audio sample in the dataset.": "deepen with the training, which is called confirmation bias."
        },
        {
          "make full use of every audio sample in the dataset.": "Inspired by [10] when the mixture of correct and incorrect labels are fed to the deep"
        },
        {
          "make full use of every audio sample in the dataset.": "neural networks\n(DNNs), networks have a tendency to learn the latter after\nthe former."
        },
        {
          "make full use of every audio sample in the dataset.": "Therefore, we propose a semi-supervised self-learning (SSSL)\nframework,\nto model\neach"
        },
        {
          "make full use of every audio sample in the dataset.": "training sample’s\nloss value,\nand to distinguish the\ntraining samples most\nlikely to be"
        },
        {
          "make full use of every audio sample in the dataset.": "clean from those most likely to be noisy. Then we use the mixup [11] data augmentation"
        },
        {
          "make full use of every audio sample in the dataset.": "algorithm and consistency regularization to prevent\nthe confirmation bias of\nthe model’s"
        },
        {
          "make full use of every audio sample in the dataset.": "prediction. After obtaining a label noise-robust\nsegment-level emotion predictor, we can"
        },
        {
          "make full use of every audio sample in the dataset.": "use it\nto generate the predicted probabilities of each segment\nin the song-level data as a"
        },
        {
          "make full use of every audio sample in the dataset.": "structured feature representation. Finally, a second machine learning algorithm is employed"
        },
        {
          "make full use of every audio sample in the dataset.": "to predict the overall emotion for each song. Our main contributions are:"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "use it\nto generate the predicted probabilities of each segment\nin the song-level data as a": "structured feature representation. Finally, a second machine learning algorithm is employed"
        },
        {
          "use it\nto generate the predicted probabilities of each segment\nin the song-level data as a": "to predict the overall emotion for each song. Our main contributions are:"
        },
        {
          "use it\nto generate the predicted probabilities of each segment\nin the song-level data as a": "1.\nInstead of directly inheriting clip-level\nlabels for each segment or unsupervised meth-"
        },
        {
          "use it\nto generate the predicted probabilities of each segment\nin the song-level data as a": "ods that do not use labels at all, we use semi-supervised learning to deal with noisy"
        },
        {
          "use it\nto generate the predicted probabilities of each segment\nin the song-level data as a": "labels."
        },
        {
          "use it\nto generate the predicted probabilities of each segment\nin the song-level data as a": "2. Combining noisy label processing with semi-supervised learning, to mitigate the con-"
        },
        {
          "use it\nto generate the predicted probabilities of each segment\nin the song-level data as a": "firmation bias issue associated with self-training, which can lead to the accumulation"
        },
        {
          "use it\nto generate the predicted probabilities of each segment\nin the song-level data as a": "of model errors."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "of the song. Figure 1 illustrates the overall\nframework of our proposed algorithm."
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "Semi-supervised \nLoss MIX"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "Mixup\nX\nSelf-learning"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "Loss TOTAL"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "Prediction\nSharpening"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "2-component"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "U"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "GMM"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "Loss KL"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "DNN\nDNN\nDNN\nSeg-level"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "D"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "Dataset\nUpdate \n(1)\n(2)\n(M)"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "Parameters"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "Seg-level Classifier"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "Statistics for"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "seg-level"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "p(x1)"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "predictions\nSong-"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "p(x2)"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "DNN\nSong-level"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "level"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "Inference\n(M)"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "Classifier"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "p(xN)"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "Inference for \nSong-level"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "each segment\nRepresentation"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "Fig. 1: Flow chart of the proposed method: blue arrows indicate the clean set (Gaussian"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "distribution with smaller mean), red arrows indicate the noisy set, which is treated as the"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "unlabeled set\nin subsequent\nsemi-supervised learning where the clean set\nserves as\nthe"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "labeled set"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "2.1\nSSSL Framework"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "We propose an SSSL on the\nextended segment dataset,\naiming to obtain a label-noise"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "robust segment-level classifier. Unlike conventional semi-supervised learning methods, our"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "labeled data and unlabeled data generation are dynamic. Moreover, we do not assign a"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "fixed pseudo-label to the unlabeled data."
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "At\nthe beginning of each epoch,\nthe training set\nis partitioned into a clean set and"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "a labeled noisy set using a two-component Gaussian mixture model (GMM) by the cross-"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "entropy (CE) loss value for each training sample. Then the expectation-maximization (EM)"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "algorithms are leveraged to fit\nthe GMM to the observation. Then,\nthe semi-supervised"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "learning method is used to treat the clean set as the labeled set X while the noisy set is"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "the unlabeled set U. We erase the label of the U set, use the predicted value of the model"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "as the soft label, and then use a sharpening algorithm to get the pseudo soft label. Then"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "we obtain the total\nloss using mixup and consistency regularization."
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "2.1.1\nTask Formulation"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "In a k-class classification problem,\nthe training data with n training samples denoted as"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "X = [x1, . . . , xn], along with their corresponding ground-truth labels Y = [y1, . . . , yn]. Each"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "yi represents the true class label and is represented as a k-dimensional one-hot vector. Clas-"
        },
        {
          "of the song. Then, a machine learning method is used to complete the emotion prediction": "sification problems on clean label datasets are often defined as Eq.\n(1), where θ represents"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "severe overfitting will occur.": "2.1.2\nTraining Samples Partition"
        },
        {
          "severe overfitting will occur.": "DNNs have been observed to prioritize learning from simple and logically consistent samples"
        },
        {
          "severe overfitting will occur.": "in the presence of noisy labels, resulting in reduced loss for these samples, particularly in the"
        },
        {
          "severe overfitting will occur.": "early stages of training [12]. This phenomenon suggests that the loss distributions of clean"
        },
        {
          "severe overfitting will occur.": "and noisy samples during training can be approximated by two Gaussian distributions, with"
        },
        {
          "severe overfitting will occur.": "the clean samples having a smaller mean loss. Leveraging this training characteristic, we"
        },
        {
          "severe overfitting will occur.": "employ a GMM to differentiate between noisy and clean samples by utilizing the per-sample"
        },
        {
          "severe overfitting will occur.": "loss as input. The probability density function of the loss K-component mixture model"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ˆ": "And the\nU will be generated then as Eq.\n(5)."
        },
        {
          "ˆ": "ˆ"
        },
        {
          "ˆ": "U = {(xi, ˆyi)|xi ∈ U}"
        },
        {
          "ˆ": ""
        },
        {
          "ˆ": "extent. This technique involves training on convex combinations of pairs of samples, denoted"
        },
        {
          "ˆ": "as xp and xq, along with their corresponding labels yp and yq"
        },
        {
          "ˆ": "drawn from a beta distribution randomly."
        },
        {
          "ˆ": "x = δxp + (1 − δ)xq"
        },
        {
          "ˆ": ""
        },
        {
          "ˆ": "y = δyp + (1 − δ)yq"
        },
        {
          "ˆ": ""
        },
        {
          "ˆ": "behavior between samples,\nthereby reducing fluctuations"
        },
        {
          "ˆ": ""
        },
        {
          "ˆ": "representative loss that guides the training procedure."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(10), where C represents the set of\nkth emotion for segment s is defined as Ps(Ek) in Eq.": "all segments in a single song."
        },
        {
          "(10), where C represents the set of\nkth emotion for segment s is defined as Ps(Ek) in Eq.": "f k\nPs(Ek)\n1 = max"
        },
        {
          "(10), where C represents the set of\nkth emotion for segment s is defined as Ps(Ek) in Eq.": "s∈C"
        },
        {
          "(10), where C represents the set of\nkth emotion for segment s is defined as Ps(Ek) in Eq.": "f k\nPs(Ek)\n2 = min"
        },
        {
          "(10), where C represents the set of\nkth emotion for segment s is defined as Ps(Ek) in Eq.": "s∈C"
        },
        {
          "(10), where C represents the set of\nkth emotion for segment s is defined as Ps(Ek) in Eq.": "f k\n3∼5 = Quartiles1∼3{Ps(Ek)}"
        },
        {
          "(10), where C represents the set of\nkth emotion for segment s is defined as Ps(Ek) in Eq.": "(10)"
        },
        {
          "(10), where C represents the set of\nkth emotion for segment s is defined as Ps(Ek) in Eq.": "1"
        },
        {
          "(10), where C represents the set of\nkth emotion for segment s is defined as Ps(Ek) in Eq.": "X s\nf k\nPs(Ek)\n6 ="
        },
        {
          "(10), where C represents the set of\nkth emotion for segment s is defined as Ps(Ek) in Eq.": "|C|"
        },
        {
          "(10), where C represents the set of\nkth emotion for segment s is defined as Ps(Ek) in Eq.": "∈C"
        },
        {
          "(10), where C represents the set of\nkth emotion for segment s is defined as Ps(Ek) in Eq.": "|Ps(Ek > θ)|"
        },
        {
          "(10), where C represents the set of\nkth emotion for segment s is defined as Ps(Ek) in Eq.": "f k\n7 ="
        },
        {
          "(10), where C represents the set of\nkth emotion for segment s is defined as Ps(Ek) in Eq.": "|C|"
        },
        {
          "(10), where C represents the set of\nkth emotion for segment s is defined as Ps(Ek) in Eq.": "Features f k\nand f k\ncorrespond to the maximum value of the segment-level prob-\n1 , f k\n3"
        },
        {
          "(10), where C represents the set of\nkth emotion for segment s is defined as Ps(Ek) in Eq.": "ability of\nemotion in the song, minimum, and average values,\nrespectively.\nThe\nthe kth"
        },
        {
          "(10), where C represents the set of\nkth emotion for segment s is defined as Ps(Ek) in Eq.": "features f k\ncorrespond to the kth emotion’s three quartiles in the song respectively.\n3 ∼ f k"
        },
        {
          "(10), where C represents the set of\nkth emotion for segment s is defined as Ps(Ek) in Eq.": "The feature f k\nrepresents the percentage of segments with a high probability of sentiment\n7"
        },
        {
          "(10), where C represents the set of\nkth emotion for segment s is defined as Ps(Ek) in Eq.": "k. The feature exhibits low sensitivity to a threshold, allowing for empirical selection. This"
        },
        {
          "(10), where C represents the set of\nkth emotion for segment s is defined as Ps(Ek) in Eq.": "aggregation step produces a feature representation of dimension 7 × K for each song. With"
        },
        {
          "(10), where C represents the set of\nkth emotion for segment s is defined as Ps(Ek) in Eq.": "this\nset of\nsong-level\nfeature\nrepresentations, we\ncan train a secondary classifier\nthat\nis"
        },
        {
          "(10), where C represents the set of\nkth emotion for segment s is defined as Ps(Ek) in Eq.": "relatively simple in nature to make decisions at the song level."
        },
        {
          "(10), where C represents the set of\nkth emotion for segment s is defined as Ps(Ek) in Eq.": "3\nExperiments"
        },
        {
          "(10), where C represents the set of\nkth emotion for segment s is defined as Ps(Ek) in Eq.": "We tested our proposed algorithm on three public datasets: PMEmo dataset [15], Emotion"
        },
        {
          "(10), where C represents the set of\nkth emotion for segment s is defined as Ps(Ek) in Eq.": "in Music (EiM)\n[16] and 4Q dataset\n[17].\nSince deep learning is data hungry,\nthese data"
        },
        {
          "(10), where C represents the set of\nkth emotion for segment s is defined as Ps(Ek) in Eq.": "volumes are difficult to support models with strong training generalization ability and are"
        },
        {
          "(10), where C represents the set of\nkth emotion for segment s is defined as Ps(Ek) in Eq.": "easy to overfit."
        },
        {
          "(10), where C represents the set of\nkth emotion for segment s is defined as Ps(Ek) in Eq.": "3.1\nExperiment Setup"
        },
        {
          "(10), where C represents the set of\nkth emotion for segment s is defined as Ps(Ek) in Eq.": "We convert each segment unit\ninto a mel\nspectrogram, with the Hanning window length"
        },
        {
          "(10), where C represents the set of\nkth emotion for segment s is defined as Ps(Ek) in Eq.": "of 1,024 and window hop size of 512 using the librosa [18]. We utilize 128 mel bins. Once"
        },
        {
          "(10), where C represents the set of\nkth emotion for segment s is defined as Ps(Ek) in Eq.": "the audio clip is segmented into smaller parts, the amount of training samples is dozens of"
        },
        {
          "(10), where C represents the set of\nkth emotion for segment s is defined as Ps(Ek) in Eq.": "times larger than the original. Our segment classifier uses VGG16 [19], we only modify the"
        },
        {
          "(10), where C represents the set of\nkth emotion for segment s is defined as Ps(Ek) in Eq.": "input channel and output number of classification. The support vector machine (SVM) is"
        },
        {
          "(10), where C represents the set of\nkth emotion for segment s is defined as Ps(Ek) in Eq.": "used for song-level decisions."
        },
        {
          "(10), where C represents the set of\nkth emotion for segment s is defined as Ps(Ek) in Eq.": "3.2\nDatasets"
        },
        {
          "(10), where C represents the set of\nkth emotion for segment s is defined as Ps(Ek) in Eq.": "PMEmo: This dataset contains 794 pieces of music, all of which are pop music in mp3"
        },
        {
          "(10), where C represents the set of\nkth emotion for segment s is defined as Ps(Ek) in Eq.": "format. The audio duration is mainly around 45 seconds. We utilize 767 of these clips with"
        },
        {
          "(10), where C represents the set of\nkth emotion for segment s is defined as Ps(Ek) in Eq.": "static valence/arousal (V/A) annotations in our work."
        },
        {
          "(10), where C represents the set of\nkth emotion for segment s is defined as Ps(Ek) in Eq.": "4Q: There are 900 music clips in this dataset. All music clips are divided into four parts"
        },
        {
          "(10), where C represents the set of\nkth emotion for segment s is defined as Ps(Ek) in Eq.": "according to Russell’s V/A quadrant, with 225 clips\nin each part. Most audio clips are"
        },
        {
          "(10), where C represents the set of\nkth emotion for segment s is defined as Ps(Ek) in Eq.": "approximately 30 seconds."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 1: for the sample number comparison between",
      "data": [
        {
          "EiM: The dataset consists of 1,000 music samples, each 45 seconds\nlong, obtained from": "sources\nlike Jamendo, with copyrighted music. The labels\nfor\nthis dataset were obtained"
        },
        {
          "EiM: The dataset consists of 1,000 music samples, each 45 seconds\nlong, obtained from": "through crowdsourcing platforms. Just like in previous works, we utilized a set of 744 audio"
        },
        {
          "EiM: The dataset consists of 1,000 music samples, each 45 seconds\nlong, obtained from": "samples after removing duplicates."
        },
        {
          "EiM: The dataset consists of 1,000 music samples, each 45 seconds\nlong, obtained from": "3.3\nAudio Pre-processing"
        },
        {
          "EiM: The dataset consists of 1,000 music samples, each 45 seconds\nlong, obtained from": "In the previous works,\nin the methods of taking the entire clip as input [20, 21], 30 seconds"
        },
        {
          "EiM: The dataset consists of 1,000 music samples, each 45 seconds\nlong, obtained from": "of audio are generally reserved, and the part less than 30 seconds is padded with zeros.\nIn"
        },
        {
          "EiM: The dataset consists of 1,000 music samples, each 45 seconds\nlong, obtained from": "[8],\nfor clips less than 30 seconds, they padded them to 30 seconds by repeating themselves"
        },
        {
          "EiM: The dataset consists of 1,000 music samples, each 45 seconds\nlong, obtained from": "continuously from the beginning to the end."
        },
        {
          "EiM: The dataset consists of 1,000 music samples, each 45 seconds\nlong, obtained from": "Since the variance of sample duration in different data sets is large, when the method"
        },
        {
          "EiM: The dataset consists of 1,000 music samples, each 45 seconds\nlong, obtained from": "of filling zeros\nis used, many blank segments will be generated, which is\ninvalid training"
        },
        {
          "EiM: The dataset consists of 1,000 music samples, each 45 seconds\nlong, obtained from": "data. Using the circular padding method will generate too many repeated training samples."
        },
        {
          "EiM: The dataset consists of 1,000 music samples, each 45 seconds\nlong, obtained from": "Therefore, we did not do any padding."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 1: Number of samples after segmentation",
      "data": [
        {
          "Table 1: Number of samples after segmentation": "#Song"
        },
        {
          "Table 1: Number of samples after segmentation": ""
        },
        {
          "Table 1: Number of samples after segmentation": ""
        },
        {
          "Table 1: Number of samples after segmentation": "767"
        },
        {
          "Table 1: Number of samples after segmentation": ""
        },
        {
          "Table 1: Number of samples after segmentation": ""
        },
        {
          "Table 1: Number of samples after segmentation": ""
        },
        {
          "Table 1: Number of samples after segmentation": ""
        },
        {
          "Table 1: Number of samples after segmentation": "900"
        },
        {
          "Table 1: Number of samples after segmentation": ""
        },
        {
          "Table 1: Number of samples after segmentation": ""
        },
        {
          "Table 1: Number of samples after segmentation": ""
        },
        {
          "Table 1: Number of samples after segmentation": ""
        },
        {
          "Table 1: Number of samples after segmentation": "1,000"
        },
        {
          "Table 1: Number of samples after segmentation": ""
        },
        {
          "Table 1: Number of samples after segmentation": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 1: Number of samples after segmentation",
      "data": [
        {
          "4(3)\n26,973": "5(4)\n25,971"
        },
        {
          "4(3)\n26,973": "long segments, which will\nlead to data redundancy and make the model overfit the data."
        },
        {
          "4(3)\n26,973": "Leading to poor generalization ability to new data."
        },
        {
          "4(3)\n26,973": "4.2\nValidation Experiment"
        },
        {
          "4(3)\n26,973": "Figure 2 shows an example of\nemotion predictions\nfor\neach segment of\nthe\ntest\nsample"
        },
        {
          "4(3)\n26,973": "MT0010465830 in the 4Q dataset, which is labeled as happy (Q1). The segment classifier"
        },
        {
          "4(3)\n26,973": "has four outputs corresponding to four different emotional states: Q1, Q2, Q3, and Q4."
        },
        {
          "4(3)\n26,973": "As shown in the figure, the probabilities for each emotion change throughout the entire"
        },
        {
          "4(3)\n26,973": "song. The true emotion of the whole song is Q1, and as seen in the figure, the Q1 probability"
        },
        {
          "4(3)\n26,973": "is highest in most segments, but there are also other emotions dominating some parts of the"
        },
        {
          "4(3)\n26,973": "song. While not all songs exhibit distinctive segment-level outputs, we can employ the song-"
        },
        {
          "4(3)\n26,973": "level classifier to discern them. The high-pitched and cheerful singing of the male singer at"
        },
        {
          "4(3)\n26,973": "the beginning of the song kept the Q1 prediction probability the highest for the first few"
        },
        {
          "4(3)\n26,973": "seconds. Then the vocals quickly changed to a low tone, and Q3 (sadness) took over. At"
        },
        {
          "4(3)\n26,973": "7s, the trumpet and drums come in and shift the emotion back to happiness.\nIn the 20s,"
        },
        {
          "4(3)\n26,973": "although the vocals turned to a low tone again,\nthe background drumbeats and trumpet"
        },
        {
          "4(3)\n26,973": "sounds\ncontinued,\nso the Q1 probability only slightly decreased,\nstill dominant overall."
        },
        {
          "4(3)\n26,973": "The audio content confirms\nthat\nthe emotional expression in music changes dynamically"
        },
        {
          "4(3)\n26,973": "throughout the song, rather than being consistent throughout."
        },
        {
          "4(3)\n26,973": "4.3\nExperimental Results Compared with Other Models"
        },
        {
          "4(3)\n26,973": "In order\nto make a fair\ncomparison, our\nexperimental\nresults are all\nthe average values"
        },
        {
          "4(3)\n26,973": "obtained under the cross-validation of ten fold. Among them, the proposed* is the ablation"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 2: Experimental results with different segment duration",
      "data": [
        {
          "Table 2: Experimental results with different segment duration": ""
        },
        {
          "Table 2: Experimental results with different segment duration": "Seg Dur."
        },
        {
          "Table 2: Experimental results with different segment duration": ""
        },
        {
          "Table 2: Experimental results with different segment duration": "1"
        },
        {
          "Table 2: Experimental results with different segment duration": "2"
        },
        {
          "Table 2: Experimental results with different segment duration": "3"
        },
        {
          "Table 2: Experimental results with different segment duration": "4"
        },
        {
          "Table 2: Experimental results with different segment duration": "5"
        },
        {
          "Table 2: Experimental results with different segment duration": "1"
        },
        {
          "Table 2: Experimental results with different segment duration": "2"
        },
        {
          "Table 2: Experimental results with different segment duration": "3"
        },
        {
          "Table 2: Experimental results with different segment duration": "4"
        },
        {
          "Table 2: Experimental results with different segment duration": "5"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 3: Comparison with other methods",
      "data": [
        {
          "Table 3: Comparison with other methods": ""
        },
        {
          "Table 3: Comparison with other methods": ""
        },
        {
          "Table 3: Comparison with other methods": "V-acc"
        },
        {
          "Table 3: Comparison with other methods": "70.43"
        },
        {
          "Table 3: Comparison with other methods": "79.01"
        },
        {
          "Table 3: Comparison with other methods": ""
        },
        {
          "Table 3: Comparison with other methods": "79.01"
        },
        {
          "Table 3: Comparison with other methods": "83.19"
        },
        {
          "Table 3: Comparison with other methods": "67.11"
        },
        {
          "Table 3: Comparison with other methods": "–"
        },
        {
          "Table 3: Comparison with other methods": "–"
        },
        {
          "Table 3: Comparison with other methods": "76.32"
        },
        {
          "Table 3: Comparison with other methods": "77.20"
        },
        {
          "Table 3: Comparison with other methods": "–"
        },
        {
          "Table 3: Comparison with other methods": "–"
        },
        {
          "Table 3: Comparison with other methods": "–"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Conference on Sound and Music Technology: Revised Selected Papers\nfrom CMST.": "Springer, 2022, pp. 79–90."
        },
        {
          "Conference on Sound and Music Technology: Revised Selected Papers\nfrom CMST.": "[4] Emmanuel Bigand, Sandrine Vieillard, Fran¸cois Madurell, Jeremy Marozeau, and Alice"
        },
        {
          "Conference on Sound and Music Technology: Revised Selected Papers\nfrom CMST.": "Dacquet,\n“Multidimensional\nscaling of emotional\nresponses\nto music: The effect of"
        },
        {
          "Conference on Sound and Music Technology: Revised Selected Papers\nfrom CMST.": "musical expertise and of the duration of the excerpts,” Cognition & Emotion, vol. 19,"
        },
        {
          "Conference on Sound and Music Technology: Revised Selected Papers\nfrom CMST.": "no. 8, pp. 1113–1139, 2005."
        },
        {
          "Conference on Sound and Music Technology: Revised Selected Papers\nfrom CMST.": "[5] Kun Han, Dong Yu, and Ivan Tashev, “Speech emotion recognition using deep neural"
        },
        {
          "Conference on Sound and Music Technology: Revised Selected Papers\nfrom CMST.": "network and extreme learning machine,” in Interspeech 2014, 2014."
        },
        {
          "Conference on Sound and Music Technology: Revised Selected Papers\nfrom CMST.": "[6] Shuiyang Mao, PC Ching, and Tan Lee,\n“Deep learning of segment-level\nfeature rep-"
        },
        {
          "Conference on Sound and Music Technology: Revised Selected Papers\nfrom CMST.": "resentation with multiple instance learning for utterance-level speech emotion recogni-"
        },
        {
          "Conference on Sound and Music Technology: Revised Selected Papers\nfrom CMST.": "tion.,” in Interspeech, 2019, pp. 1686–1690."
        },
        {
          "Conference on Sound and Music Technology: Revised Selected Papers\nfrom CMST.": "[7] Rajib Sarkar, Sombuddha Choudhury, Saikat Dutta, Aneek Roy, and Sanjoy Kumar"
        },
        {
          "Conference on Sound and Music Technology: Revised Selected Papers\nfrom CMST.": "Saha, “Recognition of emotion in music based on deep convolutional neural network,”"
        },
        {
          "Conference on Sound and Music Technology: Revised Selected Papers\nfrom CMST.": "Multimedia Tools and Applications, vol. 79, no. 1, pp. 765–783, 2020."
        },
        {
          "Conference on Sound and Music Technology: Revised Selected Papers\nfrom CMST.": "[8] Na He and Sam Ferguson,\n“Music emotion recognition based on segment-level\ntwo-"
        },
        {
          "Conference on Sound and Music Technology: Revised Selected Papers\nfrom CMST.": "stage learning,” International Journal of Multimedia Information Retrieval, pp. 1–12,"
        },
        {
          "Conference on Sound and Music Technology: Revised Selected Papers\nfrom CMST.": "2022."
        },
        {
          "Conference on Sound and Music Technology: Revised Selected Papers\nfrom CMST.": "[9] Shuiyang Mao, P. C. Ching, and Tan Lee, “Enhancing segment-based speech emotion"
        },
        {
          "Conference on Sound and Music Technology: Revised Selected Papers\nfrom CMST.": "recognition by iterative self-learning,” IEEE ACM Trans. Audio Speech Lang. Process.,"
        },
        {
          "Conference on Sound and Music Technology: Revised Selected Papers\nfrom CMST.": "vol. 30, pp. 123–134, 2022."
        },
        {
          "Conference on Sound and Music Technology: Revised Selected Papers\nfrom CMST.": "[10] Daiki Tanaka, Daiki\nIkami, Toshihiko Yamasaki, and Kiyoharu Aizawa,\n“Joint op-"
        },
        {
          "Conference on Sound and Music Technology: Revised Selected Papers\nfrom CMST.": "timization framework for\nlearning with noisy labels,”\nin Proceedings of\nthe\nIEEE"
        },
        {
          "Conference on Sound and Music Technology: Revised Selected Papers\nfrom CMST.": "conference on computer vision and pattern recognition, 2018, pp. 5552–5560."
        },
        {
          "Conference on Sound and Music Technology: Revised Selected Papers\nfrom CMST.": "[11] Hongyi Zhang, Moustapha Ciss´e, Yann N. Dauphin, and David Lopez-Paz,\n“mixup:"
        },
        {
          "Conference on Sound and Music Technology: Revised Selected Papers\nfrom CMST.": "Beyond empirical\nrisk minimization,”\nin 6th International Conference on Learning"
        },
        {
          "Conference on Sound and Music Technology: Revised Selected Papers\nfrom CMST.": "Representations, 2018."
        },
        {
          "Conference on Sound and Music Technology: Revised Selected Papers\nfrom CMST.": "[12] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals, “Un-"
        },
        {
          "Conference on Sound and Music Technology: Revised Selected Papers\nfrom CMST.": "derstanding deep learning (still) requires rethinking generalization,” Communications"
        },
        {
          "Conference on Sound and Music Technology: Revised Selected Papers\nfrom CMST.": "of\nthe ACM, vol. 64, no. 3, pp. 107–115, 2021."
        },
        {
          "Conference on Sound and Music Technology: Revised Selected Papers\nfrom CMST.": "[13] Eric Arazo, Diego Ortego, Paul Albert, Noel O’Connor,\nand Kevin McGuinness,"
        },
        {
          "Conference on Sound and Music Technology: Revised Selected Papers\nfrom CMST.": "“Unsupervised label noise modeling and loss correction,”\nin International conference"
        },
        {
          "Conference on Sound and Music Technology: Revised Selected Papers\nfrom CMST.": "on machine learning. PMLR, 2019, pp. 312–321."
        },
        {
          "Conference on Sound and Music Technology: Revised Selected Papers\nfrom CMST.": "[14] Lijun Wu, Juntao Li, Yue Wang, Qi Meng, Tao Qin, Wei Chen, Min Zhang, Tie-Yan"
        },
        {
          "Conference on Sound and Music Technology: Revised Selected Papers\nfrom CMST.": "Liu, et al.,\n“R-drop: Regularized dropout\nfor neural networks,” Advances in Neural"
        },
        {
          "Conference on Sound and Music Technology: Revised Selected Papers\nfrom CMST.": "Information Processing Systems, vol. 34, pp. 10890–10905, 2021."
        },
        {
          "Conference on Sound and Music Technology: Revised Selected Papers\nfrom CMST.": "[15] Kejun Zhang, Hui Zhang, Simeng Li, Changyuan Yang,\nand Lingyun Sun,\n“The"
        },
        {
          "Conference on Sound and Music Technology: Revised Selected Papers\nfrom CMST.": "pmemo dataset\nfor music emotion recognition,”\nin Proceedings of\nthe 2018 acm on"
        },
        {
          "Conference on Sound and Music Technology: Revised Selected Papers\nfrom CMST.": "international conference on multimedia retrieval, 2018, pp. 135–142."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[16] Mohammad Soleymani, Micheal N Caro, Erik M Schmidt, Cheng-Ya Sha,\nand Yi-": "Hsuan Yang,\n“1000 songs for emotional analysis of music,”\nin Proceedings of\nthe 2nd"
        },
        {
          "[16] Mohammad Soleymani, Micheal N Caro, Erik M Schmidt, Cheng-Ya Sha,\nand Yi-": "ACM international workshop on Crowdsourcing for multimedia, 2013, pp. 1–6."
        },
        {
          "[16] Mohammad Soleymani, Micheal N Caro, Erik M Schmidt, Cheng-Ya Sha,\nand Yi-": "[17] Renato Panda, Ricardo Malheiro, and Rui Pedro Paiva, “Novel audio features for music"
        },
        {
          "[16] Mohammad Soleymani, Micheal N Caro, Erik M Schmidt, Cheng-Ya Sha,\nand Yi-": "emotion recognition,”\nIEEE Transactions on Affective Computing, vol. 11, no. 4, pp."
        },
        {
          "[16] Mohammad Soleymani, Micheal N Caro, Erik M Schmidt, Cheng-Ya Sha,\nand Yi-": "614–626, 2018."
        },
        {
          "[16] Mohammad Soleymani, Micheal N Caro, Erik M Schmidt, Cheng-Ya Sha,\nand Yi-": "[18] Brian McFee, Colin Raffel, Dawen Liang, Daniel P Ellis, Matt McVicar, Eric Bat-"
        },
        {
          "[16] Mohammad Soleymani, Micheal N Caro, Erik M Schmidt, Cheng-Ya Sha,\nand Yi-": "tenberg, and Oriol Nieto,\n“librosa: Audio and music signal analysis\nin python,”\nin"
        },
        {
          "[16] Mohammad Soleymani, Micheal N Caro, Erik M Schmidt, Cheng-Ya Sha,\nand Yi-": "Proceedings of\nthe 14th python in science conference, 2015, vol. 8, pp. 18–25."
        },
        {
          "[16] Mohammad Soleymani, Micheal N Caro, Erik M Schmidt, Cheng-Ya Sha,\nand Yi-": "[19] Karen Simonyan and Andrew Zisserman, “Very deep convolutional networks for large-"
        },
        {
          "[16] Mohammad Soleymani, Micheal N Caro, Erik M Schmidt, Cheng-Ya Sha,\nand Yi-": "scale image recognition,” in 3rd International Conference on Learning Representations,"
        },
        {
          "[16] Mohammad Soleymani, Micheal N Caro, Erik M Schmidt, Cheng-Ya Sha,\nand Yi-": "ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015."
        },
        {
          "[16] Mohammad Soleymani, Micheal N Caro, Erik M Schmidt, Cheng-Ya Sha,\nand Yi-": "[20] R´emi Delbouys, Romain Hennequin, Francesco Piccoli,\nJimena Royo-Letelier,\nand"
        },
        {
          "[16] Mohammad Soleymani, Micheal N Caro, Erik M Schmidt, Cheng-Ya Sha,\nand Yi-": "Manuel Moussallam,\n“Music mood detection based on audio and lyrics with deep"
        },
        {
          "[16] Mohammad Soleymani, Micheal N Caro, Erik M Schmidt, Cheng-Ya Sha,\nand Yi-": "neural net,”\nin Proceedings of\nthe 19th International Society for Music Information"
        },
        {
          "[16] Mohammad Soleymani, Micheal N Caro, Erik M Schmidt, Cheng-Ya Sha,\nand Yi-": "Retrieval Conference, ISMIR, 2018, pp. 370–375."
        },
        {
          "[16] Mohammad Soleymani, Micheal N Caro, Erik M Schmidt, Cheng-Ya Sha,\nand Yi-": "[21] Jacopo de Berardinis, Angelo Cangelosi, and Eduardo Coutinho, “The multiple voices"
        },
        {
          "[16] Mohammad Soleymani, Micheal N Caro, Erik M Schmidt, Cheng-Ya Sha,\nand Yi-": "of musical emotions: Source separation for improving music emotion recognition models"
        },
        {
          "[16] Mohammad Soleymani, Micheal N Caro, Erik M Schmidt, Cheng-Ya Sha,\nand Yi-": "and their interpretability,”\nin Proceedings of\nthe 21st\ninternational society for music"
        },
        {
          "[16] Mohammad Soleymani, Micheal N Caro, Erik M Schmidt, Cheng-Ya Sha,\nand Yi-": "information retrieval conference, 2020, pp. 310–317."
        },
        {
          "[16] Mohammad Soleymani, Micheal N Caro, Erik M Schmidt, Cheng-Ya Sha,\nand Yi-": "[22] Zhongzhe Xiao, Emmanuel Dellandrea, Weibei Dou, and Liming Chen,\n“What is the"
        },
        {
          "[16] Mohammad Soleymani, Micheal N Caro, Erik M Schmidt, Cheng-Ya Sha,\nand Yi-": "best segment duration for music mood analysis?,” in 2008 International Workshop on"
        },
        {
          "[16] Mohammad Soleymani, Micheal N Caro, Erik M Schmidt, Cheng-Ya Sha,\nand Yi-": "Content-Based Multimedia Indexing. IEEE, 2008, pp. 17–24."
        },
        {
          "[16] Mohammad Soleymani, Micheal N Caro, Erik M Schmidt, Cheng-Ya Sha,\nand Yi-": "[23] Guanghao Yin, Shouqian Sun, Hui Zhang, Dian Yu, Chao Li, Kejun Zhang, and Ning"
        },
        {
          "[16] Mohammad Soleymani, Micheal N Caro, Erik M Schmidt, Cheng-Ya Sha,\nand Yi-": "Zou,\n“User independent emotion recognition with residual signal-image network,”\nin"
        },
        {
          "[16] Mohammad Soleymani, Micheal N Caro, Erik M Schmidt, Cheng-Ya Sha,\nand Yi-": "2019 IEEE International Conference on Image Processing\n(ICIP).\nIEEE, 2019, pp."
        },
        {
          "[16] Mohammad Soleymani, Micheal N Caro, Erik M Schmidt, Cheng-Ya Sha,\nand Yi-": "3277–3281."
        },
        {
          "[16] Mohammad Soleymani, Micheal N Caro, Erik M Schmidt, Cheng-Ya Sha,\nand Yi-": "[24] Eunjeong Stella Koh and Shlomo Dubnov,\n“Comparison and analysis of deep audio"
        },
        {
          "[16] Mohammad Soleymani, Micheal N Caro, Erik M Schmidt, Cheng-Ya Sha,\nand Yi-": "embeddings\nfor music emotion recognition,”\nin Proceedings of\nthe 4th Workshop on"
        },
        {
          "[16] Mohammad Soleymani, Micheal N Caro, Erik M Schmidt, Cheng-Ya Sha,\nand Yi-": "Affective Content Analysis (AffCon) co-located with Thirty-Fifth AAAI Conference on"
        },
        {
          "[16] Mohammad Soleymani, Micheal N Caro, Erik M Schmidt, Cheng-Ya Sha,\nand Yi-": "Artificial Intelligence (AAAI), 2021, vol. 2897, pp. 15–22."
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Music emotion recognition: Toward new, robust standards in personalized and context-sensitive applications",
      "authors": [
        "Juan Sebastián Gómez-Cañón",
        "Estefanía Cano",
        "Tuomas Eerola",
        "Perfecto Herrera",
        "Xiao Hu",
        "Yi-Hsuan Yang",
        "Emilia Gómez"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "2",
      "title": "Singer identification using deep timbre feature learning with knn-net",
      "authors": [
        "Xulong Zhang",
        "Jiale Qian",
        "Yi Yu",
        "Yifu Sun",
        "Wei Li"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "3",
      "title": "Investigation of singing voice separation for singing voice detection in polyphonic music",
      "authors": [
        "Yifu Sun",
        "Xulong Zhang",
        "Xi Chen",
        "Yi Yu",
        "Wei Li"
      ],
      "year": "2022",
      "venue": "Proceedings of the 9th Conference on Sound and Music Technology: Revised Selected Papers from CMST"
    },
    {
      "citation_id": "4",
      "title": "Multidimensional scaling of emotional responses to music: The effect of musical expertise and of the duration of the excerpts",
      "authors": [
        "Emmanuel Bigand",
        "Sandrine Vieillard",
        "François Madurell",
        "Jeremy Marozeau",
        "Alice Dacquet"
      ],
      "year": "2005",
      "venue": "Cognition & Emotion"
    },
    {
      "citation_id": "5",
      "title": "Speech emotion recognition using deep neural network and extreme learning machine",
      "authors": [
        "Kun Han",
        "Dong Yu",
        "Ivan Tashev"
      ],
      "year": "2014",
      "venue": "Speech emotion recognition using deep neural network and extreme learning machine"
    },
    {
      "citation_id": "6",
      "title": "Deep learning of segment-level feature representation with multiple instance learning for utterance-level speech emotion recognition",
      "authors": [
        "Shuiyang Mao",
        "Tan Ching",
        "Lee"
      ],
      "year": "2019",
      "venue": "Deep learning of segment-level feature representation with multiple instance learning for utterance-level speech emotion recognition"
    },
    {
      "citation_id": "7",
      "title": "Recognition of emotion in music based on deep convolutional neural network",
      "authors": [
        "Rajib Sarkar",
        "Sombuddha Choudhury",
        "Saikat Dutta",
        "Aneek Roy",
        "Sanjoy Saha"
      ],
      "year": "2020",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "8",
      "title": "Music emotion recognition based on segment-level twostage learning",
      "authors": [
        "Na He",
        "Sam Ferguson"
      ],
      "year": "2022",
      "venue": "International Journal of Multimedia Information Retrieval"
    },
    {
      "citation_id": "9",
      "title": "Enhancing segment-based speech emotion recognition by iterative self-learning",
      "authors": [
        "Shuiyang Mao",
        "P Ching",
        "Tan Lee"
      ],
      "year": "2022",
      "venue": "IEEE ACM Trans. Audio Speech Lang. Process"
    },
    {
      "citation_id": "10",
      "title": "Joint optimization framework for learning with noisy labels",
      "authors": [
        "Daiki Tanaka",
        "Daiki Ikami",
        "Toshihiko Yamasaki",
        "Kiyoharu Aizawa"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "11",
      "title": "mixup: Beyond empirical risk minimization",
      "authors": [
        "Hongyi Zhang",
        "Moustapha Cissé",
        "Yann Dauphin",
        "David Lopez-Paz"
      ],
      "year": "2018",
      "venue": "6th International Conference on Learning Representations"
    },
    {
      "citation_id": "12",
      "title": "Understanding deep learning (still) requires rethinking generalization",
      "authors": [
        "Chiyuan Zhang",
        "Samy Bengio",
        "Moritz Hardt",
        "Benjamin Recht",
        "Oriol Vinyals"
      ],
      "year": "2021",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "13",
      "title": "Unsupervised label noise modeling and loss correction",
      "authors": [
        "Eric Arazo",
        "Diego Ortego",
        "Paul Albert",
        "O' Noel",
        "Kevin Connor",
        "Mcguinness"
      ],
      "year": "2019",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "14",
      "title": "R-drop: Regularized dropout for neural networks",
      "authors": [
        "Lijun Wu",
        "Juntao Li",
        "Yue Wang",
        "Qi Meng",
        "Tao Qin",
        "Wei Chen",
        "Min Zhang",
        "Tie-Yan Liu"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "15",
      "title": "The pmemo dataset for music emotion recognition",
      "authors": [
        "Kejun Zhang",
        "Hui Zhang",
        "Simeng Li",
        "Changyuan Yang",
        "Lingyun Sun"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 acm on international conference on multimedia retrieval"
    },
    {
      "citation_id": "16",
      "title": "1000 songs for emotional analysis of music",
      "authors": [
        "Mohammad Soleymani",
        "Micheal Caro",
        "Erik Schmidt",
        "Cheng-Ya Sha",
        "Yi-Hsuan Yang"
      ],
      "year": "2013",
      "venue": "Proceedings of the 2nd ACM international workshop on Crowdsourcing for multimedia"
    },
    {
      "citation_id": "17",
      "title": "Novel audio features for music emotion recognition",
      "authors": [
        "Renato Panda",
        "Ricardo Malheiro",
        "Rui Pedro"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "18",
      "title": "Eric Battenberg, and Oriol Nieto, \"librosa: Audio and music signal analysis in python",
      "authors": [
        "Brian Mcfee",
        "Colin Raffel",
        "Dawen Liang",
        "P Daniel",
        "Matt Ellis",
        "Mcvicar"
      ],
      "year": "2015",
      "venue": "Proceedings of the 14th python in science conference"
    },
    {
      "citation_id": "19",
      "title": "Very deep convolutional networks for largescale image recognition",
      "authors": [
        "Karen Simonyan",
        "Andrew Zisserman"
      ],
      "year": "2015",
      "venue": "3rd International Conference on Learning Representations, ICLR 2015"
    },
    {
      "citation_id": "20",
      "title": "Music mood detection based on audio and lyrics with deep neural net",
      "authors": [
        "Rémi Delbouys",
        "Romain Hennequin",
        "Francesco Piccoli",
        "Jimena Royo-Letelier",
        "Manuel Moussallam"
      ],
      "year": "2018",
      "venue": "Proceedings of the 19th International Society for Music Information Retrieval Conference"
    },
    {
      "citation_id": "21",
      "title": "The multiple voices of musical emotions: Source separation for improving music emotion recognition models and their interpretability",
      "authors": [
        "Angelo Jacopo De Berardinis",
        "Eduardo Cangelosi",
        "Coutinho"
      ],
      "year": "2020",
      "venue": "Proceedings of the 21st international society for music information retrieval conference"
    },
    {
      "citation_id": "22",
      "title": "What is the best segment duration for music mood analysis?",
      "authors": [
        "Zhongzhe Xiao",
        "Emmanuel Dellandrea",
        "Weibei Dou",
        "Liming Chen"
      ],
      "year": "2008",
      "venue": "2008 International Workshop on Content-Based Multimedia Indexing"
    },
    {
      "citation_id": "23",
      "title": "User independent emotion recognition with residual signal-image network",
      "authors": [
        "Guanghao Yin",
        "Shouqian Sun",
        "Hui Zhang",
        "Dian Yu",
        "Chao Li",
        "Kejun Zhang",
        "Ning Zou"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "24",
      "title": "Comparison and analysis of deep audio embeddings for music emotion recognition",
      "authors": [
        "Stella Eunjeong",
        "Shlomo Koh",
        "Dubnov"
      ],
      "year": "2021",
      "venue": "Proceedings of the 4th Workshop on Affective Content Analysis (AffCon) co-located with Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI)"
    }
  ]
}