{
  "paper_id": "2307.03068v1",
  "title": "A Hybrid End-To-End Spatio-Temporal Attention Neural Network With Graph-Smooth Signals For Eeg Emotion Recognition",
  "published": "2023-07-06T15:35:14Z",
  "authors": [
    "Shadi Sartipi",
    "Mastaneh Torkamani-Azar",
    "Mujdat Cetin"
  ],
  "keywords": [
    "Emotion",
    "Electroencephalography",
    "Graph Filtering",
    "Recurrent Attention Network",
    "Spatio-Temporal Encoding",
    "Transfer Learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recently, physiological data such as electroencephalography (EEG) signals have attracted significant attention in affective computing. In this context, the main goal is to design an automated model that can assess emotional states. Lately, deep neural networks have shown promising performance in emotion recognition tasks. However, designing a deep architecture that can extract practical information from raw data is still a challenge. Here, we introduce a deep neural network that acquires interpretable physiological representations by a hybrid structure of spatio-temporal encoding and recurrent attention network blocks. Furthermore, a preprocessing step is applied to the raw data using graph signal processing tools to perform graph smoothing in the spatial domain. We demonstrate that our proposed architecture exceeds state-of-the-art results for emotion classification on the publicly available DEAP dataset. To explore the generality of the learned model, we also evaluate the performance of our architecture towards transfer learning (TL) by transferring the model parameters from a specific source to other target domains. Using DEAP as the source dataset, we demonstrate the effectiveness of our model in performing crossmodality TL and improving emotion classification accuracy on DREAMER and the Emotional English Word (EEWD) datasets, which involve EEG-based emotion classification tasks with different stimuli.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "A FFECTIVE computing is a popular field of study wherein researchers try to develop automatic recognition systems or devices that can interpret or respond to human emotional states. Brain-Computer Interfaces (BCI) link brain activity with external devices  [1] . Recently, emotion recognition using physiological signals attracted a notable amount of attention  [2] . Physiological signals acquired with wearable devices include electroencephalogram (EEG), electrocardiogram (ECG), electromyogram (EMG), blood pressure, galvanic skin response (GSR), eye-tracking metrics such as pupil dilation and gaze entropy, body temperature, and movement kinematics, to name a few. The neural activity of cortical regions can be recorded by multichannel EEG in a way that can preserve the spectral and rhythmic characteristics of brain signals  [3] .\n\nComparing EEG with other non-invasive recording methods shows that EEG has better temporal resolution and can acquire brain signals per millisecond. High temporal resolution and ease of use made EEG one of the most practical ways of handling tasks related to cognitive and affective reactions  [3] . However, EEG suffers from low signal-to-noise ratio (SNR) and poor spatial resolution. Accordingly, it is challenging to use EEG signals for downstream tasks when compared to MRI and fNIRS  [4] ,  [5] .\n\nEEG signals are primarily analyzed in particular frequency bands, including theta (θ : 4-8 Hz), alpha (α : 8-12 Hz), beta (β : 12-29 Hz), and gamma (γ : > 30 Hz). Most of the early pieces of work on EEG-based emotion classification have generally relied on two main steps: extracting the informative features and defining a supervised machine learning approach. Wang et al. evaluated the performance of three different features, namely power spectral density (PSD), wavelet entropy, and nonlinear dynamical features with kernel support vector machine (SVM)  [6] . Zheng et al.  [7]  investigated the critical frequency bands and channels with differential entropy (DE), DE asymmetry, and PSD features. They explored the performance of different features by Knearest neighbors (K-NN), SVM, and deep belief networks (DBN). In  [8] , the authors offered an approach to calculate spectral and temporal entropies by decomposing EEG data via Fourier-Bessel series expansion-based empirical wavelet transform. Then, K-NN and Shannon entropies were computed after multi-scaling operation in the spectral and temporal domains.  [9]  proposed a new rhythm sequencing approach to find the best rhythmic features from the sequence of multichannel EEG data. Finally,  [10]  applied transfer learning to address the issue of distribution shift between the training and test data.\n\nNevertheless, covering all the manually extracted features both in time and frequency domains is complicated. Besides, the susceptibility of EEG signals to artifacts severely degrades the performance of classical machine learning approaches  [11] . To address this issue, one can exploit end-to-end models that start with raw signals rather than extracted features. End-toend deep learning (DL) approaches have been indicated to surpass classic approaches in various fields, including speech recognition  [12] , computer vision  [13] , and biomedical signal processing  [14] . In contrast to shallow classifiers that re-quire feature engineering, deep neural networks automatically extract practical features from the given signals and learn the low-and high-level representations of the input data  [15] . Several deep learning architectures and methodologies have been proposed for EEG-based BCIs; see e.g.,  [16] -  [19] . Schirrmeister et al.  [20]  designed a deep convolutional neural network (CNN) based architecture with temporal and spatial convolutional (conv) filters followed by conv-pooling blocks to reduce the dimension. Zhao et al. investigated the fusion of three different modalities, namely EEG, raw eyemovement-images (EIG), and eye movement features (EYE)  [21] . Feeding the fusion of different modalities to a dense coattention symmetric network resulted in higher classification performance than the single modality. In  [17] , authors applied an attention technique to set different weight importance to EEG channels. Then, they extracted spatio-temporal features by applying CNN and a recurrent neural network (RNN). Multi-column convolutional neural network (MCNN) was introduced by authors of  [22]  for emotional states classification. They tested their work in a subject-independent manner by assuming five participants as the test data without performing cross-validation. A novel architecture coined as frame-level distilling neural network (FLDNet), that learns the distilled properties from the correlation of various frames, was introduced in  [23] . They presented a triple-net structure to distill the learned features of each net consecutively. The deep forest was proposed in  [24] , where EEG data were converted to twodimensional (2D) frame sequences by considering the spatial position of the EEG channels and then fed to the model. The proposed model was insensitive to hyper-parameter settings. EEG-based emotion recognition via DL is still in its early years. Therefore, there is yet to find a better deep structure. Furthermore, although DL models have been successful in EEG analysis, there are few studies that not only explore the performance of the learned representation but also consider their generalization ability on another dataset with a similar task  [25] .\n\nThe main goal of this study is to design a new deep architecture to enhance the performance of the current algorithms for EEG-based emotion recognition. It has been established in different areas that CNNs are effective in capturing the spatial representations, while RNNs capture the temporal dependencies well  [26] . Recording EEG data from multiple electrodes over the scalp during a period of time forms both spatial and temporal structures. In order to analyze these structured time series successfully, the extracted information from spatial structures and temporal dynamics should be accounted for  [27] . We propose the hybrid end-toend spatio-temporal attention neural network (STANN) with smooth signals over graphs to consider these two aspects of the data within a unified architecture. As the central contribution of this work, STANN consists of two parallel blocks: the spatio-temporal encoding block and the recurrent attention network block. Considering the complex structure of brain signals and their time-varying character, we propose the idea of applying the graph Fourier transform (GFT) and low pass graph filtering  [28]  in a pre-processing step. GFT is considered as a solution for overcoming the low SNR of EEG signals  [29] . Accordingly, our proposed smooth signal spatio-temporal attention neural network (SS-STANN) simultaneously learns both spatial information and discriminative time dependencies. To evaluate the performance, we apply the introduced method on the publicly available EEG dataset named DEAP that contains discrete ratings for valence, arousal, and dominance in response to audio-visual stimuli  [30] . In our comprehensive experimental analysis, we demonstrate the superiority of STANN when using either raw EEG signals or smoothed graph signals as an input.\n\nDeep learning models commonly require a large number of parameters to be trained compared to traditional machine learning methods. Thus, DL models need a significant amount of data  [31] . One of the challenges related to EEG tasks and DL is insufficient labeled data for similar tasks. A number of transfer-based approaches have been proposed to address this issue that leverage a pre-existing large enough dataset known as the source dataset. Yet, there would be inconsistency across target and source domains which necessitates fine-tuning  [32]  the network for the target data. We demonstrate the applicability of the information learned through our proposed hybrid architecture in similar emotion classification tasks by applying transfer learning and fine-tuning in order to investigate the generality of our proposed model. We consider 5 layouts for tuning the pre-existing network with confined EEG data. As the target data, we investigate the EEG signals of publicly available DREAMER dataset  [33]  and an Emotional English Word dataset (EEWD) recorded at Sabanci University with different stimuli types that can enlighten the capability of proposed model in cross-modal emotion learning  [34] . The major contributions of our work are summarized as follows:\n\n• A novel deep architecture that considers spatial and temporal information of time-series data is proposed for EEG emotion classification. The proposed hybrid network encodes the spatio-temporal and attentive temporal information in parallel. A preliminary version of this work was presented in  [27] . While  [27]  contained the initial idea of the STANN framework, this paper extends that preliminary work in several major ways: (1) the approach we present here involves the use of graph signal processing (GSP) tools to perform graph smoothness in the spatial domain,  (2)  we propose and demonstrate the use of transfer learning within our proposed framework, (3) by visualizing activations of certain layers in our network,  we examine how our proposed approach encodes spatial information in the brain, (4) we perform a more extensive comparison of our proposed method to the state-of-the-art, and (5) we extend our experimental analysis to three datasets to demonstrate the effectiveness of the proposed method.\n\nThe rest of this paper is structured as follows. Section II introduces the proposed SS-STANN method. Section III describes the datasets and reports the experimental results. Section V concludes the paper.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Method",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Overview",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "B. Preprocessing Using Graph Filtering",
      "text": "EEG data are recorded during a total time period T from n different electrodes mounted over the scalp, which results in a two-dimensional (2D) signal, X ∈ R n×T . Recent works commonly apply hand-crafted features or raw EEG data as the input of deep neural networks. Exploring structural and functional connectivity of the brain  [35]  and tracking the relative spatial positions of EEG nodes could be in use of decoding responses elicited from sensory stimuli  [36] . In order to exploit GSP tools, we need to define an underlying graph. Thus, we calculate the pairwise Euclidean distances of EEG electrodes and build the graph accordingly. In this way, we solely require the Cartesian coordinates of electrodes while the classical common spatial pattern (CSP) filtering depends on individual subjects or tasks. Let us consider an undirected, weighted graph G(V, E, A), where V = {1, 2, ..., n} is the set of nodes or EEG channels, E ⊆ V × V is the set of edges or spatial connections, and A ∈ R n×n is the adjacency matrix. The edge weights A ij are inversely proportional to the pairwise Euclidean distances between node i and j. Subsequently, one can compute the distance d ij as follows:\n\nFor each electrode, K-NN is considered to construct the adjacency matrix while keeping it symmetric to represent the brain topology  [37] . To avoid a densely-connected graph, we set K to 2 and 4. In the literature, a 2-NN topology was motivated by separating the graph into fronto-temporal and parietooccipital networks  [38] . The 4-NN graph brings engagement with central areas as well. Figure  2  shows the final adjacency matrices for these K values and their corresponding scalp topologies for 32 nodes with the 10-20 electrode placement setup.\n\nFurthermore, one can extract informative features using the spectral representation of spatial signals. Using GFT one can analyze the spatial frequency of the signals defined over the graph. The combinatorial graph Laplacian L = D -A is needed for the calculation of GFT in which\n\nis a diagonal matrix of nodal degrees  [28] . Given graph signal X one can compute GFT with respect to L as follows:\n\nwhere V is the orthonormal n × n matrix of eigenvectors of the matrix L.\n\nSince the electrodes installed in adjacent locations detect electrical activities of common sources  [29] , we apply smoothing via lowpass graph filtering with respect to the defined graph. This will ensure the similarity of the behavior among neighbor electrodes. Low frequencies in the graph correspond to small eigenvalues. Considering h = [ h1 , h2 , ..., hn ] the ideal lowpass filter with bandwidth w ∈ {1, 2, ..., n} where hi = 0 if i > w, the GFT coefficients corresponding to the low frequencies with respect to G are given by:\n\nwhere hi is equal to one for w = [1, n 2 ] and zero otherwise. While we choose the filter bandwidth as n 2 , users can choose a different number depending on the level of smoothing they want to apply. A smaller bandwidth would lead to more smoothing.\n\nNext, the inverse GFT (iGFT), X smooth = V Xlow , is applied to obtain the smoothed data in the spatial domain  [28] .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. The Proposed Hybrid Ss-Stann Network",
      "text": "After the preprocessing and smoothing steps, a sliding window with length k is applied to obtain the EEG slices i, i.e., Z i ∈ R n×k . Based on the overview presented in Figure  1 , the proposed network is organized into two main blocks: the spatio-temporal encoding (STE) and the recurrent attention network (RAN) blocks.\n\nThe STE is designed to extract the spatio-temporal information from temporal slices, Z i . This block consists multiple columns of 2D CNNs. The STE follows the structure proposed in  [39] ,  [40] . This architecture contains several independently acting columns that function as a deep network. All columns receive the same input data and their weights are initialized randomly. The output of this model is equal to the average of all the columns' outputs. In this work, each column contains a series of 2D CNN, batch normalization, and average-pooling followed by dropout layers with different kernel sizes and a number of filters. Presuming STE has M columns, the feature map of the m th column would be f m . M feature maps,{f m } m=M m=1 , are merged to get the final feature map f . The final feature map passes through the 1 × 1 conv layer. This structure enables using different kernel sizes per column to detect the informative features at very different temporal and spatial scales across nearby EEG channels. Details of the implementation will be described in Section III.\n\nThe RAN block is composed of two bidirectional LSTM layers and an attention mechanism. LSTM networks are recurrent neural networks that capture the dependencies within time steps from sequential data. In LSTMs, the temporal dynamic behavior is captured by feedback connections  [41] . Since RNNs are trained by back-propagation through time, a RNN cell is replaced by a LSTM cell to avoid the vanishing gradient problem  [41] .\n\nLet x t and h t denote the input data and the hidden state at time t, respectively. LSTM performance is controlled by three gates: (1) the forget gate f t selects the information to keep or forget, (2) the input gate i t controls the flow of the input, and (3) the output gate o t that calculates the output of the given updated cell (Figure  3 ). Formulas governing the operations in the LSTM cell are as follows  [41] :\n\nwhere W (.) , and b (.) are weights and biases, respectively. C t is the cell state at time t. The operator • denotes the elementwise vector multiplication. A bidirectional LSTM (BiLSTM) consists of two LSTM blocks that allow the layer to receive information from the sequential input data simultaneously in forward and backward directions. The output of each layer is the concatenation of outputs of two LSTM blocks, i.e,\n\nwhere -→ h f and ←h b correspond to the forward and backward hidden states, respectively  [41] .\n\nIn several sequence-based applications such as semantics analysis, natural language processing, and medical imaging, certain time steps of the input data might contain the most discriminative information, and attention mechanism addresses this issue by focusing on specific time steps  [42] . In this mechanism, the most discriminative task-related features are calculated by multiplication of outputs of hidden states by trainable weights. The output of the attention layer, v, is computed as bellow:\n\nwhere h i denotes the output of the i th LSTM layer, and W and b are trainable parameters.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "D. Training, Optimization, And Transfer Learning",
      "text": "In order to examine the performance of the proposed network, the features of STE and RAN block are fused and fed to the dense layer. Next, the encoded representation is fed to the final softmax classifier. The cross-entropy loss, L, is calculated as follows:\n\nwhere Y i is the ground truth emotion label for each data sample and Ŷi is the predicted label. Finally, the weights and the biases are trained with batch gradient descent.\n\nThe trained model is used to perform the supervised emotion classification task. Additionally, in the case of medical imaging in general and EEG-based diagnosis in particular, there exist legitimate interests and needs for using such a training model to solve a similar problem with insufficient training data  [34] ,  [43] ,  [44] . To this end, the model has to be trained on the whole data to get the transferable model parameters. The target data would never be seen in the training phase.\n\nTo investigate the possibility of using this trained network in similar EEG-based emotion recognition tasks, we propose and implement a transfer learning (TL) approach. The goal of TL is to test our model ability in real-life conditions where the available amount of labeled data is not sufficient. TL helps to improve the learning capability of the target data by leveraging the knowledge of the source domain. In this study, we investigate transferring the learned model parameters assuming that individual models across different datasets with similar tasks should share some parameters. Firstly, the model is fully trained using sufficient labeled data (source dataset). Second, we peruse different schemes to tune the pre-trained network via the target dataset. The source and target datasets involve EEG-based emotion recognition experiments with different stimuli.\n\nFigure  4  demonstrates five different schemes that we consider in the calibration (fine-tuning) session. The TL schemes in our STE blocks are inspired by observations in CNNbased TL frameworks in computer vision where usually the later network layers are retrained, as the earlier layers are responsible for generic features  [45] . In our work, we consider different retrainable cells in both STE and RAN blocks. In particular, going from scheme (a) to scheme (e), we change the status of exactly one layer either to retrainable or nonretrainable (frozen) at each step. In each scheme, blocks marked with a cross are left unchanged during fine-tuning of the network. The number of retrainable parameters in scheme (a) to scheme (e) is equal to 239100, 311420, 280295, 207975, and 53735, respectively.\n\nDue to the variations in inter-dataset samples, we use a small part of the new data (target data), N, to calibrate and finetune our pre-trained model. Since the amount of calibration data is limited, we scale down the initial learning rate to avoid clobbering in initialization  [46] . Scaling down the initial learning rate η with the scale α can be defined as:\n\nwhere Φ i is the trainable parameters at the i th iteration and L is the cross-entropy loss function. Here, α is set to 0.1.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iii. Dataset And Implementation",
      "text": "A. Datasets 1) DEAP Dataset: The DEAP dataset was recorded from 32 individuals each having rated 40 music videos for 60 s  [30] . After each video, the participants performed a selfassessment to show their emotional states by rating the level of valence, arousal, dominance, and liking from 1 to 9. The physiological recordings consist of 32 channels of EEG signals and 8 channels of peripheral physiological data. Here, we just consider the EEG recordings of each trial. The preprocessing scheme is as follows: 1) down-sampling the data to 128 Hz, 2) averaging to the common reference, 3) removing electrooculography (EOG) artifacts, and 4) applying a bandpass filter with the frequency range of  [4, 45]  Hz. Accordingly, each recording contains 3 s pre-trial relaxing phase followed by 60 s trial data.\n\n2) DREAMER Dataset: The DREAMER dataset was recorded with a 14-channel, Emotiv EPOC wireless EEG headset  [33] . The data were recorded with a 128 Hz sampling frequency from 23 participants while watching 18 film clips. Each film clip lasted 65 to 393 s to elicit the emotional states. Before data collection, participants watched a neutral film clip to neutralize the emotional state. Data collected while watching this neutral film served as the baseline. Participants were asked to assess their emotional states by rating valence, arousal, and dominance levels in each video from 1 to 5.\n\nTo have consistency with the DEAP dataset, each recording contains 3 s pre-trial relaxing phase followed by 60 s trial data and is band-pass filtered from 4.0 to 45.0 Hz.\n\n3) Emotional English Word Dataset (EEWD): Data collection was performed via 64 Ag/AgCl active electrodes located over the scalp based on the 10-10 International Electrode Placement System while participants were rating emotional words. Participants provided signed informed consents in accordance with the Sabanci University Research Ethics Council guidelines. EEG data were recorded using BioSemi ActiveTwo systems (Biosemi Inc., Amsterdam, the Netherlands) in a dimly lit EEG room within a Faraday cage. A dataset of highly-arousing English words was formed such that 65 negative words (arousal> 6 and valence< 3) and 63 positive words (arousal> 6 and valence> 7) were selected from the Affective Norms for English Words (ANEW) dataset  [47] . Details of compiling this small dataset of Original English List (OEL) is presented in more details in  [48] . Thirty native Turkish speakers participated in the experiment where English served as their secondary language. The experiment consisted of four blocks and each block contained 32 randomly selected words. Each word was presented for 1 s and then the participants were asked to rate the valence and arousal in the range of 1 to 9 using a set of pictorial self-assessment manikins (SAM)  [48] . The data of two participants were discarded due to technical problems. The recorded signals were down-sampled from 2048 Hz to 128 Hz, EOG artifacts were removed via independent component analysis (ICA), and a bandpass filter from 4.0 to  45.0 Hz was applied. All preprocessing steps were conducted using the EEGLAB toolbox  [49] .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Implementation Of The Proposed Network",
      "text": "The proposed network consists of two parts that operate in parallel  [27] . The input of STE, IN1, has the dimensions of n×k×1 where k is set to 128 corresponding to the 1 s slicing window length. In order to select the STE parameters, we perform the grid search on the subset of the kernel sizes,  [9, 7, 5, 3] , and the parameters corresponding to the best performance on the training data samples selected. Table  I  provides the details of the STE structure. Each averagepooling layer is followed by a dropout layer. The dropout probability rates for each dropout layer are set to 0.5 and 0.4, respectively. In order to prevent edge information loss, the same zero-padding technique is used in each convolution (conv) operation. Here, we adopt rectified linear unit (ReLU) which has been used in many related CNN-based applications. After concatenating the outputs of all columns, a 1 × 1 conv filter is applied to compute the spatial feature maps.\n\nThe input of the RAN block, IN2, has the size of k × n. RAN consists of two BiLSTM layers with the same hidden layer size. The hidden layer and time steps are set to 80 and 128, respectively. We choose hyperbolic tangent (tanh) as the activation function for all BiLSTM cells. Each pair of forward and backward LSTM cells is followed by a dropout layer with probability rates of 0.3 and 0.2, respectively. BiLSTM outputs are then used as the input of the attention mechanism.\n\nThe spatial and temporal representations extracted from STE and RAN blocks are flattened and concatenated. Next, we apply a fully connected layer where the number of hidden units is 128. In the end, we exploit the SoftMax operation to obtain classification labels.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Iv. Experiments",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Results And Analysis On The Deap Dataset",
      "text": "In this section, we investigate the performance of the proposed DL architecture on the DEAP dataset. The DEAP dataset involves subjects watching long, continuous videos, hence trials can be defined arbitrarily as smaller segments from this dataset. While the DEAP dataset defines 60 s EEG intervals as trials, various methods have used different intervals as samples for training and testing. In this work, the trial data are baseline corrected and a non-overlapping sliding window with a length of 1 s is applied to slice the 60 s trials. The final size of each data sample equals 32×128 where 32 is the number of the EEG nodes and 128 is the number of time samples. Thus, the data for each participant consists of 40 × 60 = 2400 data samples. While our experiments and those of other methods we compare against have allowed non-overlapping 1 s data samples from any 60 s trial to be assigned to training or test sets randomly, a different approach could be to assign all 1 s samples from a particular trial to the training or test set. In order to explore different frequency bandwidths, each data sample is filtered into five subbands: theta, alpha, beta, gamma, and wide-band. In order to validate the performance of the proposed method, we consider three binary classification problems, i.e., high-versus-low valence, high-versuslow arousal, and high-versus-low dominance. Considering a threshold of 5, we quantize the 9-level ratings of valence, arousal, and dominance into two levels to obtain a binary problem. The model is trained using subject-dependent 10-fold cross-validation (CV). The validation process is repeated 10 times and the average classification performance is reported.\n\nAdam optimizer  [50]  is used to minimize the cross-entropy between the predicted labels and true labels. The epochs and batch-size are picked as 50 and 300, respectively. Grid search is applied to select the hyper-parameters that maximize the average classification accuracy based on training data.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "1) Ablation Study:",
      "text": "To analyze the impacts of using the station-temporal encoding, recurrent network, and attention mechanism, we have performed an ablation study by considering each block of the proposed STANN, namely STE and RAN as baseline models and evaluating their performance separately using the aforementioned parameters. In order to assess the effect of graph smoothing, we calculate the graph-smoothed signals and assess the network performance in frequency subbands and present the performance results of the proposed method with two different input modalities, i.e., raw EEG data and smooth EEG data. Since the GFT computing and smoothness are not dependent on a specific task or subject, this would not interfere with the automated operation of our model. Details of the ablation experiments are provided in Table  II .\n\nTables III, IV, and V show the obtained average accuracies and standard deviations (SD) for binary valence, binary arousal, and binary dominance classification for the baseline models and the proposed method based on data from different frequency subbands with different inputs. To address cases involving unbalanced data, we also report F1-scores for the proposed model in all three classification problems. SS2-(.) and SS4-(.) correspond to graph-smoothed signals with 2-NN and 4-NN adjacency matrices, respectively. Results of DEAP dataset classification in Tables III to V demonstrate that graph smoothing leads to better performance than the use of raw EEG input data. Moreover, in the majority of classification sce- These results indicate that our proposed method exceeds the baseline models and that graph smoothing enhances the overall classification performance. Moreover, our findings show that beta and wide-band frequency subbands outperform other spectral features in binary classifications problems of highversus-low valence, arousal, and dominance dimensions which is in line with the role of different frequency subbands in characterizing affective states  [7] ,  [51] .\n\n2) Comparison with the-state-of-the-art: We compare the performance of our proposed SS4-SSTANN method for the classification of valence and arousal from wide-band EEG of the DEAP dataset with a number of state-of-the-art solutions namely DCCA  [52] , ECLGCNN  [53] , DGCNN  [54] , CVCNN  [55] , CRAM  [56] , ACRNN  [17] , S-EEGNet  [19] , and Casc-CNN-LSTM  [26] . In DCCA, firstly, DE features are computed in four different frequency bands and then the network computes the representations of two modalities by passing them through multiple stacked layers of nonlinear transformations.ECLGCNN uses the infusion of graph convolutional neural networks with LSTMs while DE of the windowed EEG data are considered as the input. DGCNN 1  computes DE features and applies a 2400 feature vector as the input of the graph CNN. CVCNN utilizes raw EEG and normalized EEG data in combination with PSD features. CRAM extracts spatio-",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Table Vi Comparison Of The Proposed Ss4-Stann Method With Recent State-Of-The-Art Solution For Valence And Arousal",
      "text": "CLASSIFICATION FROM WIDE-BAND DATA OF THE DEAP DATASET.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Method",
      "text": "Valence (%) Arousal (%) Proposed method with SS4-STANN 95.6 97.0 DCCA  [52]  84.3 85.6 ECLGCNN  [53]  90.5 90.6 DGCNN  [54]  92.5 93.5 CVCNN  [55]  88.8 86.9 CRAM  [56]  85.5 83.6 ACRNN  [17]  89.9 88.3 S-EEGNet  [19]  89.9 88.3 Casc-CNN-LSTM  [26]  93.6 93.2\n\nCol. 1",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "High Valence Low Valence (High-Low) Valence",
      "text": "Col. 2\n\nCol. 3 temporal information along with attentive temporal dynamics in a cascaded format. The model uses a CNN layer with a fixed kernel and filter size and the temporal information is extracted from the extracted spatial features which makes the temporal information dependent on the spatial features. ACRNN uses an attention technique to get different weights for each EEG channel followed by a CNN to extract spatial features. S-EEGNet applies the Hilbert-Huang transform to preprocess the EEG data before feeding the data to a separable CNN. Casc-CNN-LSTM applies hybrid convolutional recurrent neural networks by using transformed 1D EEG vector sequences into 2D mesh-like matrices. Table VI presents a comparison of the proposed SS4-STANN on wide-band data with the abovementioned methods from recent literature and demonstrates the superiority of our proposed approach. The reported results are all subject-dependent with a 10-fold CV except  [19]  where the authors performed a 4-fold CV. features for each kernel separately. This operation results in a 32-dimensional vector for each kernel. The feature vector is then normalized in the range of 0 to 1.\n\nFigures  5  and 6  present the topographic scalp plots for the first kernels of each column for valence and arousal, respectively. The representations are averaged over the data samples and subjects. For high-vs-low valence and arousal problems, most of the activities are over frontal, temporal, and central lobes which is consistent with the literature regarding the processing of human emotions  [57] . The valence difference plots for all columns in Figure  5  show the role of temporal and parietal lobes in positive and negative emotions  [58] ,  [59] . The difference plot for arousal classification in column 2 of Figure  6  shows higher activity in the left cortex and frontal lobe similar to previous observations in the literature  [60] . Moreover, the plots show that arousal processing has a more wide scattered pattern over the brain than valence  [25] .",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "B. Evaluating Transfer Learning Performance",
      "text": "To show that the learned representations based on SS-STANN have the generality to be applied in similar tasks that have limited labeled data, we perform TL in two different schemes, namely cross-subject TL and cross-dataset TL. In order to make a fair comparison among the different amounts of calibration data, we set 10% of the target data samples as a test set, and the calibration data is selected among the rest 90% of the data.\n\n1) Cross-Subject TL: To perform cross-subject TL, one subject of the DEAP dataset is considered the target subject and the rest of the subjects are source subjects. Given its superior performance in the supervised classification task, we use SS4-STANN for the TL experiments. First, the network is trained for 100 epochs on the source subjects' data samples to get the pre-trained network. Second, we use N data samples of the target subject's data to fine-tune the pre-trained model. During the fine-tuning process, the epoch values are set to    VII  and VIII , it is clear that TL helps to increase performance across different subjects.\n\n2) Cross-Dataset TL: To apply the cross-dataset TL, we trained our proposed model on the whole DEAP dataset and tuned the network on new EEG emotion recognition datasets collected with different stimuli. To this end, we choose the publicly available DREAMER dataset and EEWD. To have consistency between datasets' characteristics, namely EEG electrodes, frequency bands of interest, and the sliding window for segmentation, we choose the following settings. Fourteen common electrodes among all datasets are selected, namely AF3, F7, F3, FC5, T7, P7, O1, O2, P8, T8, FC6, F4, F8, and AF4. The same data slicing process is also applied to the DREAMER dataset. Since the trial length in DREAMER varies from trial to trial, we select the last 60 s of each trial and segment it into 1 s data samples. The final number of data samples corresponding to DREAMER is equal to 18×60 = 1080 per participant. The length of each trial sample in EEWD is 1 s which leaves us with 128 time steps. For each value of N, the best performing TL scheme is indicated in bold.\n\nTL process contains two main steps. First, the proposed network is trained on the whole DEAP dataset. Given its superior performance in the supervised classification task, we use SS4-STANN for the transfer learning experiments. 10% of the wide-band EEG samples of each subject in the DEAP dataset is considered for validation and the rest of samples are set aside for training. The network is trained for 100 epochs and the model parameters corresponding to the least validation loss are considered as the final pre-trained network. Second, we use N samples of the target data for calibration to fine-tune the pre-trained model.\n\nIn order to have a binary classification problem for the DREAMER dataset, the valence and arousal ratings are divided into two levels using a threshold of 3. Table  X  presents the TL results related to the DREAMER dataset for different schemes and various amounts of calibration data. We also report F1-score values to avoid the possibility of bias to one class. To avoid overfitting in the fine tuning process for N equals 1 trial per class, the number of epochs is set to 1, and the epoch values is set 20 when N equals 10%, 20%, and 90% of data samples. In the latter case, early stopping with a patience parameter of 10 is set for the tuning process.\n\nAs expected, an increase in the amount of calibration data improves the classification performance. To show the effectiveness of transferring pre-trained model parameters, the performance related to N = 10%, 20%, and 90% of data samples without applying TL is considered as the baseline which is shown in Table  IX . Observing the results in Tables IX and X, it is clear that tuning the pre-trained network increases the classification performance for binary valence and arousal problems. Without using TL, the valence classification accuracy for N = 10%, 20%, and 90% of data samples are 65.8%, 67.6%, and 70.8%, respectively. However, TL increases the performance for the same N values to 72.0%, 74.4%, and 83.0%. Similar performance improvement is observed for the arousal classification problem. Considering Tables IX and X, for different N values, the results corresponding for without TL and with TL increase from 73.5%, 75.6%, and 77.6% to 78.2%, 81.0%, and 87.2%.\n\nTo evaluate transfer learning of models trained with EEG in response to video clips to EEG signals obtained in response to written words, we consider the EEWD and perform a binary valence classification scenario since all the words in that experiment are selected from the high arousal group. We consider trials corresponding to rating values lower or equal to 3 as negative valence and trials correspond to rating values higher or equal to 7 as positive valence. During the tuning process, the number of epochs and early stopping is set as    IX  for this dataset since the model overfits in early epochs.\n\n3) Verification of TL: To establish that the proposed structure works properly and TL helps to separate the components related to each class, we randomly pick the EEG samples of one of the subjects in DREAMER dataset to visualize them with t-SNE  [61]  (Figure  7 ). Figure  7 (a) shows the scatter plot of samples in the last dense layer before the classification softmax layer without TL and fine-tuning. Figure  7 (b) presents the results after TL and fine-tuning. As can be seen, before fine-tuning the process, the representations corresponding to different classes are more mixed up and TL helps make them more separable.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "V. Conclusion",
      "text": "In this study, we proposed a novel deep learning architecture for subject-dependent EEG-based emotion classification tasks. The proposed SS-STANN involves a hybrid structure with parallel STE and RAN blocks and an attention mechanism. SS-STANN captures the spatial and temporal information inherent in multi-channel EEG data while enforcing graph smoothness in the spatial domain. We demonstrated that this work performs better than other state-of-the-art solutions, with classification accuracies of over 95.0% for valence, arousal, and dominance for the DEAP dataset. Moreover, the critical frequency bands and regions are explored to validate the performance results. We also showed that the representations extracted from one EEG experiment could be used in other EEG emotion recognition tasks with similar and different stimulus modalities, highlighting the cross-modal transferability potential of the trained model and learned representations. In the future, we will investigate the effect of using different modalities along with EEG in a similar problem. Also, we will concentrate on conducting real-time experiments based on the proposed framework. Moreover, future work may involve adopting the spatio-temporal feature learning ideas presented here to problems involving other modalities of physiological data such as functional MRI (fMRI) or magnetoencephalography (MEG).",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of the proposed SS-STANN architecture. The CNN blocks (blue) contain adopt the ReLU activation function and batch normalization.",
      "page": 3
    },
    {
      "caption": "Figure 2: The adjacency matrix (left) illustration, and its corresponding graph",
      "page": 3
    },
    {
      "caption": "Figure 1: presents an overview of the proposed pipeline.",
      "page": 3
    },
    {
      "caption": "Figure 2: shows the final adjacency",
      "page": 3
    },
    {
      "caption": "Figure 3: LSTM cell block.",
      "page": 4
    },
    {
      "caption": "Figure 3: ). Formulas governing the operations in",
      "page": 4
    },
    {
      "caption": "Figure 4: demonstrates five different schemes that we con-",
      "page": 5
    },
    {
      "caption": "Figure 4: The proposed transfer learning schemes. The blocks marked with a black cross are the ones that remain frozen during fine tuning.",
      "page": 6
    },
    {
      "caption": "Figure 5: Topographic feature maps for the weight distribution of the first kernel",
      "page": 8
    },
    {
      "caption": "Figure 6: Topographic feature maps for the weight distribution of the first kernel",
      "page": 8
    },
    {
      "caption": "Figure 5: show the role of temporal",
      "page": 8
    },
    {
      "caption": "Figure 6: shows higher activity in the left cortex and frontal",
      "page": 8
    },
    {
      "caption": "Figure 7: ). Figure 7(a) shows the scatter plot",
      "page": 10
    },
    {
      "caption": "Figure 7: (b) presents",
      "page": 10
    },
    {
      "caption": "Figure 7: Scatter plot of representative samples in the last dense layer ahead of",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Block\nType\nFilters\nKernel\nOutput Shape\nParameters": "80\n1 × 1\n16 × 64 × 80\n3600\nC2 3\nConv2D+BN"
        },
        {
          "Block\nType\nFilters\nKernel\nOutput Shape\nParameters": "−\n2 × 2\n8 × 32 × 50\n−\nP2 1\nPool2D"
        },
        {
          "Block\nType\nFilters\nKernel\nOutput Shape\nParameters": "−\n2 × 2\n8 × 32 × 60\n−\nP2 2\nPool2D"
        },
        {
          "Block\nType\nFilters\nKernel\nOutput Shape\nParameters": "−\n2 × 2\n8 × 32 × 80\n−\nP2 3\nPool2D"
        },
        {
          "Block\nType\nFilters\nKernel\nOutput Shape\nParameters": "25\n3 × 3\n8 × 32 × 25\n11375\nC3 1\nConv2D+BN"
        },
        {
          "Block\nType\nFilters\nKernel\nOutput Shape\nParameters": "30\n3 × 3\n8 × 32 × 30\n16350\nC3 2\nConv2D+BN"
        },
        {
          "Block\nType\nFilters\nKernel\nOutput Shape\nParameters": "40\n3 × 3\n8 × 32 × 40\n3400\nC3 3\nConv2D+BN"
        },
        {
          "Block\nType\nFilters\nKernel\nOutput Shape\nParameters": "−\n−\n8 × 32 × 95\n−\nCon1\nconcatenate"
        },
        {
          "Block\nType\nFilters\nKernel\nOutput Shape\nParameters": "1\n1 × 1\n8 × 32 × 1\n96\nC4\nConv2D"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "RAN",
          "θ\nα\nβ\nγ\nwide-band": "87.0/4.5\n87.2/4.6\n88.1/3.5\n85.0/3.3\n90.3/3.6"
        },
        {
          "Method": "SS2-RAN",
          "θ\nα\nβ\nγ\nwide-band": "87.6/4.1\n88.2/4.5\n89.5/3.5\n85.9/3.2\n91.8/3.3"
        },
        {
          "Method": "STE",
          "θ\nα\nβ\nγ\nwide-band": "88.5/4.6\n87.0/4.0\n91.6/3.8\n84.2/3.5\n93.9/3.8"
        },
        {
          "Method": "SS2-STE",
          "θ\nα\nβ\nγ\nwide-band": "89.2/4.3\n88.1/3.6\n91.9/3.6\n85.2/3.0\n94.5/3.1"
        },
        {
          "Method": "STANN",
          "θ\nα\nβ\nγ\nwide-band": "90.2/4.7\n89.7/3.7\n92.5/3.8\n86.7/3.6\n94.9/2.8"
        },
        {
          "Method": "SS2-\nSTANN",
          "θ\nα\nβ\nγ\nwide-band": "92.5/3.2\n92.6/3.7\n91.5/3.8\n95.8/1.8\n95.4/2.3\n0.90\n0.90\n0.94\n0.88\n0.95"
        },
        {
          "Method": "SS4-\nSTANN",
          "θ\nα\nβ\nγ\nwide-band": "95.2/2.1\n92.7/4.0\n92.6/3.7\n91.6/3.7\n97.0/1.7\n0.90\n0.90\n0.93\n0.89\n0.95"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "RAN",
          "θ\nα\nβ\nγ\nwide-band": "84.6/5.5\n85.5/4.4\n87.0/5.6\n83.5/7.1\n89.0/5.5"
        },
        {
          "Method": "SS2-RAN",
          "θ\nα\nβ\nγ\nwide-band": "85.2/4.7\n85.9/4.3\n88.4/5.1\n84.2/6.6\n90.1/5.0"
        },
        {
          "Method": "STE",
          "θ\nα\nβ\nγ\nwide-band": "86.1/4.5\n86.0/3.6\n91.4/2.5\n83.2/5.0\n93.3/2.2"
        },
        {
          "Method": "SS2-STE",
          "θ\nα\nβ\nγ\nwide-band": "87.8/3.8\n89.2/5.2\n91.6/4.0\n85.2/5.8\n93.8/1.9"
        },
        {
          "Method": "STANN",
          "θ\nα\nβ\nγ\nwide-band": "88.1/3.6\n88.5/3.1\n91.2/2.2\n86.6/4.5\n94.4/2.1"
        },
        {
          "Method": "SS2-\nSTANN",
          "θ\nα\nβ\nγ\nwide-band": "91.2/3.7\n94.9/2.4\n91.1/3.7\n94.7/2.4\n89.1/4.7\n0.89\n0.89\n0.93\n0.89\n0.94"
        },
        {
          "Method": "SS4-\nSTANN",
          "θ\nα\nβ\nγ\nwide-band": "88.9/4.7\n91.7/3.9\n95.1/2.2\n91.3/3.8\n95.6/1.9\n0.89\n0.89\n0.94\n0.89\n0.94"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "RAN",
          "θ\nα\nβ\nγ\nwide-band": "87.9/5.4\n88.9/5.1\n89.0/5.9\n84.3/7.9\n89.4/5.4"
        },
        {
          "Method": "SS2-RAN",
          "θ\nα\nβ\nγ\nwide-band": "89.0/5.3\n89.5/4.7\n89.8/5.5\n85.6/7.2\n90.2/4.7"
        },
        {
          "Method": "STE",
          "θ\nα\nβ\nγ\nwide-band": "91.6/4.5\n92.5/4.4\n95.3/2.8\n88.8/5.7\n95.5/2.7"
        },
        {
          "Method": "SS2-",
          "θ\nα\nβ\nγ\nwide-band": "91.9/4.5\n92.5/4.1\n95.5/2.6\n89.3/5.2\n95.7/2.3"
        },
        {
          "Method": "STANN",
          "θ\nα\nβ\nγ\nwide-band": "92.2/3.9\n93.9/3.9\n96.3/2.8\n90.4/5.5\n96.3/2.3"
        },
        {
          "Method": "SS2-\nSTANN",
          "θ\nα\nβ\nγ\nwide-band": "92.8/3.8\n93.0/4.2\n95.5/2.7\n91.6/4.6\n96.1/2.4\n0.90\n0.90\n0.94\n0.88\n0.94"
        },
        {
          "Method": "SS4-\nSTANN",
          "θ\nα\nβ\nγ\nwide-band": "92.7/4.0\n93.2/4.0\n95.6/2.3\n91.7/4.4\n96.8/1.9\n0.89\n0.90\n0.94\n0.88\n0.95"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "Proposed method with SS4-STANN",
          "Valence (%)\nArousal\n(%)": "95.6\n97.0"
        },
        {
          "Method": "DCCA [52]",
          "Valence (%)\nArousal\n(%)": "84.3\n85.6"
        },
        {
          "Method": "ECLGCNN [53]",
          "Valence (%)\nArousal\n(%)": "90.5\n90.6"
        },
        {
          "Method": "DGCNN [54]",
          "Valence (%)\nArousal\n(%)": "92.5\n93.5"
        },
        {
          "Method": "CVCNN [55]",
          "Valence (%)\nArousal\n(%)": "88.8\n86.9"
        },
        {
          "Method": "CRAM [56]",
          "Valence (%)\nArousal\n(%)": "85.5\n83.6"
        },
        {
          "Method": "ACRNN [17]",
          "Valence (%)\nArousal\n(%)": "89.9\n88.3"
        },
        {
          "Method": "S-EEGNet\n[19]",
          "Valence (%)\nArousal\n(%)": "89.9\n88.3"
        },
        {
          "Method": "Casc-CNN-LSTM [26]",
          "Valence (%)\nArousal\n(%)": "93.6\n93.2"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Amount of calibration data (N)": "1 trial per class\nAcc.\n(%)\nF-1 score",
          "Valence\na\nb\nc\nd\ne": "54.1\n54.1\n54.2\n54.3\n53.3\n0.48\n0.48\n0.49\n0.50\n0.46",
          "Arousal\na\nb\nc\nd\ne": "58.3\n59.0\n58.5\n58.6\n58.7\n0.48\n0.49\n0.47\n0.47\n0.50"
        },
        {
          "Amount of calibration data (N)": "10% of data samples\nAcc.\n(%)\nF-1 score",
          "Valence\na\nb\nc\nd\ne": "76.0\n76.2\n75.8\n75.8\n75.9\n0.73\n0.74\n0.73\n0.74\n0.74",
          "Arousal\na\nb\nc\nd\ne": "74.0\n74.5\n73.6\n73.8\n74.0\n0.70\n0.71\n0.69\n0.70\n0.70"
        },
        {
          "Amount of calibration data (N)": "20% of data samples\nAcc.\n(%)\nF-1 score",
          "Valence\na\nb\nc\nd\ne": "83.1\n82.2\n82.6\n82.9\n83.0\n0.82\n0.81\n0.82\n0.81\n0.81",
          "Arousal\na\nb\nc\nd\ne": "81.1\n81.3\n80.8\n80.9\n81.1\n0.78\n0.78\n0.78\n0.78\n0.78"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Amount of calibration data (N)": "1 trial per class\nAcc.\n(%)\nF-1 score",
          "Valence\na\nb\nc\nd\ne": "61.5\n64.8\n63.0\n63.3\n62.2\n0.56\n0.59\n0.57\n0.60\n0.57",
          "Arousal\na\nb\nc\nd\ne": "64.9\n68.6\n67.5\n69.1\n65.8\n0.49\n0.57\n0.58\n0.58\n0.57"
        },
        {
          "Amount of calibration data (N)": "10% of data samples\nAcc.\n(%)\nF-1 score",
          "Valence\na\nb\nc\nd\ne": "71.1\n70.4\n72.0\n70.9\n71.9\n0.68\n0.66\n0.67\n0.66\n0.67",
          "Arousal\na\nb\nc\nd\ne": "78.1\n78.2\n77.9\n77.9\n77.4\n0.67\n0.68\n0.66\n0.66\n0.66"
        },
        {
          "Amount of calibration data (N)": "20% of data samples\nAcc.\n(%)\nF-1 score",
          "Valence\na\nb\nc\nd\ne": "74.4\n73.9\n73.2\n73.1\n73.9\n0.71\n0.70\n0.69\n0.69\n0.70",
          "Arousal\na\nb\nc\nd\ne": "80.0\n80.1\n79.8\n81.0\n79.5\n0.71\n0.71\n0.72\n0.72\n0.69"
        },
        {
          "Amount of calibration data (N)": "90% of data samples\nAcc.\n(%)\nF-1 score",
          "Valence\na\nb\nc\nd\ne": "82.7\n83.0\n79.7\n81.6\n82.0\n0.79\n0.80\n0.73\n0.76\n0.80",
          "Arousal\na\nb\nc\nd\ne": "83.0\n87.2\n84.5\n85.0\n86.4\n0.78\n0.85\n0.82\n0.79\n0.83"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Amount of calibration data (N)": "1 trial per class\nAcc.\n(%)\nF-1 score",
          "Valence\na\nb\nc\nd\ne": "68.2\n66.5\n64.5\n67.3\n65.0\n0.68\n0.66\n0.65\n0.64\n0.64"
        },
        {
          "Amount of calibration data (N)": "10% of data samples\nAcc.\n(%)\nF-1 score",
          "Valence\na\nb\nc\nd\ne": "67.0\n67.7\n68.9\n66.9\n67.0\n0.66\n0.67\n0.68\n0.66\n0.67"
        },
        {
          "Amount of calibration data (N)": "20% of data samples\nAcc.\n(%)\nF-1 score",
          "Valence\na\nb\nc\nd\ne": "73.0\n73.5\n70.6\n70.2\n70.9\n0.71\n0.73\n0.69\n0.69\n0.70"
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Brain-computer interfaces for communication and control",
      "authors": [
        "J Wolpaw",
        "N Birbaumer",
        "D Mcfarland",
        "G Pfurtscheller",
        "T Vaughan"
      ],
      "year": "2002",
      "venue": "Clinical neurophysiology"
    },
    {
      "citation_id": "2",
      "title": "Emotion detection using electroencephalography signals and a zero-time windowing-based epoch estimation and relevant electrode identification",
      "authors": [
        "S Gannouni",
        "A Aledaily",
        "K Belwafi",
        "H Aboalsamh"
      ],
      "year": "2021",
      "venue": "Sci. Rep"
    },
    {
      "citation_id": "3",
      "title": "Emotions recognition using EEG signals: A survey",
      "authors": [
        "S Alarcao",
        "M Fonseca"
      ],
      "year": "2017",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "4",
      "title": "Utilization of a combined EEG/NIRS system to predict driver drowsiness",
      "authors": [
        "T Nguyen",
        "S Ahn",
        "H Jang",
        "S Jun",
        "J Kim"
      ],
      "year": "2017",
      "venue": "Sci. Rep"
    },
    {
      "citation_id": "5",
      "title": "Bimodal data fusion of simultaneous measurements of EEG and fNIRS during lower limb movements",
      "authors": [
        "M Al-Quraishi",
        "I Elamvazuthi",
        "T Tang",
        "M Al-Qurishi",
        "S Adil",
        "M Ebrahim"
      ],
      "year": "2021",
      "venue": "Brain Sciences"
    },
    {
      "citation_id": "6",
      "title": "Emotional state classification from EEG data using machine learning approach",
      "authors": [
        "X.-W Wang",
        "D Nie",
        "B.-L Lu"
      ],
      "year": "2014",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "7",
      "title": "Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    },
    {
      "citation_id": "8",
      "title": "A novel multivariate-multiscale approach for computing EEG spectral and temporal complexity for human emotion recognition",
      "authors": [
        "A Bhattacharyya",
        "R Tripathy",
        "L Garg",
        "R Pachori"
      ],
      "year": "2020",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "9",
      "title": "Emotion recognition based on EEG brain rhythm sequencing technique",
      "authors": [
        "J Li",
        "S Barma",
        "S Pun",
        "M Vai",
        "P Mak"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "10",
      "title": "Takagi-sugeno-kang transfer learning fuzzy logic system for the adaptive recognition of epileptic electroencephalogram signals",
      "authors": [
        "C Yang",
        "Z Deng",
        "K.-S Choi",
        "S Wang"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Fuzzy Systems"
    },
    {
      "citation_id": "11",
      "title": "EEG artifacts handling in a real practical brain-computer interface controlled vehicle",
      "authors": [
        "A Jafarifarmand",
        "M Badamchizadeh"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Neural Syst. Rehabilitation Eng"
    },
    {
      "citation_id": "12",
      "title": "New types of deep neural network learning for speech recognition and related applications: An overview",
      "authors": [
        "L Deng",
        "G Hinton",
        "B Kingsbury"
      ],
      "year": "2013",
      "venue": "2013 IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "13",
      "title": "Spatial transformer networks",
      "authors": [
        "M Jaderberg",
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2015",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "14",
      "title": "Spatio-spectral representation learning for electroencephalographic gait-pattern classification",
      "authors": [
        "S Goh",
        "H Abbass",
        "K Tan",
        "A Al-Mamun",
        "N Thakor",
        "A Bezerianos",
        "J Li"
      ],
      "year": "2018",
      "venue": "IEEE Trans. Neural Syst. Rehabilitation Eng"
    },
    {
      "citation_id": "15",
      "title": "Deep learning",
      "authors": [
        "Y Lecun",
        "Y Bengio",
        "G Hinton"
      ],
      "year": "2015",
      "venue": "nature"
    },
    {
      "citation_id": "16",
      "title": "A survey on deep learning-based non-invasive brain signals: recent advances and new frontiers",
      "authors": [
        "X Zhang",
        "L Yao",
        "X Wang",
        "J Monaghan",
        "D Mcalpine",
        "Y Zhang"
      ],
      "year": "2020",
      "venue": "J. Neural. Eng"
    },
    {
      "citation_id": "17",
      "title": "EEGbased emotion recognition via channel-wise attention and self attention",
      "authors": [
        "W Tao",
        "C Li",
        "R Song",
        "J Cheng",
        "Y Liu",
        "F Wan",
        "X Chen"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "18",
      "title": "Prediction of reaction time and vigilance variability from spatio-spectral features of resting-state EEG in a long sustained attention task",
      "authors": [
        "M Torkamani-Azar",
        "S Kanik",
        "S Aydin",
        "M Cetin"
      ],
      "year": "2020",
      "venue": "IEEE J. Biomed. Health. Inform"
    },
    {
      "citation_id": "19",
      "title": "S-EEGNet: Electroencephalogram signal classification based on a separable convolution neural network with bilinear interpolation",
      "authors": [
        "W Huang",
        "Y Xue",
        "L Hu",
        "H Liuli"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "20",
      "title": "Deep learning with convolutional neural networks for EEG decoding and visualization",
      "authors": [
        "R Schirrmeister",
        "J Springenberg",
        "L Fiederer",
        "M Glasstetter",
        "K Eggensperger",
        "M Tangermann",
        "F Hutter",
        "W Burgard",
        "T Ball"
      ],
      "year": "2017",
      "venue": "Human brain mapping"
    },
    {
      "citation_id": "21",
      "title": "Multimodal emotion recognition using a modified dense co-attention symmetric network",
      "authors": [
        "Z.-W Zhao",
        "W Liu",
        "B.-L Lu"
      ],
      "year": "2021",
      "venue": "2021 10th International IEEE/EMBS Conference on Neural Engineering (NER)"
    },
    {
      "citation_id": "22",
      "title": "A multi-column CNN model for emotion recognition from EEG signals",
      "authors": [
        "H Yang",
        "J Han",
        "K Min"
      ],
      "year": "2019",
      "venue": "Sensors"
    },
    {
      "citation_id": "23",
      "title": "Fldnet: Frame level distilling neural network for EEG emotion recognition",
      "authors": [
        "Z Wang",
        "T Gu",
        "Y Zhu",
        "D Li",
        "H Yang",
        "W Du"
      ],
      "year": "2021",
      "venue": "IEEE J. Biomed. Health. Inform"
    },
    {
      "citation_id": "24",
      "title": "Emotion recognition from multi-channel EEG via deep forest",
      "authors": [
        "J Cheng",
        "M Chen",
        "C Li",
        "Y Liu",
        "R Song",
        "A Liu",
        "X Chen"
      ],
      "year": "2020",
      "venue": "IEEE J. Biomed. Health. Inform"
    },
    {
      "citation_id": "25",
      "title": "Utilizing deep learning towards multi-modal bio-sensing and vision-based affective computing",
      "authors": [
        "S Siddharth",
        "T.-P Jung",
        "T Sejnowski"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "26",
      "title": "Emotion recognition from spatiotemporal EEG representations with hybrid convolutional recurrent neural networks via wearable multi-channel headset",
      "authors": [
        "J Chen",
        "D Jiang",
        "Y Zhang",
        "P Zhang"
      ],
      "year": "2020",
      "venue": "Computer Communications"
    },
    {
      "citation_id": "27",
      "title": "EEG emotion recognition via graph-based spatio-temporal attention neural networks",
      "authors": [
        "S Sartipi",
        "M Torkamani-Azar",
        "M Cetin"
      ],
      "year": "2021",
      "venue": "2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)"
    },
    {
      "citation_id": "28",
      "title": "Online discriminative graph learning from multi-class smooth signals",
      "authors": [
        "S Saboksayr",
        "G Mateos",
        "M Cetin"
      ],
      "year": "2021",
      "venue": "Signal Processing"
    },
    {
      "citation_id": "29",
      "title": "Smoothing of spatial filter by graph fourier transform for EEG signals",
      "authors": [
        "H Higashi",
        "T Tanaka",
        "Y Tanaka"
      ],
      "year": "2014",
      "venue": "Signal and Information Processing Association Annual Summit and Conference (APSIPA)"
    },
    {
      "citation_id": "30",
      "title": "Asia-Pacific. IEEE",
      "year": "2014",
      "venue": "Asia-Pacific. IEEE"
    },
    {
      "citation_id": "31",
      "title": "DEAP: A database for emotion analysis; using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2011",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "32",
      "title": "Adaptive transfer learning for EEG motor imagery classification with deep convolutional neural network",
      "authors": [
        "K Zhang",
        "N Robinson",
        "S.-W Lee",
        "C Guan"
      ],
      "year": "2021",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "33",
      "title": "Leveraging anatomical information to improve transfer learning in brain-computer interfaces",
      "authors": [
        "M Wronkiewicz",
        "E Larson",
        "A Lee"
      ],
      "year": "2015",
      "venue": "J. Neural. Eng"
    },
    {
      "citation_id": "34",
      "title": "Dreamer: A database for emotion recognition through EEG and ecg signals from wireless low-cost offthe-shelf devices",
      "authors": [
        "S Katsigiannis",
        "N Ramzan"
      ],
      "year": "2017",
      "venue": "IEEE J. Biomed. Health. Inform"
    },
    {
      "citation_id": "35",
      "title": "Combining cross-modal knowledge transfer and semi-supervised learning for speech emotion recognition",
      "authors": [
        "S Zhang",
        "M Chen",
        "J Chen",
        "Y.-F Li",
        "Y Wu",
        "M Li",
        "C Zhu"
      ],
      "year": "2021",
      "venue": "Knowl. Based Syst"
    },
    {
      "citation_id": "36",
      "title": "Structural and functional brain networks: from connections to cognition",
      "authors": [
        "H.-J Park",
        "K Friston"
      ],
      "year": "2013",
      "venue": "Science"
    },
    {
      "citation_id": "37",
      "title": "EEG based emotion recognition by combining functional connectivity network and local activations",
      "authors": [
        "P Li",
        "H Liu",
        "Y Si",
        "C Li",
        "F Li",
        "X Zhu",
        "X Huang",
        "Y Zeng",
        "D Yao",
        "Y Zhang"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Biomed. Eng"
    },
    {
      "citation_id": "38",
      "title": "Complex brain networks: graph theoretical analysis of structural and functional systems",
      "authors": [
        "E Bullmore",
        "O Sporns"
      ],
      "year": "2009",
      "venue": "Nature reviews neuroscience"
    },
    {
      "citation_id": "39",
      "title": "A graph signal processing framework for the classification of temporal brain data",
      "authors": [
        "S Itani",
        "D Thanou"
      ],
      "year": "2021",
      "venue": "2020 28th European Signal Processing Conference (EUSIPCO)"
    },
    {
      "citation_id": "40",
      "title": "Multi-column deep neural networks for image classification",
      "authors": [
        "D Ciregan",
        "U Meier",
        "J Schmidhuber"
      ],
      "year": "2012",
      "venue": "Multi-column deep neural networks for image classification"
    },
    {
      "citation_id": "41",
      "title": "Single-image crowd counting via multi-column convolutional neural network",
      "authors": [
        "Y Zhang",
        "D Zhou",
        "S Chen",
        "S Gao",
        "Y Ma"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "42",
      "title": "A critical review of recurrent neural networks for sequence learning",
      "authors": [
        "Z Lipton",
        "J Berkowitz",
        "C Elkan"
      ],
      "year": "2015",
      "venue": "A critical review of recurrent neural networks for sequence learning",
      "arxiv": "arXiv:1506.00019"
    },
    {
      "citation_id": "43",
      "title": "Neural machine translation by jointly learning to align and translate",
      "authors": [
        "D Bahdanau",
        "K Cho",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Neural machine translation by jointly learning to align and translate",
      "arxiv": "arXiv:1409.0473"
    },
    {
      "citation_id": "44",
      "title": "Can emotion be transferred?-a review on transfer learning for eeg-based emotion recognition",
      "authors": [
        "W Li",
        "W Huan",
        "B Hou",
        "Y Tian",
        "Z Zhang",
        "A Song"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "45",
      "title": "A review on transfer learning in eeg signal analysis",
      "authors": [
        "Z Wan",
        "R Yang",
        "M Huang",
        "N Zeng",
        "X Liu"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "46",
      "title": "Convolutional neural networks,\" in Handbook of medical image computing and computer assisted intervention",
      "authors": [
        "J Teuwen",
        "N Moriakov"
      ],
      "year": "2020",
      "venue": "Convolutional neural networks,\" in Handbook of medical image computing and computer assisted intervention"
    },
    {
      "citation_id": "47",
      "title": "Region-based convolutional networks for accurate object detection and segmentation",
      "authors": [
        "R Girshick",
        "J Donahue",
        "T Darrell",
        "J Malik"
      ],
      "year": "2015",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "48",
      "title": "Affective norms for english words (anew): Affective ratings of words and instruction manual",
      "authors": [
        "M Bradley",
        "P Lang"
      ],
      "year": "2010",
      "venue": "Affective norms for english words (anew): Affective ratings of words and instruction manual"
    },
    {
      "citation_id": "49",
      "title": "Emotionality of turkish language and primary adaptation of affective english norms for turkish",
      "authors": [
        "M Torkamani-Azar",
        "S Kanik",
        "A Vardan",
        "C Aydin",
        "M Cetin"
      ],
      "year": "2019",
      "venue": "Current Psychology"
    },
    {
      "citation_id": "50",
      "title": "EEGLAB: an open source toolbox for analysis of single-trial EEG dynamics including independent component analysis",
      "authors": [
        "A Delorme",
        "S Makeig"
      ],
      "year": "2004",
      "venue": "Journal of neuroscience methods"
    },
    {
      "citation_id": "51",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "52",
      "title": "EEG alpha activity reflects attentional demands, and beta activity reflects emotional and cognitive processes",
      "authors": [
        "W Ray",
        "H Cole"
      ],
      "year": "1985",
      "venue": "Science"
    },
    {
      "citation_id": "53",
      "title": "Comparing recognition performance and robustness of multimodal deep learning models for multimodal emotion recognition",
      "authors": [
        "W Liu",
        "J.-L Qiu",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "54",
      "title": "EEG emotion recognition using fusion model of graph convolutional neural networks and lstm",
      "authors": [
        "Y Yin",
        "X Zheng",
        "B Hu",
        "Y Zhang",
        "X Cui"
      ],
      "year": "2021",
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "55",
      "title": "EEG emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2018",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "56",
      "title": "Accurate EEG-based emotion recognition on combined features using deep convolutional neural networks",
      "authors": [
        "J Chen",
        "P Zhang",
        "Z Mao",
        "Y Huang",
        "D Jiang",
        "Y Zhang"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "57",
      "title": "A convolutional recurrent attention model for subject-independent EEG signal analysis",
      "authors": [
        "D Zhang",
        "L Yao",
        "K Chen",
        "J Monaghan"
      ],
      "year": "2019",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "58",
      "title": "Functional neuroanatomy of emotion: a meta-analysis of emotion activation studies in pet and fmri",
      "authors": [
        "K Phan",
        "T Wager",
        "S Taylor",
        "I Liberzon"
      ],
      "year": "2002",
      "venue": "Neuroimage"
    },
    {
      "citation_id": "59",
      "title": "EEG-based emotion recognition during watching movies",
      "authors": [
        "D Nie",
        "X.-W Wang",
        "L.-C Shi",
        "B.-L Lu"
      ],
      "year": "2011",
      "venue": "Neural Engineering. IEEE"
    },
    {
      "citation_id": "60",
      "title": "Effect of negative and positive emotions on EEG spectral asymmetry",
      "authors": [
        "L Orgo",
        "M Bachmann",
        "J Lass",
        "H Hinrikus"
      ],
      "year": "2015",
      "venue": "2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)"
    },
    {
      "citation_id": "61",
      "title": "The role of asymmetric frontal cortical activity in emotion-related phenomena: A review and update",
      "authors": [
        "E Harmon-Jones",
        "P Gable",
        "C Peterson"
      ],
      "year": "2010",
      "venue": "Biological psychology"
    },
    {
      "citation_id": "62",
      "title": "Virtual and real world adaptation for pedestrian detection",
      "authors": [
        "D Vazquez",
        "A Lopez",
        "J Marin",
        "D Ponsa",
        "D Geronimo"
      ],
      "year": "2013",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "63",
      "title": "Her research interests include brain-computer interfaces (BCI), affective computing, sleep scoring, machine learning, and signal processing of physiological data. Shadi received the IEEE Brain Best Paper Award in 2021",
      "venue": "Shadi Sartipi is currently pursuing a Ph"
    }
  ]
}