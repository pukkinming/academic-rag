{
  "paper_id": "2411.02625v2",
  "title": "Emosphere++: Emotion-Controllable Zero-Shot Text-To-Speech Via Emotion-Adaptive Spherical Vector",
  "published": "2024-11-04T21:33:56Z",
  "authors": [
    "Deok-Hyeon Cho",
    "Hyung-Seok Oh",
    "Seung-Bin Kim",
    "Seong-Whan Lee"
  ],
  "keywords": [
    "Emotional speech synthesis",
    "emotion transfer",
    "emotion style and intensity control",
    "zero-shot text-to-speech"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotional text-to-speech (TTS) has advanced significantly, but challenges persist due to the complexity of emotions and limitations in emotional speech datasets and models. A key issue with previous studies is the reliance on limited emotional speech datasets or extensive manual annotations, which restrict generalization across different speakers and emotional styles. To address this, we propose EmoSphere++, an emotioncontrollable zero-shot TTS model capable of generating expressive speech with fine-grained control over emotional style and intensity-without requiring manual annotations. We introduce a novel emotion-adaptive spherical vector that effectively captures emotional style and intensity, along with a joint attribute style encoder that enhances generalization to both seen and unseen speakers. To further improve emotion transfer in zero-shot scenarios, we introduce an additional disentanglement method to enhance the style transfer performance for zero-shot scenarios. Through both objective and subjective evaluations, we demonstrate the benefits of the proposed model in emotion style and intensity modeling, as well as its effectiveness in enhancing emotional expressiveness across both seen and unseen speakers.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "E MOTIONS are interrelated in a highly systematic fash- ion  [1] . For example, the emotion of sadness can be expressed with derivative states of primary emotions such as feeling hurt or lonely, depending on the style and intensity. In speech synthesis, the ability to generate expressive and controllable emotional speech is essential for creating natural and effective human-computer interactions, as emotions are nuanced and can manifest in varying styles and intensities. Recently, emotional text-to-speech (TTS) technology has experienced rapid developments, increasing the interest in global interpretable emotion control  [2] -  [5] . Controllable emotional TTS represents a breakthrough in reproducing human-like emotions in speech synthesis, thus enabling more emotionally intelligent interactions between humans and computers. -W. Lee are with the Department of Artificial Intelligence, Korea University, 145, Anam-ro, Seongbuk-gu, Seoul 02841, Republic of Korea.E-mail: (dh cho@korea.ac.kr hs oh@korea.ac.kr sb-kim@korea.ac.kr sw.lee@korea.ac.kr). Although researchers have made significant progress in controlling emotional intensity, the ability to precisely control emotional style remains a challenge.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Style Shift",
      "text": "Modeling diverse emotional styles and intensities is a major challenge in controllable emotional TTS. Unlike discrete emotion categories, emotional style and intensity are highly subjective and complex, making them difficult to accurately represent. Two general approaches for achieving controllable emotional TTS involve controlling conditioning features or manipulating internal emotion representations. That is, one approach uses conditioning features of emotion intensity, such as relative ranking matrices  [6] -  [11] , distance-based quantization  [12] , or voiced, unvoiced, and silence (VUS) states  [13] . The alternative approach involves the manipulation of internal emotion representations through the application of scaling factors  [14] ,  [15]  or interpolation of the embedding space  [16] . However, despite these methods, the explicit control of emotion style and intensity remains a largely unexplored topic in emotional speech synthesis.\n\nAnother approach to controlling emotional expression involves utilizing emotional dimensions. Compared to the discrete emotion approach, the dimensional approach, such as Russell's circumplex model, provides a more precise method for capturing the nuances between different emotional states  [1] . Recently, studies on TTS systems have attempted to control emotional attributes through the emotion dimension  [17] ,  [18] . In one of these studies, a prosody control block is extended by incorporating the continuous space of arousal and valence to allow interpretable emotional prosody control  [18] . Another study proposes an expressive TTS model with a semisupervised latent variable to control emotions in six discrete emotional states of arousal-valence combinations  [17] . However, this setup requires labor-intensive annotations, which are more expensive to obtain than categorical labels and more susceptible to annotator bias. The emotional dimension model also exhibits limitations when explicitly controlling emotion style and intensity. To address these challenges, EmoSphere-TTS  [3]  models derivative emotions through emotional attribute prediction and discrete emotion labels, enabling explicit control over emotion style and intensity. However, limitations persist due to reliance on predefined emotion and speaker labels.\n\nMost emotional TTS systems utilize Sequence-to-sequence (Seq2Seq) models, which not only predict the duration of speech automatically but also learn feature mapping and alignment simultaneously  [19] ,  [20] . The attention mechanism in these models allows them to focus on the emotionally emphasized parts of an utterance  [21] . However, Seq2Seq models face the typical challenges of auto-regressive models, such as long-term dependence and repetition problems. Furthermore, most emotional speech syntheses adopt fine-tuning to control emotion intensity on a single speaker dataset; however, some of these methods exhibit noticeably degraded speech quality  [7] ,  [10] . Researchers have explored acoustic models and additional discriminators for emotion transfer to enhance the capture of expressiveness when synthesizing acoustic features  [3] ,  [22] -  [24] . However, existing methods primarily focus on emotion transfer using discrete emotion labels, which overlook the complexity of emotions conveyed in human speech  [15] ,  [25] . Furthermore, these approaches often rely on additional discriminators to enhance expressiveness, adding complexity to the model while struggling to fully capture emotional nuance. To address these challenges, a TTS system capable of generalizing across zero-shot emotion transfer scenarios is needed, ensuring more accurate emotion synthesis without relying on predefined labels  [2] ,  [26] -  [29] .\n\nAs previously discussed, existing study and prior work  [3]  face the following challenges: (1) Defining and modeling emotional style and intensity as derivatives of primary emotions, while accounting for characteristics such as the distribution of emotion categories; (2) Effectively integrating global and fine-grained emotion representations to enhance emotional expressiveness, while ensuring robust generalization across unseen speakers and emotions; (3) Designing a TTS system capable of achieving high generalization and expressive capability in zero-shot style transfer scenarios, without relying on additional modules; and (4) Evaluating synthesized emotional speech beyond global emotion assessment, to include subjective measurement of detailed emotion styles. To build upon these discussions, we were inspired by psychological studies  [30] ,  [31]  that have explored frameworks and methods for measuring complex emotions that arise from more primary emotional states. Additionally, with the increasing demand for personalized speech generation, we aimed to address the challenges in TTS models by focusing on achieving high generalization capability and producing high-quality speech. In this article, we present the following key contributions:\n\n• We introduce an emotion-adaptive coordinate transformation that models the emotion-adaptive spherical vector (EASV), enabling more interpretable and controllable synthesis of emotion style and intensity. • We introduce a joint attribute style encoder along with an additional disentanglement module, enabling the model to perform emotion transfer even in zero-shot scenarios where reference speakers and emotions are not explicitly labeled.\n\n• We propose a novel objective evaluation method, spherical vector angle similarity (SVAS), to evaluate overall emotion accuracy while also capturing subtle variations in speech emotion styles with greater precision.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Background And Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Characterization Of Emotions",
      "text": "The processes of defining and expressing emotions have garnered significant interest in psychology  [1] ,  [32] -  [34] . Emotion theorists divide emotion theory into discrete  [30] ,  [35]  and dimensional models  [1] ,  [36] . Discrete models represent emotions as distinct, separate categories, while dimensional models provide a continuous and fine-grained description, capturing the complexity and variability of emotional experiences.\n\nEmotion labels correspond very closely to the categories we use in our daily lives. Paul Ekman  [35]  derived six primary emotions: happiness, anger, disgust, sadness, anxiety, and surprise based on universally recognized facial expressions. However, this approach overlooks the nuanced variations of emotions. For instance, Plutchik's emotion wheel  [30]  proposes eight primary emotions and suggests that all other emotions arise as derivative states of the primary emotions. By adjusting the intensity of primary emotions on the wheel, a broader spectrum of emotional experiences can be represented. Although people can explicitly express emotions, modeling the relationships between discrete emotional states presents a challenge.\n\nResearchers have introduced dimensional models to computationally interpret the relationships between emotional states. Dimensional models represent emotions along three continuous dimensions  [1] ,  [36] : valence represents the positivity or negativity of an emotion, arousal indicates the intensity of the emotion provoked by a stimulus, and dominance denotes the level of control exerted by the stimulus. Russell's circumplex model  [1]  suggests a two-dimensional circular space that spans the independent and bipolar dimensions of arousal and valence. Building on this, researchers have attempted to extend the model to the third dimension of dominance to denote the location of emotion within this space  [36] . In the valence-arousal space, intensity is often equated with arousal; moreover, Reisenzein demonstrated that using the angle and length of the vector in polar coordinates is the only possible option for interpreting the relationships between emotions  [31] ,  [37] . Recently, the speech processing domain has seen various studies on emotion recognition that leverage the emotional dimension  [38] -  [41] . Despite these efforts in psychology, the current literature on speech synthesis is still insufficient in effectively modeling and controlling the subtle variations of emotions. Inspired by several psychological theories  [30] ,  [31] , we hypothesize that the dimensional model allows speech synthesis models to express derived emotions of primary emotions, as shown in Fig.  1 (a) . This approach enables researchers to generate, control, and manipulate a wide range of emotions more easily in real-life applications.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Controllable Emotional Speech Synthesis",
      "text": "Recently, speech synthesis models have exhibited significant developments  [23] ,  [42] ,  [43] ; therefore, controllable emotional speech synthesis research is being aggressively pursued [3]-  [5] . Researchers typically use the following controllable emotional speech synthesis methods to utilize general emotional datasets: 1) emotion label-and 2) reference-based approaches.\n\nThe emotion label-based approach aims to properly model conditioning input to reflect the complex nature of emotions. Researchers typically model emotion intensity using a learned ranking function  [44] , as employed in  [6] -  [11] . The ranking function  [44]  seeks a ranking matrix based on the relationships between the dimensional-driven and different global emotional expressions using support vector machines. The model receives the emotional intensity of emotional samples as a conditioning input for training. However, this method tends to rely on emotion labels and introduces bias into training through separate stages. Most models that use ranking functions adopt fine-tuning to control emotion intensity on a single-speaker dataset; however, some of these methods have noticeably degraded speech quality  [7] ,  [10] . Moreover, certain research studies have utilized conditioning input such as distance-based quantization  [12]  and VUS states  [13]  to model emotional intensity. However, these methods are still limited to several predefined emotion labels and lack differentiation among samples within the same emotion label.\n\nAs emotional speech synthesis often lacks multiple emotional style labels, reference-based approaches are famous for using reference audio to transfer emotional styles. Several studies have controlled emotion intensity through operations on representative emotion embedding. The scaling factors approach  [14] ,  [15]  reflects fine-grained emotion representation through multiplication. In addition to the scaling approach, the interpolation approach proposed by  [16]  controls emotion intensity through an inter-to-intra emotional distance ratio algorithm. Despite these techniques, the structure of the embedding space influences model performance and complicates the process of finding optimal parameters for scaling or interpolation.\n\nHowever, these methods cannot be tuned explicitly like label-based approaches, nor do they capture the fine-grained emotion representations achievable by reference-based methods. EmoSphere-TTS  [3]  solves this by proposing a spherical emotion vector to control the emotional style and intensity of the synthetic speech. However, the lack of consideration for emotion category distribution in emotion style and intensity modeling can lead to unnatural variations in certain styles and intensities. This study addresses the lack of emotion category distribution-based modeling by explicitly modeling emotion style and intensity variations based on EmoSphere-TTS  [3] , thereby bridging this research gap.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Style And Emotion Transfer In Text-To-Speech",
      "text": "The TTS research community has long explored methods for style and emotion transfer. A significant advancement came with the introduction of global style tokens (GSTs)  [45] , which provided a framework for capturing and transferring speaking styles. One such derivative is Mellotron  [46] , an autoregressive multi-speaker TTS model based on Tacotron that utilizes GSTs. Li et al.  [15]  proposed a module for disentangling and transferring emotions across different speakers to achieve a desired emotional tone while maintaining the identity of the speaker. However, these methods exhibit limitations in capturing different styles and zero-shot scenarios owing to the restricted speaker lookup table and the insufficient performance of the disentangling modules.\n\nIn response, iEmoTTS  [47]  and YourTTS  [48]  used pretrained speaker embedding for robust zero-shot performance. Additionally, GenerSpeech  [49]  proposed a multi-level style adapter to obtain different styles, including a global latent representation with speaker and emotion features. However, previous research has lacked methods to effectively process style and disentangle speech factors. Our work handles wellformed style processing through joint attribute style encoders and incorporation of additional loss to exhibit strong generalization performance for zero-shot scenarios.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Speaker And Emotion Feature Disentanglement Methods",
      "text": "Some prosodic features are inherently associated with the speaker's identity, making complete disentanglement challenging. Therefore, an effective disentangling method is critical to adequately separate speaker identity from emotion-related prosodic features, ensuring clear and accurate emotional transfer without compromising the target speaker's timbre. Researchers typically implement the disentanglement module via explicit labels, such as the gradient reversal layer (GRL)  [50] , as employed in  [22] ,  [47] . However, using explicit labels for disentanglement introduces a trade-off between preserving emotional information and separating speaker identity, making hyperparameter optimization challenging and leading to suboptimal performance and synthesis quality  [15] . Vector quantization (VQ)  [51]  offers an alternative approach to separating information without relying on explicit labels. Although VQ is effective in separating information without explicit labels  [47] ,  [49] , it often results in unintended information loss and requires complex optimization to balance compression with reconstruction quality. To address these issues,  [15]  proposed an orthogonal loss to compensate for the embedding of emotion for the loss of emotional information caused by the disentanglement of speaker information  [52] . Building on these advancements, we aim to minimize information loss during disentanglement using an orthogonal loss-based approach while enhancing the expressiveness of synthesized speech.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Iii. Proposed Method",
      "text": "This paper introduces EmoSphere++, an emotioncontrollable zero-shot TTS model that can control emotional style and intensity to resemble natural human speech. Our work is grounded in the emotion wheel theory  [30] , which suggests that all other emotions are derived from the primary emotions, and the circumplex model  [1] ,  [37] , representing emotions through coordinate transformations based on intensity. We propose that derivative emotions can be characterized by the coordinate transformation in the valence-arousal-dominance (VAD) dimension, reflecting the style and intensity of the primary emotions, as shown in Fig.  1 (c ).\n\nBuilding on this, EmoSphere-TTS  [3]  was previously introduced to model emotion style and intensity using a spherical emotion vector. However, the lack of consideration for emotion category distribution can lead to unnatural variations in certain styles and intensities. To address this, we propose an emotionadaptive coordinate transformation that better models diverse emotional styles and intensities. Additionally, we introduce a joint attribute style encoder to enable emotion-controllable zero-shot TTS across a broader range of emotions, overcoming the limitations of relying on predefined emotion and speaker labels, which restrict flexibility in emotional expression. Furthermore, we achieve competitive performance solely through a conditional flow matching (CFM)-based decoder, eliminating the need for an additional discriminator module while enhancing emotional expressiveness and speech quality. The details of our approach are defined in the following subsections.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Emotion-Adaptive Coordinate Transformation",
      "text": "Several studies  [3] ,  [7] ,  [10] ,  [12]  assume that emotional intensity decreases when approaching a neutral state and use this as the basis for modeling emotion intensity. As shown in Fig.  2 (a) , previous studies  [3]  defined the center coordinates based on this assumption, where the intensity of emotion decreases as it approaches the neutral emotion center M , formulated as follows:\n\nwhere N n is the total number of neutral coordinates e n i . However, since the mean-based method does not account for the distribution of other emotions, such as variance, it fails to fully capture the relationship between the neutral emotion and the target emotion. Specifically, we define other emotions as all emotions except the neutral emotion and the target emotion as a specific emotion selected from these other emotions. To address this, our approach simultaneously considers the distributions of the neutral emotion and the corresponding target emotion, extracting an adaptive spherical vector for each target emotion.\n\nOur method models a spherical coordinate system for each target emotion by considering the distribution of the neutral emotion and its corresponding target emotion, as shown in Fig.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "(B). Our Approach Is Based On Two Fundamental Assumptions:",
      "text": "(1) the emotional intensity increases as it moves farther from the center of the emotion-adaptive spherical coordinate system and (2) the angle from the center of the emotion-adaptive spherical coordinate determines the emotional style. Initially, Compute centroid coordinate as M k by Eq. (  3 )\n\nCompute shifted VAD as e k i by Eq. (  4 ) Spherical transformation as s k i ∈ S k by Eq. (  5 )",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "End For",
      "text": "Calculate (r min , r max ) the interquartile range of r for s k i in S k do:\n\nCompute r IQR by Eq. (  6 )\n\nend for end for we adopted a specific emotional attribute prediction model  [53]  ψ to predict the VAD value e k i in emotion class k:\n\nHere e k i and e n i denote the i-th coordinate of the k-th target emotion coordinate set E k and the neutral coordinate set E n , respectively. Consequently, the centroid coordinates are the ones that maximize the distances from the target emotion category while minimizing the distance within the neutral emotion category. Then, transformation via the representative central coordinates M k to spherical coordinates s k i = (r, ϑ, φ) can be formulated as follows:\n\nAfter the emotion-adaptive coordinate transformation, we applied the interquartile range (IQR) technique  [54]  to adjust the data based on the median, thereby reducing the influence of outliers as follows:\n\nHere r min and r max are bounds derived based on the IQR technique with r min set as the first quartile minus 1.5 times the IQR and r max as the third quartile plus 1.5 times the IQR. The detailed procedure for obtaining the emotion-adaptive spherical vector set S is outlined in Algorithm 1.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Joint Attribute Style Encoder",
      "text": "The voice typically contains highly dynamic style attributes (e.g. speaker identities, prosody and emotions), making the TTS model difficult to model and transfer in a zero-shot scenario. As shown in Fig.  3 , we propose a joint attribute style encoder for both broad and fine-grained stylization.\n\nThe emotion encoder includes a fine-tuned categorical emotion recognition model for global emotion features and an additional EASV extractor for dimensional-driven emotion features. The global emotion encoder extracts the fixed-size hidden embedding from the categorical emotion recognition 1    [55] , which utilizes a multilayer transformer to capture comprehensive emotional representations. Meanwhile, the EASV extractor, described in Section III-A, generates dimensional-driven emotion features, which explicitly encode fine-grained variations in emotional expressions (e.g., intensity and style). Finally, a fully connected layer processes global and dimensional-driven emotion features, combining them into a fixed-size hidden embedding.\n\nThe speaker encoder provides speaker-related information to the TTS model. A pretrained speech encoder extracts a speaker embedding  [56]  from the reference speech for zeroshot emotion transfer. We used the WavLM Base model  [57]  as the speaker verification model 2  , building the model on the HuBERT  [58]  framework to focus on modeling speech content and preserving speaker identity. WavLM-based approaches can capture and represent speaker-specific information and emotional nuances in speech  [59] ,  [60] . Similar to emotion embeddings, fixed-size hidden embeddings are processed in the fully connected layer and combined to generate joint attribute style embedding e sty .",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "C. Preliminary On Conditional Flow Matching-Based Model",
      "text": "EmoSphere++ adopts a CFM-based decoder that generates flow through the ordinary differential equation. Building on the success of flow matching in the speech synthesis task  [26] -  [28] , we utilized a CFM-based decoder that is designed to model a conditional vector field u t . Following  [61] , we define the flow ϕ as the mapping between two density functions:\n\nHere, v t represents a time-dependent vector field that defines the path of the probability flow over time t ∈ [0, 1]. Specifically, it describes a conditional flow process in which the conditional flow ϕ t,x1 represents simple linear trajectories between the data point x 1 drawn from the target distribution q(x) and prior distribution x 0 ∼ N (0, I):\n\nwhere σ min is the hyper-parameter for small amounts of white noise. The vector field in the decoder is trained using the following objectives:\n\nwhere µ represents the predicted average acoustic features (e.g.,  Mel-spectrogram)  given the text and the chosen durations, using a text encoder and duration predictor. e sty denotes the joint attribute style embedding.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "D. Training Objective",
      "text": "Alongside traditional losses for training TTS systems, we introduce a disentanglement method to help in modeling our system to control and transfer emotion style and intensity.\n\nFirst, text encoder and duration predictor architectures were implemented following  [26] . Duration-model training uses monotonic alignment search to compute the duration loss L dur and the prior loss L enc , as described in  [62] . The decoder follows the CFM loss L cf m described in Section III-C.\n\nInspired by  [15] , we introduced an additional orthogonality loss of the disentanglement method. In the style encoder, emotion and speaker embeddings contain overlapping information regarding each other. The speaker and emotion embeddings should be 1) discriminative in distinguishing identities and 2) independent of each other, ensuring effective generalization performance for both seen and unseen speakers. To mitigate the impact of speaker and emotion embedding leakage on model performance, we propose a normalized orthogonality loss L ort to enhance the decoupling capability of the model. Unlike the existing loss  [15]  applied only to embeddings from the same audio, this method normalizes all sample pairs to enhance generalization in zero-shot scenarios. In this case, L ort can be expressed as: where n is the batch size, and the Frobenius norm ∥•∥ is used to calculate the interaction between the emotion embedding e i and speaker embedding s j for all pairs of samples (i, j).\n\nConsequently, the final objective function is defined as:\n\nwhere λ enc , λ cf m , λ dur , and λ ort are the loss weights, which we set to 1.0, 1.0, 1.0, and 0.02, respectively.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "E. Control Of Emotional Style And Intensity",
      "text": "Fig.  4  illustrates the proposed emotion-controllable zeroshot TTS framework, which synthesizes emotional speech for both seen and unseen speakers based on reference speech and EASV. The framework comprises three main modules: the text encoder, joint attribute style encoder, and CFM decoder.\n\nThe joint attribute style encoder captures the emotion and speaker information in an embedding from the reference speech. By varying the length and angle in the EASV, we can manipulate the levels of style and intensity at runtime and efficiently synthesize the desired emotional effects. In a zeroshot scenario, the framework can synthesize speech for unseen speakers by inputting the unseen target reference speech into the speaker encoder.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Iv. Experiments A. Experimental Setup",
      "text": "We conducted experiments using the emotional speech dataset (ESD) 3    [63] , which contains 350 parallel utterances spoken by ten English speakers in five emotional states (neutral, happy, angry, sad, and surprise). Following the prescribed data partitioning criteria, we extracted one sample for each emotion from every speaker, resulting in 17,500 samples. The validation set comprised 20 samples for each emotion per speaker, totaling 1,000 samples, whereas the test set comprised 30 samples for each emotion per speaker, totaling 1,500 samples. The zero-shot scenario used two unseen speakers, one English-speaking male (\"0013\") and one English-speaking female (\"0019\"), by excluding them from the training process.\n\nMoreover, we utilized the MSP-Podcast corpus dataset  [64]  and the Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset  [65] , along with the ESD dataset, to analyze the prosodic variation of EASV and to verify whether the models can reflect the styles using predicted VAD values. The MSP-Podcast corpus dataset comprises approximately 237 hours of speech data annotated with both categorical emotion labels and dimensional VAD values. The training set includes eight categorical emotion classes (happiness, sadness, fear, surprise, contempt, disgust, and neutral) collected from 454 speakers. For dimensional emotion labels, raters evaluated VAD using a seven-point Likert scale. The IEMOCAP contains ten speakers with nine emotions (angry, excited, fear, sad, surprised, frustrated, happy, disappointed, and neutral) and dimensional labels such as VAD. For analyzing the prosodic variation of EASV, we used the MSP-Podcast and IEMOCAP datasets while ensuring consistency with ESD by selecting only five categorical emotion labels that match those in the ESD dataset. Additionally, we used the IEMOCAP dataset to verify whether the models can reflect styles using predicted VAD values. The validation set included ten samples per speaker, while the test set comprised 15 samples per speaker.\n\nFor the Mel-spectrogram, we transformed audio using the short-time Fourier transform with a hop size of 256, a window size of 1,024, an fast Fourier transform size of 1,024, and 80 Mel bins. We converted the text to phoneme using the grapheme-to-phoneme tool of the Festival Speech Synthesis System  [66]  to serve as the input to the text encoder. We designed parallel and non-parallel test scenarios at runtime depending on whether the input text matches the reference speech.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "B. Implementation Details",
      "text": "For the acoustic model, we followed the Matcha-TTS  [26]  configuration, utilizing the text encoder and duration predictor in the encoder, along with a 1D U-Net based CFM decoder. The CFM decoder consists of two downsampling blocks, followed by two midblock and two upsampling blocks, each containing a Transformer layer with a hidden dimensionality of 256, an attention module with dimensionality of 64, and \"snakebeta\" activations  [67] . Following  [68] , the text encoder was modified by incorporating relative position representations and adding a residual connection to the encoder pre-net. Based on  [69] , the duration predictor consists of two convolutional layers with rectified linear unit activation, followed by layer normalization, dropout, and a projection layer. Regarding the emotional attribute prediction  4  , we adopt a system proposed in  [53] , which predicts VAD using wav2vec 2.0  [70]  and a linear predictor. In the joint attribute style encoder, the emotion and speaker global encoders utilize emotion2vec model  5    [55]  and the WavLM-based speaker verification model  6  , respectively.\n\nDuring training, both global encoders are frozen and extract hidden embeddings, which are then passed through a twolayer fully connected network. We trained the generator using random segments of 32 frames from the Mel-spectrogram, with a batch size of 32 and a total of 11M training steps. The AdamW optimizer was used, with a 1 × 10 -4 learning rate. In the inference stage, the guidance level γ was set to 100. We trained the vocoder using the official BigVGAN 7  [67]  implementation, incorporating LibriTTS  [71] , voice cloning toolkit 8 , and ESD datasets. All comparison models were trained using a single NVIDIA RTX A6000 GPU.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "C. Evaluation",
      "text": "1) Subjective Metrics: We adopted two subjective metrics: 1) a mean opinion score (MOS) evaluation and 2) a preference test to evaluate emotion expressiveness. We conducted subjective evaluations using Amazon Mechanical Turk. All subjects were required to listen with headphones and replay each sample 2-3 times.\n\nWe conducted a MOS evaluation for naturalness (nMOS), speaker similarity (sMOS), and emotion similarity (eMOS) using a nine-point scale ranging from 1 to 5, with increments of 0.5 units. The results are presented with a confidence interval of 95%. 20 subjects evaluated the samples by assessing the full set of extracted samples, where two samples were randomly selected for each emotion and speaker combination from the entire test set of 100 samples (2 × 5 (# of emotions) × 10 (8 seen speakers and 2 unseen speakers)), ensuring consistency across models.\n\nWe also conducted a preference test to evaluate the modeling of emotion expressiveness. To demonstrate the success of our modeling, we synthesized speech with three different levels of emotion intensity (weak, medium, and strong) from pairs of speech that share the same emotion and style. Evaluators were presented with two different sentences with the same emotion and style, each with varying intensities, and tasked with selecting the one exhibiting a stronger emotion. We uniformly referred to scores 0.1 as weak, 0.5 as medium, and 0.9 as strong. 20 subjects evaluated the samples by assessing the full set of extracted paired samples, where two pairs (four samples) were randomly selected for each emotion and speaker combination from a total of 200 samples in the test set (2 × 2 (pairs) × 5 (# of emotions) × 10 (8 seen speakers and 2 unseen speakers)), ensuring consistency across models.\n\n2) Objective Metrics: To evaluate linguistic consistency, we calculated the word error rate (WER) using the Whisper large model  [72]  (WER Whis ) or wav2vec model  [70]  (WER w2v ). For WER AVG , we computed the average of the WER obtained from both the Whisper and wav2vec models. For the speaker similarity measurements, we calculated the speaker embedding cosine similarity via Resemblyzer 9 (SECS R ) or WavLM 10 (SECS W ) between the target and converted speech. For SECS AVG , we computed the average of the speaker embedding cosine similarities obtained from both the Resemblyzer  and WavLM models. For prosodic evaluation, we computed the root mean square error for both pitch error (RMSE f0 ) and periodicity error (RMSE period ), along with the F1 score of voiced/unvoiced classification (F1 v/uv ). For emotionally expressive evaluation, we determined the emotion classification accuracy (ECA) using a prebuilt emotion classification model emotion2vec  [55]  and the emotion embedding cosine similarity (EECS)  [22]  by computing the cosine similarity of the emotion2vec hidden emotion embedding between the synthesized audio and arbitrary reference audio with the target emotion. We used emotion2vec+ base 11 , a pretrained model that supports nine classes, and only used the five sentiment classes in the ESD dataset for evaluation. Moreover, we propose the spherical vector angle similarity (SVAS) to evaluate 11 https://github.com/ddlBoJack/emotion2vec the emotion of synthesized speech. The SVAS is obtained by computing the cosine similarity of the angle of the emotion spherical vector between the synthesized audio and reference audio with the target emotion. We used an emotional attribute prediction model 12    [53]  to predict the VAD values and applied a Cartesian-to-spherical transformation through fixed neutral center coordinates to extract the angle of the emotion spherical vector. Unlike conventional metrics, SVAS not only captures global emotional information but also provides a fine-grained evaluation of subtle emotional variations, enabling a more comprehensive assessment of synthesized speech emotions. All evaluations were conducted using the official test set of the ESD dataset  13    [63]  to ensure a standardized evaluation.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "D. Comparison Models",
      "text": "We compared the similarity and quality of samples generated using the proposed EmoSphere++ with those produced by other systems. We used the same vocoder and open code, and the comparative models are summarized as follows:\n\n• GT and BigVGAN  [67] : The ground truth (GT) audio and waveforms are generated from the ground truth Melspectrogram using vocoder.\n\n• Mellotron  [46] : This auto-regressive multi-speaker TTS model allows for direct style control by conditioning on rhythm and pitch utilizes conditioning GSTs.\n\n• Mixedemotion  [10] : A relative attribute ranking-based model that pre-computes intensity values for mixed emotion synthesis and allows manual control at run time.\n\n• YourTTS  [48] : A zero-shot multi-speaker TTS with the pre-trained speaker encoder. Unlike other comparison models, it does not use a pre-trained vocoder through end-to-end training.\n\n• GenerSpeech  [49] : A high-fidelity zero-shot style transfer method for out-of-domain TTS. We used the emo-tion2vec model instead of the fine-tuned wav2vec 2.0 model to capture global style for a fair comparison.\n\n• iEmoTTS  [47] : A non-autoregressive TTS model for a cross-speaker emotion transfer system developed based on timbre-prosody disentanglement. • EmoSphere++: Our proposed CFM-based emotioncontrollable zero-shot TTS with EASV.\n\nTo ensure a fair comparison between the proposed method and existing emotion intensity modeling approaches, we trained the models using the same dataset and configurations as Matcha-TTS  [26] , including the encoder (i.e., text encoder and duration predictor) and the U-Net-based decoder. We replaced the speaker and emotion attribute control module for a contrastive study with two competing controllable emotion methods: scaling factor and relative attributes through comprehensive experiments.\n\n• Matcha-TTS w/ Scaling Factor: Here, the emotion embedding, extracted through the emotion disentangling module, is multiplied by a scaling factor to control the emotion strength  [15] . The system manages the speaker identity controller using speaker embeddings obtained from a speaker look-up table. • Matcha-TTS w/ Relative Attributes: Here, the relative attributes vector is obtained from a ranking function to control the emotion strength  [7] . We employed the same emotion encoder and speaker look-up table, except for fine-tuning the emotion encoder with a single-speaker emotion dataset. In summary, we conducted emotion transfer experiments to compare EmoSphere++ with other systems, including Mellotron, Mixedemotion, YourTTS, GenerSpeech, and iEmoTTS. Moreover, we evaluated emotion intensity control by comparing it with other intensity control methods, such as Matcha-TTS w/ Scaling Factor and Matcha-TTS w/ Relative Attributes.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "V. Results",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "A. Analysis Of Emotion-Adaptive Spherical Vectors For Emotional Style And Intensity",
      "text": "As previously mentioned, we characterized the derivative states of emotions through emotion style and intensity using the emotion-adaptive spherical vector (EASV). To evaluate the effectiveness of modeling based on emotion style and intensity, we conducted an analysis using the ESD dataset along with large-scale datasets, including MSP-Podcast corpus  [64]  and IEMOCAP datasets  [65] . In this study, we divided the emotion space into eight regions (\"I\"∼\"VIII\") based on the VAD axes and then shifted the style along spherical spaces,  Fig.  5 . Pitch tendency track according to intensity for different emotions. Pitch values were calculated by averaging the synthesized speech for each intensity across all test sentences. Since the intensity of ground truth (GT) speech cannot be adjusted, the GT line represents the pitch tendency based on the emotionadaptive spherical vector intensity labels across all test sentences, serving as a reference guideline. Fig.  6 . Pitch tendency track according to intensity for an unseen speaker.\n\nusing only the five categorical emotion labels from the ESD dataset for consistency. As shown in Table  I , we illustrate the prosodic variation and distribution of large-scale emotional speech datasets based on emotion style and intensity modeling. The analysis details are described in the following subsections.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "1) Analysis Of Emotion Dataset Distribution By Style And Intensity:",
      "text": "To demonstrate the diverse modeling of emotions, we analyzed the distribution of emotion styles and intensities using EASV modeling. We observe that each emotion exhibits a range of styles and intensities, reflecting the natural variability in emotional expression. By analyzing the number of modeled instances, we find that the spherical vectors representing each emotion tend to cluster around specific styles that are frequently associated with that emotion. This suggests that certain emotion styles are more commonly expressed in realworld speech. For example, in the case of surprise, a higher pitch is more frequently observed than a lower pitch. However, for styles with fewer data instances, the distribution showed less distinct patterns, likely due to the limited availability of data. In summary, the results support the assumption that speech emotion analysis should account for diverse styles based on primary emotional states.\n\n2) Prosodic Variation with Intensity Based on the Valence-Arousal-Dominance (VAD) Axis: In this section, we describe the analysis of how prosodic variation varies with emotion intensity based on the VAD axis. We represent the region divided into thirds by intensity with Q1, Q2, and Q3, using 0.33 and 0.66 as thresholds. In psychology  [1] ,  [36] , valence represents the positivity or negativity of an emotion, arousal indicates the intensity of the emotion provoked by a stimulus, and dominance denotes the level of control exerted by the stimulus. Building on this, we show how prosodic variations reflect diverse emotions through emotion style and intensity along the VAD axis.\n\nExisting studies  [11]  on emotion control have demonstrated the effectiveness of analyzing prosodic features such as pitch, energy, and duration. Expanding on this approach, we conducted a prosodic analysis and found that: 1) positive valence is associated with higher prosodic feature value as the emotional intensity increases. 2) positive arousal leads to increased patterns of prosodic feature value change. 3) positive dominance results in a narrower prosodic variation range. To further analyze and clearly illustrate this prosodic variation, we introduced three key elements in our analysis. First, to compare differences based on valence, we calculated the average prosodic feature values AVG. for each emotion style. Second, to observe increases and decreases related to arousal, we highlighted the highest and lowest prosodic feature values within each intensity level using green and red colors, respectively. Lastly, to examine the variation ranges influenced by dominance, we introduced Qc, which represents the absolute difference between the highest and lowest prosodic feature values. These measures provide a more comprehensive understanding of how prosodic features vary with emotional style and intensity.\n\nTo validate these findings, we examined specific cases where emotion styles differ along a single VAD dimension while remaining constant in the others. First, when comparing styles \"III\" and \"IV\", where only valence differs, we observe that positive valence is associated with higher average prosodic feature values AVG.. Specifically, across all emotions, converting to positive valence results in an average increase of 13.45 in pitch, 1.4 in energy, and 0.25 in duration. Similarly, when comparing styles \"I\" and \"V\", where only dominance differs, we find that Qc is smaller in the positive dominance condition. Specifically, converting to positive dominance results in an average decrease of 6.78 in pitch, 0.35 in energy, and 0.02 in duration across all emotions. Lastly, the positive arousal style of \"I\" exhibits increasing prosodic patterns, whereas the negative arousal style of \"IV\" shows decreasing patterns. These results show that prosodic variations align with the VAD characterization in most cases, supporting the effectiveness of EASV in modeling emotion style and intensity.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "B. Model Performance",
      "text": "We conducted experiments including seen and unseen speaker scenarios to evaluate EmoSphere++ and baseline models for non-parallel style transfer. We split our experiments into two categories: 1) seen non-parallel style transfer and 2) unseen non-parallel style transfer.\n\n1) Seen Non-Parallel Style Transfer: We first demonstrate the robustness of our proposed model in seen non-parallel style transfer, where a TTS system synthesizes a seen speaker. We select another fixed pair of reference signals from the test set of the same emotion and speaker. As shown in Table  II , EmoSphere++ outperforms the previous methods in terms of both subjective and objective evaluations. In terms of naturalness and linguistic consistency, EmoSphere++ achieves the highest nMOS with a score of 3.92 and performed strongly in WER with a score of 15.52 compared to those of the baseline models. Regarding emotion and speaker style similarity, EmoSphere++ scores the highest overall eMOS of 3.86 and sMOS of 3.97. The objective results of the speaker metric SECS and emotion metrics ECA, SVAS, and EECS show that EmoSphere++ outperforms state-of-the-art models in transferring custom speech styles. The proposed method demonstrates superior performance in style transfer and quality compared to previous approaches.\n\n2) Unseen Non-Parallel Style Transfer: Subsequently, we explored the robustness of the proposed model in the case of unseen non-parallel style transfer. We set up zero-shot scenarios with unseen speakers for the ESD dataset and tested how the TTS model reproduces each speaker style when synthesizing different emotional phrases. Mellotron  [46]  and Mixedemotion  [10]  are not suitable for zero-shot scenarios because they rely on speaker lookup tables and are fine-tuned to a single speaker. As shown in Table  III , the result indicates the drop in overall metrics, indicating that adapting to unseen speakers is more complex than adapting to unseen emotions. However, the most consistent performance in the various emotion zero-shot scenarios suggests that EmoSphere++ can adjust to a wide range of unseen emotions.  control methods (i.e., Matcha-TTS w/ Scaling Factor, Matcha-TTS w/ Relative Attributes), as described in Section IV-D.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "C. Emotion",
      "text": "We evaluated the performance of these two methods in terms of performance and control. To demonstrate the ability of our model to control intensity, we synthesized speech with three different levels of emotion intensity (weak, medium, and strong). In the relative attribute model and EmoSphere++, we uniformly refer to values of 0.1 as weak, 0.5 as medium, and 0.9 as strong. The scaling factor cannot assign intensity values; therefore, we set the scalar factor at 1, 2, and 3 to represent weak, medium, and strong emotion intensities, as in the original setting. The GT line reflects the pitch tendency of the original speech, based on inherent EASV intensity labels across all test sentences, serving as a reference guideline.\n\nPerformance. We explored the expressiveness of our proposed model in non-parallel style transfer, where a TTS system synthesizes both the prosodic style of a reference signal and modeling emotion attribute. The results are compiled and presented in Table  IV  for easy comparison. Compared to transferring emotion from label-based emotion with relative attributes and reference-based emotion with a scaling factor, improved speech quality and expressiveness are achieved using EASV.\n\nControl. We calculated the average pitch of the synthesized speech across all test sentences based on the intensity of each emotion. Fig.  5  shows the following: 1) The Matcha-TTS w/ Relative Attributes reflects the adjustment of emotional properties, but also often shows a limited adjustable range and tends to be reduced to a more uniform style. This outcome suggests subtle emotional nuances cannot be easily captured using only emotion labels. 2) The Matcha-TTS w/ Scaling Factor exhibits the most variation, but often becomes unstable when adjusted on labels such as sad. Therefore, determining an appropriate scaling factor is difficult, and adjustments may lead to instability in audio quality. Conversely, the pitch tendency plot of EmoSphere++ closely follows the GT line, reflecting intensity variations based on emotion. This result indicates that the proposed model synthesizes speech according to the given intensity scale while effectively capturing variations that align more closely with natural emotional speech patterns.\n\n2) Intensity Control in the Zero-Shot Scenario: To demonstrate the ability to control emotional expression in the zeroshot scenario, we visualized the tendency of the pitch as shown in Fig.  6 . We calculated the average pitch of the synthesized speech for the unseen speaker across all test sentences based on the intensity of each emotion. The pitch trend graph changes with intensity, reflecting the nature of the emotion. We observe a decrease in pitch for the sad emotion, whereas the pitch tends to increase as intensity rises for other emotions. This pattern indicates that the synthesized speech in EmoSphere++ can control the intensity of each emotion, even for the zero-shot scenario.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "D. Emotion Style Shift 1) Visual Comparisons:",
      "text": "We visualized the prosodic attributes of pitch related to the emotion intensity to gain an intuitive understanding of emotion style. To illustrate the variation patterns in emotion intensity with shifted emotion style, we visualized the pitch track changes for a sample, both from seen and unseen speakers. As analyzed in Section V-A, the prosodic pattern based on emotional intensity reflects the characteristics of the VAD axis. As shown in Fig.  7 , we visualized the pitch contour of sad utterances with the most varied styles. For example, style vectors with positive V axes have higher average pitch values and reduced duration; positive A axes have a pitch that tends to increase the changing patterns. By contrast, duration decreases, and positive D axes have a narrow range in changing patterns with a broader duration. These results indicate that the proposed EASV is meaningfully characterized and enables emotion style and intensity controllable speech synthesis in both seen and unseen.\n\n2) Comparison of Emotional Consistency: Fig.  8  shows the resulting ECA for representative combinations of style shift. Across all emotions, we observe a similar emotional consistency when shifting to the representative style combinations as when maintaining the original style. These results indicate that the proposed model effectively adjusts emotion styles while maintaining emotional consistency across all style transformations. Therefore, as hypothesized in various psychological theories  [30] ,  [31] , spherical vectors of emotion style can be characterized as derivatives of basic emotions.   of the proposed joint attribute style encoder modules and the existing disentangling approaches, we trained the models using the same dataset and configurations as EmoSphere++. We replaced the existing emotion disentangling method with three competing disentangling approaches: 1) w/ Gradient Reversal Layer  [22] , which employs adversarial speaker training using the GRL  [50]  applied after the fully connected layer that follows the global encoder; 2) w/ Vector Quantization  [47] , which implements a bottleneck layer via a modified VQ layer  [51]  applied after the fully connected layer that follows the global encoder; and 3) w/ Orthogonality Loss  [15] , which constrains the emotion embedding and speaker embedding using an orthogonal loss  [52]  without normalizing all sample pairs. To further evaluate the effectiveness of the joint attribute style encoder, we conducted additional ablation studies by removing individual encoder components: w/o Global Emotion Encoder, where the global emotion embedding is excluded and w/o Dimensional Emotion Encoder, where the dimensional-driven emotion embedding is removed. Additionally, to analyze the role of the disentangling method, we trained a variant of the proposed model without the disentangling method, referred to as w/o Disentangling Method. In the w/o Global Emotion Encoder setting, the disentangling method was modified to use the dimensionaldriven emotion embedding in place of the global emotion embedding.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "E. Ablation Study",
      "text": "We used comparative subjective metrics (nMOS, sMOS, and sMOS) and objective metrics (SECS, ECA, and EECS) to assess the expressiveness and quality of the generated speech. The experimental results show that improvements in both quality and expressiveness highlight the effectiveness of disentangling emotion and speaker embeddings, ensuring clearer emotional transfer without compromising speaker identity. Furthermore, our findings demonstrate that integrating global and dimensional-driven features in the joint attribute style encoder enables the model to capture both broad and finegrained characteristics, further enhancing expressive. Specifically, the proposed normalized orthogonality loss is crucial for preserving emotional expressiveness in unseen cases, reinforcing its importance for achieving strong generalization performance in zero-shot scenarios.\n\n2) Effectiveness of Predicted versus Real VAD Values: To compare the impact of emotional attribute prediction models  [53] , we conducted comparative experiments using the IEMO-CAP dataset  [65] , which includes emotion dimension annotations. As a training input for the emotion-adaptive coordinate transformation, we compared two features: real VAD values from human labeled annotations and predicted VAD values from the emotional attribute prediction  [53] . Table  VI  shows that the objective metrics of prosodic expressiveness for both real and predicted VAD values are similar, suggesting that the predicted values perform comparably to manual annotations. Additionally, as shown in Fig.  9 , both the real and predicted VAD values exhibit a consistent and accurate pattern across all emotions, aligning well with the ground truth in intensity control. The results indicate that the VAD values predicted from the emotional attribute prediction  [53]  provide reliable emotion style and intensity modeling comparable to the real VAD values.\n\n3) Comparison of Coordinate Transformation: For a fair comparison, we compared intensity accuracy using speech pairs generated from the same model, with identical emotion and style, differing only in their coordinate transformation approach. As shown in Fig.  10 , all types of emotion intensity pairs (W<M, M<S, W<S) demonstrate accuracy across individual emotions, styles, and overall. We referred to values of W as weak, M as medium, and S as strong. This experiment was conducted to evaluate whether more precise modeling of emotion style and intensity leads to clearly distinguishable synthesized speech when given pseudo-labels as input. By assessing the accuracy of intensity differentiation, we aim to validate the effectiveness of EASV in generating perceptually distinct emotional variations. The EASV method demonstrates consistently high average accuracy for individual emotions and overall. These results suggest that, while the mean-based approach of SEV captures the emotion to a certain extent, it remains unstable at specific intensity levels and styles. This indicates that considering the distribution of other emotional categories in EASV further improves the modeling of emotion style and intensity.\n\nVI. DISCUSSION This study represents an initial attempt at modeling and synthesizing emotion styles and intensities for controllable emotional speech synthesis. Although we have demonstrated the effectiveness of our method, some related issues remain unresolved. We discuss these issues and aim to inspire future studies.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "A. Limitations Of Data Imbalance",
      "text": "As mentioned in Section V-A, the emotional styles modeled in each spherical vector exhibit unique characteristics that are confined to specific emotions. These results indicate that people tend to express particular emotional styles more frequently in reality; therefore, we focused on using only representative styles. This simplification may pose a limitation in capturing the full range of emotional nuances. However, researchers can address this issue by expanding the model to include more diverse datasets. Moreover, this serves as the initial attempt to model and synthesize emotion styles and intensities, demonstrating the potential of this approach.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "B. Remaining Challenges Of Emotional Attribute Prediction Model",
      "text": "As we summarized the emotional attribute prediction models in Section III-A, we utilized the fine-tuned wav2vec 2.0 for an emotional attribute prediction  [53]  task. In the study, VAD predicted by the wav2vec 2.0-based emotional attribute prediction model  [53]  was significant. Hence, we utilized VAD pseudo-labels to avoid the inherent subjectivity and high costs associated with data collection. However, the approach relies on the performance of the trained emotional attribute prediction model and contains similar biases and challenges to those encountered in emotional attribute prediction  [53] . We expect the extended VAD predictor  [24]  to mitigate the biases inherent in emotional attribute prediction while providing more accurate VAD estimations, ultimately addressing this issue.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Vii. Conclusion",
      "text": "In this paper, we presented EmoSphere++, an emotioncontrollable zero-shot TTS model that can control emotional style and intensity to resemble natural human speech. To achieve this, we proposed the novel emotion-adaptive spherical vector (EASV), which models emotional style and intensity as derivatives of primary emotions. Building on this, our comprehensive analysis validates that a dimensional model can characterize emotional states in relation to primary emotions. Additionally, a zero-shot speech synthesis framework with rich expressiveness and controllability was developed, utilizing a joint attribute style encoder with additional loss functions, without being restricted by predefined speaker and emotion labels. The experimental results thoroughly analyze the components of our model and demonstrate its ability to effectively synthesize and control speech performance, even in zero-shot scenarios. We demonstrated that by controlling the spherical vector along the VAD axis, explicit adjustments to emotional style and intensity enable fine-grained emotional expression. Ablation studies further confirm that the proposed EASV effectively synthesizes the complex nature of emotion. While this article only focused on studying emotion-controllable TTS for a limited set of emotions, our proposed spherical vector can enable complete emotion control in most existing emotional speech synthesis frameworks. Future work will expand these experiments to include emotional voice conversion.",
      "page_start": 14,
      "page_end": 14
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (a) Three-dimensional valence-arousal-dominance (VAD) cubes of",
      "page": 1
    },
    {
      "caption": "Figure 2: Illustration of coordinate transformations: (a) Cartesian-to-spherical coordinate transformation [3] and (b) Emotion-adaptive coordinate transformation.",
      "page": 3
    },
    {
      "caption": "Figure 1: (a). This approach enables researchers to generate,",
      "page": 3
    },
    {
      "caption": "Figure 3: Training diagram of the EmoSphere++ framework. The framework",
      "page": 4
    },
    {
      "caption": "Figure 2: (a), previous studies [3] defined the center coordinates",
      "page": 4
    },
    {
      "caption": "Figure 2: (b). Our approach is based on two fundamental assumptions:",
      "page": 4
    },
    {
      "caption": "Figure 3: , we propose a joint attribute",
      "page": 5
    },
    {
      "caption": "Figure 4: Run-time diagram of the proposed EmoSphere++ framework. We can",
      "page": 6
    },
    {
      "caption": "Figure 4: illustrates the proposed emotion-controllable zero-",
      "page": 6
    },
    {
      "caption": "Figure 5: Pitch tendency track according to intensity for different emotions. Pitch values were calculated by averaging the synthesized speech for each intensity",
      "page": 10
    },
    {
      "caption": "Figure 6: Pitch tendency track according to intensity for an unseen speaker.",
      "page": 10
    },
    {
      "caption": "Figure 7: Pitch tracks of a sample demonstrating the effects of emotional style",
      "page": 11
    },
    {
      "caption": "Figure 8: Comparison of the emotion classification accuracy scores of shifting",
      "page": 11
    },
    {
      "caption": "Figure 5: shows the following: 1) The Matcha-TTS w/",
      "page": 12
    },
    {
      "caption": "Figure 6: We calculated the average pitch of the",
      "page": 12
    },
    {
      "caption": "Figure 9: Pitch tendency track according to intensity for each emotion. Pitch values were calculated by averaging the synthesized speech for each intensity across",
      "page": 13
    },
    {
      "caption": "Figure 9: , both the real and predicted",
      "page": 13
    },
    {
      "caption": "Figure 10: Evaluation process rates the discriminability of synthesized speech",
      "page": 14
    },
    {
      "caption": "Figure 10: , all types of emotion inten-",
      "page": 14
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "Bitter",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "Distan",
          "Column_7": "t E",
          "Column_8": "",
          "Column_9": "xcite",
          "Startled\nd Confu\nSurprise": "d C\nSurprise",
          "Column_11": "",
          "Column_12": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "An",
          "Column_4": "",
          "Column_5": "gry",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Startled\nd Confu\nSurprise": "",
          "Column_11": "",
          "Column_12": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "Cri",
          "Column_4": "tical",
          "Column_5": "Aggr",
          "Column_6": "essive",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "Perp",
          "Startled\nd Confu\nSurprise": "lexe",
          "Column_11": "dAm",
          "Column_12": "azed\ned"
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "Play",
          "Startled\nd Confu\nSurprise": "ful",
          "Column_11": "erest",
          "Column_12": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "T",
          "Column_8": "rustin",
          "Column_9": "g",
          "Startled\nd Confu\nSurprise": "Int",
          "Column_11": "",
          "Column_12": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "Neut",
          "Column_7": "ral",
          "Column_8": "",
          "Column_9": "Hap",
          "Startled\nd Confu\nSurprise": "py",
          "Column_11": "",
          "Column_12": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "peac",
          "Column_9": "eful",
          "Startled\nd Confu\nSurprise": "powe",
          "Column_11": "rful",
          "Column_12": ""
        },
        {
          "Column_1": "",
          "Column_2": "Des",
          "Column_3": "pair",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Startled\nd Confu\nSurprise": "",
          "Column_11": "",
          "Column_12": ""
        },
        {
          "Column_1": "Guilty\nSad\nLo\nVulnerable",
          "Column_2": "y\nSad",
          "Column_3": "",
          "Column_4": "Hur",
          "Column_5": "t",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Startled\nd Confu\nSurprise": "",
          "Column_11": "",
          "Column_12": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "Val",
          "Column_9": "",
          "Startled\nd Confu\nSurprise": "ence",
          "Column_11": "",
          "Column_12": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "nely",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Startled\nd Confu\nSurprise": "",
          "Column_11": "",
          "Column_12": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "𝑰=",
          "Column_8": "𝟎.𝟕𝟓",
          "Column_9": "",
          "𝑰=𝟏": "Sad",
          "Column_11": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "𝑰=",
          "Column_7": "𝟎.𝟓",
          "Column_8": "",
          "Column_9": "",
          "𝑰=𝟏": "",
          "Column_11": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "𝑰=𝟎.",
          "Column_5": "",
          "Column_6": "𝟐𝟓In",
          "Column_7": "tensit",
          "Column_8": "y Con",
          "Column_9": "",
          "𝑰=𝟏": "trol",
          "Column_11": ""
        },
        {
          "Column_1": "",
          "Column_2": "𝑰=",
          "Column_3": "𝟎",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "𝑰=𝟏": "",
          "Column_11": ""
        },
        {
          "Column_1": "",
          "Column_2": "Neutr",
          "Column_3": "",
          "Column_4": "al",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "𝑰=𝟏": "",
          "Column_11": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "L": "𝑰=",
          "onely": "𝟎.𝟕𝟓",
          "𝑰=𝟏": "",
          "Column_8": "Style",
          "Column_9": "Shift"
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "𝑰=",
          "L": "𝟎.𝟓",
          "onely": "",
          "𝑰=𝟏": "",
          "Column_8": "",
          "Column_9": "Sad"
        },
        {
          "Column_1": "",
          "Column_2": "𝑰=𝟎",
          "Column_3": "",
          "Column_4": ".𝟐𝟓I",
          "L": "ntensi",
          "onely": "ty Co",
          "𝑰=𝟏": "ntrol",
          "Column_8": "",
          "Column_9": ""
        },
        {
          "Column_1": "𝑰=",
          "Column_2": "𝟎",
          "Column_3": "",
          "Column_4": "",
          "L": "",
          "onely": "",
          "𝑰=𝟏": "",
          "Column_8": "",
          "Column_9": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "L": "",
          "onely": "",
          "𝑰=𝟏": "",
          "Column_8": "Hurt",
          "Column_9": ""
        },
        {
          "Column_1": "Neutr",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "L": "",
          "onely": "",
          "𝑰=𝟏": "",
          "Column_8": "",
          "Column_9": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Cartesian Coordinate\nDominance(0,0,1) (0,1,1)\n(1,0,1) (1,1,1)\n(0,0,0) (0,1,0)\nValence\n(1,0,0) (1,1,0)\nArousal\nNeutral Angry Happy Sad Surprise": "",
          "Spherical Coordinate\nDominance 𝜽\n(𝒓,𝜽,𝝋)\n𝑴 𝒓\n𝑴 𝝋": "Valence\nArousal\nAngry Happy Sad Surprise"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "𝒆𝒔𝒕𝒚",
          "Column_2": "",
          "𝒕 𝑿": "",
          "𝒕": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "Dimensional-Driven Emotion\nVAD Extractor\nEmotion-Adaptive\nCoordinate Transformation"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Global-level Emotion\nEmotion\nEncoder": "Local-level Emotion\nVAD Extractor\nEmotion-Adaptive\nCoordinate Transformation\nManual Emotion-Adaptive\nSpherical Vector\n(Intensity: weak, Style: VI)",
          "Column_2": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dimensional-Driven Emotion\nManual Controlled\nSpherical Vector\nVAD Extractor\nor\nStyle (Angle): IV (𝜋,−𝜋)\n4 4\nEmotion-Adaptive\nCoordinate Transformation Intensity (Length): weak (0.1)": "",
          "Column_2": "Style (Angle): IV (𝜋,−𝜋)\n4 4\nIntensity (Length): weak (0.1)",
          "Column_3": "",
          "Column_4": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "All(Average)": "I(+V+A+D)",
          "-": "0.4",
          "41,845": "3,649",
          "48.7": "72.5",
          "3.2": "7.8",
          "3.7": ""
        },
        {
          "All(Average)": "III(-V-A+D)",
          "-": "2.1",
          "41,845": "4,327",
          "48.7": "54.4",
          "3.2": "2.7",
          "3.7": ""
        },
        {
          "All(Average)": "IV(+V-A+D)",
          "-": "1.2",
          "41,845": "319",
          "48.7": "70.4",
          "3.2": "5.9",
          "3.7": ""
        },
        {
          "All(Average)": "V(+V+A-D)",
          "-": "0.4",
          "41,845": "5,354",
          "48.7": "73.5",
          "3.2": "8.3",
          "3.7": ""
        },
        {
          "All(Average)": "VI(-V+A-D)",
          "-": "1.5",
          "41,845": "240",
          "48.7": "53.4",
          "3.2": "4.9",
          "3.7": ""
        },
        {
          "All(Average)": "VII(-V-A-D)",
          "-": "2.3",
          "41,845": "2,941",
          "48.7": "53.1",
          "3.2": "3.0",
          "3.7": ""
        },
        {
          "All(Average)": "VIII(+V-A-D)",
          "-": "0.3",
          "41,845": "189",
          "48.7": "70.5",
          "3.2": "5.4",
          "3.7": ""
        },
        {
          "All(Average)": "All(Average)",
          "-": "-",
          "41,845": "17,019",
          "48.7": "64.0",
          "3.2": "5.4",
          "3.7": "3.5"
        },
        {
          "All(Average)": "I(+V+A+D)",
          "-": "0.5",
          "41,845": "7,775",
          "48.7": "65.1",
          "3.2": "6.6",
          "3.7": ""
        },
        {
          "All(Average)": "II(-V+A+D)",
          "-": "0.4",
          "41,845": "664",
          "48.7": "50.9",
          "3.2": "3.8",
          "3.7": ""
        },
        {
          "All(Average)": "III(-V-A+D)",
          "-": "0.9",
          "41,845": "5,149",
          "48.7": "47.5",
          "3.2": "3.2",
          "3.7": ""
        },
        {
          "All(Average)": "IV(+V-A+D)",
          "-": "0.3",
          "41,845": "694",
          "48.7": "59.9",
          "3.2": "4.5",
          "3.7": ""
        },
        {
          "All(Average)": "V(+V+A-D)",
          "-": "1.7",
          "41,845": "4,952",
          "48.7": "68.7",
          "3.2": "5.8",
          "3.7": ""
        },
        {
          "All(Average)": "VI(-V+A-D)",
          "-": "1.8",
          "41,845": "750",
          "48.7": "56.6",
          "3.2": "2.8",
          "3.7": ""
        },
        {
          "All(Average)": "VII(-V-A-D)",
          "-": "1.5",
          "41,845": "6,699",
          "48.7": "49.9",
          "3.2": "3.0",
          "3.7": ""
        },
        {
          "All(Average)": "VIII(+V-A-D)",
          "-": "2.4",
          "41,845": "417",
          "48.7": "65.6",
          "3.2": "4.9",
          "3.7": ""
        },
        {
          "All(Average)": "All(Average)",
          "-": "-",
          "41,845": "27,100",
          "48.7": "58.0",
          "3.2": "4.3",
          "3.7": "3.7"
        },
        {
          "All(Average)": "I(+V+A+D)",
          "-": "0.8",
          "41,845": "3,364",
          "48.7": "54.6",
          "3.2": "3.0",
          "3.7": ""
        },
        {
          "All(Average)": "II(-V+A+D)",
          "-": "0.9",
          "41,845": "299",
          "48.7": "44.1",
          "3.2": "1.3",
          "3.7": ""
        },
        {
          "All(Average)": "III(-V-A+D)",
          "-": "0.1",
          "41,845": "2,631",
          "48.7": "41.3",
          "3.2": "1.2",
          "3.7": ""
        },
        {
          "All(Average)": "IV(+V-A+D)",
          "-": "1.6",
          "41,845": "209",
          "48.7": "50.8",
          "3.2": "1.9",
          "3.7": ""
        },
        {
          "All(Average)": "V(+V+A-D)",
          "-": "0.5",
          "41,845": "2,805",
          "48.7": "57.5",
          "3.2": "3.2",
          "3.7": ""
        },
        {
          "All(Average)": "VI(-V+A-D)",
          "-": "1.2",
          "41,845": "259",
          "48.7": "43.8",
          "3.2": "1.5",
          "3.7": ""
        },
        {
          "All(Average)": "VII(-V-A-D)",
          "-": "0.2",
          "41,845": "3,440",
          "48.7": "41.8",
          "3.2": "1.0",
          "3.7": ""
        },
        {
          "All(Average)": "VIII(+V-A-D)",
          "-": "1.6",
          "41,845": "200",
          "48.7": "57.5",
          "3.2": "1.3",
          "3.7": ""
        },
        {
          "All(Average)": "All(Average)",
          "-": "-",
          "41,845": "13,207",
          "48.7": "48.9",
          "3.2": "1.8",
          "3.7": "3.0"
        },
        {
          "All(Average)": "I(+V+A+D)",
          "-": "1.0",
          "41,845": "1,627",
          "48.7": "71.9",
          "3.2": "4.8",
          "3.7": ""
        },
        {
          "All(Average)": "III(-V-A+D)",
          "-": "1.1",
          "41,845": "1,747",
          "48.7": "50.5",
          "3.2": "3.2",
          "3.7": ""
        },
        {
          "All(Average)": "IV(+V-A+D)",
          "-": "0.6",
          "41,845": "166",
          "48.7": "66.4",
          "3.2": "4.4",
          "3.7": ""
        },
        {
          "All(Average)": "V(+V+A-D)",
          "-": "0.2",
          "41,845": "1,991",
          "48.7": "79.7",
          "3.2": "3.8",
          "3.7": ""
        },
        {
          "All(Average)": "VII(-V-A-D)",
          "-": "0.4",
          "41,845": "1,533",
          "48.7": "54.4",
          "3.2": "2.7",
          "3.7": ""
        },
        {
          "All(Average)": "VIII(+V-A-D)",
          "-": "0.8",
          "41,845": "139",
          "48.7": "82.7",
          "3.2": "3.2",
          "3.7": ""
        },
        {
          "All(Average)": "All(Average)",
          "-": "-",
          "41,845": "7,203",
          "48.7": "67.6",
          "3.2": "3.7",
          "3.7": "2.8"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "SubjectiveEvaluation": "nMOS(↑) sMOS(↑) eMOS(↑)",
          "Column_2": "WERWhis(↓) WERw2v(↓) WERAVG(↓)",
          "Column_3": "SECSR(↑) SECSW(↑) SECSAVG(↑)"
        },
        {
          "SubjectiveEvaluation": "4.06±0.05 4.15±0.05 4.11±0.05\n3.95±0.05 4.01±0.06 3.98±0.06",
          "Column_2": "11.42 14.99 13.21\n11.36 15.15 13.26",
          "Column_3": "0.7358 0.9264 0.8311\n0.7271 0.9231 0.8251"
        },
        {
          "SubjectiveEvaluation": "3.42±0.07 3.95±0.07 3.75±0.07\n3.36±0.07 3.89±0.07 3.69±0.07",
          "Column_2": "14.49 18.90 16.70\n20.24 26.48 23.36",
          "Column_3": "0.6981 0.8939 0.7960\n0.6920 0.8764 0.7842"
        },
        {
          "SubjectiveEvaluation": "3.52±0.06 3.94±0.06 3.75±0.07\n3.85±0.06 3.96±0.06 3.86±0.06\n3.77±0.06 3.94±0.06 3.79±0.07",
          "Column_2": "28.71 32.79 30.75\n16.63 22.56 19.60\n26.07 29.74 27.91",
          "Column_3": "0.7296 0.6751 0.6811\n0.7074 0.8888 0.7981\n0.6153 0.8208 0.7181"
        },
        {
          "SubjectiveEvaluation": "3.92±0.06 3.97±0.06 3.86±0.06",
          "Column_2": "15.52 18.85 17.19",
          "Column_3": "0.7314 0.9047 0.8181"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "SubjectiveEvaluation": "nMOS(↑) sMOS(↑) eMOS(↑)",
          "Column_2": "WERWhis(↓) WERw2v(↓) WERAVG(↓)",
          "Column_3": "SECSR(↑) SECSW(↑) SECSAVG(↑)"
        },
        {
          "SubjectiveEvaluation": "4.02±0.05 4.04±0.05 4.08±0.05\n3.93±0.06 4.01±0.05 4.04±0.06",
          "Column_2": "13.06 15.15 14.11\n13.11 17.07 15.09",
          "Column_3": "0.7725 0.9282 0.8504\n0.7603 0.9275 0.8439"
        },
        {
          "SubjectiveEvaluation": "3.67±0.07 3.93±0.05 3.71±0.08\n3.90±0.06 3.96±0.06 3.83±0.07\n3.83±0.06 3.93±0.05 3.82±0.08",
          "Column_2": "26.48 32.78 29.63\n15.42 23.46 19.44\n26.74 32.70 29.72",
          "Column_3": "0.6870 0.8220 0.7545\n0.6812 0.8191 0.7502\n0.5970 0.7480 0.6725"
        },
        {
          "SubjectiveEvaluation": "3.91±0.06 3.99±0.05 3.85±0.07",
          "Column_2": "14.41 18.43 16.42",
          "Column_3": "0.6543 0.8640 0.7592"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "SubjectiveEvaluation": "nMOS(↑) sMOS(↑) eMOS(↑)",
          "Column_2": "WERWhis(↓) WERw2v(↓) WERAVG(↓)",
          "Column_3": "SECSR(↑) SECSW(↑) SECSAVG(↑)"
        },
        {
          "SubjectiveEvaluation": "4.06±0.05 4.15±0.05 4.11±0.05\n3.95±0.05 4.01±0.06 3.98±0.06",
          "Column_2": "11.42 14.99 13.21\n11.36 15.15 13.26",
          "Column_3": "0.7358 0.9264 0.8311\n0.7271 0.9231 0.8251"
        },
        {
          "SubjectiveEvaluation": "3.81±0.06 3.95±0.06 3.82±0.07",
          "Column_2": "18.36 23.57 20.97",
          "Column_3": "0.7212 0.9012 0.8112"
        },
        {
          "SubjectiveEvaluation": "3.86±0.06 3.94±0.06 3.84±0.07",
          "Column_2": "15.53 19.87 17.70",
          "Column_3": "0.7335 0.9066 0.8201"
        },
        {
          "SubjectiveEvaluation": "3.92±0.06 3.97±0.06 3.86±0.06",
          "Column_2": "15.52 18.85 17.19",
          "Column_3": "0.7314 0.9047 0.8181"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "Seen Speaker\nBase Style Shifted Style\n(a) III (−V−A+D) V (+V+A−D)\nBase Style Shifted Style\n(a)Angry (b)Happy\n(b) VIII (+V−A−D) III (−V−A+D)\nUnseen Speaker\nBase Style Shifted Style\n(c)Sad (d)Surprise\nFig.8. Comparisonoftheemotionclassificationaccuracyscoresofshifting\n(c) V (+V+A−D) III (−V−A+D)\nemotionstyle.\nBase Style Shifted Style\nWe select another fixed pair of reference signals from the\ntest set of the same emotion and speaker. As shown in\nTable II, EmoSphere++ outperforms the previous methods\nin terms of both subjective and objective evaluations. In\n(d) III (−V−A+D) VIII (+V−A−D)\nterms of naturalness and linguistic consistency, EmoSphere++\nFig.7. Pitchtracksofasampledemonstratingtheeffectsofemotionalstyle achievesthehighestnMOSwithascoreof3.92andperformed\nshift in sad emotion, where A, V, and D represent arousal, valence, and strongly in WER with a score of 15.52 compared to those",
          "(a)Angry (b)Happy\n(c)Sad (d)Surprise\nFig.8. Comparisonoftheemotionclassificationaccuracyscoresofshifting\nemotionstyle.\nWe select another fixed pair of reference signals from the\ntest set of the same emotion and speaker. As shown in\nTable II, EmoSphere++ outperforms the previous methods\nin terms of both subjective and objective evaluations. In\nterms of naturalness and linguistic consistency, EmoSphere++": "(a)Angry (b)Happy\n(c)Sad (d)Surprise\nFig.8. Comparisonoftheemotionclassificationaccuracyscoresofshifting\nemotionstyle.\nWe select another fixed pair of reference signals from the\ntest set of the same emotion and speaker. As shown in\nTable II, EmoSphere++ outperforms the previous methods\nin terms of both subjective and objective evaluations. In\nterms of naturalness and linguistic consistency, EmoSphere++"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "Seen Speaker"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "Unseen Speaker"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "SeenSpeaker": "SubjectiveEvaluation",
          "Column_2": "ObjectiveEvaluation",
          "Column_3": "",
          "Column_4": "SubjectiveEvaluation",
          "Column_5": ""
        },
        {
          "SeenSpeaker": "nMOS(↑) sMOS(↑) eMOS(↑)",
          "Column_2": "SECSAVG(↑)",
          "Column_3": "ECA(↑) EECS(↑)",
          "Column_4": "nMOS(↑) sMOS(↑) eMOS(↑)",
          "Column_5": "SECSAVG(↑)"
        },
        {
          "SeenSpeaker": "4.23±0.04 4.19±0.03 4.08±0.03\n4.22±0.04 4.13±0.04 4.05±0.04",
          "Column_2": "0.8311\n0.8251",
          "Column_3": "95.53 0.9487\n94.25 0.9389",
          "Column_4": "4.16±0.05 4.11±0.05 4.07±0.03\n4.12±0.06 4.05±0.04 4.02±0.04",
          "Column_5": "0.8504\n0.8439"
        },
        {
          "SeenSpeaker": "3.83±0.05 3.78±0.05 3.75±0.04\n3.79±0.05 3.75±0.04 3.74±0.05\n3.76±0.05 3.77±0.04 3.51±0.05",
          "Column_2": "0.8171\n0.8165\n0.8178",
          "Column_3": "92.86 0.9218\n92.18 0.9272\n77.68 0.8171",
          "Column_4": "3.70±0.08 3.83±0.05 3.70±0.04\n3.73±0.08 3.81±0.04 3.72±0.05\n3.64±0.08 3.79±0.05 3.60±0.04",
          "Column_5": "0.7573\n0.7574\n0.7568"
        },
        {
          "SeenSpeaker": "3.82±0.05 3.76±0.04 3.77±0.04\n3.78±0.05 3.74±0.03 3.76±0.05\n3.79±0.06 3.76±0.04 3.76±0.04",
          "Column_2": "0.8169\n0.8143\n0.8160",
          "Column_3": "92.76 0.9283\n92.34 0.9235\n92.26 0.9191",
          "Column_4": "3.75±0.07 3.81±0.05 3.71±0.04\n3.76±0.08 3.67±0.05 3.74±0.04\n3.74±0.08 3.74±0.05 3.73±0.05",
          "Column_5": "0.7566\n0.7262\n0.7549"
        },
        {
          "SeenSpeaker": "3.87±0.05 3.77±0.04 3.78±0.05",
          "Column_2": "0.8181",
          "Column_3": "93.53 0.9270",
          "Column_4": "3.76±0.07 3.83±0.05 3.75±0.04",
          "Column_5": "0.7592"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "WERWhis(↓) WERw2v(↓) WERAVG(↓)": "13.70 28.24 20.97\n14.93 13.95 14.44",
          "SECSR(↑) SECSW(↑) SECSAVG(↑)": "0.7383 0.8506 0.7945\n0.6449 0.8496 0.7473",
          "RMSEf0(↓) RMSEperiod(↓) F1V/UV(↑)": "- - -\n3.02 0.3262 0.6070"
        },
        {
          "WERWhis(↓) WERw2v(↓) WERAVG(↓)": "28.51 46.25 37.38\n27.07 46.53 36.80",
          "SECSR(↑) SECSW(↑) SECSAVG(↑)": "0.6153 0.8283 0.7218\n0.6149 0.8336 0.7243",
          "RMSEf0(↓) RMSEperiod(↓) F1V/UV(↑)": "20.17 0.4580 0.4971\n19.95 0.4587 0.4997"
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "2",
      "title": "Laugh now cry later: Controlling time-varying emotional states of flow-matching-based zero-shot text-tospeech",
      "authors": [
        "H Wu",
        "X Wang",
        "S Eskimez",
        "M Thakker",
        "D Tompkins",
        "C.-H Tsai",
        "C Li",
        "Z Xiao",
        "S Zhao",
        "J Li"
      ],
      "venue": "IEEE Spoken Language Technology Workshop"
    },
    {
      "citation_id": "3",
      "title": "Emospheretts: Emotional style and intensity modeling via spherical emotion vector for controllable emotional text-to-speech",
      "authors": [
        "D.-H Cho",
        "H.-S Oh",
        "S.-B Kim",
        "S.-H Lee",
        "S.-W Lee"
      ],
      "year": "2024",
      "venue": "Proceedings of the Interspeech"
    },
    {
      "citation_id": "4",
      "title": "Towards realistic emotional voice conversion using controllable emotional intensity",
      "authors": [
        "T Qi",
        "S Wang",
        "C Lu",
        "Y Zhao",
        "Y Zong",
        "W Zheng"
      ],
      "year": "2024",
      "venue": "Proceedings of the Interspeech"
    },
    {
      "citation_id": "5",
      "title": "Controllable multi-speaker emotional speech synthesis with emotion representation of high generalization capability",
      "authors": [
        "J Zheng",
        "J Zhou",
        "W Zheng",
        "L Tao",
        "H Kwan"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "Controlling emotion strength with relative attribute for end-to-end speech synthesis",
      "authors": [
        "X Zhu",
        "S Yang",
        "G Yang",
        "L Xie"
      ],
      "year": "2019",
      "venue": "IEEE Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "7",
      "title": "Emotion intensity and its control for emotional voice conversion",
      "authors": [
        "K Zhou",
        "B Sisman",
        "R Rana",
        "B Schuller",
        "H Li"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "8",
      "title": "Fine-grained emotion strength transfer, control and prediction for emotional speech synthesis",
      "authors": [
        "Y Lei",
        "S Yang",
        "L Xie"
      ],
      "year": "2021",
      "venue": "2021 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "9",
      "title": "Msemotts: Multi-scale emotion transfer, prediction, and control for emotional speech synthesis",
      "authors": [
        "Y Lei",
        "S Yang",
        "X Wang",
        "L Xie"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "10",
      "title": "Speech synthesis with mixed emotions",
      "authors": [
        "K Zhou",
        "B Sisman",
        "R Rana",
        "B Schuller",
        "H Li"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "Hierarchical emotion prediction and control in text-to-speech synthesis",
      "authors": [
        "S Inoue",
        "K Zhou",
        "S Wang",
        "H Li"
      ],
      "year": "2024",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "Emoq-tts: Emotion intensity quantization for fine-grained controllable emotional text-tospeech",
      "authors": [
        "C.-B Im",
        "S.-H Lee",
        "S.-B Kim",
        "S.-W Lee"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "Controlling the strength of emotions in speech-like emotional sound generated by wavenet",
      "authors": [
        "K Matsumoto",
        "S Hara",
        "M Abe"
      ],
      "year": "2020",
      "venue": "Proceedings of the Interspeech"
    },
    {
      "citation_id": "14",
      "title": "Crossspeaker Emotion Transfer Based On Prosody Compensation for Endto-End Speech Synthesis",
      "authors": [
        "T Li",
        "X Wang",
        "Q Xie",
        "Z Wang",
        "M Jiang",
        "L Xie"
      ],
      "year": "2022",
      "venue": "Proceedings of the Interspeech"
    },
    {
      "citation_id": "15",
      "title": "Cross-speaker emotion disentangling and transfer for end-to-end speech synthesis",
      "authors": [
        "T Li",
        "X Wang",
        "Q Xie",
        "Z Wang",
        "L Xie"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "16",
      "title": "Emotional speech synthesis with rich and granularized control",
      "authors": [
        "S.-Y Um",
        "S Oh",
        "K Byun",
        "I Jang",
        "C Ahn",
        "H.-G Kang"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "Semi-supervised generative modeling for controllable speech synthesis",
      "authors": [
        "R Habib",
        "S Mariooryad",
        "M Shannon",
        "E Battenberg",
        "R Skerry-Ryan",
        "D Stanton",
        "D Kao",
        "T Bagby"
      ],
      "year": "2019",
      "venue": "Proceedings of the International Conference on Learning Representations"
    },
    {
      "citation_id": "18",
      "title": "Emotional Prosody Control for Speech Generation",
      "authors": [
        "S Sivaprasad",
        "S Kosgi",
        "V Gandhi"
      ],
      "year": "2021",
      "venue": "Proceedings of the Interspeech"
    },
    {
      "citation_id": "19",
      "title": "Sequence-to-sequence modelling of f0 for speech emotion conversion",
      "authors": [
        "C Robinson",
        "N Obin",
        "A Roebel"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "20",
      "title": "Emotional voice conversion using multitask learning with text-to-speech",
      "authors": [
        "T.-H Kim",
        "S Cho",
        "S Choi",
        "S Park",
        "S.-Y Lee"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "21",
      "title": "An overview & analysis of sequence-to-sequence emotional voice conversion",
      "authors": [
        "Z Yang",
        "X Jing",
        "A Triantafyllopoulos",
        "M Song",
        "I Aslan",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "Proceedings of the Interspeech"
    },
    {
      "citation_id": "22",
      "title": "Durflex-evc: Durationflexible emotional voice conversion leveraging discrete representations without text alignment",
      "authors": [
        "H.-S Oh",
        "S.-H Lee",
        "D.-H Cho",
        "S.-W Lee"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "23",
      "title": "Diffprosody: Diffusion-based latent prosody generation for expressive speech synthesis with prosody conditional adversarial training",
      "authors": [
        "H.-S Oh",
        "S.-H Lee",
        "S.-W Lee"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "24",
      "title": "Emotional dimension control in language model-based text-to-speech: Spanning a broad spectrum of human emotions",
      "authors": [
        "K Zhou",
        "Y Zhang",
        "S Zhao",
        "H Wang",
        "Z Pan",
        "D Ng",
        "C Zhang",
        "C Ni",
        "Y Ma",
        "T Nguyen"
      ],
      "year": "2024",
      "venue": "Emotional dimension control in language model-based text-to-speech: Spanning a broad spectrum of human emotions",
      "arxiv": "arXiv:2409.16681"
    },
    {
      "citation_id": "25",
      "title": "Cross-speaker style transfer with prosody bottleneck in neural speech synthesis",
      "authors": [
        "S Pan",
        "L He"
      ],
      "year": "2021",
      "venue": "Proceedings of the Interspeech"
    },
    {
      "citation_id": "26",
      "title": "Matchatts: A fast tts architecture with conditional flow matching",
      "authors": [
        "S Mehta",
        "R Tu",
        "J Beskow",
        "É Székely",
        "G Henter"
      ],
      "year": "2024",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "27",
      "title": "P-flow: a fast and data-efficient zeroshot tts through speech prompting",
      "authors": [
        "S Kim",
        "K Shih",
        "J Santos",
        "E Bakhturina",
        "M Desta",
        "R Valle",
        "S Yoon",
        "B Catanzaro"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "28",
      "title": "Voicebox: Text-guided multilingual universal speech generation at scale",
      "authors": [
        "M Le",
        "A Vyas",
        "B Shi",
        "B Karrer",
        "L Sari",
        "R Moritz",
        "M Williamson",
        "V Manohar",
        "Y Adi",
        "J Mahadeokar"
      ],
      "year": "2024",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "29",
      "title": "E2 tts: Embarrassingly easy fully non-autoregressive zero-shot tts",
      "authors": [
        "S Eskimez",
        "X Wang",
        "M Thakker",
        "C Li",
        "C.-H Tsai",
        "Z Xiao",
        "H Yang",
        "Z Zhu",
        "M Tang",
        "X Tan"
      ],
      "venue": "IEEE Spoken Language Technology Workshop"
    },
    {
      "citation_id": "30",
      "title": "Theories of Emotion",
      "authors": [
        "R Plutchik",
        "H Kellerman"
      ],
      "year": "2013",
      "venue": "Theories of Emotion"
    },
    {
      "citation_id": "31",
      "title": "Pleasure-arousal theory and the intensity of emotions",
      "authors": [
        "R Reisenzein"
      ],
      "year": "1994",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "32",
      "title": "The dictionary of affect in language",
      "authors": [
        "C Whissell"
      ],
      "year": "1989",
      "venue": "The measurement of emotions"
    },
    {
      "citation_id": "33",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "34",
      "title": "Expressing degree of activation in synthetic speech",
      "authors": [
        "M Schroder"
      ],
      "year": "2006",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "35",
      "title": "A new pan-cultural facial expression of emotion",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1986",
      "venue": "Motivation and emotion"
    },
    {
      "citation_id": "36",
      "title": "Evidence for a three-factor theory of emotions",
      "authors": [
        "J Russell",
        "A Mehrabian"
      ],
      "year": "1977",
      "venue": "Journal of research in Personality"
    },
    {
      "citation_id": "37",
      "title": "A cognitive architecture for modeling emotion dynamics: Intensity estimation from physiological signals",
      "authors": [
        "R Jenke",
        "A Peer"
      ],
      "year": "2018",
      "venue": "Cognitive Systems Research"
    },
    {
      "citation_id": "38",
      "title": "Disentangled variational autoencoder for emotion recognition in conversations",
      "authors": [
        "K Yang",
        "T Zhang",
        "S Ananiadou"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "39",
      "title": "Cluster-level contrastive learning for emotion recognition in conversations",
      "authors": [
        "K Yang",
        "T Zhang",
        "H Alhuzali",
        "S Ananiadou"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "40",
      "title": "Mes-p: An emotional tonal speech dataset in mandarin with distal and proximal labels",
      "authors": [
        "Z Xiao",
        "Y Chen",
        "W Dou",
        "Z Tao",
        "L Chen"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "41",
      "title": "Audio-based granularityadapted emotion classification",
      "authors": [
        "S Shepstone",
        "Z.-H Tan",
        "S Jensen"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "42",
      "title": "Audio superresolution with robust speech representation learning of masked autoencoder",
      "authors": [
        "S.-B Kim",
        "S.-H Lee",
        "H.-Y Choi",
        "S.-W Lee"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "43",
      "title": "Can we generate emotional pronunciations for expressive speech synthesis",
      "authors": [
        "M Tahon",
        "G Lecorvé",
        "D Lolive"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "44",
      "title": "Relative attributes",
      "authors": [
        "D Parikh",
        "K Grauman"
      ],
      "year": "2011",
      "venue": "Proceedings of the International Conference on Computer Vision"
    },
    {
      "citation_id": "45",
      "title": "Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis",
      "authors": [
        "Y Wang",
        "D Stanton",
        "Y Zhang",
        "R.-S Ryan",
        "E Battenberg",
        "J Shor",
        "Y Xiao",
        "Y Jia",
        "F Ren",
        "R Saurous"
      ],
      "year": "2018",
      "venue": "Proceedings of the International Conference on Learning Representations"
    },
    {
      "citation_id": "46",
      "title": "Mellotron: Multispeaker expressive voice synthesis by conditioning on rhythm, pitch and global style tokens",
      "authors": [
        "R Valle",
        "J Li",
        "R Prenger",
        "B Catanzaro"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "47",
      "title": "iemotts: Toward robust cross-speaker emotion transfer and control for speech synthesis based on disentanglement between prosody and timbre",
      "authors": [
        "G Zhang",
        "Y Qin",
        "W Zhang",
        "J Wu",
        "M Li",
        "Y Gai",
        "F Jiang",
        "T Lee"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "48",
      "title": "Yourtts: Towards zero-shot multi-speaker tts and zero-shot voice conversion for everyone",
      "authors": [
        "E Casanova",
        "J Weber",
        "C Shulby",
        "A Junior",
        "E Gölge",
        "M Ponti"
      ],
      "year": "2022",
      "venue": "Proceedings of the International Conference on Machine Learning"
    },
    {
      "citation_id": "49",
      "title": "Generspeech: Towards style transfer for generalizable out-of-domain text-to-speech",
      "authors": [
        "R Huang",
        "Y Ren",
        "J Liu",
        "C Cui",
        "Z Zhao"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "50",
      "title": "Domain-adversarial training of neural networks",
      "authors": [
        "Y Ganin",
        "E Ustinova",
        "H Ajakan",
        "P Germain",
        "H Larochelle",
        "F Laviolette",
        "M March",
        "V Lempitsky"
      ],
      "year": "2016",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "51",
      "title": "Neural discrete representation learning",
      "authors": [
        "A Van Den",
        "O Oord",
        "Vinyals"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "52",
      "title": "Orthogonal projection loss",
      "authors": [
        "K Ranasinghe",
        "M Naseer",
        "M Hayat",
        "S Khan",
        "F Khan"
      ],
      "year": "2021",
      "venue": "Proc. IEEE Int. Conf. Comput. Vis"
    },
    {
      "citation_id": "53",
      "title": "Dawn of the transformer era in speech emotion recognition: Closing the valence gap",
      "authors": [
        "J Wagner",
        "A Triantafyllopoulos",
        "H Wierstorf",
        "M Schmitt",
        "F Burkhardt",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "54",
      "title": "A review of statistical outlier methods",
      "authors": [
        "S Walfish"
      ],
      "year": "2006",
      "venue": "Pharmaceutical technology"
    },
    {
      "citation_id": "55",
      "title": "emotion2vec: Self-supervised pre-training for speech emotion representation",
      "authors": [
        "Z Ma",
        "Z Zheng",
        "J Ye",
        "J Li",
        "Z Gao",
        "S Zhang",
        "X Chen"
      ],
      "year": "2024",
      "venue": "Proceedings of the ACL Findings"
    },
    {
      "citation_id": "56",
      "title": "Transfer learning from speaker verification to multispeaker text-to-speech synthesis",
      "authors": [
        "Y Jia",
        "Y Zhang",
        "R Weiss",
        "Q Wang",
        "J Shen",
        "F Ren",
        "P Nguyen",
        "R Pang",
        "I Moreno",
        "Y Wu"
      ],
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "57",
      "title": "Wavlm: Large-scale self-supervised pretraining for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "58",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "59",
      "title": "A statistical wavlm embedding features with auto-encoder for speech emotion recognition",
      "authors": [
        "A Chakhtouna",
        "S Sekkate",
        "A Adib"
      ],
      "year": "2023",
      "venue": "Biologically Inspired Cognitive Architectures Meeting"
    },
    {
      "citation_id": "60",
      "title": "Singleand cross-lingual speech emotion recognition based on wavlm domain emotion embedding",
      "authors": [
        "J Yang",
        "J Liu",
        "K Huang",
        "J Xia",
        "Z Zhu",
        "H Zhang"
      ],
      "year": "2024",
      "venue": "Electronics"
    },
    {
      "citation_id": "61",
      "title": "Flow matching for generative modeling",
      "authors": [
        "Y Lipman",
        "R Chen",
        "H Ben-Hamu",
        "M Nickel",
        "M Le"
      ],
      "year": "2022",
      "venue": "Proceedings of the International Conference on Learning Representations"
    },
    {
      "citation_id": "62",
      "title": "Gradtts: A diffusion probabilistic model for text-to-speech",
      "authors": [
        "V Popov",
        "I Vovk",
        "V Gogoryan",
        "T Sadekova",
        "M Kudinov"
      ],
      "year": "2021",
      "venue": "Proceedings of the International Conference on Machine Learning"
    },
    {
      "citation_id": "63",
      "title": "Emotional Voice Conversion: Theory, databases and ESD",
      "authors": [
        "K Zhou",
        "B Sisman",
        "R Liu",
        "H Li"
      ],
      "year": "2022",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "64",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "65",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "66",
      "title": "The festival speech synthesis system, version 1.4. 2",
      "authors": [
        "A Black",
        "P Taylor",
        "R Caley",
        "R Clark",
        "K Richmond",
        "S King",
        "V Strom",
        "H Zen"
      ],
      "year": "2001",
      "venue": "The festival speech synthesis system, version 1.4. 2"
    },
    {
      "citation_id": "67",
      "title": "Bigvgan: A universal neural vocoder with large-scale training",
      "authors": [
        "W S.-G. Lee",
        "B Ping",
        "B Ginsburg",
        "S Catanzaro",
        "Yoon"
      ],
      "year": "2023",
      "venue": "The Eleventh International Conference on Learning Representations"
    },
    {
      "citation_id": "68",
      "title": "Neural speech synthesis with transformer network",
      "authors": [
        "N Li",
        "S Liu",
        "Y Liu",
        "S Zhao",
        "M Liu"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "69",
      "title": "Glow-tts: A generative flow for text-to-speech via monotonic alignment search",
      "authors": [
        "J Kim",
        "S Kim",
        "J Kong",
        "S Yoon"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "70",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "71",
      "title": "LibriTTS: A Corpus Derived from LibriSpeech for Textto-Speech",
      "authors": [
        "H Zen",
        "V Dang",
        "R Clark",
        "Y Zhang",
        "R Weiss",
        "Y Jia",
        "Z Chen",
        "Y Wu"
      ],
      "year": "2019",
      "venue": "Proceedings of the Interspeech"
    },
    {
      "citation_id": "72",
      "title": "He is currently working toward an integrated master's and Ph.D. degree with the Department of Artificial Intelligence, Korea University, Seoul, South Korea. His research interests include artificial intelligence and audio signal processing. Hyung-Seok Oh received the B.S. degree in Computer Science and Engineering from Konkuk University, Seoul, South Korea, in 2021. He is currently working toward an integrated master's and Ph",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2022",
      "venue": "Proceedings of the International Conference on Machine Learning"
    },
    {
      "citation_id": "73",
      "title": "He is currently pursuing an integrated Master's and Ph.D. degrees with the Department of Artificial Intelligence, Korea University, South Korea. His research interests include artificial intelligence and audio signal processing",
      "venue": "He is currently pursuing an integrated Master's and Ph.D. degrees with the Department of Artificial Intelligence, Korea University, South Korea. His research interests include artificial intelligence and audio signal processing"
    }
  ]
}