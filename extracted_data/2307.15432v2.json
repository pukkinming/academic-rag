{
  "paper_id": "2307.15432v2",
  "title": "Cfn-Esa: A Cross-Modal Fusion Network With Emotion-Shift Awareness For Dialogue Emotion Recognition",
  "published": "2023-07-28T09:29:42Z",
  "authors": [
    "Jiang Li",
    "Xiaoping Wang",
    "Yingjian Liu",
    "Zhigang Zeng"
  ],
  "keywords": [
    "Emotion recognition in conversation",
    "multimodal fusion",
    "cross-modal association",
    "emotion shift"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal emotion recognition in conversation (ERC) has garnered growing attention from research communities in various fields. In this paper, we propose a Crossmodal Fusion Network with Emotion-Shift Awareness (CFN-ESA) for ERC. Extant approaches employ each modality equally without distinguishing the amount of emotional information in these modalities, rendering it hard to adequately extract complementary information from multimodal data. To cope with this problem, in CFN-ESA, we treat textual modality as the primary source of emotional information, while visual and acoustic modalities are taken as the secondary sources. Besides, most multimodal ERC models ignore emotion-shift information and overfocus on contextual information, leading to the failure of emotion recognition under emotion-shift scenario. We elaborate an emotion-shift module to address this challenge. CFN-ESA mainly consists of unimodal encoder (RUME), cross-modal encoder (ACME), and emotion-shift module (LESM). RUME is applied to extract conversation-level contextual emotional cues while pulling together data distributions between modalities; ACME is utilized to perform multimodal interaction centered on textual modality; LESM is used to model emotion shift and capture emotion-shift information, thereby guiding the learning of the main task. Experimental results demonstrate that CFN-ESA can effectively promote performance for ERC and remarkably outperform state-of-the-art models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "R ECENTLY, multimodal learning has attracted the at- tention of both academia and industry, and has been widely applied in many fields, such as biometrics, information retrieval, autonomous driving, and emotion recognition. With the advancement of technologies, the abundance of multimodal data can be more conveniently available for research purposes. In realistic life, multimodal data mainly contains three contents, i.e., transcribed text, visual image or video, and acoustic The authors are with the School of Artificial Intelligence and Automation, Huazhong University of Science and Technology (HUST), the Institute of Artificial Intelligence, HUST, the Hubei Key Laboratory of Braininspired Intelligent Systems, HUST, and the Key Laboratory of Image Processing and Intelligent Control (HUST), Ministry of Education, Wuhan 430074, China (e-mail: lijfrank@hust.edu.cn; wangxiaoping@hust.edu.cn; vir-tualmoon999@gmail.com; zgzeng@hust.edu.cn).\n\nDigital Object Identifier 10.1109/TAFFC.2024.3389453 speech. Multimodal fusion is one of the prominent branches of multimodal learning, whose main purpose is to utilize the organic combination of information from multiple modalities to collaboratively achieve the final downstream task. Thus, how to adequately extract the inter-modal complementary information becomes a formidable challenge in the domain of multimodal fusion.\n\nThe whole thing! Can we go? From the facial expression of the speaker who utters u 5 , it is known that the emotion should be anger, which is true emotion of the utterance.\n\nThe target of emotion recognition in conversation (ERC) is to understand and analyze each utterance in the conversation and render the corresponding emotion. This task has recently drawn widespread interest from researchers in the areas of natural language processing, computer vision, and multimodal learning due to its promising applications, such as humanmachine interface in intelligent robots and opinion mining in social media. Most previous ERC models are based on individual modalities, such as text  [1] -  [5]  and speech  [6] -  [9] . However, very often, the emotions of human beings are elusive. As shown in Fig.  1 , textual uni-modality may not be capable of correctly recognizing emotions in some scenarios, 0000-0000 © 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. arXiv:2307.15432v2 [cs.CL] 13 Apr 2024 e.g., the emotion directly expressed by the text is neutral, but the corresponding facial expression is actually another emotion, e.g., anger. From this example, it can be argued that the model cannot understand and convey human emotions well with only a single modality. As multi-modality gets closer to real-world application scenarios, multimodal ERC has gained numerous research. The information contained in a single modality may not be sufficient or representative enough, while a multimodal-based model can make up for the shortcoming of the unimodal approach and thus improve the performance and robustness of the existing system. Simultaneously, multimodal ERC is more in line with the multiple ways (e.g., language, voice, and facial expressions) in which people express their emotions. Unlike traditional affective computing missions in unimodal  [1] ,  [2] ,  [5]  and non-conversational  [10] -  [12]  scenarios, multimodal ERC suffers from harsher challenges due to the complex relationship between multiple modalities and conversational contexts.\n\nAlthough previous studies have made impressive progress, these approaches either ignore the association between multimodal information or model multi-modality insufficiently. Some methods  [13] -  [16]  directly concatenate multimodal data without considering the association between multiple modalities. Moreover, there is a certain amount of noise in each modality itself, and together with the heterogeneity gap  [17]  of multimodal data, this direct concatenation manner may cause more noise. While some approaches  [18] -  [21]  perform associative modeling for multimodal data, there are flaws in their modeling styles. For instance, these methods assume that each modality contributes equally to the emotional expression of the utterance, which is not the case. The findings of extant multimodal ERC studies  [20] ,  [22]  indicate that textual modalities contain more valuable emotional information in comparison to visual and acoustic modalities. Consequently, exploiting each modality equally may not adequately extract multimodal complementary information when engaging in multimodal interaction, making it difficult to effectively maximize the performance of the model. Towards the above issues, we construct a novel network for conversational emotion recognition to efficiently model the association with multimodal data. We treat visual and acoustic modalities as sources of auxiliary information that are utilized to complement the representation of textual information; in turn, textual information is employed to augment visual and acoustic representations.\n\nExtant efforts  [16] ,  [23] ,  [24]  have revealed that emotion shift can constrain the performance of emotion recognition and is one of the challenges faced by ERC. Emotion shift describes the change of emotions in two utterances. More concretely, if two utterances shift from one emotion to another, i.e., the emotions of two utterances are different, then the emotion shift has occurred; conversely, the emotion shift has not occurred if the emotions of two utterances are identical. Contextual modeling, which inherently relies on aggregating emotional cues from surrounding utterances, often tends to preserve emotional consistency across the conversation. Nevertheless, this inherent tendency may inadvertently undermine the model's capacity to accurately recognize emotions under situations where emotion shifts occur, thus highlighting the need for advanced strategies to address this critical aspect of ERC. Existing approaches fail to consider emotion-shift information and concentrate too much on contextual information, causing the imbalance between context-and self-modeling. In other words, the importance of self-information (complementary information from the current utterance but belonging to the other two modalities) is prone to be neglected. To alleviate this problem, we devise an emotion-shift module as the auxiliary task of ERC, which guides the main task of ERC to optimize the emotional expression of utterances by taking into account emotion-shift factor.\n\nTo summarize, we propose a Cross-modal Fusion Network with Emotion-Shift Awareness (CFN-ESA) for ERC. Our CFN-ESA can efficiently extract multimodal complementary information, which mainly consists of three components, i.e., recurrence based uni-modality encoder (RUME), attention based cross-modality encoder (ACME), and label based emotion-shift module (LESM). RUME can capture intramodal contextual emotional cues while narrowing the heterogeneity gap of multimodal data by sharing parameters. ACME perceives textual modality as the primary source of emotional information and two other modalities as the secondary sources, and employs multi-head attention networks to adequately model multimodal interaction. LESM is employed as an auxiliary task of the ERC to explicitly model emotion shift and extract emotion-shift information, thereby enabling the main task to implicitly reduce intra-modal contextual modeling under emotion-shift scenario. Two public benchmark datasets, MELD and IEMOCAP, are leveraged to conduct numerous experiments for demonstrating the effectiveness of the proposed CFN-ESA. We also explore the impact under different network settings and test the performance of each component in CFN-ESA. To put it in a nutshell, the main contributions of this work include:\n\n1) A novel multimodal ERC method named CFN-ESA is proposed, which is mainly composed of uni-modality encoder (RUME), cross-modality encoder (ACME), and emotion-shift module (LESM). 2) RUME can extracts intra-modal contextual information while mitigating the heterogeneity gap issue; ACME can model multimodal interaction and adequately captures inter-modal complementary information. 3) LESM is utilized as an auxiliary task of the model to extract emotion-shift information, which in turn guides the main task for learning. 4) We conduct abundant experiments on two datasets and the results attest to the superiority of CFN-ESA over all baselines.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Works",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Emotion Recognition In Conversation",
      "text": "With the mounting interest in the study of dialogue systems, the identification of emotion in the conversation has become a hot research topic. Most previous ERC methods are based on textual modality, which primarily employ gated recurrent unit (GRU), long and short term memory (LSTM) network, and graph neural network (GNN) to model contexts. AGHMN  [1]  mainly consisted of hierarchical memory network (HMN) and bidirectional gated recurrent unit (BiGRU), where HMN was used to extract the interactive information between historical utterances, and BiGRU was used for the summarization of short-and long-term memory with the help of attentional weights. DialogXL  [2]  applied the pre-trained language model XLNet  [25]  to the ERC task. To achieve this purpose, Di-alogXL handled long-term context with enhanced memory and speaker dependencies with dialogue-aware self-attention. I-GCN  [3]  utilized graph convolutional network to extract the semantic associative information of utterances and the temporal sequence information of dialogues. The method firstly exploited graph structure to represent dialogues at different times and then employed incremental graph structure to simulate the process of dynamic dialogues. CauAIN  [4]  consisted of two main causal-aware interactions, namely causal cue retrieval and causal utterance traceback, which introduced commonsense knowledge as a cue for detecting emotional causes in a dialogue, explicitly modeling intra-and inter-speaker dependencies. CoG-BART  [5]  was an ERC approach that employed both contrastive learning and generative modeling, which utilized BART  [26]  as a backbone model, and enhanced the emotional expression of utterances through contrastive loss and generative loss.\n\nThe approaches based on acoustic modality are often termed as speech emotion recognition (SER). ISNet  [6]  was an individual standardization network that adopted automatically generated benchmark for individual standardization to deal with the problem of inter-individual emotion confusion in SER. MTL-AUG  [7]  was a semi-supervised multitask learning framework that employed speech-based augmentation types, while treating augmented classification and unsupervised reconstruction as auxiliary tasks to enable multi-task training to achieve the learning of generic representations without the need for meta-labeling. BAT  [8]  split the hybrid spectrogram into blocks and computed self-attention by combining these blocks with tokens, meanwhile utilizing the cross-block attention mechanism to facilitate the information interaction between blocks. In order to gain a deeper understanding of emotions conveyed in speech, Huang et al.  [27] ,  [28]  carried out in-depth investigations on emotion change detection. These studies provided insights into emotion change and could inspire future work in the field of ERC. Furthermore, while there exist some visual modality-based methods  [29] -  [31]  known as facial expression recognition, they are mostly outside the scope of the ERC task.\n\nThere have been some multimodal ERC efforts recently. MMGCN  [18]  exploited GNN to capture contextual and modal interactive information, which not only compensated for the shortcomings of previous methods that are unable to leverage multimodal dependencies, but also efficiently incorporated the speaker's information for ERC. DialogueTRM  [22]  used hierarchical Transformer to manage differentiated contextual preferences within each modality, and designed multi-grained interactive fusion to learn the different contributions of multiple modalities. MetaDrop  [19]  presented a dyadic contain or drop decision-making mechanism to learn adaptive fusion paths while extracting multimodal dependencies and contex-tual relationships. HU-Dialogue  [21]  introduced hierarchical uncertainty for ERC, containing a regularization based attention module that was perturbed by source-adaptive noise to model context-level uncertainty. MM-DFN  [20]  utilized a graph based dynamic fusion module to track conversational contexts in various semantic spaces and to enhance complementarity between modalities. COGMEN  [32]  was a multimodal ERC model that used a GNN architecture to model local dependencies and global contexts in the conversation, which effectively improved the performance of the model. UniMSE  [33]  integrated acoustic and visual features with textual features by applying T5  [34] , and performed intermodal contrastive learning to obtain differentiated multimodal representations. Inspired by the phenomenon of emotional ups and downs in conversations, Agarwal et al.  [35]  proposed an emotion-shift component to enhance the performance of multimodal ERC. We observe that their presented method aligns with a similar research trajectory to the method in our paper. In general, distinct from traditional affective computing tasks in single-modal and non-conversational settings, multimodal ERC is more challenging due to the complex relationship of multiple modalities and dialogue contexts.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Multi-Head Attention Network",
      "text": "Vaswani et al.  [36]  proposed the Transformer architecture for machine translation task, which achieved exceptional performance. Since then, the multi-head attention (MHA) network of Transformer has been widely applied in the fields of natural language processing, computer vision, and multimodal learning. MulT  [37]  employed multiple attention networks to model interactions among multimodal sequences with varying temporal steps, serving the purpose of multimodal sentiment analysis. AuxFormer  [38]  utilized a main audio-visual fusion network based on multi-head attention to achieve multimodal alignment and fusion, while two auxiliary networks were used to make the emotion information flow to the main network. Wagner et al.  [39]  revealed through extensive experiments that Transformer-based speech emotion recognition exhibited higher robustness and generalizability relative to other architecture-based approaches. ViT  [40]  applied pure Transformer directly to image sequence patches and achieved superior outcomes with few computational resources. BLIP-2  [41]  guided vision-language pre-training from frozen pretrained image encoders and frozen large language models, and compensated for the modality gap with a lightweight query Transformer. LLaVA  [42]  achieved universal visionlanguage understanding by bridging a visual encoder and a large language model, and facilitated future studies on visual instruction following. In this paper, we adopt MHA networks to extract multimodal complementary information, i.e., they are utilized to construct attention based cross-modality encoder (ACME). Here, the scaled dot-product attention is first defined:\n\nwhere query Q, key K, and value V are the packed feature representations; d k denotes the dimension of K or V; SMAX(•) denotes the softmax function. MHA is a network structure that can enhance the stability and performance of the scaled dot-product attention. The distinction is that different heads employ different query, key, and value matrices. MHA can be computed as follows:\n\nwhere CAT(•) denotes the concatenation operation; W Q,i , W K,i , and W V,i are the learnable parameters, which can project Q, K, and V into different representation subspaces, respectively; W mha is also the trainable parameter.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Proposed Model",
      "text": "This section is a detailed description of our proposed model. As shown in Fig.  2 , CFN-ESA mainly consists of the recurrence based uni-modality encoder (Uni-Modality Encoding), attention based cross-modality encoder (Cross-Modality Encoding), emotion classifier (Classifier), and label based emotion-shift module (Emotion-Shift Optimizing).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Problem Definition",
      "text": "the goal of ERC is to predict the emotion state e i for each utterance u i in U . In other words, the task of ERC is to learn a function F(•) with learnable parameters that maps the feature representation x i of an utterance u i to the corresponding emotion e i , i.e., e i = F(x i ).\n\nHere, a conversation is expressed by\n\nIn our work, a conversation involves three modalities, i.e., textual (T ), visual (V ), and acoustic (A) modalities, so each utterance u i can be represented as",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Recurrence Based Uni-Modality Encoder",
      "text": "To extract dialogue-level contextual emotional cues, we employ recurrence based uni-modality encoder (RUME) to encode the utterance in each of three modalities. Inspired by the structure of Transformer  [36] , we add fully connected networks and residual operations to RUME to improve the expressiveness and stability of recurrent neural network (RNN). Our uni-modality encoder is shown in Fig.  3 . Specifically, the structure of RUME can be formalized as:\n\nwhere X denotes the feature matrix of all utterances; RNN(•), LN(•), and FF(•) denote the RNN, normalization, and feedforward network layers, respectively. In this work, the RNN(•) and LN(•) default to bidirectional GRU and layer normalization; while the feed-forward layer consists of two fully connected networks, which can be represented as,\n\nwhere FC(•) and DP(•) denote the fully connected network and dropout operation, respectively, and α(•) denotes the activation function.\n\nNote that in order to make the data distribution for each modal utterance as close as possible (i.e., to alleviate the heterogeneity gap problem for multimodal data), we utilize the uni-modality encoder with shared parameter for all three Note that the information updating network for visual modality is similar to that for acoustic modality. modalities. That is, X m f r = RUME(X m ), where m ∈ {T, V, A} and RUME(•) denotes the uni-modality encoder.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Attention Based Cross-Modality Encoder",
      "text": "Multimodal ERC can compensate for the lack of information in unimodal methods. In this work, we devise attention based cross-modality encoder (ACME) to extract complementary information from multimodal emotion data. As shown in Fig.  4 , we take inspiration from the Transformer structure and mainly adopt the attention network layer, feedforward network layer, and residual operation to construct our ACME. Several studies  [18] ,  [22]  on multimodal ERC have revealed that the amount of emotional information embedded in visual and acoustic modalities is lower than that in textual modalities, and thus the expression of emotion in these models is limited. Based on this assumption, we take both visual and acoustic features as complementary information to complement the emotional expression of textual features. In turn, textual features of utterances are used to enhance the visual and acoustic representations. Furthermore, in RUME, it is laborious for RNN to focus on the global contextual information of the utterance. Therefore, we employ a self-attention layer to capture global contextual emotional cues before performing cross-modal interaction. The designed ACME is composed of the following three stages.\n\n(1) Enhancing the global contextual awareness of the utterance. The feature matrices X m from three modalities are taken as the inputs to three MHA networks, and the direct output X m s is summed with the input X m (i.e., the residual operation) to obtain feature matrix X m sr . This process can be expressed by equations as:\n\nwhere MHA(•) denotes the MHA network.\n\n(2) Performing the cross-modal interaction modeling. The above results are employed as inputs to four MHA networks in pairwise manner, and the information for each modality is updated. In the following, we describe the information update for each modality separately.\n\nFor the information update in textual modality, there are mainly two MHA networks and the feature matrices from three modalities being leveraged. Specifically, the textual feature matrix X T sr is utilized as the query Q in one MHA network, and the visual feature matrix X V sr is utilized as the key K and the value V, and the output X T ←V c is a textual feature matrix with visual information; similarly, the query Q in another MHA network comes from X T sr , the key K and value V are X A sr , and we obtain X T ←A c , a textual feature matrix with acoustic information; we further concatenate X T ←V c and X T ←A c to get X T c , and at the same time, we apply the residual operation to add X T , X T sr , and X T c to obtain the new textual feature matrix X T cr . The above process can be formalized as:\n\nwhere CAT(•) represents the concatenation operation.\n\nFor the information update in visual modality, one attention network and the feature matrices from two modalities are mainly used. Specifically, we take the visual feature matrix X V sr as the query Q in the MHA network, and the textual feature matrix X T sr as the key K and value V, to obtain the visual feature matrix X V ←T c with textual information enhancement; similar to the textual information updating process, the residual operation is applied to add X V , X V sr , and X V ←T c to gain the new visual feature matrix X V cr . The above process can be formalized as:\n\nThe information updating process in acoustic modality is similar to that in visual modality, which can be expressed by the following equation:\n\n(3) Improving the expressiveness and stability of the model. We take X m cr as the input to each of three feedforward network layers to obtain X m f ; at the same time, the residual operation is used to sum X m , X m cr , X m f to obtain the feature matrix X m f r . The above process is expressed by the equation as follows:",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "D. Emotion Classifier",
      "text": "After multiple layers of RUME and ACME encoding, we obtain the final feature matrices H T , H V , and H A . Then they are concatenated to obtain fused feature matrix H. Finally, the feature dimensions of H are converted to |E| (number of emotions) with an emotion classifier, and thus we obtain the predicted emotion e ′ i (e ′ i ∈ E). The process can be formulated as follows:\n\nwhere h i ∈ H; W l and W smax are learnable parameters; ARGMAX(•) denotes the argmax function. We define the loss function as follows:\n\nwhere n(i) is the number of utterances of the i-th dialogue, and N is the number of all dialogues in training set; y ′ ij denotes the probability distribution of predicted emotion label of the j-th utterance in the i-th dialogue, and y ij denotes the ground truth label.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "E. Label Based Emotion-Shift Module",
      "text": "In order to extract emotion-shift information and enhance the emotional expression of the utterance, we introduce the label based emotion-shift module (LESM) to explicitly model the emotion-shift between utterances. LESM consists of three main steps, i.e., firstly, constructing the probability tensor of emotion-shift, then generating the label matrix of emotionshift, and finally, performing the training exploiting the loss of emotion-shift. Our LESM is used as an auxiliary task to guide the learning of the main task, thereby empowering the main task to reduce intra-modal conceptual modeling during emotion shift scene and instead focus on cross-modal interactive modeling.\n\n1) Emotion-Shift Probability: Inspired by SimCSE  [43] , we employ two parameter-shared ACMEs to generate two feature matrices with different representations but consistent emotion semantics. In other words, the output X m (m ∈ {T, V, A}) of RUME is treated as the inputs to two parameter-shared ACMEs, and then two fused feature matrices H and H ′ are obtained. Here, An example of the above process can be illustrated in Fig.  5 . Specifically, assume that there exist three utterances and the corresponding feature vectors are x m 1 , x m 2 , and x m 3 . These feature vectors are taken as inputs to two parametershared ACMEs, and thus the fused feature vectors h i and h ′ i (i = 1, 2, 3) are obtained, where\n\n, and h ′ 3 ); and similarly, concatenating h 2 with each h ′ i ; and for h 3 , the same concatenation operation is adopted. Finally, the 3 × 3 × 2|F | dimensional emotion-shift probability tensor T 123 is obtained.\n\n2) Emotion-Shift Label: We annotate emotion-shift status between utterances based on true emotion labels of the dataset. Concretely, if the true emotions of two utterances are the same, then we annotate their shift status as 0, meaning that emotion shift has not occurred; conversely, if their true emotions are different, then we annotate the shift status as 1, meaning that emotion shift has occurred. By the above operation, we obtain the |U | × |U | dimensional emotion-shift label matrix.\n\n3) Emotion-Shift Loss: After constructing the emotion-shift probabilities and labels, we require to define the corresponding emotion-shift loss for training. LESM is a binary-classified auxiliary task, which aims to correctly distinguish the emotionshift states between utterances. In this way, the model is prompted to capture emotion-shift information, thereby guiding it to attenuate focus on contextual information. First, in order to obtain the predicted emotion-shift state s ′ ij (s ′ ij ∈ {0, 1}), we convert the feature dimension of the probability tensor T to 2 with the fully-connected layer. The above process is as follows:\n\nwhere t ij denotes emotion-shift probability vector between the i-th and j-th utterances, t ij ∈ T ; z ′ ij is the probability distribution of predicted emotion-shift label between the i-th and j-th utterances; W ′ l and W smaxs are learnable parameters. The defined emotion-shift loss is:",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Acme Acme Concatenation Concatenation",
      "text": "where n(i) is the number of utterances of the i-th dialogue, and N is the number of all dialogues in training set; z ′ ijk denotes the probability distribution of predicted emotion-shift label between the j-th and k-th utterances in the i-th dialogue, and z ijk denotes the ground truth label.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "F. Training Objective",
      "text": "We combine the classification loss L c and emotion-shift loss L s to get the final training objective,\n\nwhere λ is a trade-off parameter with a value in the range [0,1], η is the L2-regularizer weight, and W is the set of all learnable parameters. Further, λ can be set manually or automatically adjusted using the method of Kendall et al.  [44] .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Iv. Experimental Setup",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A. Datasets",
      "text": "We adopt two public dialogue emotion datasets: MELD  [45]  and IEMOCAP  [46]  . The statistics of them are shown in TABLE  I . MELD is a multimodal and multiparty dataset containing more than 1,400 dialogues and 13,000 utterances from the TV series Friends. There are seven emotion labels in the dataset, i.e., anger, disgust, sadness, joy, surprise, fear, and neutral. 1,153 dialogues with 11,098 utterances are employed as the training and validation sets, where the 10% of utterances is selected as the validation set. The remaining 2,610 utterances in the dataset are served as the test set, which contains 280 dialogues. IEMOCAP is an acted, multimodal and multi-speaker dataset consisting of dyadic conversations, which contains textual, visual, and acoustic modalities. The dataset consists of 151 dialogues and 7,433 utterances labelled with six emotion categories: happy, sad, neutral, angry, excited, and frustrated. We adopt 120 dialogues with 5,810 utterances for training and validation, and the rest for testing. Here, the validation set is randomly selected from the training set with a ratio of 10%. The utterance-level features are extracted in the following manner. The visual and acoustic features are extracted with the way of MMGCN  [18] , i.e., the visual features are extracted using a DenseNet  [47]  pre-traind on the Facial Expression Recognition Plus corpus  [48] , the acoustic features are extracted using the OpenSmile toolkit with IS10 configuration  [49] . The textual feature is processed adopting the approach of COSMIC  [23] , i.e., the RoBERTa  [50]  model is applied for pre-training and fine-tuning to extract textual features.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "B. Training Details",
      "text": "The operating system we used is Ubuntu with version 20.04, and the deep learning framework is Pytorch 2.0.0. All experiments are conducted on a single NVIDIA GeForce RTX 3090. In our experiments, the maximum epoch is set to 80, and the basis network of RUME is GRU by default; AdamW  [51]  is employed as the optimizer with the L2 regularization factor of 1e-4; and the number of heads in all MHA networks is set to 8. For the MELD dataset, the learning rate is set to 1e-5, and the batch size is set to 64; the number of network layers for RUME and ACME are 2 and 3, respectively, with corresponding dropout rates of 0.1 and 0.3, respectively; we manually set the trade-off parameter λ to 0.9 by default. For the IEMOCAP dataset, the learning rate is set to 2e-5, and the batch size is set to 32; the number of network layers for RUME and ACME are 2 and 5, respectively, with Results for MMGCN are from MM-DFN, other results are from the original papers. W-F1, F1, and Acc denote the accuracy (%), F1 score (%), and weighted F1 score (%), respectively. The marker † indicates the best result from the five experiments, and the marker ‡ denotes the confidence interval.\n\ncorresponding dropout rates of 0.2 and 0.4, respectively; the trade-off parameter λ is manually set to 1.0.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Comparative Methods And Evaluation Metrics",
      "text": "The baselines we use are divided into two categories: text based methods and multi-modality based methods. The text based approaches include AGHMN  [1] , DialogXL  [2] , I-GCN  [3] , CauAIN  [4] , CoG-BART  [5] . The multi-modality based approaches include MMGCN  [18] , DialogueTRM  [22] , MetaDrop  [19] , HU-Dialogue  [21] , MM-DFN  [20] , COG-MEN  [32] , UniMSE  [33] .\n\nFollowing previous works  [18] ,  [20] , we report the accuracy (Acc) and weighted F1 score (W-F1) to measure overall performance on these two public datasets (i.e., MELD and IEMOCAP), and also present F1 score for each emotion class.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "V. Results And Analysis A. Comparison To Baselines On The Meld Dataset",
      "text": "We report the experimental results of CFN-ESA on the MELD dataset in TABLE II. As can be seen from the table, the proposed CFN-ESA outperforms the results of all the baseline models in terms of weighted F1 score and accuracy. Among all the textual unimodal models, the weighted F1 score of CauAIN is 65.46%, which is the highest experimental performance. Our CFN-ESA is 66.70%, which is an improvement of 1.24% relative to CauAIN. This result suggests that the acoustic and visual modalities in CFN-ESA can contribute complementary information to effectively improve the performance of the model. Relative to MetaDrop's weighted F1 score of 66.08%, the proposed CFN-ESA improves by 0.62%. The accuracy of MetaDrop is 66.42%, while that of CFN-ESA is 67.85%, with the former being 1.43% lower than the latter. Comparing the accuracy of CFN-ESA and DialogueTRM, the accuracy of CFN-ESA improves by 2.15% relative to that of DialogueTRM, yielding similar results as above. These comparative results indicate that our model can more effectively model multimodal emotion datasets.\n\nAs can be noticed from",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "B. Comparison To Baselines On The Iemocap Dataset",
      "text": "The comparison results of CFN-ESA on the IEMOCAP dataset are reported in TABLE III. We can state that CFN-ESA achieves the best performance with the weighted F1 score accuracy of 71.04% and 70.78%, respectively. Focusing our attention on the comparison with the unimodal approaches. Relative to the weighted F1 score of 66.18% for CoG-BART, that for CFN-ESA has an improvement of 4.86%. The accuracy of I-GCN is 65.50%, which is 5.28% lower than that of our CFN-ESA. This phenomenon can indicate that CFN-ESA effectively leverages the information from multiple modalities and alleviates the problem of insufficient",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "C. Effect Of Different Modal Settings",
      "text": "As shown in TABLE IV, we examine the effects of different modal settings on the proposed model. Specifically, the input configurations for the three shared-parameter RUMEs are as follows: (1) when exploiting three modalities, the inputs to three RUMEs are X T , X V , and X A , where T , V , and A denote the textual, visual, and acoustic modalities, respectively;\n\n(2) when the textual and visual modalities are utilized, the inputs to three RUMEs are X T , X V , and X V , respectively;\n\n(3) when engaging with both textual and acoustic modalities, the inputs to three RUMEs are X T , X A , and X A , respectively; (4) In the case of utilizing the visual and acoustic modalities exclusively, the inputs to three RUMEs are X V , X A , and X A , respectively; and (5) when working with a single modality, the inputs to all three RUMEs are X m , where m ∈ {T, V, A}. T, V, and A is textual, visual, and acoustic modalities, respectively.\n\nAs we expected, the tri-modal setting achieves the best performance relative to the bi-modal and unimodal settings. Among all the unimodal settings, the textual setting attains 67.09% accuracy on the MELD dataset and 66.57% F1 score on the IEMOCAP dataset, which is much higher than two other unimodal settings and reaches the best performance. These results indicate that the textual modality contains more emotional information than other two modalities. Compared to visual unimodal setting, the acoustic unimodal setting yields better experimental results on both datasets. The plausible explanation for this observation is that the image often incorporate a more intricate background and are susceptible to a higher degree of ambient noise interference.\n\nThe performance of the bi-modal settings with text is better compared to the visual-acoustic setting. On the IEMOCAP dataset, the textual-acoustic setting achieves an accuracy of 68.67%, which is 1.09% higher than the result of the textualvisual setting. Similar experimental results also appear on the MELD dataset. In addition, Fig.  8  illustrates the comparison among the textual unimodal, textual-visual bi-modal, textualacoustic bi-modal, and tri-modal settings. It can be observed that the bi-modal setting with visual or acoustic modality has a higher performance than the textual setting. This indicates that the multimodal settings can effectively improve the performance of the ERC task. Similarly, the experimental results of the tri-modal setting with both visual and acoustic modalities are better compared to the bi-modal setting.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "D. Impact Of Different Network Depths",
      "text": "We explore the effect of different network depths (number of layers) on the performance in this subsection. We first fix the network depth of one encoder unchanged, then vary the network depth of the other, and record the experimental results. Note that these experiments are conducted on the IEMOCAP dataset. Fig.  9a  depicts the effect of RUME with different network depths on the experimental results. As evidenced by the depicted figure, it is discernible that the performance of CFN-ESA initially ascends and subsequently descends with an increase in network depth, reaching its peak at a depth of 2 layers. The impact of ACME with varying network depths on our model is illustrated in Fig.  9b , which reveals a similar trend to that observed in Fig.  9a . Specifically, the experimental outcomes exhibit a pattern of escalation followed by attenuation, with the optimal network depth being 5 layers.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "E. Impact Of Different Trade-Off Parameters",
      "text": "In our experiments, the trade-off parameter λ can be set in two ways, that is, manual setting and automatic setting using the method of Kendall et al.  [44] . In this subsection, we investigate the effects of different trade-off parameters on the performance. Table  V  demonstrates the effect of λ on the results on the MELD and IEMOCAP datasets. It can be seen that: (1) on the MELD dataset, the best experimental results are achieved when λ is manually set to 0.9; and (2) on the IEMOCAP dataset, the best weighted F1 score is attained when λ is manually set to 1.0, whereas automatically setting λ results in the best accuracy.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "F. Ablation Studies",
      "text": "To demonstrate the effectiveness of each module in CFN-ESA, we perform a series of ablation experiments in this subsection. Specifically, we remove the recurrence based unimodality encoder (RUME), attention based cross-modality encoder (ACME), and label based emotion-shift module (LESM), respectively, then report the experimental results. The results are showed in TABLE VI. Validity of RUME: When our RUME is removed, the weighted F1 score of the proposed model on the MELD dataset decreases from 66.70% to 66.37%; while on the IEMOCAP dataset, the accuracy of CFN-ESA decreases by 0.45% from the original 70.78%. The primary reason for the declines is that CFN-ESA loses the ability to model local context. Thus, our CFN-ESA relies on RUME to extract dialogue-level contextual information.\n\nValidity of ACME: Since the input to LESM depends on two forward propagations of ACME, the input comes from the results of two forward propagations of RUME when ACME is removed. To put it differently, we directly use RUME instead of ACME. As can be seen from the table, when we remove ACME, the accuracy of our CFN-ESA on the MELD dataset decreases by 0.65%, obtaining a result of 67.20%; while on the IEMOCAP dataset, the model's weighted F1 scores show a significant decrease of 2.96%. The above results indicate that our ACME plays an essential role in adequately capturing multimodal complementary information and has the capability to cross-modal interaction. Validity of LESM: In similar fashion to the experimental results discussed previously, both the weighted F1 score and accuracy of the proposed CFN-ESA decline when we remove LESM. On the MELD dataset, the weight F1 score of our model drops to 66.40%; on the IEMOCAP dataset, the accuracy of CFN-ESA decreases by 0.77% from 70.78%. These phenomena suggest that LESM, as an auxiliary task to ERC, can capture emotion-shift information in conversations, which facilitates the optimization and enhancement of emotional expression for utterances.\n\nOverall, regardless of which module of CFN-ESA is removed, there are degradation in the performance on these two datasets. It can be visualized in Fig.  10  that the performance of CFN-ESA decreases after the removal of different modules. In summary, it can be stated that these modules we designed for the model are valid.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "G. Comparison Of Acme And Transformer Encoder",
      "text": "In this subsection, we attempt to replace ACME with a Transformer encoder (TFE)  [36] , employing two distinct input schemes: TFE-1 and TFE-2. They are defined as follows:\n\n(1) in TFE-1, the three uni-modal representations derived from RUME are combined at the sequence level, resulting in an input sequence length of 3 × N umU tter and a feature dimension of DimF eat; (2) in TFE-2, the three uni-modal representations from RUME are concatenated at the feature level, yielding an input sequence length of N umU tter and a feature dimension of 3×DimF eat. Here, N umU tter denotes the number of utterances, and DimF eat represents the feature dimension for each utterance. As illustrated in Fig.  11 , if there are 3 utterances with a feature dimension of 4 per utterance, then under these schemes, (1) TFE-1 has a sequence length of 9 and a feature dimension of 4; (2) TFE-2 has a sequence length of 3 and a feature dimension of 12.\n\nTABLE VII reports the experimental results of these two schemes on the MELD and IEMOCAP datasets. On the MELD dataset, the scheme TFE-2 obtains an F1 score of 66.25%, which is superior to TFE-1. The opposite result appears on the IEMOCAP dataset, with scheme TFE-1 achieving a higher F1 score compared to TFE-2. Regardless, ACME consistently surpasses both TFE schemes in terms of performance, indicating that our proposed ACME exhibits superior multimodal modeling capabilities over TFE.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "H. Correlation Analysis",
      "text": "In Fig.  12 , we display the variations in F1 scores for both emotion-shift prediction and emotion prediction, with the aim to explore their inherent correlation. On the MELD dataset, as shown in Fig.  12a , the scores for both emotion-shift prediction and emotion prediction are gradually increasing simultaneously as the epoch grows. As can be noticed from Fig.  12b , a parallel trend is also evident on the IEMOCAP dataset. According to the above phenomena, it can be inferred that there exists a direct correlation between emotion-shift prediction and emotion prediction tasks. This means that emotion-shift prediction (auxiliary task) can promote emotion prediction (main task), which further validates the importance of LESM. In addition, these observations can inspire future methods to focus on improving the performance of the emotion-shift module, making the emotion prediction more accurate.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "I. Sentiment Classification",
      "text": "We replace emotion with sentiment in this subsection in order to conduct the task of sentiment classification in conversations. In other words, we transform CFN-ESA into a threeclassification (i.e., neutral, positive, and negative) model. Note that since the IEMOCAP dataset does not contain sentiment labels, we need to merge the original emotion. The specific merging scheme is as follows: sad, angry, and frustrated are merged into negative; happy and excited are merged into positive; neutral remains unchanged.\n\nThe experimental results of our sentiment classification are reported in TABLE VIII. It can be observed that after the emotions are coarsened into sentiments, the weighted F1 scores and accuracies of CFN-ESA on these two datasets are improved. For instance, the accuracy of CFN-ESA on the MELD dataset is improved from 67.85% to 73.75%, with an increase of 5.9%; on the IEMOCAP dataset, the weighted F1 score of the proposed CFN-ESA improves from 71.04% to 84.49%, with an increment of 13.45%.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "J. Case Study",
      "text": "We discuss a case of emotion shift in this subsection. Fig.  13  shows a conversational scenario in the IEMOCAP dataset. When a speaker utters several consecutive times with the true emotion neutral, most models such as MM-DFN tend to predict the emotion of next utterance as neutral. This is due to the fact that these models tend to model based on context, which leads to overly focusing on the contextual information and ignoring the inter-modal self-information. On the contrary, since CFN-ESA can capture emotion-shift information exploiting LESM, which enables the model to strike a trade-off between contextual modeling and selfmodeling, e.g., capturing more inter-modal self-information (AKA multimodal complementary information), it identifies the next utterance as the correct emotion anger.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "K. Error Studies",
      "text": "Fig.  14  shows confusion matrices of our CFN-ESA on the MELD and IEMOCAP datasets. Comparing these two subfigures, it can be concluded that the classification effect of CF-ESA on the IEMOCAP dataset is better than that on the MELD dataset. One primary reason is that MELD is a severely class-imbalanced dataset, where fear, sadness, and disgust belong to the extreme minority classes. As can be witnessed in Fig.  14a , the above three classes perform the worst. In most cases, the model tends to recognize them as the majority class (i.e., neutral) on the MELD dataset.\n\nAnother limitation is that, like most ERC models, our CFN-ESA suffers from the similar-emotion problem. In other words, because the characteristics of some emotions is close to or belongs to the same sentiment, it is difficult for CFN-ESA to differentiate them. For example, on the MELD dataset, the true emotion disgust is easily classified as anger; on the IEMOCAP data, the proposed CFN-ESA recognizes the true emotion happy as excited in some cases, as well as detects the true angry as frustrated. In the case of class imbalance, a minority class itself is hard to recognize correctly, and it is recognized as either the majority class or similar emotion. For example, in Fig.  14a , disgust is easily categorized as either the majority class neutral or similar emotion anger. Thus, the similar-emotion problem becomes more severe in classimbalanced case.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "Previous multimodal ERC models exist some flaws, such as (1) failure to distinguish the amount of emotional information in each modality, which causes difficulty in adequately modeling multimodal data; and (2) failure to consider emotionshift information and overfocusing on capturing intra-modal contextual information, which results in the model not being ability to correctly identify emotions under some emotionshift scenarios. To address the above issues, we propose a multimodal conversational emotion recognition network, CFN-ESA, to efficiently capture multimodal emotional information, providing a new modeling scheme for the ERC task. Our CFN-ESA mainly contains recurrence based uni-modality encoder (RUME), attention based cross-modality encoder (ACME), and label based emotion-shift module (LESM). The function of RUME is to capture intra-modal contextual information at the conversation level and to narrow the differences in the distribution of multimodal data; ACME takes textual modality as the main source of emotional information, which can effectively  \"Ses05M_impro07_F016\": \"neutral\", \"Ses05M_impro07_F017\": \"neutral\", \"Ses05M_impro07_F018\": \"neutral\", \"Ses05M_impro07_F019\": \"neutral\", \"Ses05M_impro07_F020\": \"neutral\", \"Ses05M_impro07_F021\": \"excited\", \"Ses05M_impro07_F022\": \"excited\",\n\nPredict of extract inter-modal complementary information; and LESM is used to extract emotion-shift information, which guides the main task to reduce intra-modal contextual modeling under emotion-shift scenario, thereby optimizing the emotional expression of the utterance. To demonstrate the effectiveness of CFN-ESA, we conduct comparison experiments and ablation studies on two conversational emotion datasets (i.e., MELD and IEMOCAP). The results of comparison experiments prove that the proposed CFN-ESA outperforms all baselines; the results of ablation studies verify that each component in CFN-ESA can effectively upgrade the performance of the model.\n\nTheoretically, the visual information plays an instrumental role in providing direct emotional cues for the model. Since the visual data often involves a lot of noise from complex environmental scenes, our approach, like most models, has difficulty capturing visual emotional information. Exploring methods that fully utilize the visual modality is a worthwhile research direction in future work. The architecture based on emotion shift merits deeper investigation. The phenomenon of emotion shift is pervasive in dialogue systems and often exerts a detrimental effect on the performance of the model. Consequently, in future work, it is plausible to incorporate emotionshift prediction as an auxiliary task with the aim to enhance its precision, thereby potentially leading to further improvements in the performance. Also, verifying the generalizability of ERC models is an intriguing subject. For instance,  (1)  training the model on an independent dataset and subsequently testing its performance on another, thereby providing empirical evidence for its cross-dataset recognition ability; and (2) applying the model to a more challenging real-world dataset in order to substantiate its robustness and practical effectiveness under extreme or unpredictable conditions.",
      "page_start": 12,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: A conversational scene from the MELD dataset. If only textual",
      "page": 1
    },
    {
      "caption": "Figure 1: , textual uni-modality may not be",
      "page": 1
    },
    {
      "caption": "Figure 2: The overall architecture of our CFN-ESA. First, the utterance-level features of visual, textual, and acoustic modalities are extracted by DenseNet,",
      "page": 4
    },
    {
      "caption": "Figure 2: , CFN-ESA mainly consists of the",
      "page": 4
    },
    {
      "caption": "Figure 3: The network structure of RUME. Note that RUME shares parameters",
      "page": 4
    },
    {
      "caption": "Figure 3: Specifically, the",
      "page": 4
    },
    {
      "caption": "Figure 4: The network structure of ACME. (a), (b), and (c) show the structure for visual, textual, and acoustic information updating in ACME, respectively.",
      "page": 5
    },
    {
      "caption": "Figure 4: , we take inspiration from the Transformer structure",
      "page": 5
    },
    {
      "caption": "Figure 5: Specifically, assume that there exist three utterances",
      "page": 6
    },
    {
      "caption": "Figure 5: An example of constructing emotion-shift probability tensor T123. Here, T123 can be viewed as a 3 × 3 dimensional matrix composed of feature",
      "page": 7
    },
    {
      "caption": "Figure 6: F1 scores of CFN-ESA and MM-DFN for each emotion class.",
      "page": 8
    },
    {
      "caption": "Figure 6: It is obvious that our CFN-",
      "page": 8
    },
    {
      "caption": "Figure 7: shows the T-SNE visualization of the original feature and",
      "page": 9
    },
    {
      "caption": "Figure 7: Comparison of T-SNE visualization before and after feature extraction",
      "page": 9
    },
    {
      "caption": "Figure 8: illustrates the comparison",
      "page": 10
    },
    {
      "caption": "Figure 8: Comparison of different modal settings with textual modality.",
      "page": 10
    },
    {
      "caption": "Figure 9: a depicts the effect of RUME with different",
      "page": 10
    },
    {
      "caption": "Figure 9: b, which reveals",
      "page": 10
    },
    {
      "caption": "Figure 9: a. Specifically, the",
      "page": 10
    },
    {
      "caption": "Figure 9: Effect of different network depths on the performance. The subfigure on the left (or right) indicates the effect of network depth for RUME (or ACME).",
      "page": 11
    },
    {
      "caption": "Figure 10: Visualization results for removing different modules on the perfor-",
      "page": 11
    },
    {
      "caption": "Figure 10: that the performance",
      "page": 11
    },
    {
      "caption": "Figure 11: , if there",
      "page": 11
    },
    {
      "caption": "Figure 11: Illustration of two input schemes (i.e., TFE-1 and TFE-2) for TFE.",
      "page": 12
    },
    {
      "caption": "Figure 12: , we display the variations in F1 scores for",
      "page": 12
    },
    {
      "caption": "Figure 12: a, the scores for both emotion-shift",
      "page": 12
    },
    {
      "caption": "Figure 12: b, a parallel trend is also evident on the IEMOCAP",
      "page": 12
    },
    {
      "caption": "Figure 13: shows a conversational scenario in the IEMOCAP dataset.",
      "page": 12
    },
    {
      "caption": "Figure 14: shows confusion matrices of our CFN-ESA on",
      "page": 12
    },
    {
      "caption": "Figure 14: a, the above three classes perform the",
      "page": 12
    },
    {
      "caption": "Figure 14: a, disgust is easily categorized as either",
      "page": 12
    },
    {
      "caption": "Figure 12: The variations in F1 scores for both emotion-shift prediction and emotion prediction as the epoch increases on the MELD and IEMOCAP datasets.",
      "page": 13
    },
    {
      "caption": "Figure 13: A conversational case in the IEMOCAP dataset.",
      "page": 13
    },
    {
      "caption": "Figure 14: Confusion matrices on the MELD and IEMOCAP datasets. Note that in the confusion matrices, we convert predicted quantities into proportions.",
      "page": 14
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Models": "",
          "MELD": "neutral\nsurprise\nfear\nsadness\njoy\ndisgust\nanger\nF1\nF1\nF1\nF1\nF1\nF1\nF1"
        },
        {
          "Models": "AGHMN [1]\nDialogXL [2]\nI-GCN [3]\nCauAIN [4]\nCoG-BART [5]",
          "MELD": "76.40\n49.70\n11.50\n27.00\n52.40\n14.00\n39.40\n-\n-\n-\n-\n-\n-\n-\n78.00\n51.60\n8.00\n38.50\n54.70\n11.80\n43.50\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-"
        },
        {
          "Models": "MMGCN [18]\nDialogueTRM [22]\nMetaDrop [19]\nHU-Dialogue [21]\nMM-DFN [20]\nUniMSE [33]",
          "MELD": "76.33\n48.15\n-\n26.74\n53.02\n-\n46.09\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n77.76\n50.69\n-\n22.93\n54.78\n-\n47.82\n-\n-\n-\n-\n-\n-\n-"
        },
        {
          "Models": "CFN-ESA",
          "MELD": "80.05†\n58.78†\n21.62†\n41.82†\n66.50†\n26.92†\n54.18†\n79.93±0.40‡\n58.47±0.37‡\n22.41±2.24‡\n41.16±2.23‡\n64.78±1.25‡\n30.14±2.50‡\n53.91±1.25‡"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Models": "",
          "IEMOCAP": "happy\nsad\nneutral\nangry\nexcited\nfrustrated\nF1\nF1\nF1\nF1\nF1\nF1"
        },
        {
          "Models": "AGHMN [1]\nDialogXL [2]\nI-GCN [3]\nCauAIN [4]\nCoG-BART [5]",
          "IEMOCAP": "52.10\n73.30\n58.40\n61.90\n69.70\n62.30\n-\n-\n-\n-\n-\n-\n83.80\n50.00\n59.30\n64.60\n74.30\n59.00\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-"
        },
        {
          "Models": "MMGCN [18]\nDialogueTRM [22]\nMetaDrop [19]\nHU-Dialogue [21]\nMM-DFN [20]\nCOGMEN [32]\nUniMSE [33]",
          "IEMOCAP": "45.14\n77.16\n64.36\n68.82\n74.71\n61.40\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n75.56\n42.22\n78.98\n66.42\n69.77\n66.33\n51.90\n81.70\n68.60\n66.00\n75.30\n58.20\n-\n-\n-\n-\n-\n-"
        },
        {
          "Models": "CFN-ESA",
          "IEMOCAP": "53.67†\n71.65†\n70.32†\n68.06†\n80.60†\n74.82†\n56.76±2.60‡\n81.34±0.62‡\n71.19±0.63‡\n68.23±2.98‡\n75.83±1.71‡\n65.50±3.19‡"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Modal Settings": "Textual\nVisual\nAcoustic",
          "MELD\nW-F1\nAcc": "65.81\n67.09\n32.05\n48.05\n41.46\n49.35",
          "IEMOCAP\nW-F1\nAcc": "66.57\n66.56\n44.23\n45.01\n51.08\n53.45"
        },
        {
          "Modal Settings": "T + V\nT + A\nV + A",
          "MELD\nW-F1\nAcc": "65.93\n67.13\n65.94\n67.16\n43.25\n50.34",
          "IEMOCAP\nW-F1\nAcc": "67.86\n67.58\n68.46\n68.67\n59.83\n60.36"
        },
        {
          "Modal Settings": "T + V + A",
          "MELD\nW-F1\nAcc": "66.70\n67.85",
          "IEMOCAP\nW-F1\nAcc": "71.04\n70.78"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Values of λ": "Manual",
          "MELD\nW-F1\nAcc": "66.51\n67.74\n66.47\n67.70\n66.53\n67.74\n66.52\n67.74\n66.53\n67.78\n66.58\n67.78\n66.57\n67.74\n66.67\n67.82\n66.70\n67.85\n66.68\n67.82",
          "IEMOCAP\nW-F1\nAcc": "70.77\n70.52\n70.64\n70.40\n70.77\n70.52\n70.78\n70.52\n70.64\n70.40\n70.65\n70.40\n70.75\n70.52\n70.90\n70.65\n70.96\n70.72\n71.04\n70.78"
        },
        {
          "Values of λ": "Automatic",
          "MELD\nW-F1\nAcc": "66.64\n67.78",
          "IEMOCAP\nW-F1\nAcc": "70.98\n70.72"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Models": "CFN-ESA",
          "MELD\nW-F1\nAcc": "66.70\n67.85",
          "IEMOCAP\nW-F1\nAcc": "71.04\n70.78"
        },
        {
          "Models": "-w/o RUME\n-w/o ACME\n-w/o LESM",
          "MELD\nW-F1\nAcc": "66.37\n67.51\n65.97\n67.20\n66.40\n67.62",
          "IEMOCAP\nW-F1\nAcc": "70.25\n70.33\n68.08\n67.97\n70.22\n70.01"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Modules": "TFE-1\nTFE-2",
          "MELD\nW-F1\nAcc": "65.91\n67.01\n66.25\n67.55",
          "IEMOCAP\nW-F1\nAcc": "68.74\n68.67\n68.25\n69.25"
        },
        {
          "Modules": "ACME",
          "MELD\nW-F1\nAcc": "66.70\n67.85",
          "IEMOCAP\nW-F1\nAcc": "71.04\n70.78"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Models": "",
          "MELD": "neutral\npositive\nnegative\nW-F1\nAcc\nF1\nF1\nF1",
          "IEMOCAP": "neutral\npositive\nnegative\nW-F1\nAcc\nF1\nF1\nF1"
        },
        {
          "Models": "CFN-ESA-Emo\nCFN-ESA-Sent",
          "MELD": "-\n-\n-\n66.70\n67.85\n78.71\n67.06\n70.42\n73.74\n73.75",
          "IEMOCAP": "-\n-\n-\n71.04\n70.78\n88.03\n70.06\n90.99\n84.49\n84.78"
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Real-time emotion recognition via attention gated hierarchical memory network",
      "authors": [
        "M Wenxiang Jiao",
        "I King"
      ],
      "year": "2020",
      "venue": "The Thirty-Fourth AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "2",
      "title": "DialogXL: All-in-one xlnet for multi-party conversation emotion recognition",
      "authors": [
        "W Shen",
        "J Chen",
        "X Quan",
        "Z Xie"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "3",
      "title": "I-GCN: Incremental graph convolution network for conversation emotion detection",
      "authors": [
        "W Nie",
        "R Chang",
        "M Ren",
        "Y Su",
        "A Liu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "4",
      "title": "CauAIN: Causal aware interaction network for emotion recognition in conversations",
      "authors": [
        "W Zhao",
        "Y Zhao",
        "X Lu"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, L. D. Raedt"
    },
    {
      "citation_id": "5",
      "title": "Contrast and generation make bart a good dialogue emotion recognizer",
      "authors": [
        "S Li",
        "H Yan",
        "X Qiu"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "6",
      "title": "ISNet: Individual standardization network for speech emotion recognition",
      "authors": [
        "W Fan",
        "X Xu",
        "B Cai",
        "X Xing"
      ],
      "year": "2022",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "7",
      "title": "Multitask learning from augmented auxiliary data for improving speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "8",
      "title": "BAT: Block and token self-attention for speech emotion recognition",
      "authors": [
        "J Lei",
        "X Zhu",
        "Y Wang"
      ],
      "year": "2022",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "9",
      "title": "Multi-classifier interactive learning for ambiguous speech emotion recognition",
      "authors": [
        "Y Zhou",
        "X Liang",
        "Y Gu",
        "Y Yin",
        "L Yao"
      ],
      "year": "2022",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "10",
      "title": "Multimodal mutual attentionbased sentiment analysis framework adapted to complicated contexts",
      "authors": [
        "L He",
        "Z Wang",
        "L Wang",
        "F Li"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "11",
      "title": "Hybrid contrastive learning of tri-modal representation for multimodal sentiment analysis",
      "authors": [
        "S Mai",
        "Y Zeng",
        "S Zheng",
        "H Hu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "12",
      "title": "Learning modality-specific representations with self-supervised multi-task learning for multimodal sentiment analysis",
      "authors": [
        "W Yu",
        "H Xu",
        "Z Yuan",
        "J Wu"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "13",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "N Majumder",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "14",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "D Hazarika",
        "S Poria",
        "A Zadeh",
        "E Cambria",
        "L.-P Morency",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter"
    },
    {
      "citation_id": "15",
      "title": "ICON: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "D Hazarika",
        "S Poria",
        "R Mihalcea",
        "E Cambria",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "16",
      "title": "DialogueRNN: An attentive RNN for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "17",
      "title": "MISA: Modality-invariant and -specific representations for multimodal sentiment analysis",
      "authors": [
        "D Hazarika",
        "R Zimmermann",
        "S Poria"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "18",
      "title": "MMGCN: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "J Hu",
        "Y Liu",
        "J Zhao",
        "Q Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "19",
      "title": "Learning what and when to drop: Adaptive multimodal and contextual dynamics for emotion recognition in conversation",
      "authors": [
        "F Chen",
        "Z Sun",
        "D Ouyang",
        "X Liu",
        "J Shao"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "20",
      "title": "MM-DFN: Multimodal dynamic fusion network for emotion recognition in conversations",
      "authors": [
        "D Hu",
        "X Hou",
        "L Wei",
        "L Jiang",
        "Y Mo"
      ],
      "year": "2022",
      "venue": "Proceedings of ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "21",
      "title": "Modeling hierarchical uncertainty for multimodal emotion recognition in conversation",
      "authors": [
        "F Chen",
        "J Shao",
        "A Zhu",
        "D Ouyang",
        "X Liu",
        "H Shen"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "22",
      "title": "DialogueTRM: Exploring multi-modal emotional dynamics in a conversation",
      "authors": [
        "Y Mao",
        "G Liu",
        "X Wang",
        "W Gao",
        "X Li"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021"
    },
    {
      "citation_id": "23",
      "title": "COS-MIC: Commonsense knowledge for emotion identification in conversations",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "A Gelbukh",
        "R Mihalcea",
        "S Poria"
      ],
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020"
    },
    {
      "citation_id": "24",
      "title": "Directed acyclic graph network for conversational emotion recognition",
      "authors": [
        "W Shen",
        "S Wu",
        "Y Yang",
        "X Quan"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "25",
      "title": "XLNet: Generalized autoregressive pretraining for language understanding",
      "authors": [
        "Z Yang",
        "Z Dai",
        "Y Yang",
        "J Carbonell",
        "R Salakhutdinov",
        "Q Le"
      ],
      "year": "2019",
      "venue": "Proceedings of the 33rd International Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "26",
      "title": "BART: Denoising sequence-tosequence pre-training for natural language generation, translation, and comprehension",
      "authors": [
        "M Lewis",
        "Y Liu",
        "N Goyal",
        "M Ghazvininejad",
        "A Mohamed",
        "O Levy",
        "V Stoyanov",
        "L Zettlemoyer"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "27",
      "title": "An investigation of emotion changes from speech",
      "authors": [
        "Z Huang"
      ],
      "year": "2015",
      "venue": "Proceedings of 2015 International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "28",
      "title": "Detecting the instant of emotion change from speech using a martingale framework",
      "authors": [
        "Z Huang",
        "J Epps"
      ],
      "year": "2016",
      "venue": "Proceedings of 2016 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "29",
      "title": "MDAN: Multi-level dependent attention network for visual emotion analysis",
      "authors": [
        "L Xu",
        "Z Wang",
        "B Wu",
        "S Lui"
      ],
      "year": "2022",
      "venue": "Proceedings of 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "30",
      "title": "Dependency Exploitation: A unified cnn-rnn approach for visual emotion recognition",
      "authors": [
        "X Zhu",
        "L Li",
        "W Zhang",
        "T Rao",
        "M Xu",
        "Q Huang",
        "D Xu"
      ],
      "year": "2017",
      "venue": "Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "31",
      "title": "WSCNet: Weakly supervised coupled networks for visual sentiment classification and detection",
      "authors": [
        "D She",
        "J Yang",
        "M.-M Cheng",
        "Y.-K Lai",
        "P Rosin",
        "L Wang"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "32",
      "title": "COGMEN: COntextualized GNN based multimodal emotion recognitioN",
      "authors": [
        "A Joshi",
        "A Bhat",
        "A Jain",
        "A Singh",
        "A Modi"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "33",
      "title": "UniMSE: Towards unified multimodal sentiment analysis and emotion recognition",
      "authors": [
        "G Hu",
        "T.-E Lin",
        "Y Zhao",
        "G Lu",
        "Y Wu",
        "Y Li"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "34",
      "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "authors": [
        "C Raffel",
        "N Shazeer",
        "A Roberts",
        "K Lee",
        "S Narang",
        "M Matena",
        "Y Zhou",
        "W Li",
        "P Liu"
      ],
      "year": "2020",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "35",
      "title": "Shapes of emotions: Multimodal emotion recognition in conversations via emotion shifts",
      "authors": [
        "K Bansal",
        "H Agarwal",
        "A Joshi",
        "A Modi"
      ],
      "year": "2022",
      "venue": "Proceedings of the First Workshop on Performance and Interpretability Evaluations of Multimodal, Multipurpose, Massive-Scale Models, Virtual"
    },
    {
      "citation_id": "36",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "37",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Y.-H Tsai",
        "S Bai",
        "P Liang",
        "J Kolter",
        "L.-P Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "38",
      "title": "Auxformer: Robust approach to audiovisual emotion recognition",
      "authors": [
        "L Goncalves",
        "C Busso"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "39",
      "title": "Dawn of the transformer era in speech emotion recognition: Closing the valence gap",
      "authors": [
        "J Wagner",
        "A Triantafyllopoulos",
        "H Wierstorf",
        "M Schmitt",
        "F Burkhardt",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "40",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly",
        "J Uszkoreit",
        "N Houlsby"
      ],
      "year": "2021",
      "venue": "Proceedings of the Ninth International Conference on Learning Representations"
    },
    {
      "citation_id": "41",
      "title": "BLIP-2: bootstrapping languageimage pre-training with frozen image encoders and large language models",
      "authors": [
        "J Li",
        "D Li",
        "S Savarese",
        "S Hoi"
      ],
      "year": "2023",
      "venue": "Proceedings of the 40th International Conference on Machine Learning"
    },
    {
      "citation_id": "42",
      "title": "Visual instruction tuning",
      "authors": [
        "H Liu",
        "C Li",
        "Q Wu",
        "Y Lee"
      ],
      "year": "2023",
      "venue": "Proceedings of the Thirty-seventh Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "43",
      "title": "SimCSE: Simple contrastive learning of sentence embeddings",
      "authors": [
        "T Gao",
        "X Yao",
        "D Chen"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "44",
      "title": "Multi-task learning using uncertainty to weigh losses for scene geometry and semantics",
      "authors": [
        "R Cipolla",
        "Y Gal",
        "A Kendall"
      ],
      "year": "2018",
      "venue": "Proceedings of 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "45",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "46",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "47",
      "title": "Densely connected convolutional networks",
      "authors": [
        "G Huang",
        "Z Liu",
        "L Van Der Maaten",
        "K Weinberger"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "48",
      "title": "Training deep networks for facial expression recognition with crowd-sourced label distribution",
      "authors": [
        "E Barsoum",
        "C Zhang",
        "C Ferrer",
        "Z Zhang"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "49",
      "title": "Recognising realistic emotions and affect in speech: State of the art and lessons learnt from the first challenge",
      "authors": [
        "B Schuller",
        "A Batliner",
        "S Steidl",
        "D Seppi"
      ],
      "year": "2011",
      "venue": "Speech Commun"
    },
    {
      "citation_id": "50",
      "title": "RoBERTa: A robustly optimized BERT pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "RoBERTa: A robustly optimized BERT pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "51",
      "title": "Decoupled weight decay regularization",
      "authors": [
        "I Loshchilov",
        "F Hutter"
      ],
      "year": "2019",
      "venue": "Proceedings of the Seventh International Conference on Learning Representations"
    }
  ]
}