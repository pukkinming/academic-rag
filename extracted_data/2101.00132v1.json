{
  "paper_id": "2101.00132v1",
  "title": "Audio Content Analysis",
  "published": "2021-01-01T01:22:22Z",
  "authors": [
    "Alexander Lerch"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Introduction",
      "text": "Audio signals contain a wealth of information: just by listening to an audio signal, we are able to infer a variety of properties. For example, a speech signal not only transports the textual information, but might also reveal information about the speaker (gender, age, accent, etc.) and the recording environment (e.g., indoors vs. outdoors). In a music signal we might identify the instruments playing, the musical structure or musical genre, the melody, harmonies, and tonality, the projected emotion, and characteristics of the performance as well as the proficiency of the performers. An audio signal can contain and transport a wide variety of content beyond these examples; the field of Audio Content Analysis (ACA) aims at creating and using algorithms for automatically extracting this information from the raw (digital) audio signal  [56] , enabling us to sort, categorize, segment, and visualize the audio signal based on its content. Use cases include applications such as content-based automatic playlist generation and music recommendation systems, computer-assisted music production and editing, and intelligent music tutoring systems identifying mistakes and areas of improvement for young instrumentalists.\n\nThis chapter gives an overview of ACA techniques and applications. While the processing of speech signals is covered in Chapter ??, this chapter focuses on music signals, where ACA is often referred to as Music Information Retrieval (MIR)  [84, 14] , although the latter additionally encompasses the analysis and generation of symbolic (non-audio) music data such as musical scores.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Audio Content",
      "text": "It is important to identify the main sources of content in order to understand what encompasses the content of a signal. In music recordings, the content can be traced back to three origins:\n\n‚Ä¢ Composition: while in western classical music, this would be a written full score, in other musical styles it might be a lead sheet or simply a musical idea. The content related to the composition allows us to recognize different renderings of the same song or symphony as being the same piece. In most genres of western music, this encompasses musical elements such as melody, harmony, and rhythm. ‚Ä¢ Music performance: the performance realizes the composition in a unique acoustic rendition, the actual music that can be perceived. A performance communicates the explicit information from the composition but also interprets and adds to this information. This happens through variations of tempo, timing, dynamics, and playing techniques. ‚Ä¢ Music production: since the input of an analysis system is usually an audio recording, the choices made during the recording as well as the editing and processing can significantly influence the final result  [64] . Microphone positioning, filtering, and editing are examples of the production teams' impact on the final recording.\n\nAn analogy can again be found for speech recordings: the composition corresponds to the text, the performance to the actual speech, and the production to the recording and audio processing.\n\nFrom a musical point of view, audio content can categorized into timing (tempo and rhythm), dynamics, pitch, and timbre. The combination of specific characteristics and variations across these categories can convey higher level content such as musical genre or conveyed emotion. In addition to the musical content alone, a recording might also contain additional content such as song lyrics.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Generalized Audio Content Analysis System",
      "text": "A digital audio signal is represented as a series of numbers, so-called samples (see Chap. ??). Direct inspection of these samples -in case of CD audio quality 44100 per second-does not necessarily allow the observer to draw conclusions about the audio content. A long term context, however, might still give some insights: Fig.  1  shows the common waveform representation (sample values over time) of three different audio signals: a string quartet recording, a pop production, and speech.\n\nThe general shape of the waveform envelope already enables an observer to differentiate between these different signals by deriving descriptive properties related to dynamic range, fluctuations, and pauses. This can be modeled algorithmically: first, one or more descriptors which capture general properties of the audio signal can be defined (for instance, a measure for how often and how much the waveform envelope changes), and second, a mapping for different descriptor ranges to the audio signal class can be heuristically found (for instance, thresholding a descriptor for classifying the signal as pop). These two processing steps form a simplified model of a generalized system for ACA as shown in Fig.  2 : the first stage extracts descriptors, also commonly referred to as audio features, and the second stage infers the desired content information from the features. There are some parallels of these two stages to two steps in human information processing, namely perception and cognition.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Feature Extraction",
      "text": "The Feature Extraction stage has two main objectives: first, it reduces the overall amount of data to be processed, leading to a compact representation of the content. For example, some audio classification systems use only dozens or hundreds of feature values to describe a complete song instead of millions of samples. Second, it focuses on the relevant aspects of the content, stripping away irrelevant and possibly redundant information. If, for example, a system is supposed to detect the fundamental frequencies of an audio signal, the feature set should probably be invariant to loudness and timbre. It is important to note that features do not necessarily have to be musically meaningful or even interpretable by humans, it suffices that they contain the relevant information needed for inference to provide a correct mapping to the human-understandable target meta data. This is particularly true for most state-of-the-art systems: while for a long time experts carefully designed features capturing specific content information (such as the envelope variation in the simple example above), the last decade has seen a new generation of data-driven systems which automatically learn such features from data  [44] . Prominent examples of these modern systems are neural networks which can automatically learn a compressed representation of the input data as features.\n\nThe input of the majority of ACA systems is either the waveform as discussed above or some kind of spectrogram representation. A spectrogram is a pseudo-3D plot that, compared to the waveform, often gives more insights into the frequency content of the signal by plotting the magnitude of many overlapping Short Time Fourier Transforms (STFT) over time. Figure  3  shows the first 24 bars of a single saxophone playing the jazz standard 'Summertime;' the top visualizes the common waveform representation (x-axis: time, y-axis: amplitude) and the bottom displays the spectrogram (x-axis: time, y-axis: frequency, color: amplitude). Each column of the spectrogram is one magnitude spectrum of a short block of samples with low values colored blue and high values colored yellow. While the phrases are easily identifiable in both representations, the spectrogram also shows that (i) it is a recording of a single monophonic instrument (clear spectral structure of fundamental frequency and harmonics at integer multiples), (ii) it is an instrument that allows vibrato (see seconds 3 and 18), (iii) it is an instrument with a significant number of harmonics (number and strength of \"parallel\" lines), and that (iv) the melody can be directly derived from the visualization by identifying the lowest frequency of the harmonic series and mapping it to musical pitch.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Inference Stage",
      "text": "The second stage of the ACA system, the inference, takes the extracted features and maps them into a domain both usable and comprehensible by humans. In other words, it interprets the feature data and maps it to meaningful meta-data, such as a class label for the example in Fig.  1  (speech, chamber music, pop) or the pitch of a melody. This inference system can be an expert-defined algorithm, a classifier, or a regression algorithm. The more condensed and the more meaningful the features are, the less powerful the inference algorithm has to be and vice versa: a raw feature representation close to the audio sample requires a sophisticated inference approach.\n\nIt is important to note that no machine learning system will work with hundred percent accuracy in all but the most trivial tasks. Every machine model of data and its patterns will always be imperfect just like a human annotating data, and it is the goal of the scientists and engineers to minimize the number of errors.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Music Transcription",
      "text": "Music transcription systems estimate (explicit or implicit) score information from the audio recording. Music transcription broadly defined encompasses a variety of extraction tasks covering, for instance, melody, chords, musical key, musical instruments, rhythm, time signature, and structure. The basics of music transcription systems will be explained here by introducing simple example approaches to various tasks.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Musical Key Detection",
      "text": "Estimating the key of a musical audio signal has multiple applications ranging from large-scale musicological studies to enabling DJs to automatically identify songs with tonal compatibility for mash-ups. Key detection systems could work very reliably if they could simply utilize a transcribed version of all notes with pitch and length as the key is mostly defined by the pitch content of a piece. Practically, however, this approach is not necessarily the most robust approach as, on the one hand, the transcription of pitches from polyphonic audio remains to be challenging and error prone and, on the other hand, key detection approaches can work reasonably well without requiring such detailed information  [79, 22, 45] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Pitch Chroma",
      "text": "The most common feature that is used for key detection is the so-called average pitch chroma, which is an easy-to-extract octave-independent approximation of the pitch content in the audio. Its use was first proposed in the context of chord detection  [32] . Similar to many other low-level features, the pitch chroma is usually derived from a STFT computed on the blocked audio signal. For pitch chroma computation, the frequency bins are grouped to the frequency of the closest musical pitch, e.g., A4. Then, all the magnitudes belonging to the same pitch class (e.g., A3, A4, A5) are accumulated over the octaves, resulting in a 12-dimensional pitch class vector (C, C#, D, D#, etc.) per audio block. The result is shown in Fig.  4  (bottom); it is similar to the spectrogram in the sense that it is a pseudo-3D plot with time on the x-axis, however, the y-axis represents the 12 pitch classes C-B now. The pitch chroma is a fitting feature for detecting the musical key as it focuses on the tonal content, reduces timbre and rhythm interference, and removes key-irrelevant octave information  [33, 70] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Inference",
      "text": "Under the simplifying assumption that the key will not change over the course of the piece of music (an assumption mostly valid for some genres such as rock and pop but invalid for others, e.g., classical music), the average pitch chroma of the whole piece gives a good approximation of the overall pitch content of the piece. Figure  5  displays an average pitch chroma extracted from a pop song in key D Major; it shows that the pitches D and A are the most prominent (salient and often occurring) and that unlikely pitch classes such as G# are less salient. A simple key detection system uses such an extracted pitch chroma and infers the estimated key by comparing it to previously defined pitch class distribution templates, also referred to as key profiles. Examples for such templates as shown in Fig.  6  can be derived from music knowledge (diatonic), listening experiments on tonality (Krumhansl)  [52] , or data (Temperley)  [92] . One disadvantage of the diatonic profile is that the profile is identical between a major key and its relative (aeolic) minor key; the other two key profiles solve this issue by allowing different weights for different scale degrees. As the key profile of, for instance, C# Major can be assumed to be identical to the C Major key profile but shifted by one semi-tone, only two 12-dimensional templates are required for the basic task of detecting major and minor keys. The final key estimate is the key that minimizes the distance between the extracted average pitch chroma and the (shifted) major and minor key profile templates.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Results",
      "text": "Modern key detection systems achieve correct detection rates of approximately 60-80 %, depending on the data used for evaluation. The most common errors are confusions with the relative key (e.g., G Major vs. E Minor), the closely related keys (e.g., D Major vs. A Major), and the parallel key (e.g., A Major vs. A Minor), which is easily understandable due to the significantly overlapping pitch content.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Monophonic Fundamental Frequency Detection",
      "text": "The estimation of the varying fundamental frequency ùëì 0 from a single-voiced audio recording enables a variety of different applications ranging from pitch correction over automatic accompaniment systems to karaoke systems. It is generally considered to be a solved problem, although some standard systems sometimes still lack the robustness required by specific applications. Fundamental frequency detection systems detect repeating patterns in tonal, quasi-periodic components of the signal. Most approaches build on the basic assumption that the single-voiced signal is a weighted superposition of multiple sinusoidals with frequencies at integer multiples of the fundamental frequency. Once the fundamental frequency is detected, it can be easily mapped to a musical pitch.\n\nOne of the most intuitive ways of approaching fundamental frequency detection is to measure the distance between zero-crossings and local maxima (or minima) to estimate the fundamental period length, the inversion of which is the fundamental frequency ùëì 0 . While this is a simple way of approaching this problem, the results are too unreliable to make it practically usable, especially in the case of numerous harmonics.\n\nOver the past few decades, a variety of approaches to fundamental frequency detection have been proposed; the following sections present representative approaches for two analysis domains, the time domain and the frequency domain.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Auto Correlation Function",
      "text": "Nearly every pitched signal is periodic with its fundamental period length. An established way of detecting this periodicity or self-similarity is the Auto Correlation Function (ACF). Assuming that the fundamental frequency of a signal does not significantly change within a short block of samples of length N , the periodicity can be found by multiplying this block of samples ùë•(ùëõ) with a shifted version of itself ùë•(ùëõ + ùúÇ) and summing the result for each ùúÇ: cf. https://www.music-ir.org/mirex/wiki/2019:Audio_Key_Detection_Results, last accessed 01/14/2020\n\nThe result will be maximal at a shift of ùúÇ = 0 (maximum self similarity), but it will also show local maxima at multiples of the fundamental period length. This is due to the high similarity of neighboring periods of the periodic signal. Thus, the ACF indicates the similarity per shift ùúÇ. The shift of the local maximum is therefore an indicator of the length of the fundamental period length in samples. The ACF is often normalized so that ùëü ùë• ùë• (0) = 1. Figure  7  visualizes this function (bottom) for an example signal (top) for ùúÇ ‚â• 0. The ACF has been a popular pitch detection algorithm for many decades  [80]  and related and modified algorithms are still used  [66] .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Harmonic Product Spectrum",
      "text": "Intuitively, the frequency domain is a fitting domain to find the fundamental frequency as it is a relatively sparse representation of the frequency content of each block. Simply picking the location of the maximum of the STFT, however, does not lead to a reliable fundamental frequency estimate, as the loudest tonal component of natural sounds is often one of the higher harmonics instead of the fundamental frequency itself. The Harmonic Product Spectrum (HPS) is a way to address this challenge by taking advantage of the comb-like structure of the spectrum of a periodic sound in the frequency domain  [73] , as the magnitude spectrum of a periodic sound will show local maxima at the location of fundamental frequency as well as its integer multiples. The HPS is computed iteratively: first, the magnitude spectrum is decimated by keeping only every second value so that the length of the spectrum is halved. Whatever the fundamental frequency is, the location of the second harmonic now has the same index as the fundamental frequency of the original spectrum. Multiplying those two spectra will increase the value at the fundamental frequency (multiplication of local maxima) and minimize at other locations of the spectrum (lower magnitudes). This process is iteratively repeated while decimating the spectrum by factors of 3, 4, 5, etc., with the maximum at the fundamental frequency getting more and more pronounced as higher harmonics are multiplied. The location of the maximum of this HPS is, then, the estimated fundamental frequency. Figure  8  illustrates the decimated spectra and the resulting HPS. The HPS is a simple way of detecting the fundamental frequency in the frequency domain, but it faces some inherent challenges. First, the spectral frequency resolution is often insufficient especially for signals at low frequencies. Second, it might fail for signals in which one harmonic has low magnitude. Third, fundamental frequencies that fall between bin frequencies might not be detected. But even given these issues, the HPS is a clever and intriguing way of using the harmonic structure in the frequency domain to detect the fundamental frequency.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Polyphonic Fundamental Frequency Detection",
      "text": "While fundamental frequency detection for single-voiced recordings is considered a solved problem, a multi-voiced, polyphonic signal still poses challenges to state-ofthe-art systems  [7] . Historically, this task was initially approached with methods that can be summarized under the term Iterative Subtraction: first, a monophonic pitch detector is applied to the signal to detect the most salient fundamental frequency. Second, this frequency is stored is a candidate and all its harmonics are removed from the signal. Third, this process is repeated until the termination criteria are met. Termination criteria can include, for example, a maximum number of voices or a minimum remaining energy in the signal. Approaches utilizing iterative subtraction have been reasonably successful in the 2000s and have been applied to both time domain signals  [19]  and frequency domain signals  [49] .\n\nLater, Non-Negative Matrix Factorization (NMF) became widely-used for polyphonic fundamental frequency detection  [90] , and was especially successful when applied to instruments with quantized frequencies such as the piano  [8] . NMF attempts to approximate one positive matrix (usually the spectrogram) through the multiplication of two matrices which are randomly initialized and iteratively updated, based on the distance between the target spectrogram and the result of the multiplication. After convergence, the two matrices can be interpreted as the template dictionary, containing all basic sound components that might contribute to the final spectrogram, and the activation, indicating how strong each template is at each time. In the case of the piano, the templates would be all the different piano pitches and the activations indicate the volume of each pitch over time.\n\nUnsurprisingly, the majority of modern systems for polyphonic pitch detection is based on deep neural networks  [6] . Most of them work with a spectrogram-like input representation. The achieved accuracies vary, dependent on the data set used for evaluation, between 40 and 70%. These and more results can be found on the website for the so-called MIREX (Music Information Retrieval Evaluation eXchange), an annual event comparing the performance state-of-the-art systems for a variety of Music Information Retrieval tasks on a variety of data and metrics.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Musical Structure Identification",
      "text": "Most music is inherently formally organized and structured into various hierarchical levels, starting from groups of notes, bars, and phrases to sections and movements. The detection of musical structure can enable intelligent listening applications, capable of jumping to specific segments such as chorus or verse, or large-scale musicological analyses. The way humans infer structure is based on three main characteristics cf.\n\nhttps://www.music-ir.org/mirex/wiki/2019:Multiple_Fundamental_ Frequency_Estimation_\\%26_Tracking_Results_-_MIREX_Dataset, last accessed 01/14/2020  [78] , (i) novelty and contrast, meaning something new and possibly unexpected happens, (ii) homogeneity, meaning that similar parts tend to be grouped together, and (iii) repetition, meaning that the recognition of a repeated segment indicates a structural segment. All of these characteristics can be represented through a variety of musical elements including but not limited to harmony, rhythm, melody, instrumentation, tempo, and dynamics. Therefore, a system for automatic structure detection will extract one or more features representing these musical elements and compute an intermediate representation of the feature data. The most common intermediate representation for a structure detection system is a so-called Self-Similarity Matrix (SSM). The SSM is an intuitive way of indicating and visualizing structure. It is computed by extracting a meaningful feature to investigate, for example, the pitch chroma for tonal content. Then, a pairwise similarity is computed between each short-time pitch chroma and all others, leading to a self-similarity matrix as shown in Fig.  9 . The resulting SSM is symmetric; both axes of the SSM indicate time and Fig.  9  Self-Similarity Matrix for Michael Jackson's \"Bad\" the similarity between (ùë° 1 , ùë° 2 ) equals the similarity between (ùë° 2 , ùë° 1 ). The diagonal indicates the maximum of self-similarity (red). Blocks of constant red color indicate areas of high homogeneity such a held chord or a short repeated pattern, and blue vertical or horizontal lines indicate low similarity to all other points (often rests and pauses).\n\nThe SSM shown in Fig.  9  was computed from Michael Jackson's pop song \"Bad.\" We can clearly identify several structural elements of the song: the intro stops at about 19 s, the bridge at 60-69 s is followed by a chorus (69-86 s) and the same constellation is repeated (as indicated by the line parallel to the diagonal) at 120 s, the high homogeneity of the instrumental is indicated by the red box from 145-170 s, and the songs ends with four repetitions of the chorus starting at 180 s visualized by the high similarity with diagonal structure in the end.\n\nThere are several ways to use the SSM to algorithmically infer the musical structure. One way is to use image processing techniques. For instance, the start of a new segment might be indicated by peaks in the output of a high pass filter (checker kernel) swiped along the diagonal  [30] . Repetitions, indicated by lines parallel to the diagonal, might be detected with edge detection techniques on the SSM  [23] .\n\nThe formal evaluation of structure detection systems remains a challenge: human annotators of musical structure, even if in agreement, tend to annotate different structural levels. For instance, what might just be the two segment structure (A) (B) for one annotator could easily be annotated by another as (a b) (c d), which makes an evaluation by simply matching the ground truth impractical, given that both annotations are correct and the system might detect either one.",
      "page_start": 11,
      "page_end": 13
    },
    {
      "section_name": "Music Performance Analysis And Assessment",
      "text": "Since the ultimate goal of most music transcription tasks is the extraction of a scorelike description from an audio file, the performance information is often discarded. There is, however, a notable difference between the two renditions of the same sonata from two pianists or the realization of a jazz standard by different performers  [76] . Music Performance Analysis (MPA) deals with extracting this performance information from audio recordings. What constitutes performance information varies from genre to genre, but the performance parameters usually considered to be most impactful are dynamics and tempo  [55] , although expressive variations of pitch (vibrato, intonation) and playing techniques can play important roles as well.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Dynamics",
      "text": "Musical dynamics are closely related to loudness, although the absolute perceived loudness is not the only cue to indicate musical dynamics  [98] . A forte passage on a recording is, for example, easily identifiable even at low reproduction volume. Therefore, measures of acoustic intensity are outperformed by humans judging musical dynamics  [71] . Even so, it is a valid simplification to reduce the extraction of musical dynamics to extracting simple features representing the energy of a signal as described in Sect. 4.2.1.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Tempo And Beat Detection",
      "text": "The tempo of a performance has been shown to convey structural information of the music to the listener  [75] . Moreover, tempo and its variation has been linked to the perception of projected emotion  [47] . The tempo of a piece of music is set by a train of quasi-equidistant pulses, so the so-called tactus  [58] . The tactus indicates the beats at which listeners will clap or tap their foot with the music. It is important to note that the tactus is a perceptual concept: while it is determined by groups and accents of note events, the pulse of a tactus may or may not fall on a note event, and a note event may or may not fall on a pulse. The frequency of a tactus is usually in the general range of 1-3 Hz, corresponding to 60-180 BPM (Beats Per Minute)  [31] .\n\nAutomatically extracting the tempo and the beats from audio recordings enables applications in music recommendation and systems for playlist generation as well as in creative usages such as DJ software.\n\nAs the pulse train is periodically repeating, the concept of detecting the tempo shows some similarity to the task of fundamental frequency detection as both tasks focus on estimating the period length of a periodic input signal. The main difference is the time scale of this detection: while for fundamental frequency detection the period lengths of interest range from approximately 0.3-30 ms, a typical beat period length is approximately in the range of 300-1000 ms.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Novelty Function",
      "text": "Due to these different time scales and because the series of samples carries a large amount of unrelated information, a beat analysis is usually applied to an intermediary representation, the so-called novelty function  [56] . The novelty function is a timeseries of values that has local maxima at positions where \"something new is happening\" such as the onset of a new note or a drum hit and is low at times when nothing new begins such as during a held note. While early systems attempted to extract this novelty function from the time domain envelope itself  [88] , it can be extracted from a variety of representations or features, including tonal representations such as the pitch chroma and low level features describing spectral shape such as MFCCs or the spectral centroid (see Sect. 4.2.1)  [63] . The common process for extracting the novelty function involves (i) extracting the feature, (ii) computing the derivative over time, (iii) truncating negative values to zero, and (iv) smoothing the result with a lowpass filter.\n\nIf the extracted novelty function works as intended, the local maxima indicate note onsets; therefore, picking the peaks of this function is referred to as onset detection  [5] . Figure  10  shows an example of a novelty function and picked onsets.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Tempo Induction",
      "text": "As the novelty function gives an indication of note events or other musical events, there is no direct mapping from these events to tempo since, as mentioned above, note events do not necessarily fall on beat events and vice versa. However, either (or both) the novelty function or the series of detected onsets are useful representations for infering the tempo. More specifically, the periodicity of the novelty function allows to estimate the tempo.\n\nTo give an example of an early system for tempo induction, Scheirer proposed to use a bank of resonance filters  [85] . Each filter is tuned to one possible tempo, for example, 120 BPM, and the resonance frequency of the filter with the highest output energy is the most likely tempo. This means that the real tempo can only be detected if it is close to a filter frequency; thus, the number of filters combined with the overall range indicate the possible tempo resolution.\n\nOther simple ways of detecting the tempo include an Autocorrelation analysis of the novelty function  [35]  or a picking the maximum of the Inter-Onset-Interval histogram  [26] .\n\nAn example for a state of the art system for tempo induction is based on recurrent neural networks for analysis and yielding results in the range of 50-90 % depending on the dataset  [4] .",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Beat Detection",
      "text": "The knowledge of the tempo indicates the period length of the \"foot-tapping rate\" or the distance between the pulses, respectively, however, it does not imply the actual beat locations, sometimes referred to as beat phase. Thus, beat detection systems aim at detecting the beat locations from the novelty function.\n\nOne of the first systems proposed to detect beats based on oscillators with adaptive parameters spawned a whole class of beat tracking systems: here, a pulse generator predicts beat locations which are then compared to actual onset times and strengths; dependent on the distance between onset and beat as well as their positions, the pulse generator parameters are adapted to optimize the estimated fit with future onsets  [53] . Both beat phase and beat distance are adapted and estimated simultaneously. The advantage of these oscillator-based systems is that they are capable of real-time processing; their main disadvantage is slow adaptation to sudden tempo changes.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Performance Assessment",
      "text": "Instead of extracting individual performance parameters, the goal of performance assessment is the estimation of overall ratings for a performance, taking into account parameters spanning the domains pitch (vibrato rates, intonation, tuning and temperament), dynamics (accents, tension), and timbre (playing techniques, instrument settings). This is a task of considerable commercial interest as it enables musically intelligent training software.\n\nPerformance assessment is a ubiquitous aspect of music pedagogy: regular feedback from teachers improves the students' playing and auditions are used to place students in ensembles. It is, however, seen as a highly subjective and aesthetically challenging task with considerable disagreement on assessments between educators  [93, 99 ]. An automatic system for assessment of performances would provide objective, reliable, and repeatable feedback to the student during practice sessions and increase accessibility of affordable instrument education. Generally, the structure of a performance assessment system resembles the basic structure of an audio content analysis system: features describing the performance are extracted and then used in the inference stage to estimate one or more ratings.\n\nThe features might be simple standard features as used in other content analysis systems  [51]  or designed specifically for the task of performance assessment (e.g., pitch and timing stability)  [72, 1, 100] . Some systems also incorporate musical score information into the feature computation  [25, 10, 96] .\n\nThe general trend in content analysis from feature design towards feature learning can also be observed in studies on performance assessment  [57] , albeit a bit more reluctant. One of the reasons for this reluctance is the non-interpretability of learned features; an educational setting requires not only an accurate assessment but also an explanation of the reasons for that assessment (and possibly a strategy to improve the performance).\n\nThe success rate of tools for automatic performance assessment still leaves room for improvement. Most of the presented systems either work well only for very select data  [51]  or have comparably low prediction accuracies  [96, 100] , rendering them unusable in most practical scenarios.",
      "page_start": 16,
      "page_end": 17
    },
    {
      "section_name": "Music Identification And Categorization",
      "text": "A significant part of research in the area of ACA is concerned with the categorization of audio data, e.g., into musical genres. This group of tasks is -from a technical point of view-related to the estimation of similarity between different music files as well as the estimation of the emotion in a recording of music. Before we go into details on how to automatically categorize music, we will first look into music identification through audio fingerprinting, which was the first MIR technology with large scale adoption by consumers in the early 2000s.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Audio Fingerprinting",
      "text": "Audio fingerprinting aims at representing a recording in a small and unique so-called fingerprint (also: perceptual hash) in order to look up this recording in a previously prepared database and map it to the stored meta data. In contrast to many other presented systems in this chapter, fingerprinting is not concerned with extracting musical meaning from audio but solely with identifying a recording unambiguously. The two main applications of audio fingerprinting are (i) the automated monitoring of broadcast stations for independent supervision of the broadcasted songs in order to verify broadcasting rights, and (ii) end consumer services allowing the identification of audio in order to provide meta data such as artist name, song title, or album art  [17] . The design of an audio fingerprinting system has to solve multiple inherent core problems  [18] . On the one hand, the fingerprint has to be small in size to be transmitted and searched for efficiently, on the other hand, it has to be unique in order to identify one specific song from a database of possibly millions of songs. Furthermore, it has to be robust against quality degradations in the audio signal, as a user might record the audio of a song playing over speakers in a noisy environment. Last but not least, the fingerprint extraction has to be efficient enough to run on embedded and mobile devices.\n\nA fingerprinting system has two main processing steps: the fingerprint extraction (often on a mobile devices at the client side) and the identification in a database (usually on the server side), see Fig.  11",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Fingerprint Extraction",
      "text": "As the goal of audio fingerprinting is the identification of a specific recording (not a song, meaning it is supposed to differentiate between, e.g., a studio and a live version of the same song), the fingerprint does not have to contain musical information but can focus on the raw content of the audio. A simple predecessor of modern fingerprinting systems proposed for the identification of commercials in broadcast stems used segments of the time domain envelope as a fingerprint  [61] . Nowadays, the fingerprint is usually derived from the STFT. There exist several approaches for fingerprint extraction. Two prominent approaches are (i) to encode spectral bands, more specifically the changes of band energy over both time and frequency in a binary format  [38] , and (ii) to identify salient peaks of the spectrogram and encode their relative location to each other  [97] . The resulting size of the extracted fingerprint depends on the system: the first system, for example, represents three seconds of audio with 8 Kbit  [38] .",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Fingerprint Identification",
      "text": "After successful extraction, the fingerprint of an unknown query signal has to be compared with a large number of previously extracted fingerprints in a database. Since this database can be large, this comparison has to be as efficient as possible in order to minimize processing time. Fingerprint systems use multiple tricks to speed up the lookup process, including hash lookup tables in which database entries are referenced by their fingerprint content or the use of fast-to-compute distance measures such as the Hamming distance  [40] . Reordering the database entries according to their popularity can also decrease the average lookup time significantly.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Music Genre Classification",
      "text": "Music Genre Classification (MGC) is a classical machine learning task that used to be one of the most popular tasks in the field of MIR. It is obviously useful to describe and categorize large collections of music to enable music discovery or content-based music recommendation systems. Despite its initial popularity and clear demand from users for classification systems, the task has some fundamental inherent problems that concern the subjective, inconsistent, and somewhat arbitrary definition of music genre. For example, there is disagreement on genre taxonomies and what they are based on (geography: \"Indian music, \" instrumentation: \"symphonic music,\" epoch: \"baroque,\" technical requirements: \"barbershop,\" or use of the music: \"Christmas songs\")  [74] . Despite these issues, MGC systems can be \"successfully\" developed as long as they adhere to one consistent and coherent definition of genre. The performance of the system strongly depends on (i) the features and the information they are able to transport, (ii) the data that the machine learning system is being trained on, and (iii) the capabilities of the machine learning system or classifier itself.",
      "page_start": 22,
      "page_end": 22
    },
    {
      "section_name": "Feature Examples",
      "text": "Even as the definition of genre is problematic as pointed out above, it is obvious to the observant listener that the identification of specific genres may require information related to timbre, pitch, rhythm and tempo, and dynamics. Traditional systems with custom-designed features therefore use a variety of different features. Hundreds and possibly thousands of different features have been investigated over the years for their performance in MGC systems. The most successful features have been shown, interestingly enough, to be very simple features describing the spectral shape and the intensity of the signal. Many of these well-known low-level features are extracted from short blocks of audio samples, resulting in a time series of values for each feature. In a next step, features are often aggregated over time by computing, for example, their mean or their standard deviation. This means that a complete audio file is ultimately represented by one vector with a length of one or two (mean and standard deviation) times the number of features.",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "Spectral Shape (Timbre) Features",
      "text": "Features describing the spectral shape of a signal are widely used. The spectral shape significantly influences our perception of the timbre of a sound  [42] . Most of these features are extracted from the local magnitude spectrum (one column of the spectrogram as shown in Fig.  3 ). Here, only two commonly used features which have proven useful in analysis tasks will be presented as representatives of a large number of common features.\n\n‚Ä¢ Spectral Centroid: The spectral centroid is the center of mass of a magnitude spectrum, i.e., the frequency at which the spectral magnitudes can be separated into two equal parts. That means that low frequency signals will have a low centroid while substantial high frequency components and noise will increase the centroid. Despite its technical definition, the spectral centroid has been shown to be strongly correlated to the perceptual sound attribute brightness  [67, 16] . ‚Ä¢ Mel Frequency Cepstral Coefficients: The Mel Frequency Cepstral Coefficients (MFCCs) are a measure of the cosine-shape of the STFT on a logarithmic (Mel-shaped) frequency axis. To compute, the STFT axis is first split into logarithmically spaced frequency bands according to the Mel-scale modeling the human frequency perception  [91] . Then, a logarithm is applied to the spectrum and the bands are transformed into the cepstral domain with a Discrete Cosine Transform (DCT). The resulting DCT coefficients are the MFCCs. In contrast to the spectral centroid, the MFCCs are a multidimensional feature similar to the pitch chroma (Sec. 2.1.1), often representing each spectrum with 13 or more values.\n\nThe algorithm for the MFCC computation is an interesting mixture of ideas from perception, signal processing (cepstrum), and data compression (DCT)  [24] . While the interpretability of the MFCCs is limited and their perceptual meaning is questionable, they have proven surprisingly useful in a wide variety of tasks  [60, 46, 41]  and are nowadays considered a standard baseline feature.",
      "page_start": 19,
      "page_end": 20
    },
    {
      "section_name": "Intensity Features",
      "text": "Intensity features model the dynamics or loudness of a recording. The two feature examples given below represent two simple and common features.\n\n‚Ä¢ Envelope: The simplest way to describe the envelope of a signal per block is to find the absolute maximum per block, resulting in the overall shape of the waveform. This is somewhat related to the slightly more complicated Peak Programme Meter (PPM) that is used in recording studios. ‚Ä¢ Root Mean Square: The Root Mean Square (RMS) is a standard way of computing the intensity of a signal. It is the so-called effective value of the signal computed as the square root of the mean of the squared values per block. For long block sizes, it can be an efficient measure of long-term level changes. A common pre-processing step is to filter the signal before computing the RMS to take into account the sensitivity of the human ear at different frequencies for different level ranges (compare A-weighted or C-weighted sound pressure level measures).",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Additional Features",
      "text": "Musical genre is so broadly defined that features representing characteristics from many categories can be meaningful. Therefore, numerous and diverse custom-designed features have been investigated for this task. These features include, for example, stereo features representing the width and form of the stereo image  [95] , pitch content features representing the variety and ranges of pitches  [94] , and tempo and rhythm features describing tempo variation, beat strength, and rhythmic complexity  [27, 15] . Many of these features are, unlike the features introduced above, extracted from longer windows of data. For instance, attempts of describing the rhythmic content need a context window of approx. 5 s or more to be meaningful. As pointed out above, the era of feature design is nowadays considered a thing of the past (except for problems with only small amounts of available data) and modern features are often learned directly from the spectrogram. Common feature learning approaches are based, for example, on neural networks  [54, 39]  or dictionary learning  [65, 101] .",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Classification",
      "text": "The simplest, most intuitive classifier is just a threshold: if a feature value is higher than a threshold ùúñ choose class 1, otherwise class 2. A modern data-driven system derives this threshold from the data itself and generalizes to a multi-dimensional space with possibly non-linear thresholds. In other words, it learns what combination of feature values are common for each class and how to differentiate between classes given these feature values. Figure  12  shows a so-called scatter plot for two classes represented by two features (two-dimensional feature space). It can be seen that for this data, the RMS feature seems to work slightly better in separating the two classes speech and music than the spectral centroid. This visualization also emphasizes the importance of the so-called training data set; if the estimated thresholds are based on data that are not representative, the classifier will not perform well.\n\nFig.  12  A music/speech dataset visualized in a two-dimensional feature space (x-axis: average spectral centroid, y-axis: standard deviation of rms)\n\nAnother example for a basic classifier is the Nearest-Neighbor classifier  [29] . While training, it stores the location of each data point in the feature space with its corresponding class label. When queried with a new and unseen feature vector, the distance to every single training vector is computed; the final result is the class label of the closest vector, the nearest neighbor. Other classifiers model the distribution of data, avoiding to store each individual data point and allowing for simple generalization of the data. One popular classifier that models the data with Gaussian distributions is the Gaussian Mixture Model  [28] . More modern approaches maximize the separation between classes by mapping the data into high-dimensional spaces (Support Vector Machine  [9] ) or form a so-called ensemble of many simple classifiers to yield a more robust majority vote (Random Forest  [11] ). Most state-of-the-art classifiers are, if the amount of training data allows it, based on (deep) neural networks  [34] .",
      "page_start": 22,
      "page_end": 22
    },
    {
      "section_name": "Music Emotion Recognition",
      "text": "An analysis task for which many consumers seem to see an intuitive need is the extraction of the emotional content of a recording, as a majority of users search for and categorize music by describing the emotional content of the music  [48] . The task definition of Music Emotion Recognition (MER) suffers from similar, possibly more severe, restrictions in terms of subjectivity and noisiness of human-labeled data. The main issues are (i) the question of whether to estimate the conveyed emotion or the elicited emotion, i.e., a subject might perceive the music transporting a specific emotion, but might feel a completely different emotion themselves  [68, 104] , and (ii) the unclear definition of what emotions or moods are actually inherent to music listening, e.g., can music trigger basic emotions such as fear and anger  [86, 87, 104] ? Due to the (commercially) appealing applications of MER, however, these inherent problems have not stopped researchers from targeting this task  [103] . Despite many studies investigating emotional content and impact of music, however, the link between musical parameters to emotions remains largely unknown, and the audio features which could directly describe emotional content are unknown as well. Therefore, the features and approaches commonly chosen for MER are similar to genre classification as described in Sect. 4.2. In addition to such classification approaches, some methods aim at estimating emotions not by sorting them into distinct classes but locating them as coordinates in a two-dimensional valence-arousal plane as proposed by Russel  [83] . In this case, the machine learning system is not choosing one of multiple output categories but estimates two values (valence and arousal). This class of machine learning algorithms is referred to as regression algorithms.",
      "page_start": 22,
      "page_end": 22
    },
    {
      "section_name": "Current Challenges",
      "text": "Looking at the historic development in audio content analysis as well as the currently pressing research issues, three main overall challenges can be identified, (i) the amount of data available for training complex machine learning systems, (ii) the predictability of modern systems and interpretability of the results, and (iii) the inherently abstract musical language and the largely unclear relations of musical concepts and perceptual meaning.",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "Training Data",
      "text": "Machine learning systems are data-driven, meaning that they learn the most likely mapping between input (features) and output (classes) from a set of data. Thus, in order to train a system well, there exist important requirements on data. First, the training data have to be representative. That means that, on the one hand, the possible variability of the input data should be covered completely to enable the system to learn. A music genre can, for instance, not be properly represented by only one band as the system might learn to distinguish the band but not the genre. On the other hand, the distribution of songs per class should reflect the expected distribution so as to not bias the system towards majority classes. Second, the training data should not be noisy, meaning the labels should be consistent and unambiguous. This is, as mentioned above, a problem especially for MGC because of the issues with the definition of music genre. Third, the training data have to be sufficient. The more complex a task is, the more complex a system needs to be, and the more training data is required. Without a sufficient amount of data, a complex system will not be able to generalize a model from the training data and thus will \"overfit,\" meaning that the system works very well on the data it has been trained on but poorly on unseen data  [28] . The amount of training data becomes a crucial issue for machine learning approaches and systems based on Deep Neural Networks which have shown superior performance at nearly all tasks in audio content analysis but require large amounts of data for training.\n\nAlthough there is a vast amount of music data easily accessible, not all of these data can be directly used for training a machine learning system. A system for transcribing drum events from popular music, for instance, needs expert annotations precisely marking each drum hit in terms of instrument and timing (and possibly the playing technique). Marking these individual hits is, however, a very time-consuming and tedious task so that the increasing requirement for data due to the increasing system complexity usually outpaces the annotation of new data by human annotators. Given the multitude of annotations needed for various content analysis tasks, it is likely that the gap between available annotated data and required amount of training data will widen with increasing system complexity. This will result in a growing need of systems and approaches addressing this challenge, as is reflected by an increasing research interest in this problem from various angles. Current approaches include:\n\n‚Ä¢ data augmentation and synthetic data: without sufficient annotated data, the machine learning engineer can \"cheat\" by virtually increasing the amount of training data either with synthesized data (e.g., through synthesis of MIDI data) or by processing existing audio data with irrelevant transformations, for instance, pitch shifting for music genre classification or segmenting the longer file into shorter segments  [69] ; ‚Ä¢ transfer learning: although data might be scarce for one task, it might be available for related tasks; therefore, the idea of transfer learning is to take an internal representation of a system trained for one task with abundant data and use this (hopefully powerful) representation as a feature input to a more simple classifier that can be trained with significantly less data  [21] ; ‚Ä¢ weakly labeled data: annotating audio data with high accuracy is a tedious and time-consuming task, however, it becomes significantly less demanding if high time accuracy is not required by, e.g., labeling the presence of a musical instrument in a snippet of audio without pinpointing its exact occurrence time(s); this requires, however, to modify existing machine learning approaches to deal with the weak time accuracy  [36] ; ‚Ä¢ self-supervised systems and unsupervised systems which tackle the data challenge in MIR in a different way by exploring possibilities of training systems with unlabeled data; it is, for example, possible to train a complex system with high data requirements with unlabeled data by utilizing the outputs of pre-existing systems for these unlabeled data as training targets for the new system  [102]  or by utilizing synthesis iteratively to learn from unlabeled data  [20] .\n\nAs the field of machine learning evolves rapidly, it is difficult to predict if any of the methods above will become standard approaches. It is clear, however, that the lack of large-scale training data will continue to impact the progress and methods applied to audio content analysis.",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "Interpretability And Understandability",
      "text": "The increasing complexity of machine learning systems not only affects the required amount of training data, but also leads to highly complex systems which do not allow easy interpretation of internal states, easy analysis of results, or understanding of influencing factors  [59] . In traditional feature-driven designs, each feature had at the minimum a technical connotation allowing an expert to connect system outputs to somewhat interpretable inputs. As modern systems learn features from the data, it becomes increasingly difficult to derive this link, thus limiting the interpretability and control over system behavior. This restricts possibilities to tweak system outputs for specific inputs and could increase the likelihood of the system producing unexpected results especially for unseen data. Recent years have seen this problem partly being addressed in the field of audio content analysis through (i) visualization that gives insights into the networks' internal states or intermediate representations, diagnoses the embedded space, or disentangles the complex internal representations into interpretable graphs  [105] , (ii) enforcing interpretable latent spaces or intermediate representations that are humanly understandable through regularization  [37, 13] , and (iii) analysis and transformation of latent spaces to interpretable spaces  [2] .",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "Perceptual Meaning",
      "text": "A challenge unrelated to the technical progress but somewhat emphasized by the engineering-driven methodologies in the field is the question of the perceptual relevance of the results of ACA-systems. The problems of data annotation in the context of musical genre and emotion have already been discussed in Sects. 4.2 and 4.3, however, there are other fundamental issues when it comes to \"musical meaning.\" While, for example, the task of extracting drum onset times from audio is clearly defined, it is less clear what higher musical meaning can be derived from it. It is not easy to find answers to questions such as is there a generalized way of describing rhythm and rhythmic properties, and can specific rhythmic properties and/or timing variations be mapped to specific affectual responses in humans? A major obstacle impeding MIR research is the inability to successfully isolate (and therefore understand) the various score-based and performance characteristics that contribute to the music listening experience. The listener, however, has to be the ultimate judge of the usefulness of any audio content analysis system  [57] .",
      "page_start": 25,
      "page_end": 25
    },
    {
      "section_name": "Outlook",
      "text": "Audio Content Analysis is an emerging research field facing interesting challenges and enabling a wide range of future applications. We are already seeing new applications and emerging companies building on the advances made in the field.",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "Music Education",
      "text": "The idea of utilizing technology to assist music (performance) education is not new. Seashore pointed out the value of scientific observation of music performances for improving learning as early as the 1930s  [89] . One of the earliest studies exploring the potential of computer-assisted techniques in the music classroom was carried out by Allvin  [3] . Although his work was focused on using technology for providing individualized instruction and developing aural and visual aids to facilitate learning, it also highlighted the potential of using ACA techniques such as pitch detection to perform error analysis in a musical performance and provide constructive feedback to the learners through (semi-)automated music tutoring software. Such software aims at supplementing teachers by providing students with insights and interactive feedback by analyzing and assessing the audio of practice sessions. The ultimate goals of an interactive music tutor are to highlight problematic parts of the students' performance, provide a concise yet easily understandable analysis, give specific and understandable feedback on how to improve, and individualize the curriculum depending on the students' mistakes and general progress. Tools for performance assessment typically assess one or more performance parameters which are usually related to the accuracy of the performance in terms of pitch and timing  [100, 96, 77, 62]  or quality of sound (timbre)  [51, 82] . Various (commercial) solutions are already available, exhibiting a similar set of goals. These systems adopt different approaches, ranging from traditional music classroom settings to games targeting a playful learning experience. Examples for tutoring applications are SmartMusic , Yousician , Music Prodigy , and SingStar .",
      "page_start": 25,
      "page_end": 26
    },
    {
      "section_name": "Music Production",
      "text": "Knowledge of the audio content enables the improvement of music production tools in various dimensions. The most obvious enhancement can be found in terms of productivity and efficiency: the better a software understands the details of incoming audio streams or files, the better it can adapt, for instance, by applying default gain and equalization parameters  [81]  or suggest compatible recordings from a library. Systems might support editors by automatic artifact-free splicing of multiple recordings from one session or selecting error-free recordings from a set of recordings. Modern processing methods allow for subtle or dramatic timing and pitch variations in high quality -controlling them with musically relevant content-adaptive intelligence could streamline music production in unprecedented ways.\n\nModern tools also enhance the creative possibilities in the production process. For example, creating harmonically meaningful background choirs by analyzing the lead vocals and the harmony track is already technically feasible nowadays. Knowing and possibly separating sound sources in a recording could enable new ways of modifying or morphing different sounds to create new soundscapes and auditory scenes. Many more scenarios are conceivable where audio analysis will impact the production process, although the multifaceted character of the field makes it difficult to predict specific use cases.",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "Music Distribution And Consumption",
      "text": "Audio analysis has already started to transform consumer-facing industries such as streaming services with audio-based music recommendation and playlist generation systems using an in-depth understanding of the musical content  [50] . This is not only the case for the end-consumer themselves: there is also a industry need for automatically identifying music and creating playlists that conform to the company's brand image  [43] .\n\nIn the near future, we can expect the rise of creative music discovery and listening applications that enable the listener to interact not only by choosing content but interact with the content itself. This could include, for example, the gain adjustment for individual voices, replacing instruments or vocalists, or interactively changing the musical arrangement.",
      "page_start": 27,
      "page_end": 27
    },
    {
      "section_name": "Generative Music",
      "text": "An important outcome of being able to extract machine interpretable content information from audio data is the possibility for these data to feed generative algorithms. The automatic composition and rendition of music is emerging as a challenging yet popular research direction  [12] , gaining interest from both research institutions and industry. While bigger questions concerning capabilities and restrictions of computational creativity as well as aesthetic evaluation of algorithmically generated music remain largely unanswered, practical applications such as generating background music for user videos and commercial advertisements are currently in the focus of many researchers. The interactive and adaptive generation of sound tracks for video games as well as individualized generation of license-free music content for streaming are additional long-term goals of considerable commercial interest.",
      "page_start": 27,
      "page_end": 27
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows the common waveform representation (sample values over time) of three",
      "page": 2
    },
    {
      "caption": "Figure 1: Waveform representation of three diÔ¨Äerent audio recordings: speech (left), string quartet",
      "page": 3
    },
    {
      "caption": "Figure 2: the Ô¨Årst stage extracts descriptors,",
      "page": 3
    },
    {
      "caption": "Figure 2: General processing stages of a system for audio content analysis",
      "page": 3
    },
    {
      "caption": "Figure 3: Waveform (top) and spectrogram (bottom) view of a single-voiced saxophone recording of",
      "page": 4
    },
    {
      "caption": "Figure 3: shows the Ô¨Årst 24 bars of a single saxophone playing the jazz standard",
      "page": 4
    },
    {
      "caption": "Figure 1: (speech, chamber music, pop) or the pitch of a melody.",
      "page": 5
    },
    {
      "caption": "Figure 4: (bottom); it is similar",
      "page": 6
    },
    {
      "caption": "Figure 4: Spectrogram (top) and pitch chroma (bottom) view of a single-voiced saxophone recording",
      "page": 6
    },
    {
      "caption": "Figure 5: displays an average pitch chroma extracted from a pop song in key D Major; it shows",
      "page": 6
    },
    {
      "caption": "Figure 5: Average pitch chroma of a pop song in the key of D Major",
      "page": 7
    },
    {
      "caption": "Figure 6: can be derived from music knowledge (diatonic), listening experiments on tonality",
      "page": 7
    },
    {
      "caption": "Figure 6: Three common template normalized key proÔ¨Åles (C major)",
      "page": 7
    },
    {
      "caption": "Figure 7: visualizes this function (bottom) for an",
      "page": 9
    },
    {
      "caption": "Figure 7: Normalized auto correlation function (bottom) of a periodic audio input (top)",
      "page": 9
    },
    {
      "caption": "Figure 8: illustrates the decimated spectra",
      "page": 10
    },
    {
      "caption": "Figure 8: 4th order Harmonic Product Spectrum and decimated spectra (bottom) for the signal shown",
      "page": 10
    },
    {
      "caption": "Figure 9: The resulting SSM is symmetric; both axes of the SSM indicate time and",
      "page": 12
    },
    {
      "caption": "Figure 9: Self-Similarity Matrix for Michael Jackson‚Äôs ‚ÄúBad‚Äù",
      "page": 12
    },
    {
      "caption": "Figure 9: was computed from Michael Jackson‚Äôs pop song ‚ÄúBad.‚Äù",
      "page": 12
    },
    {
      "caption": "Figure 10: shows an example of a novelty function and picked onsets.",
      "page": 14
    },
    {
      "caption": "Figure 10: RectiÔ¨Åed audio envelope (top) and novelty function with picked onsets (bottom)",
      "page": 15
    },
    {
      "caption": "Figure 11: bottom. It can only identify recordings which",
      "page": 17
    },
    {
      "caption": "Figure 11: Flowchart of a general Ô¨Ångerprinting system",
      "page": 18
    },
    {
      "caption": "Figure 3: ). Here, only two commonly used features which have",
      "page": 19
    },
    {
      "caption": "Figure 12: shows a so-called scatter plot for two classes",
      "page": 21
    },
    {
      "caption": "Figure 12: A music/speech dataset visualized in a two-dimensional feature space (x-axis: average",
      "page": 21
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "Automatic Competency Assessment of Rhythm Performances of Ninth-grade and Tenth-grade Pupils",
      "authors": [
        "J Abe√üer",
        "J Hasselhorn",
        "S Grollmisch",
        "C Dittmar",
        "A Lehmann"
      ],
      "year": "2014",
      "venue": "Proceedings of the International Computer Music Conference (ICMC)"
    },
    {
      "citation_id": "2",
      "title": "Discovering Interpretable Representations for Both Deep Generative and Discriminative Models",
      "authors": [
        "T Adel",
        "Z Ghahramani",
        "A Weller"
      ],
      "year": "2018",
      "venue": "Proceedings of the International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "3",
      "title": "Computer-Assisted Music Instruction: A Look at the Potential",
      "authors": [
        "R Allvin"
      ],
      "year": "1971",
      "venue": "Journal of Research in Music Education"
    },
    {
      "citation_id": "4",
      "title": "Accurate Tempo Estimation Based on Recurrent Neural Networks and Resonating Comb Filters",
      "authors": [
        "S B√∂ck",
        "F Krebs",
        "G Widmer"
      ],
      "year": "2015",
      "venue": "Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)"
    },
    {
      "citation_id": "5",
      "title": "A tutorial on onset detection in music signals",
      "authors": [
        "J Bello",
        "L Daudet",
        "S Abdallah",
        "C Duxbury",
        "M Davies",
        "M Sandler"
      ],
      "year": "2005",
      "venue": "Transactions on Speech and Audio Processing",
      "doi": "10.1109/TSA.2005.851998"
    },
    {
      "citation_id": "6",
      "title": "Automatic Music Transcription: An Overview",
      "authors": [
        "E Benetos",
        "S Dixon",
        "Z Duan",
        "S Ewert"
      ],
      "year": "2019",
      "venue": "IEEE Signal Processing Magazine",
      "doi": "10.1109/MSP.2018.2869928"
    },
    {
      "citation_id": "7",
      "title": "Automatic music transcription: challenges and future directions",
      "authors": [
        "E Benetos",
        "S Dixon",
        "D Giannoulis",
        "H Kirchhoff",
        "A Klapuri"
      ],
      "year": "2013",
      "venue": "Journal of Intelligent Information Systems",
      "doi": "10.1007/s10844-013-0258-3"
    },
    {
      "citation_id": "8",
      "title": "Blind Signal Decompositions for Automatic Transcription of Polyphonic Music: NMF and K-SVD on the Benchmark",
      "authors": [
        "N Bertin",
        "R Badeau",
        "G Richard"
      ],
      "venue": "Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "9",
      "title": "",
      "authors": [
        "Honolulu Ieee"
      ],
      "year": "2007",
      "venue": "",
      "doi": "10.1109/ICASSP.2007.366617"
    },
    {
      "citation_id": "10",
      "title": "A Training Algorithm for Optimal Margin Classifiers",
      "authors": [
        "B Boser",
        "I Guyon",
        "V Vapnik"
      ],
      "year": "1992",
      "venue": "Proceedings of the Fifth Annual Workshop on Computational Learning Theory, COLT '92",
      "doi": "10.1145/130385.130401"
    },
    {
      "citation_id": "11",
      "title": "A Dataset and Baseline System for Singing Voice Assessment",
      "authors": [
        "B Bozkurt",
        "O Baysal",
        "D Y√ºret"
      ],
      "year": "2017",
      "venue": "Proceedings of the International Symposium on CMMR"
    },
    {
      "citation_id": "12",
      "title": "Random Forests",
      "authors": [
        "L Breiman"
      ],
      "year": "2001",
      "venue": "Machine Learning",
      "doi": "10.1023/A:1010933404324"
    },
    {
      "citation_id": "13",
      "title": "Deep Learning Techniques for Music Generation. Computational Synthesis and Creative Systems",
      "authors": [
        "J Briot",
        "G Hadjeres",
        "F Pachet"
      ],
      "year": "2020",
      "venue": "Deep Learning Techniques for Music Generation. Computational Synthesis and Creative Systems",
      "doi": "10.1007/978-3-319-70163-9"
    },
    {
      "citation_id": "14",
      "title": "MIDI-VAE: Modeling Dynamics and Instrumentation of Music with Applications to Style Transfer",
      "authors": [
        "G Brunner",
        "A Konrad",
        "Y Wang",
        "R Wattenhofer"
      ],
      "venue": "Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)"
    },
    {
      "citation_id": "15",
      "title": "",
      "authors": [
        "France Paris"
      ],
      "year": "2018",
      "venue": ""
    },
    {
      "citation_id": "16",
      "title": "Music Information Retrieval",
      "authors": [
        "J Burgoyne",
        "I Fujinaga",
        "J Downie"
      ],
      "year": "2015",
      "venue": "A New Companion to Digital Humanities",
      "doi": "10.1002/9781118680605.ch15"
    },
    {
      "citation_id": "17",
      "title": "Hierarchical Automatic Audio Signal Classification",
      "authors": [
        "J Burred",
        "A Lerch"
      ],
      "year": "2004",
      "venue": "Journal of the Audio Engineering Society (JAES)"
    },
    {
      "citation_id": "18",
      "title": "Acoustic correlates of timbre space dimensions: A confirmatory study using synthetic tones",
      "authors": [
        "A Caclin",
        "S Mcadams",
        "B Smith",
        "S Winsberg"
      ],
      "year": "2005",
      "venue": "Journal of the Acoustical Society of America (JASA)",
      "doi": "10.1121/1.1929229"
    },
    {
      "citation_id": "19",
      "title": "Audio Fingerprinting: Concepts And Applications",
      "authors": [
        "P Cano",
        "E Batlle",
        "E Gomez",
        "L Gomes",
        "M Bonnet"
      ],
      "year": "2005",
      "venue": "Computational Intelligence for Modelling and Prediction"
    },
    {
      "citation_id": "20",
      "title": "A Review of Audio Fingerprinting",
      "authors": [
        "P Cano",
        "E Batlle",
        "T Kalker",
        "J Haitsma"
      ],
      "year": "2005",
      "venue": "The Journal of VLSI Signal Processing-Systems for Signal, Image, and Video Technology",
      "doi": "10.1007/s11265-005-4151-3"
    },
    {
      "citation_id": "21",
      "title": "Multiple period estimation and pitch perception model",
      "authors": [
        "A Cheveign√©",
        "H Kawahara"
      ],
      "year": "1999",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "22",
      "title": "Deep Unsupervised Drum Transcription",
      "authors": [
        "K Choi",
        "K Cho"
      ],
      "year": "2019",
      "venue": "Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)"
    },
    {
      "citation_id": "23",
      "title": "Transfer Learning for Music Classification and Regression Tasks",
      "authors": [
        "K Choi",
        "G Fazekas",
        "M Sandler",
        "K Cho"
      ],
      "year": "2017",
      "venue": "Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)"
    },
    {
      "citation_id": "24",
      "title": "Polyphonic Audio Key Finding Using the Spiral Array CEG Algorithm",
      "authors": [
        "C Chuan",
        "E Chew"
      ],
      "year": "2005",
      "venue": "International Conference on Multimedia and Expo",
      "doi": "10.1109/ICME.2005.1521350"
    },
    {
      "citation_id": "25",
      "title": "Music Structure Analysis from Acoustic Signals",
      "authors": [
        "R Dannenberg",
        "M Goto"
      ],
      "venue": "Handbook of Signal Processing in Acoustics"
    },
    {
      "citation_id": "27",
      "title": "Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences",
      "authors": [
        "S Davis",
        "P Mermelstein"
      ],
      "year": "1980",
      "venue": "Transactions on Acoustics, Speech, and Signal Processing",
      "doi": "10.1109/TASSP.1980.1163420"
    },
    {
      "citation_id": "28",
      "title": "Automatically extracting performance data from recordings of trained singers",
      "authors": [
        "J Devaney",
        "M Mandel",
        "D Ellis",
        "I Fujinaga"
      ],
      "year": "2011",
      "venue": "Psychomusicology: Music, Mind and Brain",
      "doi": "10.1037/h0094008"
    },
    {
      "citation_id": "29",
      "title": "A Beat Tracking System for Audio Signals",
      "authors": [
        "S Dixon"
      ],
      "year": "1999",
      "venue": "Proceedings of the Conference on Mathematical and Computational Methods in Music"
    },
    {
      "citation_id": "30",
      "title": "Classification of Dance Music by Periodicity Patterns",
      "authors": [
        "S Dixon",
        "E Pampalk",
        "G Widmer"
      ],
      "year": "2003",
      "venue": "Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)"
    },
    {
      "citation_id": "31",
      "title": "Pattern Classification",
      "authors": [
        "R Duda",
        "P Hart",
        "D Stork"
      ],
      "year": "2000",
      "venue": "Pattern Classification"
    },
    {
      "citation_id": "32",
      "title": "Discriminatory Analysis -Nonparametric Discrimination: Consistency Properties",
      "authors": [
        "E Fix",
        "J Hodges"
      ],
      "year": "1951",
      "venue": "Discriminatory Analysis -Nonparametric Discrimination: Consistency Properties"
    },
    {
      "citation_id": "33",
      "title": "Automatic audio segmentation using a measure of audio novelty",
      "authors": [
        "J Foote"
      ],
      "year": "2000",
      "venue": "Proceedings of the International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "34",
      "title": "Time and Rhythm Perception",
      "authors": [
        "P Fraisse"
      ],
      "year": "1978",
      "venue": "Perceptual Coding",
      "doi": "10.1016/B978-0-12-161908-4.50012-7"
    },
    {
      "citation_id": "35",
      "title": "Realtime Chord Recognition of Musical Sound: a System Using Common Lisp Music",
      "authors": [
        "T Fujishima"
      ],
      "year": "1999",
      "venue": "Proceedings of the International Computer Music Conference (ICMC)"
    },
    {
      "citation_id": "36",
      "title": "Tonal Description of Polyphonic Audio for Music Content Processing",
      "authors": [
        "E G√≥mez"
      ],
      "year": "2006",
      "venue": "INFORMS Journal on Computing",
      "doi": "10.1287/ƒ≥oc.1040.0126"
    },
    {
      "citation_id": "37",
      "title": "Deep learning",
      "authors": [
        "I Goodfellow",
        "Y Bengio",
        "A Courville"
      ],
      "year": "2016",
      "venue": "Deep learning"
    },
    {
      "citation_id": "38",
      "title": "A Beat Induction Method for Musical Audio Signals",
      "authors": [
        "F Gouyon",
        "P Herrera"
      ],
      "year": "2003",
      "venue": "Proceedings of the 4th European Workshop on Image Analysis for Multimedia Interactive Services (WIAMIS)"
    },
    {
      "citation_id": "39",
      "title": "An Attention Mechanism for Music Instrument Recognition",
      "authors": [
        "S Gururani",
        "M Sharma",
        "A Lerch"
      ],
      "year": "2019",
      "venue": "Proceedings of the International Society for Music Information Retrieval Conference (ISMIR). International Society for Music Information Retrieval ({ISMIR})"
    },
    {
      "citation_id": "40",
      "title": "GLSR-VAE: Geodesic latent space regularization for variational autoencoder architectures",
      "authors": [
        "G Hadjeres",
        "F Nielsen",
        "F Pachet"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Symposium Series on Computational Intelligence (SSCI)",
      "doi": "10.1109/SSCI.2017.8280895"
    },
    {
      "citation_id": "41",
      "title": "A Highly Robust Audio Fingerprinting System",
      "authors": [
        "J Haitsma",
        "T Kalker"
      ],
      "year": "2002",
      "venue": "Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)"
    },
    {
      "citation_id": "42",
      "title": "Learning Features from Music Audio with Deep Belief Networks",
      "authors": [
        "P Hamel",
        "D Eck"
      ],
      "year": "2010",
      "venue": "Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)"
    },
    {
      "citation_id": "43",
      "title": "Error detecting and error correcting codes. The Bell System",
      "authors": [
        "R Hamming"
      ],
      "year": "1950",
      "venue": "Technical Journal",
      "doi": "10.1002/j.1538-7305.1950.tb00463.x"
    },
    {
      "citation_id": "44",
      "title": "Musical Instrument Recognition in Polyphonic Audio Using Source-Filter Model for Sound Separation",
      "authors": [
        "T Heittola",
        "A Klapuri",
        "T Virtanen"
      ],
      "year": "2009",
      "venue": "Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)"
    },
    {
      "citation_id": "45",
      "title": "",
      "authors": [
        "Virtanen"
      ],
      "venue": ""
    },
    {
      "citation_id": "46",
      "title": "Die Lehre von den Tonempfindungen als physiologische Grundlage f√ºr die Theorie der Musik",
      "authors": [
        "H Helmholtz"
      ],
      "year": "1870",
      "venue": "Die Lehre von den Tonempfindungen als physiologische Grundlage f√ºr die Theorie der Musik"
    },
    {
      "citation_id": "47",
      "title": "Predicting Musical Meaning in Audio Branding Scenarios",
      "authors": [
        "M Herzog",
        "S Lepa",
        "J Steffens",
        "A Schoenrock",
        "H Egermann"
      ],
      "year": "2017",
      "venue": "Proceedings of the Conference of the European Society for Cognitive Science of Music (ESCOM)"
    },
    {
      "citation_id": "48",
      "title": "Feature learning and deep architectures: new directions for music informatics",
      "authors": [
        "E Humphrey",
        "J Bello",
        "Y Lecun"
      ],
      "year": "2013",
      "venue": "Journal of Intelligent Information Systems",
      "doi": "10.1007/s10844-013-0248-5"
    },
    {
      "citation_id": "49",
      "title": "Template based key finding from audio",
      "authors": [
        "Z Izmirli"
      ],
      "year": "2005",
      "venue": "Proceedings of the International Computer Music Conference (ICMC)"
    },
    {
      "citation_id": "50",
      "title": "Evaluation of MFCC estimation techniques for music similarity",
      "authors": [
        "J Jensen",
        "M Christensen",
        "M Murthi",
        "S Jensen"
      ],
      "year": "2006",
      "venue": "Proceedings of the XIV. European Signal Processing Conference"
    },
    {
      "citation_id": "51",
      "title": "Cue Utilization in Communication of Emotion in Music Performance: Relating Performance to Perception",
      "authors": [
        "P Juslin"
      ],
      "year": "2000",
      "venue": "Journal of Experimental Psychology"
    },
    {
      "citation_id": "52",
      "title": "Categories of Music Description and Search Terms and Phrases Used by Non-Music Experts",
      "authors": [
        "J Kim",
        "N Belkin"
      ],
      "year": "2002",
      "venue": "Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)"
    },
    {
      "citation_id": "53",
      "title": "Multiple Fundamental Frequency Estimation Based on Harmonicity and Spectral Smoothness",
      "authors": [
        "A Klapuri"
      ],
      "year": "2003",
      "venue": "Transactions on Speech and Audio Processing",
      "doi": "10.1109/TSA.2003.815516"
    },
    {
      "citation_id": "54",
      "title": "Intelligent User Interfaces for Music Discovery: The Past 20 Years and What's to Come",
      "authors": [
        "P Knees",
        "M Schedl",
        "M Goto"
      ],
      "year": "2019",
      "venue": "Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)"
    },
    {
      "citation_id": "55",
      "title": "The potential for automatic assessment of trumpet tone quality",
      "authors": [
        "T Knight",
        "F Upham",
        "I Fujinaga"
      ],
      "year": "2011",
      "venue": "Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)"
    },
    {
      "citation_id": "56",
      "title": "Cognitive Foundations of Musical Pitch",
      "authors": [
        "C Krumhansl"
      ],
      "year": "1990",
      "venue": "Cognitive Foundations of Musical Pitch"
    },
    {
      "citation_id": "57",
      "title": "Beat Tracking with a Nonlinear Oscillator",
      "authors": [
        "E Large"
      ],
      "year": "1995",
      "venue": "Proceedings of the 14th International Joint Conference on Artificial Intelligence (ƒ≤CAI)"
    },
    {
      "citation_id": "58",
      "title": "Unsupervised feature learning for audio classification using convolutional deep belief networks",
      "authors": [
        "H Lee",
        "P Pham",
        "Y Largman",
        "A Ng"
      ],
      "year": "2009",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "59",
      "title": "Software-Based Extraction of Objective Parameters from Music Performances",
      "authors": [
        "A Lerch"
      ],
      "year": "2009",
      "venue": "Software-Based Extraction of Objective Parameters from Music Performances",
      "doi": "URL10.14279/depositonce-2025"
    },
    {
      "citation_id": "60",
      "title": "An Introduction to Audio Content Analysis: Applications in Signal Processing and Music Informatics",
      "authors": [
        "A Lerch"
      ],
      "year": "2012",
      "venue": "An Introduction to Audio Content Analysis: Applications in Signal Processing and Music Informatics"
    },
    {
      "citation_id": "61",
      "title": "Music Performance Analysis: A Survey",
      "authors": [
        "A Lerch",
        "C Arthur",
        "A Pati",
        "S Gururani"
      ],
      "year": "2019",
      "venue": "Proceedings of the International Society for Music Information Retrieval Conference (ISMIR). International Society for Music Information Retrieval ({ISMIR})"
    },
    {
      "citation_id": "62",
      "title": "A Generative Theory of Tonal Music",
      "authors": [
        "F Lerdahl",
        "R Jackendorf"
      ],
      "year": "1983",
      "venue": "A Generative Theory of Tonal Music"
    },
    {
      "citation_id": "63",
      "title": "The Mythos of Model Interpretability",
      "authors": [
        "Z Lipton"
      ],
      "year": "2018",
      "venue": "The Mythos of Model Interpretability",
      "doi": "10.1145/3236386.3241340"
    },
    {
      "citation_id": "64",
      "title": "Mel Frequency Cepstral Coefficients for Music Modeling",
      "authors": [
        "B Logan"
      ],
      "year": "2000",
      "venue": "Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)"
    },
    {
      "citation_id": "65",
      "title": "Detection and Logging Advertisements using its Sound",
      "authors": [
        "J Lourens"
      ],
      "year": "1990",
      "venue": "Transactions on Broadcasting",
      "doi": "10.1109/11.59850"
    },
    {
      "citation_id": "66",
      "title": "Detection of Common Mistakes in Novice Violin Playing",
      "authors": [
        "Y Luo"
      ],
      "year": "2015",
      "venue": "Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)"
    },
    {
      "citation_id": "67",
      "title": "Beat Histogram Features for Rhythm-based Musical Genre Classification Using Multiple Novelty Functions",
      "authors": [
        "A Lykartsis",
        "A Lerch"
      ],
      "year": "2015",
      "venue": "Proceedings of the International Conference on Digital Audio Effects (DAFX). Trondheim, Norway"
    },
    {
      "citation_id": "68",
      "title": "Musikaufnahmen als Datenquellen der Interpretationsanalyse",
      "authors": [
        "H Maempel"
      ],
      "year": "2011",
      "venue": "Gemessene Interpretation -Computergest√ºtzte Auff√ºhrungsanalyse im Kreuzverh√∂r der Disziplinen"
    },
    {
      "citation_id": "69",
      "title": "Online Dictionary Learning for Sparse Coding",
      "authors": [
        "J Mairal",
        "F Bach",
        "J Ponce",
        "G Sapiro"
      ],
      "year": "2009",
      "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, ICML '09",
      "doi": "10.1145/1553374.1553463"
    },
    {
      "citation_id": "70",
      "title": "PYIN: A fundamental frequency estimator using probabilistic threshold distributions",
      "authors": [
        "M Mauch",
        "S Dixon"
      ],
      "year": "2014",
      "venue": "International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "doi": "10.1109/ICASSP.2014.6853678"
    },
    {
      "citation_id": "71",
      "title": "Perceptual scaling of synthesized musical timbres: Common dimensions, specificities, and latent subject classes",
      "authors": [
        "S Mcadams",
        "S Winsberg",
        "S Donnadieu",
        "G Soete",
        "J Krimphoff"
      ],
      "year": "1995",
      "venue": "Psychological Research",
      "doi": "10.1007/BF00419633"
    },
    {
      "citation_id": "72",
      "title": "Emotion and Meaning in Music",
      "authors": [
        "L Meyer"
      ],
      "year": "1956",
      "venue": "Emotion and Meaning in Music"
    },
    {
      "citation_id": "73",
      "title": "An Analysis of the Effect of Data Augmentation Methods: Experiments for a Musical Genre Classification Task",
      "authors": [
        "R Mignot",
        "G Peeters"
      ],
      "year": "2019",
      "venue": "Transactions of the International Society for Music Information Retrieval",
      "doi": "10.5334/tismir.26"
    },
    {
      "citation_id": "74",
      "title": "Information Retrieval for Music and Motion",
      "authors": [
        "M M√ºller"
      ],
      "year": "2007",
      "venue": "Information Retrieval for Music and Motion"
    },
    {
      "citation_id": "75",
      "title": "The communication of dynamics between musicians and listeners through musical performance",
      "authors": [
        "T Nakamura"
      ],
      "year": "1987",
      "venue": "Perception & Psychophysics"
    },
    {
      "citation_id": "76",
      "title": "An automatic singing skill evaluation method for unknown melodies using pitch interval accuracy and vibrato features",
      "authors": [
        "T Nakano",
        "M Goto",
        "Y Hiraga"
      ],
      "year": "2006",
      "venue": "Rn"
    },
    {
      "citation_id": "77",
      "title": "Pitch Determination of Human Speech by the Harmonic Product Spectrum, the Harmonic Sum Spectrum, and a Maximum Likelihood Estimate",
      "authors": [
        "A Noll"
      ],
      "year": "1969",
      "venue": "Proceedings of the Symposium on Computer Processing in Communications"
    },
    {
      "citation_id": "78",
      "title": "A Taxonomy of Musical Genres",
      "authors": [
        "F Pachet",
        "D Cazaly"
      ],
      "year": "2000",
      "venue": "Proceedings of the Conference on Content-Based Multimedia Information Access"
    },
    {
      "citation_id": "79",
      "title": "Mapping Musical Thought to Musical Performance",
      "authors": [
        "C Palmer"
      ],
      "year": "1989",
      "venue": "Journal of Experimental Psychology: Human Perception and Performance"
    },
    {
      "citation_id": "80",
      "title": "Music Performance",
      "authors": [
        "C Palmer"
      ],
      "year": "1997",
      "venue": "Annual Review of Psychology"
    },
    {
      "citation_id": "81",
      "title": "Assessment of Student Music Performances Using Deep Neural Networks",
      "authors": [
        "K Pati",
        "S Gururani",
        "A Lerch"
      ],
      "year": "2018",
      "venue": "Applied Sciences",
      "doi": "10.3390/app8040507"
    },
    {
      "citation_id": "82",
      "title": "Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)",
      "authors": [
        "J Paulus",
        "M Muller",
        "A Klapuri"
      ],
      "year": "2010",
      "venue": "Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)"
    },
    {
      "citation_id": "83",
      "title": "Musical key extraction from audio",
      "authors": [
        "S Pauws"
      ],
      "year": "2004",
      "venue": "Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)"
    },
    {
      "citation_id": "84",
      "title": "On the use of autocorrelation analysis for pitch detection",
      "authors": [
        "L Rabiner"
      ],
      "year": "1977",
      "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing",
      "doi": "10.1109/TASSP.1977.1162905"
    },
    {
      "citation_id": "85",
      "title": "Applications of Cross-Adaptive Audio Effects: Automatic Mixing, Live Performance and Everything in Between",
      "authors": [
        "J Reiss",
        "Brandtsegg"
      ],
      "year": "2018",
      "venue": "Frontiers in Digital Humanities",
      "doi": "10.3389/fdigh.2018.00017"
    },
    {
      "citation_id": "86",
      "title": "A Real-Time System for Measuring Sound Goodness in Instrumental Sounds",
      "authors": [
        "O Romani Picas",
        "H Rodriguez",
        "D Dabiri",
        "H Tokuda",
        "W Hariya",
        "K Oishi",
        "X Serra"
      ],
      "year": "2015",
      "venue": "Proceedings of the Audio Engineering Society Convention"
    },
    {
      "citation_id": "87",
      "title": "A Circumplex Model of Affect",
      "authors": [
        "J Russel"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology",
      "doi": "10.1037/h0077714"
    },
    {
      "citation_id": "88",
      "title": "Music Information Retrieval: Recent Developments and Applications",
      "authors": [
        "M Schedl",
        "E G√≥mez",
        "J Urbano"
      ],
      "year": "2014",
      "venue": "Foundations and Trends¬Æ in Information Retrieval",
      "doi": "10.1561/1500000042"
    },
    {
      "citation_id": "89",
      "title": "Tempo and beat analysis of acoustic musical signals",
      "authors": [
        "E Scheirer"
      ],
      "year": "1998",
      "venue": "Journal of the Acoustical Society of America (JASA)"
    },
    {
      "citation_id": "90",
      "title": "Why Music does not Produce Basic Emotions: Pleading for a new Approach to Measuring the Emotional Effects of Music",
      "authors": [
        "K Scherer"
      ],
      "year": "2003",
      "venue": "Proceedings of the Stockholm Music Acoustics Conference (SMAC)"
    },
    {
      "citation_id": "91",
      "title": "Which Emotions Can be Induced by Music? What Are the Underlying Mechanisms? And How Can We Measure Them?",
      "authors": [
        "K Scherer"
      ],
      "year": "2004",
      "venue": "Journal of New Music Research",
      "doi": "10.1080/0929821042000317822"
    },
    {
      "citation_id": "93",
      "title": "On the Automatic Transcription of Percussive Music -From Acoustic Signal to High-Level Analysis",
      "authors": [
        "W Schloss"
      ],
      "year": "1985",
      "venue": "On the Automatic Transcription of Percussive Music -From Acoustic Signal to High-Level Analysis"
    },
    {
      "citation_id": "94",
      "title": "",
      "authors": [
        "C Seashore"
      ],
      "year": "1938",
      "venue": ""
    },
    {
      "citation_id": "95",
      "title": "Non-Negative Matrix Factorization for Polyphonic Music Transcription",
      "authors": [
        "P Smaragdis",
        "J Brown"
      ],
      "year": "2003",
      "venue": "Proceedings of the Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)",
      "doi": "10.1109/ASPAA.2003.1285860"
    },
    {
      "citation_id": "96",
      "title": "A Scale for the Measurement of the Psychological Magnitude Pitch",
      "authors": [
        "S Stevens",
        "J Volkmann",
        "E Newman"
      ],
      "year": "1937",
      "venue": "Journal of the Acoustical Society of America (JASA)",
      "doi": "10.1121/1.1915893"
    },
    {
      "citation_id": "97",
      "title": "The Tonal Properties of Pitch-Class Sets : Tonal Implication, Tonal Ambiguity, and Tonalness",
      "authors": [
        "D Temperley"
      ],
      "year": "2007",
      "venue": "Computing in Musicology"
    },
    {
      "citation_id": "98",
      "title": "Evaluating Evaluation: Musical Performance Assessment as a Research Tool",
      "authors": [
        "S Thompson",
        "A Williamon"
      ],
      "year": "2003",
      "venue": "Music Perception: An Interdisciplinary Journal",
      "doi": "10.1525/mp.2003.21.1.21"
    },
    {
      "citation_id": "99",
      "title": "Pitch Histograms in Audio and Symbolic Music Information Retrieval",
      "authors": [
        "G Tzanetakis",
        "A Ermolinskyi",
        "P Cook"
      ],
      "year": "2002",
      "venue": "Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)"
    },
    {
      "citation_id": "100",
      "title": "Stereo panning features for classifying recording production style",
      "authors": [
        "G Tzanetakis",
        "R Jones",
        "K Mcnally"
      ],
      "year": "2007",
      "venue": "Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)"
    },
    {
      "citation_id": "101",
      "title": "Objective descriptors for the assessment of student music performances",
      "authors": [
        "A Vidwans",
        "S Gururani",
        "C Wu",
        "V Subramanian",
        "R Swaminathan",
        "A Lerch"
      ],
      "year": "2017",
      "venue": "Proceedings of the AES Conference on Semantic Audio. Audio Engineering Society (AES)"
    },
    {
      "citation_id": "102",
      "title": "An Industrial Strength Audio Search Algorithm",
      "authors": [
        "A Wang"
      ],
      "year": "2003",
      "venue": "Proceedings of the International Society for Music Information Retrieval Conference (ISMIR). Washington"
    },
    {
      "citation_id": "103",
      "title": "Sound power and timbre as cues for the dynamic strength of orchestral instruments",
      "authors": [
        "S Weinzierl",
        "S Lepa",
        "F Schultz",
        "E Detzner",
        "H Von Coler",
        "G Behler"
      ],
      "year": "2018",
      "venue": "The Journal of the Acoustical Society of America",
      "doi": "10.1121/1.5053113"
    },
    {
      "citation_id": "104",
      "title": "Examining Rater Precision in Music Performance Assessment: An Analysis of Rating Scale Structure Using the Multifaceted Rasch Partial Credit Model. Music Perception",
      "authors": [
        "B Wesolowski",
        "S Wind",
        "G Engelhard"
      ],
      "year": "2016",
      "venue": "An Interdisciplinary Journal",
      "doi": "10.1525/mp.2016.33.5.662"
    },
    {
      "citation_id": "105",
      "title": "Towards the Objective Assessment of Music Performances",
      "authors": [
        "C Wu",
        "S Gururani",
        "C Laguna",
        "A Pati",
        "A Vidwans",
        "A Lerch"
      ],
      "year": "2016",
      "venue": "Proceedings of the International Conference on Music Perception and Cognition (ICMPC)"
    },
    {
      "citation_id": "106",
      "title": "Assessment of Percussive Music Performances with Feature Learning",
      "authors": [
        "C Wu",
        "A Lerch"
      ],
      "year": "2018",
      "venue": "International Journal of Semantic Computing",
      "doi": "10.1142/S1793351X18400147"
    },
    {
      "citation_id": "107",
      "title": "From Labeled to Unlabeled Data -On the Data Challenge in Automatic Drum Transcription",
      "authors": [
        "C Wu",
        "A Lerch"
      ],
      "year": "2018",
      "venue": "Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)"
    },
    {
      "citation_id": "108",
      "title": "",
      "authors": [
        "Wu-And-Lerch"
      ],
      "venue": ""
    },
    {
      "citation_id": "109",
      "title": "Machine Recognition of Music Emotion: A Review",
      "authors": [
        "Y Yang",
        "H Chen"
      ],
      "year": "2012",
      "venue": "ACM Transactions on Intelligent Systems and Technology (TIST)",
      "doi": "10.1145/2168752.2168754"
    },
    {
      "citation_id": "110",
      "title": "Emotions Evoked by the Sound of Music: Characterization, Classification, and Measurement",
      "authors": [
        "M Zentner",
        "D Grandjean",
        "K Scherer"
      ],
      "year": "2008",
      "venue": "Emotion",
      "doi": "10.1037/1528-3542.8.4.494"
    },
    {
      "citation_id": "111",
      "title": "Visual interpretability for deep learning: a survey",
      "authors": [
        "Q Zhang",
        "S Zhu"
      ],
      "year": "2018",
      "venue": "Frontiers of Information Technology & Electronic Engineering",
      "doi": "10.1631/FITEE.1700808"
    }
  ]
}