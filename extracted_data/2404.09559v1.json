{
  "paper_id": "2404.09559v1",
  "title": "Joint Contrastive Learning With Feature Alignment For Cross-Corpus Eeg-Based Emotion Recognition",
  "published": "2024-04-15T08:21:17Z",
  "authors": [
    "Qile Liu",
    "Zhihao Zhou",
    "Jiyuan Wang",
    "Zhen Liang"
  ],
  "keywords": [
    "Neural Decoding",
    "EEG",
    "Affective Computing",
    "Joint Contrastive Learning",
    "Cross-Corpus"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The integration of human emotions into multimedia applications shows great potential for enriching user experiences and enhancing engagement across various digital platforms. Unlike traditional methods such as questionnaires, facial expressions, and voice analysis, brain signals offer a more direct and objective understanding of emotional states. However, in the field of electroencephalography (EEG)-based emotion recognition, previous studies have primarily concentrated on training and testing EEG models within a single dataset, overlooking the variability across different datasets. This oversight leads to significant performance degradation when applying EEG models to cross-corpus scenarios. In this study, we propose a novel Joint Contrastive learning framework with Feature Alignment (JCFA) to address cross-corpus EEG-based emotion recognition. The JCFA model operates in two main stages. In the pre-training stage, a joint domain contrastive learning strategy is introduced to characterize generalizable time-frequency representations of EEG signals, without the use of labeled data. It extracts robust time-based and frequency-based embeddings for each EEG sample, and then aligns them within a shared latent time-frequency space. In the fine-tuning stage, JCFA is refined in conjunction with downstream tasks, where the structural connections among brain electrodes are considered. The model capability could be further enhanced for the application in emotion detection and interpretation. Extensive experimental results on two well-recognized emotional datasets show that the proposed JCFA model achieves state-of-theart (SOTA) performance, outperforming the second-best method by an average accuracy increase of 4.09% in cross-corpus EEG-based emotion recognition tasks.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Objective assessment of an individual's emotional states is of great importance in the field of human-computer interaction, disease diagnosis and rehabilitation  [3, 5, 9, 46] . Existing research mainly uses two types of data for emotion recognition: behavioral signals and physiological signals. Behavioral signals, including speech  [13] , gestures  [25] , and facial expressions  [39] , are low-cost and easily accessible but can be intentionally altered. On the other hand, physiological signals, such as electrocardiography (ECG), electromyography (EMG), and electroencephalography (EEG), offer a more reliable measure of emotions as they are less susceptible to manipulation. EEG, in particular, stands out for its ability to provide direct, objective insights into emotional states by capturing brain activity from different locations on the scalp. Therefore, researchers from diverse fields have increasingly focused on EEG-based emotion recognition in recent years  [12, 18, 19, 23] .\n\nCurrently, a variety of methods have been developed for EEGbased emotion recognition. For example, Song et al.  [33]  proposed a dynamical graph convolutional neural network (DGCNN) to dynamically learn the intrinsic relationship among brain electrodes. To capture both local and global relations among different EEG channels, Zhong et al.  [43]  proposed a regularized graph neural network (RGNN). Concerning domain adaptation techniques, Li et al.  [17]  introduced a joint distribution network (JDA) that adapts the joint distributions by simultaneously adapting marginal distributions and conditional distributions. Incorporating self-supervised learning, Shen et al.  [29]  proposed a contrastive learning method for inter-subject alignment (CLISA), and achieved state-of-the-art (SOTA) performance in cross-subject EEG-based emotion recognition tasks. More related literature review is available in Section 2. However, two critical challenges remain unaddressed in current methods. (1) Experimental protocol. Existing approaches mainly consider within-subject and cross-subject experimental protocols within one single dataset, neglecting to account for variations between different datasets. This oversight can significantly reduce the effectiveness of established methods when applied to cross-corpus scenarios  [28] . (2) Data availability. Zhou et al.  [45]  introduced an EEG-based emotion style transfer network (E 2 STN) to integrate content information from the source domain with style information from the target domain, achieving promising performance in cross-corpus scenarios. However, it requires access to all labeled source data and unlabeled target data beforehand. Considering the difficulties in collecting EEG signals and the expert knowledge and time required to label them, obtaining labeled data in advance for model training is impractical in real-world applications.\n\nTo tackle the aforementioned two critical challenges, this paper proposes a novel Joint Contrastive learning framework with Feature Alignment (JCFA) for cross-corpus EEG-based emotion recognition. The proposed JCFA model is a self-supervised learning approach that includes two main stages. In the pre-training stage, a joint contrastive learning strategy is introduced to identify aligned time-and frequency-based embeddings of EEG signals that remain consistent across diverse environmental conditions present in different datasets. In the fine-tuning stage, a further enhancement on model capability to the downstream tasks is developed by incorporating spatial features of brain electrodes using a graph convolutional network. In summary, the main contributions of our work are outlined as follows:\n\nâ€¢ We propose a novel JCFA framework designed to tackle two main critical challenges (experimental protocol and data availability) encountered in the field of cross-corpus EEGbased emotion recognition.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Related Work",
      "text": "In this section, we review the related work in terms of EEG-based emotion recognition, cross-corpus EEG emotion recognition, and contrastive learning.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Eeg-Based Emotion Recognition",
      "text": "Existing methods for EEG-based emotion recognition can be categorised into two groups. (1) Machine learning-based methods use manually extracted EEG features, such as Power Spectral Density (PSD)  [11]  and Differential Entropy (DE)  [7] , for emotion recognition. For example, Alsolamy et al.  [2]  employed PSD features as input to a support vector machine (SVM)  [34]  in emotion recognition. However, the performance of traditional machine learningbased methods tends to be relatively poor and unstable. Therefore, researchers have turned to developing various (2) deep learningbased models in recent years. For example, Song et al.  [31]  proposed a graph-embedded convolutional neural network (GECNN) to extract discriminative spatial features. Additionally, they introduced a variational instance-adaptive graph method (V-IAG)  [30] , which captures the individual dependencies among different electrodes and estimates the underlying uncertain information. To further reduce the impact of individual differences, transfer learning strategy is suggested to incorporate. For instance, Li et al.  [21]  proposed a bi-hemispheres domain adversarial neural network (BiDANN) to address domain shifts between different subjects. Considering the local and global feature distributions, Li et al.  [20]  proposed a transferable attention neural network (TANN) for learning discriminative information using attention mechanism. Further, Zhou et al.  [44]  introduced a novel transfer learning framework with prototypical representation based pairwise learning (PR-PL) to address individual differences and noisy labeling in EEG emotional data, achieving SOTA performance. However, the above methods would fail in adapting to cross-corpus scenarios, due to the substantial differences in feature representation among different datasets.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Cross-Corpus Eeg Emotion Recognition",
      "text": "Considering practical requirements, researchers have recently begun to explore EEG-based emotion recognition in cross-corpus scenarios. For example, Rayatdoost et al.  [28]  developed a deep convolutional network and evaluated the performance of existing methods in cross-corpus scenarios. Their study demonstrated a significant decline in model performance when these methods were evaluated across different corpora.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Encoder Projector",
      "text": "Frequency Spectrum abnormal detection, and emotion recognition. Moreover, Shen et al.  [29]  proposed a contrastive learning method for inter-subject alignment (CLISA) and significantly improved the performance of cross-subject EEG-based emotion recognition. To date, the applicability of self-supervised contrastive learning in cross-corpus scenarios has not been explored. Therefore, in this paper, we introduce a joint contrastive learning framework to efficiently extract time-and frequency-based embedding information that remains reliable across EEG signals collected from different datasets.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Problem Formulation",
      "text": "For an unlabeled pre-training dataset D pret = {ğ‘¥ pret ğ‘–\n\n| ğ‘– = 1, . . . , ğ‘ }, which consists of ğ‘ samples with ğ‘‰ pret channels and ğ‘‡ pret timestamps. For a labeled fine-tuning dataset D tune = {(ğ‘¥ tune ğ‘– , ğ‘¦ ğ‘– ) | ğ‘– = 1, . . . , ğ‘€ }, which contains ğ‘€ samples with ğ‘‰ tune channels and ğ‘‡ tune timestamps. Each sample ğ‘¥ tune ğ‘– is paired with a label ğ‘¦ ğ‘– âˆˆ {1, . . . , ğ¶}, where ğ¶ is the number of emotion categories. The objective is to pretrain a model F , utilizing D pret through a self-supervised learning strategy. The pre-training process enables F to derive generalizable representations of EEG signals, without the need of labeled information. Then, we refined the pre-trained model F in the supervised fine-tuning stage, using a small amount of the labeled data from D tune . The fine-tuned model F can be regarded as adept at well performing cross-corpus EEG-based emotion recognition.\n\nThe central idea of the JCFA model is inspired by a fundamental assumption in signal processing theory  [8, 27] , which suggests the existence of a latent time-frequency space. In this space, the timebased embeddings and frequency-based embeddings learned from the same time series should be closer to each other compared to embeddings from different time series. To satisfy this assumption, a joint contrastive learning strategy across time, frequency, and time-frequency domains is proposed. Here, the extracted time-and frequency-based embeddings of the same sample are aligned within the time-frequency domain. The resulting time-and frequencybased embeddings are considered generalizable representations of EEG signals, which are robust across various datasets.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Methodology",
      "text": "In this section, we propose a JCFA framework for cross-corpus EEG-based emotion recognition. The overall architecture of JCFA is shown in",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Self-Supervised Pre-Training",
      "text": "In this part, we explain the design of our pre-training stage, which uses the self-supervised joint contrastive learning strategy. ğ‘¥ T ğ‘– and ğ‘¥ F ğ‘– denote the input time domain and frequency domain raw data of an EEG sample x i . For simplicity, we use univariate (singlechannel) EEG signals as an example below to elucidate the pretraining process. Note that our approach can be straightforwardly applied to multivariate (multi-channel) EEG signals. The detailed self-supervised pre-training process is illustrated in Algorithm 1. ğ‘— and its augmentation ğ‘¥ T ğ‘— )  [4, 15, 38] . Thus, we define (ğ‘¥ T ğ‘– , ğ‘¥ T ğ‘– ) as positive pair, and (ğ‘¥ T ğ‘– , ğ‘¥ T ğ‘— ) and (ğ‘¥ T ğ‘– , ğ‘¥ T ğ‘— ) as negative pairs  [4] . To maximize the similarity between positive pairs and minimize the similarity between negative pairs, we adopt the NT-Xent (the normalized temperature-scaled cross entropy loss)  [4, 35]  to measure the distance between sample pairs. In particular, for a positive pair (ğ‘¥ T ğ‘– , ğ‘¥ T ğ‘– ), the time-based contrastive loss is defined as:",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Time",
      "text": "where sim(â€¢, â€¢) refers to the cosine similarity. 1 ğ‘–â‰ ğ‘— is an indicator function that equals to 1 when ğ‘– â‰  ğ‘— and 0 otherwise. ğœ is a temperature parameter. The x j âˆˆ D pret represents another sample ğ‘¥ T ğ‘— and its augmentation ğ‘¥ T ğ‘— that are different from x i . Note that L T,ğ‘– is computed across all positive pairs, both (ğ‘¥ T ğ‘– , ğ‘¥ T ğ‘– ) and ( ğ‘¥ T ğ‘– , ğ‘¥ T ğ‘– ). During the training process, the final time-based contrastive loss L T is computed as the average of L T,ğ‘– within a mini-batch.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Frequency Domain Contrastive Learning.",
      "text": "To exploit the frequency information in EEG signals, we design a frequency-based contrastive encoder ğ¸ F to capture robust frequency-based embeddings. First, we generate the frequency spectrum ğ‘¥ F ğ‘– from ğ‘¥ T ğ‘– by Fourier Transformation  [26] . Then, we perform data augmentation in frequency domain by perturbing the frequency spectrum. The existing research has shown that applying small perturbations to the frequency spectrum can easily lead to significant changes in the original time series  [8] . Thus, unlike the substantial perturbation by adding Gaussian noise directly to frequency spectrum in  [22] , we perturb the frequency spectrum ğ‘¥ F ğ‘– weakly by removing and adding a small portion of frequency components. Specifically, we generate a probability matrix ğ‘ˆ drawn from a uniform distribution ğ‘ˆ (0, 1), which matches the dimensionality of ğ‘¥ F ğ‘– . When removing frequency components, we zero out the amplitudes in ğ‘¥\n\nSimilar to L T,ğ‘– , we calculate L F,ğ‘– among all positive pairs, including (ğ‘¥ F ğ‘– , ğ‘¥ F ğ‘– ) and ( ğ‘¥ F ğ‘– , ğ‘¥ F ğ‘– ). In the training process, the final frequencybased contrastive loss L F is computed as the average of L F,ğ‘– within a mini-batch. The L F urges the frequency encoder ğ¸ F to produce embeddings that are robust to spectrum perturbations. ğ‘– from frequency domain into a shared latent time-frequency space using two cross-space projectors ğ‘ƒ T and ğ‘ƒ F , respectively. In particular, we have two time-based and frequencybased embeddings for each sample x i through ğ‘ƒ T and ğ‘ƒ F , which are denoted as ğ‘§ T ğ‘– = ğ‘ƒ T (â„ T ğ‘– ) and ğ‘§ F ğ‘– = ğ‘ƒ F (â„ F ğ‘– ). Then, we assume that the time-based embedding and frequency-based embedding learned from the same sample should be closer to each other in the latent space than embeddings of different samples. Therefore, we define the time-based embedding and frequency-based embedding from the same sample as positive pair. The time-and frequency-based embeddings from different samples are considered as negative pairs. In other words, (ğ‘§ T ğ‘– , ğ‘§ F ğ‘– ) is positive pair, while (ğ‘§ T ğ‘– , ğ‘§ T ğ‘— ) and (ğ‘§ T ğ‘– , ğ‘§ F ğ‘— ) are negative pairs. To fulfill the key idea of JCFA, for a positive pair (ğ‘§ T ğ‘– , ğ‘§ F ğ‘– ), we define the alignment loss L A,ğ‘– as:\n\nwhere Z pret represents the shared latent time-frequency space. z j âˆˆ Z pret denotes the projected time-and frequency-based embeddings (ğ‘§ T ğ‘— and ğ‘§ F ğ‘— ) from another sample x j . Here, L A,ğ‘– is computed across all positive pairs, including (ğ‘§ T ğ‘– , ğ‘§ F ğ‘– ) and (ğ‘§ F ğ‘– , ğ‘§ T ğ‘– ). We calculate the final alignment loss L A as the average of L A,ğ‘– within a mini-batch. Under the constraint of L A , the model is encouraged to align the time-and frequency-based embeddings for each EEG sample within the shared latent time-frequency space.\n\nAlgorithm 1 The algorithmic flow of the pre-training stage.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Require:",
      "text": "- Calculate the overall pre-training loss L pret in Eq. 4;\n\nwhere ğ›¼ and ğ›½ are two pre-defined constants that control the contribution of the contrastive and alignment losses.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Supervised Fine-Tuning",
      "text": "In the fine-tuning stage, we define a graph convolutional network ğº to enhance the capture of spatial features from time-and frequencybased embeddings derived from brain electrodes. Specifically, for an input multi-channel EEG sample x i âˆˆ D tune , we input each channel of x i into the pre-trained model F separately to produce the corresponding time-based embedding ğ‘§ T ğ‘–,ğ‘— and frequency-based embedding ğ‘§ F ğ‘–,ğ‘— . Here, ğ‘— is the channel order and ğ‘— âˆˆ {1, . . . , ğ‘‰ tune }. By aggregating ğ‘§ T ğ‘–,ğ‘— as well as ğ‘§ F ğ‘–,ğ‘— from all channels, we can obtain the final time embedding ğ‘§ T ğ‘– and frequency embedding ğ‘§ F ğ‘– of sample x i . Then, we concatenate ğ‘§ T ğ‘– and ğ‘§ F ğ‘– into a joint time-frequency embedding, denoted as ğ‘ ğ‘– = [ğ‘§ T ğ‘– ; ğ‘§ F ğ‘– ] âˆˆ R ğ‘‰ tune Ã—ğ¹ , where ğ¹ is the feature dimension. By utilizing ğ‘ ğ‘– , the pre-trained model F is further finetuned in a supervised manner using a small subset of labeled data from D tune . This fine-tuning process involves the integration of the graph convolutional network ğº and an emotion classifier ğ¶. We provide the detailed algorithmic flow of the supervised fine-tuning process in Algorithm 2. The impact of selecting a small subset for fine-tuning is carefully examined in Section 6.1.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Graph Convolutional Network.",
      "text": "To further capture the spatial features of EEG signals, we integrate a graph convolutional network ğº in the fine-tuning stage. Inspired by previous work  [14, 33] , we design a cosine similarity-based distance function to better describe the connectivity between different nodes in ğ‘ ğ‘– rather than using simple numbers 0 and 1. Then, we construct an initial adjacency matrix A âˆˆ R ğ‘‰ tune Ã—ğ‘‰ tune , each element of which is expressed as:\n\nWe clip ğ´ ğ‘šğ‘› with ğ›¿ to ensure that weak connectivity still exists between nodes ğ‘š and ğ‘› with low similarity. Based on the obtained adjacency matrix A, we use the graph convolution based on spectral graph theory to capture spatial features of brain electrodes. Specifically, we adopt the ğ¾-order Chebyshev graph convolution  [6]  for graph ğº considering the computational complexity. Here, we define ğº with only two layers to avoid over-smoothing.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Emotion Classification.",
      "text": "The final objective of JCFA is to achieve accurate emotion classification in cross-corpus scenarios. We reshape the outputs of ğº into one-dimensional vectors, and input them into an emotion classifier ğ¶ consisting of a 3-layer fully connected network for final emotion recognition. The classifier ğ¶ is trained by minimizing the cross entropy loss between the ground truth and predicted labels:\n\nwhere ğ‘ ğ‘š,ğ‘ is the probability that the ğ‘š-th sample belongs to ğ‘-th class and ğ‘¦ ğ‘š,ğ‘ âˆˆ {0, 1}. ğ‘€ is the total number of samples used in model fine-tuning.\n\nDuring the fine-tuning process, we train ğº and ğ¶ with the optimization of L cls , while jointly optimizing L T , L F and L A to finetune the pre-trained model F . In summary, the overall fine-tuning loss is defined as:\n\nwhere ğ›¼, ğ›½ and ğ›¾ are pre-defined constants controlling the weights of each loss. Note that L T , L F and L A are computed as the average of the corresponding loss over all channels.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiments 5.1 Datasets",
      "text": "We conduct extensive experiments on two well-known datasets, SEED  [42]  and SEED-IV  [41] , to evaluate the performance of our proposed JCFA model in cross-corpus scenarios. In the experiments, we only consider the case where the pre-training and fine-tuning datasets have the same emotion categories (i.e., negative, neutral and positive emotions), and we use the preprocessed 1-s EEG signals for both datasets. More details about datasets are presented in Appendix A. Generate the corresponding ğ‘§ T ğ‘–,ğ‘— and ğ‘§ F ğ‘–,ğ‘— through inputting each channel of x i into F separately; 6:\n\nCalculate the average of L T , L F and L A over all channels using Eq. 1, Eq. 2, and Eq. 3; Calculate the cosine similarity between each channel of ğ‘ ğ‘– ; 10:\n\nConstruct the adjacency matrix A using Eq. 5; Flatten the node features into one-dimensional vectors and feed them into the classifier to predict the categories; Calculate the overall fine-tuning loss L tune in Eq. 7;\n\n15:\n\nUpdate model parameters by gradient back-propagation; 16: end for",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Implementation Details",
      "text": "We use two 2-layer transformer encoders as backbones for the timebased contrastive encoder ğ¸ T and the frequency-based contrastive encoder ğ¸ F , without parameters sharing. The two cross-space projectors ğ‘ƒ T and ğ‘ƒ F are composed of two 2-layer fully connected networks, with no sharing parameters. In the pre-training stage, we set ğœ to a small value of 0.05 to increase the penalization of hard negative samples. We set ğ›¼ and ğ›½ in Eq. 4 to 0.2 and 1, respectively. In the fine-tuning stage, we set ğ¾ to a small value of 3 in ğº to avoid over-smoothing. Then, we set ğœ to 0.5, ğ›¼ and ğ›½ to 0.1, and ğ›¾ to 1. The model pre-training and fine-tuning processes are conducted for 1000 and 20 epochs, respectively. We use a batch size of 256 for pre-training and 128 for fine-tuning. The proposed JCFA model is implemented using Python 3.9 and trained with PyTorch 1.13 on an NVIDIA GeForce RTX 3090 GPU. More implementation details are in Appendix B.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Baseline Models And Experimental Settings",
      "text": "We conduct a comprehensive comparison of the proposed JCFA model with 10 existing methods, including 8 conventional methods: SVM  [34] , DANN  [10] , A-LSTM  [32] , V-IAG  [30] , GECNN  [31] , BiDANN  [21] , TANN  [20]  and E 2 STN  [45] , and 2 classical contrastive learning models: SimCLR  [4, 35]  and Mixup  [37, 40] . More details about baseline models are in Appendix C.   [32]  46.47 Â± 08.30 58.19 Â± 13.73 V-IAG  [30]  52.84 Â± 07.71 59.87 Â± 11.16 GECNN  [31]  58.02 Â± 07.03 57.25 Â± 07.53 BiDANN  [21]  49.24 Â± 10.49 60.46 Â± 11.17 TANN  [20]  58.  41   To ensure a fair comparison, we use the default hyper-parameters reported in the original paper, and follow the same experimental settings (cross-corpus subject-independent protocol) for all the above methods. Accordingly, we can obtain two kinds of experimental results: training with SEED-IV and testing on SEED (SEED-IV 3 â†’ SEED 3 ), and training with SEED and testing on SEED-IV (SEED 3 â†’ SEED-IV 3 ). We use accuracy (ACC) as the evaluation metric for model performance. All methods are evaluated using the average and standard deviation of the results for all subjects in the test set. Further information about experimental settings is available in Appendix C.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results Analysis And Comparison",
      "text": "Table  1  shows the experimental comparison results of the proposed JCFA model with existing methods for cross-corpus EEG-based emotion recognition on the SEED and SEED-IV datasets. Here, for the SEED-IV 3 â†’ SEED 3 experiment, we use the first 9 trials of each subject in the SEED dataset for model fine-tuning, and the last 6 trials for testing. For the SEED 3 â†’ SEED-IV 3 experiment, we take the first 12 trials of each subject in the SEED-IV dataset for fine-tuning and the last 6 trials for testing. The experimental results demonstrate that the proposed JCFA model achieves SOTA performance on the SEED and SEED-IV datasets compared to 10 existing methods. Specifically, JCFA achieves a classification accuracy of 67.53% with a standard deviation of 12.36% for the SEED-IV 3 â†’ SEED 3 experiment, outperforming the second-best method E 2 STN by 7.02% of accuracy. Meanwhile, JCFA achieves a recognition accuracy of 62.40% with a standard deviation of 7.54% for the SEED 3 â†’ SEED-IV 3 experiment, surpassing E 2 STN by 1.16% in accuracy. Besides, JCFA outperforms the two classical contrastive learning models, SimCLR and Mixup, which indicates that the frequency information as well as the time-frequency domain contrastive learning with alignment loss play a significant role in improving the accuracy of emotion recognition.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ablation Study",
      "text": "We conduct a comprehensive ablation study in the SEED-IV 3 â†’ SEED 3 and SEED 3 â†’ SEED-IV 3 experiments. To fully assess the validity of each module in our proposed JCFA model, we calculate six evaluation metrics: Accuracy, Precision, Recall, F1 Score, AUROC, and AUPRC. Specifically, we design four different models below. Table  2  and Table  3  show the experimental results of ablation study, which indicates that by effectively fusing all the modules, the proposed JCFA model achieves the best performance in both SEED-IV 3 â†’ SEED 3",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Discussions 6.1 Impact Of Fine-Tuning Size",
      "text": "We investigate the impact of varying the number of trials used in the fine-tuning stage on model performance. Here, we define the number of fine-tuning trials as ğ‘ T . Then, the samples from the remaining (15 -ğ‘ T ) trials in the SEED dataset and (18 -ğ‘ T ) trials in the SEED-IV dataset are allocated for testing. The experimental results in Table  4  and Table  5  show that increasing the number of fine-tuning trials can effectively improve the model performance.\n\nIn particular, the JCFA model achieves the highest classification accuracy on the SEED and SEED-IV datasets when ğ‘ T is equal to 9 and 12, respectively. On the other hand, further increasing the fine-tuning size may result in negative growth when ğ‘ T reaches a certain level. Excessive fine-tuning samples can introduce additional noise, which may interfere with the model learning process.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Visualization Of Time-Frequency Domain Contrastive Learning",
      "text": "To illustrate the effectiveness of the time-frequency domain contrastive learning with alignment loss, we randomly select 50 samples from the SEED dataset, and employ t-SNE  [36]  to visualize the time-based embeddings (colored as green) and frequency-based embeddings (colored as blue) in the shared latent time-frequency space, as shown in Fig.  3 . Without the alignment loss L A , the time embedding ğ‘§ T ğ‘– and frequency embedding ğ‘§ F ğ‘– from the same sample (marked with a red line) exhibit considerable separation. With the inclusion of the alignment loss L A , the spatial gap between the time embedding ğ‘§ T ğ‘– and frequency embedding ğ‘§ F ğ‘– originating from the same sample is diminished (highlighted with a dashed line), signifying a notable alignment between the time-and frequencybased embeddings. The observations demonstrate the efficacy of the time-frequency domain contrastive learning with alignment loss L A in bringing closer together the time embedding ğ‘§ T ğ‘– and the frequency embedding ğ‘§ F ğ‘– of the identical sample.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Impact Of Distance Metric",
      "text": "We further assess the impact of the chosen distance metric in the construction of ğº in the supervised fine-tuning stage. Three different distance metrics are individually employed in Eq. 5, and",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The overall architecture of the proposed JCFA model for cross-corpus EEG-based emotion recognition. JCFA consists",
      "page": 3
    },
    {
      "caption": "Figure 1: It consists of two stages: (1) joint contrastive",
      "page": 3
    },
    {
      "caption": "Figure 2: Confusion matrices of E2STN (the second-best",
      "page": 7
    },
    {
      "caption": "Figure 2: compares the classification confusion matrices of E2STN",
      "page": 7
    },
    {
      "caption": "Figure 3: Without the alignment loss LA, the time",
      "page": 7
    },
    {
      "caption": "Figure 4: The comparison re-",
      "page": 8
    },
    {
      "caption": "Figure 3: t-SNE visualization of embeddings in the latent",
      "page": 9
    },
    {
      "caption": "Figure 4: Comparison of model performance on the SEED",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Methods": "",
          "ACC Â± STD (%)": "SEED-IV3 â†’ SEED3"
        },
        {
          "Methods": "SVM [34]*\nDANN [10]*\nA-LSTM [32]\nV-IAG [30]\nGECNN [31]\nBiDANN [21]\nTANN [20]\nE2STN [45]\nSimCLR [4, 35]*\nMixup [37, 40]*\nJCFA (Ours)",
          "ACC Â± STD (%)": "41.77 Â± 11.13\n49.95 Â± 09.27\n46.47 Â± 08.30\n52.84 Â± 07.71\n58.02 Â± 07.03\n49.24 Â± 10.49\n58.41 Â± 07.16\n60.51 Â± 05.41\n47.27 Â± 08.44\n56.86 Â± 16.83\n67.53 Â± 12.36 (+07.02)"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Methods": "w/o LF, LA and ğº\nw/o LT, LA and ğº\nw/o LA and ğº\nw/o ğº\nFull Model (JCFA)",
          "Accuracy": "48.68 Â± 11.26\n63.09 Â± 13.97\n63.38 Â± 14.22\n66.10 Â± 12.74\n67.53 Â± 12.36",
          "Precision": "49.62 Â± 11.99\n63.53 Â± 14.06\n63.77 Â± 14.71\n66.59 Â± 12.98\n68.12 Â± 12.84",
          "Recall": "48.63 Â± 11.16\n62.87 Â± 13.99\n63.18 Â± 14.22\n65.87 Â± 12.75\n67.33 Â± 12.44",
          "F1 Score": "47.97 Â± 11.06\n62.51 Â± 14.27\n62.29 Â± 15.07\n64.94 Â± 13.83\n66.57 Â± 12.25",
          "AUROC": "66.24 Â± 11.25\n79.18 Â± 11.13\n78.19 Â± 11.10\n80.85 Â± 10.11\n82.63 Â± 10.06",
          "AUPRC": "52.27 Â± 14.05\n68.11 Â± 14.41\n67.57 Â± 14.78\n70.80 Â± 13.57\n72.46 Â± 13.49"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Methods": "w/o LF, LA and ğº\nw/o LT, LA and ğº\nw/o LA and ğº\nw/o ğº\nFull Model (JCFA)",
          "Accuracy": "46.73 Â± 05.07\n55.89 Â± 08.36\n57.86 Â± 07.99\n60.14 Â± 06.52\n62.40 Â± 07.54",
          "Precision": "44.85 Â± 04.34\n55.92 Â± 08.38\n57.18 Â± 07.59\n60.12 Â± 06.54\n61.80 Â± 08.78",
          "Recall": "44.30 Â± 05.64\n55.57 Â± 08.26\n56.96 Â± 07.76\n59.29 Â± 06.48\n60.93 Â± 08.21",
          "F1 Score": "41.95 Â± 05.24\n54.71 Â± 08.43\n56.06 Â± 07.58\n58.53 Â± 06.56\n60.09 Â± 09.04",
          "AUROC": "61.59 Â± 06.67\n74.79 Â± 07.60\n74.98 Â± 07.41\n78.53 Â± 05.98\n78.35 Â± 07.74",
          "AUPRC": "47.84 Â± 06.26\n62.58 Â± 09.30\n62.71 Â± 08.79\n66.32 Â± 07.53\n66.77 Â± 09.66"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fine-tuning Trials": "3\n6\n9\n12",
          "Accuracy": "60.82 Â± 09.33\n63.77 Â± 12.12\n67.53 Â± 12.36\n66.22 Â± 13.72",
          "Precision": "61.65 Â± 09.00\n63.82 Â± 12.53\n68.12 Â± 12.84\n67.25 Â± 12.74",
          "Recall": "60.64 Â± 09.35\n63.40 Â± 12.03\n67.33 Â± 12.44\n65.97 Â± 13.67",
          "F1 Score": "60.10 Â± 09.46\n63.01 Â± 12.26\n66.57 Â± 12.25\n65.07 Â± 14.63",
          "AUROC": "78.02 Â± 08.89\n79.95 Â± 10.25\n82.63 Â± 10.06\n81.37 Â± 10.58",
          "AUPRC": "66.70 Â± 11.17\n68.48 Â± 13.69\n72.46 Â± 13.49\n71.08 Â± 13.89"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fine-tuning Trials": "3\n6\n9\n12\n15",
          "Accuracy": "51.60 Â± 05.74\n56.39 Â± 05.84\n60.04 Â± 06.55\n62.40 Â± 07.54\n61.46 Â± 06.77",
          "Precision": "51.38 Â± 05.85\n56.20 Â± 05.60\n59.35 Â± 06.79\n61.80 Â± 08.78\n61.90 Â± 06.65",
          "Recall": "51.06 Â± 05.71\n55.46 Â± 06.22\n59.21 Â± 06.65\n60.93 Â± 07.21\n61.35 Â± 06.48",
          "F1 Score": "50.36 Â± 05.87\n54.87 Â± 06.20\n58.43 Â± 06.74\n60.09 Â± 09.04\n60.45 Â± 07.05",
          "AUROC": "69.20 Â± 05.73\n73.79 Â± 06.13\n76.20 Â± 05.84\n78.35 Â± 07.74\n78.72 Â± 06.31",
          "AUPRC": "55.82 Â± 06.19\n61.03 Â± 07.07\n64.27 Â± 07.13\n66.77 Â± 09.66\n66.75 Â± 07.32"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "recognition using EEG signals: A survey",
      "authors": [
        "M Soraia",
        "Manuel Alarcao",
        "Fonseca"
      ],
      "year": "2017",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "2",
      "title": "Emotion estimation from EEG signals during listening to Quran using PSD features",
      "authors": [
        "Mashail Alsolamy",
        "Anas Fattouh"
      ],
      "year": "2016",
      "venue": "2016 7th International Conference on computer science and information technology (CSIT)"
    },
    {
      "citation_id": "3",
      "title": "Cognitive behavioral therapy for anxiety and related disorders: A meta-analysis of randomized placebo-controlled trials",
      "authors": [
        "Leigh Joseph K Carpenter",
        "Sara Andrews",
        "Mark Witcraft",
        "Jasper Aj Powers",
        "Stefan Smits",
        "Hofmann"
      ],
      "year": "2018",
      "venue": "Depression and anxiety"
    },
    {
      "citation_id": "4",
      "title": "A simple framework for contrastive learning of visual representations",
      "authors": [
        "Ting Chen",
        "Simon Kornblith",
        "Mohammad Norouzi",
        "Geoffrey Hinton"
      ],
      "year": "2020",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "5",
      "title": "Emotion recognition in humancomputer interaction",
      "authors": [
        "Roddy Cowie",
        "Ellen Douglas-Cowie",
        "Nicolas Tsapatsoulis",
        "George Votsis",
        "Stefanos Kollias",
        "Winfried Fellenz",
        "John Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal processing magazine"
    },
    {
      "citation_id": "6",
      "title": "Convolutional neural networks on graphs with fast localized spectral filtering",
      "authors": [
        "MichaÃ«l Defferrard",
        "Xavier Bresson",
        "Pierre Vandergheynst"
      ],
      "year": "2016",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "7",
      "title": "Differential entropy feature for EEG-based emotion classification",
      "authors": [
        "Jia-Yi Ruo-Nan Duan",
        "Bao-Liang Zhu",
        "Lu"
      ],
      "year": "2013",
      "venue": "Differential entropy feature for EEG-based emotion classification"
    },
    {
      "citation_id": "8",
      "title": "Time-frequency/time-scale analysis",
      "authors": [
        "Patrick Flandrin"
      ],
      "year": "1998",
      "venue": "Time-frequency/time-scale analysis"
    },
    {
      "citation_id": "9",
      "title": "Emotion recognition in humancomputer interaction",
      "authors": [
        "Nickolaos Fragopanagos",
        "John Taylor"
      ],
      "year": "2005",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "10",
      "title": "Domain-adversarial training of neural networks",
      "authors": [
        "Yaroslav Ganin",
        "Evgeniya Ustinova",
        "Hana Ajakan",
        "Pascal Germain",
        "Hugo Larochelle",
        "FranÃ§ois Laviolette",
        "Mario March",
        "Victor Lempitsky"
      ],
      "year": "2016",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "11",
      "title": "Autocorrelation function and power spectral density of laser-produced speckle patterns",
      "authors": [
        "I Lester",
        "Goldfischer"
      ],
      "year": "1965",
      "venue": "Josa"
    },
    {
      "citation_id": "12",
      "title": "ASTDF-Net: Attention-Based Spatial-Temporal Dual-Stream Fusion Network for EEG-Based Emotion Recognition",
      "authors": [
        "Peiliang Gong",
        "Ziyu Jia",
        "Pengpai Wang",
        "Yueying Zhou",
        "Daoqiang Zhang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "13",
      "title": "Deep mul timodal learning for emotion recognition in spoken language",
      "authors": [
        "Yue Gu",
        "Shuhong Chen",
        "Ivan Marsic"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "PGCN: Pyramidal graph convolutional network for EEG emotion recognition",
      "authors": [
        "Ming Jin",
        "Enwei Zhu",
        "Changde Du",
        "Huiguang He",
        "Jinpeng Li"
      ],
      "year": "2023",
      "venue": "PGCN: Pyramidal graph convolutional network for EEG emotion recognition",
      "arxiv": "arXiv:2302.02520"
    },
    {
      "citation_id": "15",
      "title": "Clocs: Contrastive learning of cardiac signals across space, time, and patients",
      "authors": [
        "Dani Kiyasseh",
        "Tingting Zhu",
        "David Clifton"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "16",
      "title": "Domain adaptation techniques for EEG-based emotion recognition: a comparative study on two public datasets",
      "authors": [
        "Zirui Lan",
        "Olga Sourina",
        "Lipo Wang",
        "Reinhold Scherer",
        "Gernot R MÃ¼ller-Putz"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "17",
      "title": "Domain adaptation for EEG emotion recognition based on latent representation similarity",
      "authors": [
        "Jinpeng Li",
        "Shuang Qiu",
        "Changde Du",
        "Yixin Wang",
        "Huiguang He"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "18",
      "title": "A multi-domain adaptive graph convolutional network for EEG-based emotion recognition",
      "authors": [
        "Rui Li",
        "Yiting Wang",
        "Bao-Liang Lu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "19",
      "title": "EEG based emotion recognition: A tutorial and review",
      "authors": [
        "Xiang Li",
        "Yazhou Zhang",
        "Prayag Tiwari",
        "Dawei Song",
        "Bin Hu",
        "Meihong Yang",
        "Zhigang Zhao",
        "Neeraj Kumar",
        "Pekka Marttinen"
      ],
      "year": "2022",
      "venue": "Comput. Surveys"
    },
    {
      "citation_id": "20",
      "title": "A novel transferability attention neural network model for EEG emotion recognition",
      "authors": [
        "Yang Li",
        "Boxun Fu",
        "Fu Li",
        "Guangming Shi",
        "Wenming Zheng"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "21",
      "title": "A Novel Neural Network Model based on Cerebral Hemispheric Asymmetry for EEG Emotion Recognition",
      "authors": [
        "Yang Li",
        "Wenming Zheng",
        "Zhen Cui",
        "Tong Zhang",
        "Yuan Zong"
      ],
      "year": "2018",
      "venue": "IJCAI"
    },
    {
      "citation_id": "22",
      "title": "Contrastive self-supervised representation learning for sensing signals from the time-frequency perspective",
      "authors": [
        "Dongxin Liu",
        "Tianshi Wang",
        "Shengzhong Liu",
        "Ruijie Wang",
        "Shuochao Yao",
        "Tarek Abdelzaher"
      ],
      "year": "2021",
      "venue": "2021 International Conference on Computer Communications and Networks (ICCCN)"
    },
    {
      "citation_id": "23",
      "title": "Real-time movie-induced discrete emotion recognition from EEG signals",
      "authors": [
        "Yong-Jin Liu",
        "Minjing Yu",
        "Guozhen Zhao",
        "Jinjing Song",
        "Yan Ge",
        "Yuanchun Shi"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "24",
      "title": "Contrastive representation learning for electroencephalogram classification",
      "authors": [
        "Mostafa Neo Mohsenvand",
        "Mohammad Izadi",
        "Pattie Maes"
      ],
      "year": "2020",
      "venue": "Machine Learning for Health"
    },
    {
      "citation_id": "25",
      "title": "Survey on emotional body gesture recognition",
      "authors": [
        "Fatemeh Noroozi",
        "Adrian Ciprian",
        "Dorota Corneanu",
        "Tomasz KamiÅ„ska",
        "Sergio SapiÅ„ski",
        "Gholamreza Escalera",
        "Anbarjafari"
      ],
      "year": "2018",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "26",
      "title": "The fast Fourier transform",
      "authors": [
        "J Henri",
        "Henri Nussbaumer",
        "Nussbaumer"
      ],
      "year": "1982",
      "venue": "The fast Fourier transform"
    },
    {
      "citation_id": "27",
      "title": "Applications in time-frequency signal processing",
      "year": "2018",
      "venue": "Applications in time-frequency signal processing"
    },
    {
      "citation_id": "28",
      "title": "Cross-corpus EEG-based emotion recognition",
      "authors": [
        "Soheil Rayatdoost",
        "Mohammad Soleymani"
      ],
      "year": "2018",
      "venue": "IEEE 28th international workshop on machine learning for signal processing"
    },
    {
      "citation_id": "29",
      "title": "Contrastive learning of subject-invariant EEG representations for cross-subject emotion recognition",
      "authors": [
        "Xinke Shen",
        "Xianggen Liu",
        "Xin Hu",
        "Dan Zhang",
        "Sen Song"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "30",
      "title": "Variational instance-adaptive graph for EEG emotion recognition",
      "authors": [
        "Tengfei Song",
        "Suyuan Liu",
        "Wenming Zheng",
        "Yuan Zong",
        "Zhen Cui",
        "Yang Li",
        "Xiaoyan Zhou"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "31",
      "title": "Graph-embedded convolutional neural network for image-based EEG emotion recognition",
      "authors": [
        "Tengfei Song",
        "Wenming Zheng",
        "Suyuan Liu",
        "Yuan Zong",
        "Zhen Cui",
        "Yang Li"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Emerging Topics in Computing"
    },
    {
      "citation_id": "32",
      "title": "MPED: A multi-modal physiological emotion database for discrete emotion recognition",
      "authors": [
        "Tengfei Song",
        "Wenming Zheng",
        "Cheng Lu",
        "Yuan Zong",
        "Xilei Zhang",
        "Zhen Cui"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "33",
      "title": "EEG emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "Tengfei Song",
        "Wenming Zheng",
        "Peng Song",
        "Zhen Cui"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "34",
      "title": "Least squares support vector machine classifiers. Neural processing letters",
      "authors": [
        "A Johan",
        "Joos Suykens",
        "Vandewalle"
      ],
      "year": "1999",
      "venue": "Least squares support vector machine classifiers. Neural processing letters"
    },
    {
      "citation_id": "35",
      "title": "Exploring Contrastive Learning in Human Activity Recognition for Healthcare",
      "authors": [
        "Ian Chi",
        "Ignacio Tang",
        "Dimitris Perez-Pozuelo",
        "Cecilia Spathis",
        "Mascolo"
      ],
      "year": "2020",
      "venue": "Exploring Contrastive Learning in Human Activity Recognition for Healthcare",
      "arxiv": "arXiv:2011.11542"
    },
    {
      "citation_id": "36",
      "title": "Visualizing data using t-SNE",
      "authors": [
        "Laurens Van Der Maaten",
        "Geoffrey Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "37",
      "title": "Mixing up contrastive learning: Self-supervised representation learning for time series",
      "authors": [
        "Kristoffer WickstrÃ¸m",
        "Michael Kampffmeyer",
        "Karl Ã˜yvind Mikalsen",
        "Robert Jenssen"
      ],
      "year": "2022",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "38",
      "title": "Ts2vec: Towards universal representation of time series",
      "authors": [
        "Zhihan Yue",
        "Yujing Wang",
        "Juanyong Duan",
        "Tianmeng Yang",
        "Congrui Huang",
        "Yunhai Tong",
        "Bixiong Xu"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "39",
      "title": "Facial expression recognition via learning deep sparse autoencoders",
      "authors": [
        "Nianyin Zeng",
        "Hong Zhang",
        "Baoye Song",
        "Weibo Liu",
        "Yurong Li",
        "Abdullah Dobaie"
      ],
      "year": "2018",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "40",
      "title": "mixup: Beyond empirical risk minimization",
      "authors": [
        "Hongyi Zhang",
        "Moustapha Cisse",
        "David Yann N Dauphin",
        "Lopez-Paz"
      ],
      "year": "2017",
      "venue": "mixup: Beyond empirical risk minimization",
      "arxiv": "arXiv:1710.09412"
    },
    {
      "citation_id": "41",
      "title": "Emotionmeter: A multimodal framework for recognizing human emotions",
      "authors": [
        "Wei-Long Zheng",
        "Wei Liu",
        "Yifei Lu",
        "Bao-Liang Lu",
        "Andrzej Cichocki"
      ],
      "year": "2018",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "42",
      "title": "Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks",
      "authors": [
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on autonomous mental development"
    },
    {
      "citation_id": "43",
      "title": "EEG-based emotion recognition using regularized graph neural networks",
      "authors": [
        "Peixiang Zhong",
        "Di Wang",
        "Chunyan Miao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "44",
      "title": "PR-PL: A novel prototypical representation based pairwise learning framework for emotion recognition using EEG signals",
      "authors": [
        "Rushuang Zhou",
        "Zhiguo Zhang",
        "Hong Fu",
        "Li Zhang",
        "Linling Li",
        "Gan Huang",
        "Fali Li",
        "Xin Yang",
        "Yining Dong",
        "Yuan-Ting Zhang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "45",
      "title": "EEG-based Emotion Style Transfer Network for Cross-dataset Emotion Recognition",
      "authors": [
        "Yijin Zhou",
        "Fu Li",
        "Yang Li",
        "Youshuo Ji",
        "Lijian Zhang",
        "Yuanfang Chen",
        "Wenming Zheng",
        "Guangming Shi"
      ],
      "year": "2023",
      "venue": "EEG-based Emotion Style Transfer Network for Cross-dataset Emotion Recognition",
      "arxiv": "arXiv:2308.05767"
    },
    {
      "citation_id": "46",
      "title": "Emotion self-regulation training in major depressive disorder using simultaneous real-time fMRI and EEG neurofeedback",
      "authors": [
        "Vadim Zotev",
        "Ahmad Mayeli",
        "Masaya Misaki",
        "Jerzy Bodurka"
      ],
      "year": "2020",
      "venue": "NeuroImage: Clinical"
    }
  ]
}