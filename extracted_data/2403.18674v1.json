{
  "paper_id": "2403.18674v1",
  "title": "Deep Learning For Robust And Explainable Models In Computer Vision",
  "published": "2024-03-27T15:17:10Z",
  "authors": [
    "Mohammadreza Amirian"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "I want to extend my heartfelt gratitude to the individuals who have played essential roles in my academic journey while completing my Ph.D. dissertation. Their support, mentorship, and presence have been invaluable, and I am deeply appreciative. I must begin by acknowledging the support of my family: Edalat, Parivash, and Milad. Their belief in my aspirations and constant encouragement has strengthened me throughout this demanding journey. I am profoundly grateful to my distinguished Ph.D. committee, particularly Professor Friedhelm Schwenker, to whom I owe special thanks for his mentorship.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "List Of Figures",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "2.1",
      "text": "The cross-correlation function is often implemented in deep learning libraries for convolutional neural networks. For an input image, the output (kernel response) is the dot product of the vectorized kernel with a field which is the same size as the input image. The kernel slides over the entire image area with a given step size (figure adopted from  [146] ). . . . . . . . . . . . . . . .",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "2.2",
      "text": "Max-pooling (MP) and average-pooling (AP) layers with kernel size and stride of 2 for CNNs (figure adopted from  [291] ). . . . .",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "2.3",
      "text": "A convolutional neural network for representation learning from an input image, followed by a feedforward network for object classification. . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "2.4",
      "text": "The residual connection between a layer's input and output improves the gradient flow (figure is adopted from  [96] ). . . . . . .",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "2.5",
      "text": "This figure, adopted from  [260] , depicts the idea of the inception blocks and their practical implementation. . . . . . . . . . . . .",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "2.6",
      "text": "Convolutional block attention module (CBAM) with its two main components for refining channel and spatial features (figures are adopted from  [260] ). . . . . . . . . . . . . . . . . . . . . . . . .",
      "page_start": 39,
      "page_end": 39
    },
    {
      "section_name": "2.7",
      "text": "Two-and three-dimensional convolutions. 2D convolutions target images, while 3D convolutions are suitable for volumetric data (figures are adopted from  [108] ). . . . . . . . . . . . . . . . . . .",
      "page_start": 40,
      "page_end": 40
    },
    {
      "section_name": "2.8",
      "text": "Vision transformers (ViTs) for image classification (figure is adopted from  [65] ). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
      "page_start": 45,
      "page_end": 45
    },
    {
      "section_name": "2.9",
      "text": "Vision transformers (ViTs) for image semantic segmentation (figure is adopted from  [253] ). . . . . . . . . . . . . . . . . . . . . . viii Figures on the top and bottom rows visualize the position of a test image in the clusters optimized using the unsupervised loss function. The output of CNN backbones is connected to RBFs' input through a fully connected layer, and the input features of the RBFs are referred to as embeddings in this chapter. The model compares the embeddings of each image with cluster centers using a trainable similarity distance metric. The same distance metric can be used to find similar and dissimilar images to a test sample amongst training images (visualized in the table in the middle row). The RBFs apply an activation function to the distance of the training images from the cluster centers to compute activation values. The output layer of the RBF is optimized for classification based on these activation values. The entire CNN-RBF architecture is optimized end-to-end with a specific initialization (figure adopted from  [14] ). . . . . . . . . . . . . . .",
      "page_start": 46,
      "page_end": 46
    },
    {
      "section_name": "3.2",
      "text": "Activation functions for RBF networks. Here is the list of the parameters for depicting the kernels: σ = 1, α = 1/2, and β = 1/2. The proposed quadratic activation kernel is linear based on the r 2 . Consequently, the CNN goes through a completely linear forward path, and thus, gradients are computed and backpropagated efficiently (figure adopted from  [14] ). . . . . . . . . . . . . . . .",
      "page_start": 59,
      "page_end": 59
    },
    {
      "section_name": "3.3",
      "text": "Hyperparameter search results from CIFAR-10 (top) and CIFAR-100 (bottom). The top five performing sets of hyperparameters for each dataset are highlighted in yellow (figures adapted from  [14] ). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  3.4  This figure presents the location of data samples compared to the cluster centers during the training process. The centers of the clusters are in the middle of the figures. The training samples are located at a random angle based on their distance from the center of the clusters. The vertical and horizontal axes show the normalized distances (figure adopted from  [14] ). . . . . . . . . .",
      "page_start": 64,
      "page_end": 64
    },
    {
      "section_name": "3.5",
      "text": "Two-dimensional representation of the training process: the figure presents the embeddings of the convolutional backbone (top row), and the activations of the RBFs (bottom row) mapped to a two-dimensional space using t-SNE  [277] . The vertical and horizontal axes depict the normalized values; however, all sub-figures use the same normalization factors (figure adopted from  [14] ). .  3.6  This figure depicts similar and dissimilar training images for given test images based on the similarity metric computed in Equation 3.1. The figure depicts the top 7 most similar and dissimilar training images for a given test image in every two rows. The images shown in every two consecutive rows belong to one of the datasets in Table  3 .1 in the same order (figure adopted from  [14] ). ix 3.7\n\nThe presented figure visualizes the top 14 images selected using different distance metrics in the embedding space for a given test image (figure adopted from  [14] ). . . . . . . . . . . . . . . . . .",
      "page_start": 66,
      "page_end": 66
    },
    {
      "section_name": "3.8",
      "text": "The Figure illustrates the clusters contributing to a CNN-RBF network's correct class (top row) and the wrong class (bottom row). The larger image with red borders in each cluster representation is the test sample. Red circles show the distance of the samples to the cluster center, and the background is proportional to the activation values of the cluster. The brighter the activation value, the larger it is, and the maximum activation at the cluster center is equal to one (figure adopted from  [14] ). . . . . .",
      "page_start": 70,
      "page_end": 70
    },
    {
      "section_name": "4.1",
      "text": "Examples of different state-of-the-art adversarial attacks on a VGG19 model: original images and labels (left), perturbations (middle), and mislabeled adversarial examples (right). In the middle column, zero difference is encoded white, and the maximum difference is black because of visual enhancement (figure adopted from  [15] ). . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 a) Distribution of average local spatial entropy in clean images (green) versus adversarial examples (red) as computed on the Im-ageNet validation set  [223] . b) Receiver operating characteristic (ROC) curve of the performance of the detection algorithm on different attacks (figure adopted from  [15] ). . . . . . . . . . . . .",
      "page_start": 75,
      "page_end": 75
    },
    {
      "section_name": "4.3",
      "text": "Successful adversarial examples created by DeepFool  [188]  for binary and ternary classification tasks are only possible with noticeably visible perturbations (figure adopted from  [15] ). . . . .",
      "page_start": 82,
      "page_end": 82
    },
    {
      "section_name": "5.1",
      "text": "Motion Artifacts. Left: CBCT scans with motion artifacts from the test dataset. Right: Scan with artificially produced motion artifacts from the motion simulation. The scans are presented in HU with W/L=1000/0 (figure adopted from  [12] ). . . . . . . . .",
      "page_start": 89,
      "page_end": 89
    },
    {
      "section_name": "5.2",
      "text": "The architecture of the proposed dual-domain model for endto-end optimization consists of the following components: (i) a projection enhancement network (PE-Net), (ii) a projectionto-volume reconstruction layer, and (iii) a volume enhancement network (VE-Net) (figure adopted from  [12] ). . . . . . . . . . . .",
      "page_start": 91,
      "page_end": 91
    },
    {
      "section_name": "5.3",
      "text": "Example results for FDK reconstruction (volume domain optimization). Presented is the uncorrected volume using default reconstruction (left), the ground truth volume, both as difference and absolute image (\"average volume\", top right), as well as the corrected volume (bottom right). Images are presented in HU with W/L=1000/0 (figure adopted from  [12] ). . . . . . . . . Example results for FDK reconstruction (volume domain optimization). The uncorrected volume using default reconstruction (left), the ground truth volume, both as difference and absolute image (\"average volume\", top right), as well as the corrected volume (bottom right) are depicted in the table. Images are presented in HU with W/L=1000/0 (figure adopted from  [12] ). . . . 5.5\n\nThe table shows example results for iCBCT reconstruction for real-world test dataset, using the two options for the choice of ground truth. The uncorrected volumes using default reconstruction (left), the residual corrections (middle), as well as the corrected volumes (right) are presented (figure adopted from  [12] ). .",
      "page_start": 96,
      "page_end": 96
    },
    {
      "section_name": "6.1",
      "text": "Several action units (AUs) used for facial expression estimation (figure is adopted from  [8] ). . . . . . . . . . . . . . . . . . . . .",
      "page_start": 104,
      "page_end": 104
    },
    {
      "section_name": "6.2",
      "text": "Several dictionary-learned facial templates used for facial feature extraction (figure is adopted from  [8] ). . . . . . . . . . . . . . .  6.3  Presented is the proposed sequence of blocks for the automatic fusion of audiovisual and biophysiological information to predict arousal levels (figure is adopted from  [10] ). . . . . . . . . . . . .  6.4  Presented are the echo state networks (ESNs) based architectures for modeling temporal information dependencies and fusing multimodal information. One ESN is trained for each modality, and the predictions of all modalities are combined using precomputed weights according to the importance of modalities per task. . . . 6.  5  The figure depicts the temporal mismatch between audio features and gold standard labels. The average performance of predicting the arousal level of participants from audio increases considerably when features are aligned with gold-standard labels (figure is adopted from  [10] ). . . . . . . . . . . . . . . . . . . . . . . . . 6.  6  Performance of four different vision datasets in terms of ALC of MobileNetV2 as a function of weight decay and learning rate (top two rows) and averaged performance over all datasets (bottom row). The green dot indicates the best performance (figures are adopted from  [271] ). . . . . . . . . . . . . . . . . . . . . . . . . 6.  7  The proposed architecture for PrepNet model with three modules: (i) an auto-encoder aims at CT dataset homogenizer; (ii) a multiclass classifier to recognized CT-datasets; and (iii) a binary classifier for diagnosis . The loss functions of the dataset classifier and auto-encoder were trained adversarially against each other. The binary classifier for diagnosis (COVID-19) was trained independently using the preprocessed scans by auto-encoder (figure adopted from  [11] ). . . . . . . . . . . . . . . xi 6.8\n\nOriginal images from the datasets with different prepossessing methods applied (figure adopted from  [11] ). . . . . . . . . . . . . 99 6.9\n\nProbability density distribution of pairwise (Euclidean and Cosine) distances between test images' embeddings of different races.\n\nThe embeddings are computed using the VGG model fine-tuned for face recognition (VGGFace2  [35] ) with different embedding dimensionalities  (128, 256 and 2048) . The figure shows that the faces from the Caucasian race, which have the largest share of data samples, have a larger average distance than those of Africans, Asians, and Indians (figure adopted from  [87] ). . . . . 101 6.10 Classification and segmentation performance of vision transformers under different degrees of rotation. Augmentation improves the robustness of vision transformers against rotation; however, rotation invariance encoded in the method as inductive bias can improve the sample efficiency of the models. . . . . . . . . . . .  103 6.11  The proposed patch embedding methods for vision transformers: a) Isotropic path embedding for the entire image. Every patch is sampled based on the circles around the center of the patch. b) Radial patch embedding in the patch level technique samples every patch based on the pixels on a circle radius positioned at the patch's center. c) Radial patch embedding for the entire image.106 6.  12   An overview of computer vision benchmark datasets used to evaluate the performance of CNN-RBFs (table adopted from  [14] ). .",
      "page_start": 105,
      "page_end": 106
    },
    {
      "section_name": "3.2",
      "text": "List of the final hyperparameters used for each computer vision benchmark dataset to achieve the performance of CNN-RBF architectures (table adapted from  [14] ). . . . . . . . . . . . . . . .",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "3.3",
      "text": "Comparing the performance of various CNN-RBF architectures with pretraining and augmentation on benchmark computer vision datasets. The best results column is the top performance of the current state-of-the-art architecture on the benchmark dataset (table adapted from  [14] ). . . . . . . . . . . . . . . . . .",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "4.1",
      "text": "Effect of adversarial attacks on feature responses: (left) original images, and their feature responses, (right) perturbed versions, and their feature responses (figure adopted from  [15] ). . . . . . .",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "4.2",
      "text": "Input, feature response maps, and local spatial entropy for clean and perturbed images, respectively (table adopted from  [15] ). . .",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "4.3",
      "text": "The table describes the numerical evaluation of detection performance on different adversarial attacks. Column two gives the number of tested images and approximate elapsed run time.\n\nThe success of an adversarial attack is defined if a perturbation changes the prediction. Columns four and five show average confidence values of the true (ground truth) and wrong (target) classes after a successful attack. Finally, the last columns show detection rates for different false positive rates (table adopted from  [15] ). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
      "page_start": 17,
      "page_end": 18
    },
    {
      "section_name": "4.4",
      "text": "This table describes the performance of similar state-of-the-art adversarial attack detection methods. The Area Under Curve (AUC) is the average value of all attacks in the third and last row (table adopted from  [15] ). . . . . . . . . . . . . . . . . . . . Presented are the quantitative results of DL-based motion correction for CBCT data with simulated motion. The table presents the performance of the proposed motion reduction framework based on the RMSE, PSNR, and SSIM metrics and reports the mean and standard deviation of the body-masked difference (correction) volumes. The metrics are calculated between the reconstructed and ground truth volumes, converted to HU with slope and intercept of 48200 and -1106, respectively. All numerical values are averaged over the test set. The table shows the average metric together with the average gain (or loss) and the latter's standard deviation to clarify the contribution of the motion correction. For example, in the last row, the average PSNR is reported as 33.00 dB, corresponding to an average improvement of 4.62 dB, with a standard deviation of 0.82 dB. The models noted by † are used for clinical evaluation (Section 5.5.2) (figure adopted from  [12] ). . . . . . . . . . . . . . . . . . . . . . . . . .",
      "page_start": 81,
      "page_end": 81
    },
    {
      "section_name": "5.2",
      "text": "Results of the clinical evaluation. This table shows the preferences for CNN-based or default iCBCT reconstruction when using CNN models trained using either average volume or average amplitude ground truth concerning motion artifact reduction and potential applications such as plan adaptation and dose calculation, patient positioning and segmentation (table adopted from  [12] ). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
      "page_start": 100,
      "page_end": 100
    },
    {
      "section_name": "6.1",
      "text": "Performance of three automated machine learning algorithms with different paradigms on AutoML challenge datasets and their convergence time  [94]  (table adopted from  [272] ). . . . . . . . . .",
      "page_start": 111,
      "page_end": 111
    },
    {
      "section_name": "6.2",
      "text": "Test and cross-dataset performance of different methods. Using an adversarial loss to train a PrepNet improves the cross-dataset average performance (table adopted from  [11] ). . . . . . . . . . .",
      "page_start": 116,
      "page_end": 116
    },
    {
      "section_name": "6.3",
      "text": "The performance of rotation invariant vision transformers on several vision benchmark vision datasets. Rotation invariant patch embedding increases the robustness of ViTs at the expense of a decrease in performance. . . . . . . . . . . . . . . . . . . . . . .",
      "page_start": 125,
      "page_end": 125
    },
    {
      "section_name": "Introduction",
      "text": "As a result of the widespread interest in applying artificial intelligence (AI) in practice, several intriguing challenges and research topics have recently emerged due to the trustworthiness of models in various circumstances being brought into question  [109] . Researchers have cited explainability, robustness, and fairness among other hindrances in developing trustworthy AI  [123] . Therefore, understanding the reasons for failure and creating ability to explain the inner workings of neural networks has attracted researchers' attention  [64, 268] . Furthermore, developing interpretable and explainable models has become a research focus in its own right  [85] .\n\nThe three terms robustness, explanability, and interpretabilty are the fundamental concepts behind this thesis. The term robustness refers to a clearer concept compared with explanability and interpretabilty. Model robustness is proportional to the consistency of the model's performance against naturally-induced or manually-computed corruption and alterations affecting the data to deviate from the training distribution  [66, 170] . However, the other two terms, explainability and interpretability, and their boundaries and overlaps are still subjects of research at the taxonomy level  [92] . In this thesis, the terms explanability and interpretabilty are used analogously to their usage in  [222] . Accordingly, explanability refers to the explanation of models' decisions (even though these models can be intrinsically black boxes), and interpretability refers to the design patterns that are inherently interpretable and understandable by humans.\n\nThe scope of this work is narrowed down from AI in general to focus on computer vision models, including convolutional neural networks (CNNs) and vision transformers (ViTs). This thesis is motivated by practical applications and presents relevant research concerning neural networks' robustness, fairness, interpretability, and explainability. Moreover, the thesis provides not only theoretical and fundamental advances but also offers several applications in which computer vision models have been successfully used. The remainder of this chapter explains the motivations for this research and describes the scientific problem we address. Finally, this chapter provides a list of papers and publications related to this research, followed by the thesis organization.",
      "page_start": 19,
      "page_end": 20
    },
    {
      "section_name": "Motivation",
      "text": "This thesis aims at using ML and DL in practical applications, and presents several success stories in Chapter 5 and Chapter 6. Despite the numerous successful applications of DL, there are deterrents for putting it into practice in sensitive applications where humans are involved. This thesis presents relevant research tackling such challenges as follows: 1) adversarial robustness in Chapter 4, 2) explainability via interpretable classifiers in Chapter 3. The remainder of this section describes the motivation of the researchers, elaborates on related efforts, and describes the niches to which this thesis contributes.\n\nResearchers attempted to expose the complications of using DL in practice by studying robustness  [22] , fairness  [179] , explainability  [64] , interpretability  [36] , accountability  [130] , reliability  [226] , safetly  [2] , and privacy  [107]  etc. Although these themes were independently the subject of research and scientific concern, they have only recently been grouped under the overarching topic referred to as trustworthy AI in the literature  [123] . Trustworthy AI literature summarizes the research effort as developing models which are effective in practice and aligned with positive societal effects. The following paragraphs explain the key components of trustworthy AI which are considered in this thesis,explanability, interpretability, robustness and fairness, and describe the goals of the related research.\n\nAfter researchers found flaws in the preciseness and biasedness in vision models  [262, 48]  as well as natural language processing methods  [28, 301] , the European Union introduced the \"right to explain\" in the general data protection regulation (GDPR) as an attempt to protect human rights when decisions are automated  [64] . This human right relates to the human's ability to understand the AI-based agent logic in human-machine interaction  [220] . Some of the research work related to explainability targets explaining the models' decisions even if researchers treat the models as black boxes  [47] . The concept of interpretability includes research attempting to open the black box of neural networks with revealing patterns about the inner mechanism of models  [315] . Saliency map visualizations  [26]  and feature response visualization methods  [247]  are examples of researchers' endeavors in targeting interpretability.\n\nThe topic of robustness is directly related to the performance of vision models. Vision models have shown a drop in performance as the consequence of changes in data distribution based on naturally-induced variations  [66]  or manually-computed perturbations  [170] . Robustness is a significant hurdle to overcome in life-long deployments of vision models, which has inspired many recent research works. An example of the output of such research is equivariant CNNs, which aim to improve robustness against rotation and translation of input images  [45, 218, 111] . Moreover, adversarial training research attempts to neutralize the targeted attack's effect in fooling vision models  [270] . Still, there are several gaps in dataset collection and robust model development for naturally-induced variations and different illumination conditions  [149] .\n\nSocial activists and computer vision researchers recently raised concerns regarding fairness in automated decisions  [99, 23] . Automated decisions are considered unfair if they rely on sensitive variables such as gender, ethnicity, sexual orientation, or disability  [281] . The researchers identified sources of bias leading to unfair decisions, which can be divided into two general categories of algorithmic biases, and biases in the training datasets  [179] . This topic gained much attention and media coverage after the deployment of face recognition (FR) systems in public surveillance  [99, 169, 161, 48] . Since then, many researchers have attempted to tackle the problem of biasedness at the algorithmic level  [217, 238]  and collect diverse datasets to achieve fair modeling for all genders and races  [289, 244] .\n\nThis thesis explores the obstacles to using AI, particularly machine learning (ML) and deep learning (DL), in practical applications of computer vision in which robustness and explainability are of high importance. The ultimate goal of this work is the successful application of ML and DL algorithms, although this is not a trivial task and raises many additional questions that require further research.\n\nA short phrase that summarizes the long-term goal alongside the focus of this work is the development of trustworthy AI. Although the term trustworthy AI has only recently come into vogue, this thesis addresses its various components, including fairness, robustness, interpretability, and explainability. Trustworthiness includes other elements outside the scope of this research, such as security, privacy, and accountability. This thesis presents several applications besides fundamental research that support robust and explainable AI (XAI).\n\nUnderstanding the behavior of computer vision models (explainability) has always been a subject of curiosity for scientific endeavors. The first group of researchers who analyzed current models considered them to be black boxes and predicted their performance by changing the input and observing the behavior of the models' output. The second group of researchers proposed intrinsically more interpretable and explainable models. This thesis presents a chapter on using radial basis function networks (RBFs) as classifiers on top of CNNs to improve the interpretability of decision-making in computer vision models which contributes to XAI research.\n\nEarly in the development of CNNs, researchers found that computer vision models were only robust in a limited range of rotation and scaling in the input images  1  .\n\nIn addition, lighting conditions and other environmental disturbances caused errors in recognizing image patterns. Last but not least, the researchers found that optimizing images made it possible to fool the computer vision models into interpreting two images, which humans perceive to be identical, differently, leading to the computation of so-called adversarial perturbations. Since then, improving the robustness of computer vision models has become a popular research topic.\n\nResearchers have made enormous efforts to understand the models, identify the reasons behind failures, and improve the computer vision models. Interpreting the behavior of computer vision models can serve as a tool to monitor the reasons for failures. Therefore, interpretability and robustness are closely related in the literature. For example, researchers have found that computer vision models can focus on the wrong features or background information when classifying an object. This thesis includes a chapter on using feature response maps-where a computer vision focuses its attention in response to the visual input-for identifying adversarial examples.\n\nIn addition to the above theoretical developments, this thesis presents many applications inspired by its original goal. It targets the vulnerabilities (motion artifacts) found in classical computed tomography (CT) reconstruction methods as the main practical contribution. Moreover, it presents many other side contributions to affective computing, pain estimation, AutoML, AutoDL, medical data homogenization, and fairness in face recognition systems.",
      "page_start": 20,
      "page_end": 22
    },
    {
      "section_name": "Problem Statement",
      "text": "This thesis is motivated by solving real-world problems using computer vision methodology. The applications presented in Chapter 5 and Chapter 6 are derived from several real-world problems where ML and DL are useful. However, there are still gaps in the current methodologies that must be addressed in order to achieve trustworthy models for general applications. Chapter 3 and Chapter 4 present the fundamental research targeting the explainability and robustness gaps required to apply computer vision models in practice. The remainder of this section details the scientific problems addressed in each chapter individually.\n\nCurrent architectures for computer vision models based on CNNs and ViTs use a stack of convolutional or self-attention layers to develop a representation of the inputs. Despite the different architectures in the image encoder of most computer vision models, all of these models use a stack of multiple fully connected (FC) layers or multilayer perceptrons (MLP), on top of learned latent representations  [24] .\n\nResearchers commonly used FC layers as the optimal classifiers for deep models because of their efficiency in gradient backpropagation  [148] . MLPs divide the embedding space in their last layer into multiple classes using hyperplanes. The distance of the input image representations from the decision boundary drawn by the hyperplanes in the last layer of such classifiers determines the decision confidence of the models. Researchers found that such classification is not optimal for outliers, since models developed using MLP classifiers demonstrate low reliability for random (garbage) classes. This is due to the outliers being far from the classifier's decision boundary, which contributes to the models' flawed high confidence in these samples. In addition, computer vision models trained using linear classifiers are vulnerable to optimized perturbations (adversarial attacks). Ian Goodfellow has attempted to thoroughly analyze various classifiers to evaluate their robustness to adversarial attackers and garbage classes  [89] . His preliminary speculations hint that RBFs might be more robust than MLPs. However, the study was inconclusive due to the difficulties in optimizing RBF networks, even for simple tasks such as classifying handwritten digits. This thesis proposes modifications to RBF networks that improve the optimization of RBFs and shows how RBF classifiers are beneficial for interpreting the decision-making of computer vision models.\n\nAfter the advent of CNNs, researchers were very skeptical and curious about their functionality. They work as highly accurate models but seem to appear as black boxes. The result of this curiosity and scientific venture is a vast amount of literature analyzing the behavior of CNNs by computing the models' feature response maps through the inversion of the forward path. As extensive as the techniques for visualizing models are, their applications are rare. This thesis presents an example of detecting adversarial attacks using feature response maps.\n\nThe intention is that this idea inspires researchers to use their knowledge of interpreting computer vision models for architecture development and debugging.\n\nNeural networks have outperformed their competitors in approximating arbitrary functions and learning patterns from enormous amounts of data. However, AI projects still face high risks due to not achieving the intended goal, unforeseen delays, extensions, and application failures. This thesis presents several successful applications that allow the reader to understand where using ML and DL-based methods are beneficial. For example, we address motion artifacts in cone beam computed tomography (CBCT) scans. Volumetric (3D) data from CBCT scans are reconstructed from hundreds of 2D X-ray images from different angles. The analytical reconstruction algorithms are robust when the target volumes are constant and free of motion. However, this assumption does not hold due to respiratory or cardiac motions present in the human body. In this work, we demonstrate how CNNs can be used to compensate for motion artifacts in CBCT scans. Along with this application, this thesis offers several other applications to show some possible and successful venues in which ML-and DL-based models are superior to classical computer vision methods in practice.",
      "page_start": 22,
      "page_end": 23
    },
    {
      "section_name": "Contributions",
      "text": "The main contributions of this thesis to ML and DL research are as follows:\n\n• Chapter 3: Modern vision architectures use multilayer perceptrons (MLPs) in the form of fully connected layers as classifiers, as researchers have largely abandoned radial basis function networks (RBFs) due to optimization problems. This thesis provides the following developments in training RBFs as classifiers for convolutional neural networks (CNN) backbones: 1) Presentation of the first successful attempt to use RBFs as the classifier of modern computer vision models for object classification. 2) Introduction of a novel quadratic activation function to build a linear computational graph with RBFs. 3) Simultaneous optimization of supervised loss for classification and unsupervised loss for clustering  [14] .\n\n• Chapter 3: Solving the technical problems of optimizing RBFs as classifiers for computer vision models opens several possibilities for training computer vision models: 1) Combining supervised and unsupervised learning by simultaneously optimizing two target losses. 2) Learning a similarity distance metric to find similar images by optimizing the covariance matrix in the embedding space. 3) Improving the interpretability of the computer vision models by visualizing the data using prototypes and learning more about the models' decision-making  [14] .\n\n• Chapter 4: This thesis presents findings on a well-known vulnerability in the robustness of computer vision models referred to as adversarial attacks in related literature. First, the research presented in Chapter 4 shows how adversarial perturbations leave a detectable trace on the feature response map of CNNs, even though the input image remains identical. Then, feature response maps of CNNs are used with a simple and effective algorithm to detect adversarial attacks with a very competitive accuracy compared to state-of-the-art methods  [15] .\n\n• Chapter 5: Motion artifacts in medical images are a common problem, especially for lengthy acquisition times. This work provides a data-driven solution based on supervised learning to reduce motion artifacts where no analytical solution exists. The proposed solution addresses motion reduction in two reconstruction methods (analytical and iterative) and reduces artifacts in raw data (acquired projections) and reconstructed scans (volume domain). The target domain of this method is cone-beam computed tomography (CBCT) scans, which are used for automatic segmentation and dose calculation in cancer therapy. In this thesis, we present techniques for training models on simulated data that achieve an improvement of over 6 dB in terms of signal-to-noise ratio (PSNR). Moreover, the proposed models generalize to real-world data, and clinical experts have verified their performance in the first attempt at motion compensation for CBCT scans.\n\n• Chapter 6: Optimizing ML and DL models and finding the best models and architectures for small datasets is an intriguing area of research. This thesis presents the most relevant findings from research in automated machine and deep learning (AutoML and AutoDL). The experiments in the context of automated machine learning show that optimizing the parameters of Gaussian processes as surrogate models for hyperparameter spaces (HPs) is the most successful method for HP tuning and meta-learning in AutoML. Moreover, the experimental results in the context of automated deep learning show that regularization and augmentation are the keys for fitting computer vision models to small datasets, that pre-trained models consistently outperform randomly initialized ones, and that large classifiers train faster than smaller ones  [272, 271] .\n\n• Chapter 6: Domain adaptation and merging datasets from multiple data sources in medical imaging is a current research challenge. This thesis proposes an autoencoder-based architecture trained using an adversarial loss to preprocess 2D computed tomography scans for merging multiple datasets with minimal changes in the original scans. The proposed method extends classical training, validation, and testing performance to evaluate cross-dataset generalization and improves the cross-dataset performance for COVID detection from lung CT scans by over 10%  [11] .\n\n• Chapter 6: This thesis presents relevant findings on the measurement of different sorts of biases in face recognition (FR) systems and the relationship between algorithmic bias and awareness. First, after analyzing the results of different models and network embeddings, this work concludes that awareness is not a good proxy for measuring racial bias in FR systems. Second, this thesis presents evidence that models which are designed to be unaware of race are not necessarily unbiased and suggest that further measures are critical for achieving fairness in FR systems  [87, 295] .",
      "page_start": 23,
      "page_end": 24
    },
    {
      "section_name": "Publications",
      "text": "This section presents the list of peer-reviewed and published research papers connected to this thesis, divided based on the publication venue into two groups of journal and conference contributions.",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "Journal Papers",
      "text": "The following is a list of peer-reviewed and published research papers in scientific journals contributing to this thesis:\n\n• Mohammadreza Amirian, Javier Montoya, Thilo   [117] .\n\n• Kamran Kazemi, Mohammadreza Amirian, Mohammad Javad Dehghani. \"The S-transform using a new window to improve frequency and time resolutions.\" Signal, Image and Video Processing, pp. 533-541 (2014)  [124] .",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "Conference Papers",
      "text": "Here is the list of the peer-reviewed and presented research papers in scientific conferences contributing to this thesis:\n\n• Mohammadreza Amirian   [118] .",
      "page_start": 27,
      "page_end": 27
    },
    {
      "section_name": "Book Chapter",
      "text": "Here is the contribution published as a book chapter in conjunction with this thesis:\n\n• Lukas Hollenstein, Lukas Lichtensteiger, Thilo Stadelmann, Mohammadreza Amirian, Lukas Budde, Jürg Meierhofer, Rudolf M Füchslin, Thomas Friedli. \"Unsupervised learning and simulation for complexity management in business operations.\" Applied Data Science. pp.  313-331 (2019)    [102] .",
      "page_start": 29,
      "page_end": 29
    },
    {
      "section_name": "Organization Of Thesis",
      "text": "The remainder of this thesis is organized as follows:\n\n• Chapter 2 provides an overview of the necessary prerequisites and theoretical background for understanding this thesis, summarizes the related work, and relates the following chapters to the current literature. This chapter begins with preliminary content, such as the fundamentals of convolution operation and self-attention. The chapter continues with best practices in architecture search and hyperparameter tuning methods.\n\n• Chapter 3 introduces the main theoretical contribution of this thesis, namely the use of RBF networks as classifiers of CNNs for interpretable decisions. This chapter also proposes changing the training process and introduces a novel quadratic activation function to adapt RBFs for optimization with conventional CNNs.\n\n• Chapter 4 demonstrates how understanding neural networks using feature response map visualizations can improve their robustness by detecting adversarial attacks. In addition, this chapter explains guided backpropagation, a well-known technique for inverting CNN architectures and visualizing the regions of input images which are relevant to the model's classification, and shows the application of feature responses in detecting adversarial attacks.\n\n• Chapter 5 presents the main practical contribution of this thesis, in which neural networks reduce motion artifacts from CBCT scans for various reconstruction techniques. This chapter describes the first attempt to reduce motion artifacts in CBCT scans. It explains the architecture and underlying idea of how supervised learning with simulated data can address a solution for a real-world problem in which there are no ground-truth labels. This chapter includes a clinical evaluation of this method using real-world data and shows how improvement in numerical measures translates to the preferences of clinical experts.\n\n• Chapter 6 provides an overview of several applications in conjunction with this thesis. This chapter aims to draw the readers' attention to several practical problems with ongoing research, present related solutions, and suggest promising areas for future research in these applications. Deep learning has opened a great opportunity to distill information from massive datasets and optimize millions of parameters. However, these methods depend on optimization techniques that converge rapidly to an optimum which generalizes well. Therefore, understanding optimization techniques is necessary to bring the computer vision models to their optimal performance. The computer vision models presented in this thesis can overcome challenging problems that occur when using enormous datasets. These models are prone to overfitting, but they can be optimized to make correct predictions for the training data. However, the models cannot generalize to the unseen data, as is expected and observed in human vision. The last two sections of this chapter discuss the optimization methods, how to avoid overfitting, and how to improve the performance of computer vision architectures in generalization tasks with unseen data. The optimization and generalization of vision models are not the direct subjects of any chapter in this thesis; they are running themes throughout all chapters, especially in Chapter 3, Chapter 5 and parts of Chapter 6.",
      "page_start": 30,
      "page_end": 31
    },
    {
      "section_name": "Convolutional Neural Networks",
      "text": "This section reviews the basics as well as recent advances in developing CNNs for computer vision. It begins with explanations of the building blocks used in CNNs. The computer vision community initially focused on manually improving these models' internal building blocks and introduced novel and suitable building blocks. The focus changed to automated model developments and architecture search when compute resources became widely available. After explaining the basics, this section presents some of the architectural breakthroughs that have improved the accuracy of computer vision models.\n\nAutomated neural architecture search has replaced manual architecture development attempts in the next generation of image processing models. Therefore, this section also describes some of the efforts in the automated search for optimal computer vision neural architectures. Finally, this section concludes with an explanation of the basics of 3D-CNNs and UNet architectures, which are necessary for understanding the content in the final chapters of this thesis.\n\nCNN backbones have replaced hand-crafted feature extraction techniques such as scale-invariant features (SIFT)  [163]  because they can automatically learn representations of images during the optimization process. Based on this analogy, a model can be divided into two parts: 1) an encoder that converts the visual information (e.g., images) into a set of latent (intermediate) representations (model embeddings), and 2) a classifier that identifies the existing objects or segment patterns in the images. The main advantage of CNNs over manual feature extraction is the ability to optimize and fine-tune millions of parameters for encoding visual information into discriminative representations using large datasets. Much com-puter vision research focuses on optimizing models' architecture to compute more generic representations (embeddings) of images. The ultimate goal of this area of research is not only to develop models that can learn image representations but also to train models that generalize well to unseen images from the same data distribution as the training data. A computer vision model's ultimate goal is learning representations that generalize to new categories of images outside the data sets used for optimization. Although the encoder part of neural networks has been the subject of much recent research, feed-forward neural networks have often been chosen for classifiers in the literature because of their efficiency in optimization.",
      "page_start": 32,
      "page_end": 33
    },
    {
      "section_name": "Convolution Operator",
      "text": "The convolution operator of two functions shows how two input functions change their shape when shifted against each other for all possible shift values. For two one-dimensional real-valued time series (x and w), their convolution (s) can be defined as follows:\n\nwhere t is the time, and * denotes the convolution operator. For a given time shift (t), the convolution of two time series is equal to the dot product of one multiplied by the mirrored and shifted version of the other. The convolution operator is commutative due to the time inversion in the definition of the function (f * g = g * f ). Similarly, the convolution operator can be defined for two 1D discrete functions (X and W ) with time stamps i and j within the validity range determined by m as follows:\n\nBased on this interpretation of the 1D convolution operator, we can define the 2D convolution (S) for the two-dimensional image as follows:\n\nwhere I and W represent two images, i and j define the spatial coordinates of these images. The valid range for images is indicated by m and n, respectively. The convolution function shows how a given kernel (W ) changes an input image (I) after the kernel is applied. The commutativity properties also apply to two-dimensional convolutions because of the mirroring the images. Since commutativity is not an essential property of neural networks, most libraries use cross-correlation instead of convolutions for implementation:\n\nThe cross-correlation function computes the dot product of an image patch and the kernel (W ) by shifting the kernel vertically and horizontally over the input image in the range of the images' definition. The step size at which the kernel shifts after each convolutional step is called stride and is a parameter of a convolutional layer in neural networks. The cross-correlation function is often implemented in deep learning libraries for convolutional neural networks. For an input image, the output (kernel response) is the dot product of the vectorized kernel with a field which is the same size as the input image. The kernel slides over the entire image area with a given step size (figure adopted from  [146] ).",
      "page_start": 33,
      "page_end": 34
    },
    {
      "section_name": "Feature Maps",
      "text": "Applying a kernel with the cross-correlation function in equation 2.4 to an image leads to computing a so-called feature map. Depending on the type of kernels, the feature maps contain different information (see figure  2 .1). Feature maps are the first representations of the images computed in the CNNs, and visually inspecting them, along with the first layer inputs, is crucial for understanding the behavior of the entire network. The filters have the same depth as the input images (three for RGB and one for grayscale), and it is possible to visualize them along with the feature maps without complications.",
      "page_start": 35,
      "page_end": 35
    },
    {
      "section_name": "Pooling Layers",
      "text": "The pooling layers aim to summarize the previous layer's output by merging the information in a given neighborhood. Two standard techniques for pooling local information are using the maximum or average value around the center of the kernel. The output of a pooling operator, as shown in Figure  2 .2, is independent of the order of values in that specific region. The pooling layer and the crosscorrelation function are the key components of the CNN for translation invariance as inductive bias 1  in CNNs.",
      "page_start": 35,
      "page_end": 35
    },
    {
      "section_name": "Feedforward Neural Networks",
      "text": "Deep feedforward neural networks, also called multilayer perceptrons (MLPs), are often used in deep architectures to approximate functions. The goal of these networks is to approximate a function (f ) that maps a set of input features (x) to ground truth labels (y ≈ f (x)). Feedforward networks, as shown on the right side of the figure 2.3, consist of an input, an output, and several hidden layers. Each hidden layer of the feedforward network is a fully connected layer that contains an intermediate representation of features by computing the weighted combination of all features in the previous layer. Each fully connected layer in the architecture of an MLP can have a trainable bias term, denoted by x 0 in Figure  2 .3 and trainable weights. Feedforward networks are optimized using backpropagation as described in the next sections.",
      "page_start": 35,
      "page_end": 35
    },
    {
      "section_name": "Convolutional Neural Networks",
      "text": "The simplest form of convolutional nets consists of the two basic layers (convolution and pooling) explained in the previous sections. Basic networks can be constructed using successive convolutional layers to compute input representations and a pooling layer to summarize the information. However, modern convolutional architectures for vision use much more than these two layers. Figure  2 .3 illustrates a simple convolutional network. The original image's pixel values reveal only the mapped object's information for the given pixel size. Feature maps, which show the representations of the first convolutional layer, combine the local information for a given filter size (usually 3 × 3). The pooling layer combines more local information from multiple filter activations (typically 2 × 2) and increases the size of the input region that contributes to a single activation value. The region's size in the original input image contributing to a single value at each network's layer determines the so-called receptive field at a given layer.\n\nFinally, multiple convolutional and pooling layers are connected to a feedforward network for classification. A convolutional net, also called convolutional backbone, aims to compute discriminative (latent) representations of the images for each image class. These representations are finally transformed into the form of a feature vector in the last layer by flattening or global pooling. Flattening rearranges all the activations of a convolutional layer into a single vector, whereas global pooling applies the maximum and average functions to the spatial dimensions of the representations. The one-dimensional vector computed for each image is often referred to in the literature as embeddings. The embeddings of a convolutional neural network are passed to a feedforward network for object classification (Figure  2 .3).",
      "page_start": 36,
      "page_end": 37
    },
    {
      "section_name": "Advanced Blocks",
      "text": "The recent history of convolutional neural networks has had many exciting breakthroughs. However, one of the first modern convolutional neural network prototypes (LeNet-5) was only able to classify handwritten digits  [148] . Training AlexNet  [140] , a relatively small model compared to currently available networks, was only made possible by splitting the model between two graphics processing units (GPUs). Even two years after introducing AlexNet, it was impossible to train very deep VGG models end-to-end without pretraining the model layer by layer  [243] . Given the limitations of resources and algorithms prior to the feasibility of automatic neural architecture search, computer vision researchers mainly tried to incorporate inductive biases to develop better convolutional architectures. These techniques are inspired by image processing tasks and failure cases in the classification task or improvement of the optimization process and gradient flow.\n\nThe following sections review three interesting architectural advances in computer vision.",
      "page_start": 37,
      "page_end": 37
    },
    {
      "section_name": "Residual Connections",
      "text": "Residual connections in CNNs establish a bridge between the input and output of a layer  [96] . Although using a stack of multiple convolutional layers and forming deep models showed better generalization properties than shallow networks, the researchers have traditionally proposed residual connections to improve gradient flow on the backward path. Researchers introduced the idea of using residual connections at the same time that gradient vanishing was the focus of research in computer science for long short-term memory (LSTM) models  [100] . The improvement of the gradient flow also led to the breakthrough of highway networks at the same time  [249] . However, in the following years, residual networks were more commonly used in the research community.  The residual connection between a layer's input and output improves the gradient flow (figure is adopted from  [96] ).",
      "page_start": 37,
      "page_end": 37
    },
    {
      "section_name": "Inception Blocks",
      "text": "The original idea of inception blocks is to summarize the sparse latent representations of image patches into a dense form and cluster the relevant samples using convolutional filters with different patch sizes. Inspired by Arora et al.  [19] , the naive inception block finds the correlations between image patches or representations and clusters them into groups and units of highly correlated samples. Szegedy et al.  [260]  suggested using a layer of 1 × 1 convolutions to cover a small region with many clusters, which is practical for regions where clusters are densely distributed. Furthermore, larger convolutions of size 3 × 3 and 5 × 5 are used for the more spatially spread clusters. Inception blocks also include a pooling operator to maintain the translation invariance property (see Figure  2 .5a).\n\nThe concept of a naive inception block is immensely appealing; however, it suffers from practical feasibility since the computational cost of such blocks blows up within the first few layers. Thus, to reduce the computational complexity of naive inception blocks, they are implemented with 1 × 1 filters to downsample the input representations in practice, while the outputs of the layers are computed by concatenating the representations of the input computed using all four sets of filterbanks depicted in Figure  2 .5b. As a result, the inception models improved state-of-the-art performance in image recognition tasks after their advent.  This figure, adopted from  [260] , depicts the idea of the inception blocks and their practical implementation.",
      "page_start": 38,
      "page_end": 39
    },
    {
      "section_name": "Convolutional Block Attention Module",
      "text": "The main goal of the attention module in convolutional layers is to provide the ability to focus on a specific channel as well as spatial information  [302] . Therefore, this module uses a channel attention module similar to squeeze and excitation techniques  [105]  in addition to a very similar spatial attention module. The high-level idea, shown in Figure  2 .6a, is to compute a channel and a spatial attention map for the input of a given layer and multiply the activation values by these maps to focus on specific channel-spatial information. The whole convolutional block attention module (CBAM) can profit from residual connections to improve gradient flow and allow the model to skip the attention modules.\n\nThe computation of the attention maps is relatively straightforward, as shown in Figure  2 .6. To compute the channel attention maps (see Figure  2 .6b), we first employ a mean and a max pooling over the input feature maps to obtain a vector of the average and maximum of the activation values for each channel. Then, these two vectors are passed through a trainable MLP with shared weights for both pooling outputs. Finally, the activations of the MLPs are averaged and passed through a sigmoid activation to form the final channel attention maps. A similar system is used for spatial attention mechanisms by computing the average and max-pooling over the spatial information instead of the channels and by replacing the MLP with a convolutional layer (see Figure  2 .6c).",
      "page_start": 38,
      "page_end": 38
    },
    {
      "section_name": "Architecture Search",
      "text": "With the rapid increase in computer resources, computer vision researchers began to find ways to expand their search space, from optimizing hyperparameters to exploring new model architectures. Scientists who intuitively invented novel blocks for imaging models began to use their intuition to find the best search space for computer vision architectures and to optimize search techniques. The remainder of this section presents two breakthroughs in neural architecture search.",
      "page_start": 40,
      "page_end": 40
    },
    {
      "section_name": "Nasnets",
      "text": "Zoph et al.  [327]  introduced NASNets in the first famous attempt to search for the optimal architecture for image recognition. They performed the architecture search on a dataset with images of size 32 × 32 pixels from 10 object classes (CIFAR10  [139] ). However, the heuristics and inductive biases allowed successful scaling of the sought after architectures to a large dataset with 299 × 299 images of 1000 classes (ImageNet  [58] ). Zoph et al. designed a controller using recurrent neural networks (RNN) to find the optimal architecture of two motifs named normal cell and reduction cell. The convolutional architecture is constructed using a stack of such searched architectures. They considered the number of initial convolutions and motif repetitions as free parameters to solve the problem of scaling from a small dataset (CIFAR10) to a larger dataset (ImageNet). Although this research improved state-of-the-art image recognition performance by 1.2% and reduced the number of best model parameters by 28%, it was only the beginning of more exciting research in this area.",
      "page_start": 41,
      "page_end": 41
    },
    {
      "section_name": "Efficientnets",
      "text": "Tan and Le made the next breakthrough in the search for a neural architecture with a model that achieved the same performance as state of the art in image recognition, importantly it was 8.4× smaller and 6.1× faster  [264]  than competitor models. Their research focused on two directions: 1) improving architectural search and 2) introducing a compound scaling technique. As in their previous research on developing mobile neural architectures for searched networks (Mnas-Net  [263] ), Tan and Le used a reinforcement learning (RL) based method to optimize their objective function. Their objective function is to find a Paretooptimal mobile network called EfficientNet. It includes two components: 1) maximizing the accuracy of the network, similar to MnasNets (mobile NasNets), and 2) minimizing the number of FLOPs (required floating point computations) instead of latency, which is considered in MnasNets. Inspired by the architecture of mobile networks (MobileNet and MobileNetV2  [104, 225] ), the authors used the mobile inverted bottleneck (MBCon  [225] ) as the building block of Efficinet-Net. This work's second breakthrough was introducing a compound approach to scaling neural architectures, while maintaining a balance between their height, width, and depth. Their scaling method demonstrates improvements in scaling EfficientNets, MobileNets, and ResNets.",
      "page_start": 40,
      "page_end": 40
    },
    {
      "section_name": "3D Convolutional Neural Networks",
      "text": "Researchers have extended the idea of two-dimensional convolutions to threedimensional spaces where data samples span multiple images (slices) per input instance, such as in videos and volumetric medical images  [185] . Although the goal of video processing and 3D medical image processing is different, both can utilize 3D convolutional neural networks with the same architectures as in Figure  2 .7. 3D convolutional neural networks (3D-CNNs) aim to find temporal dependencies in video processing  [110]  and 3D spatial dependencies in medical imaging and point clouds  [317] . As an extension of 2D filters, 3D filters have one dimension higher -a size of 3 × 3 × 3 voxels 2  is a common choice -to find spatial or temporal information in (3D) volumetric data. The feature responses of 3D filters are computed similarly by finding the correlation between the filter and a particular spatial position of the data volume. Applying a single 3D filter to a data volume results in a 3D feature map calculation. Stacking the feature maps of multiple 3D filters results in a four-dimensional feature map at each layer of a 3D model. The pooling operation is extended to find a volume's average or maximum value with the typical size of 3 × 3 × 3. Similar to techniques used with 2D-CNNs, such as flattening and global spatial pooling, the feature maps of the last layer can be converted into embeddings for classification. The high dimensionality of the feature maps of 3D-CNNs makes their implementation very memory intensive, and processing the 3D inputs increases the computational complexity of the 3D-CNNs. However, researchers have recently explored 3D-CNNs for medical applications as the memory limits of modern GPUs have increased significantly. It seems that 3D CNNs will receive more attention in the future as computational resources continue to develop.",
      "page_start": 41,
      "page_end": 42
    },
    {
      "section_name": "Vision Transformers",
      "text": "Vision transformers (ViTs) for computer vision have emerged through the adaption of the self-attention mechanism developed in the field of natural language processing (NLP)  [279] . Researchers have searched for efficient attention mechanisms to optimally focus on the most relevant information to recognize patterns from different information sources. However, researchers in the field of NLP have only recently discovered a practical and efficient implementation of attention. The use of attention in NLP was so successful that the models developed in several NLP applications quickly outperformed the state-of-the-art  [60, 158] . The core of these recent breakthroughs in NLP promptly found its way to image processing applications. The remainder of this section aims to review ViTs and explain the main components of these models used in computer vision. This section lays the theoretical foundation for the ViTs used in the following chapters of this thesis.",
      "page_start": 42,
      "page_end": 42
    },
    {
      "section_name": "Preliminaries",
      "text": "The theoretical background of ViTs and attention mechanisms is grounded in machine translation. A brief explanation of the basic concepts is necessary to understand the rest of this section. Let us use a simple example from daily life to explain these concepts. Imagine that we make a text query to find a relevant research paper in a search engine. The search engine evaluates the query based on several keys that summarize the titles of the available papers and return the most relevant papers (values)  3  . Upon receiving a query, the search engine may use a tokenizer to segment the query sentence or break it into multiple tokens (words or punctuations). The model maps the tokens to their token IDs based on a particular tokenizer and pads it with zeros up to a certain length to form the embedding vectors of queries, keys, and values.",
      "page_start": 42,
      "page_end": 42
    },
    {
      "section_name": "Dosovitskiy Et Al. Introduced Information Conversion Into Tokens In Image Pro-",
      "text": "cessing for the first time  [65] . Based on this definition, commonly used in recent studies, they divided the input images into smaller patches of size 16 × 16 with three color channels. A random projection of the vectorized shape of these image patches is computed, and then the tokens that form the input of the vision transformer are generated. After tokenizing the information from any source, including images or text, the model focuses on the most relevant information in a query and relates them with a key to find the most appropriate values.",
      "page_start": 43,
      "page_end": 43
    },
    {
      "section_name": "Attention",
      "text": "The concept of attention, as first described in  [21] , is nothing more complicated than a weighted average of values (h) defined as follows:\n\nwhere j α j = 1. α j = 1 corresponds to the importance of each element in the vector h.\n\nAttention has been the key component in training outstanding models in NLP, such as BERT and RoBERTa  [60, 158] , through the use of keys, queries, and values from different sources in a supervised scenario. In addition, the attention mechanism is also used in self-supervised training of language models to predict missing information in training models such as GPT  [32] . Attention can estimate dependencies between two sequences and can be extended to self-attention (SA) for modeling dependencies within a text sequence. Self-attention techniques are commonly used in image processing to find local correlations between tokens computed from the same image.",
      "page_start": 44,
      "page_end": 44
    },
    {
      "section_name": "Self-Attention",
      "text": "The following steps describe how to compute the self-attention (SA) layer's output for an image ((X) ∈ IR N×T ) converted into N projected patches (tokens): 1) Calculate the projections of all tokens based on three different matrices to compute the keys, queries, and values based on all tokenized image patches. 2) Compute the attention matrix by multiplying keys and queries and normalizing the results using the softmax operation, which is defined for a vector x as follows: sof tmax(x) = exp(x i ) j exp(x j ) . 3) Multiply the attention matrix by the values to calculate the SA matrix. In practice, ViTs consist of a stack of several such SA layers, which provide the opportunity to compute the dependencies of each pixel to every other one and combine the correlations based on their importance using the attention matrix.",
      "page_start": 43,
      "page_end": 43
    },
    {
      "section_name": "Postional Encoding",
      "text": "SA layers, as described, are an effective tool for finding correlations between individual pixels. However, after converting an image to patches and computing the tokens, the SA layer is invariant to the order of the input tokens. In other words, the SA layer is independent of the order of the patches in the input images and ignores the order of the tokens in the input data. To address this deficiency of the transformers, researchers added an (absolute) positional encoding that considers the order of the input tokens in NLP models and the image patches in the ViTs. The vector of positional encoding contains additive information proportional to the absolute position of words in a phrase or patches in an image.",
      "page_start": 43,
      "page_end": 44
    },
    {
      "section_name": "Relative Postional Encoding",
      "text": "Absolute positional encoding in transformers retains the spatial information of a single patch, but fails to account for the relative distances between various patches. Shaw et al. used relative positional coding to address this shortcoming of self-attention in ViTs  [239] . First, relative positional coding computes a distance function between image patches. Then, it applies a function based on these distances to the attention matrix, instead of absolute positional encoding, which adds the positional encoding to the input tokens.  [65] , three years after Vaswani et al. introduced the attention mechanism  [279] . The architecture of their vision transformer, as shown in Figure  2 .8, consists of a transformer encoder (backbone) with a multilayer perceptron (MLP) head for classification. The input images used for image recognition are divided into patches of size 16 × 16. Then, each patch is converted (flattened) into a vector, and a linear projection is applied to compute the input tokens for the transformer architecture. The backbone of the transformer contains multiple layers of multi-head attention  4  . Positional embeddings corresponding to the position of the patches in the original image are added to the computed tokens. A stack of multiple layers of multihead attention computes a deep representation of the input images. These latent representations of the input images are passed to the classifier to classify them into distinct categories, such as dog, cat, and car. The possibility of computing the correlation between each pixel in the input images via an attention matrix makes ViTs more powerful than CNNs for a given dataset; nevertheless, ViTs are prone to overfitting. However, the higher learning capacity of ViTs provides the opportunity to use more data for training. The larger version of ImageNet with more than 21, 000 classes (ImageNet-21k) is useful for pre-training ViTs (usually, optimal performance of pre-trained CNN models is achieved with ImageNet-1k. Additional data was not as helpful as in the case of ViTs).",
      "page_start": 44,
      "page_end": 44
    },
    {
      "section_name": "Vision Transformers For Classification",
      "text": "",
      "page_start": 44,
      "page_end": 44
    },
    {
      "section_name": "Dosovitskiy Et Al. Trained The First Vision Of Vits On Imagenet",
      "text": "",
      "page_start": 44,
      "page_end": 44
    },
    {
      "section_name": "Vision Transformers For Segmentation",
      "text": "ViTs can be extended from classification to semantic segmentation using similar encoding backbones. Figure  2 .9 shows how pre-trained ViTs for classification can serve as the first building block of ViTs for semantic segmentation. The main difference between classification and segmentation ViTs starts after encoding the images into patch embeddings (z L ∈ R N ×D with N patches and tokens of D dimensions). Classifiers then predict a vector with elements that sum to one, with the values being proportional to the probability of the predicted class. The segmenter ViTs, on the other hand, approximate a segmentation map s ∈ R H×W ×K that represents the segmentation predictions for each pixel of K classes in an image of a given height (H) and width (W ). Two additional components of randomly initialized class embeddings and a mask transformer support the adaptation of ViT architecture to compute the segmentation map. After calculating the patch embeddings of the input images using pre-trained classification ViTs, the patch embeddings are concatenated with class embeddings (([cls 1 , ..., cls k ] ∈ R K×D )). The mask transformer includes several multi-headed self-attention layers where each class embedding attends each patch's pixel. At the end of the mask transformer, the normalized patch embeddings z ′ L ∈ R N ×D and the class embeddings are separated, normalized based on their ℓ 2 , and a scalar dot product of each class embedding and patch embedding is computed to create a mask for each class. To predict the final image masks, the segmentation model includes an argmax function to find the most likely class per pixel and reduce the predictions to the same size as the input image.",
      "page_start": 45,
      "page_end": 46
    },
    {
      "section_name": "Optimizing Neural Networks",
      "text": "So far, this chapter has explained several aspects of different models for image processing. This section describes how these models are optimized to fit a given dataset and find patterns in the images within a dataset. The goal of the optimization process is tunning the trainable parameters of the models (θ) for an objective function (L) such that the model can generalize to unseen data.",
      "page_start": 46,
      "page_end": 46
    },
    {
      "section_name": "Optimizing Trainable Parameters",
      "text": "The optimization's objective function, the so-called loss function, reflects the dataset's target task. For example, a classifier has a loss function that provides the highest probability for the presence of the correct class in an input image. Likewise, the segmenter computes the highest probability for the object surrounding a single pixel. The goal of the optimization algorithms is to minimize the expected value (E) of a loss function over the entire training dataset (p data ) as follows  [88] :\n\nwhere x and y denote the pair of data samples and ground truth labels. Although the training process of ML and DL models uses the training data (p data ) for optimization, the main goal is to find a model that fits the data distribution (p data ):\n\nwhere J(θ) * is the expected value of the error over the data distribution (not just the training set). The main difference between ML and DL optimization and classical problems is that the loss function depends on the training data.\n\nThe search for optimal parameters for an ML (θ M L ) for a maximum likelihood problem can be described as follows:\n\nMaximizing the likelihood of predictions with ground truth labels is equivalent to minimizing the prediction error. For discrete pairs of data samples and labels (x and y), generalization is expressed as follows:\n\nThe gradient of the loss function (g) is calculated for all parameters using training data in practice to find the optimal model's parameters:\n\nTheoretically, to use a gradient descent algorithm to optimize the neural networks based on the gradients of the parameters with respect to the loss function, we need to compute the average of gradients over the entire training dataset before updating the parameters. However, this method is computationally very expensive, and the optimization algorithm converges faster when multiple updates are made from subsets of the training dataset (mini-batches). Therefore, the gradient of the parameters with respect to the loss function is calculated for a mini-batch with a size of m samples:\n\nLarger mini-batch sizes result in cleaner gradients toward the minima of the objective function, and gradients computed with smaller batch sizes are noisier.\n\nHowever, this gradient noise can regularize the training process and improve the models' generalization. The use of mini-batches became a practical optimization approach due to their advantages in generalization and convergence speed. Neural networks and deep vision models consist of many layers with a depth of more than a hundred. The algorithm for computing the gradient of parameters for all layers is based on the chain rule, which is called backpropagation. After calculating the loss function at the end of the models' computational graph, its partial derivatives are calculated with respect to the trainable variables of the models' last layer. These gradients are backpropagated toward input images to compute the gradients of all parameters minimizing the loss function. Then, the gradients are multiplied by a learning rate, and the parameters are updated based on these multiplications. The iterative training process continues until a stopping criterion is met.",
      "page_start": 47,
      "page_end": 47
    },
    {
      "section_name": "Optimization To Generalization",
      "text": "Training neural networks and computer vision models, as described in Section 2.3.1, focuses on minimizing the prediction error on the training set. However, the main goal of training is to optimize models that generalize well to the entire data distribution outside the training set. The art of bringing computer vision models to optimal performance involves many techniques and a lot of empirical trial and error. The main goal of these techniques is to limit the networks' capacity or artificially increase the amount of data by presenting different variations of the original dataset, which forces the model to learn more generic patterns instead of memorizing individual images from the training dataset. The remainder of this section describes some well-known methods for improving generalization after optimization.",
      "page_start": 48,
      "page_end": 48
    },
    {
      "section_name": "Dropout",
      "text": "There are several reasons for poor generalization (overfitting) in deep neural networks described in the literature. These reasons include neurons' coadaptation to a particular image with poor generalization and learning dense representations of the input images. Srivastava et al.  [248]  proposed dropout as an effective technique to improve the neural network's generalization to address the above reasons for overfitting. Dropout is equivalent to randomly setting the activations of a layer for a given input image to zero with a certain probability during training. The random suppression of activations via dropout prevents the model from coadapting neurons. Furthermore, dropout leads to learning sparse representations of input images and consequently improving the generalization. Although the usage of dropout in the classifiers (fully connected layers or MLPs) is more common, it is possible to use dropout in convolutional layers as well. Srivastava et al.  [248]  originally introduced dropout to reduce overfitting during training. Nonetheless, Gal and Ghahramani proposed a framework for estimating neural network uncertainty using dropout in test time as an additional application of dropout  [81] . In their theoretical framework, dropout in neural networks has been successfully used for Bayesian inference in Gaussian processes to estimate uncertainty.",
      "page_start": 48,
      "page_end": 48
    },
    {
      "section_name": "Regularization",
      "text": "The other technique to limit the neural networks' capacity is adding a penalty to the neural network loss function that increases with the absolute value of the trainable weights. Researchers used different functions of the trainable weights as additive penalties to the original (classification or segmentation) loss function. Tibshirani and Zheng used the ℓ 1 -norm of the weights to calculate such a penalty  [267, 323] . The ℓ 1 -norm regularization, also called Lasso regression regularization, keeps the sum of the absolute values of the trainable parameters small. Lasso regularization leads to sparse weight vectors by setting some weights to zero. An alternative to Lasso regularization is to compute the penalty term using the square of the weights (ℓ 2 -norm), which leads to Ridge regression regularization  [192] . The computed regularization penalty (loss) is multiplied by a regularization factor and then added to the loss value of the training. Loshchilov and Hutter have shown that decoupling the regularization penalty from the classification loss by defining an independent weight decay from the learning rate for adaptive gradient algorithms improves the generalization performance  [162] .",
      "page_start": 49,
      "page_end": 49
    },
    {
      "section_name": "Augmentation",
      "text": "Besides dropping neurons from the neural network architecture and regularizing the weights, neural network generalization improves by presenting the models with different variations of the input data. Since the early days of deep learning research, studies have shown that CNNs have limited robustness to rotation and scaling  [148]  5 . However, researchers quickly found that computer vision models can recover such weaknesses by presenting variations of input images to the network during training time. Thus, computer vision models trained with rotated versions of the input images are robust against rotations. Augmentation is the term proposed in the literature for training computer vision models with transformed images. Since then, researchers have used various strategies to augment their input images depending on the application. Techniques for augmentation such as shearing, translation, rotation, rectification, and changing contrast, color, brightness, and sharpness are broadly used in the computer vision research com-munity  [42, 228, 242, 283, 140] .\n\nSimilar to the architectural design literature explained in section 2.1.7, augmentation methods have also evolved towards automation. An important work of research on this subject resulted in AutoAugment  [50] , which presents optimal strategies that are automatically checked against one of the largest computer vision datasets (ImageNet  [58] ). Further improvement in the speed of such a resource-exhaustive search led to the development of Fast AutoAugment, an algorithm that is lighter in terms of computational complexity and more suitable for exploring optimal augmentation strategies on private datasets  [156] .",
      "page_start": 49,
      "page_end": 50
    },
    {
      "section_name": "Related Work",
      "text": "Two critical components of the rapid increase in computational resources and datasets' size have revived machine and deep learning (ML and DL) techniques in practical applications for image pattern classification  [147] . Initially, raw pixel values for simple tasks such as classifying handwritten digits were sufficient to train neural networks and support vector machines for pattern classification  [46] . However, the urge to extract robust features for more complex computer vision problems led researchers to develop advanced methods for representation learning  [164] .\n\nML pipelines became increasingly complicated in the first decade of the twentieth century as problem-oriented feature extraction techniques grew rapidly  [142] . Classical computer vision researchers, who moved away from Fourier transforms and brought image-based prior knowledge to multiscale wavelet transform, began training sparse dictionaries from data  [221] , and robust hand-crafted features  [163]  for pattern recognition received enormous attention. Moreover, the growth of datasets quickly saturated the performance of classical ML models, and task diversity made the search space for the best priors exhaustive. DL appeared as the next breakthrough to increase the capacity of models for massive datasets and automate feature extraction and representation learning in a wide range of different tasks  [146] .\n\nThe first DL milestone in computer vision was reviving the convolutional neural networks (CNNs) for pattern recognition  [147] . CNNs introduced in the late 1980s finally found their way into practice by overcoming their high computational complexity. LeNet5  [148]  is one of the first CNN models applied to handwritten digit classification algorithms, and AlexNet  [80]  is among the first successes of deep CNNs in image classification on large datasets. The second half of the 2010s is the most prosperous time in the history of DL and CNNs in computer vision with a lot of exciting research and developments regarding various architectures  [243, 261, 225, 263]  and optimization techniques  [216, 129, 135, 162] .\n\nResearchers trained CNNs using neural architectures, search became fashionable, optimization methods evolved based on large datasets such as ImageNet  [223] , and this distilled knowledge was successfully applied to smaller datasets using transfer learning  [197] .\n\nAfter CNNs matured in image classification  [24]  and segmentation  [145] , the next generation of research focused on automatic neural architecture search  [69]  and finding optimal search spaces for efficient networks with minimal delay  [263]  and computational power consumption for mobile applications  [264] . However, the parallel increase in computational resources and the presence of massive datasets, motivated by data-driven artificial intelligence (AI) research, created the opportunity for the next breakthrough in computer vision. The next breakthrough occurred in the early 2020s by introducing vision transformers (ViTs)  [65] , and adapting self-attention, originally discovered in natural language processing (NLP) literature  [279] , for computer vision tasks. ViTs are widely used for image classification and segmentation, and these models have improved their performance with a larger version of the ImageNet dataset called ImageNet-21k with over 21, 000 classes  [213] .\n\nResearchers have expressed doubts and concerns about the robustness of computer vision models using CNNs  [89]  and their explainability  [1]  from their inception. CNNs lack some basic properties of classical methods, such as rotation equivariance  6  , despite their capability to learn translational equivariant features  [148] . Due to the lack of rotation equivariance, the performance of CNNs decreases when the input images rotate  [120] . Similar instabilities and inaccuracies have been reported due to changes in lighting conditions, contrast, image acquisition techniques, and overall data distribution drift  [62] . Studies even show that CNNs focus more on the texture than the shapes when classifying objects  [84] . Researchers also discovered that they could compute minimal perturbations, called adversarial attacks, for an input image to fool CNN models with images that are indistinguishable from one another to the human eye  [89] . They even optimized a so-called universal adversarial attack that generalizes to many images  [187] . Among all of the challenges mentioned in computer vision research related to robustness, this work presents a solution for rotational invariance in ViTs and adversarial attack detection in CNNs  [15] .\n\nCNNs were known as powerful black-box models following a similar trend to many other ML and DL-based techniques in information processing  [33] . These models can be used in many applications without additional reasoning; however, understanding these high-precision decisions is critical for applications that affect human safety and health, such as autonomous driving systems  [20]  or healthcare  [6] .\n\nThe literature that has developed around explainable AI (XAI)  [268]  and inter-pretation of computer vision models  [318]  is the result of researchers' concerns about the usage of black-box models in critical applications. Researchers have two main approaches to the interpretability and explainability of neural networks. The first group analyzes the trained models to understand the predictions using post-processing and post-hoc techniques  [47] . Another group disagrees with the idea of interpretability solely as an add-on for neural networks. Instead, these researchers point to changing the design of the neural architectures so that the decisions are transparent and explainable  [175] . Concerning the issue of explainability, this thesis proposes using radial basis neural networks as classifiers on top of the CNNs to provide more understandable information to humans about the decision-making of the models  [14] .\n\nDespite all the challenges mentioned above, computer vision breakthroughs have found their way into numerous applications  [5] . CNNs have outperformed all other methods in the majority of applications, such as object detection, recognition, and segmentation  [24, 145] . Furthermore, CNNs perform well in other classical image processing tasks such as image denoising  [63] , super-resolution  [308] , and motion deblurring  [256] . Moreover, the applications of CNNs extend not only to medical imaging for diagnosis  [227]  and automatic segmentation  [290] , but also to image quality enhancement and motion artifact reduction  [157] . In addition to all the above theoretical contributions, this paper presents practical applications of ML and DL, especially in medical imaging, and demonstrates a variety of empirical findings  [251] .",
      "page_start": 50,
      "page_end": 50
    },
    {
      "section_name": "Rbf Classifiers For Explainable Computer Vision Using Cnns",
      "text": "Radial basis function neural networks (RBFs) are prime candidates for pattern classification and regression and have been used extensively in classical machine learning applications. However, RBFs have not been integrated into contemporary deep learning research, and computer vision has continued using conventional convolutional neural networks (CNNs) because of technical difficulties. This chapter presents the techniques to adapt RBF networks as a classifier on top of CNNs by modifying the training process and introducing a new activation function to train modern vision architectures end-to-end for image classification. The specific architecture of RBFs enables them to learn a similarity distance metric to compare and categorize similar and dissimilar images. Furthermore, this chapter demonstrates that using an RBF classifier on top of any CNN architecture provides new human-interpretable insights about the decision-making process of the vision models. Finally, RBFs are successfully applied to a range of CNN architectures, and their performance on benchmark computer vision datasets is presented in this chapter. This chapter is adopted from the research published in  [14] , licensed under CC BY 4.0 1  . Inspired by the locally tuned response of biological neurons, Broomhead and Lowe introduced radial basis function neural networks (RBFs) in 1988  [31] . The modeling concept behind RBFs is a combination of unsupervised and supervised learning for pattern classification and regression. However, due to structural deficiencies, RBFs have not been integrated into contemporary research in computer vision using Convolutional Neural Networks (CNNs). This chapter presents developments in a new area of research and lays the foundation for using RBFs in deep learning and computer vision by modifying their architecture and learning process. The results demonstrate that integrating RBFs into CNN models for computer vision provides a similarity distance metric and an interpretable decision-making process.",
      "page_start": 53,
      "page_end": 54
    },
    {
      "section_name": "Introduction",
      "text": "This chapter is motivated by RBF architectures' unique opportunities when used with CNN models because of their explainability and robustness compared to linear classifiers. The new training process introduced for RBFs in this chapter provides the opportunity to use labeled and unlabeled data by optimizing two loss functions combining supervised and unsupervised learning. Moreover, the training process of RBF architectures includes optimizing a distance metric that serves as a similarity distance metric to find similar and dissimilar images. Additionally, this chapter proposes visualization techniques to illustrate the clusters and activations with training and test images to gain more insights about the reason behind the decisions made by the networks, thus improving interpretability. The contributions of this chapter to computer vision literature can be summarized as follows:\n\n• Combining supervised and unsupervised learning.\n\n• Learning a similarity distance metric to find similar images.\n\n• Improving the interpretability of decision-making.\n\nDespite the advantages of combining RBFs with modern CNN architectures, two factors in the architecture and training process of RBFs hinder their integration into CNNs. First, the nonlinear activations and computational graphs of RBFs used in the literature prevent efficient gradient flow. Secondly, RBFs assume that the training features are fixed, so the cluster centers are initialized accordingly. Nonetheless, CNN architecture dynamically learns the embeddings used as input features of RBFs. This chapter tackles the limitations of the original RBFs and presents the following contributions to RBF literature:\n\n• Introducing a quadratic activation function and a linear computational graph for end-to-end learning.\n\n• Adding an unsupervised loss term to update the cluster centers in the training process with the learned embeddings.\n\n• Applying the RBFs to computer vision in a first attempt at using deep CNN architectures.\n\nThe remainder of the chapter covers the related work in Section 3.2 followed by the theoretical background of RBFs in Section 3.3. Then, Section 3.4 presents the original research and contributions with the proposed modifications to RBFs, followed by a visual explanation of the new proposed training and decision-making process in Section 3.6.1. The experimental results of applying the proposed RBF-CNN architectures using a range of CNN backbones on benchmark datasets are presented in Section 3.5. The potential contributions of the proposed similarity distance metric on computer vision to enhance the transparency of the decisionmaking process is demonstrated in Section 3.6.2. This chapter concludes with discussions and conclusions in Section 3.7.",
      "page_start": 55,
      "page_end": 55
    },
    {
      "section_name": "Related Work",
      "text": "The research followed two approaches to optimize RBF architectures. The first approach concentrates on the training process and initialization of the networks, while the second aims to find superior activation functions. This chapter presents improvements in both research directions to integrate the RBFs into contemporary computer vision models using CNNs.\n\nRBFs were originally introduced as supervised models for classification and regression tasks. Broomhead and Lowe initially proposed drawing the cluster centers either from a uniform distribution or randomly from the training samples and then optimizing the output weights using a pseudo-inverse analytic solution  [31] .\n\nInitializing the cluster centers randomly and only training the output weights is called a one-phase training process for RBFs. Two-phase training for RBFs uses various methods to initialize the cluster centers before optimizing the output weights. Research since 1988 has used supervised and unsupervised methods to initialize the cluster centers. Moody and Darken proposed an unsupervised algorithm to initialize these cluster centers  [186] , while Schwenker et al. proposed supervised vector quantization  [236] . Decision trees were used to find centers independently by  [141]  and  [234]  before training the output weights. Finally, Schwenker et al. proposed the third phase to optimize the entire RBF network end-to-end, including output weights, the cluster center, and trainable parameters of activation functions using gradient descent  [235] .\n\nThese methods for cluster center initialization assume a fixed feature space for the input layer. However, CNNs learn the embeddings automatically and develop the feature space of the images during the training process. Therefore, this research suggests optimizing an unsupervised learning loss during the training to cope with this change in the feature space. This work differs from previous research as it combines supervised and unsupervised learning by optimizing two separate losses simultaneously using gradient descent.\n\nThe technical requirements of new applications and implementations have motivated the use of several activation functions presented in the literature of RBFs  [67] .\n\nThe Gaussian function is the kernel developed by modeling the data through a multivariate Gaussian distribution  [31] . Other functions adapted in the RBF architecture include linear kernels, thin-plate splines, logistic functions, and multiquadratic functions  [79, 208, 155, 40]  RBFs can be applied to computer vision tasks and image classification as well. Schwenker et al. used raw images as feature vectors to classify hand-written digits  [235] . Er et al. extracted the features from facial images using principal component analysis (PCA) and processed these features using Fisher's linear discriminant (FLD) technique before classifying the faces using RBFs  [182] . However, the successful rise of modern CNNs, such as LeNet-5  [148]  and AlexNet  [224] , led to a paradigm shift from using hand-crafted features to automated deep CNNbased feature and representation learning. In recent years, most computer vision tasks, like facial recognition  [177] , are dominated by modern CNN architectures as they present superior performance compared to classical methods for image processing. To the best of our knowledge, this chapter presents the first attempt to integrate RBFs into modern CNN architectures for computer vision.\n\nThis chapter relates to literature focusing on deep metric learning since RBFs automatically optimize a similarity distance metric during training based on their architecture. Euclidean distance, Mahalanobis distance, and cosine similarity have been used to evaluate the similarity between the embeddings (the features extracted from CNNs) of two images in the literature  [101, 304, 300] . Researchers have applied different strategies and loss functions to optimize these similarity metrics for same-class images while also maximizing the distance of differentclass images. The research in this area concentrates on the training process and the design of a loss function which brings similar images closer in the embedding space based on a similarity measure. Hu et al. proposed minimizing the inter-class scores and maximizing the intra-class scores based on Euclidian distances  [106] .\n\nHoffer and Ailon suggested optimizing a similarity-based loss function defined for selected triplets of images  [101] . Song et al. used the pairwise distances between images of an entire batch and proposed a structured loss function for metric learning  [246] . Similar research work has aimed at optimizing angular distance, cosine distance, and large-margin Euclidean distance of similar and dissimilar images  [287, 300, 41] .\n\nThis chapter presents a method to retrieve a ranked list of similar and dissimilar images, leading to visually appealing similarity metric learning results. However, the proposed similarity metric learned by the RBFs does not require any complicated triplet sample section or loss design. Instead, these results have been obtained using a typical supervised loss function for classification (softmax cross-entropy). Furthermore, RBFs can not only optimize for Euclidean and Mahalanobis distances but also for the entire covariance matrix.",
      "page_start": 56,
      "page_end": 58
    },
    {
      "section_name": "Radial Basis Function Networks",
      "text": "This section briefly reviews and explains the theoretical foundation of radial basis function networks. RBFs are presented in the literature as a global approximation method for learning a mapping F from a given feature space with the dimensionality of d to a label space with K dimensions (F : IR d → IR K )  [31] . In this chapter, the function F of features x approximates the one-hot encoded labels y.\n\nThe features used to train the RBFs in this chapter are the embeddings of deep CNNs, which are used to predict the class labels using end-to-end optimization.\n\nA fully connected layer connects the CNN architectures and RBFs to provide compatibility between the two architectures. The architecture of the RBF consists of input layers, a single trainable hidden layer with C cluster centers (c j ) 3.1, and an output layer.\n\nDuring the evaluation, also known as inference in deep learning, the RBF computes a distance between embeddings of CNNs and the cluster centers and applies an activation function to this distance. The network outputs are then computed by multiplying the output layer weights with the activation values. This forward path of RBFs is formally defined as:\n\nwhere r represents the distance, R j is the positive definite covariance matrix (trainable distance metric), T denotes the matrix transposition, w jk shows the weights of the output layers, h is the activation function, and w 0k are the biases. In these Equations, µ, j, and k enumerate the number of samples, cluster centers, and classes. Trainable parameters in Equation  3 .1 and 3.2 are the output weights, cluster centers, and covariance matrix.\n\nOptimizing the RBF networks with an identity covariance matrix is equivalent to training in Euclidean space. It is possible to optimize a Mahalanobis distance  [53]  by training the main diagonal on the covariance matrix. Any arbitrary distance metric can be trained by optimizing the entire covariance matrix, while projecting the matrix to the space of positive definite matrices. The distance, r, computed in Equation 3.1 is not only a measure of the proximity of an image to a cluster center; it can also be used to compare images and find similar and dissimilar images in the embedding space.\n\nThe linear and nonlinear activation functions used in RBFs are as follows  [208, 155, 40] :\n\nIn addition to the standard machine learning activation kernels in Equations 3.3 to 3.6, the kernel presented in Equation 3.7 is derived from the generalized Hardy's multiquadratic function  [79] . Du et al.  [67]  proposed the kernel in Equation  3 .9 because of its convenience for implementation on DSP units. Various activation functions for RBFs are depicted in Figure  3 .2.\n\nThe complete process of training RBFs was introduced by Schwenker et al.  [235]  as a three-phase process:\n\nUnsupervised learning: This step aims to find cluster centers that are representative of the given dataset. The k-means  [17]  clustering algorithm is widely used for this purpose. K-means iteratively finds a set of cluster centers and minimizes the overall distance between cluster centers and members over the entire dataset. The target of the k-means algorithm can be written in the following form:\n\nwhere x µ ∈ ϑ j denotes the members of the j th cluster shown by ϑ j .\n\nComputing weights: The output weights of an RBF network can be computed using a closed-form solution. The matrix of activation of the samples is defined from the training set (H) as follows:\n\nBased on Equation 3.2, the matrix of output weights (W ), which estimates the matrix of labels (Y ), is computed using the following equation:\n\nwhere H † is the Moore-Penrose pseudo-inverse matrix  [206]  of H and is computed as:\n\nEnd-to-end optimization: After initializing the RBF weights and cluster centers with clustering algorithms such as k-means, it is possible to optimize the network end-to-end via backpropagation and gradient descent. Schwenker et al. computed the gradients of the loss function for a Gaussian activation function in  [235] .",
      "page_start": 58,
      "page_end": 58
    },
    {
      "section_name": "Adapting Rbfs For Cnns",
      "text": "This section presents the adaptation steps for using RBF classifiers for CNNs as depicted in Figure  3 .1. The deep embeddings of the CNNs, computed using standard convolutional layers and inception blocks, are flattened and fed to RBFs after a fully connected layer in the architecture. The network ends in an output layer with softmax activation and is optimized end-to-end. Integrating the RBFs into deep structures and using them in conjunction with CNNs presents three challenges:\n\nInitialization: Training the RBFs from scratch with randomly initialized weights using gradient descent is quite inefficient due to inconvenient initial cluster centers. The large initial distances in high dimensional spaces lead to small activation values, and the gradients attenuate considerably after the RBF hidden layer during backpropagation. Therefore, the k-means algorithm initializes the cluster centers before starting the training. Furthermore, computing the weights from Equation 3.12 is not feasible due to the scale of computer vision datasets such as ImageNet  [58] , which has over 14 million images and 1000 classes. Hence, using gradient descent and optimizing randomly initialized output layer weight is the optimal way to proceed.",
      "page_start": 61,
      "page_end": 61
    },
    {
      "section_name": "Dynamic Input Features:",
      "text": "The input features of classical RBFs are fixed, but this assumption is not valid with respect to CNNs. As the embeddings of CNNs develop during the training process, the cluster centers initialized by the k-means algorithm are no longer optimal after a few epochs of training. This research work proposes the optimization of the k-means algorithm's target with unsupervised loss during the training process as defined in Equation  3 .10.",
      "page_start": 61,
      "page_end": 61
    },
    {
      "section_name": "Activation:",
      "text": "The nonlinear computational graph drawn by computing the distance in Equation 3.1 and applying the activations in equations 3.3-3.9 leads to inefficient gradient flow. Therefore, this research attempts to build a linear computational graph in RBFs through the introduction of a new activation function.\n\nThis section presents two modifications to classical RBFs to make them suitable for deep CNNs. First, it introduces an additional loss term to the RBFs' hidden layer. This term is based on the target function of the k-means algorithm defined in Equation  3 .10 and continues in the unsupervised learning process during the development of the embeddings. The second contribution of this section is the introduction of a new quadratic kernel to build a linear computational graph for efficient optimization using backpropagation.",
      "page_start": 61,
      "page_end": 61
    },
    {
      "section_name": "Introducing Unsupervised Learning Loss",
      "text": "The embeddings of CNNs change during the training process, which necessitates updating the cluster centers with an unsupervised loss. Therefore, introducing an additional term to the RBFs' supervised loss function to optimize the cluster centers during training using the k-means unsupervised loss in Equation 3.10 is crucial for optimizing CNNs with an RBFs classifier (CNN-RBFs) end-to-end.\n\nThe final loss of a CNN-RBF network is computed as follows:\n\nLoss rbf = Loss supervised + λLoss unsupervised  (3.14)  where the classification loss Loss classification is any arbitrary loss function, for instance categorical cross-entropy.\n\nIt is conventional to use clustering algorithms such as the k-means or expectationmaximization (EM) algorithms to initialize the cluster centers. The loss function in Equation 3.14 is optimized using gradient descent by minimizing the distance of the embeddings for each sample from its nearest cluster center regardless of the class labels. The distance from the nearest cluster center is computed using the distance metric R j defined in Equation 3.1.",
      "page_start": 61,
      "page_end": 61
    },
    {
      "section_name": "Quadratic Kernel",
      "text": "The kernels used for classical RBFs are nonlinear and increase the model's complexity. The architectures proposed in Figure  3 .1 profit from using the state-ofthe-art models for representation learning, i.e., CNNs, as a backbone. Therefore, CNN-RBF architectures can be trained with simpler linear models to improve the gradient flow during backpropagation. The proposed quadratic activation function is linear in the space of r 2 and is defined as follows:\n\nwhere σ is the trainable parameter that determines the width of the kernel. The proposed kernel is depicted in Figure  3 .2 alongside the conventional activation functions. The proposed quadratic kernel reduces the nonlinearity of the CNN-RBF computational graph for backpropagation. The squares of the distances between cluster centers and samples are computed by linear matrix multiplication in Equation  3 .1 and the application of the proposed linear activation for r 2 . Thus, the gradients of deep embeddings propagate backward through a distance computation with matrix multiplication and linear activations.",
      "page_start": 62,
      "page_end": 62
    },
    {
      "section_name": "Experimental Results",
      "text": "This section presents the experimental results that reinforce the applicability of RBFs to CNNs on several standard computer vision benchmark datasets and investigates the effect of tweaking various hyperparameters of the CNN-RBF architectures in the training phase and generalization to test data. Three convolutional backbones EfficientNet-B0  [264] , InceptionV2  [261] , and ResNet50  [96]  compute the embedding of CNN-RBFs in this section. A list of the benchmark computer vision datasets used in this section is presented in Table  3    [139]  50 000 10 000 100 Oxford-IIIT Pets  [200]  3 680 3 369 37 Oxford Flowers  [193]  1 020 6 140 102 FGVC Aircraft  [173]  6 667 3 333 100 Caltech Birds  [297]  5 996 5 794 200\n\nTable  3 .1: An overview of computer vision benchmark datasets used to evaluate the performance of CNN-RBFs (table adopted from  [14] ). consists of a set of optimal and automatically discovered augmentation policies for the ImageNet  [58]  dataset. The CNN-RBF architecture demonstrated in Figure  3 .1 has two further hyperparameters: the number of cluster centers and the input dimensions of the RBF network. The models are optimized using an AdamW  [162]  optimizer with different learning rates and weight decay serving as tunable hyperparameters. The other hyperparameters are the loss constant (λ) from Equation  3 .14, dropout rate, and batch size.\n\nThe hyperparameter searches in Figure  3 .3 are conducted using the hyperband  [150]  algorithm with 4 agents running in parallel on two Tesla V100 graphic processing units (GPUs) for approximately 10 days. It should be noted that dropout is only applied after the CNN backbone and before the fully connected layer in Figure  3 .1. The output of the fully connected layer, without any activation function, is used as input features of the RBFs. The results in Figure  3 .3 show that training CNN-RBF architectures leads to high performances, even with a wide range of hyperparameters. However, achieving good test performance with a high dropout rate and a large input dimension is challenging. CNN-RBF architectures show a better performance without dropout and rectified linear unit (ReLU) activations in the input layer of RBFs. Thus, dropout is neglected for further hyperparameter searches conducted on the other datasets in Table  3 .1. The list of optimal hyperparameters for all datasets is presented in Table  3 .2.\n\nSeveral CNN-based backbone architectures are used within the CNN-RBFs to train models for all computer vision datasets. The experimental results of applying the CNN-RBFs to the computer vision benchmark datasets with the standard",
      "page_start": 62,
      "page_end": 63
    },
    {
      "section_name": "Visualization Of The Rbf Classifiers",
      "text": "This section attempts to visually explain the training and test processes of vision models using RBF classifiers. First, it demonstrates the training process for the simple task of classifying hand-written digits. Then, the distribution and training samples of active clusters for a test sample are depicted for more complicated object detection tasks described in the previous section.",
      "page_start": 65,
      "page_end": 65
    },
    {
      "section_name": "Visualization Of The Training Process",
      "text": "This section visualizes the performance of the RBFs classifiers for CNNs on a simple dataset. The experiments are conducted on the modified national institute of standards and technology (MNIST) dataset  [148] , a dataset of hand-written digits including ten classes. Learning the dataset is considered a simple task in computer vision. The simplicity of the dataset and learning task allows for the visualization of the training process at a fine level of detail. CNN-RBF architecture has the same number of cluster centers as classes (ten) in the dataset to depict the training process in this experiment. The network architecture in this section consists of a four-layer CNN, and the output of these layers is connected to the RBF after a global average pooling layer and a fully connected layer. are shown with a number corresponding to their class, and the color is similar for samples of the same class in Figure  3 .4. To reduce the overlap between close samples, a random uniform noise of amplitude 0.1 is added to the distance of the samples from the cluster centers.\n\nMinimizing the unsupervised loss in Equation 3.10 reduces the distance of the data samples from the cluster centers. Furthermore, the supervised loss enforces the samples of the same class to maintain the same distance from cluster centers, as the activations are the only information for the network's final decision. The circles with samples of the same class around the cluster centers demonstrate the effect of supervised loss in training. Notably, the clusters presented in Figure  3 .4 are selected to illustrate the concepts underlying training CNN-RBFs optimally.\n\nFigure  3 .5 illustrates the two-dimensional mapping of the CNN embeddings (top row) and RBF activations (bottom row) using t-SNE  [277] . The effect of both supervised and unsupervised loss from Equation 3.14 is evident in this figure. The data samples split into clusters regardless of their class labels in the embedding space of CNN due to the unsupervised loss (top row in Figure  3 .5). The activation values divide into clusters corresponding to the class labels, a process encouraged by the supervised loss.",
      "page_start": 65,
      "page_end": 66
    },
    {
      "section_name": "Similarity Metric Learning And Interpretability",
      "text": "Using a different approximation strategy compared with fully connected layers provides CNN-RBFs with the chance to probe the decision-making process based on the following visual clues:\n\n• Similar images as measured by the similarity distance metric of RBFs trained on CNN embeddings\n\n• Clusters visualization with a higher contribution to the network's decision and distance of the samples from the centers of these clusters\n\nThe embeddings of the CNN are compared with the position of the cluster centers using the learned distance metric from the RBFs. The learned distance metric of the RBFs can measure the similarity between a test image and similar images from the training data. Figure  3 .6 shows the similar images found in the training dataset for a given test sample as determined by the similarity distance metric in Equation 3.1. The most similar and dissimilar images are computed using the following criteria:\n\nx similar = arg min\n\nx dissimilar = arg max\n\nwhere x test presents the input of RBFs for a given test image, x µ train shows the input vector for training samples, and µ enumerates the training samples from 1 to N . x simliar and x dissimliar represent the most similar and dissimilar images to the given test image (x test ) respectively, and R denotes the positive definite covariance matrix similar to Equation 3.1. The same similarity metrics in Equation 3.16 and Equation 3.17 allow computing a ranked list of similar and dissimilar images for a given test sample.",
      "page_start": 67,
      "page_end": 67
    },
    {
      "section_name": "Test Image",
      "text": "Similar and dissimilar images   3 .1 in the same order (figure adopted from  [14] ). The presented figure visualizes the top 14 images selected using different distance metrics in the embedding space for a given test image (figure adopted from  [14] ).\n\nThe active clusters for every sample provide the reasoning behind the final decision of CNN-RBFs. The clusters can be depicted using the distance of images from their centers. Figure  3 .8 shows training samples and their distances from the cluster centers against a test sample. The product of activations and output weights determines the final decision of the RBFs. Thus, the importance of a cluster for a decision can be determined by sorting the product of activations and class weights.  The brighter the activation value, the larger it is, and the maximum activation at the cluster center is equal to one (figure adopted from  [14] ).",
      "page_start": 68,
      "page_end": 68
    },
    {
      "section_name": "Discussions And Conclusions",
      "text": "This chapter presents fundamental architectural modifications to RBFs for integrating them into CNN architectures for computer vision. The experimental results indicate that integrating RBFs classifiers into CNN architectures achieves competitive performances in benchmark computer vision datasets by combining supervised and unsupervised learning. The proposed activation and training process is compatible with any arbitrary state-of-the-art CNN architecture, including inception blocks and residual connections. The small gap between the CNN-RBFs performance and the best CNN models is a subject for future research to find optimal regularization methods for RBF networks. Using RBF architectures with CNNs introduces two unique and network-specific opportunities for learning a similarity distance metric and interpreting the decision-making process in more detail. The classification of similar and dissimilar images found using a similarity distance metric trained by RBFs is interpretable by humans. The cluster representations are currently only used to trace the decision-making process. In the current research, the distribution of images around clusters is not visually conclusive because they are optimized in an unsupervised manner.",
      "page_start": 70,
      "page_end": 70
    },
    {
      "section_name": "Using Interpretability To Detect Adversarial Attacks For Robust Cnns",
      "text": "The existence of adversarial attacks on convolutional neural networks (CNN) questions the fitness of such models for sensitive and serious practical applications. The adversarial attacks are minimal changes computed for a given input image, provoking a misclassification even though both images appear identical to a human observer; they are, therefore, difficult to detect. In a different context, backpropagated activations of CNNs' hidden layers for a given input image, so-called feature responses, have been helpful for humans to visualize and understand what the CNN looks at while computing its output class. This chapter presents a novel method to detect adversarial examples and identify manipulated images by tracking adversarial perturbations in feature responses. This method is fully human-explainable and allows the automatic detection of adversarial attacks using the average local spatial entropy of feature maps for a given input image without altering the original network architecture. Experiments confirm the validity and functionality of our approach for detecting state-of-the-art attacks on large-scale models trained on ImageNet.\n\nAlongside the contribution of this chapter to the robustness of computer vision models through the detection of adversarial attacks, this chapter presents one of the few applications of explainable artificial intelligence (XAI) for debugging models. Although feature response (maps) were initially intended to open the black box of vision models, they can also be used in practice for debugging, as presented in this chapter. This chapter demonstrates a successful application of vision model interpretability and XAI, which will hopefully inspire future work in the direction of debugging and designing models based on human-explainable methods. This chapter is adopted from the research published in  [15] , licensed under CC BY 4.0 1  .",
      "page_start": 71,
      "page_end": 72
    },
    {
      "section_name": "Introduction",
      "text": "The success of deep neural nets for pattern recognition  [230]  has been one of the primary drivers behind the recent surge of interest in AI. A substantial part of this success is due to the application of convolutional neural networks (CNNs)  [148, 43]  and their descendants on image recognition tasks. Moreover, the respective methods have reached a level of sophistication where they are now being used in business and industry  [251]  and lead to a wide variety of deployed models for critical applications like automated driving  [27]  or biometrics  [325] .\n\nHowever, concerns regarding the reliability of deep neural networks have been raised after the discovery of so-called adversarial examples or attacks  [262] . These examples are specifically generated to fool a CNN into misclassifying visually very similar images or images that appear identical to the human eye with high confidence through the addition of barely visible perturbations  [188]  (see Figure  4 .1). The perturbations are computed using an optimization process on the input: the network weights are fixed, and the input pixels are optimized for the dual target of (a) classifying the input differently than the ground truth class and (b) minimizing the changes to the input. A growing body of literature confirms the importance of this discovery on practical applications of neural networks  [3] . The existence of adversarial attacks provokes questions on how CNNs achieve their performance and in what respect their decision-making differs from humans. In addition, adversarial attacks can threaten serious deployments of CNNs in the applications with the possibility of tailor-made attacks.\n\nFor instance, Su et al.  [255]  report on successfully attacking neural networks by modifying a single pixel. This attack works without having access to the internal structure or the gradients in the network under attack. Moosavi-Dezfooli et al.  [187]  further show the existence of universal adversarial perturbations that can be added to any image to fool a specific model. Furthermore, the impact of similar attacks extends beyond classification  [184] , attacks are transferable to other modalities  [44] , and also work on models distinct from neural networks  [198] . Finally, adversarial attacks have been shown to work reliably even after perturbed images have been printed and captured again via a mobile phone camera  [144] .\n\nApparently, this research area touches on a weak spot concerning the robustness of CNNs in critical applications involving human privacy or security.\n\nOn the other hand, there is a recent interest in the explainability of AI agents, particularly using machine and deep learning models  [280, 195] . It goes hand in hand with societal developments, like the new European legislation on data protection that affects any organization using algorithms on personal data  [90] . While neural networks are publicly perceived as \"black boxes\" concerning how they arrive at their conclusions  [1] , several methods have been developed recently to deliver insight into the representation and decision surface of trained models, improving interpretability. Prime candidates amongst these methods are feature response visualization approaches that provide information regarding operations in hidden layers of a CNN visible  [316, 247, 194] . They can thus serve a human engineer as a diagnostic tool in support of reasoning over the success and failure of a model on the task at hand. This chapter presents a method for using a specific form of CNN feature visualization, namely feature response maps, using guided backpropagation technique  [247] , to not only trace the effect of adversarial attacks but also to detect them. This method traces the attacks on algorithmic decisions throughout CNNs. Moreover, it uses feature response maps as input to a novel automated detection approach based on a statistical analysis of feature maps' average local spatial entropy. The goal is to decide if a model is currently under attack by the given input. The proposed approach has the advantage over existing methods because it does not change the network architecture, i.e., it does not affect the classification accuracy but is explainable to humans. Experiments on the validation set of ImageNet  [223]  with VGG19 networks  [243]  show the validity of our approach for detecting various state-of-the-art attacks.\n\nThe remainder of this chapter is organized as follows: Section 4.2 reviews related work in contrast to our approach. Section 4.3 presents the background on adversarial attacks and feature response estimation before introducing the proposed approach in detail in Section 4.4. Section 4.5 reports on the experimental evaluations, and Section 4.6 concludes the chapter with an outlook on future work.",
      "page_start": 72,
      "page_end": 72
    },
    {
      "section_name": "Related Work",
      "text": "Work on adversarial examples for neural networks is a very active research field. Potential attacks and defenses are published at a high rate and have been surveyed by Akhtar and Mian  [3] . Amongst potential defenses directly comparable to our approach are those that focus solely on detecting a possible attack and not on additionally recovering correct classification.\n\nOn the one hand, several detection approaches exist that exploit specific abnormal behavioral traces that adversarial examples leave while passing through a neural network. Liang et al.  [153]  consider the artificial perturbations as noise in the input and attempt to detect it by quantizing and smoothing image filters. This method used a similar concept, which is the basis of SqueezeNet introduced by Xu et al.  [307] , which compares the network's output on the raw and filtered input, and raises a flag if it detects a large difference between both. Feinman et al.  [74]  observe the network's output confidence as estimated by dropout in the forward pass  [81] , and Lu et al.'s SafetyNet  [165]  looks for abnormal patterns in the ReLU activations of higher layers. In contrast, the method presented in this chapter performs detection based on statistics of activation patterns in the complete representation learning part of the network as observed in feature response maps, whereas Li  [151]  directly observes convolutional filter statistics there.\n\nOn the other hand, the second class of detection approaches trains sophisticated classifiers for directly sorting out adversarially optimized inputs: Meng and Chen's MagNet  [181]  learns the manifold of friendly images, rejects far away ones as hostile and modifies close outliers to be attracted to the friendly images' manifold before feeding them back to the network under attack. Grosse et al.  [93]  enhance the output of an attacked classifier by an additional class and retrain the model to classify adversarial examples as such directly. Metzen et al.  [183]  have a similar goal but target it via an additional subnetwork. In contrast, this chapter presents a method that uses a simple threshold-based detector and pushes all decision power to the human-explainable feature extraction via the feature response maps.\n\nFinally, as shown in  [3] , different and mutually exclusive explanations for the existence of adversarial examples and the nature of neural network decision boundaries exist in the literature. Because our method enables a human investigator to trace attacks visually, it will be instrumental in taking this debate further.",
      "page_start": 73,
      "page_end": 74
    },
    {
      "section_name": "Background",
      "text": "This section briefly presents adversarial attacks and the theory of feature response estimation before assembling both parts into the proposed detection approach in the next section.",
      "page_start": 74,
      "page_end": 74
    },
    {
      "section_name": "Adversarial Attacks",
      "text": "The principal idea behind adversarial attacks is to find a small perturbation for a given image that changes the prediction of a CNN. Pioneering work demonstrated that negligible and visually insignificant perturbations could lead to considerable deviations in the networks' output  [262] . The optimization problem of finding a perturbation η for a normalized clean image I ∈ R m , where m is the size (width×height) of the image, is stated as follows  [262] :\n\nwhere C(.) presents the classifier, and ℓ is the ground truth label. Szegedy et al.  [262]  proposed a solution for the optimization problem of finding the perturba-  tions in Equation  4 .1 for arbitrary labels ℓ ′ that differ from the ground truth. However, they used box-constrained limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS)  [78]  to find perturbations satisfying Equation  4 .1. Optimization based on the L-BFGS algorithm for finding adversarial attacks is computationally inefficient compared to gradient-based methods. Therefore, in this chapter, a few different gradient-based attacks, a one-pixel attack, and a boundary attack are used to compute adversarial examples, as explained in the following paragraphs (see Figure  4 .1).\n\nFast gradient sign method (FGSM)  [89]  is a method suggested for computing adversarial perturbations based on the gradient ∇ I J(θ, I, ℓ) of the cost function with respect to the original image pixel values:\n\nwhere θ represents the network parameters and ϵ is a constant factor that constrains the max-norm (l ∞ ) of the additive perturbation (η). The ground truth label is presented by ℓ in Equation  4 .2. The sign function is Equation  4 .2 which computes the elementwise sign of the gradient of the loss function with respect to the input image. Optimizing the perturbation in Equation  4 .2 in a single step is called the fast gradient sign method (FGSM) in the literature. This method is a white box attack, i.e. the algorithm for finding the adversarial example requires information on weights, gradients, and the network's architecture.\n\nGradient attack is a straightforward realization of finding adversarial perturbations in the FoolBox toolbox  [211] . It optimizes pixel values of an original image to minimize the ground truth label confidence in a single step based on the gradient values instead of their sign proposed in the FGSM method.\n\nOne pixel attack  [255]  is a semi-black box approach to compute adversarial examples using an evolutionary algorithm  [252] . The algorithm is not a white box since it does not need the gradient information of the classifier. However, it is not a full black box as it needs the class probabilities. The iterative algorithm starts with randomly initialized parent perturbations. The generated offspring compete with their parents at each iteration, and the winners advance to the next step. The algorithm stops when the ground truth label probability is lower than 5%.\n\nDeepFool  [188]  is a white box iterative approach in which the closest direction to the decision boundary is computed in every step. It is equivalent to finding the corresponding path to the orthogonal projection of the data point onto the affine hyperplane, which separates the binary classes. The initial method for binary classifiers can be extended to a multi-class task by considering the task multiple one-versus-all binary classifications. After finding the optimal updates toward the decision boundary, the perturbation is added to the given image. The iterations continue estimating the optimal perturbation and apply it to the perturbed image from the last step until the network fails to predict the ground truth label.\n\nBoundary attack is a black-box attack proposed by Brendel et al. in  [30] . The algorithm starts with an adversarial image from another class compared with the target image and iteratively optimizes the distance between this image and the target image. It searches for an adversarial example with a minimum distance from the target image that keeps its original class throughoutd the optimization.",
      "page_start": 74,
      "page_end": 76
    },
    {
      "section_name": "Feature Response Estimation",
      "text": "The Erhan et al.  [72]  used backpropagation for visualizing feature responses of CNNs.\n\nThey evaluate an arbitrary image in the forward pass and retain the activation values; then backpropagate from the last convolutional layer to the original image. As a result, the feature response maps have higher intensities in the regions that cause larger network activation values (see Figure  4 .1). Moreover, the information on max-pooling layers in the forward pass can further improve the quality of these visualizations. Zeiler et al.  [316]  proposed computing \"switches\", to consider the position of maximum values in all pooling layers, and then construct the feature response maps using transposed convolutional  [68]  layers.\n\nUltimately, Springenberg et al.  [247]  proposed a combination of both methods called \"guided backpropagation\". In this approach, the information of \"switches\" (max-pooling spatial information) is kept, and the activations are propagated backward with the guidance of the \"switch\" information. This method leads to the best performance in the visualization of the inner workings of the network.\n\nTherefore, guided backpropagation is used for computing feature response maps in this chapter.",
      "page_start": 77,
      "page_end": 77
    },
    {
      "section_name": "Explainable Adversarial Attacks Detection",
      "text": "After reviewing the necessary background in the last section, this section presents this thesis's contribution to tracing adversarial examples in feature response maps, which inspired the novel approach to the automatic detection of adversarial perturbations in images. In this manner, visual representations of neural networks' inner workings also provide expert human guidance in developing CNNs that have increased reliability and explainability.",
      "page_start": 78,
      "page_end": 78
    },
    {
      "section_name": "Tracing Adversarial Attacks",
      "text": "The research question followed in this chapter is whether explainability methods can provide insight into the reasons behind the misclassification of adversarial examples. The effect of adversarial attacks in the feature response maps of CNNs is traced in Figure  4 .1. The general phenomenon observed in all images is that the feature response maps' active region for adversarial examples is widely spread. In contrast, Figure  4 .1 demonstrates that the network looks at a smaller region of the image, i.e. is more focused, in the case of not manipulated samples.\n\nThe adversarial images are visually very similar to the original ones. However, they are not recognizable by deep CNNs. The original idea behind this study is that the focus of CNNs changes when facing an adversarial attack, leading to incorrect predictions. Conversely, the network makes the correct prediction once it focuses on the right region of the image. Visualizing the feature responses provides this and other exciting information regarding decision-making in computer vision models. For instance, the image of the submarine in Figure  4 .1 can be considered a good candidate for an adversarial attack since the CNN is making the decision based on an object in the background (see the feature response maps of the original submarine in Figure  4 .1).",
      "page_start": 78,
      "page_end": 78
    },
    {
      "section_name": "Detecting Adversarial Attacks",
      "text": "Experiments tracing the effect of adversarial attacks on feature response maps thus suggest that a CNN classifier focuses on a broader region of the input if it is deliberately perturbed.  maps are initially converted to grayscale images, and local spatial entropies are computed based on the grayscale feature response maps as follows  [37] :\n\nwhere S k is the local spatial entropy of a small part (patch) of the input image and h k represents the normalized 2D histogram value of the k th patch. The indices i and j scan through the height and width of the image patches. The patch size is 3 × 3, the same as the filter size of the first layer of the VGG19 model used. The local spatial entropies of corresponding feature responses are presented in Figure  4 .2, and their difference for clean and adversarial examples suggests a likely chance of detecting perturbed images based on these maps.\n\nAccordingly, the proposed method in this chapter uses the average local spatial entropy of an image as the final single measure to decide whether an attack has occurred or not. The average local spatial entropy S is defined as:\n\nwhere K is the number of patches on the complete feature response maps and S k shows the local spatial entropy as defined in Equation  4 .3 and depicted in the last row of Figure  4 .2. The proposed detector makes the final decision by comparing the average local spatial entropy from Equation  4 .4 with a selected threshold to measure the spatial complexity of the feature response maps.",
      "page_start": 78,
      "page_end": 79
    },
    {
      "section_name": "Experimental Results",
      "text": "The first experiments visually compare the approximated distribution of the averaged local spatial entropy of feature response maps in clean and perturbed images to evaluate the value of the explained metric in Equation  4 .4. The validation set of ImageNet  [223]  with more than 50, 000 images from 1, 000 classes is the subject of this study, and feature response maps are computed for the VGG19 model  [243] . Perturbations for this experiment are computed using several different methods, and the distribution of average local spatial entropies is depicted for the fast gradient sign attack (FGSM).   Computing adversarial perturbations using evolutionary and iterative algorithms is demanding in terms of time and computational resources. To apply the proposed detector to a wide range of adversarial attacks, many images are randomly drawn from the validation set of ImageNet for each attack to evaluate the detection performance of the presented method. The selection of images for each attack is made sequentially by class and filename and comprises only the number of images per method that can be computed within a reasonable amount of time, using a reasonable number of resources (see Table  4 .3). The experiments are based on the FoolBox benchmarking implementation 2  , and the attacks are computed using TitanX graphics processing unit (GPUs).\n\nFigure  4 .2b presents the receiver operating characteristics (ROC) of the proposed detector; numerical evaluations are provided in Table  4 .3. Our detection method performs better for gradient-based perturbations compared to the single-pixel attack. Furthermore, Table  4 .3 suggests that the best adversarial attack detection performance is achieved for FGSM and boundary attack perturbations, where the network confidences on the ground truth labels are changed the most after manipulating the images. This observation suggests that the proposed detector is more sensitive to stronger attacks in fooling the network with a more drastic effect on target class confidence. Using feature response maps, the proposed method detects more than 91% of the perturbed images with a low false positive rate (1%). In general, it is difficult to directly compare different studies on attack detection since they use a wide variety of neural network models, datasets, attacks, and experimental setups. Table  4 .4 presents a short overview of the performances of current detection approaches. The approach presented in this chapter is most similar to the methods of Liang et al. (  [153] ) and Xu et al.  [307] . The detector proposed in this chapter outperforms both of the aforementioned methods based on the presented results in their work. However, it is not fully known if they used identical implementations and parameterizations of the attacks (e.g., a subset of images and learning rates for optimizing the perturbations). Similarly, adaptive noise reduction in the original publication  [153]  is only applied to four classes of the ImageNet dataset, and the method presented to detect adversarial attacks is implemented based on CaffeNet deep learning framework, which differs from our experimental setup.",
      "page_start": 80,
      "page_end": 81
    },
    {
      "section_name": "Discussion And Conclusion",
      "text": "The results presented in this chapter demonstrate the relevance and importance of adversarial attacks and the necessity to improve the robustness of CNNs against such perturbations. However, preliminary experiments on binary (cat versus dog  [200] ) and ternary (among three classes of cars  [137] ) classification tasks suggest that it is more challenging to find adversarial examples where marginal changes in the images are invisible to humans in such a setting. These tasks are proxies for the kind of few-class classification settings frequently arising in practice.  This chapter offers an approach to detect adversarial attacks based on humanexplainable feature response maps. The proposed method traces the effect of adversarial perturbations on the networks' focus region in original images, which inspired a simple yet robust approach for detecting adversarial attacks automatically. The proposed method is based on thresholding the averaged local spatial entropy of the feature response maps and detecting at least 91% of state-of-the-art adversarial attacks with a low false positive rate on the validation set of ImageNet. However, the results are not directly comparable with state-of-the-art methods because of the diversity in the experimental setups and attack implementations.\n\nExperimental results verify that feature response maps are informative in detecting specific failure cases in deep CNNs. Furthermore, the proposed detector uses the explainability of neural network decisions, an increasingly important topic for developing robust and reliable vision models. Future work, therefore, will concentrate on developing reliable and explainable image classification methods for practical use cases based on these preliminary results.",
      "page_start": 82,
      "page_end": 82
    },
    {
      "section_name": "Motion Compensation In Computed Tomography Using Cnns",
      "text": "This chapter presents this thesis's main applied contribution, motion artifact reduction, which enhances the quality of cone-beam computed tomography (CBCT) scans using 3D convolutional neural networks (3D-CNNs). This application is relevant and exciting for two main reasons: 1) There is no analytical solution to the problem of motion compensation since reconstruction algorithms are developed based on the assumption of measurements from a constant volume. 2) Motion artifacts are relevant in CBCT scans because of their long acquisition time, and using CBCTs demonstrates improving cancer therapy via adaptive dose calculation and patient positing.\n\nThis chapter offers a novel deep-learning (DL) based approach that significantly reduces motion artifacts and improves scan quality.\n\nBecause motion artifact reduction has no analytical solution, 3D deep convolutional neural networks (3D-CNNs) are employed as pre-and/or post-processing steps during CBCT reconstruction to target motion artifacts using a data-driven approach. The method described in the following paragraphs is performed either using the analytical Feldkamp-Davis-Kress (FDK) with filtered backprojection (FBP) reconstruction method or using the iterative algebraic reconstruction technique (iCBCT/ART). Based on refined UNet architectures, the neural networks are trained end-to-end via supervised learning. The dataset used in this chapter is generated from 4D computed tomography (CT) scans of lungs containing ten motion phases between inhalation and exhalation and patients' breathing curves with negligible motion artifacts. The training ground truth volumes are the averaged volume over all phases in 4D-CT or the volume at the average amplitude of the breathing curve. The trained networks are validated using real-world CBCT scans and quantitative image quality metrics. In addition, a qualitative evaluation from clinical experts is performed.\n\nThe novel approach in this chapter can generalize to unseen data and yield notice-Chapter 5. Motion Compensation in CBCT Using CNNs able reductions in motion-induced artifacts and improvements in image quality compared with state-of-the-art CBCT reconstruction algorithms (up to 6.3dB and 0.19 improvements in PSNR and SSIM, respectively). The experimental findings from the simulation are confirmed by a clinical evaluation of real-world patients' scans (clinical experts reported at least a noticeable change in motion reduction over standard reconstruction in 74% of cases). To the best of our knowledge, this is the first time that inserting deep neural networks as pre-and post-processing plugins in the existing CBCT reconstruction pipeline and end-to-end training demonstrates significant improvement in imaging quality and reducing motion artifacts in CBCT scans. This chapter is mainly adopted from the research published in  [12] , licensed under CC BY-NC-ND 4.0 1  .",
      "page_start": 83,
      "page_end": 83
    },
    {
      "section_name": "Introduction",
      "text": "Computed tomography (CT) has become a versatile radiology and radiation therapy imaging technique. It obtains detailed 3D scans of the human body for diagnostics and planning therapies. Cone-beam CT (CBCT), in particular, is used for reconstructing scans from measurements of radiation therapy treatment devices (linear accelerators). CBCT reconstruction techniques in image-guided radiation therapy (IGRT)  [113]  and interventional radiology provide high spatial resolution in a cost-efficient manner  [70] . IGRT treatment is performed in up to 40 fractions. For each fraction, it is necessary to obtain the image of the day in order to position the patient accurately. Novel applications of CBCT imaging in IGRT, such as online adaptive replanning  [313]  or daily treatment planning and dose calculation  [114] , are very well-known in the scientific community.\n\nThere are two leading families of reconstruction algorithms used in modern CT scanners: (i) analytical techniques and (ii) algebraic reconstruction. The first group is based on filtered backprojection (FBP), most prominently represented by the Feldkamp-Davis-Kress (FDK) method  [75] . The second group consists of the algebraic reconstruction techniques (ART) family, which is based on reformulating and solving the reconstruction problem through an iterative optimization technique. Although the development of algebraic methods started in the late 1960s  [103] , they have only been deployed on CT scanners in the last 15 years  [91]  mainly because of their high computational cost. In recent years, this problem has disappeared due to the high availability and relatively low cost of GPUs.\n\nIterative CBCT (iCBCT) reconstruction algorithms based on the ART family introduced in  [202]  for Varian's Halcyon and TrueBeam addressed the need for superior image quality in terms of better noise suppression and improved contrast as compared with FDK and demonstrated in  [82, 133, 174, 294] . However, there is still room for improvement in these methods, if they are to tackle real-world artifacts which are not a part of theoretical and analytical solutions.\n\nImaging artifacts  [232]  are still a prevalent feature in CBCT reconstruction. The main sources of artifacts are (i) electrical and photon count noise, (ii) photons from scattered X-rays, (iii) extinction and beam hardening effects (e.g., due to metal implants), (iv) approximations in the reconstruction (due to finite beam width and detector pixel size), (v) aliasing (due to finite pixel size and cone beam divergence), (vi) ring artifacts (due to defect or miscalibrated detector elements), and (vii) patient motion.\n\nMotion artifacts arise since the reconstruction methods assume that the scanned patient is stationary. However, periodic respiratory and cardiac motions and nonperiodic motions, such as gas bubbles in the abdomen caused by the digestive system, lead to acquiring projections from different states of motion in the body. Patients' motion leads to the appearance of evident and undesirable, typically streak-shaped artifacts in reconstructed scans.\n\nThe following motion compensation strategies are used so far in IGRT clinical routine: (i) 4D or gated CBCT based on an external breathing signal  [61] , (ii) breath hold CBCT based on an external breathing signal and potential patient feedback, (iii) assisted breathing based on a ventilator system  [205] , (iv) abdominal compression devices applied to the patient  [51] , (v) internal breathing signal extraction  [7] .\n\nThis chapter presents a novel approach to mitigate motion artifacts in CBCT reconstruction using deep learning (DL). The CBCT reconstruction is integrated within a DL pipeline, where convolutional neural networks are employed as pre -and/or post-processing steps. These networks act on either the 2D X-ray projections (preprocessing), the reconstructed 3D volume (post-processing), or both. Next, the models are trained end-to-end in a supervised fashion using CBCT scans containing simulated motion artifacts and motion-free scans as ground truth. Finally, the trained models are validated quantitatively using various scan quality-related numerical metrics, and on an independent real-world patient CBCT dataset developed through qualitative clinical feedback.",
      "page_start": 84,
      "page_end": 85
    },
    {
      "section_name": "Related Work",
      "text": "Much work has been done  [232, 25, 86]  regarding the characterization and mitigation of the various kinds of artifacts that negatively impact image quality in CT and CBCT scans. In recent years, DL-based approaches have shown promising results, including applications for IGRT and adaptive radiation therapy  [203] .\n\nWürfl et al.  [303]  mapped the components of the FBP algorithm into a neural network by introducing a novel DL-enabled cone-beam back-projection layer. A forward projection operation efficiently computes the backward pass of the backprojection layer. This approach thus permits joint optimization and correction in the projection and volume domain. Moreover, Maier et al.  [171]  argued that implementing prior knowledge (such as the back-projection operation) in the form of (differentiable) known operators into DL algorithms reduces training error bounds while reducing the number of free parameters.\n\nLimited-angle CT is employed to reduce acquisition time and decrease the radiation dose, which leads to a degradation of image quality and introduces sparseness artifacts. To overcome these issues, Wang et al. recently presented an encoderdecoder architecture based on the UNet model  [219]  to reconstruct high-quality scans with fewer projections. A UNet processes scans reconstructed by the simultaneous algebraic reconstruction technique (SART) to improve the imaging quality  [18] . Experiments on chest and abdomen CT scans demonstrated the superiority of the proposed methods over existing approaches. Similarly, Schnurr et al. proposed UNet-based networks to correct limited-angle artifacts in circular tomosynthesis scans  [231] .\n\nDL-based approaches demonstrate success in metal artifact reduction (MAR)  [199, 319] . Lin et al. introduced a dual-domain architecture (DuDoNet) to jointly compensate for metal-induced artifacts in both projection and volume domains  [157] .\n\nExperimental results on the DeepLesion CT dataset  [309]  showed that the proposed method outperformed both traditional and other DL-based approaches. An improved model, DuDoNet++, was proposed to compensate for over-smoothed and distorted image reconstruction and led to improved artifact correction, especially for large metallic objects  [168] . Furthermore, there have been recent efforts in MAR using unsupervised approaches, as explained in  [154] . The U-DuDoNet model  [167]  directly models the artifact generation and compensation process in both the projection and volume domains. More recently, the interactive  [286]  and interpretable  [292]  versions of DuDoNet have been introduced to improve the interpretability and enhance the interaction between projection and volume domains.\n\nDL-based approaches have been employed to improve sparseness artifacts generated by low-dose CT reconstruction  [95, 115, 321, 136] . Chen et al. present the AirNet model to fuse analytical and iterative CT reconstruction and integrate them into DL to improve sparse-data 3D and 4D CBCT reconstruction  [38, 39] .\n\nIn the projection domain, DL-based models can also correct signal degradation caused by X-ray photons scattering within the patients' body  [172, 71] .\n\nFinally, motion artifact compensation using DL has received comparatively less interest.",
      "page_start": 86,
      "page_end": 86
    },
    {
      "section_name": "Materials And Methods",
      "text": "This section presents the preliminary knowledge and the related theory which lays the foundation of this chapter's main contributions and findings. First, this section starts with a more detailed explanation of the CBCT reconstruction techniques used in this chapter. Secondly, the motion simulation framework is explained, followed by a discussion on the simulated and real-world datasets and the clarification of their differences.",
      "page_start": 87,
      "page_end": 87
    },
    {
      "section_name": "Cbct Reconstruction",
      "text": "Both analytical and iterative methods are considered for the reconstruction of 3D CBCT volumes from 2D cone-beam projections in this research work. Feldkamp-Davis-Kress  [75]  (FDK) is an analytical reconstruction method based on filtered back-projection (FBP). Although the Tuy data-sufficiency conditions  [273]  are not met for circular trajectories of a cone-beam source, FDK provides a fast and reliable analytical approximation of the inverse Radon transform, and it has become a golden standard for 3D CBCT reconstruction  [34] . Ram-Lak filter compensates for the radial non-uniformity of the sampling density in FDK. Moreover, half-fan weighing is necessary to avoid data duplication for the datasets acquired with half-fan geometry. The projections are acquired from the full 360 • trajectory. However, the detector is shifted against the gantry to one direction to increase the field of view in half-fan geometry. Half-fan weighing is followed by cosine weighting to decrease the longitudinal full-off effect due to the cone-beam geometry.\n\nFinally, the projections are down-sampled so that their resolution matches the cut-off frequency requirement given by the target resolution of the reconstructed volume.\n\nBesides FDK, in this chapter, the iterative algebraic reconstruction technique (ART) originally based on Kaczmarz algorithm  [119]  is used for iterative CBCT (iCBCT) reconstruction. This method approximates the volume f by an iterative optimization of the data-fidelity cost function: ∥ Afp ∥ 2 , where A and p represent the forward-projection operator and projections in the attenuation space, respectively. In each iteration k, an update of the actual volume estimation is computed through back-projecting the gradient of the cost function, i.e. α A ⊤ ([Af k ] α -p α ) where p α and [Af k ] α denote the projection under angle α and corresponding forward-projection of actual estimated volume f k , respectively, and A ⊤ represents the back-projection operator. One of the advantages of the iterative methods is allowing a straightforward integration of prior knowledge into the reconstruction process through a regularization term to augment the cost function during optimization. The implementation used in this chapter employs the edge-preserving total variation regularization, which helps to reduce the noise and cone-beam artifacts in the areas far from the iso-center.\n\nIn order to significantly reduce the computational cost, the GPU implementation of ART is further accelerated through the following approaches: First, the version of ART known as simultaneous ART (SART) is used where the volume is updated in parallel for each input projection. Next, ordered subsets (OS)  [132]  and Nesterov momentum method  [190]  are employed. Finally, a destination-driven approach  [125]  is employed in the forward projection of ART and backward projection of both ART and FDK. Further details about the TV-regularized OS-SART with momentum can be found in  [207] , where the method is presented as a part of the iCBCT algorithm deployed clinically in Varian products.",
      "page_start": 87,
      "page_end": 88
    },
    {
      "section_name": "Motion Simulation",
      "text": "It is necessary to simulate motion for volumes with available ground truth to train the models using supervised learning; hence, the motion simulation method aims to generate synthetic datasets of CBCT volumes with motion artifacts. The motion simulation method starts from the phase-gated 4D CT scans described in Section 5.3.3 and a set of recorded breathing curves. The method profits from the Deeds  [97]  algorithm to perform a deformable registration between CT volumes of the end-inhale and end-exhale phases and to create a patient-specific deformation vector field (DVF).\n\nA reconstructed CT scan, its DVF, and the patients' breathing curve are sufficient requirements to simulate the motion during the CBCT acquisition. The simulation method deforms the CT volumes by interpolating the DVFs according to the breathing amplitude. It creates a forward projection at each angular step by matching its timestamp with the relevant amplitude in the breathing curve. Each projection acquired through the described motion simulation method corresponds to a different respiratory state. Then, the CBCT volume is reconstructed using either the FDK or iCBCT reconstruction algorithms. Motion artifacts are evident in the volumes reconstructed from the explained motion-simulated CBCT projection acquisition technique.",
      "page_start": 88,
      "page_end": 88
    },
    {
      "section_name": "Datasets",
      "text": "For the training and validation of the different methods, a dataset of thoracic 4D CT scans from 80 patients is split into fractions of 60%, 20%, and 20% for training, validation, and testing, respectively. They were provided as input to the motion simulation described in Section 5.3.2. The patient-specific anatomical correct deformations were extracted from the end inhale and exhale out of the ten breathing phases. To simulate plausible motion patterns during a virtual CBCT acquisition and to augment the training dataset, 400 recorded breathing curves were obtained via Varian's real-time position management (RPM) system.\n\nFor testing the developed methods on real-world (clinical) patient CBCT scans, a dataset of 77 Halcyon scans was employed. All pre-processed projection data and reconstructed volumes were given at the same size, resolution, and geometry to ensure consistency: The projection size is 320x80 pixels (with a resolution of",
      "page_start": 89,
      "page_end": 89
    },
    {
      "section_name": "Supervised Learning For Motion Artifact Reduction",
      "text": "This section presents the necessary underlying basics to start with supervised learning. First, the DL-enabled framework for reconstruction and refinement models, including UNets in the projection-and volume domains, is explained. Second, evaluation metrics for numerical analysis of simulated data are presented, and the section concludes by describing the experimental setup, including the hardware.",
      "page_start": 90,
      "page_end": 90
    },
    {
      "section_name": "Dl-Enabled Cbct Reconstruction",
      "text": "Motion leads to inconsistencies in the acquired projections, which appear as artifacts in the volume domain after reconstruction. Therefore, motion corrections can be, in principle, applied before and/or after reconstruction. The models, estimating these correction steps, are implemented as trainable neural network architectures derived from 3D refined UNet architectures.\n\nThe reconstruction algorithm used is either FDK or iterative CBCT (iCBCT) reconstruction, as discussed in Section 5.3.1. These algorithms are implemented based on forward-and back-projection layers implemented with custom compute unified device architecture (CUDA) code and interfaced as PyTorch modules. The analytical solution using filtered back-projection, inspired by FDK, is differentiable. Therefore, it is possible to back-propagate the gradient through this module and simultaneously optimize in both projection and volume domains, called dual-domain optimization. Dual-domain optimization requires a differentiable reconstruction method such as FDK and is not practical for iterative techniques.\n\nThe supervised learning approach uses the simulated motion dataset (Section 5.3.2) for training the motion compensation networks, where the loss is calculated in the volume domain. The ground truth is either calculated as the motion-averaged volume (\"average volume\") or given as the volume corresponding to the fixed motion state matching the average breathing signal amplitude (\"average amplitude\"). The networks are validated on the validation and test portions of the simulated motion dataset and an independent real-world test dataset containing real-world clinical patient CBCT scans (see Section 5.3.3). In detail, the reconstruction pipeline consists of the following components:  The architecture of the proposed dual-domain model for end-to-end optimization consists of the following components: (i) a projection enhancement network (PE-Net), (ii) a projection-to-volume reconstruction layer, and (iii) a volume enhancement network (VE-Net) (figure adopted from  [12] ).\n\nProjection Enhancement Network (PE-Net): A convolutional neural network based on UNet architectures, explained in more detail in the next section, is deployed to mitigate motion-induced artifacts in the projection space. PE-Net receives as input the acquired projections {X proj ∈ R Hp×Wp×Cp }, and enhances these projections { Xproj }, i.e. Xproj ≈ f pe net (X proj ) to remove motion artifacts in the projection space. Here, H p × W p × C p denotes the projection dimensions in terms of height, width, and number of projections.\n\nProjection-to-Volume Reconstruction Layer: The projection-to-volume reconstruction layer f rec (•) receives as input the (enhanced) projections { Xproj } and outputs a reconstructed volume {X vol ∈ R Hv×Wv×Cv }, i.e. f rec ( Xproj ) → X vol : R Hp×Wp×Cp → R Hv×Wv×Cv , where H v × W v × C v represents the volume's height, width, and number of slices. This layer corresponds to the regular FDK or iCBCT reconstruction (Section 5.3.1).",
      "page_start": 90,
      "page_end": 91
    },
    {
      "section_name": "Volume Enhancement Network (Ve-Net):",
      "text": "The VE-Net f ve net (•) is respon-sible for enhancing the reconstructed volume and compensating motion artifacts in the volume domain. As output, the VE-Net produces an enhanced volume { Xvol ∈ R Hv×Wv×Cv }, i.e. Xvol ≈ f ve net (X vol ).\n\nOur proposed dual-domain (end-to-end) model, shown in Figure  5 .2, combines the above components for motion correction in both projection-and volume domains. It consists of three different modules: (i) a projection enhancement network (PE-Net), a (ii) projection-to-volume reconstruction layer, and a (iii) volume enhancement network (VE-Net).\n\nThe following paragraphs describe the different model blocks of the proposed architecture. Note that these blocks are used in both the projection enhancement (PE-Net) and volume enhancement (VE-Net) networks.\n\nEncoder Blocks: The encoder blocks of the presented architecture in Figure  5 .2 consist of four similar submodules including 3D a convolutional layer with filters of size 3 × 3 × 3, followed by an instance normalization  [274] , the Swish activation function  [210]  and a 3D max-pooling layer of size 2 × 2 × 2. The number of convolutional filters in the first block is doubled for every next layer. Hence, the latent representations of the input volume have a larger number of channels but a smaller spatial size with a higher receptive field after the first layer.\n\nDecoder Blocks: The decoder block aims at computing the motion corrections from latent representations and has four submodules starting with a trilinear upsampling followed by 3D convolutions of size 3 × 3 × 3, instance normalization, and the Swish activation function. The number of convolutional filters is halved after each layer to make the entire model's architecture symmetric.\n\nAttention mechanisms: To further compensate for motion artifacts, the models rely optionally on attention mechanisms. More precisely, as part of the bottleneck and decoder blocks of both PE-Net VE-Net, there are channel-wise and spatial-wise attention layers  [302]  in 3D. The corresponding input feature maps are multiplied at each decoder layer with the generated attention maps to refine the original features. The model can focus on more relevant features using these attention layers. Models including attention layers, are denoted by \"Attn.\" in Table  5 .1.\n\nResidual Learning and ResUNet: Using residual learning is crucial to simplifying the learning task and improving the convergence speed. The architecture depicted in Figure  5 .2 uses two components to enhance the gradient flow and simplify the learning task. First, the proposed architecture profits from a direct residual connection from input to output (\"residual learning\") to optimize the required correction instead of reconstructing the ground truth. The proposed architecture optionally includes internal residual connections between the input and output of the convolutional layers to improve the gradient flow as described in  [320] . Networks including \"ResUNet\" layers are labeled as such in Table  5 .1.",
      "page_start": 92,
      "page_end": 92
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "The experimental results of motion compensation on the simulated dataset are reported based on numerical performances using several quantitative metrics  [293]  sensitive to the similarity of pairs of volumes (x, x ′ ). The evaluation metrics are computed by summing up the differences over all components, voxels in volumes, as follows:\n\n• peak signal-to-noise ratio: PSNR = 10 log 10 MAX 2 MSE\n\n• structural similarity index (SSIM )  [293]  In addition, Table  5 .1 reports the mean and standard deviation of the difference image x -x ′ used for reducing (correcting) the motion artifacts. All metrics are calculated in Hounsfield units (HU) from pairs of uncorrected or corrected body-masked volumes and their corresponding ground truth counterparts.",
      "page_start": 93,
      "page_end": 93
    },
    {
      "section_name": "Experimental Details",
      "text": "This section describes the experimental setup, architectural variants, optimization settings, and implementation details used for training based on a motionsimulated dataset.\n\nExperimental Setup: The volume size is 256 × 256 × 48 voxels based on the neural network architectures to optimize computational and memory allocation costs. Based on the training dataset discussed in Section 5.3.3, 720 projections are used per scan for training, and motion artifacts in CBCT scans are computed using motion simulation introduced in Section 5.3.2. The reconstruction and forward-projection geometry are selected to match the real-world test dataset, which is used for clinical evaluation in Section 5.5.\n\nData Augmentation: Five different patient breathing curves per CT scan are added for motion simulation from each original CT scan in the training dataset. Data augmentation through various breathing curves led to a considerable boost in the final performance of our motion correction models.\n\nModel Architecture: The baseline model, initially considered for motion correction, is a UNet with residual learning from input to output as depicted in Figure  5 .2. A plain UNet  [219]  architecture without residual connections is already sufficient for correcting the artifacts in volume space; however, residual learning is necessary for the more complicated tasks, including correcting the projections and dual-domain optimization. Therefore, all of our models include residual learning. The baseline UNet model has a depth of 4 and has 32 filters in the first layer. The number of filters doubles after every layer up to the middle (model's bottleneck), and the architecture is reverted afterward. The same architecture is used for both PE-Net and VE-Net. For dual-domain optimization, two such models form the architecture together. For PE-Net, the models process the projections in chunks of 192 due to memory limitations. PE-Net and VE-Net are extended with internal residual connections (\"ResUnet\") and/or channel-spatial attention (\"Attn.\") for different experiments presented in the next section.\n\nImplementation and Optimization Settings: The models used for motion compensation are implemented using the PyTorch  [201]  framework. The experiments were performed on NVIDIA V100 (A100) GPUs with 32 (40) GB of VRAM. Both projections and volumes are normalized using constant coefficients per dataset to the approximate range of zero and one before optimization. The loss function for optimizing the models is the difference between the predicted and reconstructed volume as computed by the\n\nThe AdamW  [162]  optimizer is used with a learning rate of 1.41 • 10 -6 and weight decay of 1.87 • 10 -8 in the projection domain, and a learning rate of 1.11 • 10 -4 and weight decay of 1.39 • 10 -8 in the volume domain. These parameters are the results of a joint hyperparameter sweep with other parameters, such as a number of convolutional filters, kernel size, and convolutional dilation. This experiment's batch size is 1 (due to GPU memory limitations), and training continues for a total number of 300 epochs. The model that reduces the validation loss the most during the training is selected for testing.",
      "page_start": 93,
      "page_end": 94
    },
    {
      "section_name": "Experimental Results",
      "text": "This section presents the experimental quantitative and qualitative results obtained by applying DL-based motion reduction techniques using 3D convolutional neural networks. First, the quantitative results obtained with the test portion of the simulated dataset are presented. Second, the qualitative results based on a clinical evaluation of the real-world test dataset are discussed.",
      "page_start": 94,
      "page_end": 94
    },
    {
      "section_name": "Quantitative Results",
      "text": "In order to train the model architectures (see Figure  5 .2) in a supervised scenario, only the simulated motion dataset (Section 5.3.3) is relevant. The training set is used for training the models, while the validation set results guide the optimization to select the best models and hyperparameters. Experimental results in this section consist of the final performance on the left-out test set during the training Table  5 .1: Presented are the quantitative results of DL-based motion correction for CBCT data with simulated motion. The table presents the performance of the proposed motion reduction framework based on the RMSE, PSNR, and SSIM metrics and reports the mean and standard deviation of the body-masked difference (correction) volumes. The metrics are calculated between the reconstructed and ground truth volumes, converted to HU with slope and intercept of 48200 and -1106, respectively. All numerical values are averaged over the test set. The table shows the average metric together with the average gain (or loss) and the latter's standard deviation to clarify the contribution of the motion correction. For example, in the last row, the average PSNR is reported as 33.00 dB, corresponding to an average improvement of 4.62 dB, with a standard deviation of 0.82 dB. The models noted by † are used for clinical evaluation (Section 5.5.2) (figure adopted from  [12] ).\n\nand parameter optimization.\n\nTable  5 .1 presents the numerical performance of the various architectures discussed in Section 5.3 for two reconstruction methods FDK and iCBCT, with two different sets of ground truth volumes (\"average volume\" or \"average amplitude\"). Three different neural network architectures are investigated: \"3D-UNet\" (base architecture), \"3D-ResUNet\" (UNet-based enhanced with ResUNet), and \"3D-ResUNet+Attn.\" (enhanced using both ResUNet and attention blocks). The ground truth volumes with average amplitude differ more from their corresponding uncorrected volume with motion artifacts than the ones with averaged volume. Therefore, the baseline RMSE is larger for average amplitude, and lower baseline performances in terms of PSNR and SSIM are reported in Table  5 .1. Since computing the gradients in the backward pass of the reconstruction algorithm, which is required for training models in the projection domain, is only practical for the FDK reconstruction, Table  5 .1 does not present results based on iCBCT for optimizing in the projection domain and dual-domain. The numerical results are reported based on computing the metrics as introduced in Section 5.4.2 between the body-masked ground truth and reconstructed volumes, converted to HU.",
      "page_start": 94,
      "page_end": 96
    },
    {
      "section_name": "Uncorrected Volume Ground Truth",
      "text": "Corrected Volume Presented is the uncorrected volume using default reconstruction (left), the ground truth volume, both as difference and absolute image (\"average volume\", top right), as well as the corrected volume (bottom right). Images are presented in HU with W/L=1000/0 (figure adopted from  [12] ).\n\nThe numerical evaluation demonstrates that training 3D-CNNs is consistently successful in compensating motion for both projection and volume domains, with the best performance being achieved in the volume domain. Numerically, it corresponds to a rise of 6.34 dB in PSNR and 0.1499 for SSIM for FDK with \"average volume\" ground truth. The highest improvement reported for iCBCT is 5.81 dB of PSNR and 0.1996 in SSIM with \"average amplitude\" ground truth. Table  5 .1 reports a very competitive performance in dual-domain optimization. However, most of the motion correction performance in the dual-domain setting is based on the volume domain corrections. The maximum average gained PSNR in the case of pure projection domain optimization turned out to be 1.33 dB. These results represent the first successful attempt at reducing motion artifacts in CBCT scans using deep neural networks.\n\nThe method proposed reduces motion artifacts for two reconstruction techniques (FDK and iCBCT) with several different architectures, including variants with added internal residual connections and/or channel-spatial attention. The motion compensation performance shows a small but consistent variance with the details of the neural network architecture. Reducing the motion artifacts in the projection domain is a subject for further research and optimization due to the more challenging optimization settings. Optimization in the projection domain relies on gradients propagated all the way through the CBCT reconstruction layer and suffers from the large volume of data in the projection space and current GPU memory limitations.\n\nComparing the two CBCT reconstruction algorithms, iCBCT shows more robustness against motion during acquisition time, and a slightly lower drop in baseline performances is reported. In addition, artifact reduction using 3D-CNNs in the volume domain for iCBCT reconstruction is successful and shows the same results as FDK. Figures 5.3 and 5.4 present example visualizations of the observed motion artifact improvements seen in volume domain learning on top of the FDK and iCBCT reconstructions, respectively.",
      "page_start": 97,
      "page_end": 97
    },
    {
      "section_name": "Clinical Evaluation",
      "text": "To validate the quantitative results of the previous section in a clinical setting, the trained motion compensation CNN models are applied to a real-world test dataset (see Section 5.3.3 and Figure  5 .5). Finally, the performance of the motion correction models is evaluated based on the feedback obtained from clinicians.\n\nThe real-world CBCT scans used in this study are sufficiently different from the simulated training datasets, e.g., the projection count and HU calibration, to objectively judge the models' generalization capabilities. The attenuation values of the real-world test dataset are rescaled to match the scale of the training dataset to compensate for the different calibrations.\n\nThe expert feedback was collected from a study where clinicians visually inspected 30 pairs of iCBCT reconstructed and motion-corrected volumes, 15 each for ei- The uncorrected volume using default reconstruction (left), the ground truth volume, both as difference and absolute image (\"average volume\", top right), as well as the corrected volume (bottom right) are depicted in the table. Images are presented in HU with W/L=1000/0 (figure adopted from  [12] ).\n\nther a model trained using average amplitude or average volume ground truth. The best performing CNN architectures from Table  5 .1, UNet in volume domain without residual connections or attention, were used for clinical validation. Subsequently, 20 clinicians, including radiation oncologists, medical physicists, radiation technologists, and physicians, answered several questions about their preferences for using CNN models to reduce motion artifacts compared with the standard reconstruction. Each of the clinicians identified themselves as one of three general categories: physician (26%), medical physicist (37%), and dosimetrist/radiation technician (37%).\n\nInitial feedback on the iCBCT datasets indicated the presence of severe and mild unavoidable real-world artifacts besides motion in 34% and 20% of the scans, respectively. The study participants specified their level of agreement or preference concerning (a) a reduction of the observed motion artifacts and (b) the use for various applications, including dose calculation, patient positioning, and segmentation.\n\nThis clinical evaluation, the first of its kind to the best of our knowledge, faced the challenge of subjective assessments from experts with different clinical backgrounds. For example, physicians reported a noticeable or strong improvement in CNN-based motion artifact reduction using average volume ground truth in  The table shows example results for iCBCT reconstruction for real-world test dataset, using the two options for the choice of ground truth. The uncorrected volumes using default reconstruction (left), the residual corrections (middle), as well as the corrected volumes (right) are presented (figure adopted from  [12] ).\n\n80.00% of scans, while medical physicists only reported this in 65.83% of the scans. Nonetheless, medical physicists preferred CNN-corrected volumes in 63.33% of the cases for dose calculation, while the physicians reported this in only 30.67% of the cases.\n\nTable  5 .2 presents the average overall votes and the final clinical evaluation results. Despite the differences in the improvements reported by the different experts, there is a clear positive trend showing that the proposed CNN models are indeed able to reduce motion artifacts successfully. In addition, clinicians reported a tendency toward using CNN-corrected images (using average volumes ground truth) for plan adaptation and dose calculation. One area where clinical experts preferred to use images without CNN-based reconstruction is for softtissue-based patient positioning and manual or automatic tissue segmentation, as these images are typically sharper compared with the CNN-corrected ones.\n\nNevertheless, quantitative evaluation to compute the level of agreement when applying an automatic segmentation algorithm using CBCT images with and without motion artifact correction leads to overwhelmingly positive results. The Table  5 .2: Results of the clinical evaluation. This table shows the preferences for CNN-based or default iCBCT reconstruction when using CNN models trained using either average volume or average amplitude ground truth concerning motion artifact reduction and potential applications such as plan adaptation and dose calculation, patient positioning and segmentation (table adopted from  [12] ).\n\naverage dice score measures the automatic segmentation contours in original and motion-corrected volumes. This score is averaged over 18 organs or tissues, visible in most CBCT scans, including pulmonary arteries, breast, chest wall, lung, ribs, and spinal canal. The high dice score of 0.89 (0.88) when using average volume (average amplitude) ground truth demonstrates a very high level of consistency between the obtained segmentation contours despite the low preference reported by clinical experts for using the motion-corrected images for segmentation.",
      "page_start": 97,
      "page_end": 100
    },
    {
      "section_name": "Discussions And Conclusions",
      "text": "This chapter presents the first DL-based method for globally reducing motion artifacts in reconstructed 3D CBCT images, built on top of the two reconstruction algorithms FDK and iCBCT. Neural network architectures which act either on the reconstructed CBCT volumes, the input X-ray projections or both were trained in a supervised way using a motion simulation framework to provide motion-free ground truth. The experimental results demonstrate that DL-based architectures can correct motion artifacts. So far, the best results have been obtained in the volume domain through the implementation of a refined U-net architecture.\n\nThe quantitative evaluations demonstrate that using DL through deep neural network architectures yields significant improvements in image quality and reduces motion-induced artifacts in CBCT scans. In addition, a clinical evaluation was performed, in which clinical experts confirmed the principal quantitative results for motion artifact reduction using a real-world test dataset. Clinicians confirmed that artifacts are reduced and expressed a preference for using CNNcorrected CBCT images for dose calculation. However, for patient positioning or segmentation, this could not yet be demonstrated.\n\nThere are several related avenues that could be explored in future research:\n\nFirst, the presented results show promising improvements, mainly in the volume domain, independent of the acquisition parameters and reconstruction technique. However, there is room for improvement in the projection and dual-domain setting. One potential reason is the processing of the projections in batches due to GPU memory limitations, which leads to a loss of correlation between different projection batches separately processed by the neural network. In addition, great care is necessary to ensure the backpropagation of gradients through the CBCT reconstruction layer to provide a meaningful and noise-free learning signal in the projection domain.\n\nSecond, models trained using supervised learning typically suffer from generalization to data acquired in entirely different settings. Although the calibration technique used in this study successfully reduced the performance gap between the performance of the models on simulation and real-world datasets, generalization to highly different acquisition setups and other anatomies is not certain. This provides motivation for further investigation of unsupervised learning and/or domain adaptation techniques.\n\nThird, the current motion simulation only simulates respiratory motion and does not include other effects, such as cardiac motion. Therefore, tackling cardiac motion in chest CBCT scans combined with respiratory motion remains an open problem. This method could also be extended to handle abdominal CBCT scans, including different motion effects.\n\nIn conclusion, while the initial results are very promising, future research will aim to improve adaptive treatment capabilities in IGRT, including patient positioning and tumor targeting, auto-segmentation, and dose calculation applications directly on the radiotherapy device.\n\n6 Applications in Affective Computing, Medical Imaging and Beyond\n\nThis thesis presents several contributions to diverse and interdisciplinary research niches among many applications of machine learning (ML) and deep learning (DL). Although the original papers in this section have much more extensive content, this chapter summarizes the most scientifically thrilling findings and lessons learned from applying ML and DL in the real world. This chapter is broader in terms of the various applications discussed, more diverse than previous chapters, more straightforward, and more interesting because of its diversity.\n\nDespite the brief overviews, this chapter presents many interesting practical findings and insights that are beneficial in applied research and tuning ML and DL methods to their best performances. Furthermore, this chapter describes several practical problems in ML and DL, such as affective computing, pain estimation, and data homogenization, and identifies initial solutions to this particular research area. Finally, similar to the previous chapters, the remainder of this chapter discusses some exciting directions for future research.\n\nThe chapter is organized as follows: discussing solutions for facial expression estimation, emotion recognition, and findings on automated ML and DL focusing on bringing neural networks to their best performances. The chapter then presents two medical applications targeting signal processing for pain estimation and image processing for data homogenization for DL pipelines. The last two sections of this chapter introduce and elaborate on two well-known challenges of DL: fairness and robustness.",
      "page_start": 100,
      "page_end": 101
    },
    {
      "section_name": "Affective Computing",
      "text": "This section presents two applications of machine learning in affective computing. It begins with explaining the application of support vector machines in facial expression estimation, followed by emotion recognition using audio-visual features.",
      "page_start": 104,
      "page_end": 104
    },
    {
      "section_name": "Facial Expression Estimation",
      "text": "Human facial expressions can reveal information about their affective states  [311]  or cognitive load  [134] , which are crucial in human-computer interaction (HCI).\n\nThere is extensive literature, and several surveys have been developed around facial expression estimation due to its importance. Researchers have divided the human face into several regions called action units (AUs) to quantify facial expressions. Figure  6 .1 shows a few such action units defined in the literature to measure facial expressions. The activity of AUs can be measured based on binary occurrence labels (active/deactive) or quantified in terms of intensity in discrete activation levels from zero to six. The predictions of AU intensities per frame can be considered a time series. One can compare measures such as root mean square error (RMSE) and Pearson correlation coefficient (PCC) between predictions and ground-truth labels. Since the labels are discrete, it is also possible to compute the mean intraclass correlation coefficient (ICC) as a performance measure  [240] . According to the usage of the ground-truth labels, AU activities can be predicted in binary occurrence or by estimating the intensity of a specific AU. Different types of neural networks can be used based on the goal of the facial expression task. For instance, a binary classifier can predict the occurrence of activation in an AU, or its level can be predicted using a multi-class classifier or a regression model.\n\nThe presented method in this section for AU intensity estimation consists of multiple steps, starting with preprocessing for face alignment followed by training facial expression templates from data using K-SVD dictionary learning (see Figure  6 .2). Then, each input image's features are computed based on their projec-tion onto dictionary elements to compute the facial features. Support vector machine (SVM) based classifiers and regression models are trained for AU occurrence and intensity estimation based on the computed features. Dictionary-learned features improved the baseline results using conditional random fields  [276]  by 35%. However, the DL-based method achieved superior performance on an unseen test dataset reported in the 3D facial expression recognition and analysis challenge (FERA 2017)  [324] . The main practical insights of this section can be summarized as follows: 1) Preprocessing steps such as face alignment (especially when images are from different head poses) are vital for ML and DL-based approaches.\n\n2) The performance of deep learning models has exceeded the ML-based methods for a long time; however, training regression models instead of classifiers imposes much more optimization effort and hyperparameter tuning overhead in DL than in ML. 3) Not all classes are located at an equal distance in the embedding space. Hence, using a hierarchical classifier to identify the occurrence and then predict the intensity of an AU improves the performance.\n\nMore details on implementing dictionary-learned feature extraction techniques used with a support vector regression model for facial expression recognition are presented in  [8] .",
      "page_start": 104,
      "page_end": 105
    },
    {
      "section_name": "Emotion Recognition",
      "text": "Affective computing and, more specifically, human emotion recognition is an interdisciplinary field linking cognitive sciences, psychology, and computer science. It has recently attracted more attention in the context of human-computer interaction (HCI) to interpret and understand human behavior and emotions. The idea of automatic emotion recognition is useful for making various sensory measurements, including facial video, audio, and physiological signals to predict human affective status and emotions according to the changes in the sensory information.\n\nBesides discrete emotional categories such as happiness, surprise, fear, anger, and disgust, there are two continuous dimensions of arousal and valance which can express human emotions. Arousal shows a human's level of activeness and engagement in a specific situation, while valance indicates the positiveness of a human's feelings. Arousal and valance levels can define different human feelings and affective statuses. For instance, happiness and excitement share a positive valance level with low and high arousal levels, respectively. Similarly, sadness and anger are positioned on the negative side of the valance scale with low and high arousal levels  [209] . Furthermore, researchers have also figured out that the affective status of humans in social interaction has another dimension described as dominance  [126] .\n\nBased on the briefly explained theory, Ringeval et al. designed an experiment and created the RECOLA multimodal corpus for emotion recognition  [215] . The human subjects participating in this experiment were divided into two groups for interactive sessions. Then, they were presented with a task, such as surviving an air airplane crash in the middle of a jungle. The participants were given a list of tools and asked to rank the list of tools according to their preference independently. Then, the participants of each group were connected via video call to discuss their solutions and the organizers recorded the audio-visual information and physiological signals during this interactive session. According to the progress of the discussions, participants experienced different types of natural emotions with various intensities. After the interactive session, six psychologists rated the participants' emotions based on two dimensions: arousal and valance. Then, a gold standard label was computed based on the inter-rater agreements of the psychologist's ratings at every time step. Next, audio-visual and bio-physiological information processing occurred to quantify the participants' emotions automatically through the reproduction of the gold standard labels.\n\nValstar et al. processed the raw audio-visual and bio-physiological data of the RECOLA dataset into features that are ready for ML-based pipelines for prediction  [275] . They extracted local Gabor binary patterns from three orthogonal planes (LGBP-TOP)  [4]  for appearance features and extracted facial landmarks to evaluate the face geometry  [305] . Moreover, they used the COVERED toolbox to extract voice quality and spectral features from audio  [56] . Bio-physiological recordings include electrocardiogram (ECG) and electrodermal activity (EDA) signals. All bio-physiological signals pass through band-pass filtering in the preprocessing step. The ECG signals are processed to extract heart rate (HR) and its measures of variability (HRV) as features for emotion recognition  [214] . EDA signals are also decomposed to their rapid and transient component called skin conductance response (SCR) as well as a slower basal drift denoted as skin conductance level (SCL)  [52] . Valstar et al. computed four statistical features from both EDA components and their first derivative to use in the emotion recognition pipeline  [275] .\n\nThis thesis offers an ML-based pipeline for processing the features' information and creating predictions from multiple data modalities. Figure  6 .3 demonstrates this pipeline, including random forests (RF  [29] ) for training regression models, which predict the continuous labels in two dimensions of arousal and valance based on extracted features for the four data modalities of audio, video, ECG and EDA. Furthermore, since the psychologists evaluated the emotional status of the participants based on their audio-visual signals, these modalities contain more information, and RF models have created more precise and less noisy predictions using audio-visual features. Due to the precise prediction from audio-visual features, it was possible to train a recurrent model based on echo state networks (ESNs  [112] ) to refine the prediction and consider temporal information. The idea of reservoir computing inspires ESNs. They are models with time series as input (here, the sequence of arousal or valance predictions) with internal states connected by random weights. The models' output weights are trained to minimize the loss function between the internal state of the reservoir and ground-truth labels (see Figure  6 .4). The emotion recognition pipeline described in this section uses a bi-directional ESN for audio and a standard ESN for video modalities which were selected based on their performance on each data modality. The internal state of the reservoir in bi-directional ESNs depends not only on previous samples in the time series but also on the samples that follow. An alternative to ESNs are long short-term memory (LSTM  [100] ) models as well as their bi-directional version  [233] .\n\nMultimodal information fusion is the final and essential component of the emotion recognition pipelines. Different modalities contain various levels of information for arousal and valance tasks. For instance, the arousal predictions from audio are more accurate, and video modalities include more information for the valance. Hence, averaging the predictions of all modalities does not lead to opti- mal predictions. The pipeline presented in Figure  6 .4 uses Moore pseudo-inverse to minimize the mean square error of multimodal information fusion predictions and gold standard labels. Thus, the final information fusion block is a linear combination of the predictions from each modality. More details about numerical results and methodology are presented in a paper published in conjunction with the audio-visual emotion recognition challenge (AVEC)  [275] .\n\nThe most intriguing path for future research in emotion recognition is multimodal and temporal information fusion, in addition to multi-task learning. The information fusion technique presented in this section used universal weights to combine the modalities. However, the importance of different modalities can differ from time to time based on events in the audio-visual and bio-physiological signals. Thus, developing adaptive fusion techniques with attention mechanisms can significantly improve the fusion's performance. Furthermore, arousal and valance are predicted separately in this work. These tasks can be combined into a multi-task learning problem to train models that fine-tune the predictions of one emotion dimension by being aware of the other. The ratings from psychologists and gold standard labels are not necessarily temporally synchronized with physiological and audio-visual information. Tracing such temporal mismatches between features and labels is still cumbersome, which we tackle by finding the optimal shifts as a hyperparameter (see Figure  6 .5). Developing more sophisticated methods to model such temporal dependencies in high-dimensional features or video is another thrilling venue for research.",
      "page_start": 105,
      "page_end": 105
    },
    {
      "section_name": "Automated Data Analysis",
      "text": "The breakthroughs in ML and DL provide the opportunity to leverage an immense amount of data to approximate almost any function and draw arbitrary decision boundaries for classification and regression. However, the main consequence of such a large degree of freedom was the very challenging task of selecting suitable models with optimal generalization capabilities and a set of hyperparameters for a given dataset. Researchers used to tune the parameters empirically and put their inductive biases into models. With the rise of computing resources, searching architectures and hyperparameter spaces became feasible and popular. The body of literature focusing on automated ML (AutoML) and automated DL (AutoDL) is massive, and this section offers several insights found whilst researching these two subjects.",
      "page_start": 110,
      "page_end": 110
    },
    {
      "section_name": "Automated Machine Learning",
      "text": "The goal of automated machine learning is defined as solving the combined model section and hyperparameter (CASH) optimization problem. The main goal is to develop search algorithms that can adapt to the tasks based on new trials or even further leverage previous experience from other datasets (referred to as metalearning in literature  [278] ). The AutoML challenge series provided a framework to compare methods targeting the CASH problem  [94] . The idea of this challenge is to provide two independent sets of benchmark datasets for training and testing with different tasks such as regression and various types of classification. The performance of methods developed for AutoML has been evaluated with strict limitations on time and resources. Several strategies have been adapted and used in the literature to solve the CASH problem automatically. The most straightforward strategy to target the CASH problem is the random search method.\n\nAlthough it is a naive search strategy, random search can achieve competitive results for a new task when no similar dataset or problem is available (see Table  6 .1). Furthermore, it is possible to use evolutionary selection for tuning the choices of models and hyperparameters for the target datasets. The idea is to choose the subsequent models and hyperparameters based on the previous bestperforming ones. Olson et al. proposed an evolutionary algorithm for the CASH problem called the tree-based pipeline optimization tool (TPOT  [196] ).\n\nSeveral strategies have been adapted and used in the literature to solve the CASH problem automatically. The most straightforward strategy to target the CASH problem is the random search. Although it is a naive search strategy, the random search can achieve competitive results for the new task when no similar dataset or problem is available (see Table  6 .1). Furthermore, it is possible to use evolutionary selection for tuning the choices of models and hyperparameters to the target datasets. The idea is to choose the subsequent models and hyperparameters based on the previous best-performing ones. Olson et al. proposed an evolutionary algorithm for the CASH problem called the tree-based pipeline optimization tool (TPOT  [196] ).\n\nOne method used to incorporate the previous experiences from other datasets is Bayesian optimization  [77] . The idea is simple, hence practical. The idea is to consider functions of choice (commonly Gaussian processes) with free parameters defining the space drawn by hyperparameters and objective functions. Free parameters of the Gaussian processes are updated after every trial of a set of HPs, and the performance on a given dataset is computed. The parameters of Gaussian processes, in this way, learn the connections between hyperparameters and performance on the task. Models trained based on meta-learning can transfer knowledge from training datasets to new target datasets. Auto-Sklearn is an example of such a method that not only selects the best model and hyperparameters based on the target dataset but also learns from previous runs on different datasets  [76] . Table  6 .1: Performance of three automated machine learning algorithms with different paradigms on AutoML challenge datasets and their convergence time  [94]  (table adopted from  [272] ).",
      "page_start": 110,
      "page_end": 111
    },
    {
      "section_name": "Automated Deep Learning",
      "text": "The mainstream research in AutoDL presented in Section 2.1.7 focuses on developing novel vision architectures mainly based on the ImageNet dataset. However, the other open research question with more relevance to practical problems is finding the optimal architecture and set of hyperparameters for a given dataset that is not necessarily large in terms of the number of images and classes. Searching for solutions to the AutoDL problem inspired the series of AutoDL challenges to find lightweight models with hyperparameters that can quickly adapt to new but small datasets  [160] . The target of these challenges was the area under the learning curve (ALC) instead of the final or best performance. Hence, models converging faster outperform those with slow learning and better final performance based on the final evaluation metric. This evaluation metric highly favors lightweight models, which can be fine-tuned for new datasets very quickly.\n\nDeep convolutional neural networks (CNNs) outperformed the classical methods on AutoDL for vision. Due to their design, which is optimized to learn representations from a large dataset, pretraining on ImageNet is still an undeniable part of the model preparation. The performance of small models such as ResNet18  [96]  and MobileNet-V2  [225]  which have been trained on a small dataset, show consistency by changing their learning rate for a fixed pipeline; hence, a fixed learning rate can be used for different datasets (see Figure  6 .6). However, regularization shows a more critical role in optimally fine-tuning the models to small new datasets. It is no wonder that the winning solutions of AutoDL contained the fast auto-augment method to learn augmentation strategies tailored for a given dataset.\n\nResearch in developing models for audio processing falls behind vision systems with respect to lightweight architecture searched models on large audio datasets. For example, a commonly used pretrained network for audio processing is VG-Gish  [159]  trained on Youtube-8m dataset  [189] , which is far from light-weight. Hence, searching for appropriate architectures is pivotal when searching for optimal models for small datasets. Similarly, augmentation strategies are not as well explored in audio processing, and a significant boost is expected with the development of more suitable or automated augmentation techniques.\n\nDespite the differences between audio and video processing pipelines for research in AutoDL for audio-visual data, the block diagram used for pattern classification can be summarized with similar components as depicted in Figure  6 .6.\n\nPreprocessing the data is the first step which computes the spectrogram for audio data, augmentation for images, or selects key frames from videos. Then, the raw information can be processed into latent representations using convolutional backbones. Information fusion in the following steps combines the information along the axis of time via convolutions or spatially using global pooling. The last layer is a fully connected classifier to predict the patterns from the models' final embeddings. The main advantage of such a similar architecture is the possibility of mid-level information fusion between audio-visual modalities in applications such as emotion recognition, explained in Section 6.1.2.",
      "page_start": 111,
      "page_end": 112
    },
    {
      "section_name": "Chucky Decal",
      "text": "",
      "page_start": 112,
      "page_end": 112
    },
    {
      "section_name": "Medical Diagnosis And Imaging",
      "text": "This section presents two medical applications for machine and deep learning methods. First, the application of machine learning in pain detection in medicine through bio-physiological signal processing is explained. Second, data homogenization for medical images with future applications in merging datasets and image preprocessing in federated scenarios is discussed.",
      "page_start": 114,
      "page_end": 114
    },
    {
      "section_name": "Pain Estimation",
      "text": "ML and DL methods have been widely used in medical applications such as pain estimation. Pain is an evolutionary mechanism developed in human bodies to stop and prevent external damaging stimuli or harmful behaviors. However, pain also appears as a consequence of operations in clinical settings. Not all patients, such as neonates, unconscious patients, or patients with cognitive or communicative impairments, are capable of communicating the location and level of pain when seeking treatment. Hence, automatic pain detection and intensity estimation have become more popular among researchers.\n\nWerner et al. introduced the Biovid heat pain database for automatic pain estimation from bio-physiological signals  [282] . The idea of the experiment was to estimate stimulated pain using heat induced by a thermode. The experiment started with a calibration phase when the organizers measured the participants' pain perception and tolerance thresholds. Then, the experiment began with a cold thermode, which became increasingly hotter until the participant noticed the pain (perception threshold), and stopped when the heat became unbearable for a participant (tolerance threshold). Then, the temperature between these two thresholds was linearly divided into four levels, and the participants were stimulated with four pain levels during two parts of the experiment. Each part contains twenty episodes of pain stimulation with a duration of four seconds with a break of approximately eight seconds. The bio-physiological signals were recorded during the experiment for signal processing and automated pain estimation. The signals recorded in this experiment include several data modalities such as electromyography (EMG), electrocardiography (ECG), and electrodermal activities (EDA).\n\nAfter data collection, bio-physiological signals are preprocessed for feature extraction. Multiple time and frequency domain statistical features are available and computed for pain detection and pain level estimation  [118] . The key component improving the pain estimation accuracy in this stage is the extraction of the modality-dependent features, especially in electrodermal activity (EDA) signals which contain the most relevant information for pain estimation  [9] . There needs for more research on bio-physiological pain estimation in order to be able to use supervised DL methods to optimize features (embeddings) automatically instead of computing hand-crafted features. However, this shortcoming is to some extent addressed using the unsupervised representation learning for bio-physiological signals  [266] .\n\nPain estimation based on the Biovid heat database can be considered a classification or regression task. Feature preprocessing considerably affects the accuracy of pain level quantification, and normalization of the features based on their mean and variance improves the performance of classifiers and regression models. Further improvements are achieved by normalizing the features per participant based on their baseline level of bio-physiological signals (feature personalization  [118] ). Moreover, personalization extends to another level by clustering people into several groups using Kullback-Leibler (KL) divergence and finding the closest subjects to train the model based on their data  [116] . This improvement in estimating health-related measures using personalization hints at the fruitful direction of personalized information processing and treatment for research in health care. Different classifiers and regression models such as random forests (RF  [29] ) and radial basis function networks (RBFs  [31] ) showed a similar performance after tuning, and it is possible to predict the confidence of the estimated level of pain by combining the predictions from several individual models  [116] .",
      "page_start": 114,
      "page_end": 114
    },
    {
      "section_name": "Data Homogenization",
      "text": "Deep CNNs achieved great success in a wide range of computer vision tasks and improved state-of-the-art performances by a large margin; however, early on, they showed a weakness in generalization in the presence of a change in data distribution or concept drifts. This thesis offers an idea to deal with the changes in data distribution through data homogenization and merging multiple datasets. Deep learning and computer vision literature is full of attempts at domain adaptation  [49, 288, 299]  and style transfer research  [83, 326] . However, merging a few datasets into a unified style using a preprocessing network is the novelty of the idea presented in this thesis.\n\nThe research presented in this section is conducted in the context of COVID-19 detection from 2D chest computed tomography (CT) scans. This thesis presents a preprocessing network (PrepNet  [11] ) aiming at data homogenization with minimum changes in the original images. Accordingly, the proposed techniques have two main components: an autoencoder and a dataset/technology classifier. (see Figure  6 .7 ) was trained independently using the preprocessed scans by auto-encoder (figure adopted from  [11] ).\n\nwhile the dataset classifier learns the differences between the new preprocessed scans. Two networks compete against each other to improve their performance in a similar optimization as generative adversarial networks (GANs). After sufficient training with the correct set of hyperparameters, the auto-encoder learns to bring the datasets into a joint distribution that looks similar to the human eye as well as CNNs. During the optimization, we minimize the reconstruction loss of the auto-encoder to keep the scan as unchanged as possible and only focus on erasing the dataset differences and reducing probable generative artifacts.    [11] ).\n\nThe evaluation method proposed for PrepNet not only measures the intra-dataset test performance, but also focuses on cross-dataset performance. The ultimate goal of PrepNet is to homogenize datasets so that the model trained on one can be used for diagnosis on the other datasets. Two public datasets for COVID diagnosis from CT scans called SARS-COV-2  [245]  and UCSD COVID-CT  [312]  are the subjects of this study. Figure  6 .8 depicts the performance of our proposed PrepNet and visually compares its results with classical auto-encoders and other preprocessing techniques for chest CT scans. Table  6 .2 shows the performance of the models trained on the original dataset, solely preprocessed using an auto-encoder learned in a self-supervised manner on reconstruction loss and preprocessed using PrepNet. PrepNet achieved the best cross-dataset generalization amongst all the presented methods with a minor drop in intra-dataset test performance. However, the gap between cross-dataset performance and intra-dataset performance is still significant, and there is considerable room for improvement in future research.",
      "page_start": 115,
      "page_end": 116
    },
    {
      "section_name": "Face Recognition",
      "text": "Face recognition (FR) and face matching technologies, especially in surveillance applications, were probably the most controversial models developed with (CNNs) for computer vision. The idea of identity matching and verification using images was so appealing for many applications such as online banking or intelligent surveillance that the research literature around developing models and collecting datasets for FR expanded rapidly  [178] .\n\nResearchers collected clean datasets for FR in academic research developments and datasets from real-world images  [284] . Developing loss functions to compute generic embedding was a key component of extending FR to face matching on the face, which has not been seen in the training set. Triplet loss  [257, 258, 259]  and more modern loss functions such as large margin cosine loss  [285]  and arccos loss with angular margin  [59]  are amongst such developments. Despite the scientific successes in this research area, social activists raised issues concerning fairness because of biases in FR systems. This section describes the issue of fairness and presents scientific findings in the context of FR systems.",
      "page_start": 118,
      "page_end": 118
    },
    {
      "section_name": "Algorithmic Bias In Fr Systems",
      "text": "The research progress in FR technology was quick, and the models rapidly made their way into practical applications; however, multiple reports show some biases and inaccuracies against races that have fewer images in the training datasets  [99, 169, 161] . These incidences attracted much negative feedback from society, which was reflected in the news 12 . Another reaction followed this wave with companies starting to ban the FR technology 3    [138] . As a result, using the FR technology started to be abandoned, and measuring algorithmic biases became more critical after these events  [23] .",
      "page_start": 118,
      "page_end": 118
    },
    {
      "section_name": "Measuring Bias And Awareness",
      "text": "The main findings of our research are about methods of measuring and removing biases. After all the controversies regarding biases in FR technology, researchers quickly started to seek strategies for measuring the sources of such biases in FR. The pioneering research on collecting datasets with racial diversity rapidly exposed the gap in FR models' accuracy for different races, which causes limitations in service accessibility where FR technologies are involved and raises ethical issues regarding fairness.\n\nResearchers introduced racial awareness as a proxy for measuring biases in FR models and research showed that the FR models distribute the faces based on their ethnicity in the embedding space  [289] . Accordingly, several research works suggested adversarially removing the racial information as a solution to the problem of biases in FR systems  [306, 131, 314] . However, our research in measuring the biases demonstrated that the intuitive idea that racial clustering in embedding space is correlated with biases is not always true  [87] . Instead, the reason behind racial biases comes from how the faces of different races are distributed in the embedding space (see Figure  6 .9). Similarly, blinding the FR technologies from racial information in the embedding space does not necessarily lead to decreasing the racial bias  [295] . Hence, awareness and bias are two distinct though related issues in FR, and methods dealing with ethnicities individually, such as the research presented in  [217]  does, are more appealing based on these findings.\n\nVGGFace2 (  128 ) VGGFace2 (  256 ) VGGFace2 (2048)",
      "page_start": 118,
      "page_end": 119
    },
    {
      "section_name": "Rotation-Invariant Vision Transformers",
      "text": "Inductive biases such as translation invariance undeniably accelerated the rapid advances of modern vision models based on convolutions through parameter sharing and improving sample efficiency. However, state-of-the-art models can only partially incorporate rotation invariance. Recent attempts to develop rotationinvariant techniques mainly face the challenge of high memory requirements or limiting the original model capacity. This section proposes an embedding layer method for vision transformers to leverage the invariance of self-attention layers to the order of tokens and train robust models against local and global rotation.\n\nThe proposed image embedding technique requires negligible memory overhead to train rotation invariance models on large datasets such as ImageNet  [223] . Furthermore, the proposed method improves the robustness of vision transformers against rotation on the classification task.",
      "page_start": 120,
      "page_end": 120
    },
    {
      "section_name": "Introduction And Problem Statement",
      "text": "The performance of vision models, more specifically vision transforms (ViTs), drops when the input images are not presented to the models in the original pose. Rotation and scaling are two transformations that researchers found to be a reason for the decline in the performance of vision models from early works  4  . This section presents a solution to rotation invariance in ViTs for object classification. Figure  6 .10 shows the decline in the performance of a ViT-based classifier and segmentation model after different degrees of rotation. Besides compensating for the drop in accuracy to improve the robustness of vision models, developing rotation equivariant methods was very appealing to add another inductive bias to enhance the vision models' sample efficiency and convergence speed.  equivariant features  [176] . Lifting the representations or filters to the discrete Lie groups increases the memory consumption linearly with the group size. Alternatively, the convolutional filter can be designed to be equivariant to specific transformations. Esteves et al. train isotropic filters for rotation equivariant CNNs  [73]  and Weiler et al. proposed learning the models' weights which are expansion coefficients for the steerable function space  [296] . Wiersma et al. presented a surface harmonic network with both invariant and equivariant features  [298] . The main disadvantage of optimizing equivariant filters is limiting the capacity of models for learning the data.\n\nViTs: Romero and Cordonnier used the group lifting concept to train equivariant vision transformers on a discrete group of image rotations  [218] . Hutchinson et al. adapted a similar idea, generalized it to the continuous rotation and translation equivariant models, and applied their method to pattern recognition in pointcould graphs, molecular property prediction, and chasing particle dynamics  [111] . Finally, Su et al. demonstrated that rotary positional encoding enhances the performance of natural language processing models  [254] .\n\nNext, this section reviews the main blocks and concepts used in ViTs. The explanation of rotation invariant and equivariant features is followed by selfattention and formulation under input rotation.\n\nRotation: Let X be a vectorized patch of an image with a given size, for example, 16 × 16. Then, we define a rotation matrix called R such that the transformed version of the original image x θ can be computed as follows:\n\nwhere θ shows the angle of rotation. The rotation transformation is defined using a rotation matrix, and it can be computed as follows: R = X θ X † (6.\n\n2)\n\nThe goal of the roto-translation equivariant models with a self-attention layer is computing a representation (L(x)) in which the representations rotates with the same degree as the input image:\n\nL(X θ ) ≈ L(X) (6.3)\n\nAlternatively, we can consider the images as a spatial function in 2D space having three values (RGB vector) at every position and define the rotation on every pixel coordinate ((x, y)) as follows:\n\nr θ = cos θ -sin θ sin θ cos θ (6.4)\n\nGiven the definition of the rotation matrix (r θ ) based on pixel coordinates, the inverse of the rotation operator is equal to its transpose (r θ r T = I), and the following properties hold accordingly:\n\nx θ y θ = r θ x y (6.5)\n\nRotation invariance, covariance, and equivariance: A representation (L(.)) of an input pattern (X) is invariant to rotation (R) if it does not change with the rotation of the input. The equivariance representations rotate similarly with the input's rotation; However, covariant representations change according to the original representations based on a constant function (f (.)). These definitions can be shown in the following equations:\n\nInvariant : L(X θ ) ≈ L(X) Equivariant : L(X θ ) ≈ L θ (X) Covariant : L(X θ ) ≈ f (L(X)) (6.7)\n\nSelf-Attention: The output of the self-attention layer for an image ((X) ∈ IR N×T ) converted to N tokenized patches of length (T ) can be written as follows:\n\nwhere K, Q and V shot the key, query and value. The linear weights used to compute the representations are denoted by W k , W q and W v for keys, queries and values, respectively. A shows the attention matrix and Y denotes the selfattention (SA) layer. The softmax function, denoted by sof tmax, is defined as follows:\n\nsof tmax(x i ) = exp(x i ) j exp(x j ) (6.9)\n\nSelf-Attention with Rotation: Then, we can write the rotation invariant selfattention objective as follows:",
      "page_start": 120,
      "page_end": 121
    },
    {
      "section_name": "Method And Experimental Results",
      "text": "The mathematical formulation of self-attention with rotation suggests that it is possible to constrain the key, query and value matrices to make self-attention equivariant. The necessary condition is that both rotation matrices (R and R T ) can commute 5  through XW q W T k X T and its softmax. This is the necessary condition to make self-attention equivariant (L(X θ ) = R θ L(X)) which is more complicated than invariance, and it is also more appealing since equivariant features are useable in building invariant models. However, invariant models do not necessarily provide equivariant features.\n\nThe problem of equivariant self-attention is an open problem for future research. However, this thesis offers a solution to rotation-invariant ViTs based on a fundamental property of self-attention. The self-attention mechanism is invariant to the order of the tokens, meaning the representations do not change when the order of the tokens is different. Therefore, if we transform the image so that rotation only changes the order of the tokens, then the ViT based on self-attention will be invariant and robust against rotation.\n\nThe idea of rotation invariant ViTs can be realized using a radial tokenization technique presented in Figure  6 .11. The idea is to take the tokens based on the polar coordinate and extract every token from the original image. The proposed method uses pixel values on a circle's radius placed at the center of the image instead of turning patches of size 16 × 16 into tokens. Using this embedding method, only the order of the tokens changes with the input rotations, and the whole ViT stays invariant to rotation. This idea works for global rotation; however, it can also be implemented at the patch level to tackle the local rotation of images' elements, which is more critical for medical applications  [143] .\n\nSteerable Convolutions and isotropic filters inspire this section's other patch embedding techniques. Figure  6 .11 shows how isotropic patch embeddings turn the original image into patches. The idea here is to divide the original models into patches of size 16 × 16 and then sample them via circles around the center of the patch and project them into tokens afterward. Implementing the radial patch embedding technique at the patch level can train robust models against local rotations.  A ViT architecture based on Deit's baseline model  [269]  is optimized on the Im-ageNet dataset for pertaining, and initial evaluation shows the functionalities of the proposed methods. Figure  6 .12 depicts the performance of the different patch embedding methods used to improve the robustness of the ViTs against rotation. Radial and isotropic patch embeddings demonstrate considerably higher robustness against rotation compared with the original transformer. However, it is notable that training a transformer using data augmentation is a very competitive solution to the presented problem. object detection tasks with a drop in the performance compared to the original transformer method. This preliminary study shows that transformers can run in the rotation invariant mode without memory and compute overhead by adjusting the patch embedding techniques. Compared to similar methods such as group equivariant selfattention  [218] , the proposed method profits from higher memory efficiency and angular resolution. The research questions regarding evaluating rotation invariance to ViT based on the initial motivations and goal, namely sample efficiency and training speed, are still open for further investigation.\n\nDL breakthroughs and computer vision models developed based on DL revolutionized the research areas of image, video, and information processing in the last decade. Deep CNNs have become so popular that it is incredibly cumbersome and rare to find cases in which classical approaches can still outperform CNNs on academic datasets. Despite the undeniable breakthroughs, DL methods have faced arduous challenges for deployment in practical applications. This thesis discussed many such challenges and presented scientific developments to tackle these challenges. Nonetheless, there is still considerable room for further research to accelerate the entrance of DL-based techniques into practical applications. This chapter briefly summarizes the thesis, revisits the research challenges such as trustworthiness, explainability, robustness, optimization, and fairness, points out this thesis's contribution, and draws an outline for future work.",
      "page_start": 123,
      "page_end": 127
    },
    {
      "section_name": "Summary Of Thesis",
      "text": "This thesis has been motivated by the hindrances of using DL, specifically vision models, in practical applications. After describing the challenges and laying the theoretical foundation in the first two chapters, the thesis presented an alternative to classical multilayer perceptrons (MLPs) and instead uses radial basis function networks (RBFs) as classifiers for convolutional neural networks (CNNs). RBFs have been in the scientific literature for a long time. However, they have not been optimized for CNNs before because of the complications in the optimization. This thesis offered theoretical breakthroughs to adapt RBFs for CNNs to improve the robustness and interpretability of the classification.\n\nThe interpretability of CNNs has been at the center of attention in many research works recently. However, methods such as guided-backpropagation  [247]  developed in this context have mostly been used to monitor the models' behavior  [212] .\n\nThe fourth chapter of this thesis extended the idea of understanding vision models and putting them into action for debugging CNNs and detecting adversarial attacks with the hope of inspiring more such research in the future. This thesis's fifth and sixth chapters focused on ML and DL applications. Chapter 5 described how a problem without analytical solutions can be addressed using data-driven methods and simulation. It presented motion compensation in cone-beam computed tomography (CBCT) scans using 3D-CNNs. Chapter 6 reviewed several different applications of ML and DL in affective computing and health care, and pointed at the findings of this thesis targeting fairness in facial recognition systems and robustness of vision transformers (ViTs). The optimization process is crucial in bringing vision models to performance and affects their behavior in terms of robustness, generalization, and data requirements. Chapter 6 also offered findings in hyperparameter and model optimization gained by employing ML and DL in several applications and formulating the best practices and patterns in the automated search for best ML and DL models.",
      "page_start": 127,
      "page_end": 128
    },
    {
      "section_name": "Future Research Work",
      "text": "Researchers' long-term vision of applying ML and DL in medical applications and autonomous driving systems is only feasible by establishing human trust in reliable and robust artificial intelligence (AI). Thus, trustworthiness and reliability are the overarching themes in the research community for practical AI applications with maximum performance and minimum negative impact  [123] . It is intuitively clear that a model or algorithm used in applications involving human privacy or service access has to be reliable and trustworthy. Furthermore, trustworthiness is in demand in medical applications and autonomous driving systems involving human life and security.\n\nDespite the demand for trustworthiness being intellectually evident, best practices for engineering trustworthy models for a specific application is an open problem and requires further investigation  [237] . The importance of trustworthiness is also highly dependent on the application. For instance, robustness against spoofing or adversarial attacks is more relevant to person identification problems, while adaptation to the new vendors and image acquisition parameters emerge in medical image processing. Since the term trustworthiness is generic and includes many aspects, researchers break it down into several categories with more specific definitions where it is also possible to evaluate the performance based on acceptable common-sense explanations or mathematical metrics.\n\nThe long-term vision of AI research (reliability and trustworthiness) can be divided into smaller actionable blocks that current research addresses. Explainability, robustness, and fairness are the requirements of the trustworthy AI concepts investigated in this thesis. The remainder of this section explains this thesis's contributions to the components of trustworthy AI and opportunities for mid-term research in these areas.\n\nExplanability: Answering the following three questions is the target of the research around explainable AI in computer vision: 1) How do models learn? 2) What do models learn? 3) How do models predict? The first question is the most complicated to answer. The research literature addressing this area is meager, but includes studies that use information theory to explain the behavior of models during optimization  [229, 241] . The second and third questions are more pragmatic, relevant for practical applications, well-studied, and more connected to each other  [247, 85, 212] . The features learned in vision models for decisionmaking are mainly evaluated using feature visualization techniques. These techniques compute the region of input images that the models look at to make a decision based on reverting the forward path or treating the models as a black box using iterative optimization. Moreover, researchers investigated the behavior of models as black boxes via post-hoc analysis to identify why the models predict a specific class. An alternative to black-box analysis is using methods such as Bayesian inference, which are more transparent by design. The contribution of this thesis to explainable AI research is revisiting radial basis function networks (RBFs) and adapting them as classifiers for CNNs by solving a few architectural hindrances. The proposed models compute a similarity metric between test and training images and derive visual clues about the decision-making process of the vision models. This research is the first to use RBFs on top of the traditional computer vision backbones. Evaluating the robustness of models using RBF classifiers against anomalies and adversarial attacks is an open question for future research.\n\nRobustness: Researchers very quickly discovered robustness issues in computer vision models. CNN performance shows a decline in the presence of different lighting conditions and variability in the pose of the input images. The robustness problem had even more impact in the medical domain because of manual changes in image acquisition parameters, different image acquisition vendors, and frequent imaging software and hardware updates. Domain adaptation and lifelong learning in the presence of concept drift are the offsprings of the robustness and generalization issues and have tremendous exciting research potential. This thesis presented a method for data homogenization that enhances merging data from different datasets and it is practical for domain adaptation. One of the hot topics threatening the validity of CNN's for vision problems is adversarial attacks. Researchers have found that images which appear identical to the human eye can be optimized to fool vision models into making an incorrect decision. This thesis offered a method based on reverting the CNNs to visualize the models' feature response and detect adversarial attacks with very high accuracy. This research can be extended to use black-box feature visualization to detect attacks on any model and optimize the input to reduce the adversarial effects in future work. Moreover, this thesis presented a novel embedding technique for rotation invariant vision transformers to improve model robustness against input rotation.\n\nApplying rotation invariant transformers to small datasets, especially aerial images and histology datasets, to leverage the rotation invariance as inductive bias is another promising research offspring of this thesis.\n\nFairness: Neural networks became very popular because of their strength in approximating arbitrary functions for classification or regression solely from data without any knowledge of the task. Although neural networks provide the opportunity to learn with minimal inductive biases, the optimization process instead follows the most efficient direction in parameters space to minimize the optimization objective (loss function) based on the existing biases in the datasets. Using these biases helped to solve the problems that researchers had not found any analytical solution to before, such as motion artifact reduction presented in Chapter 5. However, social activists rapidly discovered the drawbacks of learning from data in the social fairness aspect of face recognition (FR) systems for surveillance. The collected datasets were biased, in that the majority of the images were of white male celebrities, which was reflected in the trained models when they returned a higher accuracy for the majority race in the datasets. Studies showed that the models produce a lower accuracy for racial minorities, and that this inequality was even visible when comparing the models' accuracy for recognizing females and children with males. This thesis offered relevant research and findings about a standard method of measuring biases in FR systems and showed that racial awareness and bias are not necessarily correlated. The research concerning fairness is also quite an exciting and simultaneously challenging area. Data-driven techniques are an option for reducing biases by collecting datasets with equal populations from all sensitive features, such as race and gender. A balanced dataset is a solution to the problem faced by FR systems. However, problems such as recruitment and job application processing confront more challenges due to biases in ground truth labels based on previous hiring decisions, which opens a lot of fascinating topics for future research.",
      "page_start": 129,
      "page_end": 130
    },
    {
      "section_name": "Practical Discussions",
      "text": "Alongside all the debates about the trustworthiness of AI models for applications where human safety and privacy are involved  [123, 2, 107] , AI-based models have also found their way into less critical applications  [250] . However, AI projects still suffer from a very high failure rate in development and post-deployment due to problems such as concept drift. This section briefly discusses the content of this thesis related to applications and optimization. Although DL-based models are unrivaled for vision problems, their performance is highly dependent on the quality of the data. Determining the mutual information of the data samples and target pattern requires further research; a visual review of the datasets before model development is the key to success in applied projects  [122] . Neural networks are applicable and highly recommended to approximate classical methods that are computationally expensive (such as the iterative reconstruction of computed tomography scans) or enhance their performance where analytical solutions do not exist (for example, in motion artifact reduction). This thesis offered an application of three-dimensional CNNs in reducing motion artifacts in volumetric cone-beam computed tomography (CBCT) scans with great success. This research path was extraordinarily successful and gained positive feedback and attention from clinical experts. The particular area of research is novel and ripe for further research in similar applications, such as sparseness artifact reduction, auto-segmentation, and dose calculation from CBCT scans for cancer therapy.\n\nOptimization: ML and DL present the opportunity to explore and search among a family of neural networks to model all possible problems in computer vision. However, this large degree of freedom appears at the expense of the vast search space of parameters and potential models. Optimization concentrates on techniques that are key to neural architecture search, hyper-parameter (HP) tuning, and finding the shortest path to a stable minimum for a given dataset and model. This thesis presented the observed patterns for model and HP optimization based on ML and DL algorithms for small datasets and proposed combining supervised and unsupervised learning to enable the optimization of RBFs as classifiers for conventional CNN architectures. So far, neural architecture search has been aimed at minimizing the number of flops and latency in inference regardless of sample efficiency. Sample efficiency is another challenge in practical applications where data or labels are scarce. Hence, architectures searched for the highest sample efficiency are critical for practical applications. Other directions for future research include limiting search space and constraining optimization techniques to more explainable and robust methods that serve the purposes of trustworthy AI. The current top-performing computer vision models are derived from automatically searched architectures that target latency optimization and disregard models' explainability. There is a belief in a trade-off between accuracy and explainability in the scientific community  [310] . The drop in accuracy occurs when predictive complexities are removed to make the models more explainable. However, another critical research piece refers to this trade-off as a myth  [222]  and encourages researchers to optimize intrinsically interpretable models to the same level of performance as black box models. Neural architecture search in the space of intrinsically interpretable and explainable models clarifies this controversial research area to show the correctness of these contradicting opinions.",
      "page_start": 130,
      "page_end": 131
    }
  ],
  "figures": [
    {
      "caption": "Figure 2: 1). Feature maps are the",
      "page": 35
    },
    {
      "caption": "Figure 2: 2, is independent",
      "page": 35
    },
    {
      "caption": "Figure 2: 2: Max-pooling (MP) and average-pooling (AP) layers with kernel size and",
      "page": 35
    },
    {
      "caption": "Figure 2: 3, consist of an input, an output, and several hidden layers. Each",
      "page": 35
    },
    {
      "caption": "Figure 2: 3 and trainable",
      "page": 36
    },
    {
      "caption": "Figure 2: 3 illustrates a simple convolutional network. The original image’s pixel values",
      "page": 36
    },
    {
      "caption": "Figure 2: 3: A convolutional neural network for representation learning from an input",
      "page": 36
    },
    {
      "caption": "Figure 2: 4 from the original",
      "page": 37
    },
    {
      "caption": "Figure 2: 4: The residual connection between a layer’s input and output improves the",
      "page": 37
    },
    {
      "caption": "Figure 2: 5b. As a result, the inception models improved",
      "page": 38
    },
    {
      "caption": "Figure 2: 5: This figure, adopted from [260], depicts the idea of the inception blocks",
      "page": 38
    },
    {
      "caption": "Figure 2: 6a, is to compute a channel and a spatial atten-",
      "page": 38
    },
    {
      "caption": "Figure 2: 6. To compute the channel attention maps (see Figure 2.6b), we first",
      "page": 39
    },
    {
      "caption": "Figure 2: 6: Convolutional block attention module (CBAM) with its two main com-",
      "page": 39
    },
    {
      "caption": "Figure 2: 7: Two- and three-dimensional convolutions. 2D convolutions target images,",
      "page": 41
    },
    {
      "caption": "Figure 2: 8, consists of a transformer",
      "page": 44
    },
    {
      "caption": "Figure 2: 9 shows how pre-trained ViTs for classification can",
      "page": 45
    },
    {
      "caption": "Figure 2: 9: Vision transformers (ViTs) for image semantic segmentation (figure is",
      "page": 46
    },
    {
      "caption": "Figure 3: 1: Figures on the top and bottom rows visualize the position of a test image",
      "page": 54
    },
    {
      "caption": "Figure 3: 2: Activation functions for RBF networks. Here is the list of the parameters",
      "page": 59
    },
    {
      "caption": "Figure 3: 1. The deep embeddings of the CNNs, computed using",
      "page": 61
    },
    {
      "caption": "Figure 3: 1 profit from using the state-of-",
      "page": 62
    },
    {
      "caption": "Figure 3: 2 alongside the conventional activation",
      "page": 62
    },
    {
      "caption": "Figure 3: 3 shows the hyperparameter search results for object classification on",
      "page": 63
    },
    {
      "caption": "Figure 3: 1 has two further hyperparameters: the number of cluster centers and",
      "page": 63
    },
    {
      "caption": "Figure 3: 3 are conducted using the hyperband [150]",
      "page": 63
    },
    {
      "caption": "Figure 3: 3 show that training",
      "page": 63
    },
    {
      "caption": "Figure 3: 3: Hyperparameter search results from CIFAR-10 (top) and CIFAR-100 (bot-",
      "page": 64
    },
    {
      "caption": "Figure 3: 4 demonstrates the evolution of the representations around the cluster",
      "page": 65
    },
    {
      "caption": "Figure 3: 4 are placed",
      "page": 65
    },
    {
      "caption": "Figure 3: 4: This figure presents the location of data samples compared to the cluster",
      "page": 66
    },
    {
      "caption": "Figure 3: 4. To reduce the overlap between close",
      "page": 66
    },
    {
      "caption": "Figure 3: 5 illustrates the two-dimensional mapping of the CNN embeddings (top",
      "page": 67
    },
    {
      "caption": "Figure 3: 5). The activation",
      "page": 67
    },
    {
      "caption": "Figure 3: 6 shows the similar images found in the training",
      "page": 67
    },
    {
      "caption": "Figure 3: 6: This figure depicts similar and dissimilar training images for given test",
      "page": 68
    },
    {
      "caption": "Figure 3: 7 compares the performance of the similar sample selection for given",
      "page": 69
    },
    {
      "caption": "Figure 3: 7: The presented figure visualizes the top 14 images selected using differ-",
      "page": 69
    },
    {
      "caption": "Figure 3: 8 shows training samples and their distances from",
      "page": 69
    },
    {
      "caption": "Figure 3: 8 depicts the clusters with the highest contributions to",
      "page": 69
    },
    {
      "caption": "Figure 3: 8: The Figure illustrates the clusters contributing to a CNN-RBF network’s",
      "page": 70
    },
    {
      "caption": "Figure 4: 1: Examples of different state-of-the-art adversarial attacks on a VGG19",
      "page": 75
    },
    {
      "caption": "Figure 4: 1). Moreover, the information",
      "page": 77
    },
    {
      "caption": "Figure 4: 1. The general phenomenon observed in all images is that the",
      "page": 78
    },
    {
      "caption": "Figure 4: 1 demonstrates that the network looks at a smaller region",
      "page": 78
    },
    {
      "caption": "Figure 4: 1 demonstrates this connection for models’",
      "page": 78
    },
    {
      "caption": "Figure 4: 2). The feature response",
      "page": 78
    },
    {
      "caption": "Figure 4: 2, and their difference for clean and adversarial examples suggests a",
      "page": 79
    },
    {
      "caption": "Figure 4: 2. The proposed detector makes the final decision by comparing",
      "page": 79
    },
    {
      "caption": "Figure 4: 2 shows that the clean images are",
      "page": 80
    },
    {
      "caption": "Figure 4: 2: a) Distribution of average local spatial entropy in clean images (green)",
      "page": 80
    },
    {
      "caption": "Figure 4: 2b presents the receiver operating characteristics (ROC) of the proposed",
      "page": 80
    },
    {
      "caption": "Figure 4: 3 illustrates these experimental results.",
      "page": 82
    },
    {
      "caption": "Figure 5: 1 shows an example of typical motion",
      "page": 89
    },
    {
      "caption": "Figure 5: 1: Motion Artifacts. Left: CBCT scans with motion artifacts from the test",
      "page": 89
    },
    {
      "caption": "Figure 5: 2: The architecture of the proposed dual-domain model for end-to-end opti-",
      "page": 91
    },
    {
      "caption": "Figure 5: 2, combines",
      "page": 92
    },
    {
      "caption": "Figure 5: 2 uses two components to enhance the gradient flow and",
      "page": 92
    },
    {
      "caption": "Figure 5: 2. A plain UNet [219] architecture without residual connections is al-",
      "page": 93
    },
    {
      "caption": "Figure 5: 2) in a supervised scenario,",
      "page": 94
    },
    {
      "caption": "Figure 5: 3: Example results for FDK reconstruction (volume domain optimization).",
      "page": 96
    },
    {
      "caption": "Figure 5: 5). Finally, the performance of the motion",
      "page": 97
    },
    {
      "caption": "Figure 5: 4: Example results for FDK reconstruction (volume domain optimization).",
      "page": 98
    },
    {
      "caption": "Figure 5: 5: The table shows example results for iCBCT reconstruction for real-world",
      "page": 99
    },
    {
      "caption": "Figure 6: 1 shows a few such action units defined in the literature to",
      "page": 104
    },
    {
      "caption": "Figure 6: 1: Several action units (AUs) used for facial expression estimation (figure is",
      "page": 104
    },
    {
      "caption": "Figure 6: 2: Several dictionary-learned facial templates used for facial feature extrac-",
      "page": 105
    },
    {
      "caption": "Figure 6: 3 demonstrates",
      "page": 107
    },
    {
      "caption": "Figure 6: 3: Presented is the proposed sequence of blocks for the automatic fusion of",
      "page": 107
    },
    {
      "caption": "Figure 6: 4: Presented are the echo state networks (ESNs) based architectures for",
      "page": 108
    },
    {
      "caption": "Figure 6: 4 uses Moore pseudo-inverse",
      "page": 108
    },
    {
      "caption": "Figure 6: 5). Developing more sophisti-",
      "page": 109
    },
    {
      "caption": "Figure 6: 5: The figure depicts the temporal mismatch between audio features and gold",
      "page": 109
    },
    {
      "caption": "Figure 6: 6: Performance of four different vision datasets in terms of ALC of Mo-",
      "page": 113
    },
    {
      "caption": "Figure 6: 7). The autoencoder-based CNN aims to find common ground for all",
      "page": 115
    },
    {
      "caption": "Figure 6: 7: The proposed architecture for PrepNet model with three modules: (i) an",
      "page": 116
    },
    {
      "caption": "Figure 6: 8 depicts the performance of our pro-",
      "page": 117
    },
    {
      "caption": "Figure 6: 8: Original images from the datasets with different prepossessing methods",
      "page": 117
    },
    {
      "caption": "Figure 6: 9). Similarly, blinding the FR technologies",
      "page": 119
    },
    {
      "caption": "Figure 6: 9: Probability density distribution of pairwise (Euclidean and Cosine) dis-",
      "page": 119
    },
    {
      "caption": "Figure 6: 10 shows the decline in the performance of a ViT-based classifier and",
      "page": 120
    },
    {
      "caption": "Figure 6: 10: Classification and segmentation performance of vision transformers un-",
      "page": 121
    },
    {
      "caption": "Figure 6: 11. The idea is to take the tokens based on the",
      "page": 123
    },
    {
      "caption": "Figure 6: 11 shows how isotropic patch embeddings turn the",
      "page": 124
    },
    {
      "caption": "Figure 6: 11: The proposed patch embedding methods for vision transformers: a)",
      "page": 124
    },
    {
      "caption": "Figure 6: 12 depicts the performance of the different patch",
      "page": 124
    },
    {
      "caption": "Figure 6: 12: Robust training against rotation using rotation invariant patch embed-",
      "page": 125
    }
  ],
  "tables": [
    {
      "caption": "Table 3: 3: Comparing the performance of various CNN-RBF architectures with pre-",
      "data": [
        {
          "Backbone": "No-Augment\nAuto-Augment",
          "CNN-RBFs\nEfficientNet-B0 InceptionV2 ResNet50": "0.966 0.963 0.969\n0.975 0.977 0.942"
        },
        {
          "Backbone": "No-Augment\nAuto-Augment",
          "CNN-RBFs\nEfficientNet-B0 InceptionV2 ResNet50": "0.797 0.752 0.693\n0.822 0.805 0.778"
        },
        {
          "Backbone": "No-Augment\nAuto-Augment",
          "CNN-RBFs\nEfficientNet-B0 InceptionV2 ResNet50": "0.840 0.804 0.622\n0.887 0.820 0.829"
        },
        {
          "Backbone": "No-Augment\nAuto-Augment",
          "CNN-RBFs\nEfficientNet-B0 InceptionV2 ResNet50": "0.609 0.659 0.595\n0.828 0.757 0.667"
        },
        {
          "Backbone": "No-Augment\nAuto-Augment",
          "CNN-RBFs\nEfficientNet-B0 InceptionV2 ResNet50": "0.723 0.717 0.665\n0.842 0.843 0.828"
        },
        {
          "Backbone": "No-Augment\nAuto-Augment",
          "CNN-RBFs\nEfficientNet-B0 InceptionV2 ResNet50": "0.613 0.428 0.281\n0.618 0.587 0.503"
        }
      ],
      "page": 65
    },
    {
      "caption": "Table 4: 3: The table describes the numerical evaluation of detection performance",
      "data": [
        {
          "#Images\n(runtime[days])": "50,014(3)\n50,014(15)\n50,014(32)\n47,858(42)\n4,013(17)",
          "Successrate": "0.925\n0.499\n0.620\n0.606\n0.940",
          "Groundtruth\nconfidence": "0.022\n0.052\n0.037\n0.041\n0.023",
          "Targetclass\nconfidence": "0.588\n0.371\n0.463\n0.446\n0.583"
        }
      ],
      "page": 81
    },
    {
      "caption": "Table 4: 4: This table describes the performance of similar state-of-the-art adversarial",
      "data": [
        {
          "Dataset": "SVHN[139]\nImageNet(4classes)\nImageNet-1000\nMNIST\nImageNetvalidation",
          "Network": "LeNet[147]\nCaffeNet\nVGG19\nSelf-designed\nVGG19",
          "Attack": "FGSM\nDeepFool\nSeveralattacks\nFGSM(ϵ=0.3)\nSeveralattacks"
        }
      ],
      "page": 81
    },
    {
      "caption": "Table 5: 2: Results of the clinical evaluation. This table shows the preferences for",
      "data": [
        {
          "AverageVolume\nCNN(%) Equal(%) Standard(%)": "74.00 26.00 -\n49.33 22.00 28.67\n23.00 12.67 64.33\n24.33 14.67 61.00"
        }
      ],
      "page": 100
    },
    {
      "caption": "Table 6: 1: Performanceofthreeautomatedmachinelearningalgorithmswithdifferent",
      "data": [
        {
          "0.7119 55.0\n0.7146 99.4\n0.8751 201.2\n0.8665 77.5\n0.2103 190.2\n0.8371 24.1\n0.7686 48.3\n0.7406 56.3\n0.9233 28.9\n0.8154 122.3": "0.7463 90.31",
          "0.7327 54.9\n0.7392 99.3\n0.9542 201.2\n0.8908 77.4\n0.3235 216.4\n0.8214 24.0\n0.8896 48.2\n0.7634 56.2\n0.9350 28.9\n0.8880 122.2": "0.7938 92.85"
        }
      ],
      "page": 111
    },
    {
      "caption": "Table 6: 2 shows the perfor-",
      "data": [
        {
          "COVID": "Negative"
        },
        {
          "COVID": "Positive"
        },
        {
          "COVID": "Negative"
        },
        {
          "COVID": "Positive"
        }
      ],
      "page": 117
    },
    {
      "caption": "Table 6: 3: The performance of rotation invariant vision transformers on several vision",
      "data": [
        {
          "Accuracy": "top1\ntop5",
          "BaseVisionTransormer\nOriginal Isotropic Radial LocalRadial": "0.9305 0.6890 0.8575 0.8133\n0.9926 0.9302 0.9839 0.9725"
        },
        {
          "Accuracy": "top1\ntop5",
          "BaseVisionTransormer\nOriginal Isotropic Radial LocalRadial": "0.9167 0.8000 0.8510 0.8402\n0.9696 0.9186 0.9461 0.9461"
        },
        {
          "Accuracy": "top1\ntop5",
          "BaseVisionTransormer\nOriginal Isotropic Radial LocalRadial": "0.7570 0.3378 0.5794 0.5425\n0.9355 0.6439 0.8599 0.8428"
        },
        {
          "Accuracy": "top1\ntop5",
          "BaseVisionTransormer\nOriginal Isotropic Radial LocalRadial": "0.7960 0.5221 0.6933 0.6262\n0.9462 0.7886 0.8952 0.8714"
        },
        {
          "Accuracy": "top1\ntop5",
          "BaseVisionTransormer\nOriginal Isotropic Radial LocalRadial": "0.7884 0.5698 0.7091 0.6276\n0.9370 0.7813 0.8919 0.8364"
        }
      ],
      "page": 125
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "mm), and the volume size is 256 × 256 × 48 voxels (2 × 2 × 3 mm). The source-to-imager distance is 154 cm with a detector offset of 17",
      "venue": "mm), and the volume size is 256 × 256 × 48 voxels (2 × 2 × 3 mm). The source-to-imager distance is 154 cm with a detector offset of 17"
    },
    {
      "citation_id": "2",
      "title": "Defense Advanced Research Projects Agency. Broad agency announcement: Explainable artificial intelligence (XAI)",
      "year": "2016",
      "venue": "Defense Advanced Research Projects Agency. Broad agency announcement: Explainable artificial intelligence (XAI)"
    },
    {
      "citation_id": "3",
      "title": "Intelligent driver drowsiness detection for traffic safety based on multi CNN deep model and facial subsampling",
      "authors": [
        "Muneeb Ahmed",
        "Sarfaraz Masood",
        "Musheer Ahmad",
        "Ahmed El-Latif"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Intelligent Transportation Systems"
    },
    {
      "citation_id": "4",
      "title": "Threat of adversarial attacks on deep learning in computer vision: A survey",
      "authors": [
        "Naveed Akhtar",
        "Ajmal Mian"
      ],
      "year": "2018",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "5",
      "title": "Local gabor binary patterns from three orthogonal planes for automatic facial expression recognition",
      "authors": [
        "R Timur",
        "Michel Almaev",
        "Valstar"
      ],
      "year": "2013",
      "venue": "Proceedings of the Humaine Association Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "6",
      "title": "Review of deep learning: concepts, CNN architectures, challenges, applications, future directions",
      "authors": [
        "Laith Alzubaidi",
        "Jinglan Zhang",
        "Amjad Humaidi",
        "Ayad Al-Dujaili",
        "Ye Duan",
        "Omran Al-Shamma",
        "José Santamaría",
        "Mohammed Fadhel",
        "Muthana Al-Amidie",
        "Laith Farhan"
      ],
      "year": "2021",
      "venue": "Journal of big Data"
    },
    {
      "citation_id": "7",
      "title": "Explainability for artificial intelligence in healthcare: a multidisciplinary perspective",
      "authors": [
        "Julia Amann",
        "Alessandro Blasimme",
        "Effy Vayena",
        "Dietmar Frey",
        "Vince Madai"
      ],
      "year": "2020",
      "venue": "BMC Medical Informatics and Decision Making"
    },
    {
      "citation_id": "8",
      "title": "Fuad Ismail, and Ashrani Aizzuddin Abd Rahni. Evaluation methodology for respiratory signal extraction from clinical cone-beam CT (CBCT) using data-Bibliography driven methods",
      "authors": [
        "Adam Tan",
        "Mohd Amin",
        "Siti Salasiah Mokri",
        "Rozilawati Ahmad"
      ],
      "year": "2021",
      "venue": "International Journal of Integrated Engineering"
    },
    {
      "citation_id": "9",
      "title": "Support vector regression of sparse dictionary-based features for view-independent action unit intensity estimation",
      "authors": [
        "Mohammadreza Amirian",
        "Markus Kächele",
        "Günther Palm",
        "Friedhelm Schwenker"
      ],
      "year": "2017",
      "venue": "Proceedings of the 12th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "10",
      "title": "Using radial basis function neural networks for continuous and discrete pain estimation from bio-physiological signals",
      "authors": [
        "Mohammadreza Amirian",
        "Markus Kächele",
        "Friedhelm Schwenker"
      ],
      "year": "2016",
      "venue": "Friedhelm Schwenker, Hazem M. Abbas, Neamat El Gayar, and Edmondo Trentin"
    },
    {
      "citation_id": "11",
      "title": "Continuous multimodal human affect estimation using echo state networks",
      "authors": [
        "Mohammadreza Amirian",
        "Markus Kächele",
        "Patrick Thiam",
        "Viktor Kessler",
        "Friedhelm Schwenker"
      ],
      "year": "2016",
      "venue": "Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge, AVEC '16"
    },
    {
      "citation_id": "12",
      "title": "PrepNet: A convolutional auto-encoder to homogenize CT scans for cross-dataset medical image analysis",
      "authors": [
        "Mohammadreza Amirian",
        "Javier Montoya-Zegarra",
        "Jonathan Gruss",
        "Yves Stebler",
        "Ahmet Bozkir",
        "Marco Calandri",
        "Friedhelm Schwenker",
        "Thilo Stadelmann"
      ],
      "venue": "2021 14th International Congress on Image and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "Mitigation of motion-induced artifacts in cone beam computed tomography using deep convolutional neural networks",
      "authors": [
        "Mohammadreza Amirian",
        "Javier Montoya-Zegarra",
        "Ivo Herzig",
        "Peter Hotz",
        "Lukas Lichtensteiger",
        "Marco Morf",
        "Alexander Züst",
        "Pascal Paysan",
        "Igor Peterlik",
        "Stefan Scheib"
      ],
      "year": "2023",
      "venue": "Medical Physics"
    },
    {
      "citation_id": "14",
      "title": "Efficient deep cnns for cross-modal automated computer vision under time and space constraints",
      "authors": [
        "Mohammadreza Amirian",
        "Katharina Rombach",
        "Lukas Tuggener",
        "Frank-Peter Schilling",
        "Thilo Stadelmann"
      ],
      "year": "2019",
      "venue": "European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD)"
    },
    {
      "citation_id": "15",
      "title": "Radial basis function networks for convolutional neural networks to learn similarity distance metric and improve interpretability",
      "authors": [
        "Mohammadreza Amirian",
        "Friedhelm Schwenker"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "16",
      "title": "Trace and detect adversarial attacks on CNNs using feature response maps",
      "authors": [
        "Friedhelm Mohammadreza Amirian",
        "Thilo Schwenker",
        "Stadelmann"
      ],
      "year": "2018",
      "venue": "IAPR Workshop on Artificial Neural Networks in Pattern Recognition, Lecture Notes in Computer Science"
    },
    {
      "citation_id": "17",
      "title": "Two to trust: Automl for safe modelling and interpretable deep learning for robustness",
      "authors": [
        "Mohammadreza Amirian",
        "Lukas Tuggener",
        "Ricardo Chavarriaga",
        "Yvan Putra Satyawan",
        "Frank-Peter Schilling",
        "Friedhelm Schwenker",
        "Thilo Stadelmann"
      ],
      "year": "2021",
      "venue": "Trustworthy AI -Integrating Learning, Optimization and Reasoning"
    },
    {
      "citation_id": "18",
      "title": "Cluster Analysis for Applications",
      "authors": [
        "Michael R Anderberg"
      ],
      "year": "2014",
      "venue": "Cluster Analysis for Applications"
    },
    {
      "citation_id": "19",
      "title": "Simultaneous algebraic reconstruction technique (SART): A superior implementation of the ART algorithm",
      "authors": [
        "Anders Andersen",
        "Avinash Kak"
      ],
      "year": "1984",
      "venue": "Ultrasonic Imaging"
    },
    {
      "citation_id": "20",
      "title": "Provable bounds for learning some deep representations",
      "authors": [
        "Sanjeev Arora",
        "Aditya Bhaskara",
        "Rong Ge",
        "Tengyu Ma"
      ],
      "year": "2014",
      "venue": "Proceedings of the 31th International Conference on Machine Learning, ICML 2014"
    },
    {
      "citation_id": "21",
      "title": "Explainable artificial intelligence for autonomous driving: A comprehensive overview and field guide for future research directions",
      "authors": [
        "Shahin Atakishiyev",
        "Mohammad Salameh",
        "Hengshuai Yao",
        "Randy Goebel"
      ],
      "year": "2021",
      "venue": "Computing Research Repository"
    },
    {
      "citation_id": "22",
      "title": "Neural machine translation by jointly learning to align and translate",
      "authors": [
        "Dzmitry Bahdanau",
        "Kyunghyun Cho",
        "Yoshua Bengio"
      ],
      "year": "2015",
      "venue": "Proceedings of the 3rd International Conference on Learning Representations"
    },
    {
      "citation_id": "23",
      "title": "Are transformers more robust than CNNs?",
      "authors": [
        "Yutong Bai",
        "Jieru Mei",
        "Alan Yuille",
        "Cihang Xie"
      ],
      "year": "2021",
      "venue": "Proceedings of the Advances in Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "24",
      "title": "AI fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias",
      "authors": [
        "R Bellamy",
        "K Dey",
        "M Hind",
        "S Hoffman",
        "S Houde",
        "K Kannan",
        "P Lohia",
        "J Martino",
        "S Mehta",
        "A Mojsilovic",
        "S Nagar",
        "K Natesan Ramamurthy",
        "J Richards",
        "D Saha",
        "P Sattigeri",
        "M Singh",
        "K Varshney",
        "Y Zhang"
      ],
      "year": "2019",
      "venue": "IBM Journal of Research and Development"
    },
    {
      "citation_id": "25",
      "title": "Benchmark analysis of representative deep neural network architectures",
      "authors": [
        "Simone Bianco",
        "Remi Cadene",
        "Luigi Celona",
        "Paolo Napoletano"
      ],
      "year": "2018",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "26",
      "title": "CT artifacts: causes and reduction techniques",
      "authors": [
        "Franz Boas",
        "Dominik Fleischmann"
      ],
      "year": "2012",
      "venue": "Imaging in Medicine"
    },
    {
      "citation_id": "27",
      "title": "Visu-alBackProp: Efficient visualization of CNNs for autonomous driving",
      "authors": [
        "Mariusz Bojarski",
        "Anna Choromanska",
        "Krzysztof Choromanski",
        "Bernhard Firner",
        "Larry Ackel",
        "Urs Muller",
        "Phil Yeres",
        "Karol Zieba"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 IEEE International Conference on Robotics and Automation (ICRA)"
    },
    {
      "citation_id": "28",
      "title": "End to end learning for self-driving cars",
      "authors": [
        "Mariusz Bojarski",
        "Davide Testa",
        "Daniel Dworakowski",
        "Bernhard Firner",
        "Beat Flepp",
        "Prasoon Goyal",
        "Lawrence Jackel",
        "Mathew Monfort",
        "Urs Muller",
        "Jiakai Zhang",
        "Xin Zhang",
        "Jake Zhao",
        "Karol Zieba"
      ],
      "year": "2016",
      "venue": "Computing Research Repository (CoRR)"
    },
    {
      "citation_id": "29",
      "title": "Man is to computer programmer as woman is to homemaker? debiasing word embeddings",
      "authors": [
        "Tolga Bolukbasi",
        "Kai-Wei Chang",
        "James Zou",
        "Venkatesh Saligrama",
        "Adam Kalai"
      ],
      "year": "2016",
      "venue": "Proceedings of the Advances in Neural Information Processing Systems (NIPS)"
    },
    {
      "citation_id": "30",
      "title": "Random forests",
      "authors": [
        "Leo Breiman"
      ],
      "year": "2001",
      "venue": "Machine Learning"
    },
    {
      "citation_id": "31",
      "title": "Decision-based adversarial attacks: Reliable attacks against black-box machine learning models",
      "authors": [
        "Wieland Brendel",
        "Jonas Rauber",
        "Matthias Bethge"
      ],
      "year": "2018",
      "venue": "6th International Conference on Learning Representations"
    },
    {
      "citation_id": "32",
      "title": "Multivariable functional interpolation and adaptive networks, complex systems",
      "authors": [
        "David Broomhead",
        "David Lowe"
      ],
      "year": "1988",
      "venue": "Complex Systems"
    },
    {
      "citation_id": "33",
      "title": "Language models are few-shot learners",
      "authors": [
        "Tom Brown",
        "Benjamin Mann",
        "Nick Ryder",
        "Melanie Subbiah",
        "Jared Kaplan",
        "Prafulla Dhariwal",
        "Arvind Neelakantan",
        "Pranav Shyam",
        "Girish Sastry",
        "Amanda Askell"
      ],
      "year": "2020",
      "venue": "editors, Proceedings of the Advances in Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "34",
      "title": "Analysis of explainers of black box deep neural networks for computer vision: A survey",
      "authors": [
        "Vanessa Buhrmester",
        "David Münch",
        "Michael Arens"
      ],
      "year": "2021",
      "venue": "Machine Learning and Knowledge Extraction"
    },
    {
      "citation_id": "35",
      "title": "Computed Tomography: From Photon Statistics to Modern Cone-Beam CT",
      "authors": [
        "Thorsten Buzug"
      ],
      "year": "2008",
      "venue": "Computed Tomography: From Photon Statistics to Modern Cone-Beam CT"
    },
    {
      "citation_id": "36",
      "title": "VGGFace2: A dataset for recognising faces across pose and age",
      "authors": [
        "Qiong Cao",
        "Li Shen",
        "Weidi Xie",
        "M Omkar",
        "Andrew Parkhi",
        "Zisserman"
      ],
      "year": "2018",
      "venue": "Proceedings of the 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)"
    },
    {
      "citation_id": "37",
      "title": "Interpretability of deep learning models: A survey of results",
      "authors": [
        "Supriyo Chakraborty",
        "Richard Tomsett",
        "Ramya Raghavendra",
        "Daniel Harborne",
        "Moustafa Alzantot",
        "Federico Cerutti",
        "Mani Srivastava",
        "Alun Preece",
        "Simon Julier",
        "M Raghuveer",
        "Troy Rao",
        "Dave Kelley",
        "Murat Braines",
        "Christopher Sensoy",
        "Prudhvi Willis",
        "Gurram"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computed, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation"
    },
    {
      "citation_id": "38",
      "title": "An efficient blood vessel detection algorithm for retinal images using local entropy thresholding",
      "authors": [
        "Thitiporn Chanwimaluang",
        "Guoliang Fan"
      ],
      "year": "2003",
      "venue": "Proceedings of the International Symposium on Circuits and Systems IS-CAS"
    },
    {
      "citation_id": "39",
      "title": "AirNet: Fused analytical and iterative reconstruction with deep neural network regularization for sparse-data CT",
      "authors": [
        "Gaoyu Chen",
        "Xiang Hong",
        "Qiaoqiao Ding",
        "Yi Zhang",
        "Hu Chen",
        "Shujun Fu",
        "Yunsong Zhao",
        "Xiaoqun Zhang",
        "Hui Ji",
        "Ge Wang",
        "Qiu Huang",
        "Hao Gao"
      ],
      "year": "2020",
      "venue": "Medical Physics"
    },
    {
      "citation_id": "40",
      "title": "4d-AirNet: a temporally-resolved CBCT slice reconstruction method synergizing analytical and iterative method with deep learning",
      "authors": [
        "Gaoyu Chen",
        "Yunsong Zhao",
        "Qiu Huang",
        "Hao Gao"
      ],
      "year": "2020",
      "venue": "Physics in Medicine & Biology"
    },
    {
      "citation_id": "41",
      "title": "Practical identification of NARMAX models using radial basis functions",
      "authors": [
        "Sheng Chen",
        "Steve Billings",
        "Colin Cowan",
        "Peter Grant"
      ],
      "year": "1990",
      "venue": "International Journal of Control"
    },
    {
      "citation_id": "42",
      "title": "When deep learning meets metric learning: Remote sensing image scene classifica-tion via learning discriminative CNNs",
      "authors": [
        "Gong Cheng",
        "Ceyuan Yang",
        "Xiwen Yao",
        "Lei Guo",
        "Junwei Han"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Geoscience and Remote Sensing"
    },
    {
      "citation_id": "43",
      "title": "Multi-column deep neural networks for image classification",
      "authors": [
        "Dan Ciregan",
        "Ueli Meier",
        "Jürgen Schmidhuber"
      ],
      "year": "2012",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)"
    },
    {
      "citation_id": "44",
      "title": "A committee of neural networks for traffic sign classification",
      "authors": [
        "Dan Cireşan",
        "Ueli Meier",
        "Jonathan Masci",
        "Jürgen Schmidhuber"
      ],
      "year": "2011",
      "venue": "The 2011 International Joint Conference on Neural Networks"
    },
    {
      "citation_id": "45",
      "title": "Houdini: fooling deep structured visual and speech recognition models with adversarial examples",
      "authors": [
        "Moustapha Cisse",
        "Yossi Adi",
        "Natalia Neverova",
        "Joseph Keshet"
      ],
      "year": "2017",
      "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS'17"
    },
    {
      "citation_id": "46",
      "title": "Group equivariant convolutional networks",
      "authors": [
        "Taco Cohen",
        "Max Welling"
      ],
      "year": "2016",
      "venue": "Proceedings of the 33nd International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "47",
      "title": "Support-vector networks",
      "authors": [
        "Corinna Cortes",
        "Vladimir Vapnik"
      ],
      "year": "1995",
      "venue": "Machine Learning"
    },
    {
      "citation_id": "48",
      "title": "Label-free explainability for unsupervised models",
      "authors": [
        "Jonathan Crabbé",
        "Mihaela Van Der Schaar"
      ],
      "year": "2022",
      "venue": "Proceedings of the International Conference on Machine Learning, ICML"
    },
    {
      "citation_id": "49",
      "title": "Artificial intelligence's white guy problem",
      "authors": [
        "Kate Crawford"
      ],
      "year": "2022",
      "venue": "Artificial intelligence's white guy problem"
    },
    {
      "citation_id": "50",
      "title": "A comprehensive survey on domain adaptation for visual applications",
      "authors": [
        "Gabriela Csurka"
      ],
      "year": "2017",
      "venue": "Domain Adaptation in Computer Vision Applications, Advances in Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "51",
      "title": "Learning augmentation strategies from data",
      "authors": [
        "D Ekin",
        "Barret Cubuk",
        "Dandelion Zoph",
        "Vijay Mane",
        "Vasudevan",
        "V Quoc",
        "Le",
        "Autoaugment"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "52",
      "title": "Radiotherapy respiratory motion management in hepatobiliary and pancreatic malignancies: a systematic review of patient factors influencing effectiveness of motion reduction with abdominal compression",
      "authors": [
        "Mairead Daly",
        "Alan Mcwilliam",
        "Ganesh Radhakrishna",
        "Ananya Choudhury",
        "Cynthia Eccles"
      ],
      "year": "2022",
      "venue": "Acta Oncologica"
    },
    {
      "citation_id": "53",
      "title": "The electrodermal system",
      "authors": [
        "Michael Dawson",
        "Anne Schell",
        "Diane Filion"
      ],
      "year": "2017",
      "venue": "Handbook of Psychophysiology"
    },
    {
      "citation_id": "54",
      "title": "The mahalanobis distance. Chemometrics and intelligent laboratory systems",
      "authors": [
        "Roy De Maesschalck",
        "Delphine Jouan-Rimbaud",
        "Désiré Massart"
      ],
      "year": "2000",
      "venue": "The mahalanobis distance. Chemometrics and intelligent laboratory systems"
    },
    {
      "citation_id": "55",
      "title": "Classification of mammograms using convolutional neural network based feature extraction",
      "authors": [
        "Taye Girma Debelee",
        "Mohammadreza Amirian",
        "Achim Ibenthal",
        "Günther Palm",
        "Friedhelm Schwenker"
      ],
      "year": "2017",
      "venue": "International Conference on Information and Communication Technology for Develoment for Africa"
    },
    {
      "citation_id": "56",
      "title": "Classification of mammograms using texture and cnn based extracted features",
      "authors": [
        "Taye Girma Debelee",
        "Abrham Gebreselasie",
        "Friedhelm Schwenker"
      ],
      "year": "2019",
      "venue": "Biomaterials and Biomedical Engineering"
    },
    {
      "citation_id": "57",
      "title": "COVAREP -a collaborative voice analysis repository for speech technologies",
      "authors": [
        "Gilles Degottex",
        "John Kane",
        "Thomas Drugman",
        "Tuomo Raitio",
        "Stefan Scherer"
      ],
      "year": "2014",
      "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "58",
      "title": "The numerical solution of fokker-planck equation with radial basis functions (RBFs) based on the meshless technique of kansaś approach and galerkin method",
      "authors": [
        "Mehdi Dehghan",
        "Vahid Mohammadi"
      ],
      "year": "2014",
      "venue": "Engineering Analysis with Boundary Elements"
    },
    {
      "citation_id": "59",
      "title": "ImageNet: A large-scale hierarchical image database",
      "authors": [
        "Jia Deng",
        "Wei Dong",
        "Richard Socher",
        "Li-Jia Li",
        "Kai Li",
        "Li Fei-Fei"
      ],
      "year": "2009",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "60",
      "title": "ArcFace: Additive angular margin loss for deep face recognition",
      "authors": [
        "Jiankang Deng",
        "Jia Guo",
        "Niannan Xue",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "61",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "62",
      "title": "Evaluating reconstruction algorithms for respiratory motion guided acquisition",
      "authors": [
        "Owen Dillon",
        "Paul Keall",
        "Chun-Chien Shieh",
        "Ricky T O' Brien"
      ],
      "year": "2020",
      "venue": "Physics in Medicine & Biology"
    },
    {
      "citation_id": "63",
      "title": "Learning convolutional neural networks in presence of concept drift",
      "authors": [
        "Simone Disabato",
        "Manuel Roveri"
      ],
      "year": "2019",
      "venue": "Proceedings of the International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "64",
      "title": "Denoising prior driven deep neural network for image restoration",
      "authors": [
        "Weisheng Dong",
        "Peiyao Wang",
        "Wotao Yin",
        "Guangming Shi",
        "Fangfang Wu",
        "Xiaotong Lu"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "65",
      "title": "Explainable artificial intelligence: A survey",
      "authors": [
        "Filip Karlo Došilović",
        "Mario Brčić",
        "Nikica Hlupić"
      ],
      "year": "2018",
      "venue": "Proceedings of the 41st International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)"
    },
    {
      "citation_id": "66",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale. International Conference on Learning Representations",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer",
        "Alexander Kolesnikov",
        "Dirk Weissenborn",
        "Xiaohua Zhai",
        "Thomas Unterthiner",
        "Mostafa Dehghani",
        "Matthias Minderer",
        "Georg Heigold",
        "Sylvain Gelly",
        "Jakob Uszkoreit",
        "Neil Houlsby"
      ],
      "year": "2021",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale. International Conference on Learning Representations"
    },
    {
      "citation_id": "67",
      "title": "A systematic review of robustness in deep learning for computer vision: Mind the gap?",
      "authors": [
        "Nathan Drenkow",
        "Numair Sani",
        "Ilya Shpitser",
        "Mathias Unberath"
      ],
      "year": "2021",
      "venue": "Computing Research Repository (CoRR)"
    },
    {
      "citation_id": "68",
      "title": "A fast neural beamformer for antenna arrays",
      "authors": [
        "K.-L Du",
        "K Cheng",
        "M Swamy"
      ],
      "year": "2002",
      "venue": "IEEE International Conference on Communications. Conference Proceedings. ICC 2002"
    },
    {
      "citation_id": "69",
      "title": "A guide to convolution arithmetic for deep learning",
      "authors": [
        "Vincent Dumoulin",
        "Francesco Visin"
      ],
      "year": "2016",
      "venue": "Computing Research Repository (CoRR)"
    },
    {
      "citation_id": "70",
      "title": "Neural architecture search: A survey",
      "authors": [
        "Thomas Elsken",
        "Jan Hendrik Metzen",
        "Frank Hutter"
      ],
      "year": "2019",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "71",
      "title": "Evaluation of image quality for different kV cone-beam CT acquisition and reconstruction methods in the head and neck region",
      "authors": [
        "Ludvig Ulrik V Elström",
        "Jörgen Bb Muren",
        "Cai Petersen",
        "Grau"
      ],
      "year": "2011",
      "venue": "Acta Oncologica"
    },
    {
      "citation_id": "72",
      "title": "Forward and cross-scatter estimation in dual source CT using the deep scatter estimation (DSE)",
      "authors": [
        "Julien Erath",
        "Tim Vöth",
        "Joscha Maier",
        "Marc Kachelrieß"
      ],
      "year": "2019",
      "venue": "Physics of Medical Imaging"
    },
    {
      "citation_id": "73",
      "title": "Visualizing higher-layer features of a deep network",
      "authors": [
        "Yoshua Dumitru Erhan",
        "Aaron Bengio",
        "Pascal Courville",
        "Vincent"
      ],
      "year": "2009",
      "venue": "Technical Report"
    },
    {
      "citation_id": "74",
      "title": "Learning SO(3) equivariant representations with spherical CNNs",
      "authors": [
        "Carlos Esteves",
        "Christine Allen-Blanchette",
        "Ameesh Makadia",
        "Kostas Daniilidis"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "75",
      "title": "Detecting adversarial samples from artifacts",
      "authors": [
        "Reuben Feinman",
        "Ryan Curtin",
        "Saurabh Shintre",
        "Andrew Gardner"
      ],
      "year": "2017",
      "venue": "Computing Research Repository (CoRR)"
    },
    {
      "citation_id": "76",
      "title": "Practical cone-beam algorithm",
      "authors": [
        "Lee Feldkamp",
        "Lloyd Davis",
        "James Kress"
      ],
      "year": "1984",
      "venue": "Journal of the Optical Society of America A"
    },
    {
      "citation_id": "77",
      "title": "Efficient and robust automated machine learning",
      "authors": [
        "Matthias Feurer",
        "Aaron Klein",
        "Katharina Eggensperger",
        "Jost Springenberg",
        "Manuel Blum",
        "Frank Hutter"
      ],
      "year": "2015",
      "venue": "Proceedings of the Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "78",
      "title": "Using metalearning to initialize bayesian optimization of hyperparameters",
      "authors": [
        "Matthias Feurer",
        "Jost Tobias Springenberg",
        "Frank Hutter"
      ],
      "year": "2014",
      "venue": "Proceedings of the International Conference on Meta-learning and Algorithm Selection"
    },
    {
      "citation_id": "79",
      "title": "Practical Methods of Optimization",
      "authors": [
        "Roger Fletcher"
      ],
      "year": "2000",
      "venue": "Practical Methods of Optimization"
    },
    {
      "citation_id": "80",
      "title": "A critical comparison of some methods for interpolation of scattered data",
      "authors": [
        "Richard Franke"
      ],
      "year": "1979",
      "venue": "A critical comparison of some methods for interpolation of scattered data"
    },
    {
      "citation_id": "81",
      "title": "Theory of communication. part 1: The analysis of information",
      "authors": [
        "Dennis Gabor"
      ],
      "year": "1946",
      "venue": "Journal of the Institution of Electrical Engineers -Part III: Radio and Communication Engineering"
    },
    {
      "citation_id": "82",
      "title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning",
      "authors": [
        "Yarin Gal",
        "Zoubin Ghahramani"
      ],
      "year": "2016",
      "venue": "Proceedings of the 33nd International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "83",
      "title": "Improvements in CBCT image quality using a novel iterative reconstruction algorithm: A clinical evaluation",
      "authors": [
        "Stephen Gardner",
        "Weihua Mao",
        "Chang Liu",
        "Ibrahim Aref",
        "Mohamed Elshaikh",
        "Joon Lee",
        "Deepak Pradhan",
        "Benjamin Movsas",
        "Indrin Chetty",
        "Farzan Siddiqui"
      ],
      "year": "2019",
      "venue": "Advances in Radiation Oncology"
    },
    {
      "citation_id": "84",
      "title": "Image style transfer using convolutional neural networks",
      "authors": [
        "Leon Gatys",
        "Alexander Ecker",
        "Matthias Bethge"
      ],
      "year": "2016",
      "venue": "Proceedings fo the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "85",
      "title": "ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness",
      "authors": [
        "Robert Geirhos",
        "Patricia Rubisch",
        "Claudio Michaelis",
        "Matthias Bethge",
        "Felix Wichmann",
        "Wieland Brendel"
      ],
      "year": "2018",
      "venue": "Computing Research Repository (CoRR)"
    },
    {
      "citation_id": "86",
      "title": "Explaining explanations: An overview of interpretability of machine learning",
      "authors": [
        "Leilani Gilpin",
        "David Bau",
        "Ben Yuan",
        "Ayesha Bajwa",
        "Michael Specter",
        "Lalana Kagal"
      ],
      "year": "2018",
      "venue": "2018 IEEE 5th International Conference on Data Science and Advanced Analytics (DSAA)"
    },
    {
      "citation_id": "87",
      "title": "Metal artifact reduction in CT: Where are we after four decades?",
      "authors": [
        "Lars Gjesteby",
        "Bruno De Man",
        "Yannan Jin",
        "Harald Paganetti",
        "Joost Verburg",
        "Drosoula Giantsoudi",
        "Ge Wang"
      ],
      "year": "2016",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "88",
      "title": "How (not) to measure bias in face recognition networks",
      "authors": [
        "Stefan Glüge",
        "Mohammadreza Amirian",
        "Dandolo Flumini",
        "Thilo Stadelmann"
      ],
      "year": "2020",
      "venue": "Proceedings of the IAPR Workshop on Artificial Neural Networks in Pattern Recognition, Lecture Notes in Computer Science"
    },
    {
      "citation_id": "89",
      "title": "Deep Learning",
      "authors": [
        "Ian Goodfellow",
        "Yoshua Bengio",
        "Aaron Courville"
      ],
      "year": "2016",
      "venue": "Deep Learning"
    },
    {
      "citation_id": "90",
      "title": "Explaining and harnessing adversarial examples",
      "authors": [
        "Ian Goodfellow",
        "Jonathon Shlens",
        "Christian Szegedy"
      ],
      "year": "2015",
      "venue": "Proceeding of International Conference on Learning Representations"
    },
    {
      "citation_id": "91",
      "title": "European union regulations on algorithmic decision-making and a \"right to explanation",
      "authors": [
        "Bryce Goodman",
        "Seth Flaxman"
      ],
      "year": "2016",
      "venue": "AI Magazine"
    },
    {
      "citation_id": "92",
      "title": "Safire: Sinogram affirmed iterative reconstruction",
      "authors": [
        "Katharine Grant",
        "Rainer Raupach"
      ],
      "year": "2012",
      "venue": "Safire: Sinogram affirmed iterative reconstruction"
    },
    {
      "citation_id": "93",
      "title": "A global taxonomy of interpretable AI: unifying the terminology for the technical and social sciences",
      "authors": [
        "Mara Graziani",
        "Lidia Dutkiewicz",
        "Davide Calvaresi",
        "José Pereira Amorim",
        "Katerina Yordanova",
        "Mor Vered",
        "Rahul Nair",
        "Pedro Abreu",
        "Tobias Blanke",
        "Valeria Pulignano",
        "John Prior",
        "Lode Lauwaert",
        "Wessel Reijers",
        "Adrien Depeursinge",
        "Vincent Andrearczyk",
        "Henning Müller"
      ],
      "year": "2022",
      "venue": "Artificial Intelligence Review"
    },
    {
      "citation_id": "94",
      "title": "On the (statistical) detection of adversarial examples",
      "authors": [
        "Kathrin Grosse",
        "Praveen Manoharan",
        "Nicolas Papernot",
        "Michael Backes",
        "Patrick Mcdaniel"
      ],
      "year": "2017",
      "venue": "Computing Research Repository (CoRR)"
    },
    {
      "citation_id": "95",
      "title": "",
      "authors": [
        "Isabelle Guyon",
        "Lisheng Sun-Hosoya",
        "Marc Boullé",
        "Hugo Escalante",
        "Sergio Escalera",
        "Zhengying Liu",
        "Damir Jajetic",
        "Bisakha Ray",
        "Mehreen Saeed",
        "Michèle Sebag",
        "Alexander Statnikov",
        "Wei-Wei Tu"
      ],
      "year": "2017",
      "venue": ""
    },
    {
      "citation_id": "96",
      "title": "Deep residual learning for compressed sensing CT reconstruction via persistent homology analysis",
      "authors": [
        "Yo Seob Han",
        "Jaejun Yoo",
        "Jong Chul"
      ],
      "year": "2016",
      "venue": "Computing Research Repository (CoRR)"
    },
    {
      "citation_id": "97",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "98",
      "title": "MRF-based deformable registration and ventilation estimation of lung CT",
      "authors": [
        "Mattias Heinrich",
        "Mark Jenkinson",
        "Sir Michael Brady",
        "Julia Schnabel"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Medical Imaging"
    },
    {
      "citation_id": "99",
      "title": "Deep learning-based simultaneous multi-phase deformable image registration of sparse 4d-cbct",
      "authors": [
        "Ivo Herzig",
        "Pascal Paysan",
        "Stefan Scheib",
        "Alexander Züst",
        "Frank-Peter Schilling",
        "Javier Montoya",
        "Mohammadreza Amirian",
        "Thilo Stadelmann",
        "Peter Hotz",
        "Rudolf Marcel Füchslin"
      ],
      "year": "2022",
      "venue": "Medical Physics"
    },
    {
      "citation_id": "100",
      "title": "Wrongfully accused by an algorithm",
      "authors": [
        "Kashmir Hill"
      ],
      "year": "2020",
      "venue": "Wrongfully accused by an algorithm"
    },
    {
      "citation_id": "101",
      "title": "Long short-term memory",
      "authors": [
        "Sepp Hochreiter",
        "Jürgen Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "102",
      "title": "Deep metric learning using triplet network",
      "authors": [
        "Elad Hoffer",
        "Nir Ailon"
      ],
      "venue": "International Workshop on Similarity-Based Pattern Recognition"
    },
    {
      "citation_id": "103",
      "title": "",
      "authors": [
        "Springer Springer"
      ],
      "year": "2015",
      "venue": ""
    },
    {
      "citation_id": "104",
      "title": "Unsupervised learning and simulation for complexity management in business operations. Applied data science: lessons learned for the data-driven business",
      "authors": [
        "Lukas Hollenstein",
        "Lukas Lichtensteiger",
        "Thilo Stadelmann",
        "Mohammadreza Amirian",
        "Lukas Budde",
        "Jürg Meierhofer",
        "Rudolf Füchslin",
        "Thomas Friedli"
      ],
      "year": "2019",
      "venue": "Unsupervised learning and simulation for complexity management in business operations. Applied data science: lessons learned for the data-driven business"
    },
    {
      "citation_id": "105",
      "title": "Method of and apparatus for examining a body by radiation such as x or gamma radiation",
      "authors": [
        "Godfrey Hounsfield"
      ],
      "year": "1975",
      "venue": "Originating Research Org. not identified"
    },
    {
      "citation_id": "106",
      "title": "MobileNets: Efficient convolutional neural networks for mobile vision applications",
      "authors": [
        "Andrew Howard",
        "Menglong Zhu",
        "Bo Chen",
        "Dmitry Kalenichenko",
        "Weijun Wang",
        "Tobias Weyand",
        "Marco Andreetto",
        "Hartwig Adam"
      ],
      "year": "2017",
      "venue": "Computing Research Repository"
    },
    {
      "citation_id": "107",
      "title": "Squeeze-and-excitation networks",
      "authors": [
        "Jie Hu",
        "Li Shen",
        "Gang Sun"
      ],
      "year": "2018",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "108",
      "title": "Deep transfer metric learning",
      "authors": [
        "Junlin Hu",
        "Jiwen Lu",
        "Yap-Peng Tan"
      ],
      "year": "2015",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "109",
      "title": "A lightweight privacy-preserving CNN feature extraction framework for mobile sensing",
      "authors": [
        "Kai Huang",
        "Ximeng Liu",
        "Shaojing Fu",
        "Deke Guo",
        "Ming Xu"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Dependable and Secure Computing"
    },
    {
      "citation_id": "110",
      "title": "Lung nodule detection in CT using 3d convolutional neural networks",
      "authors": [
        "Xiaojie Huang",
        "Junjie Shan",
        "Vivek Vaidya"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)"
    },
    {
      "citation_id": "111",
      "title": "A survey of safety and trustworthiness of deep neural networks: Verification, testing, adversarial attack and defence, and interpretability",
      "authors": [
        "Xiaowei Huang",
        "Daniel Kroening",
        "Wenjie Ruan",
        "James Sharp",
        "Youcheng Sun",
        "Emese Thamo",
        "Min Wu",
        "Xinping Yi"
      ],
      "year": "2020",
      "venue": "Computer Science Review"
    },
    {
      "citation_id": "112",
      "title": "Efficient parallel inflated 3d convolution architecture for action recognition",
      "authors": [
        "Yukun Huang",
        "Yongcai Guo",
        "Chao Gao"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "113",
      "title": "Lietransformer: Equivariant selfattention for lie groups",
      "authors": [
        "Charline Michael J Hutchinson",
        "Sheheryar Le Lan",
        "Emilien Zaidi",
        "Yee Dupont",
        "Hyunjik Teh",
        "Kim"
      ],
      "year": "2021",
      "venue": "Proceedings of the 38th International Conference on Machine Learning, ICML 2021"
    },
    {
      "citation_id": "114",
      "title": "The \"echo state\" approach to analysing and training recurrent neural networks-with an erratum note",
      "authors": [
        "Herbert Jaeger"
      ],
      "year": "2001",
      "venue": "German National Research Center for Information Technology GMD Technical Report"
    },
    {
      "citation_id": "115",
      "title": "Flat-panel cone-beam computed tomography for image-guided radiation therapy",
      "authors": [
        "Jeffrey David A Jaffray",
        "John Siewerdsen",
        "Alvaro Wong",
        "Martinez"
      ],
      "year": "2002",
      "venue": "International Journal of Radiation Oncology*Biology*Physics"
    },
    {
      "citation_id": "116",
      "title": "Using the iterative kV CBCT reconstruction on the varian halcyon linear accelerator for radiation therapy-planning CT datasets: A feasibility study",
      "authors": [
        "Talia Jarema",
        "Trent Aland"
      ],
      "year": "2019",
      "venue": "International Journal of Radiation Oncology*Biology*Physics"
    },
    {
      "citation_id": "117",
      "title": "Deep convolutional neural network for inverse problems in imaging",
      "authors": [
        "Kyong Hwan",
        "Michael Mccann",
        "Emmanuel Froustey",
        "Michael Unser"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "118",
      "title": "Adaptive confidence learning for the personalization of pain intensity estimation systems",
      "authors": [
        "Markus Kächele",
        "Mohammadreza Amirian",
        "Patrick Thiam",
        "Philipp Werner",
        "Steffen Walter",
        "Günther Palm",
        "Friedhelm Schwenker"
      ],
      "year": "2017",
      "venue": "Evolving Systems"
    },
    {
      "citation_id": "119",
      "title": "Methods for person-centered continuous pain intensity assessment from bio-physiological channels",
      "authors": [
        "Markus Kächele",
        "Patrick Thiam",
        "Mohammadreza Amirian",
        "Friedhelm Schwenker",
        "Günther Palm"
      ],
      "year": "2016",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "120",
      "title": "Multimodal data fusion for person-independent, continuous estimation of pain intensity",
      "authors": [
        "Markus Kächele",
        "Patrick Thiam",
        "Mohammadreza Amirian",
        "Philipp Werner",
        "Steffen Walter",
        "Friedhelm Schwenker",
        "Günther Palm"
      ],
      "year": "2015",
      "venue": "Proceedings of the International Conference on Engineering Applications of Neural Networks (EANN), Communications in Computer and Information Science"
    },
    {
      "citation_id": "121",
      "title": "Angenäherte auflösung von systemen linearer gleichungen",
      "authors": [
        "Stefan Kaczmarz"
      ],
      "year": "1937",
      "venue": "Bulletin International de l' Académie Polonaise des Sciences et des Lettres"
    },
    {
      "citation_id": "122",
      "title": "Geometric robustness of deep networks: Analysis and improvement",
      "authors": [
        "Can Kanbak",
        "Seyed-Mohsen Moosavi-Dezfooli",
        "Pascal Frossard"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "123",
      "title": "On a generalized gaussian radial basis function: Analysis and applications",
      "authors": [
        "N Karimi",
        "S Kazem",
        "D Ahmadian",
        "H Adibi",
        "L Ballestra"
      ],
      "year": "2020",
      "venue": "Engineering Analysis with Boundary Elements"
    },
    {
      "citation_id": "124",
      "title": "A recipe for training neural networks",
      "authors": [
        "Andrej Karpathy"
      ],
      "year": "2019",
      "venue": "A recipe for training neural networks"
    },
    {
      "citation_id": "125",
      "title": "Trustworthy artificial intelligence: A review",
      "authors": [
        "Davinder Kaur",
        "Suleyman Uslu",
        "Kaley Rittichier",
        "Arjan Durresi"
      ],
      "venue": "ACM Computing Surveys (CSUR)"
    },
    {
      "citation_id": "126",
      "title": "The s-transform using a new window to improve frequency and time resolutions. Signal, image and Video processing",
      "authors": [
        "Kamran Kazemi",
        "Mohammadreza Amirian",
        "Mohammad Dehghani"
      ],
      "year": "2014",
      "venue": "The s-transform using a new window to improve frequency and time resolutions. Signal, image and Video processing"
    },
    {
      "citation_id": "127",
      "title": "High resolution iterative CT reconstruction using graphics hardware",
      "authors": [
        "Benjamin Keck",
        "G Hannes",
        "Holger Hofmann",
        "Markus Scherl",
        "Joachim Kowarschik",
        "Hornegger"
      ],
      "year": "2009",
      "venue": "Proceedings of the IEEE Nuclear Science Symposium Conference Record (NSS/MIC)"
    },
    {
      "citation_id": "128",
      "title": "The prosody of authentic emotions",
      "authors": [
        "Roland Kehrein"
      ],
      "year": "2002",
      "venue": "Proceedings of the International Conference on Speech Prosody"
    },
    {
      "citation_id": "129",
      "title": "Multimodal fusion including camera photoplethysmography for pain recognition",
      "authors": [
        "Viktor Kessler",
        "Patrick Thiam",
        "Mohammadreza Amirian",
        "Friedhelm Schwenker"
      ],
      "year": "2017",
      "venue": "2017 International Conference on Companion Technology (ICCT)"
    },
    {
      "citation_id": "130",
      "title": "Pain recognition with camera photoplethysmography",
      "authors": [
        "Viktor Kessler",
        "Patrick Thiam",
        "Mohammadreza Amirian",
        "Friedhelm Schwenker"
      ],
      "year": "2017",
      "venue": "2017 Seventh International Conference on Image Processing Theory, Tools and Applications (IPTA)"
    },
    {
      "citation_id": "131",
      "title": "Stochastic estimation of the maximum of a regression function",
      "authors": [
        "Jack Kiefer",
        "Jacob Wolfowitz"
      ],
      "year": "1952",
      "venue": "The Annals of Mathematical Statistics"
    },
    {
      "citation_id": "132",
      "title": "Transparency and accountability in AI decision support: Explaining and visualizing convolutional neural networks for text information",
      "authors": [
        "Buomsoo Kim",
        "Jinsoo Park",
        "Jihae Suh"
      ],
      "year": "2020",
      "venue": "Decision Support Systems"
    },
    {
      "citation_id": "133",
      "title": "Learning not to learn: Training deep neural networks with biased data",
      "authors": [
        "Byungju Kim",
        "Hyunwoo Kim",
        "Kyungsu Kim",
        "Sungjin Kim",
        "Junmo Kim"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "134",
      "title": "Combining ordered subsets and momentum for accelerated X-ray CT image reconstruction",
      "authors": [
        "Donghwan Kim",
        "Sathish Ramani",
        "Jeffrey Fessler"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Medical Imaging"
    },
    {
      "citation_id": "135",
      "title": "Early clinical experience with varian halcyon v2 linear accelerator: Dual-isocenter IMRT planning and delivery with portal dosimetry for gynecological cancer treatments",
      "authors": [
        "Hayeon Kim",
        "M Huq",
        "Ron Lalonde",
        "Christopher Houser",
        "Sushil Beriwal",
        "Dwight Heron"
      ],
      "year": "2019",
      "venue": "Journal of Applied Clinical Medical Physics"
    },
    {
      "citation_id": "136",
      "title": "Fusion architectures for multimodal cognitive load recognition",
      "authors": [
        "Daniel Kindsvater",
        "Sascha Meudt",
        "Friedhelm Schwenker"
      ],
      "year": "2016",
      "venue": "Multimodal Pattern Recognition of Social Signals in Human-Computer-Interaction (MPRSS)"
    },
    {
      "citation_id": "137",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2015",
      "venue": "Proceedings of the 3rd International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "138",
      "title": "A U-Nets cascade for sparse view computed tomography",
      "authors": [
        "Andreas Kofler",
        "Markus Haltmeier",
        "Christoph Kolbitsch",
        "Marc Kachelrieß",
        "Marc Dewey"
      ],
      "venue": "Proceedings"
    },
    {
      "citation_id": "139",
      "title": "3D object representations for fine-grained categorization",
      "authors": [
        "Jonathan Krause",
        "Michael Stark",
        "Jia Deng",
        "Li Fei-Fei"
      ],
      "year": "2013",
      "venue": "ICCV Workshops"
    },
    {
      "citation_id": "140",
      "title": "IBM CEO's letter to congress on racial justice reform",
      "authors": [
        "Arvind Krishna"
      ],
      "venue": "IBM CEO's letter to congress on racial justice reform"
    },
    {
      "citation_id": "141",
      "title": "Learning multiple layers of features from tiny images",
      "authors": [
        "Alex Krizhevsky"
      ],
      "year": "2009",
      "venue": "Learning multiple layers of features from tiny images"
    },
    {
      "citation_id": "142",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "Alex Krizhevsky",
        "Ilya Sutskever",
        "Geoffrey Hinton"
      ],
      "year": "2012",
      "venue": "Advances in neural information processing systems (NIPS)"
    },
    {
      "citation_id": "143",
      "title": "Decision trees can initialize radial-basis function networks",
      "authors": [
        "Miroslav Kubat"
      ],
      "year": "1998",
      "venue": "IEEE Transactions on Neural Networks"
    },
    {
      "citation_id": "144",
      "title": "A detailed review of feature extraction in image processing systems",
      "authors": [
        "Gaurav Kumar",
        "Pradeep Bhatia"
      ],
      "year": "2014",
      "venue": "Proceedings of the Fourth International Conference on Advanced Computing & Communication Technologies"
    },
    {
      "citation_id": "145",
      "title": "A dataset and a technique for generalized nuclear segmentation for computational pathology",
      "authors": [
        "Neeraj Kumar",
        "Ruchika Verma",
        "Sanuj Sharma",
        "Surabhi Bhargava",
        "Abhishek Vahadane",
        "Amit Sethi"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Medical Imaging"
    },
    {
      "citation_id": "146",
      "title": "Adversarial examples in the physical world",
      "authors": [
        "Alexey Kurakin",
        "Ian Goodfellow",
        "Samy Bengio"
      ],
      "year": "2016",
      "venue": "Proceeding of International Conference on Learning Representations. arXiv"
    },
    {
      "citation_id": "147",
      "title": "Survey on semantic segmentation using deep learning techniques",
      "authors": [
        "Fahad Lateef",
        "Yassine Ruichek"
      ],
      "year": "2019",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "148",
      "title": "Deep learning",
      "authors": [
        "Yann Lecun",
        "Yoshua Bengio",
        "Geoffrey Hinton"
      ],
      "year": "2015",
      "venue": "Nature"
    },
    {
      "citation_id": "149",
      "title": "Backpropagation Applied to Handwritten Zip Code Recognition. Neural computation",
      "authors": [
        "Yann Lecun",
        "Bernhard Boser",
        "John Denker",
        "Donnie Henderson",
        "Richard Howard",
        "Wayne Hubbard",
        "Lawrence Jackel"
      ],
      "year": "1989",
      "venue": "Backpropagation Applied to Handwritten Zip Code Recognition. Neural computation"
    },
    {
      "citation_id": "150",
      "title": "Gradientbased learning applied to document recognition",
      "authors": [
        "Yann Lecun",
        "Léon Bottou",
        "Yoshua Bengio",
        "Patrick Haffner"
      ],
      "year": "1998",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "151",
      "title": "Illuminationaware faster r-CNN for robust multispectral pedestrian detection",
      "authors": [
        "Chengyang Li",
        "Dan Song",
        "Ruofeng Tong",
        "Min Tang"
      ],
      "year": "2019",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "152",
      "title": "Hyperband: A novel bandit-based approach to hyperparameter optimization",
      "authors": [
        "Lisha Li",
        "Kevin Jamieson",
        "Giulia Desalvo",
        "Afshin Rostamizadeh",
        "Ameet Talwalkar"
      ],
      "year": "2018",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "153",
      "title": "Adversarial examples detection in deep networks with convolutional filter statistics",
      "authors": [
        "Xin Li",
        "Fuxin Li"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "154",
      "title": "Epileptic seizure detection in EEG signals using sparse multiscale radial basis function networks and the fisher vector approach",
      "authors": [
        "Yang Li",
        "Wei-Gang Cui",
        "Hui Huang",
        "Yu-Zhu Guo",
        "Ke Li",
        "Tao Tan"
      ],
      "year": "2019",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "155",
      "title": "Detecting adversarial image examples in deep neural networks with adaptive noise reduction",
      "authors": [
        "Bin Liang",
        "Hongcheng Li",
        "Miaoqiang Su",
        "Xirong Li",
        "Wenchang Shi",
        "Xiaofeng Wang"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Dependable and Secure Computing"
    },
    {
      "citation_id": "156",
      "title": "ADN: Artifact disentanglement network for unsupervised metal artifact reduction",
      "authors": [
        "Haofu Liao",
        "Wei-An Lin",
        "S Zhou",
        "Jiebo Luo"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Medical Imaging"
    },
    {
      "citation_id": "157",
      "title": "Relaxed conditions for radial-basis function networks to be universal approximators",
      "authors": [
        "Yi Liao",
        "Shu-Cherng Fang",
        "Henry Nuttle"
      ],
      "year": "2003",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "158",
      "title": "Fast autoaugment",
      "authors": [
        "Sungbin Lim",
        "Ildoo Kim",
        "Taesup Kim",
        "Chiheon Kim",
        "Sungwoong Kim"
      ],
      "year": "2019",
      "venue": "Proceedings of the Advances in Neural Information Processing Systems (NIPS)"
    },
    {
      "citation_id": "159",
      "title": "DuDoNet: Dual domain network for CT metal artifact reduction",
      "authors": [
        "Wei-An Lin",
        "Haofu Liao",
        "Cheng Peng",
        "Xiaohang Sun",
        "Jingdan Zhang",
        "Jiebo Luo",
        "Rama Chellappa",
        "Kevin Shaohua",
        "Zhou"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "160",
      "title": "Roberta: A robustly optimized BERT pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "Computing Research Repository (CoRR)"
    },
    {
      "citation_id": "161",
      "title": "CNN architectures for large-scale audio classification",
      "authors": [
        "Zhengying Liu",
        "Zhen Xu",
        "Shangeth Rajaa",
        "Meysam Madadi",
        "Julio Jacques Junior",
        "Sergio Escalera",
        "Adrien Pavao",
        "Sebastien Treguer",
        "Wei-Wei Tu",
        "Isabelle Guyon"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "162",
      "title": "Towards automated deep learning: Analysis of the AutoDL challenge series 2019",
      "authors": [
        "Zhengying Liu",
        "Zhen Xu",
        "Shangeth Rajaa",
        "Meysam Madadi",
        "Julio Jacques Junior",
        "Sergio Escalera",
        "Adrien Pavao",
        "Sebastien Treguer",
        "Wei-Wei Tu",
        "Isabelle Guyon"
      ],
      "year": "2020",
      "venue": "Proceedings of the NeurIPS Competition and Demonstration Track"
    },
    {
      "citation_id": "163",
      "title": "Facial recognition is accurate, if you're a white guy",
      "authors": [
        "Steve Lohr"
      ],
      "year": "2018",
      "venue": "Facial recognition is accurate, if you're a white guy"
    },
    {
      "citation_id": "164",
      "title": "Decoupled weight decay regularization",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2019",
      "venue": "Proceedings of the 7th International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "165",
      "title": "Object recognition from local scale-invariant features",
      "authors": [
        "David G Lowe"
      ],
      "year": "1999",
      "venue": "Proceedings of the Seventh IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "166",
      "title": "Distinctive image features from scale-invariant keypoints",
      "authors": [
        "G David",
        "Lowe"
      ],
      "year": "2004",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "167",
      "title": "SafetyNet: Detecting and rejecting adversarial examples robustly",
      "authors": [
        "Jiajun Lu",
        "Theerasit Issaranon",
        "David Forsyth"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "168",
      "title": "Cine cardiac MRI motion artifact reduction using a recurrent neural network",
      "authors": [
        "Qing Lyu",
        "Hongming Shan",
        "Yibin Xie",
        "Alan Kwan",
        "Yuka Otaki",
        "Keiichiro Kuronuma",
        "Debiao Li",
        "Ge Wang"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Medical Imaging"
    },
    {
      "citation_id": "169",
      "title": "U-DuDoNet: Unpaired dual-domain network for CT metal artifact reduction",
      "authors": [
        "Yuanyuan Lyu",
        "Jiajun Fu",
        "Cheng Peng",
        "Kevin Zhou"
      ],
      "year": "2021",
      "venue": "Medical Image Computing and Computer Assisted Intervention (MICCAI)"
    },
    {
      "citation_id": "170",
      "title": "Encoding metal mask projection for metal artifact reduction in computed tomography",
      "authors": [
        "Yuanyuan Lyu",
        "Wei-An Lin",
        "Haofu Liao",
        "Jingjing Lu",
        "Kevin Zhou"
      ],
      "year": "2020",
      "venue": "Proceedings of the 23rd International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)"
    },
    {
      "citation_id": "171",
      "title": "Facebook apologizes after a.i. puts 'primates' label on video of black men",
      "authors": [
        "Ryan Mac"
      ],
      "year": "2021",
      "venue": "Facebook apologizes after a.i. puts 'primates' label on video of black men"
    },
    {
      "citation_id": "172",
      "title": "Towards deep learning models resistant to adversarial attacks",
      "authors": [
        "Aleksander Madry",
        "Aleksandar Makelov",
        "Ludwig Schmidt",
        "Dimitris Tsipras",
        "Adrian Vladu"
      ],
      "year": "2018",
      "venue": "Proceedings of the 6th International Conference on Learning Representations (ICLR). OpenReview.net"
    },
    {
      "citation_id": "173",
      "title": "Learning with known operators reduces maximum error bounds",
      "authors": [
        "Andreas Maier",
        "Christopher Syben",
        "Bernhard Stimpel",
        "Tobias Würfl",
        "Mathis Hoffmann",
        "Frank Schebesch",
        "Weilin Fu",
        "Leonid Mill",
        "Lasse Kling",
        "Silke Christiansen"
      ],
      "year": "2019",
      "venue": "Nature Machine Intelligence"
    },
    {
      "citation_id": "174",
      "title": "Deep scatter estimation (DSE): feasibility of using a deep convolutional neural network for real-time x-ray scatter prediction in cone-beam CT",
      "authors": [
        "Joscha Maier",
        "Stefan Sawall",
        "Marc Kachelrieß",
        "Yannick Berker"
      ],
      "year": "2018",
      "venue": "SPIE Medical Imaging"
    },
    {
      "citation_id": "175",
      "title": "Fine-grained visual classification of aircraft",
      "authors": [
        "Subhransu Maji",
        "Esa Rahtu",
        "Juho Kannala",
        "Matthew Blaschko",
        "Andrea Vedaldi"
      ],
      "year": "2013",
      "venue": "Computing Research Repository (CoRR)"
    },
    {
      "citation_id": "176",
      "title": "Evaluation and clinical application of a commercially available iterative reconstruction algorithm for CBCTbased IGRT",
      "authors": [
        "Weihua Mao",
        "Chang Liu",
        "Stephen Gardner",
        "Farzan Siddiqui",
        "Karen Snyder",
        "Akila Kumarasiri",
        "Bo Zhao",
        "Joshua Kim",
        "Ning Winston Wen",
        "Benjamin Movsas",
        "Indrin Chetty"
      ],
      "year": "2019",
      "venue": "Technology in Cancer Research & Treatment"
    },
    {
      "citation_id": "177",
      "title": "Interpretable models for granger causality using self-explaining neural networks",
      "authors": [
        "Ričards Marcinkevičs",
        "Julia Vogt"
      ],
      "venue": "Proceedings of the 9th International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "178",
      "title": "Rotation equivariant vector field networks",
      "authors": [
        "Diego Marcos",
        "Michele Volpi",
        "Nikos Komodakis",
        "Devis Tuia"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "179",
      "title": "Deep face recognition: A survey",
      "authors": [
        "Iacopo Masi",
        "Yue Wu",
        "Tal Hassner",
        "Prem Natarajan"
      ],
      "year": "2018",
      "venue": "31st SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI)"
    },
    {
      "citation_id": "180",
      "title": "Deep face recognition: A survey",
      "authors": [
        "Iacopo Masi",
        "Yue Wu",
        "Tal Hassner",
        "Prem Natarajan"
      ],
      "year": "2018",
      "venue": "Proceedings of the 31st SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI)"
    },
    {
      "citation_id": "181",
      "title": "A survey on bias and fairness in machine learning",
      "authors": [
        "Ninareh Mehrabi",
        "Fred Morstatter",
        "Nripsuta Saxena",
        "Kristina Lerman",
        "Aram Galstyan"
      ],
      "year": "2021",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "182",
      "title": "Learning neural models for end-to-end clustering",
      "authors": [
        "Benjamin Bruno",
        "Ismail Elezi",
        "Mohammadreza Amirian",
        "Oliver Dürr",
        "Thilo Stadelmann"
      ],
      "year": "2018",
      "venue": "Artificial Neural Networks in Pattern Recognition"
    },
    {
      "citation_id": "183",
      "title": "MagNet: A two-pronged defense against adversarial examples",
      "authors": [
        "Dongyu Meng",
        "Hao Chen"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security"
    },
    {
      "citation_id": "184",
      "title": "Face recognition with radial basis function (RBF) neural networks",
      "authors": [
        "Meng Joo",
        "Shiqian Wu",
        "Juwei Lu",
        "Hock Lye"
      ],
      "year": "2002",
      "venue": "IEEE Transactions on Neural Networks"
    },
    {
      "citation_id": "185",
      "title": "On detecting adversarial perturbations",
      "authors": [
        "Jan Hendrik Metzen",
        "Tim Genewein",
        "Volker Fischer",
        "Bastian Bischoff"
      ],
      "year": "2017",
      "venue": "5th International Conference on Learning Representations, ICLR. OpenReview.net"
    },
    {
      "citation_id": "186",
      "title": "Universal adversarial perturbations against semantic image segmentation",
      "authors": [
        "Jan Hendrik Metzen",
        "Chaithanya Mummadi",
        "Thomas Kumar",
        "Volker Brox",
        "Fischer"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "187",
      "title": "V-net: Fully convolutional neural networks for volumetric medical image segmentation",
      "authors": [
        "Fausto Milletari",
        "Nassir Navab",
        "Seyed-Ahmad Ahmadi"
      ],
      "year": "2016",
      "venue": "Proceedings of the Fourth International Conference on 3D Vision (3DV)"
    },
    {
      "citation_id": "188",
      "title": "Fast learning in networks of locallytuned processing units",
      "authors": [
        "John Moody",
        "Christian Darken"
      ],
      "year": "1989",
      "venue": "Neural computation"
    },
    {
      "citation_id": "189",
      "title": "Universal adversarial perturbations",
      "authors": [
        "Seyed-Mohsen Moosavi-Dezfooli",
        "Alhussein Fawzi",
        "Omar Fawzi",
        "Pascal Frossard"
      ],
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "190",
      "title": "DeepFool: A simple and accurate method to fool deep neural networks",
      "authors": [
        "Seyed-Mohsen Moosavi-Dezfooli",
        "Alhussein Fawzi",
        "Pascal Frossard"
      ],
      "year": "2016",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "191",
      "title": "Voxceleb: A large-scale speaker identification dataset",
      "authors": [
        "Arsha Nagrani",
        "Son Chung",
        "Andrew Zisserman"
      ],
      "year": "2017",
      "venue": "Comput. Speech Lang"
    },
    {
      "citation_id": "192",
      "title": "Smooth minimization of non-smooth functions",
      "authors": [
        "Yu Nesterov"
      ],
      "year": "2005",
      "venue": "Mathematical Programming"
    },
    {
      "citation_id": "193",
      "title": "Singleshot 3d hand pose estimation using radial basis function networks trained on synthetic data",
      "authors": [
        "C Vassilis",
        "Iason Nicodemou",
        "Antonis Oikonomidis",
        "Argyros"
      ],
      "year": "2020",
      "venue": "Pattern Analysis and Applications"
    },
    {
      "citation_id": "194",
      "title": "Using maximum entropy for text classification",
      "authors": [
        "Kamal Nigam",
        "John Lafferty",
        "Andrew Mccallum"
      ],
      "year": "1999",
      "venue": "Proceedings of the IJCAI-99 workshop on machine learning for information filtering"
    },
    {
      "citation_id": "195",
      "title": "Automated flower classification over a large number of classes",
      "authors": [
        "Maria-Elena Nilsback",
        "Andrew Zisserman"
      ],
      "year": "2008",
      "venue": "Sixth Indian Conference on Computer Vision, Graphics & Image Processing"
    },
    {
      "citation_id": "196",
      "title": "Feature visualization",
      "authors": [
        "Chris Olah",
        "Alexander Mordvintsev",
        "Ludwig Schubert"
      ],
      "year": "2017",
      "venue": "Distill"
    },
    {
      "citation_id": "197",
      "title": "The building blocks of interpretability",
      "authors": [
        "Chris Olah",
        "Arvind Satyanarayan",
        "Ian Johnson",
        "Shan Carter",
        "Ludwig Schubert",
        "Katherine Ye",
        "Alexander Mordvintsev"
      ],
      "year": "2018",
      "venue": "Distill"
    },
    {
      "citation_id": "198",
      "title": "Automating biomedical data science through tree-based pipeline optimization",
      "authors": [
        "Randal Olson",
        "Ryan Urbanowicz",
        "Peter Andrews",
        "Nicole Lavender",
        "La Kidd",
        "Jason Moore"
      ],
      "year": "2016",
      "venue": "Proceedings of the European Conference on the Applications of Evolutionary Computation"
    },
    {
      "citation_id": "199",
      "title": "A survey on transfer learning",
      "authors": [
        "Jialin Sinno",
        "Qiang Pan",
        "Yang"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "200",
      "title": "Transferability in machine learning: from phenomena to black-box attacks using adversarial samples",
      "authors": [
        "Nicolas Papernot",
        "Patrick Mcdaniel",
        "Ian Goodfellow"
      ],
      "year": "2016",
      "venue": "Computing Research Repository (CoRR)"
    },
    {
      "citation_id": "201",
      "title": "CT sinogram-consistency learning for metal-induced beam hardening correction",
      "authors": [
        "Suk Hyoung",
        "Sung Park",
        "Min Lee",
        "Hwa Kim",
        "Jin Seo",
        "Yong Eun"
      ],
      "year": "2018",
      "venue": "Medical Physics"
    },
    {
      "citation_id": "202",
      "title": "Cats and dogs",
      "authors": [
        "Andrea Omkar M Parkhi",
        "Andrew Vedaldi",
        "C Zisserman",
        "Jawahar"
      ],
      "year": "2012",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "203",
      "title": "PyTorch: an imperative style, highperformance deep learning library",
      "authors": [
        "Adam Paszke",
        "Sam Gross",
        "Francisco Massa",
        "Adam Lerer",
        "James Bradbury",
        "Gregory Chanan",
        "Trevor Killeen",
        "Zeming Lin",
        "Natalia Gimelshein",
        "Luca Antiga",
        "Alban Desmaison",
        "Andreas Köpf",
        "Edward Yang",
        "Zach Devito",
        "Martin Raison",
        "Alykhan Tejani",
        "Sasank Chilamkurthy",
        "Benoit Steiner",
        "Lu Fang",
        "Junjie Bai",
        "Soumith Chintala"
      ],
      "year": "2019",
      "venue": "Proceedings of the 33rd International Conference on Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "204",
      "title": "Iterative image reconstruction in image-guided radiation therapy",
      "authors": [
        "Pascal Paysan",
        "Marcus Brehm",
        "Adam Wang",
        "Dieter Seghers",
        "Josh Star"
      ],
      "year": "2021",
      "venue": "US Patent"
    },
    {
      "citation_id": "205",
      "title": "Deep learning methods for image guidance in radiation therapy",
      "authors": [
        "Pascal Paysan",
        "Igor Peterlík",
        "Toon Roggen",
        "Liangjia Zhu",
        "Claas Wessels",
        "Jan Schreier",
        "Martin Buchacek",
        "Stefan Scheib"
      ],
      "year": "2020",
      "venue": "Proceedings of the 9th IAPR Artificial Neural Networks in Pattern Recognition Workshop"
    },
    {
      "citation_id": "206",
      "title": "Convolutional network based motion artifact reduction in conebeam CT",
      "authors": [
        "Pascal Paysan",
        "Adam Strzelecki",
        "Felipe Arrate",
        "Peter Munro",
        "Stefan Scheib"
      ],
      "year": "2019",
      "venue": "AAPM annual meeting 2019"
    },
    {
      "citation_id": "207",
      "title": "Visually guided inspiration breath-hold facilitated with nasal high flow therapy in locally advanced lung cancer",
      "authors": [
        "T Stephanie",
        "Femke Peeters",
        "Colien Vaassen",
        "Ana Hazelaar",
        "Eva Vaniqui",
        "Debby Rousch",
        "Esther Tissen",
        "Michiel Van Enckevort",
        "Michel Wolf",
        "Öllers",
        "Karolien Wouter Van Elmpt",
        "Judith Verhoeven",
        "Bettine Van Loon",
        "Dirk Vosse",
        "Gloria De Ruysscher",
        "Vilches-Freixas"
      ],
      "year": "2021",
      "venue": "Acta Oncologica"
    },
    {
      "citation_id": "208",
      "title": "A generalized inverse for matrices",
      "authors": [
        "Roger Penrose"
      ],
      "year": "1955",
      "venue": "Mathematical Proceedings of the Cambridge Philosophical Society"
    },
    {
      "citation_id": "209",
      "title": "Reducing residual-motion artifacts in iterative 3d CBCT reconstruction in imageguided radiation therapy",
      "authors": [
        "Igor Peterlik",
        "Adam Strzelecki",
        "Mathias Lehmann",
        "Philippe Messmer",
        "Peter Munro",
        "Pascal Paysan",
        "Mathieu Plamondon",
        "Dieter Seghers"
      ],
      "year": "2021",
      "venue": "Medical Physics"
    },
    {
      "citation_id": "210",
      "title": "Networks for approximation and learning",
      "authors": [
        "Tomaso Poggio",
        "Federico Girosi"
      ],
      "year": "1990",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "211",
      "title": "The circumplex model of affect: An integrative approach to affective neuroscience, cognitive development, and psychopathology",
      "authors": [
        "Jonathan Posner",
        "James Russell",
        "Bradley Peterson"
      ],
      "year": "2005",
      "venue": "Development and Psychopathology"
    },
    {
      "citation_id": "212",
      "title": "Searching for activation functions",
      "authors": [
        "Prajit Ramachandran",
        "Barret Zoph",
        "V Quoc",
        "Le"
      ],
      "year": "2017",
      "venue": "Computing Research Repository (CoRR)"
    },
    {
      "citation_id": "213",
      "title": "Foolbox: A python toolbox to benchmark the robustness of machine learning models",
      "authors": [
        "Jonas Rauber",
        "Wieland Brendel",
        "Matthias Bethge"
      ],
      "year": "2017",
      "venue": "Reliable Machine Learning in the Wild Workshop"
    },
    {
      "citation_id": "214",
      "title": "On the interpretability of artificial intelligence in radiology: Challenges and opportunities",
      "authors": [
        "Mauricio Reyes",
        "Raphael Meier",
        "Sérgio Pereira",
        "Carlos Silva",
        "Fried-Michael Dahlweid",
        "Hendrik Von Tengg-Kobligk",
        "Ronald Summers",
        "Roland Wiest"
      ],
      "year": "2020",
      "venue": "Radiology: Artificial Intelligence"
    },
    {
      "citation_id": "215",
      "title": "ImageNet-21K pretraining for the masses",
      "authors": [
        "Tal Ridnik",
        "Emanuel Ben-Baruch",
        "Asaf Noy",
        "Lihi Zelnik-Manor"
      ],
      "year": "2021",
      "venue": "Computing Research Repository (CoRR)"
    },
    {
      "citation_id": "216",
      "title": "Prediction of asynchronous dimensional emotion ratings from audiovisual and physiological data",
      "authors": [
        "Fabien Ringeval",
        "Florian Eyben",
        "Eleni Kroupi",
        "Anil Yuce",
        "Jean-Philippe Thiran",
        "Touradj Ebrahimi",
        "Denis Lalanne",
        "Björn Schuller"
      ],
      "year": "2015",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "217",
      "title": "Introducing the RECOLA multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "Fabien Ringeval",
        "Andreas Sonderegger",
        "Juergen Sauer",
        "Denis Lalanne"
      ],
      "year": "2013",
      "venue": "Proceedings of the 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "218",
      "title": "A stochastic approximation method",
      "authors": [
        "Herbert Robbins",
        "Sutton Monro"
      ],
      "year": "1951",
      "venue": "The Annals of Mathematical Statistics"
    },
    {
      "citation_id": "219",
      "title": "Face recognition: Too bias, or not too bias?",
      "authors": [
        "Gennady Joseph P Robinson",
        "Yann Livitz",
        "Can Henon",
        "Yun Qin",
        "Samson Fu",
        "Timoner"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "220",
      "title": "Group equivariant standalone self-attention for vision",
      "authors": [
        "David Romero",
        "Jean-Baptiste Cordonnier"
      ],
      "year": "2021",
      "venue": "Proceedings of the 9th International Conference on Learning Representations (ICLR). OpenReview.net"
    },
    {
      "citation_id": "221",
      "title": "U-Net: Convolutional networks for biomedical image segmentation",
      "authors": [
        "Olaf Ronneberger",
        "Philipp Fischer",
        "Thomas Brox"
      ],
      "year": "2015",
      "venue": "Proceedings of the Medical Image Computing and Computer-Assisted Intervention (MICCAI)"
    },
    {
      "citation_id": "222",
      "title": "Why, who, what, when and how about explainability in human-agent systems",
      "authors": [
        "Avi Rosenfeld",
        "Ariella Richardson"
      ],
      "year": "2020",
      "venue": "Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS '20"
    },
    {
      "citation_id": "223",
      "title": "Dictionaries for sparse representation modeling",
      "authors": [
        "Ron Rubinstein",
        "Alfred Bruckstein",
        "Michael Elad"
      ],
      "year": "2010",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "224",
      "title": "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead",
      "authors": [
        "Cynthia Rudin"
      ],
      "year": "2019",
      "venue": "Nature Machine Intelligence"
    },
    {
      "citation_id": "225",
      "title": "ImageNet large scale visual recognition challenge",
      "authors": [
        "Olga Russakovsky",
        "Jia Deng",
        "Hao Su",
        "Jonathan Krause",
        "Sanjeev Satheesh",
        "Sean Ma",
        "Zhiheng Huang",
        "Andrej Karpathy",
        "Aditya Khosla",
        "Michael Bernstein",
        "Alexander Berg",
        "Li Fei-Fei"
      ],
      "year": "2015",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "226",
      "title": "ImageNet large scale visual recognition challenge",
      "authors": [
        "Olga Russakovsky",
        "Jia Deng",
        "Hao Su",
        "Jonathan Krause",
        "Sanjeev Satheesh",
        "Sean Ma",
        "Zhiheng Huang",
        "Andrej Karpathy",
        "Aditya Khosla",
        "Michael Bernstein",
        "Alexander Berg",
        "Li Fei-Fei"
      ],
      "year": "2015",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "227",
      "title": "MobileNetV2: Inverted residuals and linear bottlenecks",
      "authors": [
        "Mark Sandler",
        "Andrew Howard",
        "Menglong Zhu",
        "Andrey Zhmoginov",
        "Liang-Chieh Chen"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "228",
      "title": "Analyzing and increasing the reliability of convolutional neural networks on GPUs",
      "authors": [
        "Fernando Fernandes Dos Santos",
        "Pedro Pimenta",
        "Caio Lunardi",
        "Lucas Draghetti",
        "Luigi Carro",
        "David Kaeli",
        "Paolo Rech"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Reliability"
    },
    {
      "citation_id": "229",
      "title": "Convolutional neural networks in medical image understanding: a survey",
      "authors": [
        "D Sarvamangala",
        "Raghavendra Kulkarni"
      ],
      "year": "2021",
      "venue": "Evolutionary Intelligence"
    },
    {
      "citation_id": "230",
      "title": "APAC: Augmented PAttern classification with neural networks",
      "authors": [
        "Ikuro Sato",
        "Hiroki Nishimura",
        "Kensuke Yokoi"
      ],
      "year": "2015",
      "venue": "Computing Research Repository (CoRR)"
    },
    {
      "citation_id": "231",
      "title": "On the information bottleneck theory of deep learning",
      "authors": [
        "Yamini Andrew M Saxe",
        "Joel Bansal",
        "Madhu Dapello",
        "Artemy Advani",
        "Brendan Kolchinsky",
        "David Tracey",
        "Cox"
      ],
      "year": "2019",
      "venue": "Journal of Statistical Mechanics: Theory and Experiment"
    },
    {
      "citation_id": "232",
      "title": "Deep learning in neural networks: An overview",
      "authors": [
        "Jürgen Schmidhuber"
      ],
      "year": "2015",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "233",
      "title": "Simulation-based deep artifact correction with convolutional neural networks for limited angle artifacts",
      "authors": [
        "Alena-Kathrin Schnurr",
        "Khanlian Chung",
        "Tom Russ",
        "Lothar Schad",
        "Frank Zöllner"
      ],
      "year": "2019",
      "venue": "Zeitschrift für Medizinische Physik"
    },
    {
      "citation_id": "234",
      "title": "Dan Dominik Bruellmann, Egor Dranischnikow, Ulrich Schwanecke, and Elmar Schoemer. Artefacts in CBCT: a review",
      "authors": [
        "Ralf Schulze",
        "Ulrich Heil",
        "D Groβ"
      ],
      "year": "2011",
      "venue": "Dentomaxillofacial Radiology"
    },
    {
      "citation_id": "235",
      "title": "Bidirectional recurrent neural networks",
      "authors": [
        "Mike Schuster",
        "Kuldip K Paliwal"
      ],
      "year": "1997",
      "venue": "IEEE Transactions on Signal Processing"
    },
    {
      "citation_id": "236",
      "title": "Initialisation of radial basis function networks using classification trees",
      "authors": [
        "Friedhelm Schwenker",
        "Christian Dietrich"
      ],
      "year": "2000",
      "venue": "Neural Network World"
    },
    {
      "citation_id": "237",
      "title": "Three learning phases for radial-basis-function networks",
      "authors": [
        "Friedhelm Schwenker",
        "Hans Kestler",
        "Günther Palm"
      ],
      "year": "2001",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "238",
      "title": "Similarities of LVQ and RBF learning-a survey of learning rules and the application to the classification of signals from high-resolution electrocardiography",
      "authors": [
        "Friedhelm",
        "Hans Schwenker",
        "Günther Kestler",
        "M Palm",
        "Höher"
      ],
      "year": "1994",
      "venue": "Proceedings of the IEEE International Conference on Systems, Man and Cybernetics"
    },
    {
      "citation_id": "239",
      "title": "Practices for engineering trustworthy machine learning applications",
      "authors": [
        "Alex Serban",
        "Koen Van Der Blom",
        "Holger Hoos",
        "Joost Visser"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/ACM 1st Workshop on AI Engineering -Software Engineering for AI (WAIN)"
    },
    {
      "citation_id": "240",
      "title": "Sensitive loss: Improving accuracy and fairness of face representations with discrimination-aware deep learning",
      "authors": [
        "Ignacio Serna",
        "Aythami Morales",
        "Julian Fierrez",
        "Nick Obradovich"
      ],
      "year": "2022",
      "venue": "Artificial Intelligence"
    },
    {
      "citation_id": "241",
      "title": "Self-attention with relative position representations",
      "authors": [
        "Peter Shaw",
        "Jakob Uszkoreit",
        "Ashish Vaswani"
      ],
      "year": "2018",
      "venue": "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "242",
      "title": "Intraclass correlations: Uses in assessing rater reliability",
      "authors": [
        "Patrick Shrout",
        "Joseph Fleiss"
      ],
      "year": "1979",
      "venue": "Psychological Bulletin"
    },
    {
      "citation_id": "243",
      "title": "Opening the black box of deep neural networks via information",
      "authors": [
        "Ravid Shwartz",
        "Naftali Tishby"
      ],
      "year": "2017",
      "venue": "Computing Research Repository (CoRR)"
    },
    {
      "citation_id": "244",
      "title": "Best practices for convolutional neural networks applied to visual document analysis",
      "authors": [
        "Patrice Simard",
        "David Steinkraus",
        "John Platt"
      ],
      "year": "2003",
      "venue": "Proceedings of the Seventh International Conference on Document Analysis and Recognition (ICDAR)"
    },
    {
      "citation_id": "245",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "Karen Simonyan",
        "Andrew Zisserman"
      ],
      "year": "2015",
      "venue": "5th International Conference on Learning Representations"
    },
    {
      "citation_id": "246",
      "title": "Ibm research releases 'diversity in faces' dataset to advance study of fairness in facial recognition systems",
      "authors": [
        "John R Smith"
      ],
      "year": "2019",
      "venue": "Ibm research releases 'diversity in faces' dataset to advance study of fairness in facial recognition systems"
    },
    {
      "citation_id": "247",
      "title": "A large dataset of real patients CT scans for COVID-19 identification",
      "authors": [
        "Eduardo Soares",
        "Plamen Angelov"
      ],
      "year": "2020",
      "venue": "A large dataset of real patients CT scans for COVID-19 identification"
    },
    {
      "citation_id": "248",
      "title": "Deep metric learning via lifted structured feature embedding",
      "authors": [
        "Hyun Oh Song",
        "Yu Xiang",
        "Stefanie Jegelka",
        "Silvio Savarese"
      ],
      "year": "2016",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "249",
      "title": "Striving for simplicity: The all convolutional net",
      "authors": [
        "Jost Tobias Springenberg",
        "Alexey Dosovitskiy",
        "Thomas Brox",
        "Martin Riedmiller"
      ],
      "year": "2015",
      "venue": "Proceedings of the 3rd International Conference on Learning Representations"
    },
    {
      "citation_id": "250",
      "title": "Dropout: a simple way to prevent neural networks from overfitting",
      "authors": [
        "Nitish Srivastava",
        "Geoffrey Hinton",
        "Alex Krizhevsky",
        "Ilya Sutskever",
        "Ruslan Salakhutdinov"
      ],
      "year": "2014",
      "venue": "The journal of machine learning research"
    },
    {
      "citation_id": "251",
      "title": "Highway networks",
      "authors": [
        "Rupesh Kumar Srivastava",
        "Klaus Greff",
        "Jürgen Schmidhuber"
      ],
      "year": "2015",
      "venue": "Computing Research Repository (CoRR)"
    },
    {
      "citation_id": "252",
      "title": "Deep learning in the wild",
      "authors": [
        "Thilo Stadelmann",
        "Mohammadreza Amirian",
        "Ismail Arabaci",
        "Marek Arnold",
        "Gilbert François Duivesteijn",
        "Ismail Elezi",
        "Melanie Geiger",
        "Stefan Lörwald",
        "Benjamin Meier",
        "Katharina Rombach",
        "Lukas Tuggener"
      ],
      "year": "2018",
      "venue": "Proceedings of the IAPR Workshop on Artificial Neural Networks in Pattern Recognition (ANNPR)"
    },
    {
      "citation_id": "253",
      "title": "Beyond ImageNet: Deep learning in industrial practice",
      "authors": [
        "Thilo Stadelmann",
        "Vasily Tolkachev",
        "Beate Sick",
        "Jan Stampfli",
        "Oliver Dürr"
      ],
      "year": "2019",
      "venue": "Applied Data Science"
    },
    {
      "citation_id": "254",
      "title": "Differential evolution-a simple and efficient heuristic for global optimization over continuous spaces",
      "authors": [
        "Rainer Storn",
        "Kenneth Price"
      ],
      "year": "1997",
      "venue": "Journal of Global Optimization"
    },
    {
      "citation_id": "255",
      "title": "Segmenter: Transformer for semantic segmentation",
      "authors": [
        "Robin Strudel",
        "Ricardo Garcia",
        "Ivan Laptev",
        "Cordelia Schmid"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "256",
      "title": "RoFormer: Enhanced transformer with rotary position embedding",
      "authors": [
        "Jianlin Su",
        "Yu Lu",
        "Shengfeng Pan",
        "Ahmed Murtadha",
        "Bo Wen",
        "Yunfeng Liu"
      ],
      "year": "2021",
      "venue": "Computing Research Repository (CoRR)"
    },
    {
      "citation_id": "257",
      "title": "One pixel attack for fooling deep neural networks",
      "authors": [
        "Jiawei Su",
        "Danilo Vargas",
        "Kouichi Sakurai"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Evolutionary Computation"
    },
    {
      "citation_id": "258",
      "title": "Learning a convolutional neural network for non-uniform motion blur removal",
      "authors": [
        "Jian Sun",
        "Wenfei Cao",
        "Zongben Xu",
        "Jean Ponce"
      ],
      "year": "2015",
      "venue": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "259",
      "title": "Deep learning face representation by joint identification-verification",
      "authors": [
        "Yi Sun",
        "Yuheng Chen",
        "Xiaogang Wang",
        "Xiaoou Tang"
      ],
      "year": "2014",
      "venue": "Proceedings of the Advances in Neural Information Processing Systems NIPS"
    },
    {
      "citation_id": "260",
      "title": "Deeply learned face representations are sparse, selective, and robust",
      "authors": [
        "Yi Sun",
        "Xiaogang Wang",
        "Xiaoou Tang"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "261",
      "title": "Sparsifying neural network connections for face recognition",
      "authors": [
        "Yi Sun",
        "Xiaogang Wang",
        "Xiaoou Tang"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "262",
      "title": "Going deeper with convolutions",
      "authors": [
        "Christian Szegedy",
        "Wei Liu",
        "Yangqing Jia",
        "Pierre Sermanet",
        "Scott Reed",
        "Dragomir Anguelov",
        "Dumitru Erhan",
        "Vincent Vanhoucke",
        "Andrew Rabinovich"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "263",
      "title": "Rethinking the inception architecture for computer vision",
      "authors": [
        "Christian Szegedy",
        "Vincent Vanhoucke",
        "Sergey Ioffe",
        "Jon Shlens",
        "Zbigniew Wojna"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "264",
      "title": "Intriguing properties of neural networks",
      "authors": [
        "Christian Szegedy",
        "Wojciech Zaremba",
        "Ilya Sutskever",
        "Joan Bruna",
        "Dumitru Erhan",
        "Ian Goodfellow",
        "Rob Fergus"
      ],
      "year": "2014",
      "venue": "Proceeding of International Conference on Learning Representations. arXiv"
    },
    {
      "citation_id": "265",
      "title": "Platform-aware neural architecture search for mobile",
      "authors": [
        "Mingxing Tan",
        "Bo Chen",
        "Ruoming Pang",
        "Vijay Vasudevan",
        "Mark Sandler",
        "Andrew Howard",
        "V Quoc",
        "Le",
        "Mnasnet"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "266",
      "title": "Rethinking model scaling for convolutional neural networks",
      "authors": [
        "Mingxing Tan",
        "V Quoc",
        "Le",
        "Efficientnet"
      ],
      "year": "2019",
      "venue": "Proceedings of the 36th International Conference on Machine Learning, ICML"
    },
    {
      "citation_id": "267",
      "title": "Multi-modal pain intensity recognition based on the senseemotion database",
      "authors": [
        "Patrick Thiam",
        "Viktor Kessler",
        "Mohammadreza Amirian",
        "Peter Bellmann",
        "Georg Layher",
        "Yan Zhang",
        "Maria Velana",
        "Sascha Gruss",
        "Steffen Walter",
        "Harald Traue",
        "Daniel Schork",
        "Jonghwa Kim",
        "Elisabeth André",
        "Heiko Neumann",
        "Friedhelm Schwenker"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "268",
      "title": "Multimodal deep denoising convolutional autoencoders for pain intensity classification based on physiological signals",
      "authors": [
        "Patrick Thiam",
        "Hans Kestler",
        "Friedhelm Schwenker"
      ],
      "year": "2020",
      "venue": "Proceedings of the 9th International Conference on Pattern Recognition Applications and Methods"
    },
    {
      "citation_id": "269",
      "title": "Regression shrinkage and selection via the lasso",
      "authors": [
        "Robert Tibshirani"
      ],
      "year": "1996",
      "venue": "Journal of the Royal Statistical Society: Series B (Methodological)"
    },
    {
      "citation_id": "270",
      "title": "A survey on explainable artificial intelligence (XAI): Toward medical XAI",
      "authors": [
        "Erico Tjoa",
        "Cuntai Guan"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "271",
      "title": "Training data-efficient image transformers & distillation through attention",
      "authors": [
        "Hugo Touvron",
        "Matthieu Cord",
        "Matthijs Douze",
        "Francisco Massa",
        "Alexandre Sablayrolles",
        "Hervé Jégou"
      ],
      "year": "2021",
      "venue": "Proceedings of the 38th International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "272",
      "title": "Adversarial training and robustness for multiple perturbations",
      "authors": [
        "Florian Tramèr",
        "Dan Boneh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 33rd International Conference on Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "273",
      "title": "Pius von Däniken, Prakhar Gupta, Frank-Peter Schilling, and Thilo Stadelmann. Design patterns for resource-constrained automated deep-learning methods",
      "authors": [
        "Lukas Tuggener",
        "Mohammadreza Amirian",
        "Fernando Benites"
      ],
      "year": "2020",
      "venue": "AI"
    },
    {
      "citation_id": "274",
      "title": "Automated machine learning in practice: State of the art and recent results",
      "authors": [
        "Lukas Tuggener",
        "Mohammadreza Amirian",
        "Katharina Rombach",
        "Stefan Lörwald",
        "Anastasia Varlet",
        "Christian Westermann",
        "Thilo Stadelmann"
      ],
      "year": "2019",
      "venue": "Proceedings of the 6th Swiss Conference on Data Science (SDS)"
    },
    {
      "citation_id": "275",
      "title": "An inversion formula for cone-beam reconstruction",
      "authors": [
        "K Heang",
        "Tuy"
      ],
      "year": "1983",
      "venue": "SIAM Journal on Applied Mathematics"
    },
    {
      "citation_id": "276",
      "title": "Instance normalization: The missing ingredient for fast stylization",
      "authors": [
        "Dmitry Ulyanov",
        "Andrea Vedaldi",
        "Victor Lempitsky"
      ],
      "year": "2016",
      "venue": "Computing Research Repository (CoRR)"
    },
    {
      "citation_id": "277",
      "title": "AVEC 2016: Depression, mood, and emotion recognition workshop and challenge",
      "authors": [
        "Michel Valstar",
        "Jonathan Gratch",
        "Björn Schuller",
        "Fabien Ringeval",
        "Denis Lalanne",
        "Mercedes Torres",
        "Stefan Scherer",
        "Giota Stratou",
        "Roddy Cowie",
        "Maja Pantic"
      ],
      "year": "2016",
      "venue": "Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "278",
      "title": "FERA 2017addressing head pose in the third facial expression recognition and analysis challenge",
      "authors": [
        "F Michel",
        "Enrique Valstar",
        "Jeffrey Sanchez-Lozano",
        "Laszlo Cohn",
        "Jeffrey Jeni",
        "Zheng Girard",
        "Lijun Zhang",
        "Maja Yin",
        "Pantic"
      ],
      "year": "2017",
      "venue": "Proceedings of the 12th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "279",
      "title": "Visualizing non-metric similarities in multiple maps",
      "authors": [
        "Laurens Van Der Maaten",
        "Geoffrey Hinton"
      ],
      "year": "2012",
      "venue": "Machine Learning"
    },
    {
      "citation_id": "280",
      "title": "Meta-learning",
      "authors": [
        "Joaquin Vanschoren"
      ],
      "year": "2019",
      "venue": "Automated Machine Learning: Methods, Systems, Challenges, The Springer Series on Challenges in Machine Learning"
    },
    {
      "citation_id": "281",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Proceedings of the Advances in Neural Information Processing Systems (NIPS)"
    },
    {
      "citation_id": "282",
      "title": "Making machine learning models interpretable",
      "authors": [
        "Alfredo Vellido",
        "José David Martín-Guerrero",
        "Paulo Lisboa"
      ],
      "year": "2012",
      "venue": "Proceedings of the European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN)"
    },
    {
      "citation_id": "283",
      "title": "Fairness definitions explained",
      "authors": [
        "Sahil Verma",
        "Julia Rubin"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE/ACM International Workshop on Software Fairness"
    },
    {
      "citation_id": "284",
      "title": "The biovid heat pain database data for the advancement and systematic validation of an automated pain recognition system",
      "authors": [
        "Steffen Walter",
        "Sascha Gruss",
        "Hagen Ehleiter",
        "Junwen Tan",
        "Harald Traue",
        "Philipp Werner",
        "Ayoub Al-Hamadi",
        "Stephen Crawcour",
        "O Adriano",
        "Gustavo Moreira Da Andrade",
        "Silva"
      ],
      "year": "2013",
      "venue": "Proceedings of the IEEE International Conference on Cybernetics (CYBCO)"
    },
    {
      "citation_id": "285",
      "title": "Regularization of neural networks using DropConnect",
      "authors": [
        "Li Wan",
        "Matthew Zeiler",
        "Sixin Zhang",
        "Yann Le Cun",
        "Rob Fergus"
      ],
      "year": "2013",
      "venue": "Proceedings of the 30th International Conference on Machine Learning"
    },
    {
      "citation_id": "286",
      "title": "PFW: A face database in the wild for studying face identification and verification in uncontrolled environment",
      "authors": [
        "Hai Wang",
        "Bongnam Kang",
        "Daijin Kim"
      ],
      "year": "2008",
      "venue": "Proceedings of the 2nd IAPR Asian Conference on Pattern Recognition"
    },
    {
      "citation_id": "287",
      "title": "CosFace: Large margin cosine loss for deep face recognition",
      "authors": [
        "Hao Wang",
        "Yitong Wang",
        "Zheng Zhou",
        "Xing Ji",
        "Dihong Gong",
        "Jingchao Zhou",
        "Zhifeng Li",
        "Wei Liu"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "288",
      "title": "InDuDoNet: An interpretable dual domain network for CT metal artifact reduction",
      "authors": [
        "Hong Wang",
        "Yuexiang Li",
        "Haimiao Zhang",
        "Jiawei Chen",
        "Kai Ma",
        "Deyu Meng",
        "Yefeng Zheng"
      ],
      "year": "2021",
      "venue": "Proceedings of the 24th International Conference Medical Image Computing and Computer Assisted Intervention (MICCAI)"
    },
    {
      "citation_id": "289",
      "title": "Deep metric learning with angular loss",
      "authors": [
        "Jian Wang",
        "Feng Zhou",
        "Shilei Wen",
        "Xiao Liu",
        "Yuanqing Lin"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "290",
      "title": "Deep visual domain adaptation: A survey",
      "authors": [
        "Mei Wang",
        "Weihong Deng"
      ],
      "year": "2018",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "291",
      "title": "Racial faces in the wild: Reducing racial bias by information maximization adaptation network",
      "authors": [
        "Mei Wang",
        "Weihong Deng",
        "Jiani Hu",
        "Xunqiang Tao",
        "Yaohai Huang"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "292",
      "title": "Medical image segmentation using deep learning: A survey",
      "authors": [
        "Risheng Wang",
        "Tao Lei",
        "Ruixia Cui",
        "Bingtao Zhang",
        "Hongying Meng",
        "Asoke Nandi"
      ],
      "year": "2020",
      "venue": "IET Image Processing"
    },
    {
      "citation_id": "293",
      "title": "Multiple sclerosis identification by 14-layer convolutional neural network with batch normalization, dropout, and stochastic pooling",
      "authors": [
        "Shui-Hua Wang",
        "Chaosheng Tang",
        "Junding Sun",
        "Jingyuan Yang",
        "Chenxi Huang",
        "Preetha Phillips",
        "Yu-Dong Zhang"
      ],
      "year": "2018",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "294",
      "title": "IDOLnet: An interactive dual-domain parallel network for CT metal artifact reduction",
      "authors": [
        "Tao Wang",
        "Zexin Lu",
        "Ziyuan Yang",
        "Wenjun Xia",
        "Mingzheng Hou",
        "Huaiqiang Sun",
        "Yan Liu",
        "Hu Chen",
        "Jiliu Zhou",
        "Yi Zhang"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Radiation and Plasma Medical Sciences"
    },
    {
      "citation_id": "295",
      "title": "Image quality assessment: From error visibility to structural similarity",
      "authors": [
        "Zhou Wang",
        "Alan Bovik",
        "Hamid Sheikh",
        "Eero Simoncelli"
      ],
      "year": "2004",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "296",
      "title": "Metal artifact reduction using iterative CBCT reconstruction algorithm for head and neck radiation therapy: A phantom and clinical study",
      "authors": [
        "Hayate Washio",
        "Shingo Ohira",
        "Yoshinori Funama",
        "Masahiro Morimoto",
        "Kentaro Wada",
        "Masashi Yagi",
        "Hiroaki Shimamoto",
        "Yuhei Koike",
        "Yoshihiro Ueda",
        "Tsukasa Karino",
        "Shoki Inui",
        "Yuya Nitta",
        "Masayoshi Miyazaki",
        "Teruki Teshima"
      ],
      "year": "2020",
      "venue": "European Journal of Radiology"
    },
    {
      "citation_id": "297",
      "title": "Bias, awareness, and ignorance in deep-learningbased face recognition",
      "authors": [
        "Samuel Wehrli",
        "Corinna Hertweck",
        "Mohammadreza Amirian",
        "Stefan Glüge",
        "Thilo Stadelmann"
      ],
      "year": "2022",
      "venue": "AI and Ethics"
    },
    {
      "citation_id": "298",
      "title": "Learning steerable filters for rotation equivariant CNNs",
      "authors": [
        "Maurice Weiler",
        "Fred Hamprecht",
        "Martin Storath"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "299",
      "title": "Caltech-UCSD birds 200. Report or Paper",
      "authors": [
        "Peter Welinder",
        "Steve Branson",
        "Takeshi Mita",
        "Catherine Wah",
        "Florian Schroff",
        "Serge Belongie",
        "Pietro Perona"
      ],
      "year": "2010",
      "venue": "Caltech-UCSD birds 200. Report or Paper"
    },
    {
      "citation_id": "300",
      "title": "CNNs on surfaces using rotation-equivariant features",
      "authors": [
        "Ruben Wiersma",
        "Elmar Eisemann",
        "Klaus Hildebrandt"
      ],
      "year": "2020",
      "venue": "ACM Transactions on Graphics"
    },
    {
      "citation_id": "301",
      "title": "A survey of unsupervised deep domain adaptation",
      "authors": [
        "Garrett Wilson",
        "Diane Cook"
      ],
      "year": "2020",
      "venue": "ACM Transactions on Intelligent Systems and Technology (TIST)"
    },
    {
      "citation_id": "302",
      "title": "Deep cosine metric learning for person re-identification",
      "authors": [
        "Nicolai Wojke",
        "Alex Bewley"
      ],
      "year": "2018",
      "venue": "IEEE Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "303",
      "title": "Why we should have seen that coming: comments on microsoft's tay \"experiment,\" and wider implications",
      "authors": [
        "Marty J Wolf",
        "Keith Miller",
        "Frances Grodzinsky"
      ],
      "year": "2017",
      "venue": "The ORBIT Journal"
    },
    {
      "citation_id": "304",
      "title": "CBAM: Convolutional block attention module",
      "authors": [
        "Sanghyun Woo",
        "Jongchan Park",
        "Joon-Young Lee",
        "In So Kweon"
      ],
      "year": "2018",
      "venue": "Proceedings of the European conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "305",
      "title": "Deep learning computed tomography: Learning projection-domain weights from image domain in limited angle problems",
      "authors": [
        "Tobias Würfl",
        "Mathis Hoffmann",
        "Vincent Christlein",
        "Katharina Breininger",
        "Yixin Huang",
        "Mathias Unberath",
        "Andreas Maier"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Medical Imaging"
    },
    {
      "citation_id": "306",
      "title": "Distance metric learning with application to clustering with side-information",
      "authors": [
        "Eric Xing",
        "Michael Jordan",
        "Stuart Russell",
        "Andrew Ng"
      ],
      "year": "2003",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "307",
      "title": "Supervised descent method and its applications to face alignment",
      "authors": [
        "Xuehan Xiong",
        "Fernando De La"
      ],
      "year": "2013",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "308",
      "title": "FairGAN: Fairnessaware generative adversarial networks",
      "authors": [
        "Depeng Xu",
        "Shuhan Yuan",
        "Lu Zhang",
        "Xintao Wu"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE International Conference on Big Data (Big Data)"
    },
    {
      "citation_id": "309",
      "title": "Feature squeezing: Detecting adversarial examples in deep neural networks",
      "authors": [
        "Weilin Xu",
        "David Evans",
        "Yanjun Qi"
      ],
      "year": "2018",
      "venue": "Proceedings 2018 Network and Distributed System Security Symposium"
    },
    {
      "citation_id": "310",
      "title": "Fast and accurate image super resolution by deep CNN with skip connection and network in network",
      "authors": [
        "Jin Yamanaka",
        "Shigesumi Kuwashima",
        "Takio Kurita"
      ],
      "venue": "Proceedings of the International Conference on Neural Information Processing"
    },
    {
      "citation_id": "311",
      "title": "",
      "authors": [
        "Springer Springer"
      ],
      "year": "2017",
      "venue": ""
    },
    {
      "citation_id": "312",
      "title": "Deep lesion graphs in the wild: Relationship learning and organization of significant radiology image findings in a diverse large-scale lesion database",
      "authors": [
        "Ke Yan",
        "Xiaosong Wang",
        "Le Lu",
        "Ling Zhang",
        "Adam Harrison",
        "Mohammadhadi Bagheri",
        "Ronald Summers"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "313",
      "title": "Unbox the black-box for the medical explainable AI via multi-modal and multi-centre data fusion: A minireview, two showcases and beyond",
      "authors": [
        "Guang Yang",
        "Qinghao Ye",
        "Jun Xia"
      ],
      "year": "2022",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "314",
      "title": "Facial expression recognition based on facial action unit",
      "authors": [
        "Jiannan Yang",
        "Fan Zhang",
        "Bike Chen",
        "Samee Khan"
      ],
      "year": "2019",
      "venue": "Proceedings of the Tenth International Green and Sustainable Computing Conference (IGSC)"
    },
    {
      "citation_id": "315",
      "title": "COVID-CT-dataset: A CT scan dataset about",
      "authors": [
        "Xingyi Yang",
        "Xuehai He",
        "Jinyu Zhao",
        "Yichen Zhang",
        "Shanghang Zhang",
        "Pengtao Xie"
      ],
      "venue": "COVID-CT-dataset: A CT scan dataset about"
    },
    {
      "citation_id": "316",
      "title": "Computing Research Repository (CoRR)",
      "year": "2020",
      "venue": "Computing Research Repository (CoRR)"
    },
    {
      "citation_id": "317",
      "title": "Initial evaluation of a novel cone-beam CT-based semi-automated online adaptive radiotherapy system for head and neck cancer treatment -a timing and automation quality study",
      "authors": [
        "Suk Yoon",
        "Hui Lin",
        "Michelle Alonso-Basanta",
        "Nate Anderson",
        "Ontida Apinorasethkul",
        "Karima Cooper",
        "Lei Dong",
        "Brian Kempsey",
        "Jaclyn Marcel",
        "James Metz",
        "Ryan Scheuermann",
        "Taoran Li"
      ],
      "year": "2020",
      "venue": "Cureus"
    },
    {
      "citation_id": "318",
      "title": "Exploring racial bias within face recognition via per-subject adversariallyenabled data augmentation",
      "authors": [
        "Seyma Yucer",
        "Samet Akçay",
        "Noura Al-Moubayed",
        "Toby Breckon"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "319",
      "title": "Explainability of deep vision-based autonomous driving systems: Review and challenges",
      "authors": [
        "Éloi Zablocki",
        "Hédi Ben-Younes",
        "Patrick Pérez",
        "Matthieu Cord"
      ],
      "year": "2022",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "320",
      "title": "Visualizing and understanding convolutional networks",
      "authors": [
        "Matthew Zeiler",
        "Rob Fergus"
      ],
      "year": "2014",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "321",
      "title": "Efficient convolutions for real-time semantic segmentation of 3d point clouds",
      "authors": [
        "Chris Zhang",
        "Wenjie Luo",
        "Raquel Urtasun"
      ],
      "year": "2018",
      "venue": "Proceedings of the International Conference on 3D Vision (3DV)"
    },
    {
      "citation_id": "322",
      "title": "Visual interpretability for deep learning: a survey",
      "authors": [
        "Quan-Shi Zhang",
        "Song-Chun Zhu"
      ],
      "year": "2018",
      "venue": "Frontiers of Information Technology & Electronic Engineering"
    },
    {
      "citation_id": "323",
      "title": "Convolutional neural network based metal artifact reduction in x-ray computed tomography",
      "authors": [
        "Yanbo Zhang",
        "Hengyong Yu"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Medical Imaging"
    },
    {
      "citation_id": "324",
      "title": "Road extraction by deep residual U-Net",
      "authors": [
        "Zhengxin Zhang",
        "Qingjie Liu",
        "Yunhong Wang"
      ],
      "year": "2018",
      "venue": "IEEE Geoscience and Remote Sensing Letters"
    },
    {
      "citation_id": "325",
      "title": "A sparse-view CT reconstruction method based on combination of DenseNet and deconvolution",
      "authors": [
        "Zhicheng Zhang",
        "Xiaokun Liang",
        "Xu Dong",
        "Yaoqin Xie",
        "Guohua Cao"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Medical Imaging"
    },
    {
      "citation_id": "326",
      "title": "Prediction of interfacial interactions related with membrane fouling in a membrane bioreactor based on radial basis function artificial neural network (ANN)",
      "authors": [
        "Zhitao Zhao",
        "Yang Lou",
        "Yifeng Chen",
        "Hongjun Lin",
        "Renjie Li",
        "Genying Yu"
      ],
      "year": "2019",
      "venue": "Bioresource Technology"
    },
    {
      "citation_id": "327",
      "title": "Statistical debugging of sampled programs",
      "authors": [
        "Alice Zheng",
        "Michael Jordan",
        "Ben Liblit",
        "Alex Aiken"
      ],
      "year": "2003",
      "venue": "Proceedings of the Advances in Neural Information Processing Systems (NIPS)"
    },
    {
      "citation_id": "328",
      "title": "Pose-independent facial action unit intensity regression based on multi-task deep transfer learning",
      "authors": [
        "Yuqian Zhou",
        "Jimin Pi",
        "Bertram Shi"
      ],
      "year": "2017",
      "venue": "Proceedings of the 12th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "329",
      "title": "Multi-label CNN based pedestrian attribute learning for soft biometrics",
      "authors": [
        "Jianqing Zhu",
        "Shengcai Liao",
        "Dong Yi",
        "Zhen Lei",
        "Stan Li"
      ],
      "year": "2015",
      "venue": "International Conference on Biometrics (ICB)"
    },
    {
      "citation_id": "330",
      "title": "Unpaired image-to-image translation using cycle-consistent adversarial networks",
      "authors": [
        "Jun-Yan Zhu",
        "Taesung Park",
        "Phillip Isola",
        "Alexei Efros"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "331",
      "title": "Learning transferable architectures for scalable image recognition",
      "authors": [
        "Barret Zoph",
        "Vijay Vasudevan",
        "Jonathon Shlens",
        "V Quoc",
        "Le"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    }
  ]
}