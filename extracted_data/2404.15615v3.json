{
  "paper_id": "2404.15615v3",
  "title": "M3D: Manifold-Based Domain Adaptation With Dynamic Distribution For Non-Deep Transfer Learning In Cross-Subject And Cross-Session Eeg-Based Emotion Recognition",
  "published": "2024-04-24T03:08:25Z",
  "authors": [
    "Ting Luo",
    "Jing Zhang",
    "Yingwei Qiu",
    "Li Zhang",
    "Yaohua Hu",
    "Zhuliang Yu",
    "Zhen Liang"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion decoding using Electroencephalography (EEG)-based affective brain-computer interfaces (aB-CIs) is crucial for affective computing but is hindered by EEG's non-stationarity, individual variability, and the high cost of large-scale labeled data. Deep learning-based approaches, while effective, require substantial computational resources and large datasets, limiting their practicality. To address these challenges, we propose Manifoldbased Domain Adaptation with Dynamic Distribution (M3D), a lightweight non-deep transfer learning framework. M3D includes four main modules: manifold feature transformation, dynamic distribution alignment, classifier learning, and ensemble learning. The data undergoes a transformation onto an optimal Grassmann manifold space, enabling dynamic alignment of the source and target domains. This process prioritizes both marginal and conditional distributions according to their significance, ensuring enhanced adaptation efficiency across various types of data. In the classifier learning, the principle of structural risk minimization is integrated to develop robust classification models. This is complemented by dynamic distribution alignment, which refines the classifier iteratively. Additionally, the ensemble learning module aggregates the classifiers obtained at different stages of the optimization process, which leverages the diversity of the classifiers to enhance the overall prediction accuracy. The proposed M3D framework",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Index Terms-Electroencephalography, Emotion Recognition, Dynamic Domain Adaptation, Non-Deep Transfer Learning, Manifold Transformation",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "E MOTION, functioning as both a physiological and psy- chological state within an individual, serves as a rich repository of insights into mental and physical well-being  [1] -  [3] . Emotion recognition stands as a crucial method for objectively discerning human emotional states. Currently, the practice relies on the analysis of either non-physiological signals or physiological signals. In contrast to cues like facial expressions, voice intonation, and gestures, Electroencephalography (EEG) stands out as a non-invasive brain information collection technique  [4] ,  [5] . Its strong correlation with human emotional states, coupled with its resistance to deception, positions EEG as a highly accurate means to objectively reflect human emotions  [6] -  [8] .\n\nEEG signals exhibit non-stationary characteristics and individual differences  [9] ,  [10] , leading to significant variations in the distribution of EEG signals within the same subject across different time periods or between different individuals. Traditional machine learning methods assume independence and identical distribution of data, causing emotion recognition models to have poor generalization across different sessions and different individuals. This limitation poses a challenge in expanding the widespread adoption and application of these models to new sessions and individuals. On the other hand, EEG signals encounter the challenge of a small sample size, typically comprising only a few hundred to a few thousand samples  [11] . The labeling process is often expensive and Classifier Learning, (4) Ensemble Learning. Here, Source and T arget refer to the source and target data, respectively. G is the Grassmann manifold, projected by the geodesic function Ψ(t) (t ∈ [0, 1]). The points Ψ(0) and Ψ(1) correspond to the embedded representations of the source and target domains in G. x i,j and x k,l are the samples from the source and target data,respectively. z ∞ i,j,k,l are the corresponding data representation in the manifold space by the geodesic transformation. H ∞ K denotes the reproducing kernel Hilbert space (RKHS), and G is the computed geodesics kernel. time-consuming, making it challenging to amass a large volume of data. This difficulty in obtaining substantial data poses a hurdle in training a reliable model.\n\nEarly EEG-based emotion recognition methods mainly rely on conventional non-deep machine learning algorithms  [12] , including Support Vector Machine (SVM), K-Nearest Neighbor (KNN), and Linear Discriminant Analysis (LDA). In these studies, there is an underlying assumption that EEG samples are independent and identically distributed. However, variations among individuals significantly undermine the effectiveness of traditional machine learning methods in crosssubject emotion recognition tasks. This results in suboptimal performance and poor generalization effects. On the other hand, transfer learning techniques can be beneficial in transferring informative data from the source (training data) to the target domain (test data), which could reduce the variations between different distributions in the modeling. More related transfer learning methods are detailed in Section II.\n\nCompared to deep learning-based transfer learning methods, non-deep learning-based transfer learning offers a more lightweight approach. Unlike deep learning methods, non-deep learning-based transfer requires less labeled data, which is better suited to the current conditions in EEG tasks. In this paper, we propose a novel non-deep learning-based transfer learning framework for cross-subject emotion recognition, named as Manifold-based Domain adaptation with Dynamic Distribution (M3D). The M3D model maps EEG data onto a manifold space, where both marginal and conditional distributions are dynamically analyzed and aligned. This process is coupled with classifier learning to minimize distributional differences between source and target domains. Subsequently, ensemble learning integrates the results to enhance the robustness of cross-subject and cross-session emotion recognition. Experimental results demonstrate that the proposed M3D model achieves performance comparable to existing deep learningbased methods. The main contributions of the paper are summarized below. First, a novel non-deep learning-based transfer learning model ((M3D) is proposed to address the variations in the distribution of EEG signals across different sessions and subjects. Second, both marginal distribution and conditional distribution are considered during domain alignment, where a dynamically adjusted weighting of the importance of the two distributions is incorporated. Third, an adaptive classifier learning together with an ensemble learning is introduced to iteratively refine the classifier learning process and leverage the strength of multiple learning models to optimize the prediction of data labels. Last, extensive experiments are validated on three well-known databases, as well as a clinical EEG database, with an average accuracy improvement of 6.67%.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Domain adaptation targets the reduction of distribution disparities between source and target domains, achieving the assumption that the source and target data is independently and identically distributed  [13] . Existing methods can be categorized into deep transfer learning and non-deep transfer learning.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Deep Transfer Learning Methods",
      "text": "The current trend in the development of transfer learning methods has mainly focused on deep learning-based approaches. For example, Li et al.  [14]  pioneered the use of the Domain Adversarial Neural Network (DANN) in the study of EEG-based emotion recognition. The optimization objective was to establish a shared common feature representation that mitigates distribution differences between the source and target domains. Building upon the aligned feature representation achieved with DANN, Zhou et al.  [15]  introduced a prototypical representation-based pairwise learning framework, aiming to further enhance cross-subject emotion recognition performance. However, the current DANN network is limited by its exclusive focus on addressing marginal distribution differences in EEG data among different individuals, neglecting joint distribution differences. Considering that individual differences in EEG arise from joint distribution differences between different EEG signals, Li et al.  [16]  extended the DANN network by incorporating the Joint Domain Adaptation Network. This extension provides a more comprehensive approach that considers both marginal and joint distribution differences in deep transfer learning studies.\n\nDespite the notable achievements of the deep transfer learning methods in emotion recognition tasks, there are certain limitations. Firstly, deep learning methods demand a substantial number of training samples. However, in EEG tasks, the available samples are typically limited to a few hundred or a few thousand. Insufficient samples would constrain performance, leading to overfitting and a decrease in generalization ability. Secondly, deep transfer learning algorithms heavily rely on a substantial amount of accurately labeled source data. If there is noticeable noise in the labels from the source domain, the recognition performance would significantly deteriorate  [17] . However, acquiring a sufficient number of samples with precise label information is not only costly but also time-consuming. Thirdly, deep transfer learning methods exhibit high computational complexity, substantial computational requirements, and involve slow and complicated training processes. The reliance on high-end hardware facilities is a significant demand, posing challenges in practical application environments where such resources may not be readily available. In contrast, non-deep transfer learning models exhibit lower complexity, reduced hardware requirements, and lightweight properties, showing the potential for good performance in widespread applications in real-life scenarios.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Non-Deep Transfer Learning Methods",
      "text": "Non-deep transfer learning methods align the feature distribution between source and target domains using conventional machine learning approaches. For example, Pan et al.  [18]  introduced a Transfer Component Analysis (TCA) algorithm to learn transfer information via reproducing kernel Hilbert space and mitigate the marginal distribution differences by maximizing the Mean Discrepancy (MMD). Zheng et al.  [19]  introduced a Transductive Parameter Transfer (TPT) algorithm, which maps personalized classifiers parameters to the target domain while minimizing the MMD for marginal distribution. Wang et al.  [20]  introduced a Stratified Transfer Learning (STL) algorithm to construct a weak classifier with source data and explore intra-class relationships for adaptive spatial dimensionality reduction.\n\nAlthough existing transfer learning strategies consider marginal or conditional distributions to reduce distribution differences, actual EEG disparities between subjects are in the joint distribution  [16] . Current algorithms focus on either marginal or conditional distributions, leading to inefficient data use. He  [16]  proposed Joint Distribution Adaptation (JDA) to match marginal and conditional distributions equally for joint distribution adjustment  [21] . However, existing methods assume equal contributions from conditional and marginal probability distributions, which is unrealistic. The importance of these distributions in the joint probability distribution varies based on domain similarity. Aligning marginal distribution is crucial when domains are highly dissimilar, while prioritizing conditional distribution is beneficial when domains are similar. Dynamically adjusting the weighting of marginal and conditional distributions based on data characteristics is a key issue in non-deep transfer learning.\n\nMost existing approaches align source and target distributions directly in the original feature space, which can be ineffective due to domain shifts. Some methods leverage the Grassmann manifold for subspace learning to transform data and extract domain-invariant features  [22] -  [24] , highlighting the need for feature transformation before distribution alignment.\n\nTo address these challenges and improve non-deep transfer learning for EEG-based emotion recognition, we propose the M3D model. It first uses TCA on differential entropy features from raw EEG signals to reduce dimensionality and narrow the distribution gap. Then, it embeds features into the Grassmann manifold via Geodesic Flow Kernel (GFK)  [25] . An adaptive factor is introduced to assess the significance of marginal and conditional distributions for better embedding. Upon achieving a robust manifold feature representation, we proceed with classifier learning to optimize the classification performance under the complex conditions of cross-subject and crosssession scenarios. In this phase, conventional machine learning methods like SVM, KNN, etc., are used as initial classifiers. Finally, an ensemble learning approach harmonizes classification results, enhancing the robustness and reliability of the framework for more accurate outcomes.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. Methodology",
      "text": "The proposed M3D model, shown in Fig.  1 , includes four main modules: manifold feature transformation, dynamic distribution alignment, classifier learning, and ensemble learning. First, input data is reduced to lower-dimensional features using TCA, and a manifold kernel G is learned to map the features into an optimal manifold space. Then, dynamic distribution alignment uses Structural Risk Minimization (SRM)  [26]  to adaptively align feature distributions within this space, improving adaptability to data variations. Next, a classifier is built on the aligned features, initially trained with traditional methods and iteratively refined by optimizing classification loss. Finally, ensemble learning combines all optimized classifiers from the iterations to produce more robust results for crosssubject and cross-session EEG-based emotion recognition.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Manifold Feature Transformation",
      "text": "Traditional feature alignment methods typically rely on original data features, which can be problematic due to deformations within the original feature space. These deformations make it difficult to effectively reduce differences between the source and target domains, negatively impacting model performance. Alternatively, the manifold space offers a solution by capturing the core aspects of data and representing original information in a more compact form  [27] ,  [28] . Importantly, features embedded in the manifold space often exhibit favorable geometric properties, enhancing their effectiveness in domain adaptation  [22] -  [24] . In the present study, our emphasis centers on leveraging the Grassmann manifold space to encapsulate the intrinsic essence of the data, which enables the creation of a more robust and dimensionality-reduced representation and offers a more effective way to align features across different domains.\n\nIn this paper, we introduce a manifold feature transformation process, as illustrated in Fig.  1 . The source and target domains are represented as D S = {(x 1 , y 1 ), ..., (x n , y n )} and D T = {(x n+1 , y n+1 ), ..., (x n+m , y n+m )}, respectively, with each domain featuring D-dimensional data representations.\n\nThrough TCA  [18] , we map both domains into a lower d-dimensional feature space (d ≪ D), denoted as T S and T T . This dimensionality reduction process simplifies data, facilitating more efficient data analysis and interpretation. Subsequently, we embed the d-dimensional feature subspace into the Grassmann manifold, denoted as G. Our goal in this manifold space is two-fold. We aim to increase the similarity between the source and target domains' data. Simultaneously, we want to retain the unique attributes and inherent characteristics of each domain's data. This approach ensures that while aligning the domains in a shared feature space, their distinctiveness remains intact.\n\nTo make the data distributions of the two domains more similar in the Grassmann manifold, we construct geodesics within G, treating the source and target domains as points in this manifold space. By moving along these geodesics, we can gradually align the data distributions of the two domains. When constructing geodesics, we apply the standard Euclidean metric to the Riemannian manifold. This allows us to describe the geodesic using a parameterization function Ψ(t), where t ranges from 0 to 1. Specifically, Ψ(0) and Ψ(1) correspond to the embedded representations of the source and target domains in G, respectively. The geodesic is the path from Ψ(0) to Ψ(1), representing the most direct connection between the two domain representations in the manifold space. To ensure a smooth and continuous transformation across the manifold, the intermediate values of t (t ∈ (0, 1)), Ψ(t) satisfies the following conditions:\n\nwhere T S ∈ R M ×d denote the set of orthonormal base for the source domain. M is the sample size. R S ∈ R M ×(D-d) signifies the orthogonal complement of T S , serving to encapsulate the dimensions outside the primary subspace spanned by T S . U 1 ∈ R d×d and U 2 ∈ R (D-d)×d are both orthogonal matrices, which can be obtained through the following two sets of Singular Value Decomposition (SVD) as\n\nwhere T T ∈ R M ×d is the orthonormal base set for the target domain. Γ and Σ are d × d diagonal matrices. Their diagonal elements are cos θ k and sin θ k (k = 1, 2, ..., d), respectively. The variable θ k represents the angles between the orthonormal bases T S and T T , with\n\n. θ k measures the alignment between the domains in G.\n\nThe operation Ψ(t) ⊤ x projects a feature vector x from the d-dimensional feature space onto the Grassmann manifold space G, where x could be any data sample from the source or target domain. Selecting an appropriate t value is crucial, as it directly influences the projection quality of x. The optimal t balances domain adaptation and the preservation of original features. This balance is essential for effective domain adaptation, as it allows for the integration of domain-specific features while minimizing the loss of critical information during the transition.\n\nWe introduce an integration strategy to find the optimal t or a set of t points between any two data samples x i and x j , and utilize a geodesic kernel to facilitate the mapping process. This method transforms the d-dimensional feature space into an infinite-dimensional feature space, reducing domain drift. For each pair of data samples x i and x j , we iteratively calculate their projections onto Ψ(t) as t ranges from 0 to 1. Each data sample is mapped across the Grassmann manifold space, capturing the evolution of its feature representation during the domain transition. The result of this process is two infinitedimensional feature vectors, denoted as z ∞ i and z ∞ j ,\n\nwhere z ∞ i and z ∞ j represent the entire trajectory of each data sample in the manifold, providing a comprehensive view of the domain adaptation process. This strategy improves the adaptability and accuracy of domain adaptation. From  [25] , the geodesic kernel is the inner product of the two infinitedimensional vectors, as\n\nwhere G ∈ R D×D is a positive semi-definite matrix, calculated as\n\nHere, Λ 1 , Λ 2 , Λ 3 are diagonal matrices, and the corresponding diagonal elements are given as\n\nwhere k ∈  [1, d] . The computed geodesics kernel G enables the effective transformation of a data sample x, initially positioned within the d-dimensional feature space, into the Grassmann manifold space, as\n\nThis transformed representation, z, integrates the intrinsic features of x as it is projected through the manifold, leveraging the geometrical properties of G to achieve a more domainadaptive feature representation and address domain drift in transfer learning. In the implement, we use DE features for manifold feature transformation across all databases.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. Dynamic Distribution Alignment",
      "text": "After the manifold feature transformation, the feature distributions between the source and target domains would exhibit greater similarity than seen with the original feature representation. Still, differences remain in both the marginal probability distribution and conditional probability distribution. Current distribution adaptation methods commonly assume equal importance for both the marginal distribution P and conditional distribution Q  [29] ,  [30] . This assumption, however, may not always be applicable. In scenarios where significant differences exist between the source and target domains, the adaptation of the marginal distribution takes on increased importance. Conversely, in situations where the source and target domains are more closely aligned, the adaptation of the conditional distribution becomes more critical. Therefore, recognizing and adjusting the emphasis on marginal or conditional distribution adaptation based on the domains' specific characteristics is essential for achieving successful distribution adaptation.\n\nIn the present study, an adaptive factor µ ∈ [0, 1] is incorporated to dynamically adjust the emphasis placed on marginal and conditional distributions. This adaptive factor allows for the flexible allocation of importance between the two types of distributions based on the characteristics of the source and target domains, which is defined as\n\nwhere D f (T S , T T ) represents the dynamic distribution adaptation between the source and target domains within the manifold feature space. D f (P s , P t ) is the marginal distribution adaptation, and\n\nis the conditional distribution adaptation for the class c (c ∈ {1, ..., C}). The total number of classes is represented by C. When µ is close to 0, it indicates significant differences between the source and target domains, thus prioritizing the adaptation of marginal distributions. When µ approaches 1, it suggests a greater similarity between the domains, thereby emphasizing the need for conditional distribution adaptation. At the midpoint, µ = 0.5, the model treats both marginal and conditional distributions with equal importance.\n\nIn  (9) , the marginal distribution adaptation D f (P s , P t ) is defined as\n\nwhere z s and z t are the source and target data represented in the manifold feature space, given in  (8) . f refers to the classifier. The term H K denotes the reproducing kernel Hilbert space (RKHS), which is a space of functions generated by the feature mapping Ψ(•). This mapping is critical for capturing the complex structures within the data by projecting it into a higher-dimensional space where linear separability is more feasible. For the computation of the conditional distribution adaptation\n\n, it calculate in a similar way, as\n\nwhere z\n\nare the c-class source and target data represented in the manifold feature space  (8) . Then, (9) could be rewritten as\n\nGiven the absence of label information for the target domain during the training phase, we cannot directly obtain f (z  (12) . To tackle this issue, we approximate f (z (c) t ) using the class-conditional probability distribution  [31] . Specifically, the process begins by training an initial weak classifier, such as a SVM or KNN, on the source data. This initial classifier, f , is subsequently utilized to infer pseudo-labels for the target data, serving as a provisional substitute for the unavailable true labels of the target domain. To enhance the reliability and accuracy of the pseudo-labels, an iterative refinement strategy is employed to improve the decodability f . This involves using the outcomes of the initial classifier to iteratively train subsequent classifiers, thereby progressively refining the pseudolabels. This iterative process aims to converge towards more accurate pseudo-labels, thus improving the model's ability to generalize from the source to the target domain effectively. Further details on the specific iterative refinement process and how it contributes to classifier learning will be discussed in Section III-C.\n\nIn  (12) , the adaptive factor µ is not a constant value predetermined by prior knowledge. Instead, it is dynamically learned based on the underlying data distribution. This parameter comprehensively incorporates both the marginal distribution difference and conditional distribution difference between domains. Specifically, we use the A-distance measurement  [32]  as a metric to estimate the marginal distribution difference, denoted as d A , between the source and target domains, as\n\nwhere ϵ (h) denotes the hinge loss from a binary classifier distinguishing between samples from the source and target domains. For the conditional distribution difference, based on the provided label information for source data and the estimated pseudo-label information of target data, we conduct the\n\nHere, T",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "S And T (C)",
      "text": "T represent the subsets of data corresponding to class c from the source and target domains, respectively. Then, the adaptive factor µ can be estimated as\n\nIn the process of dynamic distribution alignment, the adaptive factor µ is recalculated at each iteration, underscoring the iterative and responsive nature of this adaptation strategy. This re-calibration is essential to ensure that the adaptation process remains attuned to the evolving similarities and differences between the source and target domain feature distributions as they are represented in the manifold feature space. More detailed descriptions of µ are presented in the Supplementary Materials Appendix A.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Classifier Learning",
      "text": "Through the integration of manifold feature learning and dynamic distribution alignment, we can formulate an adaptive classifier f at the ιth iteration as\n\nHere, ι is the iteration number, y i is the groundtruth, and f (z i ) is the predicted classification result. ∥ • ∥ 2 H K refers to the L2 norm in the reproducing kernel Hilbert space. The dynamic distribution adaptation term, D f (T S , T T ), is obtained in  (12) . Further, a Laplacian regularization term, R f (T S , T T ), is incorporated to leverage the inherent geometric similarities among neighboring points in the manifold G  [27] . Here, the classifier f can be initialized using a commonly employed machine learning classifier, followed by adaptive optimization through an iterative learning process. η, λ and ρ are regularization parameters.\n\nIn the SRM framework with the representer theorem  [27] , f can be expanded as\n\nwhich includes both labeled and unlabeled samples from source and target domains, and\n\nis the coefficient vector. K is the kernel function.\n\nFor the SRM on the source domain, the first part in (  16 ) could be expressed as\n\nwhere  17 ) into (  18 ), then we can obtain\n\nwhere K ∈ R (n+m)×(n+m) is the kernel matrix, with K ij = K(z i , z j ). The label information of the source and target data is denoted by Y = [y 1 , ..., y n+m ]. For the target data, the corresponding label information is the pseudo label generated by the learned classifier f (•). tr(•) is the trace operation.\n\nFor the second part in  (16) , through applying  (17) , we can represent the calculation of dynamic distribution adaptation  (12)  as\n\nwhere\n\nis the class category, and C refers to the total number of classes. M 0 and M c are defined as\n\nHere,\n\nT |. The third part in (  16 ) (the Laplacian regularization term R f (T S , T T )) could be computed as\n\nwhere W ij is a pairwise similarity matrix, and L = D -W is the normalized graph Laplacian matrix. D is a diagonal matrix, with D ii = n+m j=1 W ij . W could be represented as\n\nwhere sim(•, •) is a similarity function (e.g., cosine distance) used to assess the proximity between two points. N p (z i ) represents a set of p nearest points to z i . p is a hyperparameter. Similarly, by applying  (17) , the Laplacian regularization given in  (23)  can be expressed as\n\nThen, by substituting (  19 ), (  20 ) and (  25 )into the (  16 ), the classifier f can be calculated as:\n\nSetting ∂f ∂β = 0, (26) could be solved as:\n\nAfter obtaining β * , the classifier f can be determined using  (26) . Then, the corresponding soft label of the target domain at ιth iteration could be denoted as ŷι t = f ι (z t ). In the implementation, through l iterations, the adaptive factor µ in the dynamic distribution alignment is continuously updated, and the classifier learning leverages the corresponding updated µ to further refine the pseudo label of the target domain. Calculate the adaptive factor µ using  (15) , and compute M 0 and M c using (  21 ) and (  22 ); # Classifier Learning 6:",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Similarity Matrix",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Link-Based Similarity Matrix Function",
      "text": "Solve the equation using  (27)  to compute β * and obtain the classifier f using (26); 7:\n\nUpdate the soft labels for the target data ŷι t = f (z t ); 8: end for 9: Return the classifier f ; # Ensemble Learning 10: Perform ensemble learning based on the obtained soft labels in the loop {ŷ ι t } l ι=1 .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "D. Ensemble Learning",
      "text": "Ensemble learning with LinkCluE algorithm  [33]  is adopted here to strategically reduce classification bias inherent in individual models and improve the overall generalization performance of the model. Based on the obtained ŷt in a certain round of classifier learning, the ensemble learning leverages the strengths over a group of ŷι t (ι = 1, . . . , l. Empirically, l is set to 10) and form together to generate a strong prediction results ŷt . Fig.  2  illustrates the two main steps involved in the LinkCluE algorithm: the link-based similarity matrix function and the consensus function. For the link-based similarity matrix function, it constructs a similarity matrix based",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Iv. Experimental Results",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "A. Emotional Databases",
      "text": "The model performance is carefully validated on three wellknown publicly available databases: SEED  [12] , SEED-IV  [34]  and SEED-V  [35] . The SEED database includes EEG emotion data collected from 15 participants. Each participant was exposed to a total of 15 movie clips designed to evoke three types of emotions (positive, negative, and neutral). The SEED-IV database also includes EEG emotion data from 15 participants, exposed to a total of 24 movie clips aimed at eliciting four types of emotions (happy, sad, neutral, and fear). In the SEED-V database, a total of 5 emotions (happiness, sadness, fear, disgust, and neutral) are elicited using 15 movie clips, and the simultaneous EEG signals under the 5 emotions are recorded from 16 subjects (6 males and 10 females). For both the SEED, SEED-IV, and SEED-V databases, participants undergo three sessions spread across separate days, with a one-week break between sessions. The EEG signals are recorded using the ESI Neuroscan system with 62 channels.\n\nThe EEG preprocessing follows established procedures in the literature. This approach aligns with the standard preprocessing practices in emotion-related EEG research and ensures consistency across the widely used SEED, SEED-IV, and SEED-V datasets in the literature. The process begins with the downsampling of EEG signals to 200 Hz, followed by the removal of artifacts such as the Electrooculogram (EOG) and Electromyography (EMG). Downsampling to 200 Hz effectively reduces computational complexity while preserving the temporal resolution necessary for emotion recognition  [12] ,  [34] ,  [35] . A bandpass filter with a 0.3-50 Hz range is then applied to improve the signal quality, as the majority of emotionrelated electroencephalography activities occur below 50 Hz. Following this, each trial was segmented into multiple 1second data samples. To extract emotional related information, DE features were extracted across five frequency bands: Delta (1-3 Hz), Theta (4-7 Hz), Alpha (8-13 Hz), Beta (14-30 Hz), and Gamma (31-50 Hz). Thus, for each 1-second data sample, a total of 310 features (5 frequency bands × 62 channels) were characterized, which serves as the input for the model. To explore EEG signal inter-subject differences in SEED and SEED-IV datasets, we perform a comprehensive statistical analysis of input EEG features via qualitative visualizations and quantitative comparisons. More detailed descriptions and results are presented in Supplementary Materials Appendix B.\n\nTo further assess the generalizability of the proposed M3D model beyond controlled experimental conditions, we extend model validation to a real-world clinical EEG database obtained from Hospital Universiti Sains Malaysia (HUSM)  [36] . This dataset consists of EEG recordings from 27 healthy individuals (mean age: 38.28±15.64 years) and 29 individuals diagnosed with MDD (Major Depressive Disorder) (mean age: 40.33±12.86 years). Participant classification adhered to the international diagnostic criteria outlined in the Diagnostic and Statistical Manual-IV (DSM-IV). This dataset used different devices and settings compared to the SEED and SEED-IV databases, with EEG recordings collected using the Brain Master Discovery amplifier, 19 electrodes, and a sampling rate of 256 Hz. Preprocessing involved bandpass filtering (0.5-70 Hz) and notch filtering at 50 Hz to eliminate power-line noise. This preprocessing pipeline is widely used and validated in the literature  [36]  as a reliable approach for enhancing data quality and processing efficiency without compromising informative neural features. The experimental protocol included EEG recordings under both open-eye (EO) and closedeye (EC) conditions, each lasting 5 minutes. During the EO condition, participants were instructed to remain relaxed while minimizing eye movements to maintain data quality. Similar to the SEED, SEED-IV and SEED-V databases, this database also extracts DE features across five frequency bands.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "B. Experimental Protocols",
      "text": "In order to thoroughly assess the robustness and stability of the proposed model and ensure a detailed comparison with existing literature, two types of experimental protocols are conducted.\n\n• Cross-subject single-session leave-one-subject-out cross-validation (Cross-subject single-session LOSO CV). The data samples from N subjects (SEED/SEED-IV: N=14, SEED-V: N=15) in the first session are used as the source domain, and the data samples from the remaining 1 subject in the same session are used as the target domain. This procedure repeats N+1 times (SEED/SEED-IV: N=14, SEED-V: N=15), ensuring that each participant is treated as the target domain at least once.\n\n• Cross-subject cross-session leave-one-subject-out cross-validation (Cross-subject cross-session LOSO CV). The data samples from N subjects (SEED/SEED-IV: N=14, SEED-V: N=15) across all three sessions are used as the source domain, and the data samples from the remaining 1 subject across all three sessions are used as the target domain. This procedure also repeats N+1 times (SEED/SEED-IV: N=14, SEED-V: N=15), ensuring that each participant is treated as the target domain at least once. In the present study, we adopt six widely recognized machine-learning classifiers to generate the initial classification results (ŷ 0 t ) as mentioned in Section III-C. The adopted machine-learning classifiers include KNN  [37] , SVM  [38] , DT  [39] , Adaboost classifier  [40] , GNB classifier  [41] , and Bagging classifier  [42] . In the SVM classifier, the radial basis function (RBF) kernel is utilized. In ensemble learning, the application of three similarity matrix functions (CTS, SRS, and ASRS) combined with three hierarchical agglomerative clustering algorithms (SL, CL, and AL) yields a total of 9 possible combinations. Here, the combination that achieves the highest accuracy in predicting target domain labels will be chosen as the optimal outcome. Notably, all the best results are achieved using the DT classifier and the CTS-SL approach. For performance evaluation, a comprehensive analysis of the model's effectiveness is conducted using multiple metrics, including accuracy, sensitivity, specificity, precision, F1-score, Area Under the Receiver Operating Characteristic Curve (AU-ROC) and Negative Predictive Value (NPV) metrics. We set the manifold feature dimension d = 128 in all datasets except for the MDD database (where d = 100). The regularization parameters are set as η = 0.1, ρ = 1 and λ = 0.4.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "C. Experimental Results On Cross-Subject Single-Session",
      "text": "The experimental results on SEED, SEED-IV and SEED-V database under the cross-subject single-session crossvalidation are reported in Table  I . For comprehensive evaluation, we compare our method with existing approaches under identical experimental conditions. The comparison methods included in each table are selected based on the availability of reported results under the corresponding dataset and evaluation protocol to ensure fair and consistent evaluation. The baseline methods selected for comparison include a range of classical and recent approaches, covering representative state-of-the-art techniques published in the field of EEG-based domain adaptation. Results marked with * are independently reproduced by     the authors under standardized preprocessing and evaluation protocols to ensure comparability. For the SEED database, the best performance is obtained when DT is adopted as the initial classifier, where the corresponding accuracy, specificity and precision are 84.57%, 88.78% and 76.94%. We compare the obtained experimental results with the existing literature, as shown in Table  II . The comparison results show that the proposed non-deep transfer learning model (M3D) demonstrates superior performance. In comparison to the best results obtained by the traditional machine learning methods, our model shows a performance enhancement of 8.26%. Furthermore, it performs comparably or even surpasses some deep learning methods, such as ADA  [16] , TANN  [47] , P-GCNN  [48]  and DG-DANN  [43] .\n\nFor the SEED-IV database, the best accuracy is obtained when DT is used as the initial classifier, where the corre-    III . Compared to the best results reported in the traditional machine learning methods, the proposed M3D demonstrates a slight performance advantage of 0.72%. Furthermore, it performs comparably or even surpasses some deep learning methods, including ScalingNet  [46] and MS-MDA  [50] .\n\nFor the SEED-V database, the best accuracy is obtained when DT is used as the initial classifier, where the corresponding accuracy, specificity, precision, F1-score and AUROC are 65.25%, 66.16%, 16.6%, 25.76% and 78.46%. As shown in Table  IV , the M3D model outperforms the best-performing traditional machine learning baseline by a notable 14.15%, clearly demonstrating its superior classification capability.",
      "page_start": 8,
      "page_end": 10
    },
    {
      "section_name": "D. Experimental Results On Cross-Subject Cross-Session",
      "text": "To evaluate the efficiency and stability of the proposed M3D in managing variations in EEG data collected from the same subject at different times, we also conduct a model validation under the cross-subject cross-session leave-one-subjectout cross-validation protocol. The experimental results are reported in Table  V , when different initial classifiers are used. The best model performance of SEED database is achieved when DT is adopted as the initial classifier, where the accuracy is 77.05%, and the other six indicators, except for NPV, are also the highest. The performance results against the existing methods are compared in Table  VI . It shows, the proposed model demonstrates an accuracy improvement of 5.18% in comparison to traditional machine learning methods. Further, it exhibits performance that is comparable, to some extent, with deep learning methods, showing the effectiveness of the proposed M3D in managing variations in data collected from the same subject across different time periods.\n\nFor the SEED-IV database, the performance across various initial classifiers is relatively consistent, with the highest accuracy reaching 64.9% when DT is employed as the initial classifier. Table VII reports the performance comparison results against the existing machine learning and deep learning methods. Compared to the best results reported in various literature on machine learning methods, the proposed M3D shows a performance improvement of 0.46%. Furthermore, it performs comparably or even outperforms some deep learning methods, including MetaEmotionNet  [55]  and DAN  [14] . The superior cross-session validation performance observed in SEED-IV is largely driven by its smaller per-session sample size, higher emotional class complexity, and more diverse multimodal stimuli, which collectively benefit from crosssession aggregation. Meanwhile, the SEED database's more stable per-session paradigm and stimulus uniformity make single-session validation more effective and reliable.\n\nFor the SEED-V database in the cross-session setting, performance across various initial classifiers remains relatively stable, with the highest accuracy reaching 59.15%, again observed with DT. As shown in Table  VIII , the proposed M3D model outperforms the best results reported in the literature for traditional machine learning methods by a margin of 11.69%, demonstrating a substantial improvement in generalization capability. Furthermore, M3D delivers performance that is comparable to or even surpasses certain deep learning models, including the state-of-the-art DCORAL method  [59] .",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "E. Experimental Results On Clinical Eeg Database",
      "text": "To assess the generalizability of the proposed M3D model in real-world clinical applications, we evaluate its performance on the MDD database (n=56). Following previous work  [62] , we adopt the cross-subject single-session 10-fold cross-validation protocol for fair comparison. The validation results are presented in Table  IX , demonstrating consistent performance across various initial classifiers. In particular, the highest accuracy is achieved using DT as the initial classifier, reaching 82.72% in the EC session and 78.63% in the EO session. For comparative analysis, we report performance differences against existing methods in Table  X . The proposed M3D model outperforms the best-performing traditional machine learning approach by 8.51% and 4.42% in the EC and EO sessions, respectively. Among deep learning methods, DCORAL  [59]  achieves the highest reported accuracy in prior studies, with 70.88±10.53% in the EC session and 74.49±8.17% in the EO session. M3D surpasses these results by 11.84% and 4.14% in the EC and EO sessions, respectively, demonstrating its superior ability to capture the complex patterns associated with MDD. These results underscore the effectiveness of M3D in real-world clinical settings, establishing it as a promising tool for practical clinical applications.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "V. Discussion",
      "text": "",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "A. Ablation Study",
      "text": "To comprehensively examine the contributions of the model components within the proposed M3D model, we conduct a thoroughly ablation study. Specifically, we design four model variations to isolate the effects of key modules. (1) Manifold Feature Transformation Only: the initial decision tree classifier is trained to evaluate model performance after applying only the manifold feature transformation module. (2) Dynamic Distribution Alignment and Classifier Learning Only: the DE feature is used as input, focusing solely on dynamic distribution alignment and classifier learning. (3) Without Manifold Feature Transformation: the DE feature serves as input for dynamic distribution alignment, classifier learning, and ensemble learning, excluding the manifold feature transformation module. (4) Without Ensemble Learning: the ensemble learning module is removed to evaluate its impact on model performance. Note here that all the presented experimental results in this section are based on the SEED database using cross-subject single-session leaveone-out cross-validation. The decision tree is employed as the initial classifier, with 10 iterative loops for evaluation.\n\nTable  XI  presents the results of the ablation study, demonstrating that the proposed M3D model achieves the best performance by effectively integrating all modules. Using only the manifold feature transformation module leads to the lowest performance, with a classification accuracy of 44.73±14.39%. Focusing on dynamic distribution alignment and classifier learning improves the accuracy to 69.51±16.4%. Adding ensemble learning to dynamic distribution alignment and classifier learning further increases the accuracy to 71.01±13.42%, a 1.5% improvement, highlighting the effectiveness of ensemble learning in EEG-based emotion recognition. However, removing the manifold feature transformation module causes a significant accuracy drop from 84.57±9.49% (full model) to 71.01±13.42%, emphasizing the importance of feature transformation before distribution alignment. Removing the ensemble learning module also reduces the accuracy to 79.66±9.34%. Additionally, features in the Grassmann manifold space prove more robust than those in the original feature space, improving distribution alignment. All four ablation models significantly differ from the full model (p < 0.0001). We use an independent t-test for normally distributed groups and the Wilcoxon Rank Sum test for nonnormally distributed ones, with FDR correction applied to enhance result reliability.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "B. The Effect Of The Parameter",
      "text": "We conduct a series of experiments under the cross-subject single-session setting using the SEED database to investigate the impacts of key parameters (subspace dimension d, regularization parameters η, ρ, λ), the number of iterations ι, and the training sample size on the performance of the M3D model. The results show that the model achieves the best performance when the subspace dimension d = 128, and it demonstrates good robustness to variations in the regularization parameters within specific ranges. The model's classification performance fluctuates at the beginning of iterations and stabilizes after ι = 10, demonstrating the training advantage of M3D in cross-domain tasks. Using only 3,000 training samples, the model achieves an accuracy of 83.11% with a runtime of 37.4 seconds, outperforming several deep learning methods that use all available training samples. For instance, GECNN  [48] achieves an accuracy of 82.46±10.83% and MS-MDA  [45]  obtains 79.67±8.01%. More detailed descriptions and results are presented in Supplementary Materials Appendix C. And we provide a comprehensive theoretical analysis of the computational complexity of the proposed M3D framework in Supplementary Materials Appendix D.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "C. Investigating Factors Affecting Model Performance",
      "text": "We also conduct a series of experiments to study the factors that affect the performance of the model. First, replacing the Grassmann manifold with the Stiefel manifold in the manifold feature transformation module leads to a decrease in the model's accuracy to 67.94%, demonstrating the Grassmann manifold's superiority in extracting EEG emotion features. Second, comparing TCA with Principal Component Analysis (PCA) in manifold feature transformation, TCA shows better performance in terms of accuracy and F1-score, as it can map, reduce dimensions, and maximize the similarity between domains simultaneously. Third, experiments with fixed µ values in dynamic distribution alignment show that the adaptive µ significantly improves the model performance, increasing the accuracy by 4.26%. This mechanism mimics the brain's ability to prioritize different information streams based on contextual discrepancies, representing a more biologically plausible and data-responsive adaptation process. Finally, the evaluation of ensemble learning indicates that both the voting method and the LinkCluE method can enhance the reliability of classification results, with the LinkCluE method outperforming the voting method. More detailed descriptions and results are presented in the Supplementary Materials Appendix E.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "D. Visualization Of Feature Alignment",
      "text": "We employ t-SNE visualizations and EEG topographic maps to intuitively demonstrate the alignment of source and target features before and after adaptation. For t-SNE  [63]  visualization, we project the source and target features into a two-dimensional space before and after alignment. Distinct colors represent different emotion categories, while different shapes indicate domain origin (source vs. target). The visualization (Supplementary Materials Appendix F, Fig.  S13 ) clearly demonstrates that after alignment, the distributional discrepancy between the source and target domains is significantly reduced, while the separability between different emotion classes improves.\n\nAdditionally, we conduct topographic analysis by computing the mutual information between EEG patterns and prediction labels to identify important brain patterns for emotion recognition. The mutual information between the input features of the target domain and the model prediction results is estimated using a non-parametric method  [64] -  [66] . After normalizing the mutual information matrix, we visualize the average mutual information before and after alignment. The results in Fig.  3  show that the differences among the EEG topographic maps of the three emotions increase after alignment. The confusion matrix further supports this finding, demonstrating that the model achieves better classification performance following alignment. Moreover, the analysis reveals that the EEG patterns with higher informativeness for emotion recognition are mainly located in the prefrontal region  [67] ,  [68] . More detailed descriptions and results are presented in the Supplementary Materials Appendix F.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "In this paper, we propose a novel non-deep transfer learning framework, termed M3D, for cross-subject and cross-session EEG-based emotion recognition, as well as clinical mental disorder recognition. The framework employs a manifoldbased domain adaptation with dynamic distribution alignment, adaptively balancing marginal and conditional distributions in the Grassmann manifold space. This enhances intrinsic data representation and reduces domain disparity. Additionally, optimized classifier learning with ensemble techniques improves robustness and reliability in handling EEG data variations. Extensive experiments on three benchmark databases validate the framework under two evaluation protocols. Furthermore, we demonstrate the model's effectiveness on a real clinical MDD dataset. Compared to existing methods, M3D achieves promising results against non-deep approaches and performance comparable to deep learning models, with an average improvement of 6.67% in classification accuracy. These findings highlight the potential of non-deep transfer learning in addressing individual and session variations in affective brain-computer interfaces. On the other hand, in the dynamic distribution alignment module, we did not impose constraints on the adaptive factor µ during the iteration process, which may lead to suboptimal feature alignment in certain cases. Addressing this limitation in future work could further enhance the stability and generalizability of the framework. And we plan to investigate the adaptability of the M3D framework to more diverse clinical EEG datasets (e.g., epilepsy monitoring or sleep staging) and to evaluate the stability of model performance under varying preprocessing pipelines.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Appendix I Proof Of The Computation Μ",
      "text": "The a daptive factor µ is designed to dynamically adjust the emphasis between marginal and conditional distribution adaptation based on the differences between the source and target domains. We use the A-distance measurement  [32]  as a metric to estimate these differences, which is a wellestablished measure in the domain adaptation literature. The adaptive factor µ is computed as\n\nwhere d A is the marginal distribution difference between the source and target domains and d c is the conditional distribution difference between the class c (c ∈ [1, . . . , C]) from the source and target domains. When d A is large compared to C c=1 d c , it means the marginal distribution difference between the source and target domains is significant. In this case, µ will be close to 0, indicating that more weight should be placed on marginal distribution adaptation. Conversely, when C c=1 d c is large relative to d A , µ will be close to 1, suggesting that conditional distribution adaptation is more crucial.\n\nFor the marginal distribution difference, the A-distance d A provides a quantification of the marginal distribution difference between the source domain T S and the target domain T T , and is defined as\n\nwhere ϵ (h) denotes the hinge loss, which is derived from a binary classifier trained specifically to distinguish between samples from the source and target domains. A smaller Adistance indicates a closer marginal distribution.\n\nFor the conditional distribution difference, based on the provided label information for source data and the estimated pseudo-label information of target data, we conduct the Adistance measurement within each c class as\n\nHere, T\n\nS and T (c)\n\nT represent the subsets of data corresponding to class c from the source and target domains, respectively. By aggregating these class-specific A-distances ( C c=1 d c ) and comparing them with the marginal A-distance d A , we can determine the relative importance of marginal and conditional distribution adaptation.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Appendix Ii Statistically Analyze Eeg Signal Differences Across Subjects",
      "text": "To thoroughly investigate EEG signal differences across subjects in the SEED and SEED-IV datasets, we perform a comprehensive statistical analysis on the input EEG features, which includes both qualitative visualizations and quantitative comparisons. Specifically, our analysis includes three main parts: (1) low-dimensional feature visualization using t-Distributed Stochastic Neighbor Embedding (t-SNE) to reveal the overall data distribution and inter-subject separability;  (2)  box plot analysis to display the statistical distribution of EEG features for each subject across different emotional states; and (3) statistical difference matrices constructed through rigorous hypothesis testing to quantify the inter-subject variability under the same emotion condition.\n\nFirstly, we apply t-SNE to visualize the distribution of the input EEG features in a two-dimensional space. The results (Fig.  4  and 5 ) clearly show distinct clustering patterns for different emotional states and reflect the variability of feature distributions across subjects. This visualization offers an intuitive understanding of how subject-specific factors impact feature separability and emotional clustering.\n\nSecondly, we perform feature dimensionality reduction followed by box plot analysis for each subject and emotion category (Fig.  6 7 8 9 ). The box plots provide detailed statistical information, including medians, interquartile ranges, and outliers, and the analysis results reflect the extent of variation in input EEG features among individuals. We observe that, even within the same emotional condition, the input EEG feature distributions vary significantly across subjects, suggesting that individual differences play a substantial role in neural responses to emotional stimuli, and further, an enlarged intra-subject variance is observed under cross-session settings, indicating temporal fluctuations in individual neural patterns.\n\nThirdly, to systematically quantify the degree of intersubject difference, we perform statistical difference matrices using independent sample t-tests (or Wilcoxon rank-sum tests when normality assumptions are violated), with all p-values adjusted using False Discovery Rate (FDR) correction. The resulting matrices (Fig.  10 11 12 13 ) indicate statistically significant differences (p < 0.05) between subjects under the same emotion condition, underscoring the presence of subject-specific EEG patterns. These inter-individual differences may stem  In summary, our analysis based on three parts (lowdimensional visualizations using t-SNE, distributional comparisons through box plots, and statistical testing via difference matrices) demonstrates that EEG signals exhibit strong individual variability across subjects and emotional conditions, both visually and statistically. These findings highlight the complexity of cross-subject EEG emotion recognition and emphasize the need for robust adaptation mechanisms such as our proposed M3D framework.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Appendix Iii The Effect Of The Parameter",
      "text": "We conduct supplementary experiments under the crosssubject single-session setting using the SEED database to discuss the impacts of key parameters (subspace dimension d, regularization parameters η, ρ, λ), the number of iterations Additionally, to examine the classifier learning process, we also investigate how sensitive the model is to changes in the number of iterations (ι) in the learning process. Here, we vary ι value in a range of  [2, 5, 10, 20, 30, 50] . The corresponding learning results, expressed as accuracy rates, are 80.39%, 83.81%, 84.57%, 83.84%, 83.34% and 82.55% respectively. It shows the model's classification performance experiences fluctuations during the initial few iterations but achieves stability beyond ι = 10 iterations. This demonstrates the training advantage of M3D in cross-domain tasks.\n\nWe also explore the impact of the training sample size on the model's performance, as well as their computation time. We train the model using training set samples of different scales, with the number of training samples ranging from 1,000 to 20,000. The experimental results in Fig.  15  and Table XII show that our method achieves an accuracy of 83.11% with only 3,000 training samples, and the runtime is only 37.4 seconds. Notably, this outperforms several deep learning methods that use all available training samples. For instance, GECNN  [48]  achieves an accuracy of 82.46±10.83%, MS-MDA  [45]  obtains 79.67±8.01%, and R2G-STLT  [46]  reaches 77.96±6.38%.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Appendix Iv Theoretical Analysis Of The Computational",
      "text": "",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Complexity",
      "text": "We conduct a comprehensive theoretical analysis of the computational complexity of the proposed M3D framework, taking into account its main modules: manifold feature transformation, dynamic distribution alignment, classifier learning, and ensemble learning.      XIII , it shows that employing an adaptive µ can significantly enhance model performance, resulting in a 4.26% improvement in accuracy.\n\nThe results demonstrate that dynamic distribution alignment can effectively quantify the disparity between source and target domains and accurately evaluate the significance of the differences between marginal and conditional distributions. This capability is particularly advantageous for the classification performance, which is tailored for addressing variations between distributions.\n\nTo evaluate the impact of ensemble learning on model performance, we adjust our approach by incorporating various ensemble learning techniques, namely the averaging method, the voting method, and the LinkCluE method. Note here that LinkCluE is the method implemented in the proposed M3D. Based on the obtained classification results in the classifier learning (ŷ (ι) t (ι = 1, • • • , 10)), the effectiveness of ensemble learning is analyzed. Furthermore, we draw comparisons to scenarios devoid of ensemble learning strategies by directly employing the final classification outcome from the last iteration loop in the classifier learning, denoted as ŷ(ι) t (with ι = 10), which serves as our baseline. The experimental comparison results are reported in Table  XIII . It shows that ensemble learning with voting and LinkCluE could enhance the reliability of classification results and improve the overall performance. This confirms the effectiveness of ensemble algorithms. Furthermore, the LinkCluE method outperforms the voting method, demonstrating a superior ability to synthesize classification outcomes across iterations and to discern the underlying patterns of the classification results.",
      "page_start": 15,
      "page_end": 17
    },
    {
      "section_name": "Appendix Vi Visualization Of Feature Alignment",
      "text": "We employ t-SNE visualizations and EEG topographic maps to intuitively demonstrate the alignment of source and target features before and after adaptation. Specifically, we utilize the t-SNE algorithm  [63]  to visualize the source and target features from the SEED database both before and after alignment. The results are illustrated in Fig.  16 . To enhance clarity, we assign different colors to the samples with different emotions and assign different shapes to distinguish samples from different domains. The results demonstrate that M3D reduces the distribution discrepancy among the two domains throughout the training process. And it progressively separates the samples with different emotions to minimize the emotion classification error. In summary, the results provide supplementary evidence  to support the efficacy of the proposed M3D in EEG-based emotion.\n\nIn the topographic analysis, we identify the important brain patterns for emotion recognition by computing the mutual information between the brain patterns and prediction labels. The EEG topographic maps can intuitively display the spatial distribution characteristics of neural activities related to different emotions. Specifically, we identify the important brain patterns for emotion recognition by computing the mutual information between the brain patterns and prediction labels. We have the data input of the target domain, given as X t , which is a M × 310 matrix. Here, M is the sample size in the target domain, and 310 refers to the extracted DE features from 62 channels at 5 frequency bands. The corresponding model prediction result is Ŷt , with a size of M × 3. The columns in Ŷt indicate the prediction probabilities of different emotions (negative, neutral, and positive). Then, we estimate the mutual information between the input features X t and and the prediction result Ŷt using the non-parametric method as stated in  [64] -  [66] . The obtained mutual information matrix is termed as I(X t , Ŷt ) ∈ R 3×310 , indicating a quantification of the inherent dependence between the EEG patterns and the model prediction results. I(X t , Ŷt ) is further normalized to [0, 1], and a larger value refers to a greater informativeness of the EEG patterns to the model prediction. For the Ŷt before alignment, it is the prediction result obtained by directly inputting the DE features into the DT classifier. While for the Ŷt after alignment, it is the final result obtained by inputting the DE features into the model M3D.",
      "page_start": 17,
      "page_end": 18
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The overall architecture of the proposed M3D model,",
      "page": 2
    },
    {
      "caption": "Figure 1: , includes four",
      "page": 3
    },
    {
      "caption": "Figure 1: The source and target",
      "page": 3
    },
    {
      "caption": "Figure 2: The main process of the LinkCluE ensemble algorithm.",
      "page": 7
    },
    {
      "caption": "Figure 2: illustrates the two main steps involved in the",
      "page": 7
    },
    {
      "caption": "Figure 3: Visualization of topographic analysis, histogram distri-",
      "page": 12
    },
    {
      "caption": "Figure 3: show that the differences among the",
      "page": 12
    },
    {
      "caption": "Figure 4: and 5) clearly show distinct clustering patterns for",
      "page": 13
    },
    {
      "caption": "Figure 6: -9). The box plots provide detailed statisti-",
      "page": 13
    },
    {
      "caption": "Figure 10: -13) indicate statistically significant",
      "page": 13
    },
    {
      "caption": "Figure 4: A t-SNE visualization of the input EEG features in SEED database. The blue, green, and red colors indicate negative,",
      "page": 14
    },
    {
      "caption": "Figure 5: A t-SNE visualization of the input EEG in SEED-IV database. The blue, green, red and purple colors indicate negative,",
      "page": 14
    },
    {
      "caption": "Figure 14: , the experimental results",
      "page": 14
    },
    {
      "caption": "Figure 6: Box plots of the input EEG feature distribution across",
      "page": 15
    },
    {
      "caption": "Figure 7: Box plots of the input EEG feature distribution across",
      "page": 15
    },
    {
      "caption": "Figure 15: and Table XII show",
      "page": 15
    },
    {
      "caption": "Figure 8: Box plots of the input EEG feature distribution across",
      "page": 15
    },
    {
      "caption": "Figure 9: Box plots of the input EEG feature distribution across",
      "page": 16
    },
    {
      "caption": "Figure 10: Statistical difference matrix across subjects in a single session of SEED database, with independent samples t-test",
      "page": 17
    },
    {
      "caption": "Figure 11: Statistical difference matrix across subjects and across session of SEED database, with independent samples t-test",
      "page": 17
    },
    {
      "caption": "Figure 16: To enhance clarity, we assign",
      "page": 17
    },
    {
      "caption": "Figure 12: Statistical difference matrix across subjects in a single",
      "page": 18
    },
    {
      "caption": "Figure 13: Statistical difference matrix across subjects and across",
      "page": 18
    },
    {
      "caption": "Figure 14: Parameter sensitivity and convergence analysis of M3D (a) subspace dimension d, (b) regularization parameters η,",
      "page": 19
    },
    {
      "caption": "Figure 15: The mean accuracies (%) of the proposed M3D under",
      "page": 20
    },
    {
      "caption": "Figure 16: A visualization of the learned feature representations",
      "page": 20
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract— Emotion decoding using Electroencephalog-": "raphy (EEG)-based affective brain-computer interfaces (aB-",
          "is evaluated on three benchmark EEG emotion recogni-": "tion datasets using two validation protocols (cross-subject"
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "CIs)\nis\ncrucial\nfor\naffective\ncomputing\nbut\nis\nhindered",
          "is evaluated on three benchmark EEG emotion recogni-": "single-session and cross-subject cross-session), as well"
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "by EEG’s non-stationarity,\nindividual\nvariability,\nand the",
          "is evaluated on three benchmark EEG emotion recogni-": "as on a clinical EEG dataset of Major Depressive Disorder"
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "high cost of\nlarge-scale labeled data. Deep learning-based",
          "is evaluated on three benchmark EEG emotion recogni-": "(MDD). Experimental\nresults demonstrate\nthat M3D out-"
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "approaches, while effective,\nrequire substantial computa-",
          "is evaluated on three benchmark EEG emotion recogni-": "performs traditional non-deep learning methods, achieving"
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "tional\nresources and large datasets,\nlimiting their practi-",
          "is evaluated on three benchmark EEG emotion recogni-": "an average improvement of 6.67%, while achieving deep"
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "cality. To address these challenges, we propose Manifold-",
          "is evaluated on three benchmark EEG emotion recogni-": "learning-comparable performance with significantly lower"
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "based Domain Adaptation with Dynamic Distribution (M3D),",
          "is evaluated on three benchmark EEG emotion recogni-": "data and computational requirements. These findings high-"
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "a lightweight non-deep transfer\nlearning framework. M3D",
          "is evaluated on three benchmark EEG emotion recogni-": "light\nthe potential of M3D to enhance the practicality and"
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "includes four main modules: manifold feature transforma-",
          "is evaluated on three benchmark EEG emotion recogni-": "applicability of aBCIs in real-world scenarios."
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "tion, dynamic distribution alignment,\nclassifier\nlearning,",
          "is evaluated on three benchmark EEG emotion recogni-": ""
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "",
          "is evaluated on three benchmark EEG emotion recogni-": "Index Terms— Electroencephalography, Emotion Recog-"
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "and ensemble learning. The data undergoes a transforma-",
          "is evaluated on three benchmark EEG emotion recogni-": ""
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "",
          "is evaluated on three benchmark EEG emotion recogni-": "nition, Dynamic Domain Adaptation, Non-Deep Transfer"
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "tion onto an optimal Grassmann manifold space, enabling",
          "is evaluated on three benchmark EEG emotion recogni-": ""
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "",
          "is evaluated on three benchmark EEG emotion recogni-": "Learning, Manifold Transformation"
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "dynamic alignment of the source and target domains. This",
          "is evaluated on three benchmark EEG emotion recogni-": ""
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "process prioritizes both marginal and conditional distribu-",
          "is evaluated on three benchmark EEG emotion recogni-": ""
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "tions according to their significance, ensuring enhanced",
          "is evaluated on three benchmark EEG emotion recogni-": ""
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "adaptation efficiency across various types of data.\nIn the",
          "is evaluated on three benchmark EEG emotion recogni-": ""
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "",
          "is evaluated on three benchmark EEG emotion recogni-": "I.\nINTRODUCTION"
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "classifier learning, the principle of structural risk minimiza-",
          "is evaluated on three benchmark EEG emotion recogni-": ""
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "tion is integrated to develop robust classification models.",
          "is evaluated on three benchmark EEG emotion recogni-": "functioning as both a physiological and psy-"
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "This is complemented by dynamic distribution alignment,",
          "is evaluated on three benchmark EEG emotion recogni-": "E MOTION,"
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "which\nrefines\nthe\nclassifier\niteratively. Additionally,\nthe",
          "is evaluated on three benchmark EEG emotion recogni-": "state within an individual,\nserves\nas\na\nrich"
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "",
          "is evaluated on three benchmark EEG emotion recogni-": "repository\nof\ninsights\ninto mental\nand\nphysical well-being"
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "ensemble learning module aggregates the classifiers ob-",
          "is evaluated on three benchmark EEG emotion recogni-": ""
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "",
          "is evaluated on three benchmark EEG emotion recogni-": "[1]–[3]. Emotion recognition stands as a crucial method for"
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "tained at different stages of the optimization process, which",
          "is evaluated on three benchmark EEG emotion recogni-": ""
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "leverages the diversity of\nthe classifiers to enhance the",
          "is evaluated on three benchmark EEG emotion recogni-": "objectively discerning human emotional states. Currently,\nthe"
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "overall prediction accuracy. The proposed M3D framework",
          "is evaluated on three benchmark EEG emotion recogni-": "practice relies on the analysis of either non-physiological sig-"
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "",
          "is evaluated on three benchmark EEG emotion recogni-": "nals or physiological signals. In contrast\nto cues like facial ex-"
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "This work was supported by the National Natural Science Foundation",
          "is evaluated on three benchmark EEG emotion recogni-": "pressions, voice intonation, and gestures, Electroencephalog-"
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "of China under Grant 62276169,\nthe Medical-Engineering Interdisci-",
          "is evaluated on three benchmark EEG emotion recogni-": ""
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "",
          "is evaluated on three benchmark EEG emotion recogni-": "raphy (EEG)\nstands out as a non-invasive brain information"
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "plinary Research Foundation of Shenzhen University,\nthe Shenzhen-",
          "is evaluated on three benchmark EEG emotion recogni-": ""
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "Hong Kong Institute of Brain Science-Shenzhen Fundamental Research",
          "is evaluated on three benchmark EEG emotion recogni-": "collection technique [4], [5]. Its strong correlation with human"
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "Institutions under Grant 2022SHIBS0003,\nthe STI 2030-Major Projects",
          "is evaluated on three benchmark EEG emotion recogni-": "emotional\nstates,\ncoupled with\nits\nresistance\nto\ndeception,"
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "2021ZD0200500, and the Shenzhen Science and Technology Program",
          "is evaluated on three benchmark EEG emotion recogni-": ""
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "",
          "is evaluated on three benchmark EEG emotion recogni-": "positions EEG as a highly accurate means to objectively reflect"
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "(No. JCYJ20241202124222027 and JCYJ20241202124209011).",
          "is evaluated on three benchmark EEG emotion recogni-": ""
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "Ting\nLuo,\nLi\nZhang,\nand\nZhen\nLiang\nare with\nthe\nSchool\nof",
          "is evaluated on three benchmark EEG emotion recogni-": "human emotions [6]–[8]."
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "Biomedical Engineering, Medical School, Shenzhen University, Shen-",
          "is evaluated on three benchmark EEG emotion recogni-": ""
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "",
          "is evaluated on three benchmark EEG emotion recogni-": "EEG signals\nexhibit non-stationary characteristics\nand in-"
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "zhen 518060, China, also with the Guangdong Provincial Key Labora-",
          "is evaluated on three benchmark EEG emotion recogni-": ""
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "",
          "is evaluated on three benchmark EEG emotion recogni-": "dividual differences [9],\n[10],\nleading to significant variations"
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "tory of Biomedical Measurements and Ultrasound Imaging, Shenzhen",
          "is evaluated on three benchmark EEG emotion recogni-": ""
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "518060, China. E-mail: 2310247018@email.szu.edu.cn, and {lzhang,",
          "is evaluated on three benchmark EEG emotion recogni-": "in the distribution of EEG signals within the\nsame\nsubject"
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "janezliang}@szu.edu.cn.",
          "is evaluated on three benchmark EEG emotion recogni-": ""
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "",
          "is evaluated on three benchmark EEG emotion recogni-": "across different\ntime periods or between different\nindividuals."
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "Jing Zhang is with the School of Public Health, Shenzhen University",
          "is evaluated on three benchmark EEG emotion recogni-": ""
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "",
          "is evaluated on three benchmark EEG emotion recogni-": "Traditional machine\nlearning methods\nassume\nindependence"
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "Medical School, Shenzhen, China. E-mail: zhangjing1985zj@163.com.",
          "is evaluated on three benchmark EEG emotion recogni-": ""
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "Yingwei Qiu is Department of Radiology, Nanshan Hospital of Shen-",
          "is evaluated on three benchmark EEG emotion recogni-": "and identical distribution of data, causing emotion recognition"
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "zhen University, Shenzhen, China. E-mail: qiuyw1201@gmail.com.",
          "is evaluated on three benchmark EEG emotion recogni-": ""
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "",
          "is evaluated on three benchmark EEG emotion recogni-": "models\nto have poor generalization across different\nsessions"
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "Yaohua Hu is with the School of Mathematical Sciences, Shenzhen",
          "is evaluated on three benchmark EEG emotion recogni-": ""
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "",
          "is evaluated on three benchmark EEG emotion recogni-": "and different\nindividuals. This limitation poses a challenge in"
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "University, Shenzhen, China. E-mail: mayhhu@szu.edu.cn.",
          "is evaluated on three benchmark EEG emotion recogni-": ""
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "Zhuliang Yu is with Shien-Ming Wu School of\nIntelligent Engineering,",
          "is evaluated on three benchmark EEG emotion recogni-": "expanding the widespread adoption and application of\nthese"
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "South China University of Technology, Guangzhou, Guangdong, China,",
          "is evaluated on three benchmark EEG emotion recogni-": ""
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "",
          "is evaluated on three benchmark EEG emotion recogni-": "models\nto new sessions and individuals. On the other hand,"
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "and also with Institute for Super Robotics, Guangzhou, Guangdong,",
          "is evaluated on three benchmark EEG emotion recogni-": ""
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "",
          "is evaluated on three benchmark EEG emotion recogni-": "EEG signals encounter\nthe challenge of a small sample size,"
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "China. E-mail: zlyu@scut.edu.cn.",
          "is evaluated on three benchmark EEG emotion recogni-": ""
        },
        {
          "Abstract— Emotion decoding using Electroencephalog-": "*Corresponding authors: Zhen Liang.",
          "is evaluated on three benchmark EEG emotion recogni-": "typically comprising only a few hundred to a few thousand"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "(M3D). The M3D model maps EEG data onto a manifold"
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "space, where both marginal and conditional distributions are"
        },
        {
          "2": "Source\nTarget",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "dynamically analyzed and aligned. This process\nis\ncoupled"
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "with classifier\nlearning to minimize distributional differences"
        },
        {
          "2": "TCA",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "Target\nManifold Feature",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "Source",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "between source and target domains. Subsequently, ensemble"
        },
        {
          "2": "Transformation\nsubspace",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "GFK",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "subspace",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "learning integrates\nthe\nresults\nto enhance\nthe\nrobustness of"
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "cross-subject\nand cross-session emotion recognition. Exper-"
        },
        {
          "2": "Dynamic Distribution Alignment",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "Conditional \nMarginal",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "imental\nresults\ndemonstrate\nthat\nthe\nproposed M3D model"
        },
        {
          "2": "Distribution\nDistribution",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "achieves performance comparable to existing deep learning-"
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "based methods. The main contributions of the paper are sum-"
        },
        {
          "2": "Classifier Learning",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "marized below. First, a novel non-deep learning-based transfer"
        },
        {
          "2": "Ensemble Learning",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "learning model ((M3D) is proposed to address the variations in"
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "the distribution of EEG signals across different\nsessions and"
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "subjects. Second, both marginal distribution and conditional"
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "distribution are\nconsidered during domain alignment, where"
        },
        {
          "2": "Source\nTarget",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "a dynamically adjusted weighting of\nthe\nimportance of\nthe"
        },
        {
          "2": "Fig. 1: The overall architecture of\nthe proposed M3D model,",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "two distributions is incorporated. Third, an adaptive classifier"
        },
        {
          "2": "which\nincludes\nfour mains modules.\n(1) Manifold Feature",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "learning together with an ensemble learning is\nintroduced to"
        },
        {
          "2": "Transformation,\n(2) Dynamic Distribution Alignment,\n(3)",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "iteratively refine the classifier\nlearning process and leverage"
        },
        {
          "2": "Classifier Learning, (4) Ensemble Learning. Here, Source and",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "the strength of multiple learning models to optimize the pre-"
        },
        {
          "2": "T arget refer\nto the source and target data,\nrespectively. G is",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "diction of data labels. Last, extensive experiments are validated"
        },
        {
          "2": "the Grassmann manifold, projected by the geodesic function",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "on\nthree well-known\ndatabases,\nas well\nas\na\nclinical EEG"
        },
        {
          "2": "Ψ(t) (t ∈ [0, 1]). The points Ψ(0) and Ψ(1) correspond to the",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "database, with an average accuracy improvement of 6.67%."
        },
        {
          "2": "embedded representations of the source and target domains in",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "are the samples\nfrom the source and target\nG. xi,j\nand xk,l",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "II. RELATED WORK"
        },
        {
          "2": "data,respectively. z∞",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "i,j,k,l are the corresponding data represen-",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "Domain\nadaptation\ntargets\nthe\nreduction\nof\ndistribution"
        },
        {
          "2": "tation in the manifold space by the geodesic transformation.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "disparities between source and target domains, achieving the"
        },
        {
          "2": "H ∞\ndenotes\nthe\nreproducing kernel Hilbert\nspace\n(RKHS),",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "K",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "assumption that\nthe\nsource\nand target data\nis\nindependently"
        },
        {
          "2": "and G is the computed geodesics kernel.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "and\nidentically\ndistributed\n[13].\nExisting methods\ncan\nbe"
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "categorized into deep transfer\nlearning and non-deep transfer"
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "learning."
        },
        {
          "2": "time-consuming, making it challenging to amass a large vol-",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "ume of data. This difficulty in obtaining substantial data poses",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "A. Deep Transfer Learning Methods"
        },
        {
          "2": "a hurdle in training a reliable model.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "The current\ntrend in the development of\ntransfer\nlearning"
        },
        {
          "2": "Early EEG-based emotion recognition methods mainly rely",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "methods\nhas mainly\nfocused\non\ndeep\nlearning-based\nap-"
        },
        {
          "2": "on conventional non-deep machine learning algorithms\n[12],",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "proaches. For example, Li et al. [14] pioneered the use of the"
        },
        {
          "2": "including Support Vector Machine (SVM), K-Nearest Neigh-",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "Domain Adversarial Neural Network (DANN) in the study of"
        },
        {
          "2": "bor\n(KNN),\nand Linear Discriminant Analysis\n(LDA).\nIn",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "EEG-based emotion recognition. The optimization objective"
        },
        {
          "2": "these\nstudies,\nthere\nis\nan\nunderlying\nassumption\nthat EEG",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "was to establish a shared common feature representation that"
        },
        {
          "2": "samples are independent and identically distributed. However,",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "mitigates distribution differences between the source and target"
        },
        {
          "2": "variations among individuals\nsignificantly undermine the ef-",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "domains. Building\nupon\nthe\naligned\nfeature\nrepresentation"
        },
        {
          "2": "fectiveness of\ntraditional machine learning methods in cross-",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "et\nachieved with DANN, Zhou\nal.\n[15]\nintroduced\na\npro-"
        },
        {
          "2": "subject emotion recognition tasks. This results in suboptimal",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "totypical\nrepresentation-based\npairwise\nlearning\nframework,"
        },
        {
          "2": "performance\nand\npoor\ngeneralization\neffects. On\nthe\nother",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "aiming to further enhance cross-subject emotion recognition"
        },
        {
          "2": "hand,\ntransfer\nlearning techniques can be beneficial\nin trans-",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "performance. However,\nthe current DANN network is limited"
        },
        {
          "2": "ferring informative data from the source (training data) to the",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "by\nits\nexclusive\nfocus\non\naddressing marginal\ndistribution"
        },
        {
          "2": "target domain (test data), which could reduce the variations",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "differences in EEG data among different\nindividuals, neglect-"
        },
        {
          "2": "between different distributions in the modeling. More related",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "ing joint distribution differences. Considering that\nindividual"
        },
        {
          "2": "transfer\nlearning methods are detailed in Section II.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "differences\nin EEG arise\nfrom joint distribution differences"
        },
        {
          "2": "Compared to deep learning-based transfer\nlearning meth-",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "between different EEG signals, Li\net al.\n[16]\nextended the"
        },
        {
          "2": "ods, non-deep learning-based transfer\nlearning offers a more",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "DANN network by incorporating the Joint Domain Adapta-"
        },
        {
          "2": "lightweight approach. Unlike deep learning methods, non-deep",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "tion Network. This extension provides a more comprehensive"
        },
        {
          "2": "learning-based transfer requires less labeled data, which is bet-",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "approach that considers both marginal and joint distribution"
        },
        {
          "2": "ter suited to the current conditions in EEG tasks. In this paper,",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "differences in deep transfer\nlearning studies."
        },
        {
          "2": "we propose a novel non-deep learning-based transfer learning",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "Despite\nthe\nnotable\nachievements\nof\nthe\ndeep\ntransfer"
        },
        {
          "2": "framework for\ncross-subject\nemotion recognition, named as",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "learning methods\nin\nemotion\nrecognition\ntasks,\nthere\nare"
        },
        {
          "2": "Manifold-based Domain adaptation with Dynamic Distribution",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "certain limitations. Firstly, deep learning methods demand a"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "substantial\nnumber\nof\ntraining\nsamples. However,\nin EEG\nand extract domain-invariant\nfeatures\n[22]–[24], highlighting"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "tasks,\nthe\navailable\nsamples\nare\ntypically\nlimited\nto\na\nfew\nthe need for\nfeature transformation before distribution align-"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "hundred or a few thousand.\nInsufficient\nsamples would con-\nment."
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "strain performance,\nleading to overfitting and a decrease\nin\nTo address these challenges and improve non-deep transfer"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "generalization ability. Secondly, deep transfer\nlearning algo-\nlearning for EEG-based emotion recognition, we propose the"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "rithms\nheavily\nrely\non\na\nsubstantial\namount\nof\naccurately\nM3D model. It first uses TCA on differential entropy features"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "labeled source data.\nIf\nthere is noticeable noise in the labels\nfrom raw EEG signals to reduce dimensionality and narrow the"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "from the source domain,\nthe recognition performance would\ndistribution gap. Then,\nit embeds features into the Grassmann"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "significantly deteriorate [17]. However, acquiring a sufficient\nmanifold via Geodesic Flow Kernel\n(GFK)\n[25]. An adaptive"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "number of samples with precise label\ninformation is not only\nfactor is introduced to assess the significance of marginal and"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "costly but also time-consuming. Thirdly, deep transfer learning\nconditional distributions for better embedding. Upon achieving"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "methods\nexhibit\nhigh\ncomputational\ncomplexity,\nsubstantial\na\nrobust manifold\nfeature\nrepresentation, we\nproceed with"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "computational requirements, and involve slow and complicated\nclassifier\nlearning to optimize the classification performance"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "training processes. The reliance on high-end hardware facil-\nunder\nthe\ncomplex\nconditions\nof\ncross-subject\nand\ncross-"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "ities\nis\na\nsignificant demand, posing challenges\nin practical\nsession scenarios. In this phase, conventional machine learning"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "application environments where\nsuch resources may not be\nmethods like SVM, KNN, etc., are used as initial classifiers."
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "readily available. In contrast, non-deep transfer learning mod-\nFinally,\nan ensemble\nlearning approach harmonizes\nclassifi-"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "els exhibit\nlower complexity,\nreduced hardware requirements,\ncation results, enhancing the robustness and reliability of\nthe"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "and\nlightweight\nproperties,\nshowing\nthe\npotential\nfor\ngood\nframework for more accurate outcomes."
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "performance in widespread applications in real-life scenarios."
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "III. METHODOLOGY"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "B. Non-Deep Transfer Learning Methods"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "The proposed M3D model, shown in Fig. 1,\nincludes four"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "Non-deep transfer learning methods align the feature distri-\nmain modules: manifold feature transformation, dynamic dis-"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "bution between source and target domains using conventional\ntribution alignment, classifier learning, and ensemble learning."
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "machine\nlearning approaches. For\nexample, Pan et al.\n[18]\nFirst, input data is reduced to lower-dimensional features using"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "introduced a Transfer Component Analysis\n(TCA) algorithm\nTCA, and a manifold kernel G is learned to map the features"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "to learn transfer\ninformation via\nreproducing kernel Hilbert\ninto an optimal manifold space. Then, dynamic distribution"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "space\nand mitigate\nthe marginal distribution differences by\nalignment uses Structural Risk Minimization (SRM)\n[26]\nto"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "et\nmaximizing\nthe Mean Discrepancy\n(MMD). Zheng\nal.\nadaptively align feature distributions within this\nspace,\nim-"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "[19]\nintroduced\na\nTransductive\nParameter\nTransfer\n(TPT)\nproving adaptability to data variations. Next, a classifier is built"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "algorithm, which maps personalized classifiers parameters\nto\non the aligned features,\ninitially trained with traditional meth-"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "the target domain while minimizing the MMD for marginal\nods\nand iteratively refined by optimizing classification loss."
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "distribution. Wang et al.\n[20]\nintroduced a Stratified Transfer\nFinally, ensemble learning combines all optimized classifiers"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "Learning (STL) algorithm to construct a weak classifier with\nfrom the iterations\nto produce more robust\nresults\nfor cross-"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "source data and explore intra-class\nrelationships\nfor adaptive\nsubject and cross-session EEG-based emotion recognition."
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "spatial dimensionality reduction."
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "Although\nexisting\ntransfer\nlearning\nstrategies\nconsider"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "A. Manifold Feature Transformation"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "marginal\nor\nconditional\ndistributions\nto\nreduce\ndistribution"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "Traditional\nfeature\nalignment methods\ntypically\nrely\non\ndifferences,\nactual EEG disparities\nbetween\nsubjects\nare\nin"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "original data features, which can be problematic due to defor-\nthe joint distribution [16]. Current algorithms focus on either"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "mations within the original feature space. These deformations\nmarginal or conditional distributions, leading to inefficient data"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "make\nit\ndifficult\nto\neffectively\nreduce\ndifferences\nbetween\nuse. He\n[16] proposed Joint Distribution Adaptation (JDA)"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "the\nsource\nand target domains, negatively impacting model\nto match marginal\nand conditional distributions\nequally for"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "performance. Alternatively,\nthe manifold space offers\na\nso-\njoint distribution adjustment\n[21]. However, existing methods"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "lution by capturing the core aspects of data and representing\nassume\nequal\ncontributions\nfrom conditional\nand marginal"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "original information in a more compact form [27], [28]. Impor-\nprobability distributions, which is unrealistic. The importance"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "tantly,\nfeatures embedded in the manifold space often exhibit\nof these distributions in the joint probability distribution varies"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "favorable geometric properties, enhancing their effectiveness\nbased on domain similarity. Aligning marginal distribution is"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "in\ndomain\nadaptation\n[22]–[24].\nIn\nthe\npresent\nstudy,\nour\ncrucial when domains are highly dissimilar, while prioritizing"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "emphasis centers on leveraging the Grassmann manifold space\nconditional distribution is beneficial when domains are similar."
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "to encapsulate the intrinsic essence of the data, which enables\nDynamically adjusting the weighting of marginal and condi-"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "the\ncreation\nof\na more\nrobust\nand\ndimensionality-reduced\ntional distributions based on data characteristics is a key issue"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "representation and offers a more effective way to align features\nin non-deep transfer\nlearning."
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "across different domains.\nMost\nexisting\napproaches\nalign\nsource\nand\ntarget\ndistri-"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "butions directly in the original\nfeature\nspace, which can be\nIn this paper, we introduce a manifold feature transforma-"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "ineffective due to domain shifts. Some methods\nleverage the\ntion process,\nas\nillustrated in Fig. 1. The\nsource\nand target"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n3": "Grassmann manifold for subspace learning to transform data\ndomains are represented as DS = {(x1, y1), ..., (xn, yn)} and"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4": "respectively, with\nDT = {(xn+1, yn+1), ..., (xn+m, yn+m)},",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "We introduce an integration strategy to find the optimal t or"
        },
        {
          "4": "each domain featuring D-dimensional data representations.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "a set of t points between any two data samples xi and xj, and"
        },
        {
          "4": "Through TCA [18], we map\nboth\ndomains\ninto\na\nlower",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "utilize a geodesic kernel to facilitate the mapping process. This"
        },
        {
          "4": "d-dimensional\nfeature\nspace\nand\n(d ≪ D), denoted as TS",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "method transforms\nthe d-dimensional\nfeature\nspace\ninto an"
        },
        {
          "4": "dimensionality\nreduction\nprocess\nsimplifies\ndata,\nTT . This",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "infinite-dimensional\nfeature space,\nreducing domain drift. For"
        },
        {
          "4": "facilitating more\nefficient\ndata\nanalysis\nand\ninterpretation.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "each pair of data samples xi and xj, we iteratively calculate"
        },
        {
          "4": "Subsequently, we embed the d-dimensional\nfeature subspace",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "t\ntheir projections onto Ψ(t)\nas\nranges\nfrom 0 to 1. Each"
        },
        {
          "4": "into the Grassmann manifold, denoted as G. Our goal\nin this",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "data sample is mapped across the Grassmann manifold space,"
        },
        {
          "4": "manifold space is two-fold. We aim to increase the similarity",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "capturing the evolution of its feature representation during the"
        },
        {
          "4": "between the source and target domains’ data. Simultaneously,",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "domain transition. The result of\nthis process\nis\ntwo infinite-"
        },
        {
          "4": "we want\nto retain the unique attributes and inherent charac-",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "dimensional\nfeature vectors, denoted as z∞\nand z∞\n,\ni\nj"
        },
        {
          "4": "teristics of\neach domain’s data. This\napproach ensures\nthat",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "(cid:90) 1"
        },
        {
          "4": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "(cid:16)\n(cid:17)"
        },
        {
          "4": "while\naligning the domains\nin a\nshared feature\nspace,\ntheir",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "z∞\n=\ndt,\n(3)\nΨ (t)⊤ xi\ni"
        },
        {
          "4": "distinctiveness remains intact.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "0"
        },
        {
          "4": "To make\nthe data distributions of\nthe\ntwo domains more",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "(cid:90) 1"
        },
        {
          "4": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "(cid:16)\n(cid:17)"
        },
        {
          "4": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "z∞\n=\ndt,\n(4)\nΨ (t)⊤ xj"
        },
        {
          "4": "similar\nin the Grassmann manifold, we\nconstruct geodesics",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "j"
        },
        {
          "4": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "0"
        },
        {
          "4": "within G,\ntreating the\nsource\nand target domains\nas points",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "where z∞\nand z∞\nrepresent\nthe entire trajectory of each data\ni\nj"
        },
        {
          "4": "in this manifold space. By moving along these geodesics, we",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "sample in the manifold, providing a comprehensive view of"
        },
        {
          "4": "can gradually align the data distributions of the two domains.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "the\ndomain\nadaptation\nprocess. This\nstrategy\nimproves\nthe"
        },
        {
          "4": "When constructing geodesics, we apply the standard Euclidean",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "adaptability and accuracy of domain adaptation. From [25],"
        },
        {
          "4": "metric to the Riemannian manifold. This allows us to describe",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "the geodesic kernel\nis\nthe inner product of\nthe two infinite-"
        },
        {
          "4": "the geodesic using a parameterization function Ψ(t), where t",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "dimensional vectors, as"
        },
        {
          "4": "ranges from 0 to 1. Specifically, Ψ(0) and Ψ(1) correspond to",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "(cid:90) 1"
        },
        {
          "4": "the embedded representations of the source and target domains",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "(cid:16)\n(cid:17)⊤ (cid:16)\n(cid:17)"
        },
        {
          "4": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "(cid:10)z∞\n, z∞\n(cid:11) =\ndt\nΨ (t)⊤ xi\nΨ (t)⊤ xj"
        },
        {
          "4": "in G,\nrespectively. The geodesic\nis\nthe path from Ψ(0)\nto",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "i\nj\n(5)"
        },
        {
          "4": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "0"
        },
        {
          "4": "Ψ(1), representing the most direct connection between the two",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "= x⊤\ni Gxj,"
        },
        {
          "4": "domain representations\nin the manifold space. To ensure\na",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "4": "smooth and continuous\ntransformation across\nthe manifold,",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "where G ∈ RD×D is a positive semi-definite matrix, calculated"
        },
        {
          "4": "t\nthe\nintermediate values of\n(t ∈ (0, 1)), Ψ(t)\nsatisfies\nthe",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "as"
        },
        {
          "4": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "(cid:21)"
        },
        {
          "4": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "(cid:21) (cid:20)U ⊤\nT ⊤\n(cid:20)Λ1\nΛ2"
        },
        {
          "4": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "(cid:3)"
        },
        {
          "4": "following conditions:",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "1\nS\n.\n(6)\nG = (cid:2)TSU1\nRSU2"
        },
        {
          "4": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "U ⊤\nR⊤\nΛ2\nΛ3\n2"
        },
        {
          "4": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "S"
        },
        {
          "4": "(1)\nΨ (t) = TSU1Γ (t) − RSU2Σ (t) ,",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "Here, Λ1, Λ2, Λ3 are diagonal matrices, and the corresponding"
        },
        {
          "4": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "diagonal elements are given as"
        },
        {
          "4": "denote\nthe\nset of orthonormal base\nfor\nwhere TS ∈ RM ×d",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "4": "the source domain. M is\nthe sample size. RS ∈ RM ×(D−d)",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "sin 2θk"
        },
        {
          "4": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ",\nλ1,k = 1 +"
        },
        {
          "4": "signifies the orthogonal complement of TS, serving to encap-",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "2θk"
        },
        {
          "4": "sulate the dimensions outside the primary subspace spanned",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": " \ncos 2θk − 1"
        },
        {
          "4": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ",\n(7)\nλ2,k ="
        },
        {
          "4": "by TS. U1 ∈ Rd×d and U2 ∈ R(D−d)×d are both orthogonal",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "2θk"
        },
        {
          "4": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "sin 2θk"
        },
        {
          "4": "matrices, which can be obtained through the\nfollowing two",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ",\nλ3,k = 1 −"
        },
        {
          "4": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "2θk"
        },
        {
          "4": "sets of Singular Value Decomposition (SVD) as",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "where k ∈ [1, d]. The computed geodesics kernel G enables"
        },
        {
          "4": "T ⊤\n(2)\nS TT = U1ΓV ⊤, R⊤\nS TT = −U2ΣV ⊤.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "the\neffective\ntransformation\nof\na\ndata\nsample\nx,\ninitially"
        },
        {
          "4": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "positioned within the d-dimensional\nfeature\nspace,\ninto the"
        },
        {
          "4": "for\nthe target\nwhere TT ∈ RM ×d is the orthonormal base set",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "Grassmann manifold space, as"
        },
        {
          "4": "domain. Γ and Σ are d × d diagonal matrices. Their diagonal",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "√"
        },
        {
          "4": "(k = 1, 2, ..., d),\nrespectively.\nelements are cos θk\nand sin θk",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "Gx.\nz =\n(8)"
        },
        {
          "4": "The variable θk represents the angles between the orthonormal",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "This\ntransformed\nrepresentation,\nz,\nintegrates\nthe\nintrinsic"
        },
        {
          "4": "π 2\n. θk\nbases TS\nand TT , with 0 ≤ θ1 ≤ θ2 ≤ ... ≤ θd ≤",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "features of x as it is projected through the manifold, leveraging"
        },
        {
          "4": "measures the alignment between the domains in G.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "the geometrical properties of G to achieve\na more domain-"
        },
        {
          "4": "The\noperation Ψ(t)⊤x projects\na\nfeature\nvector x from",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "adaptive\nfeature\nrepresentation and address domain drift\nin"
        },
        {
          "4": "the d-dimensional feature space onto the Grassmann manifold",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "transfer\nlearning.\nIn the implement, we use DE features\nfor"
        },
        {
          "4": "space G, where x could be any data sample from the source",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "manifold feature transformation across all databases."
        },
        {
          "4": "or\ntarget domain. Selecting an appropriate t value is crucial,",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "4": "as\nit\ndirectly\ninfluences\nthe\nprojection\nquality\nof\nx. The",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "B. Dynamic Distribution Alignment"
        },
        {
          "4": "optimal t balances domain adaptation and the preservation of",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "4": "original features. This balance is essential for effective domain",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "After the manifold feature transformation,\nthe feature distri-"
        },
        {
          "4": "adaptation, as it allows for\nthe integration of domain-specific",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "butions between the source and target domains would exhibit"
        },
        {
          "4": "features while minimizing\nthe\nloss\nof\ncritical\ninformation",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "greater similarity than seen with the original feature represen-"
        },
        {
          "4": "during the transition.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "tation. Still, differences remain in both the marginal probability"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "distribution and conditional probability distribution. Current\nbe rewritten as"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "distribution adaptation methods commonly assume equal\nim-"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "Df (TS, TT ) = (1 − µ) ∥E [f (zs)] − E [f (zt)] ∥2\nHK"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "portance for both the marginal distribution P and conditional"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "(12)\n(cid:17)(cid:105)\n(cid:104)\n(cid:16)\n(cid:17)(cid:105)\n(cid:104)\n(cid:16)"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "distribution Q [29],\n[30]. This assumption, however, may not"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "C(cid:88) c\n.\n+µ\n− E\nf\nz(c)\n∥2\n∥E\nf\nz(c)\nt"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "s\nHK"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "always be\napplicable.\nIn scenarios where\nsignificant differ-"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "=1"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "ences exist between the source and target domains, the adapta-"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "Given the absence of label information for the target domain"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "tion of the marginal distribution takes on increased importance."
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "during the training phase, we cannot directly obtain f (z(c)\n) in\nt"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "Conversely,\nin situations where the source and target domains"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "(12). To tackle this\nissue, we approximate f (z(c)\n) using the\nt"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "are more\nclosely aligned,\nthe\nadaptation of\nthe\nconditional"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "class-conditional probability distribution [31]. Specifically,\nthe"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "distribution becomes more critical. Therefore, recognizing and"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "process begins by training an initial weak classifier,\nsuch as"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "adjusting the emphasis on marginal or conditional distribution"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "a SVM or KNN, on the source data. This initial classifier, f ,"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "adaptation based on the domains’\nspecific\ncharacteristics\nis"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "is\nsubsequently utilized to infer pseudo-labels\nfor\nthe target"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "essential\nfor achieving successful distribution adaptation."
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "data,\nserving as\na provisional\nsubstitute\nfor\nthe unavailable"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "µ\n∈\n[0, 1]\nIn\nthe\npresent\nstudy,\nan\nadaptive\nfactor\nis"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "true labels of the target domain. To enhance the reliability and"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "incorporated to dynamically adjust\nthe\nemphasis placed on"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "accuracy of the pseudo-labels, an iterative refinement strategy"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "marginal\nand\nconditional\ndistributions. This\nadaptive\nfactor"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "is employed to improve the decodability f . This involves using"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "allows\nfor\nthe flexible allocation of\nimportance between the"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "the outcomes of\nthe initial classifier\nto iteratively train subse-"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "two types of distributions based on the characteristics of\nthe"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "quent\nclassifiers,\nthereby progressively refining the pseudo-"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "source and target domains, which is defined as"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "labels. This\niterative process aims\nto converge towards more"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "accurate pseudo-labels,\nthus improving the model’s ability to"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "Df (TS, TT ) = (1 − µ) Df (Ps, Pt)"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "generalize\nfrom the\nsource\nto the\ntarget domain effectively."
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "(9)\nFurther details on the specific iterative refinement process and"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "C(cid:88) c\n+µ\nD(c)\n(Qs, Qt) ,"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "f\nhow it contributes\nto classifier\nlearning will be discussed in"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "=1"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "Section III-C."
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "In (12),\nthe adaptive factor µ is not a constant value prede-"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "where Df (TS, TT ) represents the dynamic distribution adap-"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "termined by prior knowledge. Instead, it is dynamically learned"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "tation\nbetween\nthe\nsource\nand\ntarget\ndomains within\nthe"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "based\non\nthe\nunderlying\ndata\ndistribution. This\nparameter"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "manifold feature space. Df (Ps, Pt) is the marginal distribution"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "comprehensively incorporates both the marginal distribution"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "adaptation,\nand D(c)\nis\nthe\nconditional distribution\n(Qs, Qt)"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "f\ndifference and conditional distribution difference between do-"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "adaptation for the class c (c ∈ {1, ..., C}). The total number of"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "mains. Specifically, we use the A-distance measurement\n[32]"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "classes is represented by C. When µ is close to 0,\nit\nindicates"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "as\na metric\nto estimate\nthe marginal distribution difference,"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "significant differences between the source and target domains,"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "denoted as dA, between the source and target domains, as"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "thus prioritizing the adaptation of marginal distributions. When"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "(13)\ndA (TS, TT ) = 2 (1 − 2ϵ (h)) ,\nµ\napproaches\n1,\nit\nsuggests\na\ngreater\nsimilarity\nbetween"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "the\ndomains,\nthereby\nemphasizing\nthe\nneed\nfor\nconditional"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "where\nϵ (h) denotes\nthe hinge\nloss\nfrom a binary classifier"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "distribution adaptation. At\nthe midpoint, µ = 0.5,\nthe model"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "distinguishing\nbetween\nsamples\nfrom the\nsource\nand\ntarget"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "treats both marginal and conditional distributions with equal"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "domains. For the conditional distribution difference, based on"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "importance."
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "the provided label\ninformation for\nsource data and the esti-"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "In (9),\nis\nthe marginal distribution adaptation Df (Ps, Pt)"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "mated pseudo-label\ninformation of target data, we conduct\nthe"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "defined as"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "A-distance measurement within each c class (c ∈ [1, . . . , C])"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "as"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "(cid:16)\n(cid:17)"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": ",\n(10)\nDf (Ps, Pt) = ∥E [f (zs)] − E [f (zt)] ∥2"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "T (c)\n, T (c)\n.\nHK\n(14)\ndc = dA"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "S\nT"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "are\nthe\nsource\nand target data\nrepresented\nHere, T (c)\nand T (c)\nrepresent\nthe subsets of data correspond-\nwhere zs\nand zt"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "S\nT"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "in the manifold feature\nspace, given in (8). f\nrefers\nto the\ning to class c from the source and target domains, respectively."
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "Then,\nthe adaptive factor µ can be estimated as\nclassifier. The term HK denotes the reproducing kernel Hilbert"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "space (RKHS), which is a space of functions generated by the"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "dA"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": ".\nµ = 1 −\n(15)"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "feature mapping Ψ(·). This mapping is critical\nfor capturing"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "dA + (cid:80)C\nc=1 dc"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "the\ncomplex structures within the data by projecting it\ninto"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "In the process of dynamic distribution alignment,\nthe adaptive"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "a higher-dimensional space where linear separability is more"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "factor µ is\nrecalculated\nat\neach\niteration,\nunderscoring\nthe"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "feasible. For\nthe computation of\nthe conditional distribution"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "iterative and responsive nature of this adaptation strategy. This\nadaptation D(c)\nit calculate in a similar way, as\n(Qs, Qt),"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "f"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "re-calibration is essential\nto ensure that\nthe adaptation process"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "(cid:17)(cid:105)\n(cid:104)\n(cid:16)\n(cid:17)(cid:105)\n(cid:104)\n(cid:16)\nremains\nattuned to the\nevolving similarities\nand differences"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "− E\nf\nz(c)\n∥2\n,\nD(c)\nf\nz(c)\n(11)\n(Qs, Qt) = ∥E\nt"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "f\nHK"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "between\nthe\nsource\nand\ntarget\ndomain\nfeature\ndistributions"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "as\nthey are represented in the manifold feature space. More"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "z(c)\nz(c)\nwhere\nand\nare\nthe\nc-class\nsource\nand\ntarget\ndata\ndetailed descriptions of µ are presented in the Supplementary\nt"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n5": "represented in the manifold feature space (8). Then, (9) could\nMaterials Appendix A."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6": "C. Classifier Learning",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "defined as"
        },
        {
          "6": "Through the\nintegration of manifold feature\nlearning and",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "1n\n2\nzi, zj ∈ TS"
        },
        {
          "6": "dynamic distribution alignment, we can formulate an adaptive",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": " \n,\n(21)"
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "1m\n2\nzi, zj ∈ TT\n(M0)ij ="
        },
        {
          "6": "classifier f at\nthe ιth iteration as",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "− 1\notherwise"
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "mn"
        },
        {
          "6": "(cid:16)\n(cid:17)2",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "n(cid:88) i\nf ι = arg min\n+ η∥f (ι−1)∥2\nyi − f (ι−1) (zi)",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "HK",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "f ∈HK\n(16)",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "=1",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "1n\n2c\nzi, zj ∈ T (c)\nS"
        },
        {
          "6": "+λDf (ι−1) (TS, TT ) + ρRf (ι−1) (TS, TT ) .",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "1m\n2c\nzi, zj ∈ T (c)\nT"
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "(cid:40)"
        },
        {
          "6": "ι\nHere,\nis\nthe\niteration\nnumber,\nis\nthe\ngroundtruth,\nand\nyi",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": " "
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ".\n(22)\n(Mc)ij =\nzi ∈ T (c)\n, zj ∈ T (c)\n1\nS\nT"
        },
        {
          "6": "is\nthe predicted classification result. ∥ · ∥2\nrefers\nto\nf (zi)",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "−"
        },
        {
          "6": "HK",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "mcnc\nzi ∈ T (c)\n, zj ∈ T (c)"
        },
        {
          "6": "the L2\nnorm in\nthe\nreproducing\nkernel Hilbert\nspace. The",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "T\nS"
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "0\notherwise"
        },
        {
          "6": "dynamic distribution adaptation term, Df (TS, TT ), is obtained",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "in (12). Further, a Laplacian regularization term, Rf (TS, TT ),",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "is incorporated to leverage the inherent geometric similarities",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "|. The third part\nin (16) (the\nHere, nc = |T (c)\n| and mc = |T (c)"
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "S\nT"
        },
        {
          "6": "among neighboring points in the manifold G [27]. Here,\nthe",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "Laplacian regularization term Rf (TS, TT )) could be computed"
        },
        {
          "6": "classifier f\ncan be\ninitialized using a\ncommonly employed",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "as"
        },
        {
          "6": "machine learning classifier, followed by adaptive optimization",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "n+m"
        },
        {
          "6": "through an iterative learning process. η, λ and ρ are regular-",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "(cid:88)"
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "Rf (TS, TT ) =\nWij (f (zi) − f (zj))2"
        },
        {
          "6": "ization parameters.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "i,j=1"
        },
        {
          "6": "In the SRM framework with the representer\ntheorem [27],",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "(23)"
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "n+m"
        },
        {
          "6": "f can be expanded as",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "(cid:88)"
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "=\nf (zi) Lijf (zj) ,"
        },
        {
          "6": "n+m",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "i,j=1"
        },
        {
          "6": "(cid:88) i\nf (z) =\n(17)\nβiK (zi, z) ,",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "is a pairwise similarity matrix, and L = D − W\nwhere Wij"
        },
        {
          "6": "=1",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "is\nthe normalized graph Laplacian matrix. D is\na diagonal"
        },
        {
          "6": "which\nincludes\nboth\nlabeled\nand\nunlabeled\nsamples\nfrom",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "matrix, with Dii = (cid:80)n+m\nj=1 Wij. W could be represented as"
        },
        {
          "6": "source and target domains, and β = (β1, β2, . . . , βn+m)T ∈",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "R(n+m)×1",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "is the coefficient vector. K is the kernel\nfunction.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "(cid:40)"
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "sim (zi, zj)\nzi ∈ Np (zj) orzj ∈ Np (zi) ,"
        },
        {
          "6": "For the SRM on the source domain,\nthe first part\nin (16) could",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "6": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "(24)\nWij ="
        },
        {
          "6": "be expressed as",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "0\notherwise,"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "78.46\nAUROC\n77.87\n77.80\n76.92\n77.28\n77.10": ""
        },
        {
          "78.46\nAUROC\n77.87\n77.80\n76.92\n77.28\n77.10": "95.83\n95.83\nNPV\n93.01\n93.56\n95.76\n95.74"
        },
        {
          "78.46\nAUROC\n77.87\n77.80\n76.92\n77.28\n77.10": ""
        },
        {
          "78.46\nAUROC\n77.87\n77.80\n76.92\n77.28\n77.10": ""
        },
        {
          "78.46\nAUROC\n77.87\n77.80\n76.92\n77.28\n77.10": ""
        },
        {
          "78.46\nAUROC\n77.87\n77.80\n76.92\n77.28\n77.10": ""
        },
        {
          "78.46\nAUROC\n77.87\n77.80\n76.92\n77.28\n77.10": "on\nthe\nlinks\nor\nrelationships\nbetween\ndata\npoints\nin\nterms"
        },
        {
          "78.46\nAUROC\n77.87\n77.80\n76.92\n77.28\n77.10": ""
        },
        {
          "78.46\nAUROC\n77.87\n77.80\n76.92\n77.28\n77.10": "of Connected-Triple-Based Similarity (CTS), SimRank-Based"
        },
        {
          "78.46\nAUROC\n77.87\n77.80\n76.92\n77.28\n77.10": ""
        },
        {
          "78.46\nAUROC\n77.87\n77.80\n76.92\n77.28\n77.10": "Similarity (SRS), and Approximate SimRank-Based Similarity"
        },
        {
          "78.46\nAUROC\n77.87\n77.80\n76.92\n77.28\n77.10": ""
        },
        {
          "78.46\nAUROC\n77.87\n77.80\n76.92\n77.28\n77.10": "(ASRS). This matrix serves as a foundation for understanding"
        },
        {
          "78.46\nAUROC\n77.87\n77.80\n76.92\n77.28\n77.10": ""
        },
        {
          "78.46\nAUROC\n77.87\n77.80\n76.92\n77.28\n77.10": "the complex, often non-linear relationships that exist between"
        },
        {
          "78.46\nAUROC\n77.87\n77.80\n76.92\n77.28\n77.10": ""
        },
        {
          "78.46\nAUROC\n77.87\n77.80\n76.92\n77.28\n77.10": "data points,\nleveraging the structure of the data to enhance the"
        },
        {
          "78.46\nAUROC\n77.87\n77.80\n76.92\n77.28\n77.10": ""
        },
        {
          "78.46\nAUROC\n77.87\n77.80\n76.92\n77.28\n77.10": "learning process. For\nthe consensus function,\nit\nintegrates the"
        },
        {
          "78.46\nAUROC\n77.87\n77.80\n76.92\n77.28\n77.10": ""
        },
        {
          "78.46\nAUROC\n77.87\n77.80\n76.92\n77.28\n77.10": "outcomes of various individual models within the ensemble by"
        },
        {
          "78.46\nAUROC\n77.87\n77.80\n76.92\n77.28\n77.10": ""
        },
        {
          "78.46\nAUROC\n77.87\n77.80\n76.92\n77.28\n77.10": "leveraging three different hierarchical agglomerative clustering"
        },
        {
          "78.46\nAUROC\n77.87\n77.80\n76.92\n77.28\n77.10": ""
        },
        {
          "78.46\nAUROC\n77.87\n77.80\n76.92\n77.28\n77.10": "algorithms, Single Linkage (SL), Complete Linkage (CL), and"
        },
        {
          "78.46\nAUROC\n77.87\n77.80\n76.92\n77.28\n77.10": ""
        },
        {
          "78.46\nAUROC\n77.87\n77.80\n76.92\n77.28\n77.10": "Average Linkage\n(AL). This\nstep\nis\ncrucial\nfor mitigating"
        },
        {
          "78.46\nAUROC\n77.87\n77.80\n76.92\n77.28\n77.10": ""
        },
        {
          "78.46\nAUROC\n77.87\n77.80\n76.92\n77.28\n77.10": "individual model biases and errors,\nleading to more accurate"
        },
        {
          "78.46\nAUROC\n77.87\n77.80\n76.92\n77.28\n77.10": ""
        },
        {
          "78.46\nAUROC\n77.87\n77.80\n76.92\n77.28\n77.10": "and generalizable results."
        },
        {
          "78.46\nAUROC\n77.87\n77.80\n76.92\n77.28\n77.10": ""
        },
        {
          "78.46\nAUROC\n77.87\n77.80\n76.92\n77.28\n77.10": ""
        },
        {
          "78.46\nAUROC\n77.87\n77.80\n76.92\n77.28\n77.10": "The overall algorithm of the proposed M3D is illustrated in"
        },
        {
          "78.46\nAUROC\n77.87\n77.80\n76.92\n77.28\n77.10": ""
        },
        {
          "78.46\nAUROC\n77.87\n77.80\n76.92\n77.28\n77.10": "Algorithm 1."
        },
        {
          "78.46\nAUROC\n77.87\n77.80\n76.92\n77.28\n77.10": ""
        },
        {
          "78.46\nAUROC\n77.87\n77.80\n76.92\n77.28\n77.10": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "LinkCluE Algorithm": "",
          "TABLE I: Cross-subject": "SEED-IV and SEED-V with different",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "Link-based Similarity",
          "TABLE I: Cross-subject": "",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "Matrix Function",
          "TABLE I: Cross-subject": "",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "",
          "TABLE I: Cross-subject": "",
          "single-session LOSO CV on SEED,": "SVM"
        },
        {
          "LinkCluE Algorithm": "",
          "TABLE I: Cross-subject": "Accuracy",
          "single-session LOSO CV on SEED,": "84.51"
        },
        {
          "LinkCluE Algorithm": "",
          "TABLE I: Cross-subject": "",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "",
          "TABLE I: Cross-subject": "Sensitivity",
          "single-session LOSO CV on SEED,": "80.82"
        },
        {
          "LinkCluE Algorithm": "",
          "TABLE I: Cross-subject": "Specificity",
          "single-session LOSO CV on SEED,": "86.25"
        },
        {
          "LinkCluE Algorithm": "Hierarchical Clustering",
          "TABLE I: Cross-subject": "Precision",
          "single-session LOSO CV on SEED,": "73.48"
        },
        {
          "LinkCluE Algorithm": "",
          "TABLE I: Cross-subject": "F1-score",
          "single-session LOSO CV on SEED,": "76.97"
        },
        {
          "LinkCluE Algorithm": "",
          "TABLE I: Cross-subject": "AUROC",
          "single-session LOSO CV on SEED,": "88.36"
        },
        {
          "LinkCluE Algorithm": "",
          "TABLE I: Cross-subject": "NPV",
          "single-session LOSO CV on SEED,": "90.51"
        },
        {
          "LinkCluE Algorithm": "Fig. 2: The main process of the LinkCluE ensemble algorithm.",
          "TABLE I: Cross-subject": "",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "",
          "TABLE I: Cross-subject": "Accuracy",
          "single-session LOSO CV on SEED,": "58.82"
        },
        {
          "LinkCluE Algorithm": "(1) Link-based Similarity Matrix Function: takes a set of input",
          "TABLE I: Cross-subject": "",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "",
          "TABLE I: Cross-subject": "Sensitivity",
          "single-session LOSO CV on SEED,": "66.05"
        },
        {
          "LinkCluE Algorithm": "vectors ˆyι\nt",
          "TABLE I: Cross-subject": "Specificity",
          "single-session LOSO CV on SEED,": "55.99"
        },
        {
          "LinkCluE Algorithm": "",
          "TABLE I: Cross-subject": "Precision",
          "single-session LOSO CV on SEED,": "37.10"
        },
        {
          "LinkCluE Algorithm": "its output.",
          "TABLE I: Cross-subject": "",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "",
          "TABLE I: Cross-subject": "F1-score",
          "single-session LOSO CV on SEED,": "47.51"
        },
        {
          "LinkCluE Algorithm": "matrix through a hierarchical clustering algorithm to produce",
          "TABLE I: Cross-subject": "",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "",
          "TABLE I: Cross-subject": "AUROC",
          "single-session LOSO CV on SEED,": "72.20"
        },
        {
          "LinkCluE Algorithm": "the final clustering result, denoted as ˆyt.",
          "TABLE I: Cross-subject": "NPV",
          "single-session LOSO CV on SEED,": "80.75"
        },
        {
          "LinkCluE Algorithm": "",
          "TABLE I: Cross-subject": "Accuracy",
          "single-session LOSO CV on SEED,": "64.41"
        },
        {
          "LinkCluE Algorithm": "Algorithm 1 The algorithm flow of the proposed M3D model.",
          "TABLE I: Cross-subject": "Sensitivity",
          "single-session LOSO CV on SEED,": "62.30"
        },
        {
          "LinkCluE Algorithm": "",
          "TABLE I: Cross-subject": "Specificity",
          "single-session LOSO CV on SEED,": "64.57"
        },
        {
          "LinkCluE Algorithm": "Require:",
          "TABLE I: Cross-subject": "",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "",
          "TABLE I: Cross-subject": "Precision",
          "single-session LOSO CV on SEED,": "11.59"
        },
        {
          "LinkCluE Algorithm": "- Data matrix X = [Xs, Xt],",
          "TABLE I: Cross-subject": "F1-score",
          "single-session LOSO CV on SEED,": "19.54"
        },
        {
          "LinkCluE Algorithm": "",
          "TABLE I: Cross-subject": "AUROC",
          "single-session LOSO CV on SEED,": "77.80"
        },
        {
          "LinkCluE Algorithm": "dimension\nd,\nregularization",
          "TABLE I: Cross-subject": "",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "",
          "TABLE I: Cross-subject": "NPV",
          "single-session LOSO CV on SEED,": "95.83"
        },
        {
          "LinkCluE Algorithm": "iteration number ι;",
          "TABLE I: Cross-subject": "",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "Ensure: Classifier f (·);",
          "TABLE I: Cross-subject": "",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "# Manifold Feature Transformation",
          "TABLE I: Cross-subject": "",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "1: Learn the manifold feature transformation kernel G and",
          "TABLE I: Cross-subject": "",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "",
          "TABLE I: Cross-subject": "links\nor",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "obtain the manifold feature representation z using (8);",
          "TABLE I: Cross-subject": "",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "",
          "TABLE I: Cross-subject": "of Connected-Triple-Based Similarity (CTS), SimRank-Based",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "2: Train a weak classifier based on the source data DS, and",
          "TABLE I: Cross-subject": "",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "",
          "TABLE I: Cross-subject": "Similarity (SRS), and Approximate SimRank-Based Similarity",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "get\ninitialized soft\nlabels ˆy0",
          "TABLE I: Cross-subject": "",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "t",
          "TABLE I: Cross-subject": "(ASRS). This matrix serves as a foundation for understanding",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "3: Construct\nthe\ngeodesic\nline",
          "TABLE I: Cross-subject": "",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "",
          "TABLE I: Cross-subject": "the complex, often non-linear relationships that exist between",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "formed features zs = z1:n and zt = zn+1:n+m;",
          "TABLE I: Cross-subject": "",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "",
          "TABLE I: Cross-subject": "",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "4:\nfor ι = 1 to l do",
          "TABLE I: Cross-subject": "",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "",
          "TABLE I: Cross-subject": "learning process. For",
          "single-session LOSO CV on SEED,": "the consensus function,"
        },
        {
          "LinkCluE Algorithm": "# Dynamic Distribution Alignment",
          "TABLE I: Cross-subject": "",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "",
          "TABLE I: Cross-subject": "outcomes of various individual models within the ensemble by",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "5:",
          "TABLE I: Cross-subject": "",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "",
          "TABLE I: Cross-subject": "leveraging three different hierarchical agglomerative clustering",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "pute M0 and Mc using (21) and (22);",
          "TABLE I: Cross-subject": "",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "",
          "TABLE I: Cross-subject": "algorithms, Single Linkage (SL), Complete Linkage (CL), and",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "# Classifier Learning",
          "TABLE I: Cross-subject": "",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "",
          "TABLE I: Cross-subject": "",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "6:\nSolve\nthe\nequation\nusing",
          "TABLE I: Cross-subject": "",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "",
          "TABLE I: Cross-subject": "individual model biases and errors,",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "obtain the classifier f using (26);",
          "TABLE I: Cross-subject": "",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "",
          "TABLE I: Cross-subject": "and generalizable results.",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "",
          "TABLE I: Cross-subject": "",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "7:\nUpdate the soft\nlabels for",
          "TABLE I: Cross-subject": "",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "",
          "TABLE I: Cross-subject": "The overall algorithm of the proposed M3D is illustrated in",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "8:\nend for",
          "TABLE I: Cross-subject": "",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "",
          "TABLE I: Cross-subject": "",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "9: Return the classifier f ;",
          "TABLE I: Cross-subject": "",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "# Ensemble Learning",
          "TABLE I: Cross-subject": "",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "",
          "TABLE I: Cross-subject": "",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "10: Perform ensemble\nlearning",
          "TABLE I: Cross-subject": "",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "labels in the loop {ˆyι\nt}l\nι=1.",
          "TABLE I: Cross-subject": "",
          "single-session LOSO CV on SEED,": ""
        },
        {
          "LinkCluE Algorithm": "",
          "TABLE I: Cross-subject": "A. Emotional Databases",
          "single-session LOSO CV on SEED,": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "8": "week break between sessions. The EEG signals are recorded",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "• Cross-subject\nsingle-session\nleave-one-subject-out"
        },
        {
          "8": "using the ESI Neuroscan system with 62 channels.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "cross-validation\n(Cross-subject\nsingle-session LOSO"
        },
        {
          "8": "The EEG preprocessing follows established procedures\nin",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "CV). The data samples from N subjects (SEED/SEED-"
        },
        {
          "8": "the literature. This approach aligns with the standard prepro-",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "IV: N=14, SEED-V: N=15)\nin the first session are used"
        },
        {
          "8": "cessing practices in emotion-related EEG research and ensures",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "as\nthe\nsource\ndomain,\nand\nthe\ndata\nsamples\nfrom the"
        },
        {
          "8": "consistency\nacross\nthe widely\nused\nSEED,\nSEED-IV,\nand",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "remaining\n1\nsubject\nin\nthe\nsame\nsession\nare\nused\nas"
        },
        {
          "8": "SEED-V datasets\nin the\nliterature. The process begins with",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "the\ntarget\ndomain. This\nprocedure\nrepeats N+1\ntimes"
        },
        {
          "8": "the downsampling of EEG signals\nto 200 Hz,\nfollowed by",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "(SEED/SEED-IV: N=14, SEED-V: N=15), ensuring that"
        },
        {
          "8": "the removal of artifacts such as the Electrooculogram (EOG)",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "each participant\nis\ntreated as\nthe target domain at\nleast"
        },
        {
          "8": "and Electromyography\n(EMG). Downsampling\nto\n200 Hz",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "once."
        },
        {
          "8": "effectively reduces computational complexity while preserving",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "• Cross-subject\ncross-session\nleave-one-subject-out"
        },
        {
          "8": "the temporal resolution necessary for emotion recognition [12],",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "cross-validation\n(Cross-subject\ncross-session\nLOSO"
        },
        {
          "8": "[34], [35]. A bandpass filter with a 0.3-50 Hz range is then ap-",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "CV). The data samples\nfrom N subjects\n(SEED/SEED-"
        },
        {
          "8": "plied to improve the signal quality, as the majority of emotion-",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "IV: N=14, SEED-V: N=15) across all\nthree sessions are"
        },
        {
          "8": "related electroencephalography activities occur below 50 Hz.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "used as\nthe source domain, and the data samples\nfrom"
        },
        {
          "8": "Following\nthis,\neach\ntrial was\nsegmented\ninto multiple\n1-",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "the\nremaining\n1\nsubject\nacross\nall\nthree\nsessions\nare"
        },
        {
          "8": "second data samples. To extract emotional related information,",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "used as\nthe\ntarget domain. This procedure\nalso repeats"
        },
        {
          "8": "DE features were extracted across five frequency bands: Delta",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "N+1\ntimes\n(SEED/SEED-IV: N=14,\nSEED-V: N=15),"
        },
        {
          "8": "(1-3 Hz), Theta (4-7 Hz), Alpha (8-13 Hz), Beta (14-30 Hz),",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "ensuring\nthat\neach\nparticipant\nis\ntreated\nas\nthe\ntarget"
        },
        {
          "8": "and Gamma (31-50 Hz). Thus, for each 1-second data sample,",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "domain at\nleast once."
        },
        {
          "8": "a\ntotal of 310 features\n(5 frequency bands × 62 channels)",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "8": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "In\nthe\npresent\nstudy, we\nadopt\nsix widely\nrecognized"
        },
        {
          "8": "were characterized, which serves as\nthe input\nfor\nthe model.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "8": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "machine-learning classifiers\nto generate the initial classifica-"
        },
        {
          "8": "To explore EEG signal\ninter-subject differences in SEED and",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "8": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "tion results\n(ˆy0\nt ) as mentioned in Section III-C. The adopted"
        },
        {
          "8": "SEED-IV datasets, we\nperform a\ncomprehensive\nstatistical",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "8": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "machine-learning\nclassifiers\ninclude KNN [37], SVM [38],"
        },
        {
          "8": "analysis of\ninput EEG features via qualitative visualizations",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "8": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "DT [39], Adaboost\nclassifier\n[40], GNB classifier\n[41],\nand"
        },
        {
          "8": "and quantitative comparisons. More detailed descriptions and",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "8": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "Bagging classifier [42]. In the SVM classifier,\nthe radial basis"
        },
        {
          "8": "results are presented in Supplementary Materials Appendix B.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "8": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "function (RBF) kernel\nis utilized.\nIn ensemble learning,\nthe"
        },
        {
          "8": "To further assess the generalizability of\nthe proposed M3D",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "8": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "application of\nthree\nsimilarity matrix functions\n(CTS, SRS,"
        },
        {
          "8": "model beyond controlled experimental conditions, we extend",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "8": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "and ASRS)\ncombined with three hierarchical\nagglomerative"
        },
        {
          "8": "model validation to a\nreal-world clinical EEG database ob-",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "8": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "clustering algorithms\n(SL, CL,\nand AL) yields\na\ntotal of 9"
        },
        {
          "8": "tained from Hospital Universiti Sains Malaysia (HUSM) [36].",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "8": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "possible\ncombinations. Here,\nthe\ncombination that\nachieves"
        },
        {
          "8": "This\ndataset\nconsists\nof EEG recordings\nfrom 27\nhealthy",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "8": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "the highest accuracy in predicting target domain labels will be"
        },
        {
          "8": "individuals (mean age: 38.28±15.64 years) and 29 individuals",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "8": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "chosen as\nthe optimal outcome. Notably, all\nthe best\nresults"
        },
        {
          "8": "diagnosed with MDD (Major Depressive Disorder) (mean age:",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "8": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "are achieved using the DT classifier and the CTS-SL approach."
        },
        {
          "8": "40.33±12.86 years). Participant\nclassification adhered to the",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "8": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "For performance evaluation, a comprehensive analysis of\nthe"
        },
        {
          "8": "international diagnostic criteria outlined in the Diagnostic and",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "8": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "model’s\neffectiveness\nis\nconducted\nusing multiple metrics,"
        },
        {
          "8": "Statistical Manual-IV (DSM-IV). This dataset used different",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "8": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "including accuracy, sensitivity, specificity, precision, F1-score,"
        },
        {
          "8": "devices\nand settings\ncompared to the SEED and SEED-IV",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "8": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "Area Under the Receiver Operating Characteristic Curve (AU-"
        },
        {
          "8": "databases, with EEG recordings\ncollected\nusing\nthe Brain",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "8": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "ROC) and Negative Predictive Value (NPV) metrics. We set"
        },
        {
          "8": "Master Discovery\namplifier,\n19\nelectrodes,\nand\na\nsampling",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "8": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "the manifold feature dimension d = 128 in all datasets except"
        },
        {
          "8": "rate of 256 Hz. Preprocessing involved bandpass filtering (0.5-",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "8": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "for\nthe MDD database (where d = 100). The regularization"
        },
        {
          "8": "70 Hz) and notch filtering at 50 Hz to eliminate power-line",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "8": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "parameters are set as η = 0.1, ρ = 1 and λ = 0.4."
        },
        {
          "8": "noise. This preprocessing pipeline is widely used and validated",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "8": "in\nthe\nliterature\n[36]\nas\na\nreliable\napproach\nfor\nenhancing",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "8": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "C. Experimental Results on Cross-Subject"
        },
        {
          "8": "data quality and processing efficiency without compromising",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "8": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "Single-Session"
        },
        {
          "8": "informative\nneural\nfeatures. The\nexperimental\nprotocol\nin-",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "8": "cluded EEG recordings under both open-eye (EO) and closed-",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "The experimental\nresults on SEED, SEED-IV and SEED-"
        },
        {
          "8": "eye (EC) conditions, each lasting 5 minutes. During the EO",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "V database\nunder\nthe\ncross-subject\nsingle-session\ncross-"
        },
        {
          "8": "condition, participants were instructed to remain relaxed while",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "validation are reported in Table I. For comprehensive evalua-"
        },
        {
          "8": "minimizing eye movements\nto maintain data quality. Similar",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "tion, we compare our method with existing approaches under"
        },
        {
          "8": "to the SEED, SEED-IV and SEED-V databases,\nthis database",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "identical\nexperimental\nconditions. The\ncomparison methods"
        },
        {
          "8": "also extracts DE features across five frequency bands.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "included in each table are selected based on the availability of"
        },
        {
          "8": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "reported results under the corresponding dataset and evaluation"
        },
        {
          "8": "B. Experimental Protocols",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "8": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "protocol\nto ensure fair and consistent evaluation. The baseline"
        },
        {
          "8": "In order to thoroughly assess the robustness and stability of",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "methods selected for comparison include a range of classical"
        },
        {
          "8": "the proposed model\nand ensure\na detailed comparison with",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "and recent approaches, covering representative state-of-the-art"
        },
        {
          "8": "existing\nliterature,\ntwo\ntypes\nof\nexperimental\nprotocols\nare",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "techniques published in the field of EEG-based domain adapta-"
        },
        {
          "8": "conducted.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "tion. Results marked with * are independently reproduced by"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": ""
        },
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": "SEED-IV and SEED-V with different"
        },
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": ""
        },
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": ""
        },
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": ""
        },
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": "Accuracy"
        },
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": "Sensitivity"
        },
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": ""
        },
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": "Specificity"
        },
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": "Precision"
        },
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": ""
        },
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": "F1-score"
        },
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": ""
        },
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": "AUROC"
        },
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": ""
        },
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": "NPV"
        },
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": ""
        },
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": ""
        },
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": "Accuracy"
        },
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": ""
        },
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": "Sensitivity"
        },
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": "Specificity"
        },
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": "Precision"
        },
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": ""
        },
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": "F1-score"
        },
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": ""
        },
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": "AUROC"
        },
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": ""
        },
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": "NPV"
        },
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": ""
        },
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": ""
        },
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": "Accuracy"
        },
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": ""
        },
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": "Sensitivity"
        },
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": ""
        },
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": "Specificity"
        },
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": ""
        },
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": "Precision"
        },
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": ""
        },
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": "F1-score"
        },
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": ""
        },
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": "AUROC"
        },
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": "NPV"
        },
        {
          "TABLE V: Cross-subject cross-session LOSO CV on SEED,": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE VI: Performance": "",
          "comparison on SEED using cross-": ""
        },
        {
          "TABLE VI: Performance": "",
          "comparison on SEED using cross-": ""
        },
        {
          "TABLE VI: Performance": "",
          "comparison on SEED using cross-": ""
        },
        {
          "TABLE VI: Performance": "",
          "comparison on SEED using cross-": "is underlined."
        },
        {
          "TABLE VI: Performance": "",
          "comparison on SEED using cross-": ""
        },
        {
          "TABLE VI: Performance": "Methods",
          "comparison on SEED using cross-": "Methods"
        },
        {
          "TABLE VI: Performance": "",
          "comparison on SEED using cross-": ""
        },
        {
          "TABLE VI: Performance": "",
          "comparison on SEED using cross-": ""
        },
        {
          "TABLE VI: Performance": "",
          "comparison on SEED using cross-": ""
        },
        {
          "TABLE VI: Performance": "KNN [15]",
          "comparison on SEED using cross-": "CORAL [15]"
        },
        {
          "TABLE VI: Performance": "",
          "comparison on SEED using cross-": ""
        },
        {
          "TABLE VI: Performance": "Adaboost\n[15]",
          "comparison on SEED using cross-": "RF [55]"
        },
        {
          "TABLE VI: Performance": "",
          "comparison on SEED using cross-": ""
        },
        {
          "TABLE VI: Performance": "SVM [55]",
          "comparison on SEED using cross-": "TCA [56]"
        },
        {
          "TABLE VI: Performance": "",
          "comparison on SEED using cross-": ""
        },
        {
          "TABLE VI: Performance": "SA [56]",
          "comparison on SEED using cross-": "GFK [56]"
        },
        {
          "TABLE VI: Performance": "",
          "comparison on SEED using cross-": ""
        },
        {
          "TABLE VI: Performance": "",
          "comparison on SEED using cross-": ""
        },
        {
          "TABLE VI: Performance": "TSception [55]",
          "comparison on SEED using cross-": "CDCN [55]"
        },
        {
          "TABLE VI: Performance": "MLP [55]",
          "comparison on SEED using cross-": "STFFNN [55]"
        },
        {
          "TABLE VI: Performance": "",
          "comparison on SEED using cross-": ""
        },
        {
          "TABLE VI: Performance": "STRNN [55]",
          "comparison on SEED using cross-": "3D-CNN [55]"
        },
        {
          "TABLE VI: Performance": "",
          "comparison on SEED using cross-": ""
        },
        {
          "TABLE VI: Performance": "MMResLSTM [55]",
          "comparison on SEED using cross-": "ACRNN [55]"
        },
        {
          "TABLE VI: Performance": "",
          "comparison on SEED using cross-": ""
        },
        {
          "TABLE VI: Performance": "MetaEmotionNet\n[55]",
          "comparison on SEED using cross-": "A-LSTM [56]"
        },
        {
          "TABLE VI: Performance": "",
          "comparison on SEED using cross-": ""
        },
        {
          "TABLE VI: Performance": "DGCNN [56]",
          "comparison on SEED using cross-": "EEG-SWTNS [56]"
        },
        {
          "TABLE VI: Performance": "",
          "comparison on SEED using cross-": ""
        },
        {
          "TABLE VI: Performance": "M3D",
          "comparison on SEED using cross-": ""
        },
        {
          "TABLE VI: Performance": "",
          "comparison on SEED using cross-": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "traditional\nsubject single-session LOSO CV. The second-best": ""
        },
        {
          "traditional\nsubject single-session LOSO CV. The second-best": "machine learning method result\nis underlined. Models repro-"
        },
        {
          "traditional\nsubject single-session LOSO CV. The second-best": ""
        },
        {
          "traditional\nsubject single-session LOSO CV. The second-best": "duced by the authors are indicated with an asterisk (*)."
        },
        {
          "traditional\nsubject single-session LOSO CV. The second-best": ""
        },
        {
          "traditional\nsubject single-session LOSO CV. The second-best": "Methods\nMethods\nPacc\nPacc"
        },
        {
          "traditional\nsubject single-session LOSO CV. The second-best": "Traditional machine learning methods"
        },
        {
          "traditional\nsubject single-session LOSO CV. The second-best": ""
        },
        {
          "traditional\nsubject single-session LOSO CV. The second-best": "TCA* [18]\n39.32/13.18\nSA* [23]\n33.45/09.92"
        },
        {
          "traditional\nsubject single-session LOSO CV. The second-best": "GFK* [25]\n36.44/16.14\nSVM* [38]\n49.39/18.13"
        },
        {
          "traditional\nsubject single-session LOSO CV. The second-best": "Adaboost* [40]\n50.01/21.71\nKNN* [51]\n33.14/13.62"
        },
        {
          "traditional\nsubject single-session LOSO CV. The second-best": ""
        },
        {
          "traditional\nsubject single-session LOSO CV. The second-best": "KPCA* [52]\n34.41/19.21\nCORAL* [53]\n51.10/21.12"
        },
        {
          "traditional\nsubject single-session LOSO CV. The second-best": ""
        },
        {
          "traditional\nsubject single-session LOSO CV. The second-best": "Deep learning methods"
        },
        {
          "traditional\nsubject single-session LOSO CV. The second-best": ""
        },
        {
          "traditional\nsubject single-session LOSO CV. The second-best": "DDC [54]\n28.70/09.40\nJDA [54]\n29.74/06.90"
        },
        {
          "traditional\nsubject single-session LOSO CV. The second-best": "DAN [54]\n41.24/12.12\nJD-IRT [54]\n60.17/11.06"
        },
        {
          "traditional\nsubject single-session LOSO CV. The second-best": ""
        },
        {
          "traditional\nsubject single-session LOSO CV. The second-best": "M3D\n65.25/07.87"
        },
        {
          "traditional\nsubject single-session LOSO CV. The second-best": ""
        },
        {
          "traditional\nsubject single-session LOSO CV. The second-best": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 14: 15%, it exhibits performance that is comparable, to some extent,",
      "data": [
        {
          "TABLE VII:\nPerformance\ncomparison\non\nSEED-IV using": "tra-\ncross-subject\ncross-session LOSO CV. The\nsecond-best",
          "TABLE": "",
          "IX:": "validation on MDD with different",
          "Cross-subject": "",
          "single-session": "",
          "10\nfold": "",
          "cross-": ""
        },
        {
          "TABLE VII:\nPerformance\ncomparison\non\nSEED-IV using": "ditional machine learning method result\nis underlined.",
          "TABLE": "",
          "IX:": "",
          "Cross-subject": "",
          "single-session": "",
          "10\nfold": "",
          "cross-": ""
        },
        {
          "TABLE VII:\nPerformance\ncomparison\non\nSEED-IV using": "",
          "TABLE": "Session",
          "IX:": "",
          "Cross-subject": "KNN",
          "single-session": "DT",
          "10\nfold": "GNB",
          "cross-": "Bagging"
        },
        {
          "TABLE VII:\nPerformance\ncomparison\non\nSEED-IV using": "Methods\nMethods\nPacc\nPacc",
          "TABLE": "",
          "IX:": "",
          "Cross-subject": "",
          "single-session": "",
          "10\nfold": "",
          "cross-": ""
        },
        {
          "TABLE VII:\nPerformance\ncomparison\non\nSEED-IV using": "",
          "TABLE": "",
          "IX:": "Accuracy",
          "Cross-subject": "82.72",
          "single-session": "82.72",
          "10\nfold": "81.06",
          "cross-": "81.06"
        },
        {
          "TABLE VII:\nPerformance\ncomparison\non\nSEED-IV using": "",
          "TABLE": "",
          "IX:": "Sensitivity",
          "Cross-subject": "81.86",
          "single-session": "81.86",
          "10\nfold": "78.99",
          "cross-": "78.99"
        },
        {
          "TABLE VII:\nPerformance\ncomparison\non\nSEED-IV using": "Traditional machine learning methods",
          "TABLE": "",
          "IX:": "",
          "Cross-subject": "",
          "single-session": "",
          "10\nfold": "",
          "cross-": ""
        },
        {
          "TABLE VII:\nPerformance\ncomparison\non\nSEED-IV using": "",
          "TABLE": "",
          "IX:": "Specificity",
          "Cross-subject": "82.45",
          "single-session": "82.45",
          "10\nfold": "81.81",
          "cross-": "81,81"
        },
        {
          "TABLE VII:\nPerformance\ncomparison\non\nSEED-IV using": "KPCA [14]\n51.76/12.89\nTPT [14]\n52.43/14.43",
          "TABLE": "EC",
          "IX:": "Precision",
          "Cross-subject": "81.93",
          "single-session": "81.93",
          "10\nfold": "81.93",
          "cross-": "81.93"
        },
        {
          "TABLE VII:\nPerformance\ncomparison\non\nSEED-IV using": "KNN [15]\n40.83/07.28\nAdaboost\n[15]\n53.44/09.12",
          "TABLE": "",
          "IX:": "F1-score",
          "Cross-subject": "81.89",
          "single-session": "81.89",
          "10\nfold": "80.43",
          "cross-": "80.43"
        },
        {
          "TABLE VII:\nPerformance\ncomparison\non\nSEED-IV using": "CORAL [15]\n49.44/09.09\nRF [55]\n34.70/06.60",
          "TABLE": "",
          "IX:": "AUROC",
          "Cross-subject": "82.15",
          "single-session": "82.15",
          "10\nfold": "80.40",
          "cross-": "80.40"
        },
        {
          "TABLE VII:\nPerformance\ncomparison\non\nSEED-IV using": "SVM [55]\n41.10/07.40\nTCA [56]\n56.56/13.77",
          "TABLE": "",
          "IX:": "NPV",
          "Cross-subject": "82.38",
          "single-session": "82.38",
          "10\nfold": "78.86",
          "cross-": "78.86"
        },
        {
          "TABLE VII:\nPerformance\ncomparison\non\nSEED-IV using": "SA [56]\n64.44/09.46\nGFK [56]\n64.38/11.41",
          "TABLE": "",
          "IX:": "",
          "Cross-subject": "",
          "single-session": "",
          "10\nfold": "",
          "cross-": ""
        },
        {
          "TABLE VII:\nPerformance\ncomparison\non\nSEED-IV using": "",
          "TABLE": "",
          "IX:": "Accuracy",
          "Cross-subject": "74.96",
          "single-session": "78.63",
          "10\nfold": "74.96",
          "cross-": "74.96"
        },
        {
          "TABLE VII:\nPerformance\ncomparison\non\nSEED-IV using": "Deep learning methods",
          "TABLE": "",
          "IX:": "",
          "Cross-subject": "",
          "single-session": "",
          "10\nfold": "",
          "cross-": ""
        },
        {
          "TABLE VII:\nPerformance\ncomparison\non\nSEED-IV using": "",
          "TABLE": "",
          "IX:": "Sensitivity",
          "Cross-subject": "73.98",
          "single-session": "77.68",
          "10\nfold": "73.98",
          "cross-": "73.98"
        },
        {
          "TABLE VII:\nPerformance\ncomparison\non\nSEED-IV using": "DANN [14]\n54.63/08.03\nDAN [14]\n58.87/08.13",
          "TABLE": "",
          "IX:": "Specificity",
          "Cross-subject": "75.79",
          "single-session": "79.24",
          "10\nfold": "75.79",
          "cross-": "75.79"
        },
        {
          "TABLE VII:\nPerformance\ncomparison\non\nSEED-IV using": "MLP [55]\n50.70/06.30\nSTFFNN [55]\n56.70/07.30",
          "TABLE": "EO",
          "IX:": "Precision",
          "Cross-subject": "73.97",
          "single-session": "77.67",
          "10\nfold": "73.97",
          "cross-": "73.97"
        },
        {
          "TABLE VII:\nPerformance\ncomparison\non\nSEED-IV using": "ACRNN [55]\n49.20/09.20\nMMResLSTM [55]\n51.10/11.40",
          "TABLE": "",
          "IX:": "F1-score",
          "Cross-subject": "73.97",
          "single-session": "77.67",
          "10\nfold": "73.97",
          "cross-": "73.97"
        },
        {
          "TABLE VII:\nPerformance\ncomparison\non\nSEED-IV using": "TSception [55]\n56.20/09.50\nSTRNN [55]\n53.20/07.40",
          "TABLE": "",
          "IX:": "AUROC",
          "Cross-subject": "74.89",
          "single-session": "78.46",
          "10\nfold": "74.89",
          "cross-": "74.89"
        },
        {
          "TABLE VII:\nPerformance\ncomparison\non\nSEED-IV using": "3D-CNN [55]\n54.10/10.90\nCDCN [55]\n54.50/13.10",
          "TABLE": "",
          "IX:": "NPV",
          "Cross-subject": "75.80",
          "single-session": "79.25",
          "10\nfold": "75.80",
          "cross-": "75.80"
        },
        {
          "TABLE VII:\nPerformance\ncomparison\non\nSEED-IV using": "MetaEmotionNet\n[55]\n61.20/08.30\nA-LSTM [56]\n55.03/09.28",
          "TABLE": "",
          "IX:": "",
          "Cross-subject": "",
          "single-session": "",
          "10\nfold": "",
          "cross-": ""
        },
        {
          "TABLE VII:\nPerformance\ncomparison\non\nSEED-IV using": "DGCNN [56]\n52.82/09.23\nEEG-SWTNS [56]\n66.72/10.19",
          "TABLE": "",
          "IX:": "",
          "Cross-subject": "",
          "single-session": "",
          "10\nfold": "",
          "cross-": ""
        },
        {
          "TABLE VII:\nPerformance\ncomparison\non\nSEED-IV using": "M3D\n64.90/10.23",
          "TABLE": "",
          "IX:": "TABLE X: Performance",
          "Cross-subject": "",
          "single-session": "comparison",
          "10\nfold": "on MDD using",
          "cross-": "cross-"
        },
        {
          "TABLE VII:\nPerformance\ncomparison\non\nSEED-IV using": "",
          "TABLE": "",
          "IX:": "subject 10-fold cross-validation. EC:",
          "Cross-subject": "",
          "single-session": "",
          "10\nfold": "closed-eye; EO: open-",
          "cross-": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 14: 15%, it exhibits performance that is comparable, to some extent,",
      "data": [
        {
          "eye. The\nsecond-best\ntraditional machine\nlearning method": ""
        },
        {
          "eye. The\nsecond-best\ntraditional machine\nlearning method": "result\nis underlined. Models\nreproduced by the\nauthors"
        },
        {
          "eye. The\nsecond-best\ntraditional machine\nlearning method": ""
        },
        {
          "eye. The\nsecond-best\ntraditional machine\nlearning method": "indicated with an asterisk (*)."
        },
        {
          "eye. The\nsecond-best\ntraditional machine\nlearning method": ""
        },
        {
          "eye. The\nsecond-best\ntraditional machine\nlearning method": "EC session\nEO session"
        },
        {
          "eye. The\nsecond-best\ntraditional machine\nlearning method": ""
        },
        {
          "eye. The\nsecond-best\ntraditional machine\nlearning method": "Methods"
        },
        {
          "eye. The\nsecond-best\ntraditional machine\nlearning method": "Pacc\nPacc"
        },
        {
          "eye. The\nsecond-best\ntraditional machine\nlearning method": "Traditional machine learning methods"
        },
        {
          "eye. The\nsecond-best\ntraditional machine\nlearning method": "SVM* [38]\n35.56/10.59\n35.56/10.59"
        },
        {
          "eye. The\nsecond-best\ntraditional machine\nlearning method": "CORAL* [53]\n60.27/15.25\n60.12/13.58"
        },
        {
          "eye. The\nsecond-best\ntraditional machine\nlearning method": ""
        },
        {
          "eye. The\nsecond-best\ntraditional machine\nlearning method": "RF* [61]\n69.44/19.57\n69.44/19.57"
        },
        {
          "eye. The\nsecond-best\ntraditional machine\nlearning method": ""
        },
        {
          "eye. The\nsecond-best\ntraditional machine\nlearning method": "Adaboost* [40]\n70.49/16.70\n71.00/19.62"
        },
        {
          "eye. The\nsecond-best\ntraditional machine\nlearning method": ""
        },
        {
          "eye. The\nsecond-best\ntraditional machine\nlearning method": "KNN* [51]\n72.25/18.32\n72.25/18.32"
        },
        {
          "eye. The\nsecond-best\ntraditional machine\nlearning method": ""
        },
        {
          "eye. The\nsecond-best\ntraditional machine\nlearning method": "SA* [23]\n74.21/11.98\n74.21/11.98"
        },
        {
          "eye. The\nsecond-best\ntraditional machine\nlearning method": ""
        },
        {
          "eye. The\nsecond-best\ntraditional machine\nlearning method": "Deep learning methods"
        },
        {
          "eye. The\nsecond-best\ntraditional machine\nlearning method": ""
        },
        {
          "eye. The\nsecond-best\ntraditional machine\nlearning method": "DAN* [14]\n64.19/05.30\n64.99/09.01"
        },
        {
          "eye. The\nsecond-best\ntraditional machine\nlearning method": ""
        },
        {
          "eye. The\nsecond-best\ntraditional machine\nlearning method": "DDC* [60]\n64.65/04.84\n65.69/08.08"
        },
        {
          "eye. The\nsecond-best\ntraditional machine\nlearning method": ""
        },
        {
          "eye. The\nsecond-best\ntraditional machine\nlearning method": "DANN* [14]\n66.49/11.08\n70.25/09.67"
        },
        {
          "eye. The\nsecond-best\ntraditional machine\nlearning method": ""
        },
        {
          "eye. The\nsecond-best\ntraditional machine\nlearning method": "DCORAL* [59]\n70.88/10.53\n74.49/08.17"
        },
        {
          "eye. The\nsecond-best\ntraditional machine\nlearning method": ""
        },
        {
          "eye. The\nsecond-best\ntraditional machine\nlearning method": "M3D\n82.72/11.35\n78.63/13.79"
        },
        {
          "eye. The\nsecond-best\ntraditional machine\nlearning method": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "(1)\nFor the SEED-IV database,\nthe performance across various\nmodel variations\nto isolate\nthe\neffects of key modules."
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "initial\nclassifiers\nis\nrelatively\nconsistent, with\nthe\nhighest\nManifold Feature Transformation Only: the initial decision"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "accuracy reaching 64.9% when DT is\nemployed as\nthe\nini-\ntree classifier\nis\ntrained to evaluate model performance after"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "tial classifier. Table VII\nreports\nthe performance comparison\napplying only the manifold feature transformation module. (2)"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "Dynamic Distribution Alignment and Classifier Learning\nresults against the existing machine learning and deep learning"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "Only:\nmethods. Compared\nto\nthe\nbest\nresults\nreported\nin\nvarious\nthe DE feature\nis used as\ninput,\nfocusing solely on"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "(3)\nliterature on machine\nlearning methods,\nthe proposed M3D\ndynamic\ndistribution\nalignment\nand\nclassifier\nlearning."
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "shows a performance improvement of 0.46%. Furthermore,\nit\nWithout Manifold Feature Transformation: the DE feature"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "performs comparably or even outperforms some deep learning\nserves as\ninput\nfor dynamic distribution alignment, classifier"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "methods,\nincluding MetaEmotionNet\n[55]\nand DAN [14].\nlearning, and ensemble learning, excluding the manifold fea-"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "(4) Without Ensemble Learn-\nThe superior cross-session validation performance observed in\nture transformation module."
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "ing:\nSEED-IV is\nlargely driven by its\nsmaller per-session sample\nthe\nensemble\nlearning module\nis\nremoved\nto\nevaluate"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "size,\nhigher\nemotional\nclass\ncomplexity,\nand more\ndiverse\nits\nimpact\non model\nperformance. Note\nhere\nthat\nall\nthe"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "multimodal\nstimuli, which\ncollectively\nbenefit\nfrom cross-\npresented\nexperimental\nresults\nin\nthis\nsection\nare based on"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "session aggregation. Meanwhile,\nthe SEED database’s more\nthe SEED database using cross-subject\nsingle-session leave-"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "stable\nper-session\nparadigm and\nstimulus\nuniformity make\none-out cross-validation. The decision tree is employed as the"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "single-session validation more effective and reliable.\ninitial classifier, with 10 iterative loops for evaluation."
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "For\nthe\nSEED-V database\nin\nthe\ncross-session\nsetting,\nTable XI presents the results of\nthe ablation study, demon-"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "performance across various initial classifiers remains relatively\nstrating\nthat\nthe\nproposed M3D model\nachieves\nthe\nbest"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "stable, with\nthe\nhighest\naccuracy\nreaching\n59.15%,\nagain\nperformance by effectively integrating all modules. Using only"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "observed with DT. As shown in Table VIII, the proposed M3D\nthe manifold feature transformation module leads to the lowest"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "model outperforms the best results reported in the literature for\nperformance, with a classification accuracy of 44.73±14.39%."
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "traditional machine learning methods by a margin of 11.69%,\nFocusing\non\ndynamic\ndistribution\nalignment\nand\nclassifier"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "demonstrating\na\nsubstantial\nimprovement\nin\ngeneralization\nlearning improves\nthe accuracy to 69.51±16.4%. Adding en-"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "capability.\nFurthermore, M3D delivers\nperformance\nthat\nis\nsemble learning to dynamic distribution alignment and classi-"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "comparable to or even surpasses certain deep learning models,\nfier\nlearning further\nincreases the accuracy to 71.01±13.42%,"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "including the state-of-the-art DCORAL method [59].\na\n1.5% improvement,\nhighlighting\nthe\neffectiveness\nof\nen-"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "semble learning in EEG-based emotion recognition. However,"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "removing the manifold feature transformation module causes"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "E. Experimental Results on clinical EEG database"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "a significant accuracy drop from 84.57±9.49% (full model) to"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "To assess the generalizability of\nthe proposed M3D model"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "71.01±13.42%, emphasizing the importance of\nfeature trans-"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "in\nreal-world\nclinical\napplications, we\nevaluate\nits\nperfor-"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "formation before distribution alignment. Removing the ensem-"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "mance\non\nthe MDD database\n(n=56).\nFollowing\nprevious"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "ble learning module also reduces the accuracy to 79.66±9.34%."
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "work [62], we adopt\nthe cross-subject\nsingle-session 10-fold"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "Additionally, features in the Grassmann manifold space prove"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "cross-validation protocol\nfor\nfair comparison. The validation"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "more robust than those in the original feature space, improving"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "results\nare\npresented\nin Table\nIX,\ndemonstrating\nconsistent"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "distribution alignment. All\nfour ablation models\nsignificantly"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "performance across various initial classifiers. In particular,\nthe"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "differ\nfrom the full model\n(p < 0.0001). We use an indepen-"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "highest accuracy is achieved using DT as the initial classifier,"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "dent\nt-test\nfor normally distributed groups and the Wilcoxon"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "reaching 82.72% in the EC session and 78.63% in the EO ses-"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "Rank Sum test\nfor nonnormally distributed ones, with FDR"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "sion. For comparative analysis, we report performance differ-"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n11": "correction applied to enhance result\nreliability."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE XI: Performance comparison without classifier": "",
          "learning and ensemble learning, with only classifier": "",
          "learning, and with": ""
        },
        {
          "TABLE XI: Performance comparison without classifier": "",
          "learning and ensemble learning, with only classifier": "",
          "learning, and with": ""
        },
        {
          "TABLE XI: Performance comparison without classifier": "Manifold Feature Transformation",
          "learning and ensemble learning, with only classifier": "Accuracy",
          "learning, and with": "AUROC"
        },
        {
          "TABLE XI: Performance comparison without classifier": "",
          "learning and ensemble learning, with only classifier": "",
          "learning, and with": ""
        },
        {
          "TABLE XI: Performance comparison without classifier": "✓",
          "learning and ensemble learning, with only classifier": "48.27/14.39",
          "learning, and with": "61.52"
        },
        {
          "TABLE XI: Performance comparison without classifier": "",
          "learning and ensemble learning, with only classifier": "69.51/16.40",
          "learning, and with": "77.16"
        },
        {
          "TABLE XI: Performance comparison without classifier": "",
          "learning and ensemble learning, with only classifier": "71.01/13.42",
          "learning, and with": "78.25"
        },
        {
          "TABLE XI: Performance comparison without classifier": "✓",
          "learning and ensemble learning, with only classifier": "79.66/09.34",
          "learning, and with": "84.68"
        },
        {
          "TABLE XI: Performance comparison without classifier": "✓",
          "learning and ensemble learning, with only classifier": "84.57/09.49",
          "learning, and with": "88.36"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "✓\n✓": "✓\n✓\n✓",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": "84.57/09.49\n76.04\n88.78\n76.94\n76.49\n88.36\n88.27"
        },
        {
          "✓\n✓": "",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": "to prioritize different\ninformation streams based on contextual"
        },
        {
          "✓\n✓": "",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": "discrepancies,\nrepresenting a more biologically plausible and"
        },
        {
          "✓\n✓": "",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": "data-responsive adaptation process. Finally,\nthe evaluation of"
        },
        {
          "✓\n✓": "",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": "ensemble learning indicates\nthat both the voting method and"
        },
        {
          "✓\n✓": "",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": "the LinkCluE method\ncan\nenhance\nthe\nreliability\nof\nclas-"
        },
        {
          "✓\n✓": "",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": "sification results, with the LinkCluE method outperforming"
        },
        {
          "✓\n✓": "",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": "the voting method. More detailed descriptions and results are"
        },
        {
          "✓\n✓": "",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": "presented in the Supplementary Materials Appendix E."
        },
        {
          "✓\n✓": "(a)",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": ""
        },
        {
          "✓\n✓": "",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": "D. Visualization of Feature Alignment"
        },
        {
          "✓\n✓": "",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": "We\nemploy\nt-SNE\nvisualizations\nand\nEEG topographic"
        },
        {
          "✓\n✓": "",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": "maps\nto intuitively demonstrate the alignment of\nsource and"
        },
        {
          "✓\n✓": "",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": "target\nfeatures before\nand after\nadaptation. For\nt-SNE [63]"
        },
        {
          "✓\n✓": "",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": "visualization, we project\nthe\nsource\nand target\nfeatures\ninto"
        },
        {
          "✓\n✓": "",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": "a two-dimensional space before and after alignment. Distinct"
        },
        {
          "✓\n✓": "",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": "colors\nrepresent different emotion categories, while different"
        },
        {
          "✓\n✓": "",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": "shapes\nindicate\ndomain\norigin\n(source\nvs.\ntarget). The\nvi-"
        },
        {
          "✓\n✓": "(b)",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": ""
        },
        {
          "✓\n✓": "",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": "sualization (Supplementary Materials Appendix F, Fig. S13)"
        },
        {
          "✓\n✓": "Fig. 3: Visualization of topographic analysis, histogram distri-",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": "clearly\ndemonstrates\nthat\nafter\nalignment,\nthe\ndistributional"
        },
        {
          "✓\n✓": "bution of mutual information between EEG patterns and model",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": "discrepancy\nbetween\nthe\nsource\nand\ntarget\ndomains\nis\nsig-"
        },
        {
          "✓\n✓": "predictions, and confusion matrices before and after alignment",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": "nificantly\nreduced, while\nthe\nseparability\nbetween\ndifferent"
        },
        {
          "✓\n✓": "(a) before alignment,\n(b) after alignment.",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": "emotion classes improves."
        },
        {
          "✓\n✓": "",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": "Additionally, we\nconduct\ntopographic\nanalysis\nby\ncom-"
        },
        {
          "✓\n✓": "",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": "puting\nthe mutual\ninformation\nbetween EEG patterns\nand"
        },
        {
          "✓\n✓": "",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": "prediction\nlabels\nto\nidentify\nimportant\nbrain\npatterns\nfor"
        },
        {
          "✓\n✓": "[48]\nachieves\nan accuracy of 82.46±10.83% and MS-MDA",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": ""
        },
        {
          "✓\n✓": "",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": "emotion\nrecognition. The mutual\ninformation\nbetween\nthe"
        },
        {
          "✓\n✓": "[45]\nobtains\n79.67±8.01%. More\ndetailed\ndescriptions\nand",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": ""
        },
        {
          "✓\n✓": "",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": "input\nfeatures of\nthe target domain and the model prediction"
        },
        {
          "✓\n✓": "results are presented in Supplementary Materials Appendix C.",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": ""
        },
        {
          "✓\n✓": "",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": "results is estimated using a non-parametric method [64]–[66]."
        },
        {
          "✓\n✓": "And we provide a comprehensive theoretical analysis of\nthe",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": ""
        },
        {
          "✓\n✓": "",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": "After normalizing the mutual\ninformation matrix, we visualize"
        },
        {
          "✓\n✓": "computational complexity of the proposed M3D framework in",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": ""
        },
        {
          "✓\n✓": "",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": "the\naverage mutual\ninformation before\nand after\nalignment."
        },
        {
          "✓\n✓": "Supplementary Materials Appendix D.",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": ""
        },
        {
          "✓\n✓": "",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": "The\nresults\nin Fig.\n3\nshow that\nthe\ndifferences\namong the"
        },
        {
          "✓\n✓": "",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": "EEG topographic maps of\nthe three emotions\nincrease after"
        },
        {
          "✓\n✓": "C.\nInvestigating factors affecting model performance",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": ""
        },
        {
          "✓\n✓": "",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": "alignment. The confusion matrix further supports this finding,"
        },
        {
          "✓\n✓": "We also conduct a series of experiments to study the factors",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": ""
        },
        {
          "✓\n✓": "",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": "demonstrating that the model achieves better classification per-"
        },
        {
          "✓\n✓": "that affect\nthe performance of\nthe model. First,\nreplacing the",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": ""
        },
        {
          "✓\n✓": "",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": "formance following alignment. Moreover,\nthe analysis reveals"
        },
        {
          "✓\n✓": "Grassmann manifold with the Stiefel manifold in the manifold",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": ""
        },
        {
          "✓\n✓": "",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": "that\nthe EEG patterns with higher informativeness for emotion"
        },
        {
          "✓\n✓": "feature\ntransformation module\nleads\nto\na\ndecrease\nin\nthe",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": ""
        },
        {
          "✓\n✓": "",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": "recognition are mainly located in the prefrontal\nregion [67],"
        },
        {
          "✓\n✓": "model’s\naccuracy to 67.94%, demonstrating the Grassmann",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": ""
        },
        {
          "✓\n✓": "",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": "[68]. More detailed descriptions and results are presented in"
        },
        {
          "✓\n✓": "manifold’s\nsuperiority\nin\nextracting EEG emotion\nfeatures.",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": ""
        },
        {
          "✓\n✓": "",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": "the Supplementary Materials Appendix F."
        },
        {
          "✓\n✓": "Second, comparing TCA with Principal Component Analysis",
          "79.66/09.34\n74.37\n82.21\n66.86\n70.41\n84.68\n86.92": ""
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n13": "adaptively balancing marginal and conditional distributions in\nFor\nthe\nconditional\ndistribution\ndifference,\nbased\non\nthe"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n13": "the Grassmann manifold space. This enhances\nintrinsic data\nprovided label\ninformation for source data and the estimated"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n13": "representation and reduces domain disparity. Additionally, op-\npseudo-label\ninformation of\ntarget data, we\nconduct\nthe A-"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n13": "timized classifier learning with ensemble techniques improves\ndistance measurement within each c class as"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n13": "robustness\nand\nreliability\nin\nhandling EEG data\nvariations."
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n13": "),\n(30)\ndc = dA(T c\nS, T (c)"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n13": "Extensive experiments on three benchmark databases validate"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n13": "the framework under\ntwo evaluation protocols. Furthermore,"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n13": "Here, T (c)\nand T (c)\nrepresent\nthe subsets of data correspond-"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n13": "S\nT"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n13": "we demonstrate\nthe model’s\neffectiveness on a\nreal\nclinical"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n13": "ing to class c from the source and target domains, respectively."
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n13": "MDD dataset. Compared to existing methods, M3D achieves"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n13": "By aggregating these class-specific A-distances ((cid:80)C"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n13": "c=1 dc) and"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n13": "promising\nresults\nagainst\nnon-deep\napproaches\nand\nperfor-"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n13": "comparing\nthem with\ncan\nthe marginal A-distance dA, we"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n13": "mance\ncomparable\nto\ndeep\nlearning models, with\nan\naver-"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n13": "determine the relative importance of marginal and conditional"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n13": "age improvement of 6.67% in classification accuracy. These"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n13": "distribution adaptation."
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n13": "findings highlight\nthe potential of non-deep transfer\nlearning"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n13": "in\naddressing\nindividual\nand\nsession\nvariations\nin\naffective"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n13": "APPENDIX II"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n13": "brain-computer\ninterfaces. On the other hand,\nin the dynamic"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n13": "STATISTICALLY ANALYZE EEG SIGNAL DIFFERENCES"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n13": "distribution alignment module, we did not\nimpose constraints"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n13": "ACROSS SUBJECTS"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n13": "on the adaptive factor µ during the iteration process, which"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n13": "To\nthoroughly\ninvestigate EEG signal\ndifferences\nacross\nmay lead to suboptimal\nfeature\nalignment\nin certain cases."
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n13": "subjects\nin the SEED and SEED-IV datasets, we perform a\nAddressing this limitation in future work could further enhance"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n13": "comprehensive statistical analysis on the input EEG features,\nthe stability and generalizability of\nthe framework. And we"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n13": "which includes both qualitative visualizations\nand quantita-\nplan to investigate the adaptability of\nthe M3D framework to"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n13": "tive\ncomparisons.\nSpecifically,\nour\nanalysis\nincludes\nthree\nmore diverse clinical EEG datasets (e.g., epilepsy monitoring"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n13": "main parts:\n(1)\nlow-dimensional\nfeature visualization using t-\nor sleep staging) and to evaluate the stability of model perfor-"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n13": "Distributed Stochastic Neighbor Embedding (t-SNE) to reveal\nmance under varying preprocessing pipelines."
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n13": "the overall data distribution and inter-subject separability;\n(2)"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 5: A t-SNE visualization of the input EEG in SEED-IV database. The blue, green, red and purple colors indicate negative,": "neutral, positive and fear emotions."
        },
        {
          "Fig. 5: A t-SNE visualization of the input EEG in SEED-IV database. The blue, green, red and purple colors indicate negative,": "from a range of\nfactors,\nincluding variations in physiological"
        },
        {
          "Fig. 5: A t-SNE visualization of the input EEG in SEED-IV database. The blue, green, red and purple colors indicate negative,": "structure, psychological characteristics, and individual cogni-"
        },
        {
          "Fig. 5: A t-SNE visualization of the input EEG in SEED-IV database. The blue, green, red and purple colors indicate negative,": "tive processing mechanisms."
        },
        {
          "Fig. 5: A t-SNE visualization of the input EEG in SEED-IV database. The blue, green, red and purple colors indicate negative,": "In\nsummary,\nour\nanalysis\nbased\non\nthree\nparts\n(low-"
        },
        {
          "Fig. 5: A t-SNE visualization of the input EEG in SEED-IV database. The blue, green, red and purple colors indicate negative,": "dimensional visualizations using t-SNE, distributional compar-"
        },
        {
          "Fig. 5: A t-SNE visualization of the input EEG in SEED-IV database. The blue, green, red and purple colors indicate negative,": "isons\nthrough box plots, and statistical\ntesting via difference"
        },
        {
          "Fig. 5: A t-SNE visualization of the input EEG in SEED-IV database. The blue, green, red and purple colors indicate negative,": "matrices) demonstrates\nthat EEG signals exhibit\nstrong indi-"
        },
        {
          "Fig. 5: A t-SNE visualization of the input EEG in SEED-IV database. The blue, green, red and purple colors indicate negative,": "vidual\nvariability\nacross\nsubjects\nand\nemotional\nconditions,"
        },
        {
          "Fig. 5: A t-SNE visualization of the input EEG in SEED-IV database. The blue, green, red and purple colors indicate negative,": "both\nvisually\nand\nstatistically. These findings\nhighlight\nthe"
        },
        {
          "Fig. 5: A t-SNE visualization of the input EEG in SEED-IV database. The blue, green, red and purple colors indicate negative,": "complexity\nof\ncross-subject EEG emotion\nrecognition\nand"
        },
        {
          "Fig. 5: A t-SNE visualization of the input EEG in SEED-IV database. The blue, green, red and purple colors indicate negative,": "emphasize\nthe need for\nrobust\nadaptation mechanisms\nsuch"
        },
        {
          "Fig. 5: A t-SNE visualization of the input EEG in SEED-IV database. The blue, green, red and purple colors indicate negative,": "as our proposed M3D framework."
        },
        {
          "Fig. 5: A t-SNE visualization of the input EEG in SEED-IV database. The blue, green, red and purple colors indicate negative,": ""
        },
        {
          "Fig. 5: A t-SNE visualization of the input EEG in SEED-IV database. The blue, green, red and purple colors indicate negative,": "APPENDIX III"
        },
        {
          "Fig. 5: A t-SNE visualization of the input EEG in SEED-IV database. The blue, green, red and purple colors indicate negative,": ""
        },
        {
          "Fig. 5: A t-SNE visualization of the input EEG in SEED-IV database. The blue, green, red and purple colors indicate negative,": "THE EFFECT OF THE PARAMETER"
        },
        {
          "Fig. 5: A t-SNE visualization of the input EEG in SEED-IV database. The blue, green, red and purple colors indicate negative,": ""
        },
        {
          "Fig. 5: A t-SNE visualization of the input EEG in SEED-IV database. The blue, green, red and purple colors indicate negative,": "We\nconduct\nsupplementary\nexperiments\nunder\nthe\ncross-"
        },
        {
          "Fig. 5: A t-SNE visualization of the input EEG in SEED-IV database. The blue, green, red and purple colors indicate negative,": "subject\nsingle-session\nsetting\nusing\nthe\nSEED database\nto"
        },
        {
          "Fig. 5: A t-SNE visualization of the input EEG in SEED-IV database. The blue, green, red and purple colors indicate negative,": "discuss\nthe\nimpacts of key parameters\n(subspace dimension"
        },
        {
          "Fig. 5: A t-SNE visualization of the input EEG in SEED-IV database. The blue, green, red and purple colors indicate negative,": "d, regularization parameters η, ρ, λ),\nthe number of iterations"
        },
        {
          "Fig. 5: A t-SNE visualization of the input EEG in SEED-IV database. The blue, green, red and purple colors indicate negative,": "ι,\nand\nthe\ntraining\nsample\nsize\non\nthe\nperformance\nof\nthe"
        },
        {
          "Fig. 5: A t-SNE visualization of the input EEG in SEED-IV database. The blue, green, red and purple colors indicate negative,": "d\nM3D model. The\nsubspace\ndimension\nis\nvaried\nacross"
        },
        {
          "Fig. 5: A t-SNE visualization of the input EEG in SEED-IV database. The blue, green, red and purple colors indicate negative,": "[10, 24, 36, 48, 64, 128, 256, 310], while the regularization pa-"
        },
        {
          "Fig. 5: A t-SNE visualization of the input EEG in SEED-IV database. The blue, green, red and purple colors indicate negative,": "rameters η, ρ, and λ are explored at multiple values within"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "Negative\nNegative"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "Neutral\nNeutral"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "Positive\nPositive"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "Fig. 6: Box plots of the input EEG feature distribution across"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "15 subjects and 3 emotions\n(negative, neutral, positive)\nin a"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "Fear\nsingle session of SEED database."
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "Fig. 8: Box plots of the input EEG feature distribution across"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "15 individuals and 4 emotions (negative, neutral, positive, fear)\nNegative"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "in a single session of SEED-IV database."
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "n\nLet\nand m be\nthe\nsample\nsizes\nof\nthe\nsource\nand"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "Neutral\ntarget domains\nrespectively,\nand d be\nthe dimensionality of"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "the\nreduced features.\nIn the dynamic distribution alignment"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "module,\nthe\ncomputational\ncost\nis mainly consumed in the"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "dimensionality reduction process\n(such as Transfer Compo-"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "nent Analysis, TCA)\nand\nthe\ncomputation\nof\nthe\ngeodesic"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "Positive\nkernel G on the Grassmann manifold. Specifically,\nthe time"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "complexity of\nthe dimensionality reduction is O(d(n + m)2)."
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "Calculating the geodesic kernel G involves matrix operations"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "such\nas\nsingular\nvalue\ndecomposition\n(SVD),\nleading\nto\na"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "Fig. 7: Box plots of the input EEG feature distribution across"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "time complexity of O(d3). Key computations in the dynamic"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "15 subjects and 3 emotions (negative, neutral, positive) in the"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "distribution alignment module include calculating the adaptive"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "cross session of SEED database."
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "factor µ and constructing the marginal and conditional distri-"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "bution matrices. The calculation of µ incurs a complexity of"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "O(n + m), while computing the distribution matrices requires"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "with the number of\ntraining samples\nranging from 1,000 to\nO(C(n + m)2), where C is the total number of classes. In the"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "20,000. The experimental results in Fig. 15 and Table XII show\nclassifier learning module,\nit\ninvolves solving an optimization"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "that our method achieves\nan accuracy of 83.11% with only\nproblem through matrix\ninversion, which\nleads\nto\na\ntime"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "3,000 training samples, and the runtime is only 37.4 seconds.\ncomplexity of O((n + m)3). Since this process\nis\niteratively"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "Notably,\nthis outperforms several deep learning methods that\nperformed l\ntimes,\nthe overall complexity of\nthis module is"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "use all available training samples. For\ninstance, GECNN [48]\nO(l(n+m+C(n+m)2 +(n+m)3)). In the ensemble learning"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "achieves an accuracy of 82.46±10.83%, MS-MDA [45] obtains\nmodule,\nthe\ncomputational\ncost\nis dominated by calculating"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "79.67±8.01%, and R2G-STLT [46]\nreaches 77.96±6.38%.\nthe\nsimilarity matrix with O(m2)\ncomplexity,\nfollowed\nby"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "hierarchical clustering with O(m2 log m) complexity."
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "By combining the complexities of all modules,\nthe overall"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "APPENDIX IV"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "time complexity of M3D can be expressed as:"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "THEORETICAL ANALYSIS OF THE COMPUTATIONAL"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "COMPLEXITY"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "We\nconduct\na\ncomprehensive\ntheoretical\nanalysis\nof\nthe\nO(cid:0)d(n + m)2 + d3 + l(n + m + C(n + m)2"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "computational complexity of\nthe proposed M3D framework,"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "+ (n + m)3 + m2 + m2 log m(cid:1).\n(31)"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "taking into account\nits main modules: manifold feature trans-"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "In\nour\nexperimental\nsetting,\nconsidering\nthat\nl,\nd, C and\nformation, dynamic distribution alignment, classifier learning,"
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING\n15": "m are\nrelatively small\ncompared to (n + m),\nthe dominant\nand ensemble learning."
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "AUROC": "NPV",
          "78.28": "89.03",
          "88.36": "88.27"
        },
        {
          "AUROC": "",
          "78.28": "",
          "88.36": ""
        },
        {
          "AUROC": "µ",
          "78.28": "0.5",
          "88.36": "dynamic"
        },
        {
          "AUROC": "Accuracy",
          "78.28": "81.55",
          "88.36": "84.57"
        },
        {
          "AUROC": "Sensitivity",
          "78.28": "75.42",
          "88.36": "76.04"
        },
        {
          "AUROC": "",
          "78.28": "",
          "88.36": ""
        },
        {
          "AUROC": "Specificity",
          "78.28": "84.60",
          "88.36": "88.78"
        },
        {
          "AUROC": "Precision",
          "78.28": "70.94",
          "88.36": "76.94"
        },
        {
          "AUROC": "F1-Score",
          "78.28": "73.11",
          "88.36": "76.49"
        },
        {
          "AUROC": "AUROC",
          "78.28": "86.16",
          "88.36": "88.36"
        },
        {
          "AUROC": "NPV",
          "78.28": "87.35",
          "88.36": "88.27"
        },
        {
          "AUROC": "Ensemble Methods",
          "78.28": "Averaging",
          "88.36": "LinkCluE"
        },
        {
          "AUROC": "",
          "78.28": "",
          "88.36": ""
        },
        {
          "AUROC": "Accuracy",
          "78.28": "78.60",
          "88.36": "84.57"
        },
        {
          "AUROC": "",
          "78.28": "",
          "88.36": ""
        },
        {
          "AUROC": "Sensitivity",
          "78.28": "79.58",
          "88.36": "76.04"
        },
        {
          "AUROC": "",
          "78.28": "",
          "88.36": ""
        },
        {
          "AUROC": "Specificity",
          "78.28": "78.23",
          "88.36": "88.78"
        },
        {
          "AUROC": "",
          "78.28": "",
          "88.36": ""
        },
        {
          "AUROC": "Precision",
          "78.28": "57.86",
          "88.36": "76.94"
        },
        {
          "AUROC": "F1-Score",
          "78.28": "67.00",
          "88.36": "76.49"
        },
        {
          "AUROC": "AUROC",
          "78.28": "84.17",
          "88.36": "88.36"
        },
        {
          "AUROC": "",
          "78.28": "",
          "88.36": ""
        },
        {
          "AUROC": "NPV",
          "78.28": "91.07",
          "88.36": "88.27"
        },
        {
          "AUROC": "",
          "78.28": "",
          "88.36": ""
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE XIII: Performance comparison using different modules.": "Stiefel manifold"
        },
        {
          "TABLE XIII: Performance comparison using different modules.": "67.94"
        },
        {
          "TABLE XIII: Performance comparison using different modules.": "63.30"
        },
        {
          "TABLE XIII: Performance comparison using different modules.": "70.37"
        },
        {
          "TABLE XIII: Performance comparison using different modules.": "52.77"
        },
        {
          "TABLE XIII: Performance comparison using different modules.": "57.55"
        },
        {
          "TABLE XIII: Performance comparison using different modules.": ""
        },
        {
          "TABLE XIII: Performance comparison using different modules.": "75.95"
        },
        {
          "TABLE XIII: Performance comparison using different modules.": "78.57"
        },
        {
          "TABLE XIII: Performance comparison using different modules.": "PCA"
        },
        {
          "TABLE XIII: Performance comparison using different modules.": "56.81"
        },
        {
          "TABLE XIII: Performance comparison using different modules.": "77.73"
        },
        {
          "TABLE XIII: Performance comparison using different modules.": "50.91"
        },
        {
          "TABLE XIII: Performance comparison using different modules.": "30.86"
        },
        {
          "TABLE XIII: Performance comparison using different modules.": "44.18"
        },
        {
          "TABLE XIII: Performance comparison using different modules.": "78.28"
        },
        {
          "TABLE XIII: Performance comparison using different modules.": "89.03"
        },
        {
          "TABLE XIII: Performance comparison using different modules.": ""
        },
        {
          "TABLE XIII: Performance comparison using different modules.": "0\n0.5"
        },
        {
          "TABLE XIII: Performance comparison using different modules.": "78.57\n81.55"
        },
        {
          "TABLE XIII: Performance comparison using different modules.": "70.89\n75.42"
        },
        {
          "TABLE XIII: Performance comparison using different modules.": ""
        },
        {
          "TABLE XIII: Performance comparison using different modules.": "82.80\n84.60"
        },
        {
          "TABLE XIII: Performance comparison using different modules.": "69.38\n70.94"
        },
        {
          "TABLE XIII: Performance comparison using different modules.": "70.13\n73.11"
        },
        {
          "TABLE XIII: Performance comparison using different modules.": "84.12\n86.16"
        },
        {
          "TABLE XIII: Performance comparison using different modules.": "83.80\n87.35"
        },
        {
          "TABLE XIII: Performance comparison using different modules.": "Last\nAveraging"
        },
        {
          "TABLE XIII: Performance comparison using different modules.": ""
        },
        {
          "TABLE XIII: Performance comparison using different modules.": "79.66\n78.60"
        },
        {
          "TABLE XIII: Performance comparison using different modules.": ""
        },
        {
          "TABLE XIII: Performance comparison using different modules.": "74.37\n79.58"
        },
        {
          "TABLE XIII: Performance comparison using different modules.": ""
        },
        {
          "TABLE XIII: Performance comparison using different modules.": "82.21\n78.23"
        },
        {
          "TABLE XIII: Performance comparison using different modules.": ""
        },
        {
          "TABLE XIII: Performance comparison using different modules.": "66.86\n57.86"
        },
        {
          "TABLE XIII: Performance comparison using different modules.": "70.41\n67.00"
        },
        {
          "TABLE XIII: Performance comparison using different modules.": "84.68\n84.17"
        },
        {
          "TABLE XIII: Performance comparison using different modules.": ""
        },
        {
          "TABLE XIII: Performance comparison using different modules.": "86.92\n91.07"
        },
        {
          "TABLE XIII: Performance comparison using different modules.": ""
        },
        {
          "TABLE XIII: Performance comparison using different modules.": ""
        },
        {
          "TABLE XIII: Performance comparison using different modules.": "efficacy of TCA in the manifold feature"
        },
        {
          "TABLE XIII: Performance comparison using different modules.": "transformation, we assess the model performance by compar-"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 11: Statistical difference matrix across\nsubjects and across": "and FDR correction (a) Negative,\n(b) Neutral,\n(c) Positive.",
          "session of SEED database, with independent\nsamples\nt-test": ""
        },
        {
          "Fig. 11: Statistical difference matrix across\nsubjects and across": "conditional distribution. As\nreported in Table XIII,\nit\nshows",
          "session of SEED database, with independent\nsamples\nt-test": "ensemble learning with voting and LinkCluE could enhance"
        },
        {
          "Fig. 11: Statistical difference matrix across\nsubjects and across": "that employing an adaptive µ can significantly enhance model",
          "session of SEED database, with independent\nsamples\nt-test": "the reliability of classification results and improve the overall"
        },
        {
          "Fig. 11: Statistical difference matrix across\nsubjects and across": "performance,\nresulting in a 4.26% improvement\nin accuracy.",
          "session of SEED database, with independent\nsamples\nt-test": "performance. This confirms the effectiveness of ensemble al-"
        },
        {
          "Fig. 11: Statistical difference matrix across\nsubjects and across": "The results demonstrate that dynamic distribution alignment",
          "session of SEED database, with independent\nsamples\nt-test": "gorithms. Furthermore,\nthe LinkCluE method outperforms the"
        },
        {
          "Fig. 11: Statistical difference matrix across\nsubjects and across": "can\neffectively\nquantify\nthe\ndisparity\nbetween\nsource\nand",
          "session of SEED database, with independent\nsamples\nt-test": "voting method, demonstrating a superior ability to synthesize"
        },
        {
          "Fig. 11: Statistical difference matrix across\nsubjects and across": "target\ndomains\nand\naccurately\nevaluate\nthe\nsignificance\nof",
          "session of SEED database, with independent\nsamples\nt-test": "classification\noutcomes\nacross\niterations\nand\nto\ndiscern\nthe"
        },
        {
          "Fig. 11: Statistical difference matrix across\nsubjects and across": "the differences between marginal and conditional distributions.",
          "session of SEED database, with independent\nsamples\nt-test": "underlying patterns of\nthe classification results."
        },
        {
          "Fig. 11: Statistical difference matrix across\nsubjects and across": "This capability is particularly advantageous for\nthe classifica-",
          "session of SEED database, with independent\nsamples\nt-test": ""
        },
        {
          "Fig. 11: Statistical difference matrix across\nsubjects and across": "tion performance, which is\ntailored for addressing variations",
          "session of SEED database, with independent\nsamples\nt-test": "APPENDIX VI"
        },
        {
          "Fig. 11: Statistical difference matrix across\nsubjects and across": "between distributions.",
          "session of SEED database, with independent\nsamples\nt-test": "VISUALIZATION OF FEATURE ALIGNMENT"
        },
        {
          "Fig. 11: Statistical difference matrix across\nsubjects and across": "To\nevaluate\nthe\nimpact\nof\nensemble\nlearning\non model",
          "session of SEED database, with independent\nsamples\nt-test": "We employ t-SNE visualizations and EEG topographic maps"
        },
        {
          "Fig. 11: Statistical difference matrix across\nsubjects and across": "performance, we adjust our approach by incorporating various",
          "session of SEED database, with independent\nsamples\nt-test": "to intuitively demonstrate the alignment of source and target"
        },
        {
          "Fig. 11: Statistical difference matrix across\nsubjects and across": "ensemble learning techniques, namely the averaging method,",
          "session of SEED database, with independent\nsamples\nt-test": "features before and after adaptation. Specifically, we utilize the"
        },
        {
          "Fig. 11: Statistical difference matrix across\nsubjects and across": "the voting method, and the LinkCluE method. Note here that",
          "session of SEED database, with independent\nsamples\nt-test": "t-SNE algorithm [63] to visualize the source and target features"
        },
        {
          "Fig. 11: Statistical difference matrix across\nsubjects and across": "LinkCluE is\nthe method implemented in the proposed M3D.",
          "session of SEED database, with independent\nsamples\nt-test": "from the SEED database both before and after alignment. The"
        },
        {
          "Fig. 11: Statistical difference matrix across\nsubjects and across": "Based on the obtained classification results\nin the\nclassifier",
          "session of SEED database, with independent\nsamples\nt-test": "results are illustrated in Fig. 16. To enhance clarity, we assign"
        },
        {
          "Fig. 11: Statistical difference matrix across\nsubjects and across": "learning (ˆy(ι)\n(ι = 1, · · ·\n, 10)),\nthe effectiveness of ensemble\nt",
          "session of SEED database, with independent\nsamples\nt-test": "different\ncolors\nto the\nsamples with different\nemotions\nand"
        },
        {
          "Fig. 11: Statistical difference matrix across\nsubjects and across": "learning is\nanalyzed. Furthermore, we draw comparisons\nto",
          "session of SEED database, with independent\nsamples\nt-test": "assign different\nshapes\nto distinguish samples\nfrom different"
        },
        {
          "Fig. 11: Statistical difference matrix across\nsubjects and across": "scenarios devoid of ensemble learning strategies by directly",
          "session of SEED database, with independent\nsamples\nt-test": "domains. The results demonstrate that M3D reduces\nthe dis-"
        },
        {
          "Fig. 11: Statistical difference matrix across\nsubjects and across": "employing the final\nclassification outcome\nfrom the\nlast\nit-",
          "session of SEED database, with independent\nsamples\nt-test": "tribution discrepancy among the two domains throughout\nthe"
        },
        {
          "Fig. 11: Statistical difference matrix across\nsubjects and across": "y(ι)\neration loop in the classifier\nlearning, denoted as\n(with\nt",
          "session of SEED database, with independent\nsamples\nt-test": "training process. And it progressively separates\nthe samples"
        },
        {
          "Fig. 11: Statistical difference matrix across\nsubjects and across": "ι = 10), which\nserves\nas\nour\nbaseline. The\nexperimental",
          "session of SEED database, with independent\nsamples\nt-test": "with different emotions to minimize the emotion classification"
        },
        {
          "Fig. 11: Statistical difference matrix across\nsubjects and across": "comparison results are reported in Table XIII.\nIt\nshows\nthat",
          "session of SEED database, with independent\nsamples\nt-test": "error. In summary,\nthe results provide supplementary evidence"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "REFERENCES": ""
        },
        {
          "REFERENCES": ""
        },
        {
          "REFERENCES": "[1] R. W. Picard, E. Vyzas,\nand J. Healey,\n“Toward machine\nemotional"
        },
        {
          "REFERENCES": "intelligence: analysis of affective physiological state,” IEEE Transactions"
        },
        {
          "REFERENCES": "on Pattern Analysis and Machine Intelligence, vol. 23, no. 10, pp. 1175–"
        },
        {
          "REFERENCES": ""
        },
        {
          "REFERENCES": "1191, 2001."
        },
        {
          "REFERENCES": "[2] A. Scarinzi, “The mind and its stories. narrative universals and human"
        },
        {
          "REFERENCES": "emotion,” Orbis Litterarum, vol. 61, no. 4, pp. 339–340, 2006."
        },
        {
          "REFERENCES": "[3]\nT. E. Kraynak, A. L. Marsland,\nand P.\nJ. Gianaros,\n“Neural mecha-"
        },
        {
          "REFERENCES": ""
        },
        {
          "REFERENCES": "nisms linking emotion with cardiovascular disease,” Current Cardiology"
        },
        {
          "REFERENCES": ""
        },
        {
          "REFERENCES": "Reports, vol. 20, pp. 1–10, 2018."
        },
        {
          "REFERENCES": "[4]\nZ. Liang, R. Zhou, L. Zhang, L. Li, G. Huang, Z. Zhang, and S.\nIshii,"
        },
        {
          "REFERENCES": "“EEGFuseNet: Hybrid unsupervised deep feature characterization and"
        },
        {
          "REFERENCES": ""
        },
        {
          "REFERENCES": "fusion\nfor\nhigh-dimensional\nEEG with\nan\napplication\nto\nemotion"
        },
        {
          "REFERENCES": ""
        },
        {
          "REFERENCES": "recognition,” IEEE Transactions on Neural Systems and Rehabilitation"
        },
        {
          "REFERENCES": "Engineering, vol. 29, pp. 1913–1925, 2021."
        },
        {
          "REFERENCES": "[5] W. Ye, Z. Zhang, F. Teng, M. Zhang, J. Wang, D. Ni, F. Li, P. Xu, and"
        },
        {
          "REFERENCES": "Z. Liang, “Semi-supervised dual-stream self-attentive adversarial graph"
        },
        {
          "REFERENCES": ""
        },
        {
          "REFERENCES": "contrastive\nlearning for\ncross-subject\neeg-based emotion recognition,”"
        },
        {
          "REFERENCES": ""
        },
        {
          "REFERENCES": "IEEE Transactions on Affective Computing, vol. 16, no. 1, pp. 290–305,"
        },
        {
          "REFERENCES": ""
        },
        {
          "REFERENCES": "2025."
        },
        {
          "REFERENCES": "[6]\nT. Musha, Y. Terasaki, H. A. Haque,\nand G. A.\nIvamitsky,\n“Feature"
        },
        {
          "REFERENCES": ""
        },
        {
          "REFERENCES": "and\nextraction\nfrom EEGs\nassociated with\nemotions,” Artificial Life"
        },
        {
          "REFERENCES": "Robotics, vol. 1, no. 1, pp. 15–19, 1997."
        },
        {
          "REFERENCES": ""
        },
        {
          "REFERENCES": "[7] M. Teplan et al., “Fundamentals of EEG measurement,” Measurement"
        },
        {
          "REFERENCES": "Science Review, vol. 2, no. 2, pp. 1–11, 2002."
        },
        {
          "REFERENCES": ""
        },
        {
          "REFERENCES": "[8]\nS. M. Alarcao and M.\nJ. Fonseca, “Emotions\nrecognition using EEG"
        },
        {
          "REFERENCES": ""
        },
        {
          "REFERENCES": "signals: A survey,” IEEE Transactions on Affective Computing, vol. 10,"
        },
        {
          "REFERENCES": "no. 3, pp. 374–393, 2017."
        },
        {
          "REFERENCES": "[9] V. Jayaram, M. Alamgir, Y. Altun, B. Schlkopf, and M. Grosse-Wentrup,"
        },
        {
          "REFERENCES": "“Transfer\nlearning in brain-computer\ninterfaces,” IEEE Computational"
        },
        {
          "REFERENCES": ""
        },
        {
          "REFERENCES": "Intelligence Magazine, vol. 11, no. 1, pp. 20–31, 2015."
        },
        {
          "REFERENCES": ""
        },
        {
          "REFERENCES": "[10] X. Si, H. He, J. Yu, and D. Ming, “Cross-subject emotion recognition"
        },
        {
          "REFERENCES": "brain–computer interface based on fnirs and dbjnet,” Cyborg and Bionic"
        },
        {
          "REFERENCES": "Systems, vol. 4, p. 0045, 2023."
        },
        {
          "REFERENCES": ""
        },
        {
          "REFERENCES": "[11]\nZ. Liang, S. Oba, and S. Ishii, “An unsupervised EEG decoding system"
        },
        {
          "REFERENCES": ""
        },
        {
          "REFERENCES": ""
        },
        {
          "REFERENCES": "for human emotion recognition,” Neural Networks, vol. 116, pp. 257–"
        },
        {
          "REFERENCES": "268, 2019."
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": "[12] W. L. Zheng\nand B. L. Lu,\n“Investigating\ncritical\nfrequency\nbands"
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": "and\nchannels\nfor EEG-based\nemotion\nrecognition with\ndeep\nneural"
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": "IEEE Transactions\nnetworks,”\non Autonomous Mental Development,"
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": "vol. 7, no. 3, pp. 1–1, 2015."
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": "[13] R. Zhou, W. Ye, Z. Zhang, Y. Luo, L. Zhang, L. Li, G. Huang, Y. Dong,"
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": "Y\n.-T. Zhang, and Z. Liang, “EEGMatch: Learning with incomplete labels"
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": "for semisupervised EEG-based cross-subject emotion recognition,” IEEE"
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": "Transactions on Neural Networks and Learning Systems, 2024."
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": "[14] H. Li, Y.-M.\nJin, W.-L. Zheng, and B.-L. Lu, “Cross-subject emotion"
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": ""
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": "Information\nrecognition\nusing\ndeep\nadaptation\nnetworks,”\nin Neural"
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": ""
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": "Processing: 25th International Conference,\nICONIP 2018, Siem Reap,"
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": ""
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": "Cambodia, December 13–16, 2018, Proceedings, Part V 25.\nSpringer,"
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": ""
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": "2018, pp. 403–413."
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": ""
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": "[15] R. Zhou, Z. Zhang, H. Fu, L. Zhang, L. Li, G. Huang, F. Li, X. Yang,"
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": ""
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": "Y\n. Dong, Y.-T. Zhang et al., “PR-PL: A novel prototypical representation"
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": ""
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": "based pairwise learning framework for emotion recognition using EEG"
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": ""
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": "signals,” IEEE Transactions on Affective Computing, vol. 15, no. 2, pp."
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": ""
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": "657–670, 2023."
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": ""
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": "[16]\nJ. Li, S. Qiu, C. Du, Y. Wang, and H. He, “Domain adaptation for EEG"
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": ""
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": "IEEE\nemotion\nrecognition\nbased\non\nlatent\nrepresentation\nsimilarity,”"
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": ""
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": "Transactions on Cognitive and Developmental Systems, vol. PP, no. 99,"
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": ""
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": "pp. 1–1, 2019."
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": ""
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": "[17] X. Zhu and X. Wu, “Class noise vs. attribute noise: A quantitative study,”"
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": ""
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": "Artificial\nIntelligence Review, vol. 22, pp. 177–210, 2004."
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": ""
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": "[18]\nS. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang, “Domain adaptation via"
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": "transfer component analysis,” IEEE Transactions on Neural Networks,"
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": "vol. 22, no. 2, pp. 199–210, 2011."
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": "[19] W.-L. Zheng and B.-L. Lu, “Personalizing EEG-based affective models"
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": "the twenty-fifth International\nwith transfer\nlearning,” in Proceedings of"
        },
        {
          "(c)\nregularization parameters ρ,\n(d)\nregularization parameters λ.": "Joint Conference on Artificial\nIntelligence, 2016, pp. 2732–2738."
        }
      ],
      "page": 19
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "parametric\nregression,” The American Statistician, vol. 46, no. 3, pp."
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "175–185, 1992."
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "[38] C. Cortes and V. Vapnik, “Support-vector networks,” Machine Learning,"
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "vol. 20, no. 3, pp. 273–297, 1995."
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "[39] H. H. Patel and P. Prajapati, “Study and analysis of decision tree based"
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "classification algorithms,” International Journal of Computer Sciences"
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "and Engineering, vol. 6, no. 10, pp. 74–78, 2018."
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "[40] Y. Freund and R. E. Schapire, “A decision-theoretic generalization of"
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "on-line learning and an application to boosting,” Journal of Computer"
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "and System Sciences, vol. 55, no. 1, pp. 119–139, 1997."
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "[41]\nI. Rish, “An empirical\nstudy of\nthe naive bayes classifier,” Journal of"
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "Universal Computer Science, vol. 1, no. 2, p. 127, 2001."
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "[42]\nL. Breiman, “Bagging predictors,” Machine Learning, vol. 24, pp. 123–"
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "140, 1996."
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "[43] B.-Q. Ma, H. Li, W.-L. Zheng,\nand B.-L. Lu,\n“Reducing the\nsubject"
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "variability of EEG signals with adversarial domain generalization,” in"
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "Neural Information Processing: 26th International Conference, ICONIP"
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "2019, Sydney, NSW, Australia, December 12–15, 2019, Proceedings, Part"
        },
        {
          "20": "Fig. 15: The mean accuracies (%) of the proposed M3D under",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "I 26.\nSpringer, 2019, pp. 30–42."
        },
        {
          "20": "different sample sizes.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "[44] Y. Li, W. Zheng, Y. Zong, Z. Cui, T. Zhang,\nand X. Zhou,\n“A bi-"
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "hemisphere domain adversarial neural network model for EEG emotion"
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "recognition,” IEEE Transactions on Affective Computing, vol. 12, no. 2,"
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "pp. 494–504, 2018."
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "[45] Q. She, C. Zhang, F. Fang, Y. Ma, and Y. Zhang, “Multisource associate"
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "domain\nadaptation\nfor\ncross-subject\nand\ncross-session EEG emotion"
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "recognition,” IEEE Transactions on Instrumentation and Measurement,"
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "vol. 72, pp. 1–12, 2023."
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "[46] C. Cheng, W. Liu, L. Feng, and Z. Jia, “Emotion recognition using hi-"
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "erarchical spatial-temporal\nlearning transformer\nfrom regional\nto global"
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "brain,” Neural Networks, vol. 179, p. 106624, 2024."
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "[47] Y. Li, B. Fu, F. Li, G. Shi,\nand W. Zheng,\n“A novel\ntransferability"
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "attention neural network model\nfor EEG emotion recognition,” Neuro-"
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "computing, vol. 447, pp. 92–101, 2021."
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "[48]\nJ. Pan, R. Liang, Z. He,\nJ. Li, Y. Liang, X. Zhou, Y. He, and Y. Li,"
        },
        {
          "20": "(a)\n(b)",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "“ST-SCGNN: a spatio-temporal self-constructing graph neural network"
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "for cross-subject eeg-based emotion recognition and consciousness de-"
        },
        {
          "20": "Fig. 16: A visualization of the learned feature representations",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "tection,” IEEE journal of biomedical and health informatics, vol. 28,"
        },
        {
          "20": "(a) before alignment, (b) after alignment. Here,\nthe circle and",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "no. 2, pp. 777–788, 2023."
        },
        {
          "20": "triangle represent the source domain (S) and the target domain",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "[49] M. Jim´enez-Guarneros and G. Fuentes-Pineda, “Learning a robust uni-"
        },
        {
          "20": "(T ). The red, blue and green colors\nindicate happy,\nsad and",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "fied domain adaptation framework for cross-subject EEG-based emotion"
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "Signal Processing\nrecognition,” Biomedical\nand Control,\nvol.\n86,\np."
        },
        {
          "20": "neural emotions.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "105138, 2023."
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "[50] H. Chen, M. Jin, Z. Li, C. Fan, J. Li, and H. He, “MS-MDA: Multisource"
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "marginal distribution adaptation for cross-subject and cross-session EEG"
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "emotion recognition,” Frontiers\nin Neuroscience, vol. 15, p. 778488,"
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "2021."
        },
        {
          "20": "the 25th Interna-\nview on subspace-based learning,” in Proceedings of",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "20": "tional Conference on Machine learning, 2008, pp. 376–383.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "[51] D. Coomans and D. L. Massart, “Alternative k-nearest neighbour rules in"
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "supervised pattern recognition: Part 1. k-nearest neighbour classification"
        },
        {
          "20": "[29] M. Long,\nJ. Wang, G. Ding,\nJ. Sun,\nand P. S. Yu,\n“Transfer\nfeature",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "by using alternative voting rules,” Analytica Chimica Acta, vol. 136, pp."
        },
        {
          "20": "learning with joint distribution adaptation,” in Proceedings of",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "20": "",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "15–27, 1982."
        },
        {
          "20": "International Conference on Computer Vision, 2013, pp. 2200–2207.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": ""
        },
        {
          "20": "[30]\nJ. Zhang, W. Li,\nand P. Ogunbona,\n“Joint geometrical\nand statistical",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "[52]\nS. Mika, B.\nSch¨olkopf, A.\nSmola, K.-R. M¨uller, M.\nScholz,\nand"
        },
        {
          "20": "the IEEE\nalignment\nfor visual domain adaptation,” in Proceedings of",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "G. R¨atsch, “Kernel PCA and de-noising in feature spaces,” Advances"
        },
        {
          "20": "Conference\non Computer Vision\nand Pattern Recognition,\n2017,\npp.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "in Neural\nInformation Processing Systems, vol. 11, 1998."
        },
        {
          "20": "1859–1867.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "[53] B. Sun,\nJ. Feng, and K. Saenko, “Return of\nfrustratingly easy domain"
        },
        {
          "20": "[31]\nJ. Wang, Y. Chen, S. Hao, W. Feng, and Z. Shen, “Balanced distribution",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "the AAAI Conference on Artificial\nIntel-\nadaptation,” in Proceedings of"
        },
        {
          "20": "adaptation for transfer learning,” in 2017 IEEE International Conference",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "ligence, vol. 30, no. 1, 2016."
        },
        {
          "20": "on Data Mining (ICDM).\nIEEE, 2017, pp. 1129–1134.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "[54]\nL. Zhu, F. Yu, A. Huang, N. Ying, and J. Zhang, “Instance-representation"
        },
        {
          "20": "[32]\nS. Ben-David,\nJ. Blitzer, K. Crammer,\nand F. Pereira,\n“Analysis\nof",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "transfer method based on joint distribution and deep adaptation for EEG"
        },
        {
          "20": "representations for domain adaptation,” Advances in Neural Information",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "emotion recognition,” Medical & Biological Engineering & Computing,"
        },
        {
          "20": "Processing Systems, vol. 19, 2006.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "vol. 62, no. 2, pp. 479–493, 2024."
        },
        {
          "20": "[33] N. Iam-On and S. Garrett, “LinkCluE: A matlab package for link-based",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "[55] X. Ning,\nJ. Wang, Y. Lin, X. Cai, H. Chen, H. Gou, X. Li,\nand"
        },
        {
          "20": "cluster ensembles,” Journal of Statistical Software, vol. 36, pp. 1–36,",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "Z.\nJia, “MetaEmotionNet:\nspatial-spectral-temporal based attention 3D"
        },
        {
          "20": "2010.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "dense network with Meta-learning for EEG emotion recognition,” IEEE"
        },
        {
          "20": "[34] W.-L. Zheng, W. Liu, Y. Lu, B.-L. Lu, and A. Cichocki, “EmotionMeter:",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "Transactions on Instrumentation and Measurement, vol. 73, pp. 1–13,"
        },
        {
          "20": "IEEE\nA multimodal\nframework\nfor\nrecognizing\nhuman\nemotions,”",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "2023."
        },
        {
          "20": "Transactions on Cybernetics, vol. 49, no. 3, pp. 1110–1122, 2018.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "[56] M. Cai, J. Chen, C. Hua, G. Wen, and R. Fu, “EEG emotion recognition"
        },
        {
          "20": "[35] W. Liu, J.-L. Qiu, W.-L. Zheng, and B.-L. Lu, “Comparing recognition",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "using\nEEG-SWTNS\nneural\nnetwork\nthrough\nEEG spectral\nimage,”"
        },
        {
          "20": "performance\nand robustness of multimodal deep learning models\nfor",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "Information Sciences, vol. 680, p. 121198, 2024."
        },
        {
          "20": "multimodal emotion recognition,” IEEE Transactions on Cognitive and",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "[57] G. Li, N. Chen, Y. Niu, Z. Xu, Y. Dong, J. Jin, and H. Zhu, “MSLTE:"
        },
        {
          "20": "Developmental Systems, vol. 14, no. 2, pp. 715–729, 2021.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "multiple\nself-supervised\nlearning\ntasks\nfor\nenhancing EEG emotion"
        },
        {
          "20": "[36] W. Mumtaz, L. Xia,\nS.\nS. A. Ali, M. A. M. Yasin, M. Hussain,",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "recognition,” Journal of Neural Engineering, vol. 21, no. 2, p. 024003,"
        },
        {
          "20": "and A. S. Malik, “Electroencephalogram (EEG)-based computer-aided",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "2024."
        },
        {
          "20": "technique to diagnose major depressive disorder\n(MDD),” Biomedical",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "[58] Y. Zhou, F. Li, Y. Li, Y. Ji, G. Shi, W. Zheng, L. Zhang, Y. Chen, and"
        },
        {
          "20": "Signal Processing and Control, vol. 31, pp. 108–115, 2017.",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "R. Cheng,\n“Progressive graph convolution network for EEG emotion"
        },
        {
          "20": "[37] N. S. Altman,\n“An introduction to kernel\nand nearest-neighbor non-",
          "IEEE TRANSACTIONS AND JOURNALS TEMPLATE": "recognition,” Neurocomputing, vol. 544, p. 126262, 2023."
        }
      ],
      "page": 20
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING": "[59] B. Sun and K. Saenko, “Deep CORAL: Correlation alignment for deep",
          "21": ""
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING": "domain adaptation,” in Computer Vision–ECCV 2016 Workshops: Ams-",
          "21": ""
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING": "terdam, The Netherlands, October 8-10 and 15-16, 2016, Proceedings,",
          "21": ""
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING": "Part\nIII 14.\nSpringer, 2016, pp. 443–450.",
          "21": ""
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING": "[60]\nE. Tzeng,\nJ. Hoffman, N. Zhang, K. Saenko,\nand T. Darrell,\n“Deep",
          "21": ""
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING": "Domain Confusion: Maximizing for domain invariance,” arXiv preprint",
          "21": ""
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING": "arXiv:1412.3474, 2014.",
          "21": ""
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING": "[61]\nL. Breiman, “Random forests,” Machine Learning, vol. 45, pp. 5–32,",
          "21": ""
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING": "2001.",
          "21": ""
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING": "[62] W. Ye, Z. Zhang, F. Teng, M. Zhang, J. Wang, D. Ni, F. Li, P. Xu, and",
          "21": ""
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING": "Z. Liang, “Semi-supervised dual-stream self-attentive adversarial graph",
          "21": ""
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING": "contrastive learning for cross-subject EEG-based emotion recognition,”",
          "21": ""
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING": "IEEE Transactions on Affective Computing, 2024.",
          "21": ""
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING": "[63]\nL. Van\nder Maaten\nand G. Hinton,\n“Visualizing\ndata\nusing\nt-SNE.”",
          "21": ""
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING": "Journal of machine learning research, vol. 9, no. 11, 2008.",
          "21": ""
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING": "[64] N. N. L. L. F. Kozachenko, “Sample estimate of the entropy of a random",
          "21": ""
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING": "vector,” Problemy Peredachi\nInformatsii, vol. 23, no. 2, p. 9–16, 1987.",
          "21": ""
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING": "[65] A. Kraskov, H.\nSt¨ogbauer,\nand\nP. Grassberger,\n“Estimating mutual",
          "21": ""
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING": "information,” Physical Review E-Statistical, Nonlinear, and Soft Matter",
          "21": ""
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING": "Physics, vol. 69, no. 6, p. 066138, 2004.",
          "21": ""
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING": "[66] B. C. Ross, “Mutual\ninformation between discrete and continuous data",
          "21": ""
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING": "sets,” PloS one, vol. 9, no. 2, p. e87357, 2014.",
          "21": ""
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING": "[67]\nP. Zhong, D. Wang, and C. Miao, “EEG-based emotion recognition using",
          "21": ""
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING": "IEEE Transactions\non Affective\nregularized\ngraph\nneural\nnetworks,”",
          "21": ""
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING": "Computing, vol. 13, no. 3, pp. 1290–1301, 2020.",
          "21": ""
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING": "[68] W.-L. Zheng, J.-Y. Zhu, and B.-L. Lu, “Identifying stable patterns over",
          "21": ""
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING": "time for emotion recognition from EEG,” IEEE Transactions on Affective",
          "21": ""
        },
        {
          "LIANG et al.: M3D: MANIFOLD-BASED DOMAIN ADAPTATION WITH DYNAMIC DISTRIBUTION FOR NON-DEEP TRANSFER LEARNING": "Computing, vol. 10, no. 3, pp. 417–429, 2019.",
          "21": ""
        }
      ],
      "page": 21
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Toward machine emotional intelligence: analysis of affective physiological state",
      "authors": [
        "R Picard",
        "E Vyzas",
        "J Healey"
      ],
      "year": "2001",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "2",
      "title": "The mind and its stories. narrative universals and human emotion",
      "authors": [
        "A Scarinzi"
      ],
      "year": "2006",
      "venue": "Orbis Litterarum"
    },
    {
      "citation_id": "3",
      "title": "Neural mechanisms linking emotion with cardiovascular disease",
      "authors": [
        "T Kraynak",
        "A Marsland",
        "P Gianaros"
      ],
      "year": "2018",
      "venue": "Current Cardiology Reports"
    },
    {
      "citation_id": "4",
      "title": "EEGFuseNet: Hybrid unsupervised deep feature characterization and fusion for high-dimensional EEG with an application to emotion recognition",
      "authors": [
        "Z Liang",
        "R Zhou",
        "L Zhang",
        "L Li",
        "G Huang",
        "Z Zhang",
        "S Ishii"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "5",
      "title": "Semi-supervised dual-stream self-attentive adversarial graph contrastive learning for cross-subject eeg-based emotion recognition",
      "authors": [
        "W Ye",
        "Z Zhang",
        "F Teng",
        "M Zhang",
        "J Wang",
        "D Ni",
        "F Li",
        "P Xu",
        "Z Liang"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "Feature extraction from EEGs associated with emotions",
      "authors": [
        "T Musha",
        "Y Terasaki",
        "H Haque",
        "G Ivamitsky"
      ],
      "year": "1997",
      "venue": "Artificial Life and Robotics"
    },
    {
      "citation_id": "7",
      "title": "Fundamentals of EEG measurement",
      "authors": [
        "M Teplan"
      ],
      "year": "2002",
      "venue": "Measurement Science Review"
    },
    {
      "citation_id": "8",
      "title": "Emotions recognition using EEG signals: A survey",
      "authors": [
        "S Alarcao",
        "M Fonseca"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "9",
      "title": "Transfer learning in brain-computer interfaces",
      "authors": [
        "V Jayaram",
        "M Alamgir",
        "Y Altun",
        "B Schlkopf",
        "M Grosse-Wentrup"
      ],
      "year": "2015",
      "venue": "IEEE Computational Intelligence Magazine"
    },
    {
      "citation_id": "10",
      "title": "Cross-subject emotion recognition brain-computer interface based on fnirs and dbjnet",
      "authors": [
        "X Si",
        "H He",
        "J Yu",
        "D Ming"
      ],
      "year": "2023",
      "venue": "Cyborg and Bionic Systems"
    },
    {
      "citation_id": "11",
      "title": "An unsupervised EEG decoding system for human emotion recognition",
      "authors": [
        "Z Liang",
        "S Oba",
        "S Ishii"
      ],
      "year": "2019",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "12",
      "title": "Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks",
      "authors": [
        "W Zheng",
        "B Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    },
    {
      "citation_id": "13",
      "title": "EEGMatch: Learning with incomplete labels for semisupervised EEG-based cross-subject emotion recognition",
      "authors": [
        "R Zhou",
        "W Ye",
        "Z Zhang",
        "Y Luo",
        "L Zhang",
        "L Li",
        "G Huang",
        "Y Dong",
        "Y.-T Zhang",
        "Z Liang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "14",
      "title": "Cross-subject emotion recognition using deep adaptation networks",
      "authors": [
        "H Li",
        "Y.-M Jin",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2018",
      "venue": "Neural Information Processing: 25th International Conference"
    },
    {
      "citation_id": "15",
      "title": "PR-PL: A novel prototypical representation based pairwise learning framework for emotion recognition using EEG signals",
      "authors": [
        "R Zhou",
        "Z Zhang",
        "H Fu",
        "L Zhang",
        "L Li",
        "G Huang",
        "F Li",
        "X Yang",
        "Y Dong",
        "Y.-T Zhang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "Domain adaptation for EEG emotion recognition based on latent representation similarity",
      "authors": [
        "J Li",
        "S Qiu",
        "C Du",
        "Y Wang",
        "H He"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "17",
      "title": "Class noise vs. attribute noise: A quantitative study",
      "authors": [
        "X Zhu",
        "X Wu"
      ],
      "year": "2004",
      "venue": "Artificial Intelligence Review"
    },
    {
      "citation_id": "18",
      "title": "Domain adaptation via transfer component analysis",
      "authors": [
        "S Pan",
        "I Tsang",
        "J Kwok",
        "Q Yang"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Neural Networks"
    },
    {
      "citation_id": "19",
      "title": "Personalizing EEG-based affective models with transfer learning",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2016",
      "venue": "Proceedings of the twenty-fifth International Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "20",
      "title": "Stratified transfer learning for cross-domain activity recognition",
      "authors": [
        "J Wang",
        "Y Chen",
        "L Hu",
        "X Peng",
        "S Philip"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Pervasive Computing and Communications (PerCom)"
    },
    {
      "citation_id": "21",
      "title": "Fault diagnosis for electromechanical drivetrains using a joint distribution optimal deep domain adaptation approach",
      "authors": [
        "Z Liu",
        "B Lu",
        "H Wei",
        "X Li",
        "L Chen"
      ],
      "year": "2019",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "22",
      "title": "Unsupervised domain adaptation by domain invariant projection",
      "authors": [
        "M Baktashmotlagh",
        "M Harandi",
        "B Lovell",
        "M Salzmann"
      ],
      "year": "2013",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "23",
      "title": "Unsupervised visual domain adaptation using subspace alignment",
      "authors": [
        "B Fernando",
        "A Habrard",
        "M Sebban",
        "T Tuytelaars"
      ],
      "year": "2013",
      "venue": "2013 IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "24",
      "title": "Visual domain adaptation with manifold embedded distribution alignment",
      "authors": [
        "J Wang",
        "W Feng",
        "Y Chen",
        "H Yu",
        "M Huang",
        "P Yu"
      ],
      "year": "2018",
      "venue": "Proceedings of the 26th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "25",
      "title": "Geodesic flow kernel for unsupervised domain adaptation",
      "authors": [
        "B Gong",
        "Y Shi",
        "F Sha",
        "K Grauman"
      ],
      "year": "2012",
      "venue": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "26",
      "title": "An overview of statistical learning theory",
      "authors": [
        "V Vapnik"
      ],
      "year": "1999",
      "venue": "IEEE Transactions on Neural Networks"
    },
    {
      "citation_id": "27",
      "title": "Manifold Regularization: A geometric framework for learning from labeled and unlabeled examples",
      "authors": [
        "M Belkin",
        "P Niyogi",
        "V Sindhwani"
      ],
      "year": "2006",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "28",
      "title": "Grassmann Discriminant Analysis: a unifying view on subspace-based learning",
      "authors": [
        "J Hamm",
        "D Lee"
      ],
      "year": "2008",
      "venue": "Proceedings of the 25th International Conference on Machine learning"
    },
    {
      "citation_id": "29",
      "title": "Transfer feature learning with joint distribution adaptation",
      "authors": [
        "M Long",
        "J Wang",
        "G Ding",
        "J Sun",
        "P Yu"
      ],
      "year": "2013",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "30",
      "title": "Joint geometrical and statistical alignment for visual domain adaptation",
      "authors": [
        "J Zhang",
        "W Li",
        "P Ogunbona"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "31",
      "title": "Balanced distribution adaptation for transfer learning",
      "authors": [
        "J Wang",
        "Y Chen",
        "S Hao",
        "W Feng",
        "Z Shen"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Data Mining (ICDM)"
    },
    {
      "citation_id": "32",
      "title": "Analysis of representations for domain adaptation",
      "authors": [
        "S Ben-David",
        "J Blitzer",
        "K Crammer",
        "F Pereira"
      ],
      "year": "2006",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "33",
      "title": "LinkCluE: A matlab package for link-based cluster ensembles",
      "authors": [
        "N Iam-On",
        "S Garrett"
      ],
      "year": "2010",
      "venue": "Journal of Statistical Software"
    },
    {
      "citation_id": "34",
      "title": "EmotionMeter: A multimodal framework for recognizing human emotions",
      "authors": [
        "W.-L Zheng",
        "W Liu",
        "Y Lu",
        "B.-L Lu",
        "A Cichocki"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "35",
      "title": "Comparing recognition performance and robustness of multimodal deep learning models for multimodal emotion recognition",
      "authors": [
        "W Liu",
        "J.-L Qiu",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "36",
      "title": "Electroencephalogram (EEG)-based computer-aided technique to diagnose major depressive disorder (MDD)",
      "authors": [
        "W Mumtaz",
        "L Xia",
        "S Ali",
        "M Yasin",
        "M Hussain",
        "A Malik"
      ],
      "year": "2017",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "37",
      "title": "An introduction to kernel and nearest-neighbor non-parametric regression",
      "authors": [
        "N Altman"
      ],
      "year": "1992",
      "venue": "The American Statistician"
    },
    {
      "citation_id": "38",
      "title": "Support-vector networks",
      "authors": [
        "C Cortes",
        "V Vapnik"
      ],
      "year": "1995",
      "venue": "Machine Learning"
    },
    {
      "citation_id": "39",
      "title": "Study and analysis of decision tree based classification algorithms",
      "authors": [
        "H Patel",
        "P Prajapati"
      ],
      "year": "2018",
      "venue": "International Journal of Computer Sciences and Engineering"
    },
    {
      "citation_id": "40",
      "title": "A decision-theoretic generalization of on-line learning and an application to boosting",
      "authors": [
        "Y Freund",
        "R Schapire"
      ],
      "year": "1997",
      "venue": "Journal of Computer and System Sciences"
    },
    {
      "citation_id": "41",
      "title": "An empirical study of the naive bayes classifier",
      "authors": [
        "I Rish"
      ],
      "year": "2001",
      "venue": "Journal of Universal Computer Science"
    },
    {
      "citation_id": "42",
      "title": "Bagging predictors",
      "authors": [
        "L Breiman"
      ],
      "year": "1996",
      "venue": "Machine Learning"
    },
    {
      "citation_id": "43",
      "title": "Reducing the subject variability of EEG signals with adversarial domain generalization",
      "authors": [
        "B.-Q Ma",
        "H Li",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2019",
      "venue": "Neural Information Processing: 26th International Conference"
    },
    {
      "citation_id": "44",
      "title": "A bihemisphere domain adversarial neural network model for EEG emotion recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "Y Zong",
        "Z Cui",
        "T Zhang",
        "X Zhou"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "45",
      "title": "Multisource associate domain adaptation for cross-subject and cross-session EEG emotion recognition",
      "authors": [
        "Q She",
        "C Zhang",
        "F Fang",
        "Y Ma",
        "Y Zhang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "46",
      "title": "Emotion recognition using hierarchical spatial-temporal learning transformer from regional to global brain",
      "authors": [
        "C Cheng",
        "W Liu",
        "L Feng",
        "Z Jia"
      ],
      "year": "2024",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "47",
      "title": "A novel transferability attention neural network model for EEG emotion recognition",
      "authors": [
        "Y Li",
        "B Fu",
        "F Li",
        "G Shi",
        "W Zheng"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "48",
      "title": "ST-SCGNN: a spatio-temporal self-constructing graph neural network for cross-subject eeg-based emotion recognition and consciousness detection",
      "authors": [
        "J Pan",
        "R Liang",
        "Z He",
        "J Li",
        "Y Liang",
        "X Zhou",
        "Y He",
        "Y Li"
      ],
      "year": "2023",
      "venue": "IEEE journal of biomedical and health informatics"
    },
    {
      "citation_id": "49",
      "title": "Learning a robust unified domain adaptation framework for cross-subject EEG-based emotion recognition",
      "authors": [
        "M Jiménez-Guarneros",
        "G Fuentes-Pineda"
      ],
      "year": "2023",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "50",
      "title": "MS-MDA: Multisource marginal distribution adaptation for cross-subject and cross-session EEG emotion recognition",
      "authors": [
        "H Chen",
        "M Jin",
        "Z Li",
        "C Fan",
        "J Li",
        "H He"
      ],
      "year": "2021",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "51",
      "title": "Alternative k-nearest neighbour rules in supervised pattern recognition: Part 1. k-nearest neighbour classification by using alternative voting rules",
      "authors": [
        "D Coomans",
        "D Massart"
      ],
      "year": "1982",
      "venue": "Analytica Chimica Acta"
    },
    {
      "citation_id": "52",
      "title": "Kernel PCA and de-noising in feature spaces",
      "authors": [
        "S Mika",
        "B Schölkopf",
        "A Smola",
        "K.-R Müller",
        "M Scholz",
        "G Rätsch"
      ],
      "year": "1998",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "53",
      "title": "Return of frustratingly easy domain adaptation",
      "authors": [
        "B Sun",
        "J Feng",
        "K Saenko"
      ],
      "year": "2016",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "54",
      "title": "Instance-representation transfer method based on joint distribution and deep adaptation for EEG emotion recognition",
      "authors": [
        "L Zhu",
        "F Yu",
        "A Huang",
        "N Ying",
        "J Zhang"
      ],
      "year": "2024",
      "venue": "Medical & Biological Engineering & Computing"
    },
    {
      "citation_id": "55",
      "title": "MetaEmotionNet: spatial-spectral-temporal based attention 3D dense network with Meta-learning for EEG emotion recognition",
      "authors": [
        "X Ning",
        "J Wang",
        "Y Lin",
        "X Cai",
        "H Chen",
        "H Gou",
        "X Li",
        "Z Jia"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "56",
      "title": "EEG emotion recognition using EEG-SWTNS neural network through EEG spectral image",
      "authors": [
        "M Cai",
        "J Chen",
        "C Hua",
        "G Wen",
        "R Fu"
      ],
      "year": "2024",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "57",
      "title": "MSLTE: multiple self-supervised learning tasks for enhancing EEG emotion recognition",
      "authors": [
        "G Li",
        "N Chen",
        "Y Niu",
        "Z Xu",
        "Y Dong",
        "J Jin",
        "H Zhu"
      ],
      "year": "2024",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "58",
      "title": "Progressive graph convolution network for EEG emotion recognition",
      "authors": [
        "Y Zhou",
        "F Li",
        "Y Li",
        "Y Ji",
        "G Shi",
        "W Zheng",
        "L Zhang",
        "Y Chen",
        "R Cheng"
      ],
      "year": "2023",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "59",
      "title": "Deep CORAL: Correlation alignment for deep domain adaptation",
      "authors": [
        "B Sun",
        "K Saenko"
      ],
      "year": "2016",
      "venue": "Computer Vision-ECCV 2016 Workshops: Amsterdam"
    },
    {
      "citation_id": "60",
      "title": "Deep Domain Confusion: Maximizing for domain invariance",
      "authors": [
        "E Tzeng",
        "J Hoffman",
        "N Zhang",
        "K Saenko",
        "T Darrell"
      ],
      "year": "2014",
      "venue": "Deep Domain Confusion: Maximizing for domain invariance",
      "arxiv": "arXiv:1412.3474"
    },
    {
      "citation_id": "61",
      "title": "Random forests",
      "authors": [
        "L Breiman"
      ],
      "year": "2001",
      "venue": "Machine Learning"
    },
    {
      "citation_id": "62",
      "title": "Semi-supervised dual-stream self-attentive adversarial graph contrastive learning for cross-subject EEG-based emotion recognition",
      "authors": [
        "W Ye",
        "Z Zhang",
        "F Teng",
        "M Zhang",
        "J Wang",
        "D Ni",
        "F Li",
        "P Xu",
        "Z Liang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "63",
      "title": "Visualizing data using t-SNE",
      "authors": [
        "L Van Der Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "64",
      "title": "Sample estimate of the entropy of a random vector",
      "authors": [
        "N Kozachenko"
      ],
      "year": "1987",
      "venue": "Problemy Peredachi Informatsii"
    },
    {
      "citation_id": "65",
      "title": "Estimating mutual information",
      "authors": [
        "A Kraskov",
        "H Stögbauer",
        "P Grassberger"
      ],
      "year": "2004",
      "venue": "Physical Review E-Statistical, Nonlinear, and Soft Matter Physics"
    },
    {
      "citation_id": "66",
      "title": "Mutual information between discrete and continuous data sets",
      "authors": [
        "B Ross"
      ],
      "year": "2014",
      "venue": "PloS one"
    },
    {
      "citation_id": "67",
      "title": "EEG-based emotion recognition using regularized graph neural networks",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "68",
      "title": "Identifying stable patterns over time for emotion recognition from EEG",
      "authors": [
        "W.-L Zheng",
        "J.-Y Zhu",
        "B.-L Lu"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    }
  ]
}