{
  "paper_id": "2010.15229v1",
  "title": "Speech-Based Emotion Recognition Using Neural Networks And Information Visualization",
  "published": "2020-10-28T20:57:32Z",
  "authors": [
    "Jumana Almahmoud",
    "Kruthika Kikkeri"
  ],
  "keywords": [
    "Therapy",
    "Visualization",
    "ASR Human-Centered Computing ~Visualization"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Figure 1: EmoViz Dashboard. A: visualization shows emotions over time during a session. B: shows the overall emotions in a certain session. C: The waveform is colored based on emotions and mapped to the text bellow that could be filtered based on emotions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotions play a key role in human life as they affect both human physiology and psychology. In therapeutic settings, improvement relies on how coherently patients express their emotions and how well the therapist recognizes and reacts to these utterances  [2]  . However, a therapist manages many patients for an extended period of time, which introduces a high volume of data that needs to be managed and maintained. Thus, a platform which can provide informative, speechbased emotion recognition insights can be useful for a variety of applications including therapy sessions. We propose EmoViz (Figure  1 ), a dashboard which enables users to take speech samples and identify a range of emotions (e.g. happy, sad, angry, surprised, neutral, etc.) through a neural network. Through this analysis of speech signals, emotion information can be detected without the use of invasive systems such as facial recognition or internal-signal based physiological readouts (i.e. EKG, ECG, MRI)  [3, 4, 6] . The platform, EmoViz, provide users with a dashboard that visualizes informative analyses of the range of emotions over time, as well as clustering audio and texts based on emotions. EmoViz serves as a management system for therapists to track their sessions by taking advantage of speech recognition technology, emotional recognition and data visualization.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "System Components",
      "text": "The EmoViz system consists of three modules: the background, the backend, and the frontend. The background processes the neural network models and the audio transcription by taking in audio files and creating segments of emotions to associated with the transcription. Figure  2  is an illustration of the system architecture. EmoViz is open source and available online at https://github.com/JumanaFM/emoviz.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Corpus",
      "text": "Audio files of actors expressing a range of emotions was employed as an emotion corpus  [5] . This emotion corpus, known as the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS), consisted of 24 actors (12male, 12 female), who expressed eight different emotions: neutral, calm, happy, sad, angry, fearful, disgust, and sur-prised. Each actor articulated 60 unique clips for a total of 1440 audio files. The data was split into a training dataset of 1008 files and 432 test files for optimization of model. An additional emotion corpus, the Toronto Emotional Speech Set (TESS), derived from  [1]  * jumanam@mit.edu. ** kkikkeri@mit.edu. was also utilized for further experimentation. In contrast to the original training and testing dataset, the TESS corpus was comprised of two actresses (age 26 and 64) who enacted seven different emotions: anger, disgust, fear, happiness, pleasant surprise, sadness, and neutral.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Experimental Methods",
      "text": "To devise the emotion recognition model for this platform, we investigated three approaches for the neural network architecture: Deep Neural Network (DNN), with an input of audio features. Convolution Neural Network (CNN), with an input of audio features. DNN/CNN with audio and text input features. Table  1  summarizes the error rates. For DNN and CNN models without text analysis for epochs of 600. Note that the error was calculated as the total number of false detections divided by the total of detections for each emotion. The accuracy is the 1-error rate.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Dashboard",
      "text": "In order to support therapists' workflow, we built the EmoViz interface. EmoViz is a system for emotion analysis based on speech signals. In the following sections we describe the dashboard and the user experience.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Platform Description",
      "text": "EmoViz provide three different charts that provide insights about the emotional composition of audio files provided by the user. As shown in Figure  1 , one visualization is a connection between the waveform and the transcribed text. The user can jump to specific emotions or filter out others. Users can also hover over a certain emotion on the waveform to display corresponding text from the recorded session.\n\nThe other two visualizations display the overall emotions in the session and emotions over time.\n\nFor emotions over time, we display the x axis as time, and y axis as different emotions. For the donut chart, we calculated the sum of emotions that appeared during a session to give users a holistic view of the session. Finally, for the audio and transcribed text, we granted users control over this visualization, by giving them the ability to connect the audio with the text and navigate to the emotions concurrently. Additionally, we mapped the color of each emotion on the waveform and the text. This enables a user to hover over a waveform and see the corresponding text without having to scroll. The three panels of visualization are connected and could be cross filtered to represent a recorded session through different lenses (time, emotion, aggregate).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Usage Pipeline",
      "text": "Therapists can record a session using their preferred method of recording. They can then log into EmoViz and access the history of sessions for an individual patient. They can then add a new session for that patient by uploading the audio recording. The audio recording will then be internally processed such that the speech file will be transcribed into text and highlighted by the color of emotion for corresponding times and phrases. A summary of the emotions will also be displayed on the dashboard. The therapist can then filter through these emotions or explore them overtime. The user experience of EmoViz provides a richer way to reflect on a session through the chart panels by incorporating the transcribed text and recordings into the visualizaitons.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Evaluation With Therapists",
      "text": "We consulted two therapists at a local mental health centre. We performed an informal study with them and asked them about what their thoughts about the overall experience. We received positive feedback, as they explained that this system has a huge potential in the field of therapy to manage their patients. They also mention their interest in using the tool in a group therapy setting as quoted by one therapist: \"Often it can be challenging to keep track of everyone's emotion during a group session. We try to look at everyone's face, but we can't keep track of everyone.\" The other therapist also mentioned: \"I'm just thinking that this could be really useful in a group setting. You would just need everyone to consent but maybe it would be useful for the patients to have as well.\"",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Conclusion",
      "text": "We explored the implementation of a neural network model for emotion detection. This was for the purposes of developing a userfriendly platform for therapy sessions. To execute this task, we explored a variety of different neural network topologies, and input features for emotion classification. The dashboard displays a summary of the emotions expressed over time and a highlighted audio signal and transcribed text. This enables users to listen to the audio as it highlights different emotions, as well read the transcripts of the signal. Further-more, this dashboard enables the user to identify and view single emotions at a time.",
      "page_start": 2,
      "page_end": 2
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: EmoViz Dashboard. A: visualization shows emotions over time during a session. B: shows the overall emotions in a certain session.",
      "page": 1
    },
    {
      "caption": "Figure 1: ), a dashboard which enables users to take speech samples and",
      "page": 1
    },
    {
      "caption": "Figure 2: is an illustration of the system architecture. EmoViz is open",
      "page": 1
    },
    {
      "caption": "Figure 2: EmoViz System Architecture",
      "page": 2
    },
    {
      "caption": "Figure 1: , one visualization is a connection between the waveform",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table 1: summarizes the",
      "page": 2
    },
    {
      "caption": "Table 1: Summary of emotion classification error rate",
      "page": 2
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "EmoTxt: A toolkit for emotion recognition from text",
      "authors": [
        "Fabio Calefato",
        "Filippo Lanubile",
        "Nicole Novielli"
      ],
      "year": "2017",
      "venue": "2017 Seventh International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)",
      "doi": "10.1109/ACIIW.2017.8272591"
    },
    {
      "citation_id": "2",
      "title": "Using Noninvasive Wearable Computers to Recognize Human Emotions from Physiological Signals",
      "authors": [
        "Christine Lisetti",
        "Fatma Nasoz"
      ],
      "year": "2004",
      "venue": "EURASIP J. Adv. Sig. Proc",
      "doi": "10.1155/S1110865704406192"
    },
    {
      "citation_id": "3",
      "title": "Databases, features and classifiers for speech emotion recognition: a review",
      "authors": [
        "Monorama Swain",
        "Aurobinda Routray",
        "P Kabisatpathy"
      ],
      "year": "2018",
      "venue": "Int J Speech Technol",
      "doi": "10.1007/s10772-018-9491-z"
    },
    {
      "citation_id": "4",
      "title": "ASR-based Features for Emotion Recognition: A Transfer Learning Approach",
      "authors": [
        "No√© Tits",
        "Kevin Haddad",
        "Thierry Dutoit"
      ],
      "year": "2018",
      "venue": "Proceedings of Grand Challenge and Workshop on Human Multimodal Language (Challenge-HML)"
    },
    {
      "citation_id": "5",
      "title": "Studying the dynamics of emotional expression using synthesized facial muscle movements",
      "authors": [
        "T Wehrle",
        "S Kaiser",
        "S Schmidt",
        "K Scherer"
      ],
      "year": "2000",
      "venue": "J Pers Soc Psychol"
    },
    {
      "citation_id": "6",
      "title": "Intelligent Psychotherapy Assessment",
      "authors": [
        "Lyssn"
      ],
      "year": "2019",
      "venue": "Intelligent Psychotherapy Assessment"
    }
  ]
}