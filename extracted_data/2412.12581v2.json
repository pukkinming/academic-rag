{
  "paper_id": "2412.12581v2",
  "title": "Understanding Emotional Body Expressions Via Large Language Models",
  "published": "2024-12-17T06:20:39Z",
  "authors": [
    "Haifeng Lu",
    "Jiuyi Chen",
    "Feng Liang",
    "Mingkui Tan",
    "Runhao Zeng",
    "Xiping Hu"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition based on body movements is vital in human-computer interaction. However, existing emotion recognition methods predominantly focus on enhancing classification accuracy, often neglecting the provision of textual explanations to justify their classifications. In this paper, we propose an Emotion-Action Interpreter powered by Large Language Model (EAI-LLM), which not only recognizes emotions but also generates textual explanations by treating 3D body movement data as unique input tokens within large language models (LLMs). Specifically, we propose a multigranularity skeleton tokenizer designed for LLMs, which separately extracts spatio-temporal tokens and semantic tokens from the skeleton data. This approach allows LLMs to generate more nuanced classification descriptions while maintaining robust classification performance. Furthermore, we treat the skeleton sequence as a specific language and propose a unified skeleton token module. This module leverages the extensive background knowledge and language processing capabilities of LLMs to address the challenges of joint training on heterogeneous datasets, thereby significantly enhancing recognition accuracy on individual datasets. Experimental results demonstrate that our model achieves recognition accuracy comparable to existing methods. More importantly, with the support of background knowledge from LLMs, our model can generate detailed emotion descriptions based on classification results, even when trained on a limited amount of labeled skeleton data.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "In the field of human-computer interaction, a machine's ability to understand human emotions directly impacts the user's interaction experience  (Fragopanagos and Taylor 2005; Mandryk, Atkins, and Inkpen 2006; Narayanan et al. 2020) . Traditionally, the most common methods for emotion recognition involve analyzing facial expressions and vocal tones  (Li and Deng 2022; El Ayadi, Kamel, and Karray 2011) . However, accurately recognizing a user's emotions becomes particularly challenging when the user is distant from the camera or when a microphone is unavailable.\n\nNumerous studies have shown that full-body motion is a significant means of expressing emotion  (Noroozi et al. 2018; Wang et al. 2024) . Additionally, the large surface area of the human torso facilitates data collection from a distance, * Corresponding author laying the foundation for more extensive emotion recognition scenarios  (Wang et al. 2024) . With recent advancements in motion capture technology and human pose estimation algorithms  (Peng, Zheng, and Chen 2024; Zheng et al. 2023; Zhang 2012 ), the acquisition of 2D and 3D skeleton data has become more convenient and efficient. Notably, 3D skeleton data offer the advantage of being resistant to variations in viewpoint, lighting, and background clutter  (Lu, Hu, and Hu 2023) . As a result, emotion recognition based on 3D fullbody skeleton data has garnered significant attention in recent years.\n\nMost existing skeleton-based emotion recognition methods rely on handcrafted features  (Fourati, Pelachaud, and Darmon 2019; Daoudi et al. 2017; Oguz and Ertugrul 2024) . In addition, deep learning approaches such as those in  (Beyan et al. 2023) , ST-Gait++  (Lima et al. 2024) , and BPM-GCN  (Zhai et al. 2024 ) are also significant for learning emotion representations. However, these methods share some common limitations: (i) Traditional emotion recognition techniques primarily focus on improving classification accuracy and often lack the capability to provide textual explanations that support their classifications. (ii) Due to variations in data collection methods, 3D skeleton datasets are often heterogeneous, with differences in the number of joints and frame lengths. This heterogeneity impedes effective knowledge transfer between datasets and restricts the potential for improving classification accuracy within individual datasets.\n\nMotivation. Inspired by the remarkable capabilities of large language models (LLMs) across various domains  (Li et al. 2024; Qu, Cai, and Liu 2024) , we are intrigued by the potential of utilizing LLMs as emotion recognizers that can not only classify emotions from skeleton data but also generate corresponding classification criteria. Several studies have shown that LLMs hold some features that are useful for emotion recognition. Specifically, these models are pre-trained on vast corpora that include descriptions of emotional actions  (Qiu et al. 2024; Li et al. 2023a) . Therefore, converting 3D full-body skeleton data into tokens recognizable by LLMs, and utilizing the extensive background knowledge of LLMs to classify emotions and generate detailed explanations, would be highly valuable.\n\nTo address these challenges, we propose an Emotion-Action Interpreter powered by Large Language Model arXiv:2412.12581v2 [cs.HC] 20 Dec 2024 (EAI-LLM), which is capable of simultaneously recognizing emotions from skeleton data and generating detailed emotion descriptions. First, we introduce a Multi-Granularity Skeleton Tokenizer (MGST) to enhance token diversity, enabling LLMs to produce more fine-grained emotion descriptions. To further improve performance, we implement a Unified Skeleton Token (UST) module that incorporates a spatio-temporal masking mechanism. This module not only addresses the challenges posed by heterogeneous datasets but also enhances recognition accuracy on individual datasets. Additionally, we pre-train the EAI-LLM using a combination of skeleton-language data and fine-tune it on prompt-based question-and-answer tasks. To convert skeleton features into tokens recognizable by LLMs, we also introduced a skeleton-text contrastive learning framework. This approach aligns the skeleton feature space with the semantic space using contrastive loss. Experimental results demonstrate that the recognition capability of EAI-LLM is comparable to existing methods, and it can generate detailed emotion descriptions from classification results by leveraging the background knowledge of LLMs.\n\nIn summary, our main contributions include:\n\n• We propose a novel 3D full-body skeleton-based emotion recognition model called EAI-LLM. To the best of our knowledge, EAI-LLM is the first approach to utilize LLMs as emotion recognizers while generating detailed emotion descriptions directly from skeleton data. • We propose a Multi-Granularity Skeleton Tokenizer (MGST), which can enhance token diversity, with the goal of improving the model's ability to infer emotion expressed through 3D skeleton movement sequences. • We propose an Unified Skeleton Token (UST) module designed for joint training in heterogeneous datasets. It treats skeleton sequences as a specialized language, allowing heterogeneous datasets to be unified within a shared language space for training.\n\n2 Related Work",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Body Skeleton-Based Emotion Recognition",
      "text": "Many studies have focused on using body movements, postures, and gestures to recognize emotion. Traditionally, emotion recognition from body movements has relied on handcrafted features  (Piana et al. 2016; Daoudi et al. 2017; Fourati, Pelachaud, and Darmon 2019) . For instance,  (Glowinski et al. 2008 ) examined upper body motions, utilizing features such as the number of local maxima and the ratio between the maximum and the duration of the largest peaks to identify emotions. Similarly,  (Fourati and Pelachaud 2015; Fourati, Pelachaud, and Darmon 2019)  proposed a full-body motion encoding scheme that extracted over 110 motion features across three levels, anatomy, spatial direction, and posture/movement, to describe expressive movements. Additionally, (Oguz and Ertugrul 2024) explored commonly used time, frequency, and statistical-based parameters, employing feature selection methods to select four features from each frame to construct a feature matrix for emotion recognition.\n\nRecently, due to the impressive performance of neural networks  (Zeng et al. 2024; You et al. 2024) , deep learning models have dominated the field of emotion recognition from 3D skeleton movement sequences.  (Ghaleb et al. 2021)  proposed a classifier network based on Spatial-Temporal Graph Convolutional Networks  (Yan, Xiong, and Lin 2018)  to recognize emotions from body movements. In  (Zhang et al. 2021) , the authors introduced an attention-based stacked LSTM network to detect the relationship between emotions and movements.  (Beyan et al. 2023)  encoded various time intervals of 3D positional data into RGB images and then used Convolutional Neural Networks (CNNs) for classification. Similarly,  (Wang et al. 2024 ) combined handcrafted features with CNN-learned image features and employed a linear classifier for emotion prediction. Unlike these previous models, which focus solely on classification, our method not only accurately identifies emotion from skeleton data but also provides interpretable explanations, significantly broadening the model's application scenarios.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Multi-Modal Large Language Models",
      "text": "Recently, LLMs have demonstrated impressive capabilities in generation and comprehension. Early models such as BERT  (Devlin et al. 2018) ,  GPT-2 (Radford et al. 2019) , and T5  (Raffel et al. 2020) , which were trained on webscale text data, laid the groundwork for the popularity of large models. Subsequently, models with greater capacity, more parameters, and extensive training data, such as GPT-3  (Brown et al. 2020) , Alpaca  (Taori et al. 2023) , PaLM  (Chowdhery et al. 2023) , Vicuna  (Chiang et al. 2023) , and LLaMA  (Touvron et al. 2023) , have been developed. Recent advancements, such as InstructGPT  (Ouyang et al. 2022) , have focused on aligning LLMs with human instructions and feedback. ChatGPT (OpenAI 2024), Anthropic  (Anthropic 2024) , and GPT-4 (OpenAI 2023) now interact with users and answer a broad range of diverse and complex questions.\n\nWith the impressive generalization abilities of LLMs, some studies have extended these models to other modalities. Flamingo  (Alayrac et al. 2022 ) and BLIP-2  (Li et al. 2023b ) aligned pre-trained vision encoders with language models using a cross-attention mechanism, establishing a foundation for subsequent research in multi-modal LLMs. More recently, the capabilities of LLMs to handle various tasks have been further explored.  GPT-4 (OpenAI 2023)  has demonstrated powerful visual understanding and reasoning abilities, while  MiniGPT-4 (Zhu et al. 2023)  and LLaVA  (Liu et al. 2023 ) produced detailed and accurate image descriptions through a series of visual instructiontuning datasets. VideoChat  (Li et al. 2023c ) and VideoChat-GPT  (Maaz et al. 2023 ) utilized ChatGPT to generate video instruction-tuning data, extending support to video modalities. Despite these advancements, there remains a gap in multi-modal LLMs capable of processing skeleton data. In this work, we propose a new framework that enables LLMs to handle skeleton-based body movement data.\n\n3 Method Problem definition. Let X = {X t ∈ R T * J * 3 } T t=1 be a 3D skeleton sequence, where X t denotes the frame at time t with J body joint. Our objective is to input a 3D skeleton movement sequence X into the EAI-LLM, which automatically encodes X into semantic space, outputs corresponding emotion labels for the body movement data and provides detailed emotion descriptions.\n\nOverview. We propose an emotion recognition framework called EAI-LLM, as shown in Fig.  1 . First, we introduce a multi-granularity skeleton tokenizer that extracts spatiotemporal tokens and semantic tokens distinctly. Second, we employ a masking mechanism to normalize tokens of varying lengths to a unified length, thereby enabling joint training. Third, we use a linear projection layer to bridge the gap between the skeleton encoder and LLMs, embedding the extracted multi-granularity skeleton tokens together with text prompts as input for LLMs. Finally, we pre-train EAI-LLM using a composite of skeleton-language data and fine-tune it on prompt-based question-and-answer tasks.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Multi-Granularity Skeleton Tokenizer",
      "text": "Traditional skeleton-based emotion recognition methods typically use a skeleton encoder to extract features, denoted as\n\n, where C represents the base channel. These features are then spatio-temporally compressed and passed through a fully connected (FC) layer to obtain classification results. However, the fundamental design of LLMs is to process text-based inputs, making it impossible to directly input skeleton sequence X into LLMs. To bridge this gap, we develop a skeleton tokenizer to convert skeleton data into a format compatible with the input structure of LLMs.\n\nIn this paper, we propose a multi-granularity skeleton tokenizer to enhance token diversity, thereby facilitating the generation of more detailed text descriptions. First, we per-form spatio-temporal pooling (STP) on the feature F s , followed by processing through a FC layer to obtain a semantic token z s g ∈ R C . This token encapsulates the global body motion encoding information. While the semantic token is advantageous for classification tasks, the spatio-temporal pooling process can result in information loss, which is detrimental in generation tasks.\n\nTo address this, we extract spatio-temporal tokens to enhance token diversity. Specifically, we employ a temporal pooling (TP) layer and a 1×1 convolution layer to aggregate the skeleton features F s across time. Similarly, we apply a spatial pooling (SP) layer and a 1×1 convolution to average the spatial dynamics of all joints. The resulting spatiotemporal features are then flattened separately into unified representations, referred to as the spatial tokens z s s ∈ R J * C and temporal tokens z s t ∈ R T * C , respectively.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Unified Skeleton Token Module",
      "text": "Due to variations in data collection methods across different datasets, the number of joints J and frame lengths T can differ significantly among datasets. Consequently, the dimensions of the spatio-temporal tokens extracted in Sec.\n\n3.1 are inconsistent. Traditionally, classification tasks using skeleton data have addressed this heterogeneity by training separate models for each dataset. In this paper, we approach skeleton sequences as a specialized form of language, where differences in frame length and joint count are analogous to sentences of varying lengths. To unify these differences, we leverage the masking mechanism from natural language processing by applying a mask to all skeleton tokens, as shown in Eq. (  1 ). This approach allows us to merge all datasets into a larger, unified dataset in the language space for emotion representation learning.\n\nwhere M s s and M s t are a 1 × L attention map that serves to retain the original tokens and nullify the padded elements. L represents the maximum length of all tokens. ⊙ denotes element-wise multiplication.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Skeleton-Aware Large Language Model",
      "text": "LLMs typically take sentences in human language as input instructions, but the skeleton sequences we use are not well-compatible with LLMs. To enable LLMs to recognize skeleton sequences and generate text descriptions, we apply Low-Rank Adaptation (LoRA)  (Hu et al. 2021)  to LLMs so that it better understands skeleton sequences while keeping the model's pre-trained weights unchanged. Specifically, for each training sample composed of a skeleton sequence and its corresponding emotion label and emotion description, we perform the following steps: 1. We utilize a pre-trained GCN-based skeleton encoder to obtain the initial skeleton tokens (z s g , z s′ s , z s′ t ). 2. Prior work on multimodal LLMs  (Liu et al. 2023)  show that a learnable linear layer maps the visual feature space to the linguistic feature space, enabling LLMs to handle non-text data. Following this approach, we use a linear layer to transform the initial 768-dimensional skeleton token into 4096-dimensional token, effectively bridging the gap between the skeleton encoder and LLMs. 3. We design two prompts that conform to the LLaMA-2 conversation template, as shown below.\n\nEmotion Recognition #Human: <Skeleton> <Skele-tonFeature> </Skeleton> Can you tell me the emotion of this person? #Assistant:\n\nEmotion Description #Human: <Skeleton> <Skele-tonFeature> </Skeleton> The emotion of this person is [shame], please tell me some reasons for it. #Assistant: In these prompts, <SkeletonFeature >represents a skeleton token derived in step 2, and the word within square brackets represents an emotion label. 4. We concatenate the skeleton tokens with the prompt tokens and input them together into LLMs. 5. We use Eq. (  2 ) to constrain the similarity between the ground truth tokens t g and the predicted tokens t p , adjusting EAI-LLM through LoRA.\n\n2) where L ce (., .) is the cross-entropy loss.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Skeleton-Language Alignment",
      "text": "Skeleton and text data exist in distinct feature spaces, making it challenging for LLMs to interpret unaligned skeleton features. To address this, we employ contrastive learning to align the skeleton feature space with the linguistic feature space, thus improving the compatibility of the skeleton tokens with LLMs. The subsequent sections will provide a detailed explanation of the alignment process.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Text Encoder",
      "text": "This is a happy person. This is a sad person.\n\nL con ( ,z t ) Text Encoder We employ a pre-trained CLIP model (specifically ViT-L/14)  (Radford et al. 2021)  as the text encoder to extract a feature vector z t representing the text description. Drawing inspiration from ActionCLIP  (Wang et al. 2023) , we manually design the text descriptions in the format: \"This is a [happy] person.\", where the word within the brackets changes according to the skeleton label.\n\nSemantic Alignment Contrastive learning excels at unifying data from different modalities into a shared feature space, facilitating cross-modal knowledge transfer  (Radford et al. 2021; Xiang et al. 2023) . Consequently, we employ contrastive learning to align the skeleton feature space with the linguistic feature space, enabling the compatibility of these tokens with LLMs.\n\nTo bring the pairwise skeleton representation z s and text description label representation z t closer together, we calculate the skeleton-to-text and text-to-skeleton similarity scores as specified in Eq. (3).\n\nwhere cos represents cosine similarity, τ is the temperature hyper-parameter, N represents the batch size. Since the number of skeleton sequences greatly exceeds the number of emotion labels, there will be multiple skeleton sequences with the same label within the same batch. Consequently, using cross-entropy to calculate the similarity between p s2t i and p t2s i is not suitable. Therefore, we redefine the skeletontext contrastive loss using the Kullback-Leibler (KL) diver-gence, as shown in Eq. (  4 ).\n\nwhere D is the entire dataset. The ground-truth ŷ is defined as 1 for positive pairs and 0 for negative pairs. In this paper, we optimize the semantic tokens and spatiotemporal tokens using skeleton-language alignment, making them more easily recognized by LLMs. The loss functions can be represented by Eq. (  5 ) and Eq. (  6 ), respectively.\n\nwhere y is the one-hot presentation of the emotion label.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiment",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Datasets",
      "text": "Emilya The EMotional body expression In daILY Actions dataset  (Fourati and Pelachaud 2016)  comprises 8,206 samples. Eleven actors performed eight emotions-Neutral, Joy, Anger, Panic, Fear, Anxiety, Sadness, and Shame-across seven daily actions, including sitting, walking, and lifting. Data were recorded using the Xsens MVN system, capturing 28 3D joints at a frequency of 120 Hz.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Kdae",
      "text": "The Kinematic Dataset of Actors Expressing Emotions  (Zhang et al. 2020 ) was collected using a portable motion capture system that tracked 72 body markers at a frequency of 125 Hz. This dataset includes 1,402 recordings of seven emotions: Happiness, Sadness, Neutral, Anger, Disgust, Fear, and Surprise, performed by 22 semi-professional actors. For our analysis, we excluded hand joints and focused on 24 full-body joints.\n\nEGBM The Emotional Gestures and Body Movements Corpus  (Sapiński et al. 2019 ) contains 560 samples recorded by a Kinect V2 camera at 30 Hz. Sixteen Polish professional actors, evenly split between men and women aged 25 to 64, expressed seven emotions: Happiness, Sadness, Neutral, Anger, Disgust, Fear, and Surprise. Each emotion is represented by 80 samples, with 3D positions for 25 joints.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Emotion Description",
      "text": "The aforementioned datasets only include emotional labels without detailed descriptions of emotional actions. To fully leverage the generative capabilities of LLMs, we manually annotated a subset of new data with fine-grained descriptions of emotional actions. For annotation, we referred to the emotional action descriptions provided in  (Noroozi et al. 2018) . In total, 174 samples from the Emilya dataset and 105 samples from the KDAE dataset were labeled with these detailed descriptions.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Implementation Details",
      "text": "All experiments are conducted on NVIDIA 4×A100 GPUs and are implemented using the PyTorch framework  (Paszke et al. 2019 ). We use LLaMA-7B ( Training For emotion description, the model is trained for 10,000 steps using labeled datasets. The LoRA parameters are configured with a rank of 64, an alpha of 16, and a dropout rate of 0.05. The global batch size is 16, and the maximum learning rate is 1e-5. For emotion recognition, the model undergoes 800,000 training steps, with the global batch size increased to 64. The LoRA parameters and maximum learning rate remain the same as in the emotion description stage. During skeleton encoder pre-training, we optimize the model for 200 epochs using the stochastic gradient descent (SGD) optimizer with an initial learning rate of 0.1 and a batch size of 64. The learning rate is reduced by a factor of 10 at epochs 100, 150, and 175. A warm-up strategy is applied for the first 5 epochs.\n\nEvaluation Protocols We randomly split the dataset into training and testing sets at a 4:1 ratio. In emotion recognition, we extract the emotion labels from the generated sentences and compare them with the ground truth labels to compute accuracy. It is important to note that if a sentence contains multiple emotion labels or no emotion label, it will be marked as 'Error'. However, if the labels are synonyms or different grammatical forms of the same label, the recognition is deemed successful. For emotion description, we use three commonly employed metrics in natural language processing: Rouge, BLEU, and METEOR. Further information regarding implementation details, novel LLMs, new skeleton encoder, and ablation study are provided in the supplementary material.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Comparisons For Emotion Recognition",
      "text": "Evaluation on Spatio-temporal Tokens We evaluate the classification results utilizing spatial and temporal tokens both separately and in combination, as presented in Tab. 1. The results indicate that the separate classification of spatial and temporal tokens leads to lower accuracy when com-  pared to their combined application across all three datasets. The integration of spatial and temporal tokens provides the model with more comprehensive information, thereby validating the effectiveness of our proposed multi-granularity skeleton tokenizer.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Evaluation Of Different Training Strategies",
      "text": "We evaluate the effectiveness of separate training strategy and joint training strategies with UST module by using skeleton tokens at different granularities. The results, presented in Tab. 2, show that using semantic tokens with joint training significantly improved classification accuracy across all three datasets, with an average increase of 4.5%. Specifically, when employing spatio-temporal tokens, the accuracy on the EGBM dataset increased by 7.34%. However, improvements are less pronounced for other datasets, with a slight decrease in accuracy observed on the KDAE dataset. These results suggest that joint training with semantic tokens enhances knowledge transfer between datasets, leading to better classification accuracy for individual datasets.\n\nTo further investigate the impact of UST module on recognition results, we plot the confusion matrix for the Emilya dataset, which has the largest number of samples. As shown in Fig.  3 , the classification accuracy for anxiety, shame, and pride has significantly improved after employing the joint training strategy. Notably, these three emotions are absent in the KDAE and EGBM datasets. Compared to separate training, the joint training strategy has enhanced the model's recognition of emotions common to all three datasets, thereby positively influencing the recognition of emotion labels unique to the Emilya dataset.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Comparisons For Emotion Description",
      "text": "In this section, we compare the impact of semantic tokens and spatio-temporal tokens on emotion description and recognition results. Our model is designed to handle both emotion description and emotion recognition tasks simultaneously, so we also report the effects of different training orders on the results. Note that all results are obtained under joint training strategy, and both accuracy and generation metrics are averaged across the three datasets. As shown in Tab. 3, we can draw three conclusions:\n\n• In the emotion description task, spatio-temporal tokens outperform semantic tokens, whereas in the emotion recognition task, the opposite is true. This ad- vantage is likely because spatio-temporal tokens retain richer spatial and temporal information, which enhances the detail and quality of the generated descriptions. Semantic tokens are more concise and contribute better to classification.\n\n• The training order of different tasks significantly affects the recognition results. When training the emotion description and emotion recognition tasks separately, good results can be achieved regardless of which tokens are used. However, when the emotion recognition task is completed first, and then the emotion description task is fine-tuned, catastrophic forgetting occurs. This results in a significant drop in average recognition accuracy by 37.06% and 61.86% for semantic and spatio-temporal tokens, respectively. This issue may arise from conflicting demands within the model for both emotion recognition and description capabilities. EAI-LLM imposes stringent constraints on recognition outcomes, expecting results to be framed in specific sentences, such as \"This is a happy person.\". These constraints often conflict with the diverse requirements of generation tasks, leading to reduced recognition accuracy when generation tasks are performed after recognition tasks. • Semantic tokens offer a balanced trade-off between recognition accuracy and description capability. Although semantic tokens show lower accuracy compare to spatio-temporal tokens in emotion recognition tasks, their performance in subsequent emotion description tasks does not decline significantly, thereby maintaining relatively good accuracy.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Comparison With The State-Of-The-Art",
      "text": "Due to the limited reporting on skeleton-based emotion recognition and the absence of a standardized evaluation method, we have chosen to compare our approach with existing methods used in skeleton-based action recognition, which are analogous to emotion recognition. To ensure a fair comparison, we re-implemented all methods and standardized the data preprocessing procedures. As shown in Tab. 4, our method demonstrates accuracy comparable to existing approaches on the KDAE and EGBM datasets, with the best accuracy reaching 71.17% and 66.97%, respectively. Table  3 : Comparison of classification and emotion description results under different training orders. D→R indicates that the emotion description task is completed first, followed by the recognition task, while R→D indicates the reverse order.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Input Skeleton",
      "text": "Question: The emotion of this person is shame, please tell me some reasons for it.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Eai-Llm (Ours):",
      "text": "This is a shame person, with his head down and hands covering his face, showing a large amount of shame.\n\nGemini Pro: If the person's posture is collapsed inward, as if trying to make itself smaller, with hunched shoulders and a tucked chin, it could suggest shame. Additionally, the person might avoid direct eye contact, further emphasizing a desire to hide or disappear, which are common manifestations of shame.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Gpt-4:",
      "text": "The person in the video might be exhibiting shame if their movements include lowering the head, avoiding eye contact, or turning the body away as if trying to hide. They might also have slouched shoulders or a closed-off posture, indicating a desire to withdraw or make themselves smaller. These actions suggest a feeling of discomfort or embarrassment, which are typical physical manifestations of shame. However, on the Emilya dataset, the accuracy only reached 85.44%, still lagging behind other methods.\n\nDuring the annotation process of the emotion description dataset, we converte the skeleton sequences into videos and then labeled the emotional action features by viewing these videos. Detailed examples are provided in the supplementary materials. Since current multi-modal large models do not support direct input of the original skeleton sequences, we instead input the videos derived from these skeleton sequences into the multi-modal LLMs, using the same prompts as EAI-LLM. As shown in Tab. 5, when using the same prompt words, the metrics produced by our method surpass those generated by popular multi-modal LLMs.\n\nWe present the visualization results in Fig.  4 . The emotion descriptions generated by EAI-LLM exhibit significant consistency with the input sequences. In contrast, while both GPT-4 (OpenAI 2023) and Gemini 1.5 Pro (Research 2024) can also generate emotion descriptions from video, these descriptions largely reflect the models' background knowledge and do not align as well with the input sequences.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Limitations",
      "text": "Since EAI-LLM is built upon LLMs, it inherits the LLM's limitations, such as hallucination, which can lead to ambiguous outputs. For instance, when given an input labeled as \"Anxiety\", EAI-LLM might generate:\n\n• \"This person is experiencing anguish or distress, as denoted by the frown and furrowed brow.\" While this sentence describes the state of anxiety, it does not explicitly mention the word \"anxiety,\" and therefore fails to meet our accuracy criteria. • \"This person is expressing anxiety or fear.\" The output contains multiple emotional labels and cannot be clearly categorized under a specific emotional label, thereby resulting in a recognition failure.\n\nHallucinations in skeleton-based fine-grained emotion descriptions remain an unresolved issue. Employing a new evaluation protocol with a semantic detection module may be a potential solution.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we have introduced a novel emotion recognition model, EAI-LLM, designed to identify emotions from 3D full-body skeleton sequences and generate detailed emotion descriptions. We have used LLaMA as the language decoder and fine-tuned it on prompt-based question-answering tasks. Specifically, we have extracted skeleton tokens at various granularities to enable our model to produce more nuanced emotion descriptions. Additionally, the unified skeleton token module has significantly enhanced the accuracy of emotion recognition on individual datasets. Experimental results have demonstrated that our model effectively recognizes emotions from skeleton data and generates detailed emotion descriptions, even with limited data annotations. Finally, we choose to freeze the skeleton encoder's parameters, use a single linear layer, and apply the LoRA to fine-tune the LLMs in our experiments.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "B. Effectiveness Of Skeleton Encoder Setting",
      "text": "In Section 3.4, we use the semantic features to guide the skeleton encoding process, producing tokens that are more easily recognized by the LLMs. Here, we elaborate on the impact of semantic guidance on the emotion recognition task. As shown in Tab. 7, we draw the following conclusions:\n\n• When using the raw skeleton encoder to obtain tokens, our model's classification accuracy is only 46.29%. This suggests that the features extracted by the raw encoder are not well-aligned with the language space, making them difficult for the LLMs to recognize. • Even without the cross-entropy guidance, tokens extracted solely through semantic guidance demonstrate some recognition capability. Specifically, optimized semantic tokens achieve an accuracy of 78.81%, and spatio-temporal tokens reach an accuracy of 76.43%. This indicates that semantic guidance helps align the skeleton features closer to the language space, but these features alone lack sufficient differentiation for effective classification. • The cross-entropy guidance proves to be crucial for enhancing the accuracy of the emotion recognition task. When the cross-entropy and semantic alignment are used together, the accuracy of the emotion recognition task improves further. This combination enables the extracted skeleton tokens to reside within the semantic space while also possessing a certain classification capability.\n\nWe investigate the impact of different skeleton encoders on our results by selecting three classic models in skeleton data processing as skeleton encoders: AGCN  (Shi et al. 2019) , CTR-GCN  (Chen et al. 2021) , and HD-GCN  (Lee et al. 2023) . Tab. 8 presents the results of separate training and joint training using UST module. As shown in Tab. 8, the accuracy of the emotion recognition task on the Emilya dataset exceeds 80% when a separate training strategy is used. However, due to the limited sample sizes of the KDAE and EGBM datasets, the skeleton encoders are not fully trained, resulting in features that are not well-recognized by the LLMs. In the joint training strategy, CTR-GCN encoder performs well across all three datasets, with its accuracy on the Emilya dataset being only 0.07% lower than that of HD-GCN encoder. Overall, CTR-GCN encoder demonstrates relatively stable performance and is therefore selected as the primary skeleton encoder for this study.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "C. Effectiveness Of Llms Setting",
      "text": "Previous studies have demonstrated that LLMs can provide prior knowledge about body language  (Qiu et al. 2024; Li et al. 2023a) . In this context, we experiment with different LLMs, including LLaMA-7B, LLaMA-13B  (Touvron et al. 2023) , and Vicuna  (Chiang et al. 2023) , to evaluate their impact on emotion recognition. As shown in Tab. 9, when using the same LoRA parameters (r LoRA and α LoRA set to 64 and 16, respectively), the recognition accuracy of LLaMA-13B decreases by 4.81% compared to LLaMA-7B. Only after adjusting to larger parameters does LLaMA-13B's accuracy surpass that of LLaMA-7B. However, due to the greater resources and additional time required to train the LLaMA-13B model, and given its marginal performance improvement, we choose to use LLaMA-7B as the base model in our subsequent experiments.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "D. Effectiveness Of Output Setting",
      "text": "In emotion recognition task, it is essential for the model to generate sentences in a fixed format from which emotion labels can be extracted. To explore the impact of different output formats on the recognition task, we test three formats:\n\n• Output A: [Label]\n\n• Output B: This is a/an [Label] person.\n\n• Output C: This is a 3D skeleton sequence of a person.\n\nFrom their movements, it can be observed that their emotion is  [Label] .\n\nIn these formats, the emotion labels within the brackets are generated by the model. As shown in Tab. 10, when the model generates emotion labels directly or produces short sentences, the accuracy of the emotion recognition task is 80.94% and 80.63%, respectively, with no significant difference. However, when the model generates longer sentences, the accuracy of the emotion recognition task decreases by approximately 15%. This decline can be attributed to the strict format requirements for sentence generation in recognition task. As the sentences become longer, the proportion of the sentence that constitutes the emotion label decreases, causing the model's attention to be dispersed, which in turn hinders the accurate identification of emotion labels.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: This work presents a novel approach for 3D full-body skeleton-based emotion recognition using fine-tuned LLMs,",
      "page": 3
    },
    {
      "caption": "Figure 1: First, we introduce",
      "page": 3
    },
    {
      "caption": "Figure 2: Diagram of skeleton-language alignment.",
      "page": 4
    },
    {
      "caption": "Figure 3: , the classification accuracy for anxiety,",
      "page": 6
    },
    {
      "caption": "Figure 3: Confusion matrices for Emilya dataset using dif-",
      "page": 6
    },
    {
      "caption": "Figure 4: Examples for emotion description capabilities of",
      "page": 7
    },
    {
      "caption": "Figure 4: . The emo-",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "\u0000\u0014\u0000\u001a\u0000\u0019": "\u0000\u0014",
          "\u0000\u0013": "\u0000\u0014\u0000\u0016\u0000\u0018 \u0000\u0018\u0000\u0013",
          "\u0000\u0015": "\u0000\u0014",
          "\u0000\u001a": "\u0000\u0014\u0000\u0019",
          "\u0000": "\u0000\u0014",
          "\u0000\u0014": "\u0000\u0013",
          "\u0000\u0016": "\u0000\u0015",
          "\u0000$\u0000Q\u0000[\u0000L\u0000H\u0000W\u0000\\": "\u00003\u0000U\u0000L\u0000G\u0000H"
        },
        {
          "\u0000\u0014\u0000\u001a\u0000\u0019": "\u0000\u0018",
          "\u0000\u0013": "\u0000\u0016",
          "\u0000\u0015": "",
          "\u0000\u001a": "\u0000\u001b",
          "\u0000": "\u0000\u0017",
          "\u0000\u0014": "\u0000\u0013",
          "\u0000\u0016": "\u0000\u0014\u0000\u0016",
          "\u0000$\u0000Q\u0000[\u0000L\u0000H\u0000W\u0000\\": "\u0000+\u0000D\u0000S\u0000S\u0000L\u0000Q\u0000H\u0000V\u0000V"
        },
        {
          "\u0000\u0014\u0000\u001a\u0000\u0019": "\u0000\u0013",
          "\u0000\u0013": "\u0000\u0015",
          "\u0000\u0015": "\u0000\u0018",
          "\u0000\u001a": "\u0000\u0014\u0000\u001c\u0000\u0014",
          "\u0000": "\u0000\u0013",
          "\u0000\u0014": "\u0000\u0014",
          "\u0000\u0016": "\u0000\u0015",
          "\u0000$\u0000Q\u0000[\u0000L\u0000H\u0000W\u0000\\": "\u00006\u0000D\u0000G\u0000Q\u0000H\u0000V\u0000V"
        },
        {
          "\u0000\u0014\u0000\u001a\u0000\u0019": "\u0000\u0018",
          "\u0000\u0013": "\u0000\u0014",
          "\u0000\u0015": "\u0000\u0013",
          "\u0000\u001a": "\u0000\u0016",
          "\u0000": "\u0000\u0014\u0000\u001b\u0000\u0014",
          "\u0000\u0014": "\u0000\u0015",
          "\u0000\u0016": "\u00007\u0000U\u0000X\u0000H\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000V\n\u0000\u001b",
          "\u0000$\u0000Q\u0000[\u0000L\u0000H\u0000W\u0000\\": "\u0000)\u0000H\u0000D\u0000U\n\u0000\u0014\u0000\u0015"
        },
        {
          "\u0000\u0014\u0000\u001a\u0000\u0019": "\u0000\u0019",
          "\u0000\u0013": "\u0000\u0014",
          "\u0000\u0015": "\u0000\u0013",
          "\u0000\u001a": "\u0000\u0018\u0000\u0013",
          "\u0000": "\u0000\u0014",
          "\u0000\u0014": "\u0000\u0014\u0000\u0018\u0000\u0013",
          "\u0000\u0016": "\u0000\u0016",
          "\u0000$\u0000Q\u0000[\u0000L\u0000H\u0000W\u0000\\": "\u00006\u0000K\u0000D\u0000P\u0000H"
        },
        {
          "\u0000\u0014\u0000\u001a\u0000\u0019": "\u0000\u0014",
          "\u0000\u0013": "\u0000\u0014",
          "\u0000\u0015": "\u0000\u0014",
          "\u0000\u001a": "\u0000\u0014",
          "\u0000": "\u0000\u0016",
          "\u0000\u0014": "\u0000\u0014",
          "\u0000\u0016": "\u0000\u0014\u0000\u001b\u0000\u001b",
          "\u0000$\u0000Q\u0000[\u0000L\u0000H\u0000W\u0000\\": "\u0000$\u0000Q\u0000J\u0000H\u0000U"
        },
        {
          "\u0000\u0014\u0000\u001a\u0000\u0019": "\u0000\u0014",
          "\u0000\u0013": "\u0000\u0016",
          "\u0000\u0015": "\u0000\u0014\u0000\u0016\u0000\u0013",
          "\u0000\u001a": "\u0000\u001b",
          "\u0000": "\u0000\u0014",
          "\u0000\u0014": "\u0000\u0013",
          "\u0000\u0016": "\u0000\u0014",
          "\u0000$\u0000Q\u0000[\u0000L\u0000H\u0000W\u0000\\": "\u00001\u0000H\u0000X\u0000W\u0000U\u0000D\u0000O"
        },
        {
          "\u0000\u0014\u0000\u001a\u0000\u0019": "\u0000\u0013",
          "\u0000\u0013": "\u0000\u0013",
          "\u0000\u0015": "\u0000\u0013",
          "\u0000\u001a": "\u0000\u0013",
          "\u0000": "\u0000\u0013",
          "\u0000\u0014": "\u0000\u0013",
          "\u0000\u0016": "\u0000\u0013",
          "\u0000$\u0000Q\u0000[\u0000L\u0000H\u0000W\u0000\\": "\u0000(\u0000U\u0000U\u0000R\u0000U"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "\u0000\u0014\u0000\u001b\u0000\u001b": "\u0000\u0014",
          "\u0000\u0013\n\u0000\u0014\u0000\u001a\u0000\u0018": "\u0000\u0014\u0000\u001b\u0000\u0018 \u0000\u0014\u0000\u0018",
          "\u0000\u0014": "\u0000\u0014",
          "\u0000\u0017": "\u0000\u0013",
          "\u0000\u0014\u0000\u0015": "\u0000\u0017",
          "\u0000\u0015": "\u0000\u0017",
          "\u0000\u0016": "\u0000\u001a"
        },
        {
          "\u0000\u0014\u0000\u001b\u0000\u001b": "\u0000\u0019",
          "\u0000\u0013\n\u0000\u0014\u0000\u001a\u0000\u0018": "\u0000\u0014\u0000\u0018\u0000\u0013\n\u0000",
          "\u0000\u0014": "\u0000\u0013",
          "\u0000\u0017": "\u0000\u0015",
          "\u0000\u0014\u0000\u0015": "\u0000\u0014\u0000\u0013",
          "\u0000\u0015": "\u0000\u0014\u0000\u0014",
          "\u0000\u0016": "\u0000\u0015"
        },
        {
          "\u0000\u0014\u0000\u001b\u0000\u001b": "\u0000\u0014",
          "\u0000\u0013\n\u0000\u0014\u0000\u001a\u0000\u0018": "\u0000\u0014\u0000\u0015\u0000\u0018\n\u0000\u0014",
          "\u0000\u0014": "\u0000\u0014",
          "\u0000\u0017": "\u0000\u0014\u0000\u001b\u0000\u0017",
          "\u0000\u0014\u0000\u0015": "\u0000\u0015",
          "\u0000\u0015": "\u0000\u0016",
          "\u0000\u0016": "\u0000\u0018"
        },
        {
          "\u0000\u0014\u0000\u001b\u0000\u001b": "\u0000\u0015",
          "\u0000\u0013\n\u0000\u0014\u0000\u001a\u0000\u0018": "\u0000\u0014\u0000\u0013\u0000\u0013\n\u0000\u0014",
          "\u0000\u0014": "\u0000\u0015",
          "\u0000\u0017": "\u0000\u0014",
          "\u0000\u0014\u0000\u0015": "\u0000\u0014\u0000\u001c\u0000\u0019",
          "\u0000\u0015": "\u0000\u0017",
          "\u0000\u0016": "\u0000\u0017"
        },
        {
          "\u0000\u0014\u0000\u001b\u0000\u001b": "\u0000\u001a\u0000\u0018\n\u0000\u0019",
          "\u0000\u0013\n\u0000\u0014\u0000\u001a\u0000\u0018": "\u0000\u0014",
          "\u0000\u0014": "\u0000\u0014\u0000\u0018\u0000\u001a \u0000\u0014\u0000\u0015",
          "\u0000\u0017": "\u0000\u0015\u0000",
          "\u0000\u0014\u0000\u0015": "\u0000\u0018",
          "\u0000\u0015": "",
          "\u0000\u0016": "\u0000\u0018"
        },
        {
          "\u0000\u0014\u0000\u001b\u0000\u001b": "\u0000\u0016\n\u0000\u0018\u0000\u0013",
          "\u0000\u0013\n\u0000\u0014\u0000\u001a\u0000\u0018": "\u0000\u0018",
          "\u0000\u0014": "\u0000\u0013",
          "\u0000\u0017": "\u0000\u0015",
          "\u0000\u0014\u0000\u0015": "\u0000\u0015",
          "\u0000\u0015": "\u0000\u0014\u0000\u001b\u0000\u001b",
          "\u0000\u0016": "\u0000\u0014"
        },
        {
          "\u0000\u0014\u0000\u001b\u0000\u001b": "\u0000\u0015\n\u0000\u0015\u0000\u0018",
          "\u0000\u0013\n\u0000\u0014\u0000\u001a\u0000\u0018": "\u0000\u0015",
          "\u0000\u0014": "\u0000\u0013",
          "\u0000\u0017": "\u0000\u0015",
          "\u0000\u0014\u0000\u0015": "\u0000\u0016",
          "\u0000\u0015": "\u0000\u0015",
          "\u0000\u0016": "\u0000\u0015"
        },
        {
          "\u0000\u0014\u0000\u001b\u0000\u001b": "\u0000\u0013",
          "\u0000\u0013\n\u0000\u0014\u0000\u001a\u0000\u0018": "\u0000\u0013",
          "\u0000\u0014": "\u0000\u0013",
          "\u0000\u0017": "\u0000\u0013",
          "\u0000\u0014\u0000\u0015": "\u0000\u0013",
          "\u0000\u0015": "\u0000\u0013",
          "\u0000\u0016": "\u0000\u0013"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Flamingo: a visual language model for few-shot learning",
      "authors": [
        "J.-B Alayrac",
        "J Donahue",
        "P Luc",
        "A Miech",
        "I Barr",
        "Y Hasson",
        "K Lenc",
        "A Mensch",
        "K Millican",
        "M Reynolds"
      ],
      "year": "2022",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "2",
      "title": "Claude: An AI Assistant by Anthropic",
      "authors": [
        "Anthropic"
      ],
      "year": "2024",
      "venue": "Claude: An AI Assistant by Anthropic"
    },
    {
      "citation_id": "3",
      "title": "Modeling Multiple Temporal Scales of Full-Body Movements for Emotion Classification",
      "authors": [
        "C Beyan",
        "S Karumuri",
        "G Volpe",
        "A Camurri",
        "R Niewiadomski"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "4",
      "title": "Language models are few-shot learners",
      "authors": [
        "T Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J Kaplan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell"
      ],
      "year": "2020",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "5",
      "title": "Channel-wise topology refinement graph convolution for skeleton-based action recognition",
      "authors": [
        "Y Chen",
        "Z Zhang",
        "C Yuan",
        "B Li",
        "Y Deng",
        "W Hu"
      ],
      "year": "2021",
      "venue": "ICCV"
    },
    {
      "citation_id": "6",
      "title": "Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality",
      "authors": [
        "W.-L Chiang",
        "Z Li",
        "Z Lin",
        "Y Sheng",
        "Z Wu",
        "H Zhang",
        "L Zheng",
        "S Zhuang",
        "Y Zhuang",
        "J Gonzalez",
        "I Stoica",
        "E Xing"
      ],
      "year": "2023",
      "venue": "Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality"
    },
    {
      "citation_id": "7",
      "title": "Palm: Scaling language modeling with pathways",
      "authors": [
        "A Chowdhery",
        "S Narang",
        "J Devlin",
        "M Bosma",
        "G Mishra",
        "A Roberts",
        "P Barham",
        "H Chung",
        "C Sutton",
        "S Gehrmann"
      ],
      "year": "2023",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "8",
      "title": "Emotion recognition by body movement representation on the manifold of symmetric positive definite matrices",
      "authors": [
        "M Daoudi",
        "S Berretti",
        "P Pala",
        "Y Delevoye",
        "A Del Bimbo"
      ],
      "year": "2017",
      "venue": "Emotion recognition by body movement representation on the manifold of symmetric positive definite matrices"
    },
    {
      "citation_id": "9",
      "title": "Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2018",
      "venue": "Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "10",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M El Ayadi",
        "M Kamel",
        "F ; Karray",
        "N Fourati",
        "C Pelachaud"
      ],
      "year": "2011",
      "venue": "2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "11",
      "title": "Perception of emotions and body movement in the emilya database",
      "authors": [
        "N Fourati",
        "C Pelachaud"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "12",
      "title": "Contribution of temporal and multi-level body cues to emotion classification",
      "authors": [
        "N Fourati",
        "C Pelachaud",
        "P Darmon"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "13",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "N Fragopanagos",
        "J Taylor"
      ],
      "year": "2005",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "14",
      "title": "Skeleton-based explainable bodily expressed emotion recognition through graph convolutional networks",
      "authors": [
        "E Ghaleb",
        "A Mertens",
        "S Asteriadis",
        "G Weiss",
        "D Glowinski",
        "A Camurri",
        "G Volpe",
        "N Dael",
        "K Scherer"
      ],
      "year": "2008",
      "venue": "CVPR Workshops"
    },
    {
      "citation_id": "15",
      "title": "Lora: Low-rank adaptation of large language models",
      "authors": [
        "E Hu",
        "Y Shen",
        "P Wallis",
        "Z Allen-Zhu",
        "Y Li",
        "S Wang",
        "L Wang",
        "W Chen"
      ],
      "year": "2021",
      "venue": "Lora: Low-rank adaptation of large language models",
      "arxiv": "arXiv:2106.09685"
    },
    {
      "citation_id": "16",
      "title": "Hierarchically Decomposed Graph Convolutional Networks for Skeleton-Based Action Recognition",
      "authors": [
        "J Lee",
        "M Lee",
        "D Lee",
        "S Lee"
      ],
      "year": "2023",
      "venue": "ICCV"
    },
    {
      "citation_id": "17",
      "title": "Large language models understand and can be enhanced by emotional stimuli",
      "authors": [
        "C Li",
        "J Wang",
        "Y Zhang",
        "K Zhu",
        "W Hou",
        "J Lian",
        "F Luo",
        "Q Yang",
        "X Xie"
      ],
      "year": "2023",
      "venue": "Large language models understand and can be enhanced by emotional stimuli",
      "arxiv": "arXiv:2307.11760"
    },
    {
      "citation_id": "18",
      "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
      "authors": [
        "J Li",
        "D Li",
        "S Savarese",
        "S Hoi"
      ],
      "year": "2023",
      "venue": "ICML"
    },
    {
      "citation_id": "19",
      "title": "Videochat: Chat-centric video understanding",
      "authors": [
        "K Li",
        "Y He",
        "Y Wang",
        "Y Li",
        "W Wang",
        "P Luo",
        "Y Wang",
        "L Wang",
        "Y Qiao"
      ],
      "year": "2023",
      "venue": "Videochat: Chat-centric video understanding",
      "arxiv": "arXiv:2305.06355"
    },
    {
      "citation_id": "20",
      "title": "Mvbench: A comprehensive multi-modal video understanding benchmark",
      "authors": [
        "K Li",
        "Y Wang",
        "Y He",
        "Y Li",
        "Y Wang",
        "Y Liu",
        "Z Wang",
        "J Xu",
        "G Chen",
        "P Luo"
      ],
      "year": "2024",
      "venue": "CVPR"
    },
    {
      "citation_id": "21",
      "title": "Deep Facial Expression Recognition: A Survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "22",
      "title": "ST-Gait++: Leveraging spatio-temporal convolutions for gait-based emotion recognition on videos",
      "authors": [
        "M Lima",
        "W De Lima Costa",
        "E Martínez",
        "V Teichrieb"
      ],
      "year": "2024",
      "venue": "CVPR"
    },
    {
      "citation_id": "23",
      "title": "Visual Instruction Tuning",
      "authors": [
        "H Liu",
        "C Li",
        "Q Wu",
        "Y Lee"
      ],
      "year": "2023",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "24",
      "title": "See Your Emotion from Gait Using Unlabeled Skeleton Data",
      "authors": [
        "H Lu",
        "X Hu",
        "B Hu"
      ],
      "year": "2023",
      "venue": "AAAI"
    },
    {
      "citation_id": "25",
      "title": "Video-chatgpt: Towards detailed video understanding via large vision and language models",
      "authors": [
        "M Maaz",
        "H Rasheed",
        "S Khan",
        "F Khan"
      ],
      "year": "2023",
      "venue": "Video-chatgpt: Towards detailed video understanding via large vision and language models",
      "arxiv": "arXiv:2306.05424"
    },
    {
      "citation_id": "26",
      "title": "A continuous and objective evaluation of emotional experience with interactive play environments",
      "authors": [
        "R Mandryk",
        "M Atkins",
        "K Inkpen"
      ],
      "year": "2006",
      "venue": "SIGCHI"
    },
    {
      "citation_id": "27",
      "title": "Proxemo: Gait-based emotion learning and multi-view proxemic fusion for socially-aware robot navigation",
      "authors": [
        "V Narayanan",
        "B Manoghar",
        "V Dorbala",
        "D Manocha",
        "A Bera"
      ],
      "year": "2020",
      "venue": "Proxemo: Gait-based emotion learning and multi-view proxemic fusion for socially-aware robot navigation"
    },
    {
      "citation_id": "28",
      "title": "Survey on emotional body gesture recognition",
      "authors": [
        "F Noroozi",
        "C Corneanu",
        "D Kamińska",
        "T Sapiński",
        "S Escalera",
        "G Anbarjafari"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "29",
      "title": "Emotion recognition by skeleton-based spatial and temporal analysis. Expert Systems with Applications",
      "authors": [
        "A Oguz",
        "Ö Ertugrul"
      ],
      "year": "2024",
      "venue": "Emotion recognition by skeleton-based spatial and temporal analysis. Expert Systems with Applications"
    },
    {
      "citation_id": "30",
      "title": "ChatGPT: An AI Language Model",
      "authors": [
        "Openai"
      ],
      "year": "2024",
      "venue": "ChatGPT: An AI Language Model"
    },
    {
      "citation_id": "31",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "L Ouyang",
        "J Wu",
        "X Jiang",
        "D Almeida",
        "C Wainwright",
        "P Mishkin",
        "C Zhang",
        "S Agarwal",
        "K Slama",
        "A Ray"
      ],
      "year": "2022",
      "venue": "Training language models to follow instructions with human feedback"
    },
    {
      "citation_id": "32",
      "title": "Pytorch: An imperative style, high-performance deep learning library",
      "authors": [
        "A Paszke",
        "S Gross",
        "F Massa",
        "A Lerer",
        "J Bradbury",
        "G Chanan",
        "T Killeen",
        "Z Lin",
        "N Gimelshein",
        "L Antiga"
      ],
      "year": "2019",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "33",
      "title": "A Dual-Augmentor Framework for Domain Generalization in 3D Human Pose Estimation",
      "authors": [
        "Q Peng",
        "C Zheng",
        "C Chen"
      ],
      "year": "2024",
      "venue": "CVPR"
    },
    {
      "citation_id": "34",
      "title": "Adaptive body gesture representation for automatic emotion recognition",
      "authors": [
        "S Piana",
        "A Staglianò",
        "F Odone",
        "A Camurri"
      ],
      "year": "2016",
      "venue": "ACM Transactions on Interactive Intelligent Systems"
    },
    {
      "citation_id": "35",
      "title": "Language-guided Multi-modal Emotional Mimicry Intensity Estimation",
      "authors": [
        "F Qiu",
        "W Zhang",
        "C Liu",
        "L Li",
        "H Du",
        "T Guo",
        "X Yu"
      ],
      "year": "2024",
      "venue": "CVPR"
    },
    {
      "citation_id": "36",
      "title": "Llms are good action recognizers",
      "authors": [
        "H Qu",
        "Y Cai",
        "J Liu"
      ],
      "year": "2024",
      "venue": "CVPR"
    },
    {
      "citation_id": "37",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "C Hallacy",
        "A Ramesh",
        "G Goh",
        "S Agarwal",
        "G Sastry",
        "A Askell",
        "P Mishkin",
        "J Clark"
      ],
      "year": "2021",
      "venue": "ICML"
    },
    {
      "citation_id": "38",
      "title": "Language models are unsupervised multitask learners",
      "authors": [
        "A Radford",
        "J Wu",
        "R Child",
        "D Luan",
        "D Amodei",
        "I Sutskever"
      ],
      "year": "2019",
      "venue": "OpenAI blog"
    },
    {
      "citation_id": "39",
      "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "authors": [
        "C Raffel",
        "N Shazeer",
        "A Roberts",
        "K Lee",
        "S Narang",
        "M Matena",
        "Y Zhou",
        "W Li",
        "P Liu"
      ],
      "year": "2020",
      "venue": "Journal of machine learning research",
      "arxiv": "arXiv:2403.05530"
    },
    {
      "citation_id": "40",
      "title": "Multimodal database of emotional speech, video and gestures",
      "authors": [
        "T Sapiński",
        "D Kamińska",
        "A Pelikant",
        "C Ozcinar",
        "E Avots",
        "G Anbarjafari"
      ],
      "year": "2019",
      "venue": "ICPR"
    },
    {
      "citation_id": "41",
      "title": "Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition",
      "authors": [
        "L Shi",
        "Y Zhang",
        "J Cheng",
        "H Lu"
      ],
      "year": "2019",
      "venue": "CVPR"
    },
    {
      "citation_id": "42",
      "title": "Stanford Alpaca: An Instruction-following LLaMA model",
      "authors": [
        "R Taori",
        "I Gulrajani",
        "T Zhang",
        "Y Dubois",
        "X Li",
        "C Guestrin",
        "P Liang",
        "T Hashimoto"
      ],
      "year": "2023",
      "venue": "Stanford Alpaca: An Instruction-following LLaMA model"
    },
    {
      "citation_id": "43",
      "title": "Llama: Open and efficient foundation language models",
      "authors": [
        "H Touvron",
        "T Lavril",
        "G Izacard",
        "X Martinet",
        "M.-A Lachaux",
        "T Lacroix",
        "B Rozière",
        "N Goyal",
        "E Hambro",
        "F Azhar"
      ],
      "year": "2023",
      "venue": "Llama: Open and efficient foundation language models",
      "arxiv": "arXiv:2302.13971"
    },
    {
      "citation_id": "44",
      "title": "Actionclip: Adapting language-image pretrained models for video action recognition",
      "authors": [
        "M Wang",
        "J Xing",
        "J Mei",
        "Y Liu",
        "Y Jiang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "45",
      "title": "Emotion Recognition From Full-Body Motion Using Multiscale Spatio-Temporal Network",
      "authors": [
        "T Wang",
        "S Liu",
        "F He",
        "W Dai",
        "M Du",
        "Y Ke",
        "D Ming"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "46",
      "title": "Generative action description prompts for skeleton-based action recognition",
      "authors": [
        "W Xiang",
        "C Li",
        "Y Zhou",
        "B Wang",
        "L Zhang"
      ],
      "year": "2023",
      "venue": "ICCV"
    },
    {
      "citation_id": "47",
      "title": "Spatial temporal graph convolutional networks for skeleton-based action recognition",
      "authors": [
        "S Yan",
        "Y Xiong",
        "D Lin"
      ],
      "year": "2018",
      "venue": "AAAI"
    },
    {
      "citation_id": "48",
      "title": "Toward Long Video Understanding via Finedetailed Video Story Generation",
      "authors": [
        "Z You",
        "Z Wen",
        "Y Chen",
        "X Li",
        "R Zeng",
        "Y Wang",
        "M Tan"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "49",
      "title": "Improving Video Moment Retrieval by Auxiliary Moment-Query Pairs with Hyper-Interaction",
      "authors": [
        "R Zeng",
        "Y Zhuo",
        "J Li",
        "Y Yang",
        "H Wu",
        "Q Chen",
        "X Hu",
        "V Leung"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "50",
      "title": "Looking into Gait for Perceiving Emotions via Bilateral Posture and Movement Graph Convolutional Networks",
      "authors": [
        "Y Zhai",
        "G Jia",
        "Y.-K Lai",
        "J Zhang",
        "J Yang",
        "D Tao"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "51",
      "title": "Emotion recognition from body movements with as-lstm",
      "authors": [
        "H Zhang",
        "P Yi",
        "R Liu",
        "D Zhou"
      ],
      "year": "2021",
      "venue": "ICVR"
    },
    {
      "citation_id": "52",
      "title": "Kinematic dataset of actors expressing emotions. Scientific data",
      "authors": [
        "M Zhang",
        "L Yu",
        "K Zhang",
        "B Du",
        "B Zhan",
        "S Chen",
        "X Jiang",
        "S Guo",
        "J Zhao",
        "Y Wang"
      ],
      "year": "2020",
      "venue": "Kinematic dataset of actors expressing emotions. Scientific data"
    },
    {
      "citation_id": "53",
      "title": "Microsoft kinect sensor and its effect",
      "authors": [
        "Z Zhang"
      ],
      "year": "2012",
      "venue": "IEEE multimedia"
    },
    {
      "citation_id": "54",
      "title": "Deep learning-based human pose estimation: A survey",
      "authors": [
        "C Zheng",
        "W Wu",
        "C Chen",
        "T Yang",
        "S Zhu",
        "J Shen",
        "N Kehtarnavaz",
        "M Shah"
      ],
      "year": "2023",
      "venue": "Deep learning-based human pose estimation: A survey"
    },
    {
      "citation_id": "55",
      "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "authors": [
        "D Zhu",
        "J Chen",
        "X Shen",
        "X Li",
        "M Elhoseiny"
      ],
      "year": "2023",
      "venue": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "arxiv": "arXiv:2304.10592"
    }
  ]
}