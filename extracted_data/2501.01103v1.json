{
  "paper_id": "2501.01103v1",
  "title": "Learning Discriminative Features From Spectrograms Using Center Loss For Speech Emotion Recognition",
  "published": "2025-01-02T06:52:28Z",
  "authors": [
    "Dongyang Dai",
    "Zhiyong Wu",
    "Runnan Li",
    "Xixin Wu",
    "Jia Jia",
    "Helen Meng"
  ],
  "keywords": [
    "Center loss",
    "discriminative features",
    "speech emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Identifying the emotional state from speech is essential for the natural interaction of the machine with the speaker. However, extracting effective features for emotion recognition is difficult, as emotions are ambiguous. We propose a novel approach to learn discriminative features from variable length spectrograms for emotion recognition by cooperating softmax cross-entropy loss and center loss together. The softmax cross-entropy loss enables features from different emotion categories separable, and center loss efficiently pulls the features belonging to the same emotion category to their center. By combining the two losses together, the discriminative power will be highly enhanced, which leads to network learning more effective features for emotion recognition. As demonstrated by the experimental results, after introducing center loss, both the unweighted accuracy and weighted accuracy are improved by over 3% on Mel-spectrogram input, and more than 4% on Short Time Fourier Transform spectrogram input.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) is crucial for natural human-computer interaction. An SER system extracts features from the speech waveform and then classifies them into the corresponding emotion categories. And how to extract features containing enough emotional information has drawn growing interest.\n\nFor SER, traditional methods extract frame-level features from overlapped frames on speech signals and apply statistic functions on them to get additional features  [1] . Since deep neural network (DNN) can learn high-level invariant features from raw data  [2]  and deep learning brings a lot of breakthroughs in many fields  [3] , more and more methods utilizing neural networks to extract valid features from raw data have emerged. In  [4] , DNN and extreme learning machine were utilized to extract high-level features from low-level features. A bi-directional Long Short-Term Memory model was used in  [5]  to extract high level feature representations for SER. In  [6] , representation learning was performed on raw waveform for end-to-end SER. Convolutional and recurrent neural networks were applied to learn high-level representations from spectrograms in SER task  [7] .\n\nEmotions are naturally ambiguous  [8] , different types of emotions might be confusing, increasing the difficulty of extracting effective features  [9] . A trending methodology to release the ambiguity of emotion is to design an appropriate loss function instructing the neural network to learn discriminative features which have smaller intra-class variance and larger inter-class variance. A \"pairwise discriminative task\" was introduced in  [10]  to learn the similarity and distinction between two audios, which utilized cosine similarity loss together with binary cross entropy loss. In the task, pairwise audios were fed into an audio encode networks to extract audio vectors, and a following discriminative network judged whether the pairwise audios belong to the same emotion category using binary cross-entropy loss. The extracted audio vectors from the same class were made \"close\" by the effect of cosine similarity loss and they were classified by a support vector machine (SVM). In  [11] , a triplet framework was proposed to extract discriminative features by using triplet loss  [12] , whose input was triplets including two utterances from the same emotion class and one utterance from other classes. Then, similarly as  [10] , an SVM fed with extracted features was used for classifying.\n\nRecent methods to learn discriminative features for SER via using cosine similarity loss  [10]  or triplet loss  [11]  adopt a two-step strategy. These methods extract discriminative fea-tures in one step, and classify features with SVM in the other step. However, the two-step strategy may bring a reduction of SER performance as targets of the two steps may not be completely consistent. Besides, the performance of these methods depends heavily on the selection strategy of pairwise audios or triplets. In this paper, we propose a novel approach to extract discriminative features for SER from variable length spectrograms in an end-to-end manner using a jointly supervised loss consisting of softmax cross-entropy loss and center loss  [13] . Center loss pulls features in the same class closer to their class center, and softmax cross-entropy loss separates features from different emotional categories. Through optimizing center loss together with softmax cross-entropy loss, discriminative features will be learned for better SER results. Compared with cosine similarity loss method  [10]  and triplet loss method  [11] , center loss could naturally be integrated in common SER models, which dispense constituting sample pairs or sample triplets and the additional use of SVM classifier. CNN layers extract spatial information from a variable length spectrogram to get a variable length sequence, Bi-RNN compresses the variable length sequence down to a fixed-length vector. FC1 projects Bi-RNN's output to the desired dimensionality. FC2, whose output denotes the posterior class probabilities, is used to calculate softmax crossentropy loss. Softmax cross-entropy loss enables the network to learn separable features, and center loss reduces features' intra-class variation simultaneously.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "The Proposed Approach",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Model Details",
      "text": "The model takes a Short Time Fourier Transform (STFT) spectrogram or Mel-spectrogram as input, whose size is L T √ó L F . L T is variable depending on the length of audio, and L F is the dimension related to the frequency domain.\n\nAccording to the experience of computer vision, the convolutional network, whose first layer uses large convolution kernels and the remaining layers use small convolution kernels, perform well  [14, 15] . Besides, after dozens of tests, we determined the details of CNN layers as Fig.  2-(a) .\n\nThe Bi-RNN compresses variable length sequence produced by CNN layers to a fixed-length vector by concatenating the last output of forward RNN and backward RNN, as shown in Fig.  2-(b) . Bi-RNN is implemented with 128width Gated Recurrent Unit (GRU)  [16] , so the dimension of Bi-RNN's output is 256. FC1 projects Bi-RNN's output to the desired feature space of target dimension d (d = 64 in our experiments), with PReLU  [17]  activation function. We take FC1's output z ‚àà R d as the learned feature and calculate center loss according to z. The output of FC2 denotes the predicted posterior probabilities to corresponding emotion categories. And the parameters in FC2 is used for calculating softmax cross-entropy loss, which will be described in detail in section 2.2.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Softmax Cross-Entropy Loss",
      "text": "Softmax cross-entropy loss instructs the model to learn separable features, and it is common in multi-classification tasks. The softmax loss function is presented as equation 1:\n\nwhere m means the size of mini-batch and n is the number of emotion categories. z i ‚àà R d is the i-th deep feature, belonging to y i -th emotion category (y i ‚àà {1, 2, ..., n}). W j ‚àà R d denotes the j-th column of the weights W ‚àà R d√ón in FC2, b ‚àà R n is the bias in FC2 and b j is the j-th term of b.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Center Loss",
      "text": "To reduce intra-class variation of learned features, we introduce center loss in our model. Our model keeps a global center for each class and pulls features closer to their correspond-ing centers. The formula of center loss is given as follows:\n\nwhere c j (j ‚àà {1, 2, ..., n}) denotes the global class center of features corresponding to the j-th emotion category. Through optimizing the center loss, the distance between features within the same class becomes smaller. c j is initialized with 0 and updated per mini-batch iteration based on ƒãj , which is the j-th class center of features from a minibatch, caculated by equation 3 when\n\nThe global class center c j is updated as equation 4. Œ± is a hyperparameter controlling the update rate of c j when there are features corresponding to j-th emotion category in the new mini-batch, while c j keeps its previous value when no corresponding features in the new mini-batch. c t j and ƒãt j denotes the t-th iteration's value of c j and ƒãj respectively.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Weighted Loss And Joint Loss",
      "text": "Because of class imbalance, instead of using L 0 s and L 0 c directly, we assigned weights to softmax cross-entropy loss and center loss in our experiments, shown in equation 5 and equation 6. The weight œâ j (j ‚àà {1, 2, ..., n}) is in inverse proportion to the sample number of the j-th class in training set.\n\nOur neural network is trained using a joint loss comprised of softmax cross-entropy loss and center loss, calculated as equation 7. Œª is a hyperparameter trading off center loss against softmax cross-entropy loss. When Œª = 0, the model is trained using only softmax cross-entropy loss.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments And Analysis",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "Our model was tested on the Interactive Emotional Dyadic Motion Capture (IEMOCAP)  [18]  dataset, which was designed for studying multimodal expressive dyadic interactions. It contains approximately 12 hours of audiovisual data, including video, speech, motion capture of face and text transcriptions. For training and evaluation, we used categorical emotions neutral, angry, happy, sad and excited which represent the majority of the emotion categories in the database (5531 utterances), happy and excited were merged since they are close in the activation and valence domain  [18] .\n\nAs there is data imbalance between classes -neutral (30.9% of the total dataset), angry (19.9%), happy (29.6%), and sad (19.6%), we adopted both the unweighted accuracy (UA, the mean value of the recall for each class) and the weighed accuracy (WA, the number of correctly classified samples divided by the total amount of samples) as metrics to evaluate SER performance. In our experiments, the dataset was divided into 5 subsets randomly keeping the emotion distribution, 4 subsets were used for training, half of the last subset was used as development set and half as test set. We repeated cross-validation 5 times to get the final average result.\n\nOur experiments were conducted on log scale Melspectrogram and log scale STFT spectrogram respectively. To get spectrogram, a sequence of overlapping Hamming windows were applied to the speech signal, with window shift of 10msec, and window size of 40msec. The speech signal was sampled at 16kHz and the DFT length was 1024. The number of Mel bands was 128 when calculating Melspectrogram. We assumed that 14s long utterance contains enough emotional information. So for utterance whose duration is longer than 14s (2.07% of the total dataset), we only extracted the middle 14s to calculate spectrogram.\n\nDuring the training phase, we used Adam  [19]  optimizer, set learning rate to 0.0003, and the size of mini-batch was 32. We applied the parameters maximizing the UA of the development set as the model's final parameters.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Experiments On Mel-Spectrogram",
      "text": "As Œ± controls the update rate of class centers and Œª dominates the weight of center loss, we conducted experiments to investigate the effect of hyperparameter Œ± and Œª on Melspectrogram input. The experimental results are shown in Fig.  3 . Fig.  3-(a)  illustrates that the UA and WA are not sensitive to Œ±, Fig.  3-(b ) demonstrates that the SER performance can be significantly improved with proper value of Œª. When Œª = 0 (setting 1), the UA is 63.80% and the WA is 61.83%. The UA and WA is 66.86% and 65.40% respectively when Œª = 0.3, Œ± = 0.5 (setting 2). The UA and WA are both increased by over 3% when using center loss with proper hyperparameters.\n\nTo illustrate the discriminative power provided by center loss, we applied Principal Component Analysis(PCA) to embed learned features. The PCA embeddings (of features produced by once experiment in cross-validations on setting 1 and on setting 2 respectively) are drawn in Fig.  4 . Comparing  find that features belonging to the same class are more compact when using center loss. After introducing center loss together with softmax cross-entropy loss to train the model, the discriminative power can be enhanced, which leads to the model learning more effective features for SER.\n\nThe final confusion matrix was calculated by averaging confusion matrices from 5 times cross-validations, the results on setting 1 and setting 2 are shown in table 1-(a) and table 1-(b). As can be seen, after introducing center loss for enhancing discriminative power, the accuracy of each emotion has been improved to different degrees.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Experiments On Stft Spectrogram",
      "text": "We conducted experiments to prove that introducing center loss is also useful for learning effective features for SER on STFT spectrograms input, by comparing experiment result when Œª = 0 (setting 3) with Œª = 0.3, Œ± = 0.5 (setting 4). The final confusion matrices are shown in table 2-(a) and table 2-(b), which show that each emotion's accuracy has been improved after introducing center loss. The UA and WA are 60.98% and 58.93% on setting 3, while 65.13% and 62.96% on setting 4. Both the UA and WA are improved by more than 4% after introducing center loss.\n\nWe presented the UA and WA on setting 1 ‚àº setting 4 shown in Fig.  5 . We can conclude that introducing center loss could effectively improve the SER performance by comparing setting 2 with setting 1 or setting 4 with setting 3 in Fig.  5 . By comparing experimental results on setting 1 with setting 3 or setting 2 with setting 4 in Fig.  5 , we could find that learning effective features for SER on Mel-spectrogram input, which reduces the dimension based on human hearing characteristics, is easier than on STFT spectrogram input.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we presented an approach to learn discriminative features from variable length spectrograms by integrating center loss in the SER model. The 2-D PCA embedding illustrated the discriminative power when using center loss, which enables the neural network to learn more effective features for SER. Our experiment results demonstrated that center loss with proper hyperparameters is useful for improving the performance of SER on both Mel-spectrogram input and STFT spectrogram input. As center loss mainly focuses on reducing the intra-class variation of features, in future work, we will explore more in the loss function, hoping to increase the features' inter-class variation to further improve the SER performance.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Model framework. The model takes variable length spectro-",
      "page": 2
    },
    {
      "caption": "Figure 2: Model Details.(a) Model details of CNN layers, (b) Bi-RNN",
      "page": 2
    },
    {
      "caption": "Figure 3: UA and WA on log scale Mel-spectrogram input.(a) model",
      "page": 4
    },
    {
      "caption": "Figure 4: PCA embedding of features from, (a) training set on setting",
      "page": 4
    },
    {
      "caption": "Figure 5: The UA and WA on setting 1 ‚àºsetting 4.",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Reshape output: ùêø‚Ä≤& √óùëë)** (ùëë)**=96‚ãÖùêø‚Ä≤$)": "Reshape(Keep time axis)"
        },
        {
          "Reshape output: ùêø‚Ä≤& √óùëë)** (ùëë)**=96‚ãÖùêø‚Ä≤$)": "2-D convolutions output: ùêø‚Ä≤& √ó ùêø‚Äô $√ó96"
        },
        {
          "Reshape output: ùêø‚Ä≤& √óùëë)** (ùëë)**=96‚ãÖùêø‚Ä≤$)": "Max-pooling: 2 √ó2, strides [2, 2]"
        },
        {
          "Reshape output: ùêø‚Ä≤& √óùëë)** (ùëë)**=96‚ãÖùêø‚Ä≤$)": "Convolution: 96 filters of 3 √ó3, strides [1, 1], ReLUnonlinear"
        },
        {
          "Reshape output: ùêø‚Ä≤& √óùëë)** (ùëë)**=96‚ãÖùêø‚Ä≤$)": "Max-pooling: 2 √ó2, strides [2, 2]"
        },
        {
          "Reshape output: ùêø‚Ä≤& √óùëë)** (ùëë)**=96‚ãÖùêø‚Ä≤$)": "Convolution: 80 filters of 3 √ó3, strides[1, 1], ReLUnonlinear"
        },
        {
          "Reshape output: ùêø‚Ä≤& √óùëë)** (ùëë)**=96‚ãÖùêø‚Ä≤$)": "Max-pooling: 2 √ó2, strides [2, 2]"
        },
        {
          "Reshape output: ùêø‚Ä≤& √óùëë)** (ùëë)**=96‚ãÖùêø‚Ä≤$)": "Convolution: 64 filters of 3 √ó3, strides [1, 1], ReLUnonlinear"
        },
        {
          "Reshape output: ùêø‚Ä≤& √óùëë)** (ùëë)**=96‚ãÖùêø‚Ä≤$)": "Convolution: 48 filters of 7 √ó7, strides [2, 2], ReLUnonlinear"
        },
        {
          "Reshape output: ùêø‚Ä≤& √óùëë)** (ùëë)**=96‚ãÖùêø‚Ä≤$)": "Input: spectrogram LT√ó ùêø$"
        }
      ],
      "page": 2
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "3",
      "title": "Representation learning: A review and new perspectives",
      "authors": [
        "Y Bengio",
        "A Courville",
        "P Vincent"
      ],
      "year": "2013",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "4",
      "title": "Deep learning",
      "authors": [
        "Y Lecun",
        "Y Bengio",
        "G Hinton"
      ],
      "year": "2015",
      "venue": "nature"
    },
    {
      "citation_id": "5",
      "title": "Speech emotion recognition using deep neural network and extreme learning machine",
      "authors": [
        "K Han",
        "D Yu",
        "I Tashev"
      ],
      "year": "2014",
      "venue": "Fifteenth annual conference of the international speech communication association"
    },
    {
      "citation_id": "6",
      "title": "High-level feature representation using recurrent neural network for speech emotion recognition",
      "authors": [
        "J Lee",
        "I Tashev"
      ],
      "year": "2015",
      "venue": "High-level feature representation using recurrent neural network for speech emotion recognition"
    },
    {
      "citation_id": "7",
      "title": "Adieu features? endto-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis",
        "F Ringeval",
        "R Brueckner",
        "E Marchi",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "8",
      "title": "Efficient emotion recognition from speech using deep learning on spectrograms",
      "authors": [
        "A Satt",
        "S Rozenberg",
        "R Hoory"
      ],
      "year": "2017",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "9",
      "title": "Interpreting ambiguous emotional expressions",
      "authors": [
        "E Mower",
        "A Metallinou",
        "C Lee",
        "A Kazemzadeh",
        "C Busso",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2009",
      "venue": "Affective Computing and Intelligent Interaction and Workshops"
    },
    {
      "citation_id": "10",
      "title": "Long short term memory recurrent neural network based encoding method for emotion recognition in video",
      "authors": [
        "L Chao",
        "J Tao",
        "M Yang",
        "Y Li",
        "Z Wen"
      ],
      "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "A pairwise discriminative task for speech emotion recognition",
      "authors": [
        "Z Lian",
        "Y Li",
        "J Tao",
        "J Huang"
      ],
      "year": "2018",
      "venue": "A pairwise discriminative task for speech emotion recognition",
      "arxiv": "arXiv:1801.01237"
    },
    {
      "citation_id": "12",
      "title": "Speech emotion recognition from variable-length inputs with triplet loss function",
      "authors": [
        "J Huang",
        "Y Li",
        "J Tao",
        "Z Lian"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "13",
      "title": "Facenet: A unified embedding for face recognition and clustering",
      "authors": [
        "F Schroff",
        "D Kalenichenko",
        "J Philbin"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "14",
      "title": "A discriminative feature learning approach for deep face recognition",
      "authors": [
        "Y Wen",
        "K Zhang",
        "Z Li",
        "Y Qiao"
      ],
      "year": "2016",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "15",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "16",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "17",
      "title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "authors": [
        "K Cho",
        "B Van Merrienboer",
        "C Gulcehre",
        "D Bahdanau",
        "F Bougares",
        "H Schwenk",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Computer Science"
    },
    {
      "citation_id": "18",
      "title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2015",
      "venue": "IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "19",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "20",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    }
  ]
}