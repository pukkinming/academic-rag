{
  "paper_id": "2005.08453v3",
  "title": "Deep Architecture Enhancing Robustness To Noise, Adversarial Attacks, And Cross-Corpus Setting For Speech Emotion Recognition",
  "published": "2020-05-18T04:21:02Z",
  "authors": [
    "Siddique Latif",
    "Rajib Rana",
    "Sara Khalifa",
    "Raja Jurdak",
    "Björn W. Schuller"
  ],
  "keywords": [
    "speech emotion",
    "mixup",
    "data augmentation",
    "convolutional neural networks"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition systems (SER) can achieve high accuracy when the training and test data are identically distributed, but this assumption is frequently violated in practice and the performance of SER systems plummet against unforeseen data shi s. e design of robust models for accurate SER is challenging, which limits its use in practical applications. In this paper we propose a deeper neural network architecture wherein we fuse DenseNet, LSTM and Highway Network to learn powerful discriminative features which are robust to noise. We also propose data augmentation with our network architecture to further improve the robustness. We comprehensively evaluate the architecture coupled with data augmentation against (1) noise, (2) adversarial a acks and (3) cross-corpus se ings. Our evaluations on the widely used IEMOCAP and MSP-IMPROV datasets show promising results when compared with existing studies and state-of-the-art models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Despite the signi cant progress in Speech Emotion Recognition (SER) through Deep Neural Networks (DNNs), SER systems still perform poorly in noisy environments  [1, 2] , and when the imperceptible adversarial perturbation is added to test examples  [3] . e performance of state-of-the-art SER also degrades in the cross-corpus se ing when an acoustic mismatch between training and testing exists. is shows that SER systems lack robustness and generalisation which makes them susceptible to unknown test data shi s. Researchers have developed various methods to improve the performance of SER in noisy environment  [2, 4]  and cross-corpus se ing  [5] , however, signi cant performance improvement is still required.\n\nRobustness in DNNs can be enhanced by utilising very deep architectures. Many layers in a deep architecture allow learning very complex pa erns, therefore anomalies through the noise and other conditions imposed by cross-corpus can be isolated relatively easily. Studies on speech have shown that utilisation of very deep architectures has led to robustness against noisy situations by learning complex representations  [6, 7] . However, deep networks are di cult to train as it is subject to falling into local extrema, also takes longer time and powerful computational resources, e.g. GPU. Very recently, deep networks like DenseNet  [8]  and highway networks  [9]  have gained popularity, as they allow for easy training of very deep feedforward networks with considerably fewer parameters. DenseNets are densely connected Convolutional Networks and Highway networks closely follow the structure of the Long Short-Term Memory through gating mechanisms. Several studies in vision and ASR have shown the bene t of using DenseNet and highway networks, however, their performances for SER need to be evaluated. SER introduces the temporal dimension di erentiating it from images. It is also complicated than ASR, as ASR mainly deals with verbal messages, but SER works on vocal expressions that mesh verbal messages coded in an arbitrary and categorical fashion with nonverbal a ect signalling system coded in an iconic and continuous fashion  [10] .\n\nRobustness can also be enhanced using data augmentation techniques, which improve the generalisation and help DNNs to provide robustness against unseen real-time situations  [11, 12] . Recently \"mixup\"  [12]  shows great promise for data augmentation by augmenting a synthetic sample as a linear combination of the original sample. It can make the training data more diverse and the regularisation e ect more powerful, so as to further improve the generalisation ability of the network  [13] . Despite its potential, the performance of mixup is not validated for SER.\n\nBesides noise, several studies have shown that SER systems are also susceptible to adversarial a acks and their performance can signi cantly drop due to the a ack  [3, 14] . Adversarial attacks are developed by malicious adversaries to cra adversarial examples by the addition of unperceived perturbation to elicit wrong responses from machine learning (ML) models. Methods to achieve robustness against the adversarial a acks in SER systems have not been evaluated. is paper makes several contributions.\n\n1. We propose a deep SER model built on DenseNet and highway networks for robust SER.\n\n2. To further improve robustness, we propose mixup as a data augmentation method.\n\n3. We comprehensively evaluate the robustness of the proposed model in three distinct se ings (a) noisy conditions, (b) adversarial a acks, and (c) cross-corpus scenarios.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "In recent years, several studies in the image domain  [15]  and in Automatic Speech Recognition (ASR) have used deep archi-tectures to achieve robustness. In  [7] , the authors used a very deep convolutional residual network (VDCRN) for noise-robust speech recognition. ey empirically showed VDCRN is more robust to noise and able to signi cantly reduce the word error rate (WER). Studies in vision  [16, 15]  showed DenseNet is more robust compared to the other state-of-the-art models including ResNet  [17]  due to its e cient feature reuse ability. Similarly, studies  [18, 19]  have found that DenseNet can help achieve robustness and generalisability in ASR. However, none of these studies evaluates DenseNet for robust SER. In this work, we modify DenseNet architecture and use it as a feature extractor in our proposed model. Although we could not nd any study using very deep architectures for SER, several studies have considered DNNs to achieve robustness for SER. Huang et al.  [20]  used a convolutional neural network (CNN) -long-short term memory (LSTM) CNN-LSTM model for robust SER. ey found that CNN demonstrates a certain degree of noise robustness. However, this study does not utilise very deep architectures. In  [2] , the authors utilised deep residual networks as an enhancement architecture to remove noise from speech while preserving enough information for an SER system. is study was focused on speech enhancement instead of robust representation learning. Some studies  [1, 21]  also explored di erent noise removal methods for SER in noisy environments.\n\nData augmentation is a well-known practice to enlarging the size of the training set in many machine-learning applications. Recently, it has been shown that mixup data augmentation can enhance the classi ers' robustness for unseen test data  [12] . It also improves generalisation performance and model robustness against adversarial examples  [22] . e regularisation e ect of mixup has been evaluated for ASR  [23] , however, no study has evaluated mixup in SER. Here, we use mixup to improve generalisation and robustness of proposed model against noisy environment, adversarial a acks, and cross-corpus setting.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Proposed Model",
      "text": "Our proposed model is a hybrid architecture, where we use a DenseBlock for temporal feature extraction, LSTM for context aggregation and fully connected layers in highway con guration for discriminative feature learning. A schematic diagram of the proposed model is shown in Figure  1 .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Temporal Feature Capturing Using Densenet",
      "text": "e rst element of our model is DenseNet. DenseNet enables learning temporal features by introducing direct connections from each layer to all subsequent layers. Consequently, the l th layer (x l ) receives the feature maps of all preceding layers as input:\n\nHere, H l,G (.) represents to a composite function including batch normalisation (BN)  [24] , a recti ed linear unit (ReLU)  [25]  and a convolution (Conv) layer. G is the growth rate that represents the number of output feature maps. Cascading multiple layers of composite functions and feature map concatenations forms a so-called DenseBlock (L, G), which has L layers and a growth rate of G. e concatenations (Equation  1 ) in the DenseBlock causes the input size to be increased as the number of layers increases in DenseBlock. For downsampling, a transition layer is used a er each DenseBlock. e transition layer consists of a batch normalisation layer, a 1 × 1 convolutional  In the DenseNet architecture, there is a global average pooling layer that we replace with a reshape layer to adapt the dimension of feature maps to LSTM layer for context learning.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotional-Context Modelling Using Lstm",
      "text": "Emotions are context-dependent  [26]  and contexts are embedded in temporal dimension of the data  [27] . LSTM  [28]  have gated architectures, which enabling memory, helps to model temporal relationships. We, therefore, use LSTM a er the DenseBlock for context modelling.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Discriminative Feature Extraction Using Highway Network",
      "text": "Finally, we use the highway network  [9]  to extract high-level discriminative features. We also use two skip connections from each DenseBlock and the sum of these features are concatenated with the LSTM features and given to the highway network. Skip connections introduce the shortcut connection that shu les di erent levels of abstractions and also improves the gradient ow in the network  [29] . e output y of a highway block is given by:\n\nwhere H (parameters by WH ) is a nonlinear transformation on its associated input x, and C and T represent carry gates and transform gates, respectively. e layer indices and biases have been excluded in Equation 2 for simpli cation.\n\nGates in a highway network control information ow and speed up convergence enabling e ective training of DNNs across several layers without degradation  [9] . is helps the highway network to learn high-level discriminative representation  [30]  that help to achieve be er classi cation performance. erefore, we use the highway network layers prior to so max layer for classi cation.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Setup",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Dataset",
      "text": "We evaluate our model on two popular datasets: Interactive Emotional Dyadic Motion Capture (IEMOCAP)  [31]  and MSP-IMPROV  [32] .\n\ne detail of these datasets is given below. IEMOCAP: is corpus contains ve sessions, where each session has u erances from two speakers (one male and one female). Overall, there are 10 unique speakers. We consider four emotions including angry, happy, neutral and sad. To be consistent with previous studies  [33] , we merge excitement with happiness and consider it as one class: happy. MSP-IMPROV: e MSP-IMPROV dataset contains six sessions, where each session comprises of u erances from two speakers, one male, and one female.\n\nere are four emotion categories in MSP-IMPROV: angry, neutral, sad, and happy.\n\nAll were used in the experiments. DEMAND We choose the Diverse Environments Multichannel Acoustic Noise Database (DEMAND) dataset  [34]  as a source of our noise signal.\n\nis data contains the recording of various real-world noises in a variety of se ings. We select noise recordings of 16 kHz sampling rates.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data Pre-Processing",
      "text": "We use spectrogram as our starting point. We compute them using Short-Time Fourier Transform (STFT) with an overlapping Hamming window of size 25 ms with a 10 ms shi . We select the height of spectrogram equal to 128. We apply a context window of 256 frames to reach a xed width of segments of spectrograms following the procedure used in  [35] . Each segment is assigned the emotion labelling of the corresponding u erance. We train all the models using short segments, however, u erance level prediction is calculated by averaging the posterior probabilities of the respective sub-segments.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data Augmentation",
      "text": "We use the \"mixup\" data augmentation technique, which has not been used in SER. It creates training samples using following equations:\n\nHere, (xi, yi) and (xj, yj) are two randomly selected examples from training data, and λ ∈ [0, 1]. Mixup can be employed on raw speech as well as on features  [12] . We use mixup on spectrograms.\n\nWe also use the widely used speed perturbation (SP) for data augmentation. We follow  [27]  to create samples using speed perturbation. For a given u erance, we produce two versions of each u erance by applying the speed e ect at the factors of 0.9 and 1.1.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Model Con Guration",
      "text": "In our DenseNet architecture, a er the initial convolutional layer, we use two DenseBlocks and each DenseBlock consists of L = 6 layers with a growth rate of G = 24. A er the rst DenseBlocks, we place a transition layer for down-sampling the size of feature maps. is consist of a 1×1 convolutional layer followed by a 2×2 average pooling layer with stride of 2. e transition of temporal features from the DenseBlock to LSTM is made using a reshape layer. e LSTM layer serves to learn contextual information from the temporal features extracted by the DenseBlocks. A er the LSTM layer, we insert a dropout layer with a dropout value of 0.5. e sum of features discovered by each DenseBlock, having two skip connections, is concatenated with the contextual features from LSTM and given to the highway network to learn discriminative features. We apply three fully connected layers with 128 hidden units with ReLU activation in the highway block followed by a so max layer for emotion classi cation.\n\nWe also implement benchmarking models including DenseNet, DenseNet-LSTM, CNN, and CNN-LSTM. In DenseNet, we apply three DenseBlocks and each consists of L = 6 layers with a growth rate of G = 16. A er the DenseBlocks, we place a 3×3 global average pooling layer and a fully connected layer of 1 000 hidden units before a so max layer.\n\nFor DenseNet-LSTM, we use an LSTM layer instead of a global average pooling layer. In all convolutional layers used in the DenseNet based models, batch normalisation  [24]  is employed before the non-linearities. A recti ed linear unit (ReLU)  [25]  is used as the activation function.\n\nFor the CNN-LSTM, we follow the architecture con guration described in  [36] . To use CNN for benchmarking, we choose a fully connected layer instead of an LSTM layer in the above CNN-LSTM model.\n\nWe train the models using the training set and validation set is used for hyper-parameter selection. For minimisation of the cross-entropy loss function, we choose the Adam optimiser  [37] . We start the training with an initial learning rate of 10 -3 . If the validation accuracy does not improve for ve consecutive epochs, we halve the learning rate and stop the process if the validation accuracy does not improve for 20 consecutive epochs. For each model, we repeat the evaluation 10 times and report the mean and standard deviation.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments And Results",
      "text": "We apply a leave-one-speaker-out scheme for both datasets and report unweighted average recall (UAR) for both datasets. UAR is a widely used metric used for speech emotion recognition due to the dominance of class imbalanced datasets in this eld. In each session, we employ u erances from one speaker for testing and the other speakers' u erances for validation. is con guration is used for all models. Results are computed in noisy, cross-corpus, and adversarial a acks se ings.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Benchmark Results",
      "text": "e comparison of our proposed model with the benchmark models CNN, CNN-LSTM, DenseNet and DenseNet-LSTM are presented in Table  1 . All these results are computed without data augmentation. We observe that for both IEMOCAP and MSP-IMPROV datasets, our proposed model performs be er than the benchmarking models.   [36]  62.0 -CNN  [35]  61.4 55.  3  Our proposed model is also performing be er compared to standard DenseNet and DenseNet-LSTM, which shows that the use of highway connectivity in our proposed model is helping to achieve be er results compared to the variants of CNNs, DenseNet, and recent studies  [36, 35] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Noisy Environment",
      "text": "To evaluate the model in a noisy environment, we select three signal-to-noise ratio (SNR) values [0,  10, 20] . We consider the mismatched condition, where the model is trained on clean data and the test data incorporates noisy samples. We choose ve noises including kitchen, park, station, tra c, and cafeteria from the DEMAND dataset. ese noises are randomly added to the test set at three di erent SNR levels. Results on IEMOCAP data are reported in Table  2 . It can be noted from Table  2  that the proposed model provides be er results compared to all the other models.\n\nWe also observe from Table  2  that the data augmentation techniques help to improve robustness. Data augmentation using mixup achieves be er results compared to that using speed perturbation, however, the combination of mixup and speed perturbation provides the best results.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Adversarial Settings",
      "text": "In adversarial se ings, we use two adversarial a acks including the Fast Gradient Sign Method (FGSM)  [38]  and the Basic Iterative Method (BIM)  [39]  to evaluate the robustness. FGSM creates the adversarial examples by adding a scaled noise in the direction of the gradient of the loss function. Instead of applying adversarial noise in a single step like FGSM, BIM iteratively applies it multiple times. We applied these two a acks with the perturbation factor = 0.08 on di erent classi ers and performance is reported in Table  3 . We observe that our proposed model performs the best with or without data augmentation. We also observe data augmentation techniques help to improve robustness in the adversarial se ings in the same way as in the presence of noise.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Cross-Corpus Settings",
      "text": "To evaluate the proposed model in a cross-corpus se ing, we use IEMOCAP as training data and MSP-IMPROV as the test set. We randomly select 30 % of MSP-IMPROV for parameter selection and 70 % for testing, as used in  [35] . We evaluate di erent models in the cross-corpus se ing.\n\ne results are given in Figure  2 . We observe that the proposed model achieves   be er performance and data augmentation helps to improve the robustness. We also compare our results with previous studies (  [35, 40] ,) in the cross-corpus se ing in Table  4 . In  [35] , authors employ a multi-task framework and exploit larger unlabelled data for the auxiliary task to improve the generalisation of the model. In  [40] , the authors develop a multi-modal technique (audio plus text) for SER based on ASR transcriptions. ey demonstrate that the generalisability of ASR models helps to improve the generalisation of emotion classi cation models. In contrast, we propose a deeper model coupled with data augmentation to achieve improved generalisation. We are achieving be er results compared to these studies as reported in Table  4 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusions",
      "text": "is paper introduces a new hybrid model to build a robust SER system. is model exploits a DenseNet for feature extraction, LSTM for contextual learning, and deep neural network layers with highway connectivity for discriminative representation learning, and produce robust representation. is paper also proposes data augmentation to further improve the robustness of the architecture. e performance of our proposed technique is evaluated on widely used IEMOCAP and MSP-IMPROV datasets against noise, adversarial a acks, and cross-corpus settings. Results show that our proposed technique is more robust compared to existing methods and other state-of-the-art models. Results also reveal several valuable information, such as, mixup is a be er augmentation technique for SER compared to the popular speed perturbation. Results also show that DenseNet based models are more robust compared to CNN-LSTM or just CNN. In future work, we aim at further optimising these architectures and augmentation in closer loop.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: 3.1. Temporal Feature Capturing using DenseNet",
      "page": 2
    },
    {
      "caption": "Figure 1: Schematic diagram of the proposed model.",
      "page": 2
    },
    {
      "caption": "Figure 2: We observe that the proposed model achieves",
      "page": 4
    },
    {
      "caption": "Figure 2: Comparing diﬀerent models in cross-corpus SER.",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "CNN",
          "IEMOCAP": "61.5 ± 2.3",
          "MSP-IMPROV": "52.6 ± 2.5"
        },
        {
          "Model": "CNN-LSTM",
          "IEMOCAP": "62.1 ±1.8",
          "MSP-IMPROV": "53.1 ± 2.3"
        },
        {
          "Model": "DenseNet",
          "IEMOCAP": "63.2 ± 1.7",
          "MSP-IMPROV": "54.5 ± 1.9"
        },
        {
          "Model": "DenseNet-LSTM",
          "IEMOCAP": "63.5 ± 1.5",
          "MSP-IMPROV": "55.6 ± 1.6"
        },
        {
          "Model": "Proposed",
          "IEMOCAP": "64.1 ± 1.3",
          "MSP-IMPROV": "56.2 ± 1.5"
        },
        {
          "Model": "CNN-LSTM [36]",
          "IEMOCAP": "62.0",
          "MSP-IMPROV": "–"
        },
        {
          "Model": "CNN [35]",
          "IEMOCAP": "61.4",
          "MSP-IMPROV": "55.3"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Front-end feature compensation for noise robust speech emotion recognition",
      "authors": [
        "M Pandharipande",
        "R Chakraborty",
        "A Panda",
        "B Das",
        "S Kopparapu"
      ],
      "year": "2019",
      "venue": "2019 27th European Signal Processing Conference"
    },
    {
      "citation_id": "3",
      "title": "Towards robust speech emotion recognition using deep residual networks for speech enhancement",
      "authors": [
        "A Triantafyllopoulos",
        "G Keren",
        "J Wagner",
        "I Steiner",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "4",
      "title": "Adversarial machine learning and speech emotion recognition: Utilizing generative adversarial networks for robustness",
      "authors": [
        "S Latif",
        "R Rana",
        "J Qadir"
      ],
      "year": "2018",
      "venue": "Adversarial machine learning and speech emotion recognition: Utilizing generative adversarial networks for robustness",
      "arxiv": "arXiv:1811.11402"
    },
    {
      "citation_id": "5",
      "title": "Deep representation learning in speech processing: Challenges, recent advances, and future trends",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Qadir",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "Deep representation learning in speech processing: Challenges, recent advances, and future trends",
      "arxiv": "arXiv:2001.00378"
    },
    {
      "citation_id": "6",
      "title": "Transfer learning for improving speech emotion classi cation accuracy",
      "authors": [
        "S Latif",
        "R Rana",
        "S Younis",
        "J Qadir",
        "J Epps"
      ],
      "year": "2018",
      "venue": "Interspeech 2018: Proceedings"
    },
    {
      "citation_id": "7",
      "title": "Very deep convolutional neural networks for noise robust speech recognition",
      "authors": [
        "Y Qian",
        "M Bi",
        "T Tan",
        "K Yu"
      ],
      "year": "2016",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "8",
      "title": "Adaptive very deep convolutional residual network for noise robust speech recognition",
      "authors": [
        "T Tan",
        "Y Qian",
        "H Hu",
        "Y Zhou",
        "W Ding",
        "K Yu"
      ],
      "year": "2018",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "9",
      "title": "Densely connected convolutional networks",
      "authors": [
        "G Huang",
        "Z Liu",
        "L Van Der Maaten",
        "K Weinberger"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pa ern recognition"
    },
    {
      "citation_id": "10",
      "title": "Highway networks",
      "authors": [
        "R Srivastava",
        "K Gre",
        "J Schmidhuber"
      ],
      "year": "2015",
      "venue": "Highway networks",
      "arxiv": "arXiv:1505.00387"
    },
    {
      "citation_id": "11",
      "title": "Speech emotion analysis",
      "authors": [
        "P Juslin",
        "K Scherer"
      ],
      "year": "2008",
      "venue": "Scholarpedia"
    },
    {
      "citation_id": "12",
      "title": "Augmix: A simple data processing method to improve robustness and uncertainty",
      "authors": [
        "D Hendrycks",
        "N Mu",
        "E Cubuk",
        "B Zoph",
        "J Gilmer",
        "B Lakshminarayanan"
      ],
      "year": "2019",
      "venue": "ICLR"
    },
    {
      "citation_id": "13",
      "title": "mixup: Beyond empirical risk minimization",
      "authors": [
        "H Zhang",
        "M Cisse",
        "Y Dauphin",
        "D Lopez-Paz"
      ],
      "year": "2018",
      "venue": "mixup: Beyond empirical risk minimization"
    },
    {
      "citation_id": "14",
      "title": "Understanding mixup training methods",
      "authors": [
        "D Liang",
        "F Yang",
        "T Zhang",
        "P Yang"
      ],
      "year": "2018",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "15",
      "title": "Cra ing adversarial examples for speech paralinguistics applications",
      "authors": [
        "Y Gong",
        "C Poellabauer"
      ],
      "year": "2018",
      "venue": "DYnamic and Novel Advances in Machine Learning and Intelligent Cyber Security Workshop (DY-NAMICS)"
    },
    {
      "citation_id": "16",
      "title": "When NAS meets robustness: In search of robust architectures against adversarial a acks",
      "authors": [
        "M Guo",
        "Y Yang",
        "R Xu",
        "Z Liu"
      ],
      "year": "2019",
      "venue": "When NAS meets robustness: In search of robust architectures against adversarial a acks",
      "arxiv": "arXiv:1911.10695"
    },
    {
      "citation_id": "17",
      "title": "Sparsely aggregated convolutional networks",
      "authors": [
        "L Zhu",
        "R Deng",
        "M Maire",
        "Z Deng",
        "G Mori",
        "P Tan"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "18",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pa ern recognition"
    },
    {
      "citation_id": "19",
      "title": "DenseNet BLSTM for acoustic modeling in robust asr",
      "authors": [
        "M Strake",
        "P Behr",
        "T Lohrenz",
        "T Fingscheidt"
      ],
      "year": "2018",
      "venue": "2018 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "20",
      "title": "A dual-staged context aggregation method towards e cient end-to-end speech enhancement",
      "authors": [
        "K Zhen",
        "M Lee",
        "M Kim"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "21",
      "title": "Deep convolutional recurrent neural network with a ention mechanism for robust speech emotion recognition",
      "authors": [
        "C.-W Huang",
        "S Narayanan"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "22",
      "title": "Improving noise robustness of speech emotion recognition system",
      "authors": [
        "L Juszkiewicz"
      ],
      "year": "2014",
      "venue": "Intelligent Distributed Computing VII"
    },
    {
      "citation_id": "23",
      "title": "Mixup inference: Be er exploiting mixup to defend adversarial a acks",
      "authors": [
        "T Pang",
        "K Xu",
        "J Zhu"
      ],
      "year": "2019",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "24",
      "title": "Speaker adaptive training and mixup regularization for neural network acoustic models in automatic speech recognition",
      "authors": [
        "N Tomashenko",
        "Y Khokhlov",
        "Y Estève"
      ],
      "year": "2018",
      "venue": "Interspeech"
    },
    {
      "citation_id": "25",
      "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shi",
      "authors": [
        "S Io",
        "C Szegedy"
      ],
      "year": "2015",
      "venue": "Batch normalization: Accelerating deep network training by reducing internal covariate shi",
      "arxiv": "arXiv:1502.03167"
    },
    {
      "citation_id": "26",
      "title": "Recti ed linear units improve restricted boltzmann machines",
      "authors": [
        "V Nair",
        "G Hinton"
      ],
      "year": "2010",
      "venue": "Proceedings of the 27th international conference on machine learning (ICML-10)"
    },
    {
      "citation_id": "27",
      "title": "Variational autoencoders for learning latent representations of speech emotion: A preliminary study",
      "authors": [
        "S Latif",
        "R Rana",
        "J Qadir",
        "J Epps"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "28",
      "title": "Direct modelling of speech emotion from raw speech",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Epps"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "29",
      "title": "Long short-term memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "30",
      "title": "Image super-resolution using dense skip connections",
      "authors": [
        "T Tong",
        "G Li",
        "X Liu",
        "Q Gao"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "31",
      "title": "Tacotron: Towards endto-end speech synthesis",
      "authors": [
        "Y Wang",
        "R Skerry-Ryan",
        "D Stanton",
        "Y Wu",
        "R Weiss",
        "N Jaitly",
        "Z Yang",
        "Y Xiao",
        "Z Chen",
        "S Bengio"
      ],
      "year": "2017",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "32",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "33",
      "title": "Msp-improv: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "C Busso",
        "S Parthasarathy",
        "A Burmania",
        "M Abdelwahab",
        "N Sadoughi",
        "E Provost"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on A ective Computing"
    },
    {
      "citation_id": "34",
      "title": "Using regional saliency for speech emotion recognition",
      "authors": [
        "Z Aldeneh",
        "E Provost"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "35",
      "title": "e diverse environments multi-channel acoustic noise database (demand): A database of multichannel environmental noise recordings",
      "authors": [
        "J Iemann",
        "N Ito",
        "E Vincent"
      ],
      "year": "2013",
      "venue": "Proceedings of Meetings on Acoustics ICA2013"
    },
    {
      "citation_id": "36",
      "title": "Multi-task semi-supervised adversarial autoencoding for speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Epps",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on A ective Computing"
    },
    {
      "citation_id": "37",
      "title": "E cient emotion recognition from speech using deep learning on spectrograms",
      "authors": [
        "A Sa",
        "S Rozenberg",
        "R Hoory"
      ],
      "year": "2017",
      "venue": "INTER-SPEECH"
    },
    {
      "citation_id": "38",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "39",
      "title": "Explaining and harnessing adversarial examples",
      "authors": [
        "I Goodfellow",
        "J Shlens",
        "C Szegedy"
      ],
      "year": "2014",
      "venue": "Explaining and harnessing adversarial examples",
      "arxiv": "arXiv:1412.6572"
    },
    {
      "citation_id": "40",
      "title": "Adversarial examples in the physical world",
      "authors": [
        "A Kurakin",
        "I Goodfellow",
        "S Bengio"
      ],
      "year": "2016",
      "venue": "Adversarial examples in the physical world",
      "arxiv": "arXiv:1607.02533"
    },
    {
      "citation_id": "41",
      "title": "Multimodal learning for speech emotion recognition: An analysis and comparison of asr outputs with ground truth transcription",
      "authors": [
        "S Sahu",
        "V Mitra",
        "N Seneviratne",
        "C Espy-Wilson"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    }
  ]
}