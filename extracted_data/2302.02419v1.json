{
  "paper_id": "2302.02419v1",
  "title": "Deep Learning Of Segment-Level Feature Representation For Speech Emotion Recognition In Conversations",
  "published": "2023-02-05T16:15:46Z",
  "authors": [
    "Jiachen Luo",
    "Huy Phan",
    "Joshua Reiss"
  ],
  "keywords": [
    "contextual information",
    "affective computing",
    "speaker-sensitive"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Accurately detecting emotions in conversation is a necessary yet challenging task due to the complexity of emotions and dynamics in dialogues. The emotional state of a speaker can be influenced by many different factors, such as interlocutor stimulus, dialogue scene, and topic. In this work, we propose a conversational speech emotion recognition method to deal with capturing attentive contextual dependency and speaker-sensitive interactions. First, we use a pretrained VGGish model to extract segment-based audio representation in individual utterances. Second, an attentive bi-directional gated recurrent unit (GRU) models contextual-sensitive information and explores intra-and inter-speaker dependencies jointly in a dynamic manner. The experiments conducted on the standard conversational dataset MELD demonstrate the effectiveness of the proposed method when compared against state-of the-art methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Automatic recognition of human emotions has widespread applications in areas such as dialogue generation, social media analysis and human computer interaction  [1] . Speech is the main communication medium in which people can clearly and intuitively feel emotional changes. Unlike vanilla emotion recognition of sentences/utterances, emotion recognition in conversation (ERC) ideally relies on mining human emotions from conversations or dialogues having two or more interlocutors and requires context modeling of the individual utterances, and requires context modeling of the individual utterances  [2] . How to capture such information from speech signal is a challenging task.\n\nIn this work, we focus on speech signals in interactive conversation. Speech signals naturally can carry the emotional characteristics of the speaker. Conventionally, conversational emotion recognition usually requires a strong ability to model context-sensitive attributes, select crucial information, and capture speaker-sensitive dependencies  [3] . Among all the factors, speaker information is important for tracking the emotional characteristics of conversations, especially intra-and inter-speaker dependencies.\n\nIn interactive conversations, these factors lead to diverse emotional dynamics. Fig.  1  presents some examples demonstrating such patterns from the Multi-modal EmotionLines Dataset (MELD)  [4] . Conversation (a) depicts the presence of emotional inertia which To model such conversations, an architecture would need to deal with these challenges: how to capture self-and inter-speaker dependencies to govern emotional dynamics, and how to interpret latent emotions from its contextual information in the conversation flows. What's more, the raw emotion can be enhanced, weakened, or reversed based on the contextual information from neighboring utterances  [5] . For utterance-level speech emotion recognition, an underlying issue is a loss of dynamic temporal information and short-term emotion dynamics by compressing speech into utterance-level features  [6] . However, little progress has been made in analyzing the emotion estimation among segment-based feature representation in individual utterances, context-sensitive information and speaker influences in conversations. Devamanyu et al. used text modality features to model the contextual information into self-and inter-speaker emotional influences in the ERC task  [7] .\n\nIn this paper, we present an approach which can enable the coevolution of the local and global contextual information among segments in utterances and accommodate the self-influence, intra-and inter-speaker state in the emotion-aware spoken dialog system. We first use a pre-trained VGGish model to extract segment-level audio representation in an utterance. Next, a statistical strategy determines emotional dynamics of utterance-level information in speech. To dynamically integrate contextual dependency and speaker-sensitive interactions, we employ bi-directional GRU to model such relations. Overall, our contributions are summarized as follows:\n\n• A segment-level feature extraction strategy was able to empower a dialogue system with fine-grained temporal emotional representation.\n\n• We utilized the bi-directional GRU layer to capture contextsensitive information, intra-and inter-speaker dependencies on conversational emotion recognition, in combination with attention mechanism to highlight the important global contextual utterances.\n\n• Our proposed approach was shown to be superior to state-ofthe-art methods for conversational emotion recognition.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Existing Literature",
      "text": "Global and local audio features of speech emotion recognition systems are typically classified into the following four categories: prosodic features, spectral features, voice quality features, and Teager Energy Operator-based features  [8] . Traditionally, a number of spectral features are generally depicted using one of the cepstrumbased representations available. Commonly, Mel-frequency cepstral coefficients (MFCC) or Mel-scale spectrograms were used, and in some studies, formants, and other information were utilized as well  [8] . Suraj et al. demonstrated the effectiveness of convolutional neural networks in emotion classification with MFCCs  [5] . Besides, the direct use of Mel-scale spectrograms for ERC was proved successful as well  [9] . In this work, we use a pretrained model to extract high-level acoustic features from low-level Mel-scale spectrograms for emotion recognition.\n\nERC requires deep understanding of human interactions in conversations [10∼13]. Some of the important works attributes emotional dynamics to be interactive phenomena  [13, 16] , rather than being within-person. We utilize this trait in the design of our model that incorporates inter-speaker dynamic in a conversation. Since conversations have a natural temporal nature, context also play a crucial role in emotion analysis  [4, 17] . Poria et al. employed a bi-directional LSTM to capture temporal context information from surrounding utterances of the same speaker to infer emotions  [4] . However, there is no provision to model context and speaker interactive influences.\n\nIt should be noted that our method is different from previous strategies for ERC. We propose to capture this contextual-sensitive information via hierarchical recurrent networks. Additionally, our proposed approach adopts an interactive scheme that actively models intra-and inter-speaker emotional dynamics in conversations.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "Our proposed approach consists of three major modules: 1) pretrained segment-level audio representation, 2) grouped parallel statistical operation, referred to as statistical unit (SU), and 3) a dialogaware interaction module that aims to model the interactions in the dialogue and then makes the emotional state prediction.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Pre-Trained Audio Embeddings",
      "text": "On the audio end, an audio segment of 0.96s is converted into log Mel-spectrogram and encoded using a 128-dimensional feature vector extracted from the last fully connected layer of a pre-trained VGGish model. Especially, the shorter samples were zero-padded before transformation into log Mel-spectrogram. Non-overlapping segments are used during segmentation. The model was trained on AudioSet, consisting of 100 million YouTube videos  [14] . For each utterance, a sequence of L embedding vectors was produced, where L represents the number of audio segments that the audio signal is partitioned into (see Fig.  2 ).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Statistical Unit",
      "text": "We use a statistical unit with three parallel one-dimensional statistics along the sequence direction to reduce the sequence of L segmentwise embedding vectors and produce utterance-wise embedding vectors: average, max and min, as shown in Fig.  2 . Finally, we concatenate them into one feature vector for utterance-wise representation.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Model",
      "text": "The proposed model has four branches of bi-directional GRU cells to capture the cumulative context, intra-speaker state, inter-speaker influence and emotion state of the participant (see Fig.  3 ).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Attentive Contextual State",
      "text": "In conversational emotion recognition, to determine the emotional state of an utterance at timestamp t, the preceding utterances can be considered as its cumulative context. The context state stores and propagates overall utterance-level information along the sequence of the conversation flow. The contextual state Ct-1, intra-speaker state St-1 and inter-speaker state It-1 of the previous utterance, and audio representation ut at timestamp t are used to update the contextual information from Ct-1 to Ct (see Fig.  3 ). The steps in the attentive contextual state update Ct are described using the following formula and shown in Fig.  3 .\n\nwhere ⊕ represents concatenation. At the time step t = 0, the context state is randomly initialized. In order to amplify the contribution of the context-rich information, we employ soft-attention from the history interactive context to combine long-context speaker interaction influences and conversational dependence  [15] . We pool the attention vector at from the surrounding context history [C1, C2, . . . , Ct-1] using soft-attention. This contextual attention vector at can be computed as follows:\n\nαiCi.\n\n(2)",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Self-Speaker State",
      "text": "The self-influence is conditioned on how the speakers tend to maintain emotions during the conversations. This state is also known as emotional inertia, as speakers may not always express explicitly their feeling or outlook through reactions. Concretely, self-influence only involves speaker himself/herself. Self-Influence Module consists of two GRU Sλ : GRUSS and GRUSL, respectively. For For λ ∈ {S, L}, S denotes the speaker and L denotes the listener. GRU Sλ attempts to memorize the emotional inertia of P λ which represents the emotional dependency of the person with their own previous states. A dialogue involves two parties, the speaker and others belonged to the listener. For time step t, the self-speaker state of the person S λt is updated by the previous self-speaker state of the person S λ(t-1) , and the cumulative contextual vector C λt , and the utterance u λt . At time step t, the self-speaker state S λt can be computed as:",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Intra-Speaker State",
      "text": "Naturally, the intra-speaker state is easily observed, felt, and understood by the other participants. More concretely, this state is usually about the expressions, reactions, and responses  [3] . Since utterances constantly interfere with each other, we construct an attentive interactive module called Attention Interactive Dependency. For the utterance at time t the intra-speaker It is updated by the previous intra-speaker state It-1, attentive contextual vector at, and utterance ut. At time step t, the intra-speaker state It can be computed as:",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion State",
      "text": "The emotion state performs utterance's emotion and emotional category. For the utterance at time t the emotion state Et depends upon the previous emotion state Et-1 and the composite of the attentive contextual state Ct, self-speaker state St, intra-speaker It, and the utterance ut. Then the emotion state Et can be computed as:\n\n(5)",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Classification",
      "text": "The final output emotion state Et are fed into two fully-connected layers with a residual connection. To train the model, categorical cross-entropy loss with softmax activation in the last layer is used as the loss function, and L2-regularization is applied by adding a penalty in the cost function.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Database And Metrics",
      "text": "We used the multi-modal and multi-speaker conversational dataset, namely Multi-modal EmotionLines Dataset (MELD)  [4] . MELD contains acoustic, textual, and visual information for 13798 utterances and 1433 conversations from the TV series \"Friends\". There are seven emotion categories including: anger, disgust, sadness, joy, neutral, surprise and fear. The dataset is split into the training set, validation set and test set which contains 9989, 1109, and 2610 utterances, respectively  [4] . In this work, we only used acoustic modality in related experiments  [4] . Due to the natural imbalance across various emotions, we chose a weighted average F1 measure as the evaluation metric.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Baselines And State-Of-The-Art",
      "text": "Totally 6 state-of-the-art methods are compared in the experiments to verify the effectiveness of our proposed approach (Table  1 ). bc-LSTM are traditional context dependent sentiment analysis  [4] .\n\nWhile CMN  [13] , ICON  [16]  and DialogueRNN  [17]  are mainly to model speaker dynamic, M2FNet  [18]  and MMTr  [19]  are attentionbased method. The brief introductions of these 6 compared methods are presented as follows:\n\n• Bidirectional Contextual LSTM (bc-LSTM) leverages an utterance-level LSTM to learn context dependency. However, the contextual-LSTM model does not accommodate inter-speaker dependencies  [4] .\n\n• Conversational Memory Network (CMN) extracts utterance context from dialogue history information using speaker-dependent gated recurrent units. Such memories are then merged leveraging attention-based hops to capture inter-speaker dependencies  [13] .\n\n• Interactive Conversational Memory Network (ICON) extends CMN to model the self-and inter-speaker sentiment influences and store contextual summaries by using an interactive memory network  [16] .\n\n• DialogueRNN utilizes GRU to capture the participant emotional states throughout conversation and the sentence-context representation between speakers  [17] .\n\n• M2FNet employs a multi head attention-based fusion mechanism to learn emotion-rich latent information of the audio, text and visual modality  [18] .\n\n• MMTr acquires emotional cues at both levels of the speaker's self-context and contextual context and learns the information interactions between multiple modalities  [19] .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Acoustic Features",
      "text": "The audio files were resampled to 16 kHz. In order to extract the Mel-spectrogram, a linear spectrogram was first computed using the Short-Time Fourier Transform with a 25 ms Hamming window and a 10 ms overlapping. After that, a bank of 64 Mel-filters was applied in the frequency range of 125-7500 Hz. These features were then segmented into non-overlapping examples of 0.96 seconds. Especially, the shorter samples were zero-padded b efore transformation into log Mel-spectrogram. Finally, Each segment was fed into a pre-trained VGGish model obtaining a 128-dimensional embedding vector for identifying emotion.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Model Configuration",
      "text": "We implemented our proposed model using the Pytorch 1.11.0 framework. The model was trained with Adam optimizer with an initial learning rate of 1e-4 and a batch size of 32. Cross-entropy loss was utilized as the loss function. To prevent overfitting, the network was regularized by L2 norm of the model's parameters with a weight of 3e-4.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results And Discussion",
      "text": "Overall results of our proposed model and the previous state-of-theart result are presented in Table1. The performance of our method reaches F1 score of 40.9%. The proposed model is the best performance compared to the state-of-the art and baseline methods and achieves substantial improvements on Anger and Joy (Table  1 ).\n\nAblation study (see Table  2 ) verifies the importance of the interand intra-speaker state that it is better to consider among attentive context-sensitive dependency, intra-and inter-speaker influence for conversational emotion recognition than only considering context information from single speaker alone. Besides, the model's performance is worse when the attentive contextual state module is removed, which indicates that modeling of attentive long-term context dependency is more critical than the modeling of intraspeaker-sensitive interactions. Quantitatively, our method uses segment-based feature representation for utterance-level classification. Emotions are brief in duration, most lasting only up to a few seconds. Thus, a segmentwise approach is beneficial for capturing temporal information and short-term emotion interaction. In addition, the pre-trained VGGish model retains a major portion of their prior knowledge for better high-level emotional audio features extraction. In particular, a pretrained model is useful in the current situation where the dataset has limited size and is unbalanced.\n\nMoreover, our model infuses attentive contextual representation from surrounding utterance history, adding it to self-speaker state and intra-speaker influence to capture emotional dynamics on multiturn conversations. In this regard, it is more advantageous than prior approaches like bc-LSTM, which often loses the ability to determine this kind of situation. For example, character Bob suddenly shifts emotions from neutral to happy when Jill told him \"I am getting married soon\" with a rising tone shade of joy and excited. The attention mechanism is applied to amplify important global contextual conversational dependence (see Fig.  4 ). In addition, the inter-speaker interactions are either synchronous (for example, cheer after speaking good news) or asynchronous (for example, laughter after speaking something funny). Further analysis on why our method prediction is better could shed light on attentive contextual dependency and speaker-sensitive influence.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we proposed the aggregation of segment-level speech spectrogram for utterance-level emotion classification in conversations. It capitalized on inferring the contextual information that incorporates dynamic self-, intra-and inter-speaker influence. An attention-based mechanism was employed to determine the important contextual-sensitive information from surrounding utterances history. The bi-directional GRU was used to capture contextual dependency, self-and inter-speaker influence. The experiment results demonstrate the effectiveness of the proposed model.\n\nThere are some limitations to our method. The generalization, detailed ablation studies and analysis in several other benchmark datasets need to be explored further to increase dataset's scope and advance research in ERC. Additionally, it is known that text in speech, facial expressions, body movements, and other modalites all convey emotions, and hence can provide additional information. A promising further direction of research would be to apply recently advanced approaches to transfer the knowledge from multimodal systems to unimodal systems, thus improving the performance for unimodal systems.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: presents some examples demonstrating such",
      "page": 1
    },
    {
      "caption": "Figure 1: Emotion dynamic of speakers in a dialogue in comparison.",
      "page": 1
    },
    {
      "caption": "Figure 2: The aggregation of segment-level embedding extracted from",
      "page": 2
    },
    {
      "caption": "Figure 2: Finally, we concate-",
      "page": 2
    },
    {
      "caption": "Figure 3: ). The steps",
      "page": 2
    },
    {
      "caption": "Figure 3: Ct = GRUC(Ct−1, (St−1 ⊕It−1 ⊕ut))",
      "page": 2
    },
    {
      "caption": "Figure 3: The overall architecture of our proposed model for emotion",
      "page": 3
    },
    {
      "caption": "Figure 4: Attention weight visualization of our model from the cases",
      "page": 4
    },
    {
      "caption": "Figure 4: ). In addition, the inter-speaker in-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ABSTRACT": "Accurately detecting emotions\nin conversation is a necessary yet"
        },
        {
          "ABSTRACT": "challenging task due to the complexity of emotions and dynamics"
        },
        {
          "ABSTRACT": "in dialogues.\nThe emotional state of a speaker can be inﬂuenced"
        },
        {
          "ABSTRACT": "by many different\nfactors,\nsuch as\ninterlocutor\nstimulus, dialogue"
        },
        {
          "ABSTRACT": "scene, and topic.\nIn this work, we propose a conversational speech"
        },
        {
          "ABSTRACT": "emotion recognition method to deal with capturing attentive contex-"
        },
        {
          "ABSTRACT": "tual dependency and speaker-sensitive interactions. First, we use a"
        },
        {
          "ABSTRACT": "pretrained VGGish model\nto extract segment-based audio represen-"
        },
        {
          "ABSTRACT": "tation in individual utterances.\nSecond, an attentive bi-directional"
        },
        {
          "ABSTRACT": "gated recurrent unit (GRU) models contextual-sensitive information"
        },
        {
          "ABSTRACT": "and explores intra- and inter-speaker dependencies jointly in a dy-"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "namic manner.\nThe experiments conducted on the standard con-"
        },
        {
          "ABSTRACT": "versational dataset MELD demonstrate the effectiveness of the pro-"
        },
        {
          "ABSTRACT": "posed method when compared against state-of the-art methods."
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "Index Terms— contextual\ninformation,\naffective\ncomputing,"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "speaker-sensitive"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "1.\nINTRODUCTION"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "Automatic recognition of human emotions has widespread appli-"
        },
        {
          "ABSTRACT": "cations in areas such as dialogue generation, social media analysis"
        },
        {
          "ABSTRACT": "and human computer\ninteraction [1].\nSpeech is\nthe main com-"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "munication medium in which people\ncan clearly and intuitively"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "feel\nemotional\nchanges.\nUnlike\nvanilla\nemotion\nrecognition\nof"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "sentences/utterances,\nemotion recognition in conversation (ERC)"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "ideally relies on mining human emotions\nfrom conversations or"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "dialogues having two or more\ninterlocutors\nand requires\ncontext"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "modeling of the individual utterances, and requires context model-"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "ing of the individual utterances [2]. How to capture such information"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "from speech signal is a challenging task."
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "In this work, we focus on speech signals in interactive conversa-"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "tion. Speech signals naturally can carry the emotional characteristics"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "of the speaker. Conventionally, conversational emotion recognition"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "usually requires a strong ability to model context-sensitive attributes,"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "select crucial\ninformation, and capture speaker-sensitive dependen-"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "cies [3]. Among all the factors, speaker information is important for"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "tracking the emotional characteristics of conversations, especially"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "intra- and inter-speaker dependencies."
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "In interactive conversations,\nthese factors lead to diverse emo-"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "tional dynamics. Fig. 1 presents some examples demonstrating such"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "patterns from the Multi-modal EmotionLines Dataset (MELD) [4]."
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "Conversation (a) depicts\nthe presence of emotional\ninertia which"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "∗The work was done when H. Phan was at Centre for Digital Music,"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "Queen Mary University of London, UK and prior to joining Amazon."
        },
        {
          "ABSTRACT": "Thanks to the China Scholarship Council and Queen Mary University of"
        },
        {
          "ABSTRACT": "London for funding."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "power a dialogue system with ﬁne-grained temporal emotional rep-"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "resentation."
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "• We utilized the bi-directional GRU layer\nto capture context-"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "sensitive\ninformation,\nintra-\nand\ninter-speaker\ndependencies\non"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "conversational emotion recognition,\nin combination with attention"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "mechanism to highlight the important global contextual utterances."
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "• Our proposed approach was shown to be superior to state-of-"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "the-art methods for conversational emotion recognition."
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": ""
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "2. EXISTING LITERATURE"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": ""
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "Global\nand\nlocal\naudio\nfeatures\nof\nspeech\nemotion\nrecognition"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "systems are typically classiﬁed into the following four categories:"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": ""
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "prosodic features, spectral features, voice quality features, and Tea-"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": ""
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "ger Energy Operator-based features [8]. Traditionally, a number of"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": ""
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "spectral features are generally depicted using one of the cepstrum-"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": ""
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "based representations available. Commonly, Mel-frequency cepstral"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": ""
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "coefﬁcients (MFCC) or Mel-scale spectrograms were used, and in"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "some studies, formants, and other information were utilized as well"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": ""
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "[8].\nSuraj et al.\ndemonstrated the effectiveness of convolutional"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "neural networks in emotion classiﬁcation with MFCCs [5]. Besides,"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "the direct use of Mel-scale spectrograms for ERC was proved suc-"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "cessful as well [9]. In this work, we use a pretrained model to extract"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "high-level acoustic features from low-level Mel-scale spectrograms"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "for emotion recognition."
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": ""
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "ERC requires deep understanding of human interactions in con-"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "versations [10∼13].\nSome of\nthe important works attributes emo-"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "tional dynamics to be interactive phenomena [13,16], rather than be-"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "ing within-person. We utilize this trait in the design of our model that"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "incorporates inter-speaker dynamic in a conversation. Since conver-"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "sations have a natural\ntemporal nature, context also play a crucial"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "role in emotion analysis [4,17]. Poria et al. employed a bi-directional"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "LSTM to capture temporal context information from surrounding ut-"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "terances of the same speaker to infer emotions [4]. However,\nthere"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "is no provision to model context and speaker interactive inﬂuences."
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "It should be noted that our method is different\nfrom previous"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "strategies for ERC. We propose to capture this contextual-sensitive"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": ""
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "information via hierarchical\nrecurrent networks. Additionally, our"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "proposed approach adopts an interactive scheme that actively models"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": ""
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "intra- and inter-speaker emotional dynamics in conversations."
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": ""
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": ""
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "3. METHODOLOGY"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": ""
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "Our proposed approach consists of\nthree major modules:\n1) pre-"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "trained segment-level audio representation, 2) grouped parallel sta-"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "tistical operation, referred to as statistical unit (SU), and 3) a dialog-"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "aware interaction module that aims to model\nthe interactions in the"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": ""
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "dialogue and then makes the emotional state prediction."
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": ""
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": ""
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "3.1. Pre-Trained Audio Embeddings"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": ""
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": ""
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "On the audio end, an audio segment of 0.96s is converted into log"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": ""
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": ""
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "Mel-spectrogram and encoded using a 128-dimensional feature vec-"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "tor extracted from the last\nfully connected layer of a pre-trained"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "VGGish model. Especially,\nthe shorter samples were zero-padded"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": ""
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "before transformation into log Mel-spectrogram. Non-overlapping"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "segments are used during segmentation. The model was trained on"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "AudioSet, consisting of 100 million YouTube videos [14]. For each"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "utterance, a sequence of L embedding vectors was produced, where"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "L represents the number of audio segments that\nthe audio signal\nis"
        },
        {
          "• A segment-level\nfeature extraction strategy was able to em-": "partitioned into (see Fig. 2)."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3.3.5. Classiﬁcation": "The ﬁnal output emotion state Et are fed into two fully-connected"
        },
        {
          "3.3.5. Classiﬁcation": "layers with a residual connection.\nTo train the model, categorical"
        },
        {
          "3.3.5. Classiﬁcation": "cross-entropy loss with softmax activation in the last\nlayer\nis used"
        },
        {
          "3.3.5. Classiﬁcation": "as the loss function, and L2-regularization is applied by adding a"
        },
        {
          "3.3.5. Classiﬁcation": "penalty in the cost function."
        },
        {
          "3.3.5. Classiﬁcation": "4. EXPERIMENTS"
        },
        {
          "3.3.5. Classiﬁcation": "4.1. Database and Metrics"
        },
        {
          "3.3.5. Classiﬁcation": "We used the multi-modal and multi-speaker conversational dataset,"
        },
        {
          "3.3.5. Classiﬁcation": "namely Multi-modal EmotionLines Dataset\n(MELD)[4].\nMELD"
        },
        {
          "3.3.5. Classiﬁcation": "contains acoustic,\ntextual, and visual\ninformation for 13798 utter-"
        },
        {
          "3.3.5. Classiﬁcation": "ances and 1433 conversations from the TV series “Friends”. There"
        },
        {
          "3.3.5. Classiﬁcation": "are\nseven emotion categories\nincluding:\nanger,\ndisgust,\nsadness,"
        },
        {
          "3.3.5. Classiﬁcation": "joy, neutral, surprise and fear. The dataset\nis split\ninto the training"
        },
        {
          "3.3.5. Classiﬁcation": "set, validation set and test set which contains 9989, 1109, and 2610"
        },
        {
          "3.3.5. Classiﬁcation": "utterances,\nrespectively [4].\nIn this work, we only used acoustic"
        },
        {
          "3.3.5. Classiﬁcation": "modality in related experiments [4]. Due to the natural\nimbalance"
        },
        {
          "3.3.5. Classiﬁcation": "across various emotions, we chose a weighted average F1 measure"
        },
        {
          "3.3.5. Classiﬁcation": "as the evaluation metric."
        },
        {
          "3.3.5. Classiﬁcation": ""
        },
        {
          "3.3.5. Classiﬁcation": "4.2. Baselines and State-of-the-Art"
        },
        {
          "3.3.5. Classiﬁcation": ""
        },
        {
          "3.3.5. Classiﬁcation": "Totally 6 state-of-the-art methods are compared in the experiments"
        },
        {
          "3.3.5. Classiﬁcation": "to\nverify\nthe\neffectiveness\nof\nour\nproposed\napproach\n(Table\n1)."
        },
        {
          "3.3.5. Classiﬁcation": "bc-LSTM are traditional context dependent sentiment analysis [4]."
        },
        {
          "3.3.5. Classiﬁcation": "While CMN [13], ICON[16] and DialogueRNN [17] are mainly to"
        },
        {
          "3.3.5. Classiﬁcation": "model speaker dynamic, M2FNet [18] and MMTr [19] are attention-"
        },
        {
          "3.3.5. Classiﬁcation": "based method. The brief introductions of these 6 compared methods"
        },
        {
          "3.3.5. Classiﬁcation": "are presented as follows:"
        },
        {
          "3.3.5. Classiﬁcation": "• Bidirectional Contextual LSTM (bc-LSTM)\nleverages\nan"
        },
        {
          "3.3.5. Classiﬁcation": "utterance-level LSTM to learn context dependency. However,\nthe"
        },
        {
          "3.3.5. Classiﬁcation": "contextual-LSTM model does not accommodate inter-speaker de-"
        },
        {
          "3.3.5. Classiﬁcation": "pendencies [4]."
        },
        {
          "3.3.5. Classiﬁcation": "• Conversational Memory Network (CMN) extracts utterance"
        },
        {
          "3.3.5. Classiﬁcation": "context from dialogue history information using speaker-dependent"
        },
        {
          "3.3.5. Classiﬁcation": "gated recurrent units.\nSuch memories are then merged leveraging"
        },
        {
          "3.3.5. Classiﬁcation": "attention-based hops to capture inter-speaker dependencies [13]."
        },
        {
          "3.3.5. Classiﬁcation": "• Interactive Conversational Memory Network (ICON) extends"
        },
        {
          "3.3.5. Classiﬁcation": ""
        },
        {
          "3.3.5. Classiﬁcation": "CMN to model\nthe self- and inter-speaker sentiment\ninﬂuences and"
        },
        {
          "3.3.5. Classiﬁcation": "store contextual summaries by using an interactive memory network"
        },
        {
          "3.3.5. Classiﬁcation": "[16]."
        },
        {
          "3.3.5. Classiﬁcation": "• DialogueRNN utilizes GRU to capture the participant emo-"
        },
        {
          "3.3.5. Classiﬁcation": "tional states throughout conversation and the sentence-context rep-"
        },
        {
          "3.3.5. Classiﬁcation": "resentation between speakers [17]."
        },
        {
          "3.3.5. Classiﬁcation": "• M2FNet employs a multi head attention-based fusion mecha-"
        },
        {
          "3.3.5. Classiﬁcation": "nism to learn emotion-rich latent\ninformation of the audio,\ntext and"
        },
        {
          "3.3.5. Classiﬁcation": "visual modality [18]."
        },
        {
          "3.3.5. Classiﬁcation": "• MMTr acquires emotional cues at both levels of the speaker’s"
        },
        {
          "3.3.5. Classiﬁcation": "self-context and contextual context and learns the information inter-"
        },
        {
          "3.3.5. Classiﬁcation": "actions between multiple modalities [19]."
        },
        {
          "3.3.5. Classiﬁcation": ""
        },
        {
          "3.3.5. Classiﬁcation": "4.3. Acoustic Features"
        },
        {
          "3.3.5. Classiﬁcation": ""
        },
        {
          "3.3.5. Classiﬁcation": "The audio ﬁles were resampled to 16 kHz.\nIn order\nto extract\nthe"
        },
        {
          "3.3.5. Classiﬁcation": "Mel-spectrogram, a linear spectrogram was ﬁrst computed using the"
        },
        {
          "3.3.5. Classiﬁcation": "Short-Time Fourier Transform with a 25 ms Hamming window and a"
        },
        {
          "3.3.5. Classiﬁcation": "10 ms overlapping. After that, a bank of 64 Mel-ﬁlters was applied in"
        },
        {
          "3.3.5. Classiﬁcation": "the frequency range of 125-7500 Hz. These features were then seg-"
        },
        {
          "3.3.5. Classiﬁcation": "mented into non-overlapping examples of 0.96 seconds. Especially,"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: ) verifies the importance of the inter- somethingfunny). Furtheranalysisonwhyourmethodprediction",
      "data": [
        {
          "Table 1: Performance comparison with the state-of-the-art and baselines on MELD": "Anger"
        },
        {
          "Table 1: Performance comparison with the state-of-the-art and baselines on MELD": "21.9"
        },
        {
          "Table 1: Performance comparison with the state-of-the-art and baselines on MELD": "29.6"
        },
        {
          "Table 1: Performance comparison with the state-of-the-art and baselines on MELD": "31.5"
        },
        {
          "Table 1: Performance comparison with the state-of-the-art and baselines on MELD": "32.1"
        },
        {
          "Table 1: Performance comparison with the state-of-the-art and baselines on MELD": "-"
        },
        {
          "Table 1: Performance comparison with the state-of-the-art and baselines on MELD": "-"
        },
        {
          "Table 1: Performance comparison with the state-of-the-art and baselines on MELD": "32.6"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: ) verifies the importance of the inter- somethingfunny). Furtheranalysisonwhyourmethodprediction",
      "data": [
        {
          "MMTr\n-\n-\n-\n-": "Proposed method\n32.6\n0\n0\n25.21",
          "-\n-\n-\n38.8": "63.85\n6.28\n14.58\n40.9"
        },
        {
          "MMTr\n-\n-\n-\n-": "the shorter samples were zero-padded b efore transformation into log",
          "-\n-\n-\n38.8": ""
        },
        {
          "MMTr\n-\n-\n-\n-": "Mel-spectrogram. Finally, Each segment was fed into a pre-trained",
          "-\n-\n-\n38.8": ""
        },
        {
          "MMTr\n-\n-\n-\n-": "VGGish model obtaining a 128-dimensional embedding vector\nfor",
          "-\n-\n-\n38.8": ""
        },
        {
          "MMTr\n-\n-\n-\n-": "identifying emotion.",
          "-\n-\n-\n38.8": ""
        },
        {
          "MMTr\n-\n-\n-\n-": "4.4. Model Conﬁguration",
          "-\n-\n-\n38.8": ""
        },
        {
          "MMTr\n-\n-\n-\n-": "",
          "-\n-\n-\n38.8": "Fig. 4. Attention weight visualization of our model from the cases"
        },
        {
          "MMTr\n-\n-\n-\n-": "We\nimplemented\nour\nproposed model\nusing\nthe Pytorch\n1.11.0",
          "-\n-\n-\n38.8": ""
        },
        {
          "MMTr\n-\n-\n-\n-": "",
          "-\n-\n-\n38.8": "in MELD."
        },
        {
          "MMTr\n-\n-\n-\n-": "framework.\nThe model was trained with Adam optimizer with an",
          "-\n-\n-\n38.8": ""
        },
        {
          "MMTr\n-\n-\n-\n-": "initial\nlearning rate of 1e-4 and a batch size of 32. Cross-entropy",
          "-\n-\n-\n38.8": ""
        },
        {
          "MMTr\n-\n-\n-\n-": "loss was utilized as\nthe loss\nfunction.\nTo prevent overﬁtting,\nthe",
          "-\n-\n-\n38.8": ""
        },
        {
          "MMTr\n-\n-\n-\n-": "network was regularized by L2 norm of the model’s parameters with",
          "-\n-\n-\n38.8": ""
        },
        {
          "MMTr\n-\n-\n-\n-": "",
          "-\n-\n-\n38.8": "and intra-speaker inﬂuence to capture emotional dynamics on multi-"
        },
        {
          "MMTr\n-\n-\n-\n-": "a weight of 3e-4.",
          "-\n-\n-\n38.8": ""
        },
        {
          "MMTr\n-\n-\n-\n-": "",
          "-\n-\n-\n38.8": "turn conversations. In this regard, it is more advantageous than prior"
        },
        {
          "MMTr\n-\n-\n-\n-": "",
          "-\n-\n-\n38.8": "approaches like bc-LSTM, which often loses the ability to determine"
        },
        {
          "MMTr\n-\n-\n-\n-": "5. RESULTS AND DISCUSSION",
          "-\n-\n-\n38.8": "this kind of situation.\nFor example, character Bob suddenly shifts"
        },
        {
          "MMTr\n-\n-\n-\n-": "",
          "-\n-\n-\n38.8": "emotions from neutral to happy when Jill told him “I am getting mar-"
        },
        {
          "MMTr\n-\n-\n-\n-": "Overall results of our proposed model and the previous state-of-the-",
          "-\n-\n-\n38.8": "ried soon” with a rising tone shade of joy and excited. The attention"
        },
        {
          "MMTr\n-\n-\n-\n-": "art result are presented in Table1. The performance of our method",
          "-\n-\n-\n38.8": "mechanism is applied to amplify important global contextual conver-"
        },
        {
          "MMTr\n-\n-\n-\n-": "reaches F1 score of 40.9%.\nThe proposed model\nis the best per-",
          "-\n-\n-\n38.8": "sational dependence (see Fig. 4).\nIn addition,\nthe inter-speaker in-"
        },
        {
          "MMTr\n-\n-\n-\n-": "formance\ncompared to the\nstate-of-the\nart\nand baseline methods",
          "-\n-\n-\n38.8": "teractions are either synchronous (for example, cheer after speaking"
        },
        {
          "MMTr\n-\n-\n-\n-": "and achieves substantial improvements on Anger and Joy (Table 1).",
          "-\n-\n-\n38.8": "good news) or asynchronous (for example,\nlaughter after speaking"
        },
        {
          "MMTr\n-\n-\n-\n-": "Ablation study (see Table 2) veriﬁes\nthe importance of\nthe inter-",
          "-\n-\n-\n38.8": "something funny). Further analysis on why our method prediction"
        },
        {
          "MMTr\n-\n-\n-\n-": "and intra-speaker\nstate\nthat\nit\nis better\nto consider\namong atten-",
          "-\n-\n-\n38.8": "is better could shed light on attentive contextual dependency and"
        },
        {
          "MMTr\n-\n-\n-\n-": "tive context-sensitive dependency, intra- and inter-speaker inﬂuence",
          "-\n-\n-\n38.8": "speaker-sensitive inﬂuence."
        },
        {
          "MMTr\n-\n-\n-\n-": "for conversational emotion recognition than only considering con-",
          "-\n-\n-\n38.8": ""
        },
        {
          "MMTr\n-\n-\n-\n-": "text\ninformation from single speaker alone.\nBesides,\nthe model’s",
          "-\n-\n-\n38.8": ""
        },
        {
          "MMTr\n-\n-\n-\n-": "performance is worse when the attentive contextual\nstate module",
          "-\n-\n-\n38.8": ""
        },
        {
          "MMTr\n-\n-\n-\n-": "is\nremoved, which indicates\nthat modeling of attentive long-term",
          "-\n-\n-\n38.8": ""
        },
        {
          "MMTr\n-\n-\n-\n-": "context dependency is more\ncritical\nthan the modeling of\nintra-",
          "-\n-\n-\n38.8": "6. CONCLUSION"
        },
        {
          "MMTr\n-\n-\n-\n-": "speaker-sensitive interactions.",
          "-\n-\n-\n38.8": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: ) verifies the importance of the inter- somethingfunny). Furtheranalysisonwhyourmethodprediction",
      "data": [
        {
          "speaker-sensitive interactions.": ""
        },
        {
          "speaker-sensitive interactions.": "Table 2:Albation study on the MELD dataset"
        },
        {
          "speaker-sensitive interactions.": ""
        },
        {
          "speaker-sensitive interactions.": ""
        },
        {
          "speaker-sensitive interactions.": "Method"
        },
        {
          "speaker-sensitive interactions.": ""
        },
        {
          "speaker-sensitive interactions.": "w/o pretained VGGish"
        },
        {
          "speaker-sensitive interactions.": ""
        },
        {
          "speaker-sensitive interactions.": "w/o attentive contextual state"
        },
        {
          "speaker-sensitive interactions.": ""
        },
        {
          "speaker-sensitive interactions.": "w/o self-speaker state"
        },
        {
          "speaker-sensitive interactions.": ""
        },
        {
          "speaker-sensitive interactions.": "w/o intra-speaker state"
        },
        {
          "speaker-sensitive interactions.": ""
        },
        {
          "speaker-sensitive interactions.": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "7. REFERENCES": "",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": "Set: An ontology and human-labeled dataset for audio events,”"
        },
        {
          "7. REFERENCES": "[1] Wang W, Song W, Tao A.\n“A systematic\nreview on affec-",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": "IEEE International Conference on Acoustics, 2017, 776-780."
        },
        {
          "7. REFERENCES": "tive computing:\nemotion models, databases,\nand recent ad-",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": "[15] Xia X,\nJiang D, Sahli H.\n“Learning Salient Segments\nfor"
        },
        {
          "7. REFERENCES": "vances,”Information Fusion, 2022, 83-84:19-52.",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": "Speech Emotion Recognition Using Attentive Temporal Pool-"
        },
        {
          "7. REFERENCES": "[2] Aneesh M, Martin R.\n“Speech Emotion Recognition Using",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": "ing,” IEEE Access, 2020, 8:151740-151752."
        },
        {
          "7. REFERENCES": "Quaternion Convolutional Neural Networks,”IEEE Interna-",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": "[16] Hazarika D, Poria S, Mihalcea R, Cambria E, Zimmermann R."
        },
        {
          "7. REFERENCES": "tional Conference on Acoustics, Speech and Signal Processing",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": "“ICON: Interactive Conversational Memory Network for Mul-"
        },
        {
          "7. REFERENCES": "(ICASSP), 2021, 6309-6313.",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": "the 2018 Confer-\ntimodal Emotion Detection,”Proceedings of"
        },
        {
          "7. REFERENCES": "[3]\nSun L, Liu B, Tao JH, Zheng L.“Multimodal Cross- and Self-",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": "ence on Empirical Methods in Natural Language Processing,"
        },
        {
          "7. REFERENCES": "Attention Network for Speech Emotion Recognition,” IEEE In-",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": "2018, 2594-2604."
        },
        {
          "7. REFERENCES": "ternational Conference on Acoustics, Speech and Signal Pro-",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": "[17] Majumder N, Poria S, Hazarika D, Mihalcea R, Cambria E."
        },
        {
          "7. REFERENCES": "cessing (ICASSP), 2022, 4275-4279.",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": "“DialogueRNN: An Attentive RNN for Emotion Detection in"
        },
        {
          "7. REFERENCES": "[4]\nPoria S, Hazarika D, Majumder N, Naik G, Cambria E, Mihal-",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": "the Thirty-Third AAAI Confer-\nConversations,”Proceedings of"
        },
        {
          "7. REFERENCES": "cea R. “MELD: A Multimodal Multi-Party Dataset\nfor Emo-",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": "ence on Artiﬁcial Intelligence, 2019, 6818-6825."
        },
        {
          "7. REFERENCES": "the 57th\ntion Recognition in Conversations,”Proceedings of",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": "[18] Vishal C, Purbayan K, Ashish G, Nirmesh S, Pankaj W,"
        },
        {
          "7. REFERENCES": "Conference of\nthe Association for Computational Linguistics,",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": "Naoyuki O.”M2FNet: Multi-modal Fusion Network for Emo-"
        },
        {
          "7. REFERENCES": "2019, 527-536.",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": "tion Recognition in Conversation”, Conference on Computer"
        },
        {
          "7. REFERENCES": "",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": "Vision and Pattern Recognition Workshops\n(CVPRW), 2022,"
        },
        {
          "7. REFERENCES": "[5]\nJun-Heng T-L, Ching, Yaoi, Yu-Te. “Segment-based emotion",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": "4652-4661."
        },
        {
          "7. REFERENCES": "recognition from continuous Mandarin Chinese speech,” Com-",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "puters in Human Behavior, 2011, 27: 1545-1552.",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": "[19] Zou SH, Huang XY, Shen XD, Liu HK.\n”Improving mul-"
        },
        {
          "7. REFERENCES": "",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": "timodal\nfusion with Main Modal\nTransformer\nfor\nemo-"
        },
        {
          "7. REFERENCES": "[6] Mao S, Ching PC. “Deep Learning of Segment-Level Feature",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": "tion recognition in conversation”, Knowledge-Based Systems,"
        },
        {
          "7. REFERENCES": "Representation with Multiple Instance Learning for Utterance-",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": "2022, 258, 1-9."
        },
        {
          "7. REFERENCES": "Level Speech Emotion Recognition,” Interspeech, 2019, 1686-",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "1690.",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "[7] Ghosal D, Majumder N, Gelbukh A, Mihalcea R, Poria S.",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "“COSMIC: CommonSense knowledge for eMotion Identiﬁca-",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "tion in Conversations,” Findings of the Association for Compu-",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "tational Linguistics (EMNLP), 2020.",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "[8] Akc¸ay MB, O˘guz K. “Speech emotion recognition: Emotional",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "models, databases,\nfeatures, preprocessing methods, support-",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "ing modalities, and classiﬁers,” Speech Communication, 2020,",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "116:56-76.",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "[9] Neil S, Mikolaj K, Pierre B, MilosC.\n“SERAB: A Multi-",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "Lingual Benchmark for Speech Emotion Recognition,” IEEE",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "International Conference on Acoustics, Speech and Signal Pro-",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "cessing (ICASSP), 2022, 7697-7701.",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "[10]\nSidorov M, Minker W. “Emotion Recognition and Depression",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "Diagnosis by Acoustic and Visual Features,” Proceedings of",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "the 4th International Workshop on Audio/Visual Emotion Chal-",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "lenge, 2014, 81-86.",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "[11]\nPoria S, Cambria E, Hazarika D, Majumder N, Zadeh A,",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "Morency\nL-P.\n“Context-Dependent\nSentiment Analysis\nin",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "of\nthe\n55th Annual\nUser-Generated Videos,” Proceedings",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "Meeting\nof\nthe Association\nfor Computational Linguistics,",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "2017, 873-883.",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "[12] Hazarika D, Poria S, Zadeh A, Cambria E, Morency LP, Zim-",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "mermann R. “Conversational Memory Network for Emotion",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "the\nRecognition in Dyadic Dialogue Videos,” Proceedings of",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "2018 Conference of the North American Chapter of the Associ-",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "ation for Computational Linguistics: Human Language Tech-",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "nologies, 2018, 2122-2132.",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "[13] Ghosal D, Majumder N, Poria S, Chhaya N, Gelbukh A. “Di-",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "alogueGCN A Graph Convolutional Neural Network for Emo-",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "tion Recognition in Conversation,” Processing and the 9th In-",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "ternational Joint Conference on Natural Language Processing,",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        },
        {
          "7. REFERENCES": "2019, 154-164.",
          "[14] Gemmeke JF, Ellis D, Freedman D, Jansen A, Ritter M. “Audio": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "A systematic review on affective computing: emotion models, databases, and recent advances",
      "authors": [
        "W Wang",
        "W Song",
        "A Tao"
      ],
      "year": "2022",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "3",
      "title": "Speech Emotion Recognition Using Quaternion Convolutional Neural Networks",
      "authors": [
        "M Aneesh",
        "R Martin"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "4",
      "title": "Multimodal Cross-and Self-Attention Network for Speech Emotion Recognition",
      "authors": [
        "L Sun",
        "B Liu",
        "J Tao",
        "L Zheng"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "5",
      "title": "MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Conference"
    },
    {
      "citation_id": "6",
      "title": "Segment-based emotion recognition from continuous Mandarin Chinese speech",
      "authors": [
        "Jun-Heng T-L Ching",
        "Yu-Te Yaoi"
      ],
      "year": "2011",
      "venue": "Computers in Human Behavior"
    },
    {
      "citation_id": "7",
      "title": "Deep Learning of Segment-Level Feature Representation with Multiple Instance Learning for Utterance-Level Speech Emotion Recognition",
      "authors": [
        "S Mao",
        "P Ching"
      ],
      "year": "2019",
      "venue": "Deep Learning of Segment-Level Feature Representation with Multiple Instance Learning for Utterance-Level Speech Emotion Recognition"
    },
    {
      "citation_id": "8",
      "title": "COSMIC: CommonSense knowledge for eMotion Identification in Conversations",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "A Gelbukh",
        "R Mihalcea",
        "S Poria"
      ],
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics (EMNLP)"
    },
    {
      "citation_id": "9",
      "title": "Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers",
      "authors": [
        "M Akc ¸ay",
        "K Oguz"
      ],
      "year": "2020",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "10",
      "title": "SERAB: A Multi-Lingual Benchmark for Speech Emotion Recognition",
      "authors": [
        "S Neil",
        "K Mikolaj",
        "B Pierre",
        "Milosc"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "11",
      "title": "Emotion Recognition and Depression Diagnosis by Acoustic and Visual Features",
      "authors": [
        "M Sidorov",
        "W Minker"
      ],
      "year": "2014",
      "venue": "Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "12",
      "title": "Context-Dependent Sentiment Analysis in User-Generated Videos",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "N Majumder",
        "A Zadeh",
        "L-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "13",
      "title": "Conversational Memory Network for Emotion Recognition in Dyadic Dialogue Videos",
      "authors": [
        "D Hazarika",
        "S Poria",
        "A Zadeh",
        "E Cambria",
        "L Morency",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "14",
      "title": "Di-alogueGCN A Graph Convolutional Neural Network for Emotion Recognition in Conversation",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya",
        "A Gelbukh"
      ],
      "year": "2019",
      "venue": "Processing and the 9th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "15",
      "title": "Audio Set: An ontology and human-labeled dataset for audio events",
      "authors": [
        "J Gemmeke",
        "D Ellis",
        "D Freedman",
        "A Jansen",
        "M Ritter"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Acoustics"
    },
    {
      "citation_id": "16",
      "title": "Learning Salient Segments for Speech Emotion Recognition Using Attentive Temporal Pooling",
      "authors": [
        "X Xia",
        "D Jiang",
        "H Sahli"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "17",
      "title": "ICON: Interactive Conversational Memory Network for Multimodal Emotion Detection",
      "authors": [
        "D Hazarika",
        "S Poria",
        "R Mihalcea",
        "E Cambria",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "18",
      "title": "DialogueRNN: An Attentive RNN for Emotion Detection in Conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "19",
      "title": "M2FNet: Multi-modal Fusion Network for Emotion Recognition in Conversation",
      "authors": [
        "C Vishal",
        "K Purbayan",
        "G Ashish",
        "S Nirmesh",
        "W Pankaj",
        "O Naoyuki"
      ],
      "year": "2022",
      "venue": "Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "20",
      "title": "Improving multimodal fusion with Main Modal Transformer for emotion recognition in conversation",
      "authors": [
        "S Zou",
        "X Huang",
        "X Shen",
        "H Liu"
      ],
      "year": "2022",
      "venue": "Knowledge-Based Systems"
    }
  ]
}