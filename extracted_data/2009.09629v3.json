{
  "paper_id": "2009.09629v3",
  "title": "Modality-Transferable Emotion Embeddings For Low-Resource Multimodal Emotion Recognition",
  "published": "2020-09-21T06:10:39Z",
  "authors": [
    "Wenliang Dai",
    "Zihan Liu",
    "Tiezheng Yu",
    "Pascale Fung"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Despite the recent achievements made in the multi-modal emotion recognition task, two problems still exist and have not been well investigated: 1) the relationship between different emotion categories are not utilized, which leads to sub-optimal performance; and 2) current models fail to cope well with low-resource emotions, especially for unseen emotions. In this paper, we propose a modality-transferable model with emotion embeddings to tackle the aforementioned issues. We use pre-trained word embeddings to represent emotion categories for textual data. Then, two mapping functions are learned to transfer these embeddings into visual and acoustic spaces. For each modality, the model calculates the representation distance between the input sequence and target emotions and makes predictions based on the distances. By doing so, our model can directly adapt to the unseen emotions in any modality since we have their pre-trained embeddings and modality mapping functions. Experiments show that our model achieves stateof-the-art performance on most of the emotion categories. Besides, our model also outperforms existing baselines in the zero-shot and few-shot scenarios for unseen emotions 1 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Multi-modal emotion recognition is an increasingly popular but challenging task. One main challenge is that labelled data is difficult to come by as humans find it time-consuming to discern emotion categories from either speech or video. Indeed we humans express emotions through a combination of modalities, including the way we speak, the words we use, facial expressions and sometimes gestures. It is also much more comfortable for humans to understand each other's emotions when they can both  hear and see the other person. It follows that multimodal emotion recognition can, therefore, yield more reliable results than restricting machines to a single modality.\n\nIn the past few years, much research has been done to better understand intra-modality and intermodality dynamics, and modality fusion is a widely studied approach. For example,  Zadeh et al. (2017)  proposed a tensor fusion network that combines three modalities from vectors to a tensor using the Cartesian product. In addition, the attention mechanism is commonly used to do modality fusion  (Zadeh et al., 2018a; Wang et al., 2018; Liang et al., 2018; Hazarika et al., 2018; Pham et al., 2018; Tsai et al., 2019a) . Although significant improvements have been made on the multi-modal emotion recognition task, however, the relationship between emotions has not been well modelled, which can lead to sub-optimal performance. Also, the problem of low-resource multi-modal emotion recognition is not adequately studied. Multi-modal emotion recognition data is hard to collect and annotate, especially for low-resource emotions (e.g., surprise) that are rarely seen in daily life, which motivates us to investigate this problem.\n\nIn this paper, we propose a modality-transferable network with cross-modality emotion embeddings to model the relationship between emotions. Given that emotion embeddings contain semantic information and emotion relations in the vector space, we use them to represent emotion categories and measure the similarity of the representations between the input sentence and target emotions to make predictions. Concretely, for the textual modality, we use the pre-trained GloVe  (Pennington et al., 2014)  embeddings of emotion words as the emotion embeddings. As there are no pre-trained emotion embeddings for the visual and acoustic modalities, the model learns two mapping functions, f t→v and f t→a , to transfer the emotion embeddings from the textual space to the visual and acoustic spaces (Figure  1 ). Therefore, for each modality, there will be a dedicated set of emotion embeddings. The distances computed in all modalities will be finally fused, and the model will make predictions based on that.\n\nBenefiting from this prediction mechanism, our model can easily carry out zero-shot learning (ZSL) to identify unseen emotion categories using the embeddings from unseen emotions. The intuition behind it is that the pre-trained and projected emotion embeddings form a semantic knowledge space, which is shared by both the seen and unseen classes. Furthermore, with the help of embedding mapping functions, the model can also perform ZSL on a single modality during inference time. When a few samples from unseen emotions are available, our model can adapt to new emotions without forgetting the previous emotions by using joint training and continual learning  (Lopez-Paz and Ranzato, 2017) .\n\nOur contributions in this work are three-fold:\n\n• We introduce a simple but effective end-toend model for the multi-modal emotion recognition task. It learns the relationship of different emotion categories using emotion embeddings.\n\n• To the best of our knowledge, this paper is the first to investigate multi-modal emotion recognition in the low-resource scenario. Our model can directly adapt to an unseen emotion, even if only one modality is available.\n\n• Experimental results show that our model achieves state-of-the-art results on most emotion categories. We also provide a thorough analysis of zero-shot and few-shot learning.\n\n2 Related Works",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multi-Modal Emotion Recognition",
      "text": "Since the early 2010s, multi-modal emotion recognition has drawn more and more attention with the rise of deep learning and its advances in computer vision and natural language processing  (Baltrušaitis et al., 2018) .  Schuller et al. (2011)  proposed the first Audio-Visual Emotion Challenge and Workshop (AVEC), which focused on multimodal emotion analysis for health. In recent years, most achievements in this area aimed to find a better modality fusion method.  Zadeh et al. (2017)  introduced a tensor fusion network that combined data representation from each modality to a tensor by performing the Cartesian product. In addition, the attention mechanism  (Bahdanau et al., 2015)  has been widely applied to do modality fusion and emphasis  (Zadeh et al., 2018a; Pham et al., 2018; Tsai et al., 2019a) . Furthermore,  Liu et al. (2018)  proposed a low-rank architecture to decrease the problem complexity, and  Tsai et al. (2019b)  introduced a modality re-construction method to generate occasional missing data in a modality. Although prior works have made progress on this task, the relationship between emotion categories has not been well modelled in previous works, except by  Xu et al. (2020) , who captured emotion correlations using graph networks for emotion recognition. However, the model is only based on a single textual modality. Additionally, the previous studies have not put much effort toward unseen and lowresource emotion categories, which is a problem of multi-modal emotion data by nature.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Modality Mapping Modality Fusion",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Loss",
      "text": "...",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Lstm T Lstm T Lstm T Lstm T Lstm A Lstm A Lstm A",
      "text": "Figure  2 : The architecture of our proposed multi-modal emotion recognition model. It consists of three LSTM networks, one emotion embedding mapping module, and one modality fusion module. For each modality, the input is a sequence of length T . Each modality has a set of emotion embeddings by mapping the GloVe textual emotion embeddings to the other modalities using f t→v and f t→a . The whole architecture is optimized end-to-end.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Zero/Few-Shot And Continual Learning",
      "text": "Zero-shot and few-shot learning methods, which address the data scarcity scenario, have been applied to many popular machine learning tasks where zero or only a few training samples are available for the target tasks or domains, such as machine translation  (Johnson et al., 2017; Gu et al., 2018) , dialogue generation  (Zhao and Eskenazi, 2018; Madotto et al., 2019) , dialogue state tracking  (Liu et al., 2019c; Wu et al., 2019) , slot filling  (Bapna et al., 2017; Liu et al., 2019b Liu et al., , 2020)) , and accented speech recognition  (Winata et al., 2020) . They have also been adopted in multiple cross-lingual tasks, such as named entity recognition  (Xie et al., 2018; Ni et al., 2017) , part-ofspeech tagging  (Wisniewski et al., 2014; Huck et al., 2019) , and question answering  (Liu et al., 2019a; Lewis et al., 2019) . Recently, several methods have been proposed for continual learning  (Rusu et al., 2016; Kirkpatrick et al., 2017; Lopez-Paz and Ranzato, 2017; Fernando et al., 2017; Lee et al., 2017) , and these were applied to some NLP tasks, such as opinion mining  (Shu et al., 2016) , document classification  (Shu et al., 2017) , and dialogue state tracking  (Wu et al., 2019) .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Methodology",
      "text": "As shown in Figure  2 , our model consists of three parts: intra-modal encoder networks, emotion em-bedding mapping modules, and an inter-modal fusion module. In this section, we first define the problem, and then we introduce the details of our model.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Problem Definition",
      "text": "We define the input multi-modal data samples as\n\n, in which I denotes the total number of samples, and t, a, v denote the textual, acoustic, and visual modalities, respectively. For each modality, there is a set of emotion embeddings that represent the semantic meanings for the emotion categories to be recognized. In the textual modality, we have E t = {e t k } K k=1 , which is from the pre-trained GloVe embeddings. In acoustic and visual modalities, we have\n\n, which are mapped from E t by the mapping function f t→v and f t→a . K denotes the number of emotion categories, and it can be changed to fit different tasks and zero-shot learning. Y = {y i } I i=1 denotes the annotations for multilabel emotion recognition, where y i is a vector of length K with binary values.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Intra-Modality Encoder Networks",
      "text": "As shown in Figure  2 , for each data sample, there are three sequences of length T from the three modalities. For each modality, we use a bi-directional Long-Short Term Memory (LSTM)  (Hochreiter and Schmidhuber, 1997)  net-work as the encoder to process the sequence and get a vector representation. In other words, for the i th data sample, we will have three vectors, r\n\nthat represent the textual, acoustic, and visual modalities. Here, d t , d a , and d v are the dimensions of the emotion embeddings of the textual, acoustic, and visual modalities, respectively.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Modality Mapping Module",
      "text": "As mentioned in Section 1, previous works do not consider the connections in different emotion categories, and the only information about emotions is in the annotations. In our model, we use emotion word embeddings to inject the semantic information of emotions into the model. Additionally, emotion embeddings also contain the relationships between emotion categories. For the textual modality, we use pre-trained GloVe  (Pennington et al., 2014)  embeddings of K emotion words, denoted as E t ∈ R K×dt . For the other two modalities, because there are no off-the-shelf pre-trained emotion embeddings, our model learns two mapping functions which project the vectors from the textual space into the acoustic and visual spaces:\n\n(1)\n\n(2)",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Modality Fusion And Prediction",
      "text": "To predict the emotions for input sentences, we calculate the similarity scores between the sequence representation and the emotion embeddings for each modality. As shown in Eq.3, for a data sample i, every modality will have a vector of similarity scores of length K by dot product attention. We further add a modality fusion module to weighted sum all the vectors, in which the weights are also optimized end-to-end (Eq.4). Finally, as the datasets are multi-labelled, the sigmoid activation function is applied to each score in the fused vector s (i) , and a threshold is used to decide whether an emotion exists or not.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Unseen Emotion Prediction",
      "text": "Collecting numerous training samples for a new emotion, especially for a low-resource emotion, is expensive and time-consuming. Therefore, in this section, we concentrate on the ability of our model to generalize to an unseen target emotion by considering the scenario where we have zero or only a few training samples in an unseen emotion.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Zero-Shot Emotion Prediction",
      "text": "Ideally, our model is able to directly adapt to a new emotion based on its embedding. Given a new text emotion embedding e t k+1 , we can generate the visual and acoustic emotion embeddings e v k+1 and e a k+1 , respectively, using the already learned mapping functions f t→v and f t→a . After that, the similarity scores between the input sentence and the new emotion can be computed for each modality.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Few-Shot Emotion Prediction",
      "text": "In this section, we assume 1% of the positive training samples in a new emotion are available, and to balance the training samples, we take the same amount of negative training samples for the new emotion. However, the model could lose its ability to predict the original emotions when we simply fine-tune it on the training samples of a new emotion. To cope with this issue, we propose two finetuning settings. First, after we obtain the trained model in the source emotions, we jointly train it with the training samples of the source emotions and the new target emotion. Second, we utilize a continual learning method, gradient episodic memory (GEM)  (Lopez-Paz and Ranzato, 2017) , to prevent the catastrophic forgetting of previously learned knowledge. The purpose of using continual learning is that we do not need to retrain with all the data from previously learned emotions since the data might not be available. We describe the training process of GEM as follows:\n\nWe define Θ S as the model's parameters trained in the source emotions, and Θ denotes the current optimized parameters based on the target emotion data. GEM keeps a small number of samples N from the source emotions, and a constraint is applied on the gradient to prevent the loss on the stored samples from increasing when the model learns the new target emotion. The training process can be formulated as\n\nwhere L(Θ, N ) is the loss value of the N stored samples.\n\nIn this section, we first introduce the two public datasets we use and data feature extraction. Then, we discuss our evaluation metrics, including their advantages and defects. Finally, we introduce the baselines and our experimental settings.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Datasets",
      "text": "CMU-MOSEI CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI)  (Zadeh et al., 2018b)  is currently the largest public dataset for multi-modal sentiment analysis and emotion recognition. It comprises 23,453 annotated data samples extracted from 3228 videos. For emotion recognition, it consists of six basic categories: anger, disgust, fear, happy, sad, and surprise. For zero-shot and few-shot learning evaluation, we use four relatively low-resource categories among them (anger, disgust, fear, surprise). The model is trained on the other five categories when evaluating one zero-shot category. A detailed statistical table about these categories is included in Appendix A.\n\nIEMOCAP The Interactive Emotional Dyadic Motion Capture (IEMOCAP)  (Busso et al., 2008)  dataset was created for multi-modal human emotion analysis, and was collected from dialogues performed by ten actors. It is also a multi-labelled emotion recognition dataset which contains nine emotion categories. For comparison with prior works  (Wang et al., 2018; Liang et al., 2018; Pham et al., 2018; Tsai et al., 2019a)  where four (out of the nine) emotion categories are selected for training and evaluating the models, we also follow the same four categories, namely, happy, sad, angry, and neutral, to train our model. For zero-shot learning evaluation, we consider three low-resource categories from the remaining five, namely, excited, surprised, and frustrated, as unseen emotions.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Data Feature Extraction",
      "text": "We use CMU-Multimodal SDK  (Zadeh et al., 2018c)  for downloading and pre-processing the datasets. It helps to do data alignment and earlystage feature extraction for each modality. The textual data is tokenized in word level and represented using GloVe  (Pennington et al., 2014)  embeddings. Facial action units are extracted by the Facet (iMotions, 2017) to indicate muscle movements and expressions  (Ekman et al., 1980) . These are a commonly used type of feature for facial expression recognition  (Fan et al., 2020) . For acoustic data, COVAREP  (Degottex et al., 2014)  is used to extract fundamental features, such as mel-frequency cepstral coefficients (MFCCs), pitch tracking, glottal source parameters, etc.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "Weighted Accuracy Due to the imbalanced nature of the emotion recognition dataset (for each emotion category, there are many more negative samples than positive samples), we use binary weighted accuracy  (Tong et al., 2017)  on each category to better measure the model's performance. The formula is Weighted Acc. = T P × N/P + T N 2N\n\nwhere P means total positive, TP true positive, N total negative, and TN true negative.\n\nWeighted F1 In prior works  (Zadeh et al., 2018b; Akhtar et al., 2019; Tsai et al., 2019a) , the binary weighted F1 score metric is used on the CMU-MOSEI dataset, and its formula is shown in Eq.5.\n\nHere, F1 p is the F1 score that treats positive samples as positive, while F1 n treats negative samples as positive, and they are weighted by their portion of the data. However, there is one defect of using binary weighted F1 in this task. As there are many more negative samples than positive ones, we find that with the increase of the threshold, the weighted F1 score will also increase because the true negative increases. Therefore, in this paper, we do not report this metric. A detailed analysis of this is given in Appendix B.\n\nAUC Score To eliminate the effect of threshold and mitigate the defect of the weighted F1 score, we also report Area under the ROC Curve (AUC) scores. The AUC score considers classification performance on both positive and negative samples, and it is scale-and threshold-invariant.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Baselines",
      "text": "For both the CMU-MOSEI and IEMOCAP datasets, we use Early Fusion LSTM (EF-LSTM) and Late Fusion LSTM (LF-LSTM) as two baseline models. Additionally, for CMU-MOSEI, the Graph Memory Fusion Network (Graph-MFN)  (Zadeh et al., 2018b ) and a multi-task learning (MTL)",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Training Details",
      "text": "The model is trained end-to-end with the Adam optimizer (Kingma and Ba, 2015) and a scheduler that will reduce the learning rate by a factor of 0.1 when the optimization stays on a plateau for more than 5 epochs. The best hyper-parameters in our training for both datasets are shown in Table  3 . Also, we use the largest GloVe word embeddings (glove.840B.300d 2  ) for both the input text data and the emotion embeddings in the textual modality.\n\nThe weights of the textual embeddings are frozen during training to keep the pre-trained relations, which is also essential for doing zero-shot learning.\n\n6 Analysis",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Results",
      "text": "Table  1  shows our model's performance on the CMU-MOSEI dataset. Compared to existing baselines, our model surpasses them by a large margin.\n\nThe weighted accuracy (W-Acc) and AUC score are used for evaluation, with a threshold set to 0.5 to calculate the W-Acc. As discussed in Section 5.3, we do not follow the previous papers in using the weighted F1-score (W-F1) because it does not provide an effective evaluation when the dataset is very imbalanced. For example, the weakest baseline, EF-LSTM, can even achieve 90% W-F1 by predicting almost all samples as negative. More plots and analysis of this defect of W-F1 are included in Appendix B.\n\nWe further test our model on a second dataset called IEMOCAP, and the results are shown in Table 2. Similarly, our model achieves better results on most emotion categories, except happy. For a   fair comparison on IEMOCAP, we use accuracy instead of W-Acc, following the previous works compared in the table.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Effects Of Emotion Embeddings",
      "text": "Quantitatively, our model makes a large improvement in the multi-modal emotion recognition task. We think it benefits greatly from the emotion embeddings, which can model the relationships (or distances) between emotion categories. This is especially important for emotion recognition, which is a multi-label task by nature, as people can have multiple emotions at the same time. For example, if a person is surprised, it is more likely that this person is also happy and excited and is less likely to be disgusted or sad. This kind of information is expected to be modelled and captured by emotion embeddings. Intuitively, in the textual space, related emotions (e.g., angry and disgusted) tend to have closer word vectors than unrelated emotions (angry and happy). To ensure the effectiveness of word embeddings, for each emotion word, we investi-gated multiple forms of it. For example, for surprised, we also tried with Surprised, (S/s)urprising, (S/s)urprise. Generally, they all show a similar trend, and in most cases, the word form that is used to describe human shows the best results. In our final setting, we iterate and pick the best performing form for each emotion category.\n\nMoreover, our model can transfer the relationship of emotion categories from the textual space to the acoustic and visual spaces using end-to-end optimized mapping functions. In Figure  3 , we show the Euclidean distances of emotion embeddings between categories. The relative positions are preserved very well after being transferred from the textual space to the visual and acoustic spaces. This indicates that the learned mapping functions (f t→v and f t→a ) are effective. Although it is not the main focus of this paper, we think improving the pre-trained textual emotion embeddings is an essential direction for future work. It can benefit all modalities and further enhance the overall performance. For example, incorporate semantic emotion information  (Xu et al., 2018)  to the original word embeddings.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Zero/Few-Shot Results",
      "text": "Benefiting from the pre-trained textual emotion embeddings and learned mapping functions, our model can recognize unseen emotion categories to a certain extent. We evaluate our model's zero-shot learning ability on the low-resource categories in CMU-MOSEI (shown in Table  4 ) and IEMOCAP (shown in Table  5 ). For a fair comparison, we use the same training setting that is used in Table  1 . This can ensure that no downgrade happens on the seen emotions, and the model is not selected to overfit a single unseen category. As we can see, the zero-shot results of the baselines are similar to random guesses, because the weights related to that unseen emotion in the model are randomly initialized and have never been optimized. For our model, the zero-shot performance is much better than that of the baselines in almost all emotions. This is because our model learns to classify emotion categories based on the similarity between the sentence representation and emotion embeddings, which enables our model better generalization ability to other unseen emotions since emotion embeddings contain semantic information in the vector space.\n\nFurthermore, we perform few-shot learning using only 1% of data of these low-resource categories. As we can see from Table  4 , using very few training samples, our model can adapt to unseen emotions without losing the performance in the source emotions. In addition, we observe that simply fine-tuning (FT) our trained model sometimes obtains inferior performance. This is because our model will gradually lose the ability to classify the source emotions, and we have to early stop the fine-tuning process, which leads to inferior performance. We can see that CL and JT prevent our model from catastrophic forgetting and improve the few-shot performance in the unseen emotion. Moreover, JT achieves slightly better performance than CL. This can be attributed to the fact that CL might still result in performance drops in source emotions since our model only observes partial samples from them. At the same time, JT directly optimizes the model on the data samples of such emotions.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Ablation Study",
      "text": "To further investigate how each individual modality influences the model, we perform comprehensive ablation studies on supervised multi-modal emotion recognition and also zero-shot prediction.\n\nIn Table  6 , we enumerate different subsets of the (textual, acoustic, visual) modalities to evaluate the effect of each single modality. Generally, the performance will increase if more modalities are available. Compared to single-modal data, multimodal data can provide supplementary information, which leads to more accurate emotion recognition. In terms of a single modality, we find that textual and acoustic are more effective than visual.\n\nSimilarly, in Table  5 , we show the zero-shot performance with different combinations of modalities during the inference time (all modalities exist in the training phase). As there are many more negative samples than positive ones in the ZSL setting, we also evaluate the models with the unweighted F1 score. Because if a model has high accuracy but a low F1, it is heavily biased to the negative samples so it cannot do classification effectively. Empirical results indicate that zero-shot on only one modality is possible. Moreover, if the data of an emotion category has strong characteristics in one modality and is ambiguous in other modalities, single-modality can even surpass multi-modality on zero-shot prediction. For example, the performance of single-modality zero-shot prediction using the acoustic modality on the surprised category is better than using all modalities.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we introduce a modality-transferable model that leverages cross-modality emotion embeddings for multi-modal emotion recognition. It makes predictions by measuring the distances between input data and target emotion categories, which is especially effective for a multi-label problem. The model also learns two mapping functions to transfer pre-trained textual emotion embeddings to acoustic and visual spaces. The empirical results demonstrate that it exhibits state-of-the-art performance on most of the categories. Enabled by the utilization of emotion embeddings, our model can carry out zero-shot learning for unseen emotion categories and can quickly adapt few-shot learning without downgrading trained categories.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "B Weighted F1 Analysis",
      "text": "In Figure  4   The W-F1 is almost proportional to the threshold values and it is still very high when the threshold is 0.9 (i.e. most data samples are predicted to be negative). Moreover, when the threshold is large, the W-F1 keeps a high value starting from epoch 1. By contrast, the W-Acc score is more reliable. It ensures the model can also retrieve positive samples. We observe a similar phenomenon on all models. As a result, we think W-F1 is unsuitable as an evaluation metric on this dataset.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An intuitive example of our method. In the",
      "page": 1
    },
    {
      "caption": "Figure 2: The architecture of our proposed multi-modal emotion recognition model. It consists of three LSTM",
      "page": 3
    },
    {
      "caption": "Figure 2: , our model consists of three",
      "page": 3
    },
    {
      "caption": "Figure 2: , for each data sample,",
      "page": 3
    },
    {
      "caption": "Figure 3: Euclidean distances between different emotion embeddings in the textual, acoustic, and visual spaces.",
      "page": 7
    },
    {
      "caption": "Figure 4: Trend lines of the weighted f1 (W-F1) score",
      "page": 12
    },
    {
      "caption": "Figure 5: Trend lines of the weighted accuracy (W-",
      "page": 12
    },
    {
      "caption": "Figure 4: and 5, we show the trends of the",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Results of multi-modal emotion recognition on the CMU-MOSEI dataset. Baselines (EF-LSTM, LF-",
      "data": [
        {
          "Emotion": "Metrics",
          "Anger": "W-Acc\nAUC",
          "Disgust": "W-Acc\nAUC",
          "Fear": "W-Acc\nAUC",
          "Happy": "W-Acc\nAUC",
          "Sad": "W-Acc\nAUC",
          "Surprise": "W-Acc\nAUC",
          "Average": "W-Acc\nAUC"
        },
        {
          "Emotion": "EF-LSTM\nLF-LSTM\nGraph-MFN\nMTL",
          "Anger": "58.5\n62.2\n57.7\n66.5\n62.6\n-\n66.8\n68.0†",
          "Disgust": "59.9\n63.9\n61.0\n71.9\n69.1\n-\n72.7\n76.7†",
          "Fear": "50.1\n69.8\n50.7\n61.1\n62.0\n-\n62.2\n42.9†",
          "Happy": "65.1\n68.9\n63.9\n68.6\n66.3\n-\n53.6\n71.4†",
          "Sad": "55.1\n58.6\n54.3\n59.6\n60.4\n-\n61.4\n57.6†",
          "Surprise": "50.6\n54.3\n51.4\n61.5\n53.7\n-\n60.6\n65.1†",
          "Average": "56.6\n63.0\n56.5\n64.9\n62.3\n-\n62.8\n63.6†"
        },
        {
          "Emotion": "Ours",
          "Anger": "67.0\n71.7",
          "Disgust": "78.3\n72.5",
          "Fear": "65.4\n71.6",
          "Happy": "67.9\n73.9",
          "Sad": "62.6\n66.7",
          "Surprise": "62.1\n66.4",
          "Average": "66.2\n71.4"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 1: Results of multi-modal emotion recognition on the CMU-MOSEI dataset. Baselines (EF-LSTM, LF-",
      "data": [
        {
          "Emotion": "Metrics",
          "Happy": "Acc\nAUC",
          "Sad": "Acc\nAUC",
          "Angry": "Acc\nAUC",
          "Neutral": "Acc\nAUC"
        },
        {
          "Emotion": "EF-LSTM\nLF-LSTM\nRMFN (Liang et al., 2018)\nRAVEN (Wang et al., 2018)\nMCTN (Pham et al., 2018)\nMulT (Tsai et al., 2019a)",
          "Happy": "85.8\n70.7\n85.2\n71.7\n87.5\n-\n87.3\n-\n84.9\n-\n83.5†\n71.2†",
          "Sad": "83.7\n85.8\n83.4\n84.4\n83.8\n-\n83.4\n-\n80.5\n-\n89.3†\n85.0†",
          "Angry": "75.8\n90.3\n79.5\n86.8\n85.1\n-\n87.3\n-\n79.7\n-\n85.5†\n92.4†",
          "Neutral": "67.1\n74.1\n66.5\n72.2\n69.5\n-\n69.7\n-\n62.3\n-\n77.2†\n71.0†"
        },
        {
          "Emotion": "Ours",
          "Happy": "74.2\n85.0",
          "Sad": "86.6\n88.4",
          "Angry": "88.1\n93.2",
          "Neutral": "71.1\n76.7"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 1: Results of multi-modal emotion recognition on the CMU-MOSEI dataset. Baselines (EF-LSTM, LF-",
      "data": [
        {
          "CMU-MOSEI\nIEMOCAP": "15\n16\n512\n32\n1e-4\n1e-3\n2\n2\n300/200/100\n300/200/100\n0.15\n0.15\n10.0\n1.0\n0\n0"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 4: Zero/few-shot results on low-resource emotion categories in CMU-MOSEI dataset. Here, FT, CL, and",
      "data": [
        {
          "Unseen emotion": "Zero-Shot",
          "Anger (unseen)": "50.6\n50.9\n48.4\n49.2\n55.9\n61.6",
          "Disgust (unseen)": "50.3\n48.2\n49.7\n44.2\n67.5\n72.7",
          "Fear (unseen)": "45.8\n42.3\n47.4\n47.3\n41.8\n40.6",
          "Surprise (unseen)": "50.2\n46.9\n48.6\n48.3\n53.4\n55.5"
        },
        {
          "Unseen emotion": "1% Few-Shot",
          "Anger (unseen)": "61.9\n58.9\n58.9\n61.5\n59.0\n61.1",
          "Disgust (unseen)": "67.9\n71.5\n68.7\n72.8\n69.2\n74.2",
          "Fear (unseen)": "43.1\n43.1\n42.6\n42.7\n41.9\n41.7",
          "Surprise (unseen)": "51.8\n53.9\n50.6\n52.5\n55.2\n58.1"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 4: Zero/few-shot results on low-resource emotion categories in CMU-MOSEI dataset. Here, FT, CL, and",
      "data": [
        {
          "Average on all categories": "Zero-shot\nOurs",
          "Except Anger": "65.6\n70.6",
          "Except Disgust": "64.4\n69.3",
          "Except Fear": "65.9\n70.9",
          "Except Surprise": "67.2\n71.4"
        },
        {
          "Average on all categories": "FT (Ours)\n1% Few-Shot\nCL (Ours)\nJT (Ours)",
          "Except Anger": "69.8\n64.4\n64.6\n69.8\n64.3\n69.3",
          "Except Disgust": "63.7\n68.5\n63.8\n68.9\n63.5\n68.8",
          "Except Fear": "65.4\n70.7\n70.9\n65.6\n65.9\n70.8",
          "Except Surprise": "65.1\n71.4\n71.5\n65.5\n66.1\n71.5"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 5: Zero-shot results on the IEMOCAP dataset.",
      "data": [
        {
          "Unseen emotion": "Metrics",
          "Excited\n(unseen)": "Acc\nF1",
          "Surprised\n(unseen)": "Acc\nF1",
          "Frustrated\n(unseen)": "Acc\nF1"
        },
        {
          "Unseen emotion": "EF-LSTM\nLF-LSTM\nMulT",
          "Excited\n(unseen)": "13.1\n23.1\n14.0\n23.3\n45.1\n27.3",
          "Surprised\n(unseen)": "11.3\n5.1\n2.6\n5.1\n41.4\n7.5",
          "Frustrated\n(unseen)": "22.9\n37.2\n23.7\n37.4\n48.7\n40.9"
        },
        {
          "Unseen emotion": "Ours (TAV)\nOurs (TA)\nOurs (TV)\nOurs (AV)\nOurs (T)\nOurs (A)\nOurs (V)",
          "Excited\n(unseen)": "82.0\n56.1\n79.9\n52.7\n75.9\n42.7\n89.1\n69.9\n72.9\n37.1\n76.9\n52.8\n82.1\n35.0",
          "Surprised\n(unseen)": "78.8\n13.1\n79.3\n14.4\n58.6\n9.1\n65.7\n13.2\n67.7\n3.1\n82.1\n16.8\n81.1\n6.2",
          "Frustrated\n(unseen)": "73.6\n57.9\n75.1\n60.1\n54.1\n13.6\n83.9\n73.6\n55.3\n9.0\n86.1\n74.8\n68.6\n44.6"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 5: Zero-shot results on the IEMOCAP dataset.",
      "data": [
        {
          "Metric": "Emotion",
          "W-Acc": "Anger\nDisgust\nFear\nHappy\nSad\nSurprise"
        },
        {
          "Metric": "T+A+V\nT+A\nT+V\nA+V\nOnly T\nOnly A\nOnly V",
          "W-Acc": "67.0\n72.5\n67.9\n62.1\n65.4\n62.6\n63.0\n65.0\n71.9\n64.8\n66.0\n59.9\n66.7\n64.9\n71.2\n67.6\n61.0\n60.4\n63.8\n71.1\n65.5\n64.5\n61.3\n55.2\n61.5\n69.0\n64.3\n64.2\n59.7\n61.2\n61.9\n71.5\n66.9\n62.7\n61.0\n54.8\n63.4\n69.7\n63.2\n63.2\n58.5\n53.3"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Multi-task learning for multimodal emotion recognition and sentiment analysis",
      "authors": [
        "Shad Md",
        "Dushyant Akhtar",
        "Deepanway Chauhan",
        "Soujanya Ghosal",
        "Asif Poria",
        "Pushpak Ekbal",
        "Bhattacharyya"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N19-1034"
    },
    {
      "citation_id": "2",
      "title": "Neural machine translation by jointly learning to align and translate",
      "authors": [
        "Dzmitry Bahdanau",
        "Kyunghyun Cho",
        "Yoshua Bengio"
      ],
      "year": "2015",
      "venue": "Neural machine translation by jointly learning to align and translate"
    },
    {
      "citation_id": "3",
      "title": "Multimodal machine learning: A survey and taxonomy",
      "authors": [
        "Tadas Baltrušaitis",
        "Chaitanya Ahuja",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "4",
      "title": "Towards zero-shot frame semantic parsing for domain scaling",
      "authors": [
        "Ankur Bapna",
        "Gokhan Tür",
        "Dilek Hakkani-Tür",
        "Larry Heck"
      ],
      "year": "2017",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "5",
      "title": "Iemocap: interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Provost",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "6",
      "title": "Covarep -a collaborative voice analysis repository for speech technologies",
      "authors": [
        "Gilles Degottex",
        "John Kane",
        "Thomas Drugman",
        "Tuomo Raitio",
        "Stefan Scherer"
      ],
      "year": "2014",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "7",
      "title": "Facial signs of emotional experience",
      "authors": [
        "Paul Ekman",
        "Sonia Wallace V Freisen",
        "Ancoli"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "8",
      "title": "Facial action unit intensity estimation via semantic correspondence learning with dynamic graph convolution",
      "authors": [
        "Yingruo Fan",
        "Jacqueline Lam",
        "Victor Li"
      ],
      "year": "2020",
      "venue": "Thirty-Fourth AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "9",
      "title": "Pathnet: Evolution channels gradient descent in super neural networks",
      "authors": [
        "Chrisantha Fernando",
        "Dylan Banarse",
        "Charles Blundell",
        "Yori Zwols",
        "David Ha",
        "Andrei Rusu",
        "Alexander Pritzel",
        "Daan Wierstra"
      ],
      "year": "2017",
      "venue": "Pathnet: Evolution channels gradient descent in super neural networks",
      "arxiv": "arXiv:1701.08734"
    },
    {
      "citation_id": "10",
      "title": "Meta-learning for lowresource neural machine translation",
      "authors": [
        "Jiatao Gu",
        "Yong Wang",
        "Yun Chen",
        "O Victor",
        "Kyunghyun Li",
        "Cho"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "11",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Amir Zadeh",
        "Erik Cambria",
        "Louis-Philippe Morency",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N18-1193"
    },
    {
      "citation_id": "12",
      "title": "Long short-term memory",
      "authors": [
        "Sepp Hochreiter",
        "Jürgen Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural Computation"
    },
    {
      "citation_id": "13",
      "title": "Cross-lingual annotation projection is effective for neural part-of-speech tagging",
      "authors": [
        "Matthias Huck",
        "Diana Dutka",
        "Alexander Fraser"
      ],
      "year": "2019",
      "venue": "Proceedings of the Sixth Workshop on NLP for Similar Languages, Varieties and Dialects"
    },
    {
      "citation_id": "14",
      "title": "Google's multilingual neural machine translation system: Enabling zero-shot translation",
      "authors": [
        "Melvin Johnson",
        "Mike Schuster",
        "V Quoc",
        "Maxim Le",
        "Yonghui Krikun",
        "Zhifeng Wu",
        "Nikhil Chen",
        "Fernanda Thorat",
        "Martin Viégas",
        "Greg Wattenberg",
        "Corrado"
      ],
      "year": "2017",
      "venue": "Transactions of the Association for Computational Linguistics"
    },
    {
      "citation_id": "15",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2015",
      "venue": "3rd International Conference on Learning Representations, ICLR 2015"
    },
    {
      "citation_id": "16",
      "title": "Overcoming catastrophic forgetting in neural networks",
      "authors": [
        "James Kirkpatrick",
        "Razvan Pascanu",
        "Neil Rabinowitz",
        "Joel Veness",
        "Guillaume Desjardins",
        "Andrei Rusu",
        "Kieran Milan",
        "John Quan",
        "Tiago Ramalho",
        "Agnieszka Grabska-Barwinska"
      ],
      "year": "2017",
      "venue": "Proceedings of the national academy of sciences"
    },
    {
      "citation_id": "17",
      "title": "Overcoming catastrophic forgetting by incremental moment matching",
      "authors": [
        "Sang-Woo Lee",
        "Jin-Hwa Kim",
        "Jaehyun Jun",
        "Jung-Woo Ha",
        "Byoung-Tak Zhang"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "18",
      "title": "Mlqa: Evaluating cross-lingual extractive question answering",
      "authors": [
        "Patrick Lewis",
        "Barlas Oguz",
        "Ruty Rinott",
        "Sebastian Riedel",
        "Holger Schwenk"
      ],
      "year": "2019",
      "venue": "Mlqa: Evaluating cross-lingual extractive question answering",
      "arxiv": "arXiv:1910.07475"
    },
    {
      "citation_id": "19",
      "title": "Multimodal language analysis with recurrent multistage fusion",
      "authors": [
        "Paul Pu Liang",
        "Ziyin Liu",
        "Amirali Bagher Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/D18-1014"
    },
    {
      "citation_id": "20",
      "title": "Xqa: A cross-lingual open-domain question answering dataset",
      "authors": [
        "Jiahua Liu",
        "Yankai Lin",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "21",
      "title": "Efficient lowrank multimodal fusion with modality-specific factors",
      "authors": [
        "Zhun Liu",
        "Ying Shen",
        "Varun Bharadhwaj Lakshminarasimhan",
        "Paul Liang",
        "Amirali Bagher Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P18-1209"
    },
    {
      "citation_id": "22",
      "title": "Zero-shot cross-lingual dialogue systems with transferable latent variables",
      "authors": [
        "Zihan Liu",
        "Jamin Shin",
        "Yan Xu",
        "Genta Indra Winata",
        "Peng Xu",
        "Andrea Madotto",
        "Pascale Fung"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"
    },
    {
      "citation_id": "23",
      "title": "Attention-informed mixed-language training for zero-shot cross-lingual task-oriented dialogue systems",
      "authors": [
        "Zihan Liu",
        "Genta Indra Winata",
        "Zhaojiang Lin",
        "Peng Xu",
        "Pascale Fung"
      ],
      "year": "2019",
      "venue": "Attention-informed mixed-language training for zero-shot cross-lingual task-oriented dialogue systems",
      "arxiv": "arXiv:1911.09273"
    },
    {
      "citation_id": "24",
      "title": "Coach: A coarse-to-fine approach for cross-domain slot filling",
      "authors": [
        "Zihan Liu",
        "Genta Indra Winata",
        "Peng Xu",
        "Pascale Fung"
      ],
      "year": "2020",
      "venue": "Coach: A coarse-to-fine approach for cross-domain slot filling",
      "arxiv": "arXiv:2004.11727"
    },
    {
      "citation_id": "25",
      "title": "Gradient episodic memory for continual learning",
      "authors": [
        "David Lopez",
        "- Paz",
        "Marc'aurelio Ranzato"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "26",
      "title": "Personalizing dialogue agents via meta-learning",
      "authors": [
        "Andrea Madotto",
        "Zhaojiang Lin",
        "Chien-Sheng Wu",
        "Pascale Fung"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "27",
      "title": "Weakly supervised cross-lingual named entity recognition via effective annotation and representation projection",
      "authors": [
        "Jian Ni",
        "Georgiana Dinu",
        "Radu Florian"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "28",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "Jeffrey Pennington",
        "Richard Socher",
        "Christopher Manning"
      ],
      "year": "2014",
      "venue": "Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "29",
      "title": "Found in translation: Learning robust joint representations by cyclic translations between modalities",
      "authors": [
        "Hai Pham",
        "Paul Liang",
        "Thomas Manzini"
      ],
      "year": "2018",
      "venue": "Louis-Philippe Morency, and Barnabás Póczos"
    },
    {
      "citation_id": "30",
      "title": "Razvan Pascanu, and Raia Hadsell. 2016. Progressive neural networks",
      "authors": [
        "Andrei Rusu",
        "Neil Rabinowitz",
        "Guillaume Desjardins",
        "Hubert Soyer",
        "James Kirkpatrick",
        "Koray Kavukcuoglu"
      ],
      "venue": "Razvan Pascanu, and Raia Hadsell. 2016. Progressive neural networks",
      "arxiv": "arXiv:1606.04671"
    },
    {
      "citation_id": "31",
      "title": "Avec 2011-the first international audio/visual emotion challenge",
      "authors": [
        "Björn Schuller",
        "Michel Valstar",
        "Florian Eyben",
        "Gary Mckeown",
        "Roddy Cowie",
        "Maja Pantic"
      ],
      "year": "2011",
      "venue": "Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "32",
      "title": "Lifelong-rl: Lifelong relaxation labeling for separating entities and aspects in opinion targets",
      "authors": [
        "Lei Shu",
        "Bing Liu",
        "Hu Xu",
        "Annice Kim"
      ],
      "year": "2016",
      "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "33",
      "title": "Doc: Deep open classification of text documents",
      "authors": [
        "Lei Shu",
        "Hu Xu",
        "Bing Liu"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "34",
      "title": "Combating human trafficking with multimodal deep models",
      "authors": [
        "Edmund Tong",
        "Amir Zadeh",
        "Cara Jones",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P17-1142"
    },
    {
      "citation_id": "35",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Pu Liang",
        "J Kolter",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "36",
      "title": "Learning factorized multimodal representations",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Paul Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Learning factorized multimodal representations"
    },
    {
      "citation_id": "37",
      "title": "Words can shift: Dynamically adjusting word representations using nonverbal behaviors",
      "authors": [
        "Yansen Wang",
        "Ying Shen",
        "Zhun Liu",
        "Paul Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "AAAI Conference on Artificial ligence. AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "38",
      "title": "Learning fast adaptation on cross-accented speech recognition",
      "authors": [
        "Genta Indra Winata",
        "Samuel Cahyawijaya",
        "Zihan Liu",
        "Zhaojiang Lin",
        "Andrea Madotto",
        "Peng Xu",
        "Pascale Fung"
      ],
      "year": "2020",
      "venue": "Learning fast adaptation on cross-accented speech recognition",
      "arxiv": "arXiv:2003.01901"
    },
    {
      "citation_id": "39",
      "title": "Crosslingual part-of-speech tagging through ambiguous learning",
      "authors": [
        "Guillaume Wisniewski",
        "Nicolas Pécheux",
        "Souhir Gahbiche-Braham",
        "Franc ¸ois Yvon"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "40",
      "title": "Transferable multi-domain state generator for task-oriented dialogue systems",
      "authors": [
        "Chien-Sheng Wu",
        "Andrea Madotto",
        "Ehsan Hosseini-Asl",
        "Caiming Xiong",
        "Richard Socher",
        "Pascale Fung"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "41",
      "title": "Neural crosslingual named entity recognition with minimal resources",
      "authors": [
        "Jiateng Xie",
        "Zhilin Yang",
        "Graham Neubig",
        "Noah Smith",
        "Jaime Carbonell"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "42",
      "title": "Emograph: Capturing emotion correlations using graph networks",
      "authors": [
        "Peng Xu",
        "Zihan Liu",
        "Genta Indra Winata",
        "Zhaojiang Lin",
        "Pascale Fung"
      ],
      "year": "2020",
      "venue": "Emograph: Capturing emotion correlations using graph networks",
      "arxiv": "arXiv:2008.09378"
    },
    {
      "citation_id": "43",
      "title": "Emo2vec: Learning generalized emotion representation by multi-task training",
      "authors": [
        "Peng Xu",
        "Andrea Madotto",
        "Chien-Sheng Wu",
        "Ji Park",
        "Pascale Fung"
      ],
      "year": "2018",
      "venue": "Emo2vec: Learning generalized emotion representation by multi-task training",
      "arxiv": "arXiv:1809.04505"
    },
    {
      "citation_id": "44",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "Amir Zadeh",
        "Minghai Chen",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/D17-1115"
    },
    {
      "citation_id": "45",
      "title": "Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. 2018a. Memory fusion network for multiview sequential learning",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Navonil Mazumder"
      ],
      "venue": "AAAI"
    },
    {
      "citation_id": "46",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "ACL"
    },
    {
      "citation_id": "47",
      "title": "Prateek Vij, Erik Cambria, and Louis-Philippe Morency. 2018c. Multi-attention recurrent network for human communication comprehension",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Soujanya Poria"
      ],
      "venue": "Thirty-Second AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "48",
      "title": "Zeroshot dialog generation with cross-domain latent actions",
      "authors": [
        "Tiancheng Zhao",
        "Maxine Eskenazi"
      ],
      "year": "2018",
      "venue": "Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue"
    }
  ]
}