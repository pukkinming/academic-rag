{
  "paper_id": "2207.09373v3",
  "title": "Multi-Task Learning Framework For Emotion Recognition In-The-Wild",
  "published": "2022-07-19T16:18:53Z",
  "authors": [
    "Tenggan Zhang",
    "Chuanhe Liu",
    "Xiaolong Liu",
    "Yuchen Liu",
    "Liyu Meng",
    "Lei Sun",
    "Wenqiang Jiang",
    "Fengyuan Zhang",
    "Jinming Zhao",
    "Qin Jin"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper presents our system for the Multi-Task Learning (MTL) Challenge in the 4th Affective Behavior Analysis in-the-wild (ABAW) competition. We explore the research problems of this challenge from three aspects: 1) For obtaining efficient and robust visual feature representations, we propose MAE-based unsupervised representation learning and IResNet/DenseNet-based supervised representation learning methods; 2) Considering the importance of temporal information in videos, we explore three types of sequential encoders to capture the temporal information, including the encoder based on transformer, the encoder based on LSTM, and the encoder based on GRU; 3) For modeling the correlation between these different tasks (i.e., valence, arousal, expression, and AU) for multi-task affective analysis, we first explore the dependency between these different tasks and propose three multi-task learning frameworks to model the correlations effectively. Our system achieves the performance of 1.7607 on the validation dataset and 1.4361 on the test dataset, ranking first in the MTL Challenge. The code is available at https://github.com/AIM3-RUC/ABAW4.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Affective computing aims to develop technologies to empower machines with the capability of observing, interpreting, and generating emotions just like humans do  [30] . There has emerged a wide range of application scenarios of affective computing, including health research, society analysis, and other interaction scenarios. More and more people are interested in affective computing due to the significant improvement of machine learning technology performance and the growing attention to the mental health field. There are lots of datasets to support the research of affective computing, including Aff-wild  [16] , Aff-wild2  [19] , and s-Aff-Wild2  [12, 13, 17, 22, 15, 21, 20, 18, 14, 33, 16] . The advancement of multi-task learning algorithms  [28]  has also boosted performance via exploring supervision from different tasks.\n\nOur system for the Multi-Task Learning (MTL) Challenge contains four key components. 1) We explore several unsupervised (MAE-based) and supervised (IResNet/DenseNet-based) visual feature representation learning methods for learning effective and robust visual representations; 2) We utilize three types of temporal encoders, including GRU  [4] , LSTM  [29]  and Transformer  [31] , to capture the sequential information in videos; 3) We employ multi-task frameworks to predict the valence, arousal, expression and AU values. Specifically, we investigate three different strategies for multi-task learning, namely Share Encoder (SE), Share Bottom of Encoder (SBE) and Share Bottom of Encoder with Hidden States Feedback (SBE-HSF); 4) Finally, we adopt ensemble strategies and cross-validation to further enhance the predictions, and we get the performance of 1.7607 on the validation dataset and 1.4361 on the test dataset, ranking first in the MTL Challenge.",
      "page_start": 1,
      "page_end": 12
    },
    {
      "section_name": "Related Works",
      "text": "There are lots of solutions proposed for former ABAW competitions. We investigate some studies for valence and arousal prediction, facial expression classification and facial action unit detection, which are based on deep learning methods.\n\nFor valence and arousal prediction,  [25]  proposes a novel architecture to fuse temporal-aware multimodal features and an ensemble method to further enhance performance of regression models.  [34]  proposes a model for continuous emotion prediction using a cross-modal co-attention mechanism with three modalities (i.e., visual, audio and linguistic information).  [27]  combines local attention with GRU and uses multimodal features to enhance the performance. For expression classification, facing the problem that the changes of features for expression are difficult to be processed by one attention module,  [32]  proposes a novel attention mechanism to capture local and semantic features.  [35]  utilizes multimodal features, including visual, audio and text to build a transformer-based framework for expression classification and AU detection. For facial action unit detection,  [8]  utilizes a multi-task approach with a center contrastive loss and ROI attention module to learn the correlations of facial action units.  [9]  proposes a model-level ensemble method to achieve comparable results.  [5]  introduces a semantic correspondence convolution module to capture the relations of AU in a heat map regression framework dynamically.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Method",
      "text": "Given an image sequence consisting of {F 1 , F 2 , ..., F n } from video X, the goal of the MTL challenge is to produce four types of emotion predictions for each frame, including the label y v for valence, the label y a for arousal, the label y e",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multi-Task Learning Framework",
      "text": "Fig.  1 . The pipeline of our method for the challenge.\n\nfor expression, and the labels {y AU 1 , y AU 2 , ..., y AU 26 } for 12 AUs. Please note that only some sampled frames in a video are annotated in the training data, and the four types of annotations may be partially missing for an image frame.\n\nOur pipeline for the challenge is shown in figure  1 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Features",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Mae-Based Features",
      "text": "The features of the first type are extracted by MAE  [6]  models † which use C-MS-Celeb  [10]  and EmotionNet  [3]  datasets at the pretraining stage. The first model is pre-trained on the C-MS-Celeb dataset and finetuned on different downstream tasks, including expression classification on the s-Aff-Wild2 dataset, AU classification task on the s-Aff-Wild2 dataset, expression classification on the AffectNet  [26]  dataset and expression classification on the dataset combining FER+  [2]  and AffectNet  [26]  datasets. As for the second model, we first use the EmotionNet dataset to pre-train the MAE model with the reconstruction task, and then use the AffectNet  [26]  dataset to fine-tune the model further.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iresnet-Based Features",
      "text": "The features of the second type are extracted by IResNet100 models. The models are pre-trained in two different settings. As for the first setting, we use FER+  [2] , RAF-DB  [24, 23] , and AffectNet  [26]  datasets to pre-train the model. Specifically, the faces are aligned by keypoints and the input size is resized into 112x112 before pre-training. As for the second setting, we use the Glint360K  [1]  dataset to pre-train the model, and then use an FAU dataset with commercial authorization to train this model further.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Densenet-Based Features",
      "text": "The features of the third type are extracted by a DenseNet  [7]  model. The pre-training stage uses FER+ and AffectNet datasets, and we also try to fine-tune the pre-trained model on the s-Aff-Wild2 dataset, including the expression classification task and AU classification task.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Temporal Encoder",
      "text": "Because the GPU memory is limited, the annotated frames are firstly split into segments. If the length of the split segment is l and n available annotated frames are contained in the video, we can split the frames into [n/l] + 1 segments, which means annotated frames {F (i-1) * l+1 , ..., F (i-1) * l+l } are contained in the i-th segment. After getting the visual features from the i-th segment f m i , three different temporal encoders including GRU, LSTM and transformer encoder are used to capture the temporal information in the video.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Gru-Based Temporal Encoder",
      "text": "We use a Gate Recurrent Unit Network (GRU) to encode the temporal information of the image sequence. Segment s i means the i-th segment, and f m i means the input of GRU is the visual features for s i . Furthermore, the hidden states of the last layer are fed from the previous segment s i-1 into the GRU to utilize the information from the last segment.\n\nwhere h i denotes the hidden states at the end of s i . h 0 is initialized to be zeros. To ensure that the last frame of s i-1 and the first frame of segment s i are consecutive frames, there is no overlap between the two adjacent segments.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Lstm-Based Temporal Encoder",
      "text": "We employ a Long Short-Term Memory Network (LSTM) to model the sequential dependencies in the video. It can be formulated as follows:\n\nThe symbols have the same meaning as in the GRU part.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Transformer-Based Temporal Encoder",
      "text": "We utilize a transformer encoder to model the temporal information in the video segment as well, which can be formulated as follows:\n\nUnlike GRU and LSTM, the transformer encoder just models the context in a single segment and ignores the dependencies of frames between segments.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Single Task Loss Function",
      "text": "We first introduce the loss function for each task in this subsection.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Valence And Arousal Estimation Task :",
      "text": "We utilize the Mean Squared Error (MSE) loss which can be formulated as\n\nwhere N denotes the number of frames in each batch, ŷi and y i denote the prediction and label of valence or arousal in each batch respectively.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Expression Classification Task :",
      "text": "We utilize the Cross Entropy (CE) loss which can be formulated as\n\nwhere C is equal to 8 which denotes the total classification number of all expression, ŷij and y ij denote the prediction and label of expression in each batch.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Au Classification Task :",
      "text": "We utilize Binary Cross Entropy (BCE) loss which can be formulated as\n\nwhere M is equal to 12 which denotes the total number of facial action units, ŷij and y ij denote the logits and label of facial action units in each batch.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Multi-Task Learning Framework",
      "text": "As we mentioned above, the overall estimation objectives can be divided into four tasks, including the estimation of valence, arousal, expression and action units on expressive facial images. These four objectives focus on different information on the facial images, where the essential information about one task may be helpful to the modeling of some other tasks. The dependencies between tasks are manifested mainly in two aspects: First, the low-level representations are common for some tasks and they can be shared to benefit each task. Second, some high-level task-specific information of one task could be important features for other tasks. For example, since the definition of expressions depends on facial action units to some extent, the high-level features in the AU detection task can help the estimation of expression.\n\nIn order to make use of such dependencies between different tasks, we make some efforts on the multi-task learning frameworks instead of the single-task models. Specifically, we propose three multi-task learning frameworks, as illustrated in Figure  2 .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Share Encoder",
      "text": "We propose the Share Encoder (SE) framework as the baseline, which is commonly used in the field of multi-task learning. In the SE framework, the temporal encoder is directly shared between different tasks, while each task retains task-specific regression or classification layers. The structure of the SE framework is shown in Figure  2 (a), which can be formulated as follows:\n\nwhere TE denotes the temporal encoder, T denotes the collection of chosen tasks in the multi-task learning framework, t denotes a specific task in {v, a, e, au}, y t i denotes the predictions of task t of segment s i , W t p and b t p denote the parameters to be optimized. Share Bottom of Encoder Under the assumption that the bottom layers of the encoder capture more basic information in facial images while the top layers encode more task-specific features, we propose to only share the bottom layers of the temporal encoder between different tasks. The structure of the Share Bottom of Encoder (SBE) framework is shown in Figure  2 (b), which can be formulated as follows:\n\nwhere TE denotes the temporal encoder, t denotes a specific task and T denotes the collection of chosen tasks, T E t denotes the task-specif temporal encoder of task t, y t i denotes the predictions of task t of segment s i , W t p and b t p denote the parameters to be optimized.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Share Bottom Of Encoder With Hidden States Feedback",
      "text": "Although the proposed SBE framework has captured the low-level shared information between different tasks, it might ignore the high-level task-specific dependencies of tasks. In order to model such high-level dependencies, we propose the Share Bottom of Encoder with Hidden States Feedback (SBE-HSF) framework, as illustrated in Figure  2(c ). In the SBE-HSF framework, all the tasks share the bottom layers of the temporal encoder and retain task-specific top layers, as in the SBE framework.\n\nAfterward, considering that the information of one task could benefit the estimation of another task, we feed the last hidden states of the temporal encoder of the source task into the temporal encoder of the target task as features. It can be formulated as follows:\n\nwhere TE denotes the temporal encoder, t denotes a specific task and T denotes the collection of chosen tasks, src and tgt denote the source and target task of the feedback structure, respectively, T E t denotes the task-specif temporal encoder of task t, y t i denotes the predictions of task t of segment s i , W t p and b t p denote the parameters to be optimized. In addition, in the backward propagation stage, the gradient of g src i is detached.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Multi-Task Loss Function",
      "text": "In the multi-task learning framework, we utilize the multi-task loss function to optimize the model, which combines the loss functions of all tasks chosen for multi-task learning:\n\nwhere t denotes a specific task and T denotes the collection of chosen tasks, L t denotes the loss function of task t, which is mentioned above, α t denotes the weight of L t which is a hyper-parameter.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Dataset",
      "text": "The Multi-Task Learning (MTL) Challenge in the fourth ABAW competition  [12]  uses the s-Aff-Wild2 dataset as the competition corpora, which is the static version of the Aff-Wild2  [19]  database and contains some specific frames of the Aff-Wild2 database.\n\nAs for feature extractors, the FER+  [2] , RAF-DB  [24, 23] , AffectNet  [26] , C-MS-Celeb  [10]  and EmotionNet  [3]  datasets are used for pre-training. In addition, an authorized commercial FAU dataset is also used to pre-train the visual feature extractor. It contains 7K images in 15 face action unit categories(AU1, AU2, AU4, AU5, AU6, AU7, AU9, AU10, AU11, AU12, AU15, AU17, AU20, AU24, and AU26).",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiment Setup",
      "text": "As for the training setting, we use Nvidia GeForce GTX 1080 Ti GPUs to train the models, and the optimizer is the Adam  [11] . The number of epochs is 50, the dropout rate of the temporal encoder and the FC layers is 0.3, the learning rate is 0.00005, the length of video segments is 250 for arousal and 64 for valence, expression and AU, and the batch size is 8.\n\nAs for the model architecture, the dimension of the feed-forward layers or the size of hidden states is 1024, the number of FC layers is 3 and the sizes of hidden states are {512, 256}. Specially, the encoder of transformer has 4 layers and 4 attention heads.\n\nAs for the smooth strategy, we search for the best window of valence and arousal for each result based on the performance on the validation set. Most window lengths are 5 and 10.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Overall Results On The Validation Set",
      "text": "In this section, we will demonstrate the overall experimental results of our proposed method for the valence, arousal, expression and action unit estimation tasks. Specifically, the experimental results are divided into three parts, including the single-task results, the exploration of multi-task dependencies and the results of multi-task learning frameworks. We report the average performance of 3 runs with different random seeds.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Single-Task Results",
      "text": "In order to verify the performance of our proposed model without utilizing the multi-task dependencies, we conduct several single-task experiments. The results are demonstrated in Table  1 .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Results Of Multi-Task Learning Frameworks",
      "text": "We try different task combinations and apply the best task combination to the multi-task learning frameworks for each task. As a result, we find the best task combinations as follows:\n\n{V, EXPR} for valence, {V, A, AU} for arousal, {V, EXPR} for expression and {V, AU} for action unit. The experimental results of our proposed multi-task learning frameworks and the comparison with single-task models are shown in Table  2 . Specifically, the combination of features is the same as that in singletask settings, and the set of tasks chosen for the multi-task learning frameworks is based on the multi-task dependencies, which have been explored above.\n\nAs is shown in the table, first, all of our proposed multi-task frameworks outperform the single-task models on valence, expression and action unit estimation tasks. On the arousal estimation task, only the SE framework performs inferior to the single-task model and the other two frameworks outperform it. These results show that our proposed multi-task learning frameworks can improve performance and surpass the single-task models.\n\nMoreover, the two proposed frameworks, SBE and SBE-HSF, show the advanced performance, where the former is an improvement on the SE framework and the latter is an improvement on the former. The SBE framework outperforms the SE frameworks, and the SBE-HSF framework outperforms the SBE framework on arousal, expression and action unit estimation tasks. It indicates our proposed multi-task learning framework can effectively improve performance.  We evaluate the proposed methods for the valence and arousal prediction task on the validation set. As is shown in the Table  3  and Table 4 , the best performance for valence is achieved by transformer-based model, and the best performance for arousal is achieved by LSTM-based model and the GRU-based  Table  5  shows the results on the validation set for expression prediction. As is shown in the table, the transformer-based model can achieve the best performance for expression and the ensemble result can achieve 0.5090 on the validation set. We use the vote strategy for expression ensemble, and we choose the class with the least number in the training set when the number of classes with the most votes is more than one. Table  6  shows the results on the validation set for AU prediction. As is shown in the table, the transformer-based model and LSTM-based model can achieve excellent performance for AU and the ensemble result can achieve 0.5664 on the validation set. We try two ensemble types for AU. The first is the vote strategy, and we predict 1 when 0 and 1 have the same number of votes. The second is averaging the probabilities from different models for each AU and search the best threshold based on the performance on the validation set for the final prediction. 6-fold cross-validation is also conducted for avoiding overfitting on the validation set. After analyzing the dataset distribution, we find the training set is about five times the size of the validation set, so we divide the training set into five folds, and each fold has approximately the same video number and frame number as the validation set. The validation set can be seen as the 6th fold. The feature set {MAE, ires100, fau, DenseNet} and the transformer-based structure are chosen for valence, expression and AU prediction. The feature set {MAE, ires100, fau, DenseNet} and the LSTM-based structure are chosen for arousal prediction. Note that we have features fine-tuned on the s-Aff-Wild2 dataset, which may interfere with the results of the corresponding task, so we remove the features fine-tuned on the s-Aff-Wild2 dataset for corresponding 6-fold crossvalidation experiments. The results are shown in Table  7 .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Results On The Test Set",
      "text": "We will briefly explain our submission strategies and show the test results of them, which are demonstrated in table  8 .\n\nWe only use a simple strategy for the 1st and 2nd submissions, which means we train models on the training set using the features we extract, and choose the models of best epochs for different tasks. Specifically, only several models are chosen to ensemble to prevent lowering the result and we use vote strategy for expression and AU ensemble for the 1st submission. Furthermore, more models are used to ensemble and we choose the best ensemble strategy to pursue the highest performance on the validation set for 2nd submission.\n\nFurther, we use two carefully designed strategies for the 3rd and 5th submissions, including Train-Val-Mix and 6-Fold. Specifically, the Train-Val-Mix As is shown in the Table8, the 6-Fold strategy achieves the best performance on the test set, and the 1st ensemble strategy also achieves competitive performance.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we introduce our framework for the Multi-Task Learning (MTL) Challenge of the 4th Affective Behavior Analysis in-the-wild (ABAW) competition. Our method utilizes visual information and uses three different sequential models to capture the sequential information. And we also explore three multitask framework strategies using the relations of different tasks. In addition, the smooth method and ensemble strategies are used to get better performance. Our method achieves the performance of 1.7607 on the validation dataset and 1.4361 on the test dataset, ranking first in the MTL Challenge.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The pipeline of our method for the challenge.",
      "page": 3
    },
    {
      "caption": "Figure 2: Share Encoder We propose the Share Encoder (SE) framework as the baseline,",
      "page": 5
    },
    {
      "caption": "Figure 2: Our proposed multi-task learning frameworks.",
      "page": 6
    },
    {
      "caption": "Figure 2: (a), which can be formulated as follows:",
      "page": 6
    },
    {
      "caption": "Figure 2: (b), which can be formulated",
      "page": 6
    },
    {
      "caption": "Figure 2: (c). In the SBE-HSF framework, all the tasks share the bottom lay-",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Table1.Theperformanceofourproposedmethodonthevalidationsetforeachsingle",
      "data": [
        {
          "Model": "Transformer",
          "Task": "Transformer Valence MAE,ires100,fau,DenseNet\nTransformer Arousal MAE,ires100,fau,DenseNet\nTransformer EXPR MAE,ires100,fau,DenseNet\nAU",
          "Features": "MAE,ires100,fau,DenseNet",
          "Performance": "0.6414\n0.6053\n0.4310\n0.4994"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 2: Specifically, the combination of features is the same as that in single-",
      "data": [
        {
          "Valence": "Tasks",
          "Arousal": "Tasks",
          "EXPR": "Tasks",
          "AU": "Tasks"
        },
        {
          "Valence": "V",
          "Arousal": "A",
          "EXPR": "EXPR",
          "AU": "AU"
        },
        {
          "Valence": "V, EXPR",
          "Arousal": "0.6529 V, A, AU 0.5989",
          "EXPR": "V, EXPR",
          "AU": "V, AU"
        },
        {
          "Valence": "",
          "Arousal": "",
          "EXPR": "V, EXPR",
          "AU": "V, AU"
        },
        {
          "Valence": "Src: V",
          "Arousal": "Src: V,AU",
          "EXPR": "Src: EXPR",
          "AU": "Src: V\nTgt: AU"
        },
        {
          "Valence": "Tgt: EXPR",
          "Arousal": "Tgt: A",
          "EXPR": "Tgt: V",
          "AU": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 4: The single model results and ensemble result on the validation set for the",
      "data": [
        {
          "Model": "LSTM\nLSTM\nGRU\nGRU",
          "Features": "MAE,ires100,fau,DenseNet V,A,AU\nMAE,ires100,fau,DenseNet V,A,AU\nMAE,ires100,fau,DenseNet V,A,AU\nMAE,ires100,DenseNet",
          "Loss": "V,A,AU",
          "Arousal-CCC": "0.6384\n0.6354\n0.6292\n0.6244"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 4: The single model results and ensemble result on the validation set for the",
      "data": [
        {
          "Model": "LSTM\nLSTM\nLSTM",
          "Features": "Transformer MAE,ires100,fau,DenseNet\nTransformer MAE,ires100,fau,DenseNet\nTransformer MAE,ires100,fau,DenseNet V,A,AU\nMAE,ires100,fau,DenseNet\nMAE,ires100,fau,DenseNet\nMAE,ires100,DenseNet",
          "Loss": "V,AU\nV,AU\nV,AU\nV,AU\nV,AU",
          "Threshold AU-F1": "0.5\n0.5\n0.5\n0.5\n0.5\n0.5"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 7: Theresultsofthe6-foldcross-validationexperiments.Thefirstfivefoldsare",
      "data": [
        {
          "Valence Arousal EXPR\nAU": "0.6742\n0.6663\n0.5681\n0.6597\n0.6784\n0.6536\n0.6706\n0.6169\n0.7015\n0.6707\n0.6672\n0.6290",
          "PM T L": "0.4013 0.5558 1.6274\n0.3673 0.5496 1.5306\n0.3327 0.5977 1.5963\n0.3851 0.5886 1.6275\n0.4389 0.5409 1.6658\n0.4156 0.5149 1.5786"
        },
        {
          "Valence Arousal EXPR\nAU": "0.6600\n0.6494",
          "PM T L": "0.3901 0.5579 1.6027"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 8: The results of different submission strategies on the test set.",
      "data": [
        {
          "Submit": "1\n2\n3\n4\n5",
          "Strategy": "Ensemble 1\nEnsemble 2\nTrain-Val-Mix 1.3717\nEnsemble 3\n6-Fold",
          "PM T L": "1.4105\n1.3189\n1.3453\n1.4361"
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Partial fc: Training 10 million identities on a single machine",
      "authors": [
        "X An",
        "X Zhu",
        "Y Gao",
        "Y Xiao",
        "Y Zhao",
        "Z Feng",
        "L Wu",
        "B Qin",
        "M Zhang",
        "D Zhang"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "2",
      "title": "Training deep networks for facial expression recognition with crowd-sourced label distribution",
      "authors": [
        "E Barsoum",
        "C Zhang",
        "C Canton Ferrer",
        "Z Zhang"
      ],
      "year": "2016",
      "venue": "ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "3",
      "title": "Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild",
      "authors": [
        "C Benitez-Quiroz",
        "R Srinivasan",
        "A Martínez"
      ],
      "year": "2016",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition",
      "doi": "10.1109/CVPR.2016.600"
    },
    {
      "citation_id": "4",
      "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "authors": [
        "J Chung",
        "C Gulcehre",
        "K Cho",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "arxiv": "arXiv:1412.3555"
    },
    {
      "citation_id": "5",
      "title": "Facial action unit intensity estimation via semantic correspondence learning with dynamic graph convolution",
      "authors": [
        "Y Fan",
        "J Lam",
        "V Li"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "6",
      "title": "Masked autoencoders are scalable vision learners",
      "authors": [
        "K He",
        "X Chen",
        "S Xie",
        "Y Li",
        "P Dollár",
        "R Girshick"
      ],
      "year": "2021",
      "venue": "Masked autoencoders are scalable vision learners"
    },
    {
      "citation_id": "7",
      "title": "Densenet: Implementing efficient convnet descriptor pyramids",
      "authors": [
        "F Iandola",
        "M Moskewicz",
        "S Karayev",
        "R Girshick",
        "T Darrell",
        "K Keutzer"
      ],
      "year": "2014",
      "venue": "Densenet: Implementing efficient convnet descriptor pyramids",
      "arxiv": "arXiv:1404.1869"
    },
    {
      "citation_id": "8",
      "title": "Facial action unit detection with transformers",
      "authors": [
        "G Jacob",
        "B Stenger"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "9",
      "title": "Model level ensemble for facial action unit recognition at the 3rd abaw challenge",
      "authors": [
        "W Jiang",
        "Y Wu",
        "F Qiao",
        "L Meng",
        "Y Deng",
        "C Liu"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "10",
      "title": "A community detection approach to cleaning extremely large face database. Computational intelligence and neuroscience",
      "authors": [
        "C Jin",
        "R Jin",
        "K Chen",
        "Y Dou"
      ],
      "year": "2018",
      "venue": "A community detection approach to cleaning extremely large face database. Computational intelligence and neuroscience"
    },
    {
      "citation_id": "11",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "12",
      "title": "Abaw: Learning from synthetic data & multi-task learning challenges",
      "authors": [
        "D Kollias"
      ],
      "year": "2022",
      "venue": "Abaw: Learning from synthetic data & multi-task learning challenges",
      "arxiv": "arXiv:2207.01138"
    },
    {
      "citation_id": "13",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & multi-task learning challenges",
      "authors": [
        "D Kollias"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "14",
      "title": "Photorealistic facial synthesis in the dimensional affect space",
      "authors": [
        "D Kollias",
        "S Cheng",
        "M Pantic",
        "S Zafeiriou"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV) Workshops"
    },
    {
      "citation_id": "15",
      "title": "Deep neural network augmentation: Generating faces for affect analysis",
      "authors": [
        "D Kollias",
        "S Cheng",
        "E Ververas",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "16",
      "title": "Recognition of affect in the wild using deep neural networks",
      "authors": [
        "D Kollias",
        "M Nicolaou",
        "I Kotsia",
        "G Zhao",
        "S Zafeiriou"
      ],
      "year": "2017",
      "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "17",
      "title": "Distribution matching for heterogeneous multi-task learning: a large-scale face study",
      "authors": [
        "D Kollias",
        "V Sharmanska",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "Distribution matching for heterogeneous multi-task learning: a large-scale face study",
      "arxiv": "arXiv:2105.03790"
    },
    {
      "citation_id": "18",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "B Schuller",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "19",
      "title": "Aff-wild2: Extending the aff-wild database for affect recognition",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2018",
      "venue": "Aff-wild2: Extending the aff-wild database for affect recognition"
    },
    {
      "citation_id": "20",
      "title": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "21",
      "title": "Va-stargan: Continuous affect generation",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "International Conference on Advanced Concepts for Intelligent Vision Systems"
    },
    {
      "citation_id": "22",
      "title": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "arxiv": "arXiv:2103.15792"
    },
    {
      "citation_id": "23",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for unconstrained facial expression recognition",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "24",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild",
      "authors": [
        "S Li",
        "W Deng",
        "J Du"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "25",
      "title": "Valence and arousal estimation based on multimodal temporal-aware features for videos in the wild",
      "authors": [
        "L Meng",
        "Y Liu",
        "X Liu",
        "Z Huang",
        "W Jiang",
        "T Zhang",
        "C Liu",
        "Q Jin"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "26",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "27",
      "title": "An ensemble approach for facial expression analysis in video",
      "authors": [
        "H Nguyen",
        "V Huynh",
        "S Kim"
      ],
      "year": "2022",
      "venue": "An ensemble approach for facial expression analysis in video",
      "arxiv": "arXiv:2203.12891"
    },
    {
      "citation_id": "28",
      "title": "An overview of multi-task learning in deep neural networks",
      "authors": [
        "S Ruder"
      ],
      "year": "2017",
      "venue": "An overview of multi-task learning in deep neural networks"
    },
    {
      "citation_id": "29",
      "title": "Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition",
      "authors": [
        "H Sak",
        "A Senior",
        "F Beaufays"
      ],
      "year": "2014",
      "venue": "Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition",
      "arxiv": "arXiv:1402.1128"
    },
    {
      "citation_id": "30",
      "title": "Affective computing: A review",
      "authors": [
        "J Tao",
        "T Tan"
      ],
      "year": "2005",
      "venue": "Affective Computing and Intelligent Interaction, First International Conference, ACII 2005",
      "doi": "10.1007/11573548_125"
    },
    {
      "citation_id": "31",
      "title": "Attention is all you need. Advances in neural information processing systems",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Attention is all you need. Advances in neural information processing systems"
    },
    {
      "citation_id": "32",
      "title": "Distract your attention: multi-head cross attention network for facial expression recognition",
      "authors": [
        "Z Wen",
        "W Lin",
        "T Wang",
        "G Xu"
      ],
      "year": "2021",
      "venue": "Distract your attention: multi-head cross attention network for facial expression recognition",
      "arxiv": "arXiv:2109.07270"
    },
    {
      "citation_id": "33",
      "title": "Aff-wild: Valence and arousal 'in-the-wild'challenge",
      "authors": [
        "S Zafeiriou",
        "D Kollias",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "I Kotsia"
      ],
      "year": "2017",
      "venue": "Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "34",
      "title": "Continuous emotion recognition using visual-audio-linguistic information: A technical report for abaw3",
      "authors": [
        "S Zhang",
        "R An",
        "Y Ding",
        "C Guan"
      ],
      "year": "2022",
      "venue": "Continuous emotion recognition using visual-audio-linguistic information: A technical report for abaw3",
      "arxiv": "arXiv:2203.13031"
    },
    {
      "citation_id": "35",
      "title": "Transformer-based multimodal information fusion for facial expression analysis",
      "authors": [
        "W Zhang",
        "F Qiu",
        "S Wang",
        "H Zeng",
        "Z Zhang",
        "R An",
        "B Ma",
        "Y Ding"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    }
  ]
}