{
  "paper_id": "2306.07115v1",
  "title": "Exploring Attention Mechanisms For Multimodal Emotion Recognition In An Emergency Call Center Corpus",
  "published": "2023-06-12T13:43:20Z",
  "authors": [
    "Théo Deschamps-Berger",
    "Lori Lamel",
    "Laurence Devillers"
  ],
  "keywords": [
    "real-life emotional corpus",
    "emergency call center",
    "speech emotion recognition",
    "Transformer-based encoders",
    "cross-attention fusion",
    "late fusion",
    "model-level fusion"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The emotion detection technology to enhance human decision-making is an important research issue for real-world applications, but real-life emotion datasets are relatively rare and small. The experiments conducted in this paper use the CEMO, which was collected in a French emergency call center. Two pre-trained models based on speech and text were fine-tuned for speech emotion recognition. Using pre-trained Transformer encoders mitigates our data's limited and sparse nature. This paper explores the different fusion strategies of these modality-specific models. In particular, fusions with and without cross-attention mechanisms were tested to gather the most relevant information from both the speech and text encoders. We show that multimodal fusion brings an absolute gain of 4-9% with respect to either single modality and that the Symmetric multi-headed cross-attention mechanism performed better than late classical fusion approaches. Our experiments also suggest that for the real-life CEMO corpus, the audio component encodes more emotive information than the textual one.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In an emergency call center, an agent has to make quick decisions that can have great consequences. Automatic emotion recognition is a key component of human decision makings. The corpus used in this research (CEMO) was collected in a French emergency call center. In application-specific data, emotions are scarce accounting from 10% (banking context)  [1]  to 30% (emergency call context) of speech turns/segments  [2] . In this study we used and fine-tuned two pre-trained Transformer encoders, trained on French data: LeBenchmark wav2vec2  [3]  which is based on the wav2vec2 model designed to be trained on audio  [4] ; and FlauBERT  [5] , trained on text and based on the BERT model  [6] . These models have demonstrated their capabilities to produce relevant representations for several downstream tasks  [7, 8] . Transformers encoder-decoder  [9]  have triggered novel ap-proaches, thanks to the self-attention mechanism  [10]  and to the cross-attention mechanism  [11]  that mixes two different embedding sequences which may come from different modalities. The attention mechanism has been used in speech emotion recognition with frame-level speech features  [12]  and with knowledge transfer  [13] . The non-sequential characteristic of the attention mechanism enables Transformer to better capture long dependencies compared to Long-Short Term Memory. Specifically, Transformer encoders have been widely used to create domain-relevant representations in sequence processing. They were naturally first used on Written Language Processing tasks, particularly with language representation models such as BERT  [6]  designed to serve as a pre-trained core model. It is trained in a self-supervised mode on huge amounts of data with the aim of being able to finetune it to specific applications and low-resource domains. In our case, the data are telephone conversations (speech and transcriptions) between call center agents and patients or relatives. In this real data recording context, data exhibiting emotion are rare and often have a wide diversity in the manner it is expressed: sometimes in the voice characteristics (prosody, fluency), in the choice of words, or a combination of both. Multimodal deep learning is well-suited to handling this problem and has garnered much interest for emotion detection  [14, 15, 16, 17] : The paralinguistic representation provides low-level emotion cues, and the semantic features bring content and context to the sentence. The flexibility of deep learning systems supports several multi-modal fusion levels: early  [18, 19] , intermediate  [20, 21, 22] , and late  [23, 24, 25] . In this paper, we focus on late fusion, which introduces the multimodal information in the later layers of the network, allowing the earlier layers to specialize the unimodal pattern learning and extraction. We compare three late fusions strategies: the first is a score average of each specific-modality encoder (Score fusion)  [23] ; the second is a shared neural network fed by the concatenation of both encoders outputs (Concatenation fusion)  [20, 26]  and the third is a Symmetric multi-head cross-attention fusion which takes in context the semantic and the paralinguistic representations to weight the final output. A similar multi-head Symmetric cross-attention method was used in a multimodal transformer for acoustics and vision in  [27]  and for speech emotion recog- Parameter fine-tuning of the wav2vec2 and FlauBERT transformer layers. The Convolutional and Embedding layers are frozen. The wav2vec2 and the FlauBERT produce paralinguistic (H p ) and semantic representations (H s ), respectively. nition with MFCC and GloVe embeddings  [28] . This paper extends our previous studies on the adaptation of Transformer encoders for speech emotion recognition and on late multimodal fusions  [29, 30] . To the best of our knowledge, the use of Symmetric cross-attention powered by fine-tuned pretrained encoders for speech emotion recognition has not been realized before. The rest of the paper is organized as follows. The next section explains how the encoders are fine-tuned for emotion recognition. Then, we detail the fusion strategies, and present the CEMO corpus and the experimental results.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Fine-Tuned Pre-Trained Encoders",
      "text": "In prior results  [30] , several pre-trained models for speech emotion recognition were adapted with the CEMO corpus  [2] . We selected among the available models the leBenchmark model (Wav2Vec2-FR-3K)  [3]  trained on 3k hours of French and the FlauBERT model  [5]  trained on 24 French subcorpora (71Gb of text). This selection is justified by the performances on our CEMO corpus  [30] . The wav2vec2-FR-3K model includes in its training database some spontaneous dialogues, telephone-recorded data, and emotional content, which have similar characteristics to the CEMO corpus. The FlauBERT model applies a basic tokenizer for French to the input sentence and embeds it with a vocabulary of 50K subword units built using the Byte Pair Encoding (BPE) algorithm  [31] . In order to adapt the wav2vec2-FR-3K and FlauBERT models, we added a classifier on top of them, adapted to our four emotional classes. The multi-head self-attention layers of the Transformer encoders were fine-tuned for speech emotion recognition with the CEMO corpus, as shown in Figure  1 , making the assumption that the first layers (Convolutional and Embedding) are reliable for this task  [7, 30] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multimodal Fusions Strategies",
      "text": "The multimodal fusion variants are applied to the outputs of both encoders, fine-tuned on speech or transcrip-tions, respectively. The wav2vec2 encoder produces a matrix composed of paralinguistic features per frame window (H p ∈ R dframes×dmodel ) while the FlauBERT encoder yields a matrix of semantic features per word (H s ∈ R dsubwords×dmodel ). For the Base configuration the encoders have d model = 768 and d model = 1024 for the Large size. We tested three methods to extract the most relevant information from both feature spaces.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Score Fusion",
      "text": "The output matrix of each encoder is averaged across the model dimension (mean(H p ) and mean(H s ) ∈ R dmodel ). The Score fusion consists of two fully connected parallel layers processing the average outputs of the two encoders.\n\nwhere W p and W s ∈ R nclasses×dmodel . The softmax outputs generated by each classifier are then averaged by class.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Concatenation Fusion",
      "text": "The Concatenation fusion consists of shared fully connected layers fed by the concatenation of the encoder's output matrices and averaged across the model dimension.\n\nwhere W c ∈ R nclasses×dmodel .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Symmetric Multi-Head Cross-Attention Fusion",
      "text": "The attention algorithm relies on the idea that each element of an input matrix can be weighted by its importance in the context. To this end, the input matrix (which we call the Key matrix) is multiplied by a context matrix (denoted as Query) and then softmax and scaled to produce an attention matrix. This attention matrix is finally multiplied by the input matrix (called Value) to produce a matrix whose elements are weighted by the context; see equation 1. In self-attention, the context matrix (Q) is equal to the input matrix (K, V ); in cross-attention, the context matrix (Q) comes from another modality.\n\nwhere d k is the dimension of queries and keys. Following Vaswani  [9] , we used multi-head attention (eq. 2) in order to increase the number of attention mechanisms on different representation subspaces, as shown in Figure  2 .\n\nwhere head = Attention And where the projections are parameter matrices:\n\nIn this work, we used 16 attention heads (h = 16) for all fusions with Base model encoders and 32 heads (h = 32) for Large encoders.\n\nTo perform cross-attention, since the dimensions of the encoder outputs for wav2vec2 and FlauBERT are different, we experimented with two alignment methods. The first method is coarsely aligning the two embedding matrices to create equivalent matrices size. To this end, the number of frames in H p is divided by the number of subwords in H s , and the aligned frames are averaged over each subword (entry #subwords in Table  4 ). In the second method, the number of frames associated with a subword depends on the number of characters in it, and in order to give more weight to longer subwords, the frames are summed rather than averaged. This is essentially equivalent to aligning at the character level and summing the frames in each character (entry #characters in Table  4 ). We experimented with three fusions: Paralinguistic, Semantic, and Symmetric multi-head cross-attention fusions. The Paralinguistic and Semantic attention fusions consist of a multi-head attention with respectively:\n\nWe propose a Symmetric multi-head cross-attention fusion, obtained by averaging the outputs of the two first multihead cross-attention, as detailed in Figure  2 . We refer to these fusions as: Paralinguistic, Semantic, and Symmetric crossattention in Table  4 .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Experimental Results",
      "text": "All experiments were assessed using a subset of the CEMO corpus described in the next subsection. The evaluation is performed on five-fold with a classical cross-speaker folding The outputs of each fold are combined for the final results.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Corpus",
      "text": "The speech corpus was collected in a French emergency call center  [2] . Data preparation is crucial to achieving good performance and robustness; here, we describe the selection of a balanced subset of the CEMO data used for model training, validation and test, as detailed in Table  1 . The selected CEMO subset (2h40) is comprised of 4224 segments, with durations ranging from 0.4 to 7.5 seconds equally distributed over the four main emotion classes: ANGER, FEAR, NEUTRAL and POSITIVE. To this end, the Fear and Neutral classes were subsampled, maintaining 1056 samples for each class for which the annotators agreed while prioritizing a broad representation of speaker diversity. The minority class Anger was completed with segments from the agents. Since the patient Anger class is composed of over 85% fine-grained segments of annoyance and impatience, the selected segments in the agents were selected from the same fine-grained labels. As seen in Table  1 , there are fewer speakers for Anger compared to the other classes. It can be noted in the Positive class has the most significant number of speakers and dialogues, potentially being richer and more heterogeneous than the other classes. Manual transcriptions of the CEMO corpus were performed by two coders, with guidelines similar to those proposed in the Amities project  [1] . The transcriptions contain about 2499 nonspeech markers, primarily pauses, breath, and other mouth noises. The vocabulary size is 2.6k, with a mean and median of about 10 words per segment (min 1, max 47).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results",
      "text": "Table  2  provides the unimodal results, which serve as a baseline for this study. During training, we use cross-entropy loss, Adam optimization  [32] , with a fixed learning rate of 10 -5 , and a mini-batch of size 8. For encoder fine-tuning, due to a large number of parameters, we used gradient norm clipping of 1, to facilitate learning stability, improving our prior results for Large encoders  [30] .\n\nBoth sized audio encoder configurations give an absolute gain of 4-5% with respect to the fine-tuned FlauBERT encoders, (c.f. Table  2 ). This result suggests that, for our reallife CEMO corpus, the paralinguistic model encodes more emotive information than the textual one. The baseline fusion methods, Multimodal Score and Concatenation, with both encoder sizes (Base and Large) outperform the single modality encoders (compare Tables  2  and 3 ). In Table  4 , we explored the cross-attention mechanisms with three attention-based fusion strategies for the last fusion. The Paralinguistic crossattention fusion almost always performed better than the Semantic fusion with both alignment strategies. The better results of the unimodal wav2vec2 compared to the FlauBERT encoder show that the paralinguistic encoder produces relevant features for speech emotion recognition. In addition to including acoustic information, the wav2vec2 encoder also implicitly includes lexical information. The encoder size influences the multimodal fusion performance. The Base and Large unimodal fine-tuned encoders have similar results for the Paralinguistic encoders (around 0.7% difference in Table  2 ), and the same results with the Semantic encoders. However, the Large configuration yielded a 2.9% improvement over the Base model for the Symmetric cross-attention fusion method with alignment based on subwords, as shown in Table 4. For a given condition (fusion method and model size), the #subwords alignment method generally performs slightly better than the #characters alignment.\n\nThe CEMO corpus contains data samples from 812 speakers, expressive voices, and nonspeech markers comprising sentences' most frequent emotional cues. The corpus has a vocabulary of only 2600 different words, of which only a few predict emotions. In particular, Fear and Anger are poorly detected by the Semantic models. These patterns make the fusion more complex. The Symmetric cross-attention fusion outperformed single modality encoders with both alignment and encoder sizes and performed slightly better than the classical fusion Score and Concatenation fusions. The best performance of the Symmetric cross-attention multimodal fusion is 79.3% UA compared with 74.5% UA (unimodal fine-tuned wav2vec2) or 70.1% UA (unimodal fine-tuned FlauBERT).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "This paper explored different fusion mechanisms in a multimodal context with real-life recordings from a French emergency call center. In this application, the agent aims to gather as much objective information about the situation as possible to make an informed decision. At the same time, the patient is more concerned with explaining the emergency, their pain, or their anxiety. These characteristics make the detection of emotion from speech very complex, as the emotion cues can be present in audio and/or lexical information. The multimodal fusions were powered by unimodal pre-trained encoders that were fine-tuned for speech emotion recognition using the CEMO corpus. Three primary multimodal fusions were tested; Score, Concatenation, and Symmetric crossattention. The best emotion recognition results for CEMO were obtained using a Symmetric fusion associated with an adequate alignment. Further experiments will be carried out using transcriptions produced by an automatic speech recognizer instead of manual references.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Parameter fine-tuning of the wav2vec2 and",
      "page": 2
    },
    {
      "caption": "Figure 1: , making the assumption that the first layers (Convolutional",
      "page": 2
    },
    {
      "caption": "Figure 2: MHead(Q, K, V ) = Concat (head1, . . . , headh) W O",
      "page": 2
    },
    {
      "caption": "Figure 2: Multimodal system with Symmetric multi-head cross-",
      "page": 3
    },
    {
      "caption": "Figure 2: We refer to these",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": "† LISN - CNRS, Sorbonne University, Orsay, France"
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": "proaches,\nthanks\nto the self-attention mechanism [10] and"
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": "to the cross-attention mechanism [11] that mixes two differ-"
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": ""
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": "ent embedding sequences which may come from different"
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": ""
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": "modalities. The attention mechanism has been used in speech"
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": ""
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": "emotion recognition with frame-level\nspeech features\n[12]"
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": ""
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": "and with knowledge transfer [13]. The non-sequential char-"
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": ""
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": "acteristic of the attention mechanism enables Transformer to"
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": ""
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": "better\ncapture\nlong dependencies\ncompared to Long-Short"
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": ""
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": "Term Memory. Specifically, Transformer encoders have been"
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": ""
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": "widely used to create domain-relevant representations in se-"
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": ""
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": "quence processing. They were naturally first used on Written"
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": ""
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": "Language Processing tasks, particularly with language rep-"
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": ""
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": "resentation models such as BERT [6] designed to serve as a"
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": ""
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": "pre-trained core model. It is trained in a self-supervised mode"
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": ""
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": "on huge amounts of data with the aim of being able to fine-"
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": ""
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": "tune it\nto specific applications and low-resource domains.\nIn"
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": ""
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": "our case,\nthe data are telephone conversations\n(speech and"
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": ""
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": "transcriptions) between call center agents and patients or rel-"
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": ""
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": "atives.\nIn this\nreal data recording context, data exhibiting"
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": ""
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": "emotion are rare and often have a wide diversity in the man-"
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": ""
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": "ner\nit\nis expressed:\nsometimes\nin the voice characteristics"
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": "(prosody, fluency),\nin the choice of words, or a combination"
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": "of both. Multimodal deep learning is well-suited to handling"
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": "this problem and has garnered much interest\nfor\nemotion"
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": "detection [14, 15, 16, 17]: The paralinguistic representation"
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": "provides low-level emotion cues, and the semantic features"
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": ""
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": "bring content and context\nto the sentence. The flexibility of"
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": "deep learning systems\nsupports\nseveral multi-modal\nfusion"
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": "levels:\nearly [18, 19],\nintermediate\n[20, 21, 22],\nand late"
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": "[23, 24, 25].\nIn this paper, we focus on late fusion, which"
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": "introduces the multimodal\ninformation in the later\nlayers of"
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": "the network, allowing the earlier layers to specialize the uni-"
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": "modal pattern learning and extraction. We compare three"
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": "late fusions\nstrategies:\nthe first\nis a score average of each"
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": "specific-modality encoder (Score fusion) [23];\nthe second is"
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": "a shared neural network fed by the concatenation of both en-"
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": "coders outputs (Concatenation fusion) [20, 26] and the third"
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": "is a Symmetric multi-head cross-attention fusion which takes"
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": "in context the semantic and the paralinguistic representations"
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": "to weight\nthe final output. A similar multi-head Symmetric"
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": "cross-attention method was used in a multimodal transformer"
        },
        {
          "⋆ LISN - CNRS, Paris-Saclay University, Orsay, France": "for acoustics and vision in [27] and for speech emotion recog-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "tions,\nrespectively.\nThe wav2vec2 encoder produces a ma-": ""
        },
        {
          "tions,\nrespectively.\nThe wav2vec2 encoder produces a ma-": "trix composed of paralinguistic features per\nframe window"
        },
        {
          "tions,\nrespectively.\nThe wav2vec2 encoder produces a ma-": ""
        },
        {
          "tions,\nrespectively.\nThe wav2vec2 encoder produces a ma-": "(Hp ∈ Rdframes×dmodel) while the FlauBERT encoder yields a"
        },
        {
          "tions,\nrespectively.\nThe wav2vec2 encoder produces a ma-": ""
        },
        {
          "tions,\nrespectively.\nThe wav2vec2 encoder produces a ma-": "matrix of semantic features per word (Hs ∈ Rdsubwords×dmodel)."
        },
        {
          "tions,\nrespectively.\nThe wav2vec2 encoder produces a ma-": ""
        },
        {
          "tions,\nrespectively.\nThe wav2vec2 encoder produces a ma-": "For\nthe Base configuration the encoders have dmodel = 768"
        },
        {
          "tions,\nrespectively.\nThe wav2vec2 encoder produces a ma-": ""
        },
        {
          "tions,\nrespectively.\nThe wav2vec2 encoder produces a ma-": "and dmodel = 1024 for the Large size. We tested three meth-"
        },
        {
          "tions,\nrespectively.\nThe wav2vec2 encoder produces a ma-": "ods to extract the most relevant information from both feature"
        },
        {
          "tions,\nrespectively.\nThe wav2vec2 encoder produces a ma-": "spaces."
        },
        {
          "tions,\nrespectively.\nThe wav2vec2 encoder produces a ma-": "3.1.\nScore fusion"
        },
        {
          "tions,\nrespectively.\nThe wav2vec2 encoder produces a ma-": "The output matrix of each encoder\nis averaged across\nthe"
        },
        {
          "tions,\nrespectively.\nThe wav2vec2 encoder produces a ma-": "model dimension (mean(Hp) and mean(Hs) ∈ Rdmodel). The"
        },
        {
          "tions,\nrespectively.\nThe wav2vec2 encoder produces a ma-": "Score fusion consists of\ntwo fully connected parallel\nlayers"
        },
        {
          "tions,\nrespectively.\nThe wav2vec2 encoder produces a ma-": "processing the average outputs of the two encoders."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: Details of the CEMO subset of speech signals and",
      "data": [
        {
          "Table 1. Details of the CEMO subset of speech signals and": "manual"
        },
        {
          "Table 1. Details of the CEMO subset of speech signals and": "POS: Positive, Total: Total number of segments."
        },
        {
          "Table 1. Details of the CEMO subset of speech signals and": "CEMOs"
        },
        {
          "Table 1. Details of the CEMO subset of speech signals and": "#Speech seg."
        },
        {
          "Table 1. Details of the CEMO subset of speech signals and": "#Speakers"
        },
        {
          "Table 1. Details of the CEMO subset of speech signals and": "#Dialogues"
        },
        {
          "Table 1. Details of the CEMO subset of speech signals and": "Duration (min, s)"
        },
        {
          "Table 1. Details of the CEMO subset of speech signals and": "Duration (mean, s)"
        },
        {
          "Table 1. Details of the CEMO subset of speech signals and": "Vocabulary size"
        },
        {
          "Table 1. Details of the CEMO subset of speech signals and": "Avg. word count"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: Unimodal results with fine-tuning for speech emo-",
      "data": [
        {
          "The CEMO corpus contains data samples from 812 speak-": "expressive voices,"
        },
        {
          "The CEMO corpus contains data samples from 812 speak-": ""
        },
        {
          "The CEMO corpus contains data samples from 812 speak-": ""
        },
        {
          "The CEMO corpus contains data samples from 812 speak-": ""
        },
        {
          "The CEMO corpus contains data samples from 812 speak-": "vocabulary of only 2600 different words, of which only a few"
        },
        {
          "The CEMO corpus contains data samples from 812 speak-": ""
        },
        {
          "The CEMO corpus contains data samples from 812 speak-": ""
        },
        {
          "The CEMO corpus contains data samples from 812 speak-": ""
        },
        {
          "The CEMO corpus contains data samples from 812 speak-": ""
        },
        {
          "The CEMO corpus contains data samples from 812 speak-": ""
        },
        {
          "The CEMO corpus contains data samples from 812 speak-": "fusion more complex. The Symmetric cross-attention fusion"
        },
        {
          "The CEMO corpus contains data samples from 812 speak-": ""
        },
        {
          "The CEMO corpus contains data samples from 812 speak-": ""
        },
        {
          "The CEMO corpus contains data samples from 812 speak-": "outperformed single modality encoders with both alignment"
        },
        {
          "The CEMO corpus contains data samples from 812 speak-": "and encoder sizes and performed slightly better than the clas-"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: Unimodal results with fine-tuning for speech emo-",
      "data": [
        {
          "the Paralinguistic encoders (around 0.7% difference in Table": ""
        },
        {
          "the Paralinguistic encoders (around 0.7% difference in Table": "2), and the same results with the Semantic encoders. How-"
        },
        {
          "the Paralinguistic encoders (around 0.7% difference in Table": ""
        },
        {
          "the Paralinguistic encoders (around 0.7% difference in Table": "ever,\nthe Large configuration yielded a 2.9% improvement"
        },
        {
          "the Paralinguistic encoders (around 0.7% difference in Table": ""
        },
        {
          "the Paralinguistic encoders (around 0.7% difference in Table": "over the Base model for the Symmetric cross-attention fusion"
        },
        {
          "the Paralinguistic encoders (around 0.7% difference in Table": ""
        },
        {
          "the Paralinguistic encoders (around 0.7% difference in Table": "method with alignment based on subwords, as shown in Ta-"
        },
        {
          "the Paralinguistic encoders (around 0.7% difference in Table": ""
        },
        {
          "the Paralinguistic encoders (around 0.7% difference in Table": ""
        },
        {
          "the Paralinguistic encoders (around 0.7% difference in Table": "ble 4. For a given condition (fusion method and model size),"
        },
        {
          "the Paralinguistic encoders (around 0.7% difference in Table": "the #subwords alignment method generally performs slightly"
        },
        {
          "the Paralinguistic encoders (around 0.7% difference in Table": ""
        },
        {
          "the Paralinguistic encoders (around 0.7% difference in Table": ""
        },
        {
          "the Paralinguistic encoders (around 0.7% difference in Table": "better than the #characters alignment."
        },
        {
          "the Paralinguistic encoders (around 0.7% difference in Table": "The CEMO corpus contains data samples from 812 speak-"
        },
        {
          "the Paralinguistic encoders (around 0.7% difference in Table": "ers,\nexpressive voices,\nand nonspeech markers comprising"
        },
        {
          "the Paralinguistic encoders (around 0.7% difference in Table": ""
        },
        {
          "the Paralinguistic encoders (around 0.7% difference in Table": "sentences’ most\nfrequent emotional cues. The corpus has a"
        },
        {
          "the Paralinguistic encoders (around 0.7% difference in Table": ""
        },
        {
          "the Paralinguistic encoders (around 0.7% difference in Table": "vocabulary of only 2600 different words, of which only a few"
        },
        {
          "the Paralinguistic encoders (around 0.7% difference in Table": ""
        },
        {
          "the Paralinguistic encoders (around 0.7% difference in Table": "predict emotions.\nIn particular, Fear and Anger are poorly"
        },
        {
          "the Paralinguistic encoders (around 0.7% difference in Table": ""
        },
        {
          "the Paralinguistic encoders (around 0.7% difference in Table": "detected by the Semantic models.\nThese patterns make the"
        },
        {
          "the Paralinguistic encoders (around 0.7% difference in Table": ""
        },
        {
          "the Paralinguistic encoders (around 0.7% difference in Table": "fusion more complex. The Symmetric cross-attention fusion"
        },
        {
          "the Paralinguistic encoders (around 0.7% difference in Table": ""
        },
        {
          "the Paralinguistic encoders (around 0.7% difference in Table": ""
        },
        {
          "the Paralinguistic encoders (around 0.7% difference in Table": "outperformed single modality encoders with both alignment"
        },
        {
          "the Paralinguistic encoders (around 0.7% difference in Table": "and encoder sizes and performed slightly better than the clas-"
        },
        {
          "the Paralinguistic encoders (around 0.7% difference in Table": "sical fusion Score and Concatenation fusions. The best per-"
        },
        {
          "the Paralinguistic encoders (around 0.7% difference in Table": "formance of the Symmetric cross-attention multimodal fusion"
        },
        {
          "the Paralinguistic encoders (around 0.7% difference in Table": "is 79.3% UA compared with 74.5% UA (unimodal fine-tuned"
        },
        {
          "the Paralinguistic encoders (around 0.7% difference in Table": "wav2vec2) or 70.1% UA (unimodal fine-tuned FlauBERT)."
        },
        {
          "the Paralinguistic encoders (around 0.7% difference in Table": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: Unimodal results with fine-tuning for speech emo-",
      "data": [
        {
          "recognizer instead of manual references.": ""
        },
        {
          "recognizer instead of manual references.": ""
        },
        {
          "recognizer instead of manual references.": "Ethics and reproducibility"
        },
        {
          "recognizer instead of manual references.": ""
        },
        {
          "recognizer instead of manual references.": ""
        },
        {
          "recognizer instead of manual references.": ""
        },
        {
          "recognizer instead of manual references.": ""
        },
        {
          "recognizer instead of manual references.": "fully respected ethical"
        },
        {
          "recognizer instead of manual references.": ""
        },
        {
          "recognizer instead of manual references.": ""
        },
        {
          "recognizer instead of manual references.": "ing the anonymity of"
        },
        {
          "recognizer instead of manual references.": ""
        },
        {
          "recognizer instead of manual references.": "carried out using Pytorch on GPUs"
        },
        {
          "recognizer instead of manual references.": "Gbytes of RAM). To ensure the reproducibility of"
        },
        {
          "recognizer instead of manual references.": ""
        },
        {
          "recognizer instead of manual references.": ""
        },
        {
          "recognizer instead of manual references.": "a"
        },
        {
          "recognizer instead of manual references.": ""
        },
        {
          "recognizer instead of manual references.": "non-deterministic"
        },
        {
          "recognizer instead of manual references.": "formed using HPC resources"
        },
        {
          "recognizer instead of manual references.": ""
        },
        {
          "recognizer instead of manual references.": ""
        },
        {
          "recognizer instead of manual references.": "2022-AD011011844R1)."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "Strengthening Monomodal Emotion Recognition\nvia"
        },
        {
          "6. REFERENCES": "[1] H. Hardy, K. Baker, L. Devillers, et al.,\n“Multi-layer",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "Training with Crossmodal Emotion Embeddings,” IEEE"
        },
        {
          "6. REFERENCES": "Dialogue Annotation for Automated Multilingual Cus-",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "Trans. on Affect. Comput., 2021."
        },
        {
          "6. REFERENCES": "tomer Service,” ISLE, 2003.",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "[18] M. Wimmer, B. Schuller, D. Arsic, et al., Low-Level Fu-"
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "sion of Audio, Video Feature for Multi-Modal Emotion"
        },
        {
          "6. REFERENCES": "[2] L. Devillers, L. Vidrascu, and L. Lamel, “Challenges in",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "Recognition., 2008."
        },
        {
          "6. REFERENCES": "real-life emotion annotation and machine learning based",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "detection,” Neural networks: INNS, 2005.",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "[19] C. Busso, S. Yildirim, M. Bulut, et al., Analysis of Emo-"
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "tion Recognition Using Facial Expressions, Speech and"
        },
        {
          "6. REFERENCES": "[3] S. Evain, H. Nguyen, H. Le, et al.,\n“LeBenchmark: A",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "Multimodal Inform., 2004."
        },
        {
          "6. REFERENCES": "Reproducible Framework for Assessing Self-Supervised",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "INTER-\nRepresentation Learning\nfrom Speech,”\nin",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "[20]\nJ. Han, Z. Zhang, N. Cummins,\net\nal.,\n“Strength"
        },
        {
          "6. REFERENCES": "SPEECH, 2021.",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "Modelling for Real-World Automatic Continuous Affect"
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "Image and Vi-\nRecognition from Audiovisual Signals,”"
        },
        {
          "6. REFERENCES": "[4] A. Baevski, Y. Zhou, A. Mohamed, et al., “Wav2vec 2.0:",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "sion Computing, pp. 76–86, 2017."
        },
        {
          "6. REFERENCES": "A Framework for Self-Supervised Learning of Speech",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "Inform. Pro-\nRepresentations,”\nin Advances in Neural",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "[21]\nJ.-B. Delbrouck, N. Tits, and S. Dupont,\n“Modulated"
        },
        {
          "6. REFERENCES": "cess. Systems, 2020.",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "Fusion using Transformer for Linguistic-Acoustic Emo-"
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "the First\nInternational\ntion Recognition,”\nin Proc. of"
        },
        {
          "6. REFERENCES": "[5] H. Le, L. Vial, J. Frej, et al., “FlauBERT: Unsupervised",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "Workshop on Natural Lang. Process. Beyond Text, 2020,"
        },
        {
          "6. REFERENCES": "Lang. Model Pre-training for French,” in Twelfth Lang.",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "pp. 1–10."
        },
        {
          "6. REFERENCES": "Resources and Evaluation Conf., May 2020.",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "[22] Z. Liu, Y. Shen, V. B. Lakshminarasimhan, et al.,\n“Ef-"
        },
        {
          "6. REFERENCES": "[6]\nJ. Devlin, M.-W. Chang, K. Lee, et al.,\n“BERT: Pre-",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "ficient Low-rank Multimodal Fusion With Modality-"
        },
        {
          "6. REFERENCES": "training of Deep Bidirectional Transformers for Lang.",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "Specific Factors,”\nin Proc. of\nthe 56th ACL, 2018, pp."
        },
        {
          "6. REFERENCES": "Understanding,”\nin NAACL: Human Lang. Technolo-",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "2247–2256."
        },
        {
          "6. REFERENCES": "gies, 2019.",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "[23] Z. Zeng, M. Pantic, and G. Roisman,\n“A Survey of Af-"
        },
        {
          "6. REFERENCES": "[7]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, et al.,",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "fect Recognition Methods: Audio, Visual, and Sponta-"
        },
        {
          "6. REFERENCES": "Dawn of the Transformer Era in Speech Emotion Recog-",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "neous Expressions,” IEEE transactions on pattern anal-"
        },
        {
          "6. REFERENCES": "nition: Closing the Valence Gap, 2022.",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "IEEE Trans. on Pattern\nysis and machine intelligence,"
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "Analysis and Machine Int., 2009."
        },
        {
          "6. REFERENCES": "[8] F. Acheampong, H. Nunoo-Mensah,\nand W. Chen,",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "[24] N.-H. Ho, H.-J. Yang,\nS.-H. Kim,\net\nal.,\n“Multi-"
        },
        {
          "6. REFERENCES": "Transformer Models for Text-based Emotion Detection:",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "modal Approach of Speech Emotion Recognition Using"
        },
        {
          "6. REFERENCES": "A Review of BERT-based Approaches, AI Review, 2021.",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "Multi-Level Multi-Head Fusion Attention-Based Recur-"
        },
        {
          "6. REFERENCES": "[9] A. Vaswani, N. Shazeer, N. Parmar, et al., “Attention is",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "rent Neural Network,” IEEE Access, 2020."
        },
        {
          "6. REFERENCES": "All you Need,” in Advances in Neural Inform. Process.",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "[25] P. Tzirakis, J. Chen, S. Zafeiriou, and B. Schuller, “End-"
        },
        {
          "6. REFERENCES": "Systems, 2017.",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "to-end multimodal affect recognition in real-world envi-"
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "ronments,” Inform. Fusion, 2021."
        },
        {
          "6. REFERENCES": "[10]\nJ. Cheng, L. Dong, and M. Lapata,\n“Long Short-Term",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "Memory-Networks for Machine Reading,”\nin Proc. of",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "[26]\nJ. Han, Z. Zhang, F. Ringeval, et al.,\n“Prediction-based"
        },
        {
          "6. REFERENCES": "Empirical Methods in Natural Lang. Process., 2016.",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "learning for continuous emotion recognition in speech,”"
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "in IEEE ICASSP, 2017."
        },
        {
          "6. REFERENCES": "[11] D. Bahdanau, K. H. Cho, and Y. Bengio,\n“Neural ma-",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "chine trans. by jointly learning to align and translate,” in",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "[27] A. Nagrani, S. Yang, A. Arnab, et al.,\n“Attention Bot-"
        },
        {
          "6. REFERENCES": "ICLR, 2015.",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "tlenecks for Multimodal Fusion,” in Advances in Neural"
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "Inform. Process. Systems, 2021."
        },
        {
          "6. REFERENCES": "[12] Y. Xie, R. Liang, Z. Liang, et al., “Speech Emotion Clas-",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "IEEE/ACM\nsification Using Attention-Based LSTM,”",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "[28] L. Sun, B. Liu, J. Tao, and Z. Lian, “Multimodal Cross-"
        },
        {
          "6. REFERENCES": "Transactions on Audio, Speech, and Language Process-",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "and Self-Attention Network for Speech Emotion Recog-"
        },
        {
          "6. REFERENCES": "ing, 2019.",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "nition,” in IEEE ICASSP, 2021."
        },
        {
          "6. REFERENCES": "[13] Z. Zhao, Z. Bao, Z. Zhang, et al., “Self-attention trans-",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "[29] T. Deschamps-Berger, L. Lamel, and L. Devillers, “End-"
        },
        {
          "6. REFERENCES": "Virtual\nfer networks for speech emotion recognition,”",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "to-End Speech Emotion Recognition:\nChallenges of"
        },
        {
          "6. REFERENCES": "Reality & Intelligent Hardware, 2021.",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "Real-Life Emergency Call Centers Data Recordings,” in"
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "ACII, 2021."
        },
        {
          "6. REFERENCES": "[14] T. Baltrusaitis, C. Ahuja, and L.-P. Morency,\n“Multi-",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "[30] T. Deschamps-Berger, L. Lamel, and L. Devillers,\n“In-"
        },
        {
          "6. REFERENCES": "modal Machine Learning: A Survey and Taxonomy,”",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "vestigating Transformer Encoders\nand Fusion Strate-"
        },
        {
          "6. REFERENCES": "IEEE Transactions on Pattern Analysis and Machine In-",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "gies for Speech Emotion Recognition in Emergency Call"
        },
        {
          "6. REFERENCES": "telligence, 2019.",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "Center Conversations.,” in ICMI, 2022."
        },
        {
          "6. REFERENCES": "[15] P.\nTzirakis,\nA. Nguyen,\nS.\nZafeiriou,\nand B. W.",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "[31] R. Sennrich, B. Haddow, and A. Birch,\n“Neural Ma-"
        },
        {
          "6. REFERENCES": "Schuller,\n“Speech Emotion Recognition Using Seman-",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "chine Trans. of Rare Words with Subword Units,”\nin"
        },
        {
          "6. REFERENCES": "tic Inform.,” in ICASSP, 2021.",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "Proc. of the 54th ACL, 2016, pp. 1715–1725."
        },
        {
          "6. REFERENCES": "[16] L. Tarantino, P. N. Garner,\nand A. Lazaridis,\n“Self-",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "[32] D. P. Kingma and L.\nJ. Ba,\n“Adam: A Method for"
        },
        {
          "6. REFERENCES": "Attention for Speech Emotion Recognition,” in INTER-",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "Stochastic Optimization,” ICLR, 2015."
        },
        {
          "6. REFERENCES": "SPEECH, 2019, pp. 2578–2582.",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "Strengthening Monomodal Emotion Recognition\nvia"
        },
        {
          "6. REFERENCES": "[1] H. Hardy, K. Baker, L. Devillers, et al.,\n“Multi-layer",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "Training with Crossmodal Emotion Embeddings,” IEEE"
        },
        {
          "6. REFERENCES": "Dialogue Annotation for Automated Multilingual Cus-",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "Trans. on Affect. Comput., 2021."
        },
        {
          "6. REFERENCES": "tomer Service,” ISLE, 2003.",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "[18] M. Wimmer, B. Schuller, D. Arsic, et al., Low-Level Fu-"
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "sion of Audio, Video Feature for Multi-Modal Emotion"
        },
        {
          "6. REFERENCES": "[2] L. Devillers, L. Vidrascu, and L. Lamel, “Challenges in",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "Recognition., 2008."
        },
        {
          "6. REFERENCES": "real-life emotion annotation and machine learning based",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "detection,” Neural networks: INNS, 2005.",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "[19] C. Busso, S. Yildirim, M. Bulut, et al., Analysis of Emo-"
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "tion Recognition Using Facial Expressions, Speech and"
        },
        {
          "6. REFERENCES": "[3] S. Evain, H. Nguyen, H. Le, et al.,\n“LeBenchmark: A",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "Multimodal Inform., 2004."
        },
        {
          "6. REFERENCES": "Reproducible Framework for Assessing Self-Supervised",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "INTER-\nRepresentation Learning\nfrom Speech,”\nin",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "[20]\nJ. Han, Z. Zhang, N. Cummins,\net\nal.,\n“Strength"
        },
        {
          "6. REFERENCES": "SPEECH, 2021.",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "Modelling for Real-World Automatic Continuous Affect"
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "Image and Vi-\nRecognition from Audiovisual Signals,”"
        },
        {
          "6. REFERENCES": "[4] A. Baevski, Y. Zhou, A. Mohamed, et al., “Wav2vec 2.0:",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "sion Computing, pp. 76–86, 2017."
        },
        {
          "6. REFERENCES": "A Framework for Self-Supervised Learning of Speech",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "Inform. Pro-\nRepresentations,”\nin Advances in Neural",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "[21]\nJ.-B. Delbrouck, N. Tits, and S. Dupont,\n“Modulated"
        },
        {
          "6. REFERENCES": "cess. Systems, 2020.",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "Fusion using Transformer for Linguistic-Acoustic Emo-"
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "the First\nInternational\ntion Recognition,”\nin Proc. of"
        },
        {
          "6. REFERENCES": "[5] H. Le, L. Vial, J. Frej, et al., “FlauBERT: Unsupervised",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "Workshop on Natural Lang. Process. Beyond Text, 2020,"
        },
        {
          "6. REFERENCES": "Lang. Model Pre-training for French,” in Twelfth Lang.",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "pp. 1–10."
        },
        {
          "6. REFERENCES": "Resources and Evaluation Conf., May 2020.",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "[22] Z. Liu, Y. Shen, V. B. Lakshminarasimhan, et al.,\n“Ef-"
        },
        {
          "6. REFERENCES": "[6]\nJ. Devlin, M.-W. Chang, K. Lee, et al.,\n“BERT: Pre-",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "ficient Low-rank Multimodal Fusion With Modality-"
        },
        {
          "6. REFERENCES": "training of Deep Bidirectional Transformers for Lang.",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "Specific Factors,”\nin Proc. of\nthe 56th ACL, 2018, pp."
        },
        {
          "6. REFERENCES": "Understanding,”\nin NAACL: Human Lang. Technolo-",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "2247–2256."
        },
        {
          "6. REFERENCES": "gies, 2019.",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "[23] Z. Zeng, M. Pantic, and G. Roisman,\n“A Survey of Af-"
        },
        {
          "6. REFERENCES": "[7]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, et al.,",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "fect Recognition Methods: Audio, Visual, and Sponta-"
        },
        {
          "6. REFERENCES": "Dawn of the Transformer Era in Speech Emotion Recog-",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "neous Expressions,” IEEE transactions on pattern anal-"
        },
        {
          "6. REFERENCES": "nition: Closing the Valence Gap, 2022.",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "IEEE Trans. on Pattern\nysis and machine intelligence,"
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "Analysis and Machine Int., 2009."
        },
        {
          "6. REFERENCES": "[8] F. Acheampong, H. Nunoo-Mensah,\nand W. Chen,",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "[24] N.-H. Ho, H.-J. Yang,\nS.-H. Kim,\net\nal.,\n“Multi-"
        },
        {
          "6. REFERENCES": "Transformer Models for Text-based Emotion Detection:",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "modal Approach of Speech Emotion Recognition Using"
        },
        {
          "6. REFERENCES": "A Review of BERT-based Approaches, AI Review, 2021.",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "Multi-Level Multi-Head Fusion Attention-Based Recur-"
        },
        {
          "6. REFERENCES": "[9] A. Vaswani, N. Shazeer, N. Parmar, et al., “Attention is",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "rent Neural Network,” IEEE Access, 2020."
        },
        {
          "6. REFERENCES": "All you Need,” in Advances in Neural Inform. Process.",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "[25] P. Tzirakis, J. Chen, S. Zafeiriou, and B. Schuller, “End-"
        },
        {
          "6. REFERENCES": "Systems, 2017.",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "to-end multimodal affect recognition in real-world envi-"
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "ronments,” Inform. Fusion, 2021."
        },
        {
          "6. REFERENCES": "[10]\nJ. Cheng, L. Dong, and M. Lapata,\n“Long Short-Term",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "Memory-Networks for Machine Reading,”\nin Proc. of",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "[26]\nJ. Han, Z. Zhang, F. Ringeval, et al.,\n“Prediction-based"
        },
        {
          "6. REFERENCES": "Empirical Methods in Natural Lang. Process., 2016.",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "learning for continuous emotion recognition in speech,”"
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "in IEEE ICASSP, 2017."
        },
        {
          "6. REFERENCES": "[11] D. Bahdanau, K. H. Cho, and Y. Bengio,\n“Neural ma-",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "chine trans. by jointly learning to align and translate,” in",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "[27] A. Nagrani, S. Yang, A. Arnab, et al.,\n“Attention Bot-"
        },
        {
          "6. REFERENCES": "ICLR, 2015.",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "tlenecks for Multimodal Fusion,” in Advances in Neural"
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "Inform. Process. Systems, 2021."
        },
        {
          "6. REFERENCES": "[12] Y. Xie, R. Liang, Z. Liang, et al., “Speech Emotion Clas-",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "IEEE/ACM\nsification Using Attention-Based LSTM,”",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "[28] L. Sun, B. Liu, J. Tao, and Z. Lian, “Multimodal Cross-"
        },
        {
          "6. REFERENCES": "Transactions on Audio, Speech, and Language Process-",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "and Self-Attention Network for Speech Emotion Recog-"
        },
        {
          "6. REFERENCES": "ing, 2019.",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "nition,” in IEEE ICASSP, 2021."
        },
        {
          "6. REFERENCES": "[13] Z. Zhao, Z. Bao, Z. Zhang, et al., “Self-attention trans-",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "[29] T. Deschamps-Berger, L. Lamel, and L. Devillers, “End-"
        },
        {
          "6. REFERENCES": "Virtual\nfer networks for speech emotion recognition,”",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "to-End Speech Emotion Recognition:\nChallenges of"
        },
        {
          "6. REFERENCES": "Reality & Intelligent Hardware, 2021.",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "Real-Life Emergency Call Centers Data Recordings,” in"
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "ACII, 2021."
        },
        {
          "6. REFERENCES": "[14] T. Baltrusaitis, C. Ahuja, and L.-P. Morency,\n“Multi-",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "[30] T. Deschamps-Berger, L. Lamel, and L. Devillers,\n“In-"
        },
        {
          "6. REFERENCES": "modal Machine Learning: A Survey and Taxonomy,”",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "vestigating Transformer Encoders\nand Fusion Strate-"
        },
        {
          "6. REFERENCES": "IEEE Transactions on Pattern Analysis and Machine In-",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "gies for Speech Emotion Recognition in Emergency Call"
        },
        {
          "6. REFERENCES": "telligence, 2019.",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "Center Conversations.,” in ICMI, 2022."
        },
        {
          "6. REFERENCES": "[15] P.\nTzirakis,\nA. Nguyen,\nS.\nZafeiriou,\nand B. W.",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "[31] R. Sennrich, B. Haddow, and A. Birch,\n“Neural Ma-"
        },
        {
          "6. REFERENCES": "Schuller,\n“Speech Emotion Recognition Using Seman-",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "chine Trans. of Rare Words with Subword Units,”\nin"
        },
        {
          "6. REFERENCES": "tic Inform.,” in ICASSP, 2021.",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "Proc. of the 54th ACL, 2016, pp. 1715–1725."
        },
        {
          "6. REFERENCES": "[16] L. Tarantino, P. N. Garner,\nand A. Lazaridis,\n“Self-",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "[32] D. P. Kingma and L.\nJ. Ba,\n“Adam: A Method for"
        },
        {
          "6. REFERENCES": "Attention for Speech Emotion Recognition,” in INTER-",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        },
        {
          "6. REFERENCES": "",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": "Stochastic Optimization,” ICLR, 2015."
        },
        {
          "6. REFERENCES": "SPEECH, 2019, pp. 2578–2582.",
          "[17]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller,\n“EmoBed:": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Multi-layer Dialogue Annotation for Automated Multilingual Customer Service",
      "authors": [
        "H Hardy",
        "K Baker",
        "L Devillers"
      ],
      "year": "2003",
      "venue": "ISLE"
    },
    {
      "citation_id": "3",
      "title": "Challenges in real-life emotion annotation and machine learning based detection",
      "authors": [
        "L Devillers",
        "L Vidrascu",
        "L Lamel"
      ],
      "year": "2005",
      "venue": "Neural networks: INNS"
    },
    {
      "citation_id": "4",
      "title": "LeBenchmark: A Reproducible Framework for Assessing Self-Supervised Representation Learning from Speech",
      "authors": [
        "S Evain",
        "H Nguyen",
        "H Le"
      ],
      "year": "2021",
      "venue": "LeBenchmark: A Reproducible Framework for Assessing Self-Supervised Representation Learning from Speech"
    },
    {
      "citation_id": "5",
      "title": "Wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed"
      ],
      "year": "2020",
      "venue": "Advances in Neural Inform"
    },
    {
      "citation_id": "6",
      "title": "FlauBERT: Unsupervised Lang. Model Pre-training for French",
      "authors": [
        "H Le",
        "L Vial",
        "J Frej"
      ],
      "year": "2020",
      "venue": "Twelfth Lang. Resources and Evaluation Conf"
    },
    {
      "citation_id": "7",
      "title": "BERT: Pretraining of Deep Bidirectional Transformers for Lang. Understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee"
      ],
      "year": "2019",
      "venue": "NAACL: Human Lang. Technologies"
    },
    {
      "citation_id": "8",
      "title": "Dawn of the Transformer Era in Speech Emotion Recognition: Closing the Valence Gap",
      "authors": [
        "J Wagner",
        "A Triantafyllopoulos",
        "H Wierstorf"
      ],
      "year": "2022",
      "venue": "Dawn of the Transformer Era in Speech Emotion Recognition: Closing the Valence Gap"
    },
    {
      "citation_id": "9",
      "title": "Transformer Models for Text-based Emotion Detection: A Review of BERT-based Approaches",
      "authors": [
        "F Acheampong",
        "H Nunoo-Mensah",
        "W Chen"
      ],
      "year": "2021",
      "venue": "AI Review"
    },
    {
      "citation_id": "10",
      "title": "Attention is All you Need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar"
      ],
      "year": "2017",
      "venue": "Advances in Neural Inform. Process. Systems"
    },
    {
      "citation_id": "11",
      "title": "Long Short-Term Memory-Networks for Machine Reading",
      "authors": [
        "J Cheng",
        "L Dong",
        "M Lapata"
      ],
      "year": "2016",
      "venue": "Proc. of Empirical Methods in Natural Lang"
    },
    {
      "citation_id": "12",
      "title": "Neural machine trans. by jointly learning to align and translate",
      "authors": [
        "D Bahdanau",
        "K Cho",
        "Y Bengio"
      ],
      "year": "2015",
      "venue": "ICLR"
    },
    {
      "citation_id": "13",
      "title": "Speech Emotion Classification Using Attention-Based LSTM",
      "authors": [
        "Y Xie",
        "R Liang",
        "Z Liang"
      ],
      "year": "2019",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "14",
      "title": "Self-attention transfer networks for speech emotion recognition",
      "authors": [
        "Z Zhao",
        "Z Bao",
        "Z Zhang"
      ],
      "year": "2021",
      "venue": "Virtual Reality & Intelligent Hardware"
    },
    {
      "citation_id": "15",
      "title": "Multimodal Machine Learning: A Survey and Taxonomy",
      "authors": [
        "T Baltrusaitis",
        "C Ahuja",
        "L.-P Morency"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "16",
      "title": "Speech Emotion Recognition Using Semantic Inform",
      "authors": [
        "P Tzirakis",
        "A Nguyen",
        "S Zafeiriou",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "Speech Emotion Recognition Using Semantic Inform"
    },
    {
      "citation_id": "17",
      "title": "Self-Attention for Speech Emotion Recognition",
      "authors": [
        "L Tarantino",
        "P Garner",
        "A Lazaridis"
      ],
      "year": "2019",
      "venue": "INTER-SPEECH"
    },
    {
      "citation_id": "18",
      "title": "EmoBed: Strengthening Monomodal Emotion Recognition via Training with Crossmodal Emotion Embeddings",
      "authors": [
        "J Han",
        "Z Zhang",
        "Z Ren",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "IEEE Trans. on Affect. Comput"
    },
    {
      "citation_id": "19",
      "title": "Low-Level Fusion of Audio, Video Feature for Multi-Modal Emotion Recognition",
      "authors": [
        "M Wimmer",
        "B Schuller",
        "D Arsic"
      ],
      "year": "2008",
      "venue": "Low-Level Fusion of Audio, Video Feature for Multi-Modal Emotion Recognition"
    },
    {
      "citation_id": "20",
      "title": "Analysis of Emotion Recognition Using Facial Expressions, Speech and Multimodal Inform",
      "authors": [
        "C Busso",
        "S Yildirim",
        "M Bulut"
      ],
      "year": "2004",
      "venue": "Analysis of Emotion Recognition Using Facial Expressions, Speech and Multimodal Inform"
    },
    {
      "citation_id": "21",
      "title": "Strength Modelling for Real-World Automatic Continuous Affect Recognition from Audiovisual Signals",
      "authors": [
        "J Han",
        "Z Zhang",
        "N Cummins"
      ],
      "year": "2017",
      "venue": "Strength Modelling for Real-World Automatic Continuous Affect Recognition from Audiovisual Signals"
    },
    {
      "citation_id": "22",
      "title": "Modulated Fusion using Transformer for Linguistic-Acoustic Emotion Recognition",
      "authors": [
        "J.-B Delbrouck",
        "N Tits",
        "S Dupont"
      ],
      "year": "2020",
      "venue": "Proc. of the First International Workshop on Natural Lang. Process. Beyond Text"
    },
    {
      "citation_id": "23",
      "title": "Efficient Low-rank Multimodal Fusion With Modality-Specific Factors",
      "authors": [
        "Z Liu",
        "Y Shen",
        "V Lakshminarasimhan"
      ],
      "year": "2018",
      "venue": "Proc. of the 56th ACL"
    },
    {
      "citation_id": "24",
      "title": "A Survey of Affect Recognition Methods: Audio, Visual, and Spontaneous Expressions",
      "authors": [
        "Z Zeng",
        "M Pantic",
        "G Roisman"
      ],
      "year": "2009",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "25",
      "title": "Multimodal Approach of Speech Emotion Recognition Using Multi-Level Multi-Head Fusion Attention-Based Recurrent Neural Network",
      "authors": [
        "N.-H Ho",
        "H.-J Yang",
        "S.-H Kim"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "26",
      "title": "Endto-end multimodal affect recognition in real-world environments",
      "authors": [
        "P Tzirakis",
        "J Chen",
        "S Zafeiriou",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "Inform. Fusion"
    },
    {
      "citation_id": "27",
      "title": "Prediction-based learning for continuous emotion recognition in speech",
      "authors": [
        "J Han",
        "Z Zhang",
        "F Ringeval"
      ],
      "year": "2017",
      "venue": "IEEE"
    },
    {
      "citation_id": "28",
      "title": "Attention Bottlenecks for Multimodal Fusion",
      "authors": [
        "A Nagrani",
        "S Yang",
        "A Arnab"
      ],
      "year": "2021",
      "venue": "Advances in Neural Inform. Process. Systems"
    },
    {
      "citation_id": "29",
      "title": "Multimodal Crossand Self-Attention Network for Speech Emotion Recognition",
      "authors": [
        "L Sun",
        "B Liu",
        "J Tao",
        "Z Lian"
      ],
      "year": "2021",
      "venue": "IEEE ICASSP"
    },
    {
      "citation_id": "30",
      "title": "Endto-End Speech Emotion Recognition: Challenges of Real-Life Emergency Call Centers Data Recordings",
      "authors": [
        "T Deschamps-Berger",
        "L Lamel",
        "L Devillers"
      ],
      "year": "2021",
      "venue": "Endto-End Speech Emotion Recognition: Challenges of Real-Life Emergency Call Centers Data Recordings"
    },
    {
      "citation_id": "31",
      "title": "vestigating Transformer Encoders and Fusion Strategies for Speech Emotion Recognition in Emergency Call Center Conversations.,\" in ICMI",
      "authors": [
        "T Deschamps-Berger",
        "L Lamel",
        "L Devillers"
      ],
      "year": "2022",
      "venue": "vestigating Transformer Encoders and Fusion Strategies for Speech Emotion Recognition in Emergency Call Center Conversations.,\" in ICMI"
    },
    {
      "citation_id": "32",
      "title": "Neural Machine Trans. of Rare Words with Subword Units",
      "authors": [
        "R Sennrich",
        "B Haddow",
        "A Birch"
      ],
      "year": "2016",
      "venue": "Proc. of the 54th ACL"
    },
    {
      "citation_id": "33",
      "title": "Adam: A Method for Stochastic Optimization",
      "authors": [
        "D Kingma",
        "L Ba"
      ],
      "year": "2015",
      "venue": "ICLR"
    }
  ]
}