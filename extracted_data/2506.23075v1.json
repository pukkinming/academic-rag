{
  "paper_id": "2506.23075v1",
  "title": "Csbrain: A Cross-Scale Spatiotemporal Brain Foundation Model For Eeg Decoding",
  "published": "2025-06-29T03:29:34Z",
  "authors": [
    "Yuchen Zhou",
    "Jiamin Wu",
    "Zichen Ren",
    "Zhouheng Yao",
    "Weiheng Lu",
    "Kunyu Peng",
    "Qihao Zheng",
    "Chunfeng Song",
    "Wanli Ouyang",
    "Chao Gou"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Understanding and decoding human brain activity from electroencephalography (EEG) signals is a fundamental problem in neuroscience and artificial intelligence, with applications ranging from cognition and emotion recognition to clinical diagnosis and brain-computer interfaces. While recent EEG foundation models have made progress in generalized brain decoding by leveraging unified architectures and large-scale pretraining, they inherit a scale-agnostic dense modeling paradigm from NLP and vision. This design overlooks an intrinsic property of neural activity-cross-scale spatiotemporal structure. Different EEG task patterns span a broad range of temporal and spatial scales, from brief neural activations to slow-varying rhythms, and from localized cortical activations to large-scale distributed interactions. Ignoring this diversity may lead to suboptimal representations and weakened generalization ability. To address these limitations, we propose CSBrain, a Cross-scale Spatiotemporal Brain foundation model for generalized EEG decoding. CSBrain introduces two key components: (i) Cross-scale Spatiotemporal Tokenization (CST), which aggregates multi-scale features within localized temporal windows and anatomical brain regions into compact scale-aware token representations; and (ii) Structured Sparse attention (SSA), which models cross-window and cross-region dependencies for diverse decoding tasks, further enriching scale diversities while eliminating the spurious dependencies. CST and SSA are alternately stacked to progressively integrate cross-scale spatiotemporal dependencies. Extensive experiments across 11 representative EEG tasks and 16 datasets demonstrate that CSBrain consistently outperforms both task-specific models and strong foundation baselines. These results establish cross-scale modeling as a key inductive bias for generalized EEG decoding and highlight CSBrain as a robust backbone for future brain-AI research. The code and model will be released.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Understanding and decoding human brain activity is a long-standing ambition of neuroscience and artificial intelligence, with implications ranging from cognitive science to medical diagnostics and brain-computer interfaces (BCI)  [1] [2] [3] [4] . Among various neural recording modalities, electroencephalography (EEG) is particularly attractive due to its non-invasive nature, high temporal resolution, and affordability  [5] [6] [7] . EEG signals are high-dimensional, multichannel time series that reflect the brain's dynamic electrical activity and are widely applied to abnormal identification  [8, 9] , motor imagery  [10, 11] , emotion analysis  [12, 13] , sleep stage analysis  [14, 15] , seizure detection  [16, 17] , and various neurological disorders  [18] [19] [20] . Over the past decade, a variety of task-specific deep learning models-spanning CNNs  [21] [22] [23] , RNNs  [24] [25] [26] , GNNs  [27] [28] [29] , and Transformers  [30] [31] [32]  -have been proposed for EEG decoding in the aforementioned applications. Although effective in narrow settings, these models are typically tightly coupled to specific datasets and task formats, limiting their generalizability and scalability  [33, 34] .\n\nMotivated by the paradigm shift in AI from task-specific to foundation models, recent efforts have explored brain foundation models  [34] [35] [36] [37] [38] [39] [40]  that aim to learn universal EEG representations across diverse brain decoding tasks via unified architectures and large-scale self-supervised pretraining. These models represent a significant step toward generalizable EEG decoding, and most adopt a pipeline transplanted from natural language processing and computer vision  [41] [42] [43] [44] [45] [46]  (as shown in Figure  1 (a)): EEG signals are first segmented into fixed-scale tokens, and then dense attention is applied across tokens to model dependencies. However, such scale-agnostic and dense modeling strategies fundamentally mismatch the intrinsic cross-scale spatiotemporal nature of EEG signals, which is critical for accurately capturing task-specific neural patterns. Specifically, different EEG decoding tasks inherently exhibit distinct spatiotemporal scales. From a temporal perspective, distinct EEG decoding tasks exhibit significant differences in neural dynamics  [47, 48] . For instance, motor and speech imagine tasks involve transient bursts of activity over short time windows  [49, 50] , whereas sleep staging relies on slower oscillatory cycles across longer timescales  [51] . From a spatial perspective, tasks differ in the activation patterns of brain regions due to the functional specialization of human brains  [49, 52] . Motor imagery typically involves localized or focal neural activations  [53] , while emotion recognition requires coordinated activity across widely distributed brain regions  [54] , reflecting longer-range spatial dependencies. Such variability in spatiotemporal scales across tasks places strong demands on token representations and modeling capacity. Most existing foundation models face three key limitations: (1) Scaleagnostic tokenization: They rely on a one-time fixed-scale tokenization strategy, resulting in token representations that fail to accommodate the diverse neural patterns across tasks. These tokens often suffer from scale mismatch and semantic dilution, which undermines their effectiveness as the most fundamental modeling units. (2) Structure-agnostic dense attention: Due to the lack of spatiotemporal scale awareness in tokenization, existing foundation models rely on dense attention across all tokens to uncover task-specific neural patterns underlying noisy signals. However, indiscriminately applying attention to suboptimal tokens, without considering the intrinsic cross-scale structures, tends to introduce spurious dependencies due to the inherently high noise of EEG signals  [11, 55] , ultimately degrading representation quality and increasing computational overhead. (3) Limited generalization: These limitations collectively hinder the generalization and adaptability of unified EEG foundation models across diverse BCI tasks with heterogeneous spatiotemporal scales.\n\nTo address these limitations, we argue that EEG foundation models must move beyond scale-agnostic, dense modeling paradigms and embrace cross-scale, structure-aware architectures that respect the intrinsic regional organisation and temporal dynamics of neural activity. In this paper, we propose CSBrain, a Cross-scale Spatiotemporal Brain foundation Model for generalized EEG decoding, as illustrated in Figure  1(b) . Guided by neurophysiological priors, CSBrain decomposes EEG signals along spatial and temporal axes into distinct brain regions and windows, and captures crossscale dependencies both within and across these structures, yielding robust and highly adaptable representations. Specifically, we propose a Cross-scale Spatiotemporal Tokenization (CST) module, which captures EEG features at multiple temporal and spatial scales within localized regions. By integrating these patterns into compact token representations, CST enables adaptable alignment with diverse spatiotemporal requirements of EEG decoding tasks. Building upon these scale-aware tokens, we propose a Structured Sparse Attention (SSA) module to further enrich the diversity of modeling scales for EEG. SSA captures long-range dependencies across temporal windows and brain regions in a structured and efficient manner. By replacing costly dense attention, SSA reduces spurious dependencies and computational overhead, yielding more discriminative representations for noisy EEG signals. Finally, CST and SSA are alternately stacked and mutually reinforcing, progressively integrating cross-scale dependencies across both temporal and spatial dimensions. This hierarchical design enables CSBrain to build robust EEG representations that reflect the complex patterns of neural activity, thereby achieving superior generalization across EEG decoding tasks with inherently diverse spatiotemporal demands. Overall, our contribution can be summarized as follows:\n\n• We propose CSBrain, a Cross-scale Spatiotemporal Brain foundation model for generalized brain decoding, offering a cross-scale, structure-aware architecture that effectively captures multi-scale spatiotemporal neural patterns across diverse tasks. • We introduce Cross-scale Spatiotemporal Tokenization (CST) to explicitly integrate multiscale neural patterns within temporal windows and brain regions into compact, scaleaware tokens. Additionally, Structured Sparse Attention (SSA) is designed to efficiently capture long-range dependencies across windows and regions, thereby enriching modeling scale diversity. Alternately stacked, CST and SSA progressively integrate cross-scale dependencies, enabling robust EEG representations across tasks.\n\n• Extensive experiments across 11 representative EEG decoding tasks and 16 public datasets demonstrate CSBrain's superiority over both task-specific and foundation models. Further analysis reveals diverse cross-scale patterns across tasks, establishing cross-scale modeling as a key inductive bias for generalized EEG decoding.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Model Architecture",
      "text": "The overall framework of CSBrain is illustrated in Figure  2 . The model first applies a standardized preprocessing module to extract initial feature representations from raw EEG signals. These features are then fed into the Cross-scale Spatiotemporal Tokenization (CST) module, which constructs robust cross-scale tokens by aggregating multi-resolution information within localized temporal windows and anatomically defined brain regions. Built upon these semantically enriched tokens, the Structured Sparse Attention (SSA) module expands the modeling scope, capturing long-range dependencies across time and brain regions while avoiding redundant interactions. CST and SSA are alternately stacked for L layers, progressively integrating cross-scale spatiotemporal dependencies. Finally, a lightweight task-specific head is attached to support various objectives, including masked reconstruction during pretraining and classification or regression for downstream tasks.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Eeg Signal Preprocessing",
      "text": "Initially, we formalize the input EEG signals as E ∈ R |Cx|×T , where C x ⊆ C denotes the set of electrodes used, and T denotes the number of timestamps. C corresponds to the standardized international 10-20 system. To enhance signal quality and ensure feature consistency across datasets, we follow prior works  [35, 34]  and apply a standardized signal preprocessing pipeline.\n\nSignal Standardization. The raw signals E are processed with a band-pass filter to remove lowfrequency drifts and high-frequency noise, followed by a notch filter to eliminate power line interfer-  , where n = T t is the number of segments per electrode. Preliminary Feature Encoding. We extract both temporal and spectral features from each segment in E p . Local temporal dynamics are captured using 1D convolutions with normalization, while spectral information is obtained by applying a Fast Fourier Transform (FFT) followed by a fully connected layer to encode frequency energy distributions. The two feature types are concatenated to form a unified feature embedding. Finally, we add a learnable positional encoding  [40] , resulting in the initial EEG representation x (0) ∈ R C×n×d , where d denotes the feature embedding dimension.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Cross-Scale Spatiotemporal Tokenization (Cst)",
      "text": "Due to the inherent cross-scale spatiotemporal nature of EEG signals, informative neural patterns emerge at heterogeneous temporal and spatial resolutions. Previous EEG decoding methods adopting a fixed token granularity cannot effectively capture such patterns, limiting its generalizability across diverse BCI tasks. To address this, we propose Cross-scale Spatiotemporal Tokenization (CST), which encodes multi-resolution information within localized temporal windows and anatomically defined brain regions into unified token representations. These tokens form the basic modeling units for subsequent attention computation. In the following, we detail the two components of CST: Temporal Tokenization, which captures multi-scale temporal patterns within local time windows (i.e., intra-window), and Spatial Tokenization, which extracts region-specific features within anatomical brain regions (i.e., intra-region).\n\nTemporal Tokenization. To enable tokenization at various temporal resolutions, we introduce multiscale temporal convolution kernels\n\n, where K denotes the number of kernels and each temporal kernel Conv (k) t has a kernel size s (k) t and output embedding dimension d k . Given the preprocessed EEG feature x (l-1) ∈ R C×n×d , for each location (i, j), where i ∈ {1, . . . , n} and j ∈ {1, . . . , C} denotes the temporal and electrode channel indices, respectively, we define a localized temporal window W (k) t (i) centered at i for the k-th scale with size s (k) t . Subsequently, the multi-scale temporal convolution is applied over these windows to extract temporal patterns at varying resolutions for each location. The process can be formulated as:\n\nwhere Proj t is a residual projection for dimension alignment. The outputs from different temporal kernels are concatenated to form a temporally cross-scale token x(l) i,j , which captures rich temporal dynamics within each channel. The resulted token representations could enhance model's robustness to signal noise and scalability to diverse decoding tasks with varying temporal patterns. Spatial Tokenization. Similar to Temporal Tokenization, we design multi-scale spatial convolution kernels\n\nand output embedding dimension d k . To encode spatial context across functionally related electrodes in each brain region at multiple scales, we utilize these convolution kernels to aggregate electrode features within anatomically defined brain regions. Specifically, given EEG feature x(l) ∈ R C×n×d after Temporal Tokenization, we first divide electrodes into a set of brain regions R = {R r } R r=1 based on the 10-20 system, where each region R r comprises spatially adjacent electrodes. For each electrode j ∈ R r , we define localized spatial neighborhoods\n\ns within brain region for the k-th scale. Finally, the spatial tokenization is performed by applying multi-scale spatial convolution kernels over their corresponding neighborhoods:\n\nwhere Proj s is a residual projection for dimension alignment. The outputs from different spatial kernels are concatenated to form the final cross-scale token representation x(l) i,j , capturing localized spatial dependencies within each brain region.\n\nx(l) i,j encapsulates spatiotemporal characteristics across diverse scales, serving as the final output of CST module and providing robust scale-aware features for downstream attention computation.\n\nTo balance representational capacity and computational efficiency, we allocate embedding dimensions across scales in an exponentially decaying scheme:\n\n, assigning higher dimensions to smaller kernels to retain fine-grained features, while allocating lower dimensions to larger kernels that summarize coarse context. This strategy aligns representational capacity with information density at each scale for balanced and efficient EEG representation.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Structured Sparse Attention (Ssa)",
      "text": "Prior approaches typically apply dense attention uniformly across fixed-scale tokens. While effective in some cases, such indiscriminate attention can introduce spurious dependencies, reduce representational quality, and increases computational costs. Meanwhile, our CST provides structured, cross-scale token representations within localized temporal windows and brain regions, laying a robust representational space for further modeling. Building upon this, we propose Structured Sparse Attention (SSA), which efficiently captures long-range dependencies across temporal windows and spatial regions while avoiding redundant interactions. SSA complements CST along both temporal and spatial scales, and together they enable structured cross-scale modeling of EEG representations. SSA consists of two components: Inter-window Attention, responsible for capturing long-range temporal dependencies across local windows, and Inter-region Attention, which models spatial dependencies across distinct brain regions.\n\nInter-window Attention. Given the cross-scale tokens\n\nx(l) i,j ∈ R C×n×d , we first perform a temporal grouping operation to form cross-window groups. Specifically, for each relative index g ∈ {1, . . . , w}, we collect all tokens that occupy the same position g in each window to form a temporal group G (g) t . Self-attention is then computed within each group to model structured long-range dependencies across time:\n\nx(l,win)\n\n(3) This design enables efficient and structured temporal interaction while avoiding redundancy.\n\nInter-region Attention. To ensure structured sparsity while maintaining coverage across all electrodes, we perform a spatial grouping operation to construct multiple spatial groups across brain regions. For each group, we sequentially sample a token x rep r from each region R r , and combine it with the average feature of R r to form a regional descriptor:\n\nwhere P(R r ) denotes the mean-pooled feature of region R r , and ϕ(•) is a learnable linear transformation. These descriptors are assembled into a spatial group G (g) s\n\n= {x 1 , x2 , . . . , xR }, over which self-attention is applied to capture structured dependencies across anatomical regions:\n\nFinally, we apply layer normalization and a feedforward network with residual connection to refine the output:\n\nTogether, SSA enables CSBrain to efficiently model long-range dependencies across both temporal and spatial dimensions by leveraging the robust cross-scale token layout provided by CST. This design progressively builds global context while preserving scale-awareness and mitigating spurious correlations in noisy signals, thereby enhancing generalization across diverse EEG decoding tasks.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Pretraining With Masked Autoencoding",
      "text": "To learn generalizable EEG representations, we adopt a self-supervised pretraining strategy based on masked autoencoding  [45, 56] , as shown in Figure  3 . This process encourages the model to capture meaningful spatiotemporal dependencies from unlabeled EEG signals, laying the foundation for effective downstream transfer.\n\nMasking Strategy. Given a preprocessed EEG signal E p ∈ R C×n×t , we randomly mask a fixed ratio r ∈ (0, 1) of segments along the temporal axis using a Bernoulli sampling scheme. This results in two subsets: visible segments E v ∈ R C×nv×t and masked segments E m ∈ R C×nm×t , where n v + n m = n and nm n = r. The masked EEG sequence is encoded by alternately stacked CST and SSA modules, which jointly model neural dependencies across multiple spatiotemporal scales. Masked segments are represented by learnable embeddings and integrated alongside visible tokens during encoding.\n\nReconstruction Objective. The reconstruction head consists of a lightweight fully connected layer, which takes as input the encoded visible tokens and learnable masked embeddings, and projects them into the original EEG signal space. Let Êm ∈ R C×nm×t denote the predicted EEG segments and E m the corresponding ground truth. The reconstruction loss is defined as the mean squared error (MSE) over all masked positions:\n\nThis masked autoencoding objective drives the model to recover meaningful spatiotemporal dependencies from context, yielding robust and transferable EEG representations.\n\n3 Experiments",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Pre-Training Setup",
      "text": "Data Preprocessing. We pre-train CSBrain on the Temple University Hospital EEG corpus (TUEG) dataset, which has been shown effective for foundation model studies  [40, 57] . We apply standard In all experiments, we strictly follow the training, validation, and test splits to ensure fair and consistent evaluation. To the best of our knowledge, this is among the most comprehensive evaluations conducted on EEG foundation models to date. See supplementary for details.\n\nBaselines & Metrics. We extensively compare CSBrain with two major categories of baselines:\n\n(1) representative task-specific EEG decoding models, including EEGNet  [21] , EEGConformer (Conformer)  [30] , SPaRCNet  [22] , ContraWR  [23] , CNN-Transformer (C-Trans)  [32] , FFCL  [24] , and ST-Transformer (ST-Trans)  [31] ; and (2) recent EEG foundation models, including BIOT  [34] , LaBraM  [35] , and CBraMod  [40] . Following prior works  [34, 35, 40] , we report Balanced Accuracy, Cohen's Kappa, and Weighted F1 for multiclass classification tasks; Balanced Accuracy, AUC-PR, and AUROC for binary classification; and Pearson correlation, R2 score, and RMSE for regression tasks. See supplementary for details.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experimental Results",
      "text": "Performance Comparison. As shown in Table  2 , we highlight two key observations: (1) CSBrain consistently achieves state-ofthe-art performance across nearly all tasks and metrics. In particular, it significantly outperforms all baselines on datasets such as BCIC-IV-2a, BCIC2020-3, Siena, and TUSL. Even in cases where CSBrain ranks second on certain metrics, the performance gap to the best-performing method remains marginal. From the macro-average in the last row of Table  2 , CSBrain achieves the highest overall score across all tasks, outperforming strong foundation models such as CbraMod, LaBraM, and BIOT by 3.35%, 3.98%, and 7.73%, respectively. These results demonstrate CSBrain's strong generalization and adaptability to diverse EEG decoding scenarios with varying spatiotemporal demands. (2) Foundation models outperform task-specific models by a significant margin. Across all 16 datasets, EEG foundation models-including BIOT, LaBraM, CbraMod, and CSBrain-consistently outperform task-specific models. This also validates the advantage of unified model architectures trained with large-scale pretraining, which can extract generalizable neural representations across heterogeneous EEG domains.\n\nTokenization Comparison. To investigate how spatiotemporal scale affects EEG representation from the perspective of token construction, we vary the number of kernels K in the CST module to control token granularity. Specifically, we compare single-scale (K = 1), dual-scale (K = 2), and full crossscale (K = 3) configurations. Additionally, to evaluate the effect of stacking, we compare our full design (with CST applied before each SSA module) against a variant where cross-scale tokenization (K = 3) is applied only once before the first SSA. For efficiency, all variants are pre-trained and fine-tuned using 30% of the training data. As shown in Fig.  4 , our SSA (K = 3, stacked) consistently achieves the best performance across four representative tasks: emotion recognition, motor imagery, event classification, and imagined speech. In contrast, the single-scale variant (K = 1) performs the worst. Notably, on the motor imagery task, SSA outperforms the single-scale counterpart by 10.8%, 20.7%, and 12.1% on three evaluation metrics, respectively. Moreover, we observe that alternately stacking SSA modules throughout the network consistently outperforms applying SSA only once. This is because a single application of SSA before attention computation may fail to fully capture the broader cross-scale dependencies. These results highlight the importance of the proposed SSA for capturing diverse EEG dynamics across tasks.  Topography Visualization. To further examine the spatial dynamics captured by CSBrain, we visualize activation topographies across different EEG decoding tasks. Specifically, we apply Gradientweighted Class Activation Mapping (Grad-CAM)  [72]  to compute the contribution of each EEG channel to the model's predictions, as shown in Figure  6 . Different tasks elicit distinct activation patterns and scales. Vigilance states primarily activate the temporal and occipital lobes, indicating continuous engagement of auditory and visual systems. Motor imagery evokes highly localized activations over the contralateral motor cortex, reflecting ERD/ERS (event-related desynchronization / synchronization) phenomenon  [73] . In contrast, emotion recognition and imagined speech exhibit broad, distributed activations. Emotion tasks elicit significant activation across the frontal and occipital lobes, consistent with the findings in  [29] . Speech imagery evokes significant activation in frontal and temporal lobes  [74] , which are critically involved in imagined speech processing  [75] . These observations further validate that different EEG decoding tasks exhibit distinct activation regions and scales, reinforcing the necessity of cross-scale modeling for EEG foundation models. By explicitly modeling cross-scale structure, CSBrain effectively adapts to task-specific neural patterns across diverse brain decoding scenarios.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we demonstrate that modeling the cross-scale spatiotemporal structure of EEG signals is essential for building generalizable brain foundation models. Through a unified architecture composed of Cross-scale Spatiotemporal Tokenization and Structured Sparse Attention, our proposed CSBrain effectively captures neural dependencies spanning multiple spatiotemporal scales. Experimental results across 11 EEG decoding tasks and 16 datasets validate the effectiveness of CSBrain. These findings establish cross-scale spatiotemporal modeling as a critical inductive bias for robust, scalable, and physiologically-aligned EEG representation learning. We believe this work lays a solid foundation for future advancements in brain-AI integration and generalized neural decoding.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "A Related Work",
      "text": "Electroencephalography (EEG) provides a direct, non-invasive window into human brain activity, and plays a pivotal role in cognitive science, medical diagnostics, and brain-computer interface (BCI) systems  [1] [2] [3] . By placing electrodes on the scalp, EEG captures and amplifies electrical signals produced by coordinated neuronal activity. Its high temporal fidelity, affordability, and portability make EEG especially attractive for large-scale deployment in both clinical and everyday settings  [5] [6] [7] . As BCI applications expand across emotion recognition  [12, 13] , seizure detection  [16, 17] , sleep staging  [14, 15] , and various neurological disorders  [18] [19] [20] , there is a growing need for scalable, general-purpose EEG models that can robustly decode neural signals across diverse tasks and recording conditions.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "A.1 Task-Specific Eeg Decoding Models",
      "text": "Conventional EEG decoding methods typically follow a two-stage pipeline: handcrafted feature extraction followed by shallow classification. A wide range of handcrafted features have been utilized, including time-domain statistics, frequency-domain power spectra (e.g., via Fast Fourier Transform  [76] ), time-frequency features (e.g., STFT  [77] , DWT  [78] ), and spatial filters such as Common Spatial Pattern  [79]  or Riemannian geometry-based methods  [80] . These features are typically fed into classical classifiers including linear discriminant analysis  [81] , support vector machines  [82] , k-nearest neighbors  [83] , or logistic regression  [84] . Despite their simplicity, these methods often struggle with generalization due to strong reliance on task-specific priors, limited capacity to model nonlinear dynamics, and poor robustness to inter-subject and inter-session variability.\n\nWith the rise of deep learning, end-to-end models have increasingly replaced handcrafted pipelines by directly learning representations from raw EEG signals. CNN-based architectures  [21, 85, 86]  have proven effective at capturing local spatiotemporal patterns, while RNNs, particularly LSTMs  [25, 87, 88] , are commonly adopted to model temporal dependencies. Transformers  [31, 89]  have recently gained popularity due to their superior ability to capture long-range interactions and global context. To combine local and global modeling, hybrid models such as CNN-LSTM  [24]  and CNN-Transformer  [32, 30]  have been explored. In parallel, GNN-based methods  [90, 29]  aim to incorporate topological priors by modeling spatial relationships among electrodes. While these deep models have achieved promising results in tasks such as motor imagery, emotion recognition, and sleep staging, they are typically tightly coupled to specific datasets and task formats. Adapting them to new tasks often requires retraining and architectural redesign. These limitations have motivated a shift toward EEG foundation models, which aim to learn robust and generalizable representations from unified architectures and large-scale self-supervised pretraining. Such models hold the potential to enhance generalization and enable effective transfer across tasks and domains.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "A.2 Brain Foundation Model",
      "text": "Foundation models have transformed fields such as natural language processing (NLP) and computer vision (CV), enabling strong generalization across tasks through unified architectures and large-scale pretraining. Models like BERT  [91] , GPT  [92] , CLIP  [93] , MAE  [45] , and SimMIM  [56]  demonstrate that unified architectures trained on large-scale data can be efficiently adapted to a wide range of downstream applications  [94] [95] [96] [97] . Inspired by these successes, researchers have begun to explore foundation models for neural signal processing, particularly EEG.\n\nA number of EEG foundation models have emerged with the goal of learning generalized representations from large-scale data using unified model architectures. Early efforts such as BENDR  [44] , BrainBERT  [98] , Brant  [99] , EEG2Rep  [100] , and BIOT  [34]  adopt masked modeling or contrastive learning objectives to encode general-purpose neural features. LaBraM  [35]  further enhances crossdataset and multi-paradigm generalization through structured tokenization and vector-quantized neural codes. More recently, EEGPT  [36]  introduces spatiotemporal alignment, while CBraMod  [40]  incorporates parallel spatial-temporal attention and dynamic positional encoding to better capture signal structure. These models represent a significant step toward generalizable EEG decoding, and most adopt a pipeline transplanted from NLP and CV, in which EEG signals are divided into fixed-scale tokens and processed via dense attention across all tokens. However, this scale-agnostic and fully dense modeling paradigm fails to align with the intrinsic cross-scale spatiotemporal nature of EEG signals, an essential property for accurately capturing task-specific neural dynamics.\n\nTo address these limitations, we argue that EEG foundation models must move beyond scale-invariant dense modeling and embrace cross-scale, structure-aware architectures that respect the inherent regional organization and temporal dynamics of brain activity. In this paper, we propose CSBrain, a Cross-scale Spatiotemporal Brain foundation model for generalized EEG decoding. CSBrain achieves superior generalization across diverse EEG tasks with varying spatiotemporal demands.",
      "page_start": 17,
      "page_end": 18
    },
    {
      "section_name": "B Additional Pretraining Details B.1 Hyperparameter",
      "text": "We summarize the detailed hyperparameter settings of CSBrain in Table  3 , including the input configuration, model architecture, optimization settings, and training strategy.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "C Additional Downstream Experimental Details C.1 Hyperparameter Settings",
      "text": "For downstream finetuning, we load the pre-trained weights of CSBrain and replace the reconstruction head with a task-specific head composed of a multilayer perceptron. The learned EEG representations are flattened and fed into this head to perform downstream classification or regression. We then finetune the entire model on the target dataset. Table  4  lists the default hyperparameter settings used in downstream finetuning. We adopt binary cross-entropy (BCE) loss for binary classification, cross-entropy loss for multi-class classification, and mean squared error (MSE) for regression.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "C.2 Baselines",
      "text": "Task-specific Models:\n\n• EEGNet  [21]  is a lightweight CNN that employs depthwise and separable convolutions for decoding EEG signals from various BCI paradigms.\n\n• EEGConformer  [30]  is a hybrid architecture that combines CNN and transformer for EEG decoding. Specifically, the CNN module captures local spatiotemporal features, while the transformer models long-range dependencies for global temporal representation.\n\n• SPaRCNet  [22]  is a 1D CNN with dense residual connections for EEG decoding, incorporating adaptive depth and channel selection mechanisms.\n\n• ContraWR  [23]  converts EEG signals into multi-channel spectrograms, which are then classified using a ResNet-style 2D CNN.\n\n• CNN-Transformer  [32]  is a CNN enhanced with transformer layers and trained using belief matching (BM) loss to detect channel-wise EEG artifacts.\n\n• FFCL  [24]  integrates parallel CNN and LSTM branches, where the CNN extracts spatial features and the LSTM captures temporal dynamics. Features from the different branches are fused through a fully connected layer to enhance EEG classification.\n\n• ST-Transformer  [31]  is a transformer-based architecture that applies attention mechanisms along both the feature-channel and segmented time dimensions to capture global spatiotemporal dependencies from EEG signals.\n\nFoundation Models:\n\n• BIOT  [34]  is a linear transformer designed for universal biosignal representation learning through a hybrid supervised-unsupervised pre-training strategy. Notably, the pre-trained BIOT accepts a maximum of 18 EEG channels as input. Therefore, for signals with more than 18 channels, a 1×1 convolution is applied to project them into an 18-channel format. • LaBraM  [35]  is an EEG foundation model that learns generic representations by predicting masked neural tokens using a full-attention Transformer. • CBraMod  [40]  is an EEG foundation model designed to address the heterogeneous spatiotemporal structure of EEG signals through a criss-cross Transformer backbone.",
      "page_start": 19,
      "page_end": 20
    },
    {
      "section_name": "C.3 Metrics",
      "text": "Consistency with prior work  [34, 35, 40] , we evaluate performance using the metrics as follows:\n\nBalanced Accuracy addresses class imbalance by averaging the recall across all classes, offering a more reliable evaluation metric than standard accuracy in imbalanced datasets:\n\nwhere C is the number of classes, T P i and F N i denote true positives and false negatives for class i.\n\nCohen's Kappa measures the agreement between predicted and true labels agreement while correcting for chance, offering insight into classifier performance beyond random predictions:\n\nwhere p o is the observed agreement and p e is the expected agreement.\n\nWeighted F1-score is the harmonic mean of precision and recall for each class, weighted by the number of true instances (i.e., class support). It provides a performance measure that accounts for label imbalance in multi-class classification:\n\nwhere w i is the weight of class i based on its support. The precision is defined as:\n\nwhere F P i denotes false positives for class i.\n\nAUC-PR evaluates the trade-off between precision and recall across various decision thresholds. It is computed as the area under the curve plotted with precision on the y-axis and recall on the x-axis:\n\nA higher AUC-PR indicates better performance in correctly identifying positive instances with fewer false positives. In practice, AUC-PR is typically estimated by numerical integration of discrete precision-recall points.\n\nAUROC measures the model's ability to distinguish between positive and negative classes by plotting the True Positive Rate (TPR) against the False Positive Rate (FPR) at various thresholds. These are defined as:\n\nwhere T N denotes true negatives. The AUROC is the area under this curve:\n\nThe AUROC is the area under the ROC curve, typically computed using numerical integration of discrete (FPR, TPR) points. An AUROC of 0.5 indicates random guessing, while 1.0 indicates perfect discrimination.\n\nPearson Correlation measures the linear dependence between predicted and true values, ranging from -1 (perfect negative correlation) to +1 (perfect positive correlation):\n\nwhere y i is the i-th ground truth value, ŷi is the corresponding predicted value, ȳ and ȳ denote the mean of true and predicted values, respectively, and N is the total number of samples.\n\nR2 Score represents the proportion of variance in the dependent variable that is explained by the model, where a value of 1 indicates perfect predictive performance:\n\nAn R2 score less than 0 indicates that the model performs worse than simply predicting the mean of the target variable.\n\nRoot Mean Squared Error (RMSE) measures the standard deviation of prediction errors, quantifying the average magnitude of differences between predicted and actual values, where lower values indicate better model fit:\n\nIn this work, we report Balanced Accuracy, Cohen's Kappa, and Weighted F1 for multiclass classification tasks; Balanced Accuracy, AUC-PR, and AUROC for binary classification; and Pearson correlation, R2 score, and RMSE for regression tasks.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "D Additional Results On Downstream Tasks",
      "text": "To comprehensively evaluate the generalizability of our model, we conduct experiments on 11 representative BCI tasks across 16 publicly available EEG datasets. These tasks cover a wide range of applications, including Motor Imagery Classification (BCIC-IV-2a  [58] , PhysioNet-MI  [59] , SHU-MI  [60] ), Emotion Recognition (FACED  [61] , SEED-V  [62] ), Seizure Detection (CHB-MIT  [63, 64] , Siena  [63, 65] ), Sleep Staging (ISRUC  [66] , HMC  [67] ), Imagined Speech Classification (BCIC2020-3  [50] ), Vigilance Estimation (SEED-VIG  [68] ), Mental Stress Detection (MentalArithmetic  [63, 69] ), Mental Disorder Diagnosis (Mumtaz2016  [70] ), Event Type Classification (TUEV  [71] ), Abnormal Detection (TUAB  [71] ), and Slowing Event Classification (TUSL  [71] ). For all tasks, we strictly follow the established train/validation/test splits to ensure fair and reproducible evaluation. This region assignment follows a standard practice in the literature  [101] [102] [103] [104] [105] , where the 10-20 system's electrode labels are used as proxies for anatomical brain regions, specifically including the Frontal, Central, Parietal, Temporal, and Occipital areas. When existing baselines are unavailable or incompatible due to inconsistent settings, we re-implement and re-train them under a unified protocol for consistent comparison. Note that to ensure stable training, we apply minor hyperparameter adjustments (e.g., learning rate) for a few datasets when necessary. In the following sections, We detail the dataset setups for each task and analyze the corresponding results.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "D.1 Motor Imagery Classification",
      "text": "Motor Imagery (MI) classification aims to decode a user's intention by recognizing EEG patterns associated with imagined movements of specific body parts. It serves as a key component in BCI systems, enabling direct neural control in assistive technologies. We evaluate our model on three widely used MI datasets, BCIC-IV-2a  [58] , PhysioNet-MI  [59] , and SHU-MI  [60] , covering both multiclass and binary classification settings.\n\nBCIC-IV-2a comprises EEG recordings from 9 subjects performing 4 motor imagery tasks: imagining movements of the left hand, right hand, both feet and tongue. Recordings were collected over two sessions on separate days using 22 electrodes at a sampling rate of 250 Hz. Each session contains 288 EEG trials (72 per class). Following  [40] , we extract the  [2, 6] -second interval from each trial, yielding a total of 5,088 samples. The signals are subsequently resampled at 200 Hz. We adopt a strict subject-independent split: subject 1-5 for training, 6-7 for validation, and 8-9 for testing. We train our model with a learning rate of 0.0001 and a weight decay of 0.05. Table  5  summarizes the performance of our model and various baselines on BCIC-IV-2a. CSBrain achieves the highest performance across all three metrics: 0.5657 balanced accuracy, 0.4209 Cohen's kappa, and 0.5637 weighted F1. CSBrain outperforms the second-best CBraMod by +5.2% in balanced accuracy and +6.9% in kappa. These results highlight the effectiveness of our cross-scale spatiotemporal modeling design in capturing nuanced motor imagery dynamics.",
      "page_start": 21,
      "page_end": 22
    },
    {
      "section_name": "Task-Specific Models",
      "text": "EEGNet  [21]  0.4482 ± 0.0094 0.2693 ± 0.0121 0.4226 ± 0.0108 EEGConformer  [30]  0.4696 ± 0.0106 0.2924 ± 0.0141 0.4533 ± 0.0128 SPaRCNet  [22]  0.4635 ± 0.0117 0.2847 ± 0.0147 0.4432 ± 0.0126 ContraWR  [23]  0.4678 ± 0.0125 0.2905 ± 0.0160 0.4413 ± 0.0142 CNN-Transformer  [32]  0.4600 ± 0.0108 0.2800 ± 0.0148 0.4460 ± 0.0114 FFCL  [24]  0.4470 ± 0.0143 0.2627 ± 0.0176 0.4238 ± 0.0139 ST-Transformer  [31]  0",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "Method Balanced Accuracy Auc-Pr Auroc",
      "text": "",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "Task-Specific Models",
      "text": "EEGNet  [21]  0.5889 ± 0.0177 0.6311 ± 0.0142 0.6283 ± 0.0152 EEGConformer  [30]  0.5900 ± 0.0107 0.6370 ± 0.0093 0.6351 ± 0.0101 SPaRCNet  [22]  0.5978 ± 0.0097 0.6510 ± 0.0062 0.6431 ± 0.0117 ContraWR  [23]  0.5873 ± 0.0128 0.6315 ± 0.0105 0.6273 ± 0.0113 CNN-Transformer  [32]  0.5975 ± 0.0169 0.6412 ± 0.0076 0.6343 ± 0.0082 FFCL  [24]  0.5692 ± 0.0252 0.5943 ± 0.0171 0.6326 ± 0.0082 ST-Transformer  [31]  0.5992 ± 0.0206 0.6394 ± 0.0122 0.6431 ± 0.0111 Foundation Models BIOT  [34]  0.6179 ± 0.0183 0.6770 ± 0.0119 0.6609 ± 0.0127 LaBraM-Base  [35]  0.6166 ± 0.0192 0.6761 ± 0.0083 0.6604 ± 0.0091 CBraMod  [40]  0",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "Task-Specific Models",
      "text": "EEGNet  [21]  0.5814 ± 0.0125 0.4468 ± 0.0199 0.5796 ± 0.0115 EEGConformer  [30]  0.6049 ± 0.0104 0.4736 ± 0.0171 0.6062 ± 0.0095 SPaRCNet  [22]  0.5932 ± 0.0152 0.4564 ± 0.0234 0.5937 ± 0.0147 ContraWR  [23]  0.5892 ± 0.0133 0.4527 ± 0.0248 0.5918 ± 0.0116 CNN-Transformer  [32]  0.6053 ± 0.0118 0.4725 ± 0.0223 0.6041 ± 0.0105 FFCL  [24]  0.5726 ± 0.0092 0.4323 ± 0.0182 0.5701 ± 0.0079 ST-Transformer  [31]  0.6035 ± 0.0081 0.4712 ± 0.0199 0.6053 ± 0.0075 Foundation Models BIOT  [34]  0.6153 ± 0.0154 0.4875 ± 0.0272 0.6158 ± 0.0197 LaBraM-Base  [35]  0.6173 ± 0.0122 0.4912 ± 0.0192 0.6177 ± 0.0141 CBraMod  [40]  0.6174 ± 0.0036 0.4898 ± 0.0048 0.6179 ± 0.0035 CSBrain (Ours) 0.6304 ± 0.0090 0.5071 ± 0.0120 0.6308 ± 0.0095",
      "page_start": 25,
      "page_end": 25
    },
    {
      "section_name": "D.2 Emotion Recognition",
      "text": "Emotion recognition from EEG signals aims to decode a subject's affective state by analyzing brain activity patterns associated with different emotional experiences. EEG offers a direct, real-time, and non-invasive approach to capturing internal emotional responses. However, this task remains highly challenging due to the subjective nature of emotions, significant inter-subject variability, and the inherently noisy and non-stationary characteristics of EEG signals. To evaluate the effectiveness and generalizability of our model in decoding emotional states, we conduct experiments on two widely-used EEG emotion datasets: SEED-V  [62] , which involves five emotion categories and high-resolution short-segment EEG recordings across multiple sessions, and FACED  [61] , which focuses on fine-grained nine-class classification with long EEG segments.\n\nSEED-V is a benchmark dataset for EEG-based emotion recognition. It includes five categories (happy, sad, neutral, disgust, fear), with EEG collected from 16 subjects over three sessions each, using 62 channels at a high sampling rate of 1000 Hz. The data is segmented into 117,744 1-second samples, resampled to 200 Hz. We divide each 15-trial session into three equal parts (5:5:5) for training, validation, and testing. The model is trained using the same settings as FACED (learning rate 0.0001, weight decay 0.05). Table  8  shows the performance on SEED-V. CSBrain again surpasses all baselines, achieving 0.4091 balanced accuracy, 0.2569 Cohen's kappa, and 0.4101 weighted F1. Compared to CBraMod, which previously achieved the best results, CSBrain improves the kappa score by +2.1%, indicating better discriminative capacity across short-time emotional states.",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "Table 8: Results On Emotion Recognition (Seed-V, 5-Class).",
      "text": "Method Balanced Accuracy Cohen's Kappa Weighted F1",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "Task-Specific Models",
      "text": "EEGNet  [21]  0.2961 ± 0.0102 0.1006 ± 0.0143 0.2749 ± 0.0098 EEGConformer  [30]  0.3537 ± 0.0112 0.1772 ± 0.0174 0.3487 ± 0.0136 SPaRCNet  [22]  0.2949 ± 0.0078 0.1121 ± 0.0139 0.2979 ± 0.0083 ContraWR  [23]  0.3546 ± 0.0105 0.1905 ± 0.0188 0.3544 ± 0.0121 CNN-Transformer  [32]  0.3678 ± 0.0078 0.2072 ± 0.0183 0.3642 ± 0.0088 FFCL  [24]  0.3641 ± 0.0092 0.2078 ± 0.0201 0.3645 ± 0.0132 ST-Transformer  [31]  0",
      "page_start": 25,
      "page_end": 25
    },
    {
      "section_name": "Task-Specific Models",
      "text": "EEGNet  [21]  0.4090 ± 0.0122 0.3342 ± 0.0251 0.4124 ± 0.0141 EEGConformer  [30]  0.4559 ± 0.0125 0.3858 ± 0.0186 0.4514 ± 0.0107 SPaRCNet  [22]  0.4673 ± 0.0155 0.3978 ± 0.0289 0.4729 ± 0.0133 ContraWR  [23]  0.4887 ± 0.0078 0.4231 ± 0.0151 0.4884 ± 0.0074 CNN-Transformer  [32]  0.4697 ± 0.0132 0.4017 ± 0.0166 0.4702 ± 0.0125 FFCL  [24]  0.4673 ± 0.0158 0.3987 ± 0.0338 0.4699 ± 0.0145 ST-Transformer  [31]  0.4810 ± 0.0079 0.4137 ± 0.0133 0.4795 ± 0.0096 Foundation Models BIOT  [34]  0.5118 ± 0.0118 0.4476 ± 0.0254 0.5136 ± 0.0112 LaBraM-Base  [35]  0.5273 ± 0.0107 0.4698 ± 0.0188 0.5288 ± 0.0102 CBraMod  [40]  0.5509 ± 0.0089 0.5041 ± 0.0122 0.5618 ± 0.0093 CSBrain (Ours) 0.5752 ± 0.0042 0.5204 ± 0.0036 0.5796 ± 0.0031",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "D.3 Seizure Detection",
      "text": "Seizure detection aims to identify pathological brain activity associated with epileptic seizures by analyzing abnormal EEG patterns. Accurate and timely seizure detection is critical for clinical diagnosis, long-term monitoring, and closed-loop therapeutic systems. We evaluate our model on two benchmark datasets: CHB-MIT  [63, 64] , and Siena  [63, 65] .\n\nCHB-MIT is a widely-used clinical EEG benchmark collected from 23 pediatric patients with intractable epilepsy at the Children's Hospital Boston. EEG recordings span multiple days per subject, captured under seizure-monitoring protocols following withdrawal of anti-seizure medication. All signals are recorded using the international 10-20 electrode placement system at 256 Hz, and labeled into binary classes: seizure and non-seizure. Following prior work  [34, 40] , we retain the 16 commonly used bipolar montage channels and resample all EEG signals to 200 Hz. The data is then segmented into 10-second windows, resulting in 326,993 samples. To ensure fair and consistent evaluation, we adopt a subject-independent split: subjects 1-19 for training, 20-21 for validation, and 22-23 for testing. The model is trained with a learning rate of 0.0001 and a weight decay of 0.05. Table  10  shows the performance on CHB-MIT. Our CSBrain achieves the best performance in terms of AUC-PR and AUROC, outperforming all baselines by a large margin. Specifically, it obtains an AUC-PR of 0.5164, exceeding the second-best CBraMod (0.3689) by +14.75%, and an AUROC of 0.8915, higher than CBraMod's 0.8892 (+0.23%). Although its balanced accuracy (0.7262) is marginally lower than CBraMod (0.7398), the substantial gain in AUC-PR, an especially informative metric under class imbalance, demonstrates the robustness of CSBrain for seizure detection.\n\nSiena is a clinical EEG database collected from 14 adult patients at the Unit of Neurology and Neurophysiology, University of Siena. EEG recordings were acquired via Video-EEG monitoring at 512 Hz using the international 10-20 electrode placement system. Data annotation follows the International League Against Epilepsy (ILAE) criteria, with seizure and non-seizure labels rigorously verified by expert clinicians based on combined electrophysiological and clinical review. We retained the 29 EEG channels that were consistently available across all 14 subjects. All signals are resampled to 200 Hz. We segment the data into 10-second samples, yielding a total of 51,307 samples. For fair evaluation, we employ a subject-independent split: all data from the last two subjects (PN16 and PN17) are held out as the test set, while the remaining 12 subjects' data are split into train and validation sets at an 8:2 ratio, maintaining this proportion for each subject and across all label categories. Table  11  summarizes the results on the Siena dataset. CSBrain achieves the best performance across all evaluation metrics, with a balanced accuracy of 0.7662, an AUC-PR",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "Method Balanced Accuracy Auc-Pr Auroc",
      "text": "",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "Task-Specific Models",
      "text": "EEGNet  [21]  0.5658 ± 0.0106 0.1914 ± 0.0182 0.8048 ± 0.0136 EEGConformer  [30]  0.5976 ± 0.0141 0.2209 ± 0.0215 0.8226 ± 0.0170 SPaRCNet  [22]  0.5876 ± 0.0191 0.1247 ± 0.0119 0.8143 ± 0.0148 ContraWR  [23]  0.6344 ± 0.0002 0.2264 ± 0.0174 0.8097 ± 0.0114 CNN-Transformer  [32]  0.6389 ± 0.0067 0.2479 ± 0.0227 0.8662 ± 0.0082 FFCL  [24]  0.6262 ± 0.0104 0.2049 ± 0.0346 0.8271 ± 0.0051 ST-Transformer  [31]  0.5915 ± 0.0195 0.1422 ± 0.0094 0.8237 ± 0.0491 Foundation Models BIOT  [34]  0.7068 ± 0.0457 0.3277 ± 0.0460 0.8761 ± 0.0284 LaBraM-Base  [35]  0.7075 ± 0.0358 0.3287 ± 0.0402 0.8679 ± 0.0199 CBraMod  [40]  0",
      "page_start": 25,
      "page_end": 25
    },
    {
      "section_name": "Method Balanced Accuracy Auc-Pr Auroc",
      "text": "",
      "page_start": 25,
      "page_end": 25
    },
    {
      "section_name": "Task-Specific Models",
      "text": "EEGNet  [21]  0.7487 ± 0.0521 0.3753 ± 0.0867 0.8687 ± 0.0527 EEGConformer  [30]  0.7556 ± 0.0210 0.2091 ± 0.0786 0.8159 ± 0.0261 SPaRCNet  [22]  0.6572 ± 0.0381 0.3164 ± 0.0659 0.7334 ± 0.0857 ContraWR  [23]  0.6546 ± 0.0311 0.3711 ± 0.0405 0.7819 ± 0.0596 CNN-Transformer  [32]  0.6982 ± 0.0560 0.3835 ± 0.0709 0.8719 ± 0.0403 FFCL  [24]  0.6616 ± 0.0391 0.3938 ± 0.0903 0.8154 ± 0.1155 ST-Transformer  [31]  0.7527 ± 0.0381 0.3636 ± 0.0252 0.8884 ± 0.0091 Foundation Models BIOT  [34]  0.7352 ± 0.0669 0.3809 ± 0.0892 0.9029 ± 0.0304 LaBraM-Base  [35]  0.7082 ± 0.0329 0.3122 ± 0.0976 0.8814 ± 0.0328 CBraMod  [40]  0.7317 ± 0.0647 0.4107 ± 0.0720 0. we adopt a subject-independent split for evaluation: subjects 1-80 are used for training, 81-90 for validation, and 91-100 for testing. For consistency across models, all sample encoders (including CSBrain and baselines) are followed by a one-layer Transformer as a shared sequence encoder. Table  12  reports the performance on ISRUC. CSBrain achieves the highest balanced accuracy of 0.7925 and strong overall performance across Cohen's kappa (0.7406) and weighted F1 (0.7990). Compared to CBraMod, CSBrain improves balanced accuracy by 0.6 points (0.7925 vs. 0.7865), while maintaining highly competitive scores in the other two metrics.",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "Task-Specific Models",
      "text": "EEGNet  [21]  0.7154 ± 0.0121 0.7040 ± 0.0173 0.7513 ± 0.0124 EEGConformer  [30]  0.7400 ± 0.0133 0.7143 ± 0.0162 0.7634 ± 0.0151 SPaRCNet  [22]  0.7487 ± 0.0075 0.7097 ± 0.0132 0.7624 ± 0.0092 ContraWR  [23]  0.7402 ± 0.0126 0.7178 ± 0.0156 0.7610 ± 0.0137 CNN-Transformer  [32]  0.7363 ± 0.0087 0.7129 ± 0.0121 0.7719 ± 0.0153 FFCL  [24]  0.7277 ± 0.0182 0.7016 ± 0.0292 0.7614 ± 0.0197 ST-Transformer  [31]  0.7381 ± 0.0205 0.7013 ± 0.0352 0.7681 ± 0.0175 Foundation Models BIOT  [34]  0.7527 ± 0.0121 0.7291 ± 0.0230 0.7790 ± 0.0146 LaBraM-Base  [35]  0.7633 ± 0.0102 0.7231 ± 0.0182 0.7810 ± 0.0133 CBraMod  [40]  0.7865 ± 0.0110 0.7442 ± 0.0152 0.8011 ± 0.0099 CSBrain (Ours) 0.7925 ± 0.0030 0.7406 ± 0.0102 0.7990 ± 0.0091 HMC contains overnight polysomnography (PSG) recordings of 151 adult subjects. EEG signals were recorded from four channels (F4-M1, C4-M1, O2-M1, and C3-M2) at 256 Hz. All signals were segmented into 30-second epochs and annotated by sleep experts according to the AASM guidelines, resulting in 137,243 labeled samples across five classes. All signals are resampled to 200 Hz. Following prior work  [108] , we adopt a subject-independent evaluation split: the first 100 subjects are used for training, the middle 25 for validation, and the remaining 26 for testing. Table  13  summarizes the performance of all methods on HMC. CSBrain achieves the highest balanced accuracy of 0.7345, along with top scores in Cohen's kappa (0.6818) and weighted F1 (0.7506), outperforming all baselines. Compared to the best-performing baseline LaBraM-Base, CSBrain improves balanced accuracy by 0.68 points and weighted F1 by 0.52 points, while maintaining a comparable Cohen's kappa score.\n\nTable  13 : Results on Sleep Staging (HMC, 5-class).",
      "page_start": 27,
      "page_end": 27
    },
    {
      "section_name": "Method",
      "text": "Balanced Accuracy Cohen's Kappa Weighted F1",
      "page_start": 28,
      "page_end": 28
    },
    {
      "section_name": "Task-Specific Models",
      "text": "EEGNet  [21]  0.6534 ± 0.0122 0.5886 ± 0.0201 0.6536 ± 0.0168 EEGConformer  [30]  0.7149 ± 0.0086 0.6432 ± 0.0055 0.7080 ± 0.0039 SPaRCNet  [22]  0.4756 ± 0.1109 0.3147 ± 0.1315 0.4108 ± 0.1310 ContraWR  [23]  0.4242 ± 0.0541 0.2340 ± 0.0554 0.2987 ± 0.0288 CNN-Transformer  [32]  0.6573 ± 0.0141 0.5961 ± 0.0105 0.6896 ± 0.0065 FFCL  [24]  0.4427 ± 0.0702 0.2542 ± 0.0654 0.2902 ± 0.0485 ST-Transformer  [31]  0.2559 ± 0.0141 0.0503 ± 0.0183 0.1428 ± 0.0122 Foundation Models BIOT  [34]  0.6862 ± 0.0041 0.6295 ± 0.0113 0.7091 ± 0.0147 LaBraM-Base  [35]  0.7277 ± 0.0101 0.6813 ± 0.0053 0.7454 ± 0.0027 CBraMod  [40]  0.7269 ± 0.0041 0.6685 ± 0.0104 0.7395 ± 0.0089 CSBrain (Ours) 0.7345 ± 0.0047 0.6818 ± 0.0046 0.7506 ± 0.0042",
      "page_start": 29,
      "page_end": 29
    },
    {
      "section_name": "D.5 Imagined Speech Classification",
      "text": "Imagined speech classification aims to decode phonological representations embedded in neural activity. This task enables individuals with speech impairments resulting from conditions such as stroke, trauma, or amyotrophic lateral sclerosis (ALS) to communicate without overt articulation. Moreover, this approach enhances the understanding of the neural mechanisms underlying speech production and perception, facilitating advances in cognitive neuroscience and augmentative communication technologies. We evaluate our model on the BCIC2020-3 dataset  [50] , released as part of the 2020 International BCI Competition. During the data collection experiment, 15 subjects were instructed to imagine five speech-related words or phrases (\"hello,\" \"help me,\" \"stop,\" \"thank you,\" and \"yes\"), and EEG signals were recorded from 64 channels at a sampling rate of 256 Hz. BCIC2020-3 dataset is split into training, validation, and test sets as defined by the competition. Specifically, for each subject, 60 trials per class are provided for training, 10 for validation, and 10 for testing. Each sample consists of a 3-second EEG recording. Therefore, we obtained 6000 64-channel 3-second EEG trials, which are resampled to 200 Hz. A learning rate of 0.0001 and a weight decay of 0.01 are used for our model training. Table  14  reports the performance on BCIC2020-3. CSBrain achieves the highest balanced accuracy (0.6004), along with strong overall performance in terms of Cohen's kappa (0.5006) and weighted F1 (0.6003). Compared to CBraMod, CSBrain improves the balanced accuracy by 6.3 points (0.6004 vs. 0.5373), while maintaining competitive performance on the other two metrics.\n\nTable  14 : Results on Imagined Speech Classification (BCIC2020-3, 5-class).\n\nMethod Balanced Accuracy Cohen's Kappa Weighted F1",
      "page_start": 26,
      "page_end": 27
    },
    {
      "section_name": "Task-Specific Models",
      "text": "EEGNet  [21]  0.4413 ± 0.0096 0.3016 ± 0.0123 0.4413 ± 0.0102 EEGConformer  [30]  0.4506 ± 0.0133 0.3133 ± 0.0183 0.4488 ± 0.0154 SPaRCNet  [22]  0.4426 ± 0.0156 0.3033 ± 0.0233 0.4420 ± 0.0108 ContraWR  [23]  0.4257 ± 0.0162 0.3078 ± 0.0218 0.4407 ± 0.0182 CNN-Transformer  [32]  0.4533 ± 0.0092 0.3166 ± 0.0118 0.4506 ± 0.0127 FFCL  [24]  0.4678 ± 0.0197 0.3301 ± 0.0359 0.4689 ± 0.0205 ST-Transformer  [31]  0.4126 ± 0.0122 0.2941 ± 0.0159 0.4247 ± 0.0138 Foundation Models BIOT  [34]  0.4920 ± 0.0086 0.3650 ± 0.0176 0.4917 ± 0.0079 LaBraM-Base  [35]  0.5060 ± 0.0155 0.3800 ± 0.0242 0.5054 ± 0.0205 CBraMod  [40]  0.5373 ± 0.0108 0.4216 ± 0.0163 0.5383 ± 0.0096 CSBrain (Ours) 0.6004 ± 0.0187 0.5006 ± 0.0233 0.6003 ± 0.0192",
      "page_start": 28,
      "page_end": 28
    },
    {
      "section_name": "D.6 Vigilance Estimation",
      "text": "Vigilance estimation plays a critical role in monitoring cognitive states in real-world scenarios such as driving, aviation, and industrial operations. By measuring fatigue levels from EEG signals, this approach can enhance safety by reducing the risk of accidents in high-stakes environments. Advances in vigilance estimation may facilitate the development of more robust and reliable monitoring systems. We evaluate our model on the SEED-VIG dataset  [68] . SEED-VIG simulates a real-world driving environment to induce a gradual transition from a vigilance to a fatigued state. The dataset was collected while subjects operated the simulated driving system. Vigilance levels were labeled using the PERCLOS indicator obtained from SMI eye-tracking glasses. EEG signals were recorded from 21 subjects using 17 channels at 200 Hz, and segmented into 20,355 8-second samples. We adopt a subject-independent evaluation split: subject 1 to 13 for training, subject 14 to 17 for validation and subject 18 to 21 for test. To stabilize training, we use a learning rate of 0.0005 and a weight decay of 0.05, slightly differing from the default settings. As shown in Table  15 , CSBrain achieves the best overall performance, outperforming all baselines in both R 2 score (0.2363) and RMSE (0.2774), while achieving a competitive Pearson's correlation coefficient (0.6314) close to the strongest baseline (LaBraM, 0.6347). Compared to the best-performing foundation model LaBraM, CSBrain improves R 2 by a substantial margin (+5.6 points) and reduces RMSE by 0.97, highlighting the effectiveness of its cross-scale modeling strategy in capturing subtle fluctuations in cognitive state.",
      "page_start": 27,
      "page_end": 27
    },
    {
      "section_name": "Task-Specific Models",
      "text": "EEGNet  [21]  0.5127 ± 0.0357 0.1960 ± 0.0427 0.2847 ± 0.0076 EEGConformer  [30]  0.5800 ± 0.0174 0.2065 ± 0.0230 0.2829 ± 0.0041 SPaRCNet  [22]  0.5709 ± 0.0362 0.2185 ± 0.0601 0.2806 ± 0.0108 ContraWR  [23]  0.5235 ± 0.0335 0.0727 ± 0.0540 0.3057 ± 0.0090 CNN-Transformer  [32]  0.5829 ± 0.0246 0.1796 ± 0.0105 0.2877 ± 0.0018 FFCL  [24]  0.4923 ± 0.0313 0.1740 ± 0.0530 0.2885 ± 0.0093 ST-Transformer  [31]  0.6020 ± 0.0327 0.1138 ± 0.1151 0.2983 ± 0.0199 Foundation Models BIOT  [34]  0.6114 ± 0.0169 0.1232 ± 0.0778 0.2971 ± 0.0128 LaBraM-Base  [35]  0.6347 ± 0.0135 0.1808 ± 0.0958 0.2871 ± 0.0166 CBraMod  [40]  0.5502 ± 0.0115 0.0737 ± 0.0167 0.3057 ± 0.0027 CSBrain (Ours) 0.6314 ± 0.0356 0.2363 ± 0.0519 0.2774 ± 0.0094",
      "page_start": 28,
      "page_end": 28
    },
    {
      "section_name": "D.7 Mental Stress Detection",
      "text": "Mental stress detection holds significant potential for both clinical and real-world applications. It enables objective assessment of psychological stress levels, overcoming the limitations of subjective self-reports. Therefore, it is particularly valuable for workplace mental health monitoring and the diagnosis of stress-related disorders. We evaluate our model on the MentalArithmetic dataset  [63, 69] . MentalArithmetic comprises EEG recordings from 36 subjects collected before and during a mental arithmetic task. EEG trials were recorded before the task were labeled as \"no mental stress,\" while those were recorded during the task were labeled as \"mental stress.\" All EEG signals were recorded from 20 electrodes at a sampling rate of 500Hz. The signals were resampled to 200Hz and segmented into 1,707 samples of 5-second trails. We adopt a subject-independent evaluation split: subject 1 to 28 for training, subject 29 to 32 for validation, and subject 33 to 36 for testing.\n\nTable  16  reports the performance on MentalArithmetic. CSBrain achieves the highest balanced accuracy (0.7558), along with strong overall performance in terms of AUC-PR (0.6696) and AUROC (0.8478). Compared to CBraMod, CSBrain improves the balanced accuracy by 3.0 points (0.7558 vs. 0.7256), while maintaining comparable performance on the other two metrics. The superior results of CSBrain can be attributed to its cross-scale spatiotemporal modeling, which effectively captures the short-term neural fluctuations and longer-range contextual dependencies underlying stress-induced brain dynamics. These findings highlight the importance of structure-aware foundation models in cognitive state classification tasks.",
      "page_start": 28,
      "page_end": 29
    },
    {
      "section_name": "Method Balanced Accuracy Auc-Pr Auroc",
      "text": "",
      "page_start": 29,
      "page_end": 29
    },
    {
      "section_name": "Task-Specific Models",
      "text": "EEGNet  [21]  0.6770 ± 0.0116 0.5763 ± 0.0102 0.7321 ± 0.0108 EEGConformer  [30]  0.6805 ± 0.0123 0.5829 ± 0.0134 0.7424 ± 0.0128 SPaRCNet  [22]  0.6879 ± 0.0107 0.5825 ± 0.0193 0.7418 ± 0.0132 ContraWR  [23]  0.6631 ± 0.0097 0.5787 ± 0.0164 0.7332 ± 0.0082 CNN-Transformer  [32]  0.6779 ± 0.0268 0.5777 ± 0.0285 0.7258 ± 0.0336 FFCL  [24]  0.6798 ± 0.0142 0.5786 ± 0.0266 0.7330 ± 0.0198 ST-Transformer  [31]  0.6631 ± 0.0173 0.5672 ± 0.0259 0.7132 ± 0.0174 Foundation Models BIOT  [34]  0.6875 ± 0.0186 0.6004 ± 0.0195 0.7536 ± 0.0144 LaBraM-Base  [35]  0.6909 ± 0.0125 0.5999 ± 0.0155 0.7721 ± 0.0093 CBraMod  [40]  0",
      "page_start": 30,
      "page_end": 30
    },
    {
      "section_name": "Method Balanced Accuracy Auc-Pr Auroc",
      "text": "",
      "page_start": 30,
      "page_end": 30
    },
    {
      "section_name": "Task-Specific Models",
      "text": "EEGNet  [21]  0.9232 ± 0.0104 0.9626 ± 0.0095 0.9639 ± 0.0093 EEGConformer  [30]  0.9308 ± 0.0117 0.9684 ± 0.0105 0.9702 ± 0.0101 SPaRCNet  [22]  0.9316 ± 0.0095 0.9754 ± 0.0065 0.9781 ± 0.0063 ContraWR  [23]  0.9195 ± 0.0115 0.9589 ± 0.0102 0.9621 ± 0.0092 CNN-Transformer  [32]  0.9305 ± 0.0068 0.9757 ± 0.0074 0.9742 ± 0.0059 FFCL  [24]  0.9314 ± 0.0083 0.9717 ± 0.0021 0.9753 ± 0.0033 ST-Transformer  [31]  0.9135 ± 0.0103 0.9578 ± 0.0086 0.9594 ± 0.0059 Foundation Models BIOT  [34]  0.9358 ± 0.0052 0.9736 ± 0.0034 0.9758 ± 0.0042 LaBraM-Base  [35]  0.9409 ± 0.0079 0.9798 ± 0.0093 0.9782 ± 0.0057 CBraMod  [40]  0",
      "page_start": 31,
      "page_end": 31
    },
    {
      "section_name": "Task-Specific Models",
      "text": "EEGNet  [21]  0.3876 ± 0.0143 0.3577 ± 0.0155 0.6539 ± 0.0120 EEGConformer  [30]  0.4074 ± 0.0164 0.3967 ± 0.0195 0.6983 ± 0.0152 SPaRCNet  [22]  0.4161 ± 0.0262 0.4233 ± 0.0181 0.7024 ± 0.0104 ContraWR  [23]  0.4384 ± 0.0349 0.3912 ± 0.0237 0.6893 ± 0.0136 CNN-Transformer  [32]  0.4087 ± 0.0161 0.3815 ± 0.0134 0.6854 ± 0.0293 FFCL  [24]  0.3979 ± 0.0104 0.3732 ± 0.0188 0.6783 ± 0.0120 ST-Transformer  [31]  0.3984 ± 0.0228 0.3765 ± 0.0306 0.6823 ± 0.0190 Foundation Models BIOT  [34]  0.5281 ± 0.0225 0.5273 ± 0.0249 0.7492 ± 0.0082 LaBraM-Base  [35]  0.6409 ± 0.0065 0.6637 ± 0.0093 0.8312 ± 0.0052 CBraMod  [40]  0.6671 ± 0.0107 0.6772 ± 0.0096 0.8342 ± 0.0064 CSBrain (Ours) 0.6903 ± 0.0059 0.6833 ± 0.0047 0.8333 ± 0.0057",
      "page_start": 32,
      "page_end": 32
    },
    {
      "section_name": "D.10 Abnormal Detection",
      "text": "Abnormal detection enables the identification of pathological neuroelectric activity, which can significantly reduce the workload of medical personnel while ensuring timely alerts for various abnormal EEG events through continuous monitoring. TUAB  [71]  is an EEG dataset commonly used to evaluate the abnormal detection performance of decoding models. All EEG signals were recorded from 23 channels at 256 Hz and annotated as either normal or abnormal. Following prior work  [40] , we use 16 commonly adopted bipolar montage channels based on the international 10-20 system for TUAB.",
      "page_start": 29,
      "page_end": 29
    },
    {
      "section_name": "Method Balanced Accuracy Auc-Pr Auroc",
      "text": "",
      "page_start": 29,
      "page_end": 29
    },
    {
      "section_name": "Task-Specific Models",
      "text": "EEGNet  [21]  0.7642 ± 0.0036 0.8299 ± 0.0043 0.8412 ± 0.0031 EEGConformer  [30]  0.7758 ± 0.0049 0.8427 ± 0.0054 0.8445 ± 0.0038 SPaRCNet  [22]  0.7896 ± 0.0018 0.8414 ± 0.0018 0.8676 ± 0.0012 ContraWR  [23]  0.7746 ± 0.0041 0.8421 ± 0.0104 0.8456 ± 0.0074 CNN-Transformer  [32]  0.7777 ± 0.0022 0.8433 ± 0.0039 0.8461 ± 0.0013 FFCL  [24]  0.7848 ± 0.0038 0.8448 ± 0.0065 0.8569 ± 0.0051 ST-Transformer  [31]  0",
      "page_start": 30,
      "page_end": 30
    },
    {
      "section_name": "Task-Specific Models",
      "text": "EEGNet  [21]  0.4562 ± 0.0729 0.1955 ± 0.1115 0.4405 ± 0.0960 EEGConformer  [30]  0.5512 ± 0.0666 0.3072 ± 0.0877 0.4895 ± 0.0733 SPaRCNet  [22]  0.4185 ± 0.0452 0.1399 ± 0.0799 0.3500 ± 0.0968 ContraWR  [23]  0.5857 ± 0.0662 0.3567 ± 0.0968 0.5458 ± 0.0798 CNN-Transformer  [32]  0.3575 ± 0.0151 0.0306 ± 0.0179 0.2235 ± 0.0251 FFCL  [24]  0.3159 ± 0.0688 0.0628 ± 0.0880 0.2120 ± 0.0786 ST-Transformer  [31]  0.4000 ± 0.0329 0.0866 ± 0.0449 0.3793 ± 0.0459 Foundation Models BIOT  [34]  0.5785 ± 0.0303 0.2012 ± 0.0212 0.2394 ± 0.0404 LaBraM-Base  [35]  0.7625 ± 0.0131 0.6407 ± 0.0304 0.7614 ± 0.0210 CBraMod  [40]  0.7388 ± 0.0320 0.6149 ± 0.0550 0.7453 ± 0.0360 CSBrain (Ours) 0.8571 ± 0.0240 0.7828 ± 0.0270 0.8568 ± 0.0180",
      "page_start": 31,
      "page_end": 31
    },
    {
      "section_name": "E Additional Analysis And Ablations E.1 The Effectiveness Of Pretraining",
      "text": "We evaluate the effectiveness of large-scale pretraining by comparing CSBrain with and without pretraining across four representative EEG decoding tasks. The non-pre-trained model uses the same architecture but is trained from scratch on the downstream dataset only. Figure  8  summarizes the performance across three standard metrics. Pretraining leads to consistent performance gains across all datasets, confirming its general effectiveness. The improvement is especially pronounced in datalimited scenarios. On BCIC-IV-2a (5,000 samples), pretraining improves balanced accuracy from 0.39 to 0.57, Cohen's kappa from 0.23 to 0.42, and weighted F1 from 0.39 to 0.56. This demonstrates that pretraining is especially beneficial when downstream data is scarce and task difficulty is high. On SEED-V and TUEV, both of which contain over 100,000 samples, pretraining still yields substantial improvements. For TUAB, which contains over 400,000 labeled EEG samples, the gains from pretraining are relatively modest. This suggests that when downstream training data is sufficiently large and diverse, training from scratch becomes more competitive. Nevertheless, pretraining still brings consistent improvements.\n\nIn summary, large-scale pretraining is universally beneficial across tasks. The performance gain is especially prominent in data-limited settings, while still being meaningful on larger-scale datasets. These results support the core motivation of EEG foundation models: leveraging unlabeled EEG at scale to improve downstream performance, particularly when labeled data is limited.",
      "page_start": 31,
      "page_end": 31
    },
    {
      "section_name": "E.2 Scaling Analysis",
      "text": "As a foundation model for EEG decoding, CSBrain is expected to benefit from larger pretraining datasets and higher model capacities. To examine this, we perform a scaling analysis from two perspectives: (i) the amount of pretraining data, and (ii) the model capacity (number of parameters).\n\nEffect of Pretraining Data Volume. We vary the scale of unlabeled EEG data used for pretraining, ranging from 10 hours to 9000 hours. As shown in Figure  9 , model performance improves consistently with increased pretraining data. Accuracy, Cohen's kappa, and F1 score all show steady gains as data size increases, particularly between 500 and 5000 hours. Beyond 5000 hours, the improvement begins to plateau, suggesting diminishing returns under fixed model capacity.\n\nEffect of Model Capacity. We investigate the impact of model size by varying the number of Transformer layers in CSBrain from 2 to 12, which corresponds to model sizes from 1.4M to 4.9M parameters. As illustrated in Figure  10 , downstream performance improves steadily with larger model capacity, indicating that deeper networks provide stronger representational power for EEG decoding.\n\nTogether, these results follow the general trend of neural scaling laws  [109] , where both data and model scale contribute to performance improvements. While our current exploration is constrained  by computational resources and may not reach the scaling limits, we believe further scaling in both dimensions will continue to yield gains. Determining how much EEG data is required to effectively pretrain large-scale EEG foundation models remains a critical open question. We call for broader community efforts to collaboratively explore this direction, in order to unlock the full potential of unified architectures and large-scale pretraining for EEG-based foundation modeling.",
      "page_start": 31,
      "page_end": 32
    },
    {
      "section_name": "E.3 Ablation On Regional Descriptor",
      "text": "We conduct an ablation study to evaluate the impact of different regional descriptor designs in the inter-region attention module. In our default configuration, for each region R r , we construct a descriptor by combining two components: a sequentially sampled token x rep r and the mean-pooled feature across all tokens in R r . This hybrid design aims to preserve both localized temporal structure and global regional context. Here, we compare four variants: (1) Token only: Using only the sequentially sampled token x rep r from each region. (2) Mean only: Using only the average feature of all tokens in each region. (3) Random Sample: Randomly sampling a token from each region, and combining it with the regional average. (4) Ours: Sequentially sampling a token from each region and combining it with the average feature.\n\nFigure  11 : Ablation on regional descriptor design.\n\nFigure  11  reports the results of our ablation study on regional descriptor design across four representative tasks. The default setting consistently achieves the best performance across all tasks and metrics. Using only the token or only the mean leads to noticeable drops in accuracy and robustness, indicating that both local and regional information is essential. Random sampling performs slightly better than the single-source variants but still underperforms the default design. These results confirm that combining local token features with region-level context enhances inter-region attention, while sequential sampling ensures consistent spatial coverage.",
      "page_start": 32,
      "page_end": 33
    },
    {
      "section_name": "F Additional Visualizations F.1 Topography Visualization",
      "text": "In this section, we present topography visualizations for four representative datasets: SEED-VIG, BCIC-IV-2a, SEED-V, and BCIC2020-3. Specifically, we apply Gradient-weighted Class Activation Mapping (Grad-CAM)  [72]  to estimate the contribution of each EEG channel to the model's predictions. Across different EEG tasks, the visualizations reveal distinct activation topographies, which reflect task-specific activation patterns and demonstrate the model's ability to capture spatially meaningful representations.\n\nFor the SEED-VIG dataset, as shown in Figure  12 , vigilance states primarily activate the temporal and occipital lobes, indicating sustained engagement of the auditory and visual systems. In contrast, the fatigued state exhibits reduced activation in these lobes, suggesting a marked decline in auditory and visual information processing. For the BCIC-IV-2a dataset, as shown in Figure  13 , left-or right-hand motor imagery induces localized activation in the contralateral motor cortex, whereas motor imagery involving both feet or the tongue results in more symmetrical activation across the bilateral motor cortex. These findings suggest that motor imagery effectively modulates neural activity in the motor cortex, as reflected by event-related desynchronization (ERD) and synchronization (ERS)  [73] .  For the SEED-V dataset, as shown in Figure  14 , emotional tasks elicit significant activation across the frontal and occipital lobes, consistent with the finding in  [29] . Notably, different emotional produce distinct activation patterns, providing a critical neurophysiological basis for their identification.\n\nFor the BCIC2020-3 dataset, as shown in Figure  15 , speech imagery evokes significant activation in the frontal and temporal lobes  [74] , which are crucial for imagined speech processing  [75] . In particular, strong activation is observed in left-hemisphere electrodes F7, FT7, FC5, and F5, which correspond primarily to Broca's area. This finding further supports the involvement of Broca's area in speech processing  [75] . Overall, these visualizations confirm that different EEG decoding tasks",
      "page_start": 33,
      "page_end": 33
    },
    {
      "section_name": "F.2 Representation Visualization",
      "text": "To qualitatively assess the discriminative capability of the representations learned by our model, we conduct t-SNE visualizations  [110]  on two representative datasets: BCIC-IV-2a (motor imagery classification) and BCIC2020-3 (imagined speech classification). For each dataset, we compare the feature distributions of raw EEG signals and those produced by CSBrain after encoding. As shown in Figure  16  and Figure  17 , the features learned by CSBrain exhibit significantly improved clustering effects compared to the raw EEG input. In BCIC-IV-2a (Figure  16 ), different motor imagery classes form more compact and well-separated clusters, indicating that CSBrain effectively captures task-relevant discriminative patterns. Similarly, for BCIC2020-3 (Figure  17 ), imagined speech categories that are difficult to distinguish in the raw EEG space become more distinguishable after CSBrain encoding.",
      "page_start": 34,
      "page_end": 34
    },
    {
      "section_name": "F.3 Channel Relationship Visualization",
      "text": "To further investigate the spatial dependencies captured by CSBrain, we visualize the inter-channel relationships learned on two representative datasets: BCIC-IV-2a and BCIC2020-3. Specifically, we first compute the average token embedding for each EEG channel from the final layer representations. We then calculate the cosine similarity between every pair of channel embeddings to quantify interchannel relationships and visualize those with a similarity greater than 0.5. As shown in Figure  18  and Figure  19 , the learned channel similarity patterns reveal two notable characteristics. First, channels within the same anatomical brain region tend to exhibit high similarity, indicating that CSBrain captures strong intra-region dependencies. Second, we observe meaningful long-range inter-region connections, such as between frontal and parietal regions or temporal and occipital regions, suggesting that our model also learns physiologically plausible global interactions across distant brain areas. These results validate the effectiveness of our cross-scale modeling approach in capturing both local and global spatial dynamics, which are essential for robust and generalizable EEG decoding.",
      "page_start": 35,
      "page_end": 35
    },
    {
      "section_name": "G Discussion",
      "text": "",
      "page_start": 35,
      "page_end": 35
    },
    {
      "section_name": "G.1 Implication",
      "text": "Our work emphasizes the critical role of cross-scale spatiotemporal modeling in the design of brain foundation models, a factor often overlooked in prior efforts. CSBrain represents a paradigm shift from scale-agnostic, densely connected modeling to a cross-scale, structure-aware architecture. By explicitly encoding multi-resolution dynamics through Cross-scale Spatiotemporal Tokenization (CST) and Structured Sparse Attention (SSA), our approach more effectively reflects the complex patterns of neural activity, thereby achieving superior generalization across EEG decoding tasks with inherently diverse spatiotemporal demands.\n\nThis modeling perspective provides a reusable architectural template for future EEG foundation models and has the potential to generalize to other neurophysiological modalities. The strong performance of CSBrain across 11 representative tasks and 16 public datasets suggests that unified architectures when guided by neurophysiological priors, can scale effectively across heterogeneous brain decoding scenarios. Furthermore, we will publicly release benchmarking protocols, source code, and pre-trained models to support future research and foster community development.",
      "page_start": 35,
      "page_end": 36
    },
    {
      "section_name": "G.2 Limitation",
      "text": "Despite its strong generalization ability, CSBrain still faces several limitations. Firstly, our current exploration of model and data scaling is constrained by computational resources. Following the general trend observed in neural scaling laws  [109] , model performance tends to improve predictably with increased model capacity and training data size. While our results suggest similar patterns in EEG modeling, we are unable to empirically assess scaling behavior at the scale of visionlanguage or language-only models (e.g., CLIP or GPT). Moreover, an open question remains: how much EEG data is sufficient to pretrain high-quality foundation models? Due to the sparse, noisy, and heterogeneous nature of EEG datasets, further work is needed to address large-scale dataset unification, normalization, and cross-institutional collaboration. We believe such efforts will be essential to unlocking the full potential of EEG-based foundation modeling and call for broader community involvement to collectively advance this direction.",
      "page_start": 36,
      "page_end": 36
    },
    {
      "section_name": "G.3 Future Work",
      "text": "A natural extension of this work is to investigate the scaling behavior of EEG foundation models further. This includes expanding pretraining datasets, increasing model capacity, and generalization across downstream tasks. Understanding the efficiency and limits of scaling in EEG foundation models will be key to maximizing their representational capacity and real-world applicability. Additionally, the cross-scale modeling paradigm in CSBrain opens the door to unified cross-modal brain foundation models. By integrating signals beyond EEG, such as MEG or fMRI, which offer",
      "page_start": 36,
      "page_end": 36
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (a)): EEG signals are first segmented into fixed-scale tokens, and then dense attention is",
      "page": 2
    },
    {
      "caption": "Figure 1: (b). Guided by neurophysiological priors, CSBrain decomposes EEG",
      "page": 2
    },
    {
      "caption": "Figure 1: Comparison between previous brain foundation models and our proposed CSBrain. (a)",
      "page": 3
    },
    {
      "caption": "Figure 2: The model first applies a standardized",
      "page": 3
    },
    {
      "caption": "Figure 2: Overview of the CSBrain architecture. After EEG signal preprocessing, the Cross-",
      "page": 4
    },
    {
      "caption": "Figure 3: The overview of CSBrain pre-training process.",
      "page": 6
    },
    {
      "caption": "Figure 3: This process encourages the model to capture",
      "page": 6
    },
    {
      "caption": "Figure 4: , our SSA (K = 3, stacked) consistently",
      "page": 8
    },
    {
      "caption": "Figure 4: The performance comparisons of tokenization.",
      "page": 9
    },
    {
      "caption": "Figure 5: The performance comparisons of attention mechanism.",
      "page": 9
    },
    {
      "caption": "Figure 5: , SSA consistently achieves the best performance across Balanced",
      "page": 9
    },
    {
      "caption": "Figure 6: Different tasks elicit distinct activation",
      "page": 9
    },
    {
      "caption": "Figure 6: Topography visualization on four different tasks. More in the supplementary materials.",
      "page": 10
    },
    {
      "caption": "Figure 7: shows the training loss curve of CSBrain during pretraining over 40 epochs. The loss",
      "page": 18
    },
    {
      "caption": "Figure 7: Training loss curve during CSBrain pretraining.",
      "page": 19
    },
    {
      "caption": "Figure 8: Performance comparison between CSBrain without and with pretraining.",
      "page": 31
    },
    {
      "caption": "Figure 8: summarizes the",
      "page": 31
    },
    {
      "caption": "Figure 9: , model performance improves consistently",
      "page": 31
    },
    {
      "caption": "Figure 10: , downstream performance improves steadily with larger model",
      "page": 31
    },
    {
      "caption": "Figure 9: Effect of pretraining data volume on CSBrain performance.",
      "page": 32
    },
    {
      "caption": "Figure 10: Effect of model parameter size on CSBrain performance.",
      "page": 32
    },
    {
      "caption": "Figure 11: Ablation on regional descriptor design.",
      "page": 32
    },
    {
      "caption": "Figure 11: reports the results of our ablation study on regional descriptor design across four repre-",
      "page": 32
    },
    {
      "caption": "Figure 12: , vigilance states primarily activate the temporal",
      "page": 33
    },
    {
      "caption": "Figure 12: Topography visualization of SEED-VIG.",
      "page": 33
    },
    {
      "caption": "Figure 13: , left- or right-hand motor imagery induces",
      "page": 33
    },
    {
      "caption": "Figure 13: Topography visualization of BCIC-IV-2a.",
      "page": 33
    },
    {
      "caption": "Figure 14: , emotional tasks elicit significant activation across",
      "page": 33
    },
    {
      "caption": "Figure 15: , speech imagery evokes significant activation",
      "page": 33
    },
    {
      "caption": "Figure 14: Topography visualization of SEED-V.",
      "page": 34
    },
    {
      "caption": "Figure 15: Topography visualization of BCIC2020-3.",
      "page": 34
    },
    {
      "caption": "Figure 16: and Figure 17, the features learned by CSBrain exhibit significantly improved",
      "page": 34
    },
    {
      "caption": "Figure 16: ), different motor",
      "page": 34
    },
    {
      "caption": "Figure 17: ), imagined",
      "page": 34
    },
    {
      "caption": "Figure 16: t-SNE visualization of feature representations on BCIC-IV-2a. Left: raw EEG signals;",
      "page": 35
    },
    {
      "caption": "Figure 17: t-SNE visualization of feature representations on BCIC2020-3. Left: raw EEG signals;",
      "page": 35
    },
    {
      "caption": "Figure 19: , the learned channel similarity patterns reveal two notable characteristics. First, channels",
      "page": 35
    },
    {
      "caption": "Figure 18: Channel similarity relationships of BCIC-IV-2a.",
      "page": 36
    },
    {
      "caption": "Figure 19: Channel similarity relationships of BCIC2020-3.",
      "page": 37
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Overview of Downstream BCI datasets and tasks used for evaluation.",
      "page": 7
    },
    {
      "caption": "Table 1: These tasks include Motor Imagery Classification (BCIC-IV-2a, PhysioNet-",
      "page": 7
    },
    {
      "caption": "Table 2: All results are averaged over five runs with different random seeds.",
      "page": 7
    },
    {
      "caption": "Table 2: Performance comparison on 11 BCI tasks Using 16 datasets.",
      "page": 8
    },
    {
      "caption": "Table 2: , we highlight two key observations: (1) CSBrain consistently achieves state-of-",
      "page": 8
    },
    {
      "caption": "Table 2: , CSBrain achieves the highest overall",
      "page": 8
    },
    {
      "caption": "Table 3: , including the input",
      "page": 18
    },
    {
      "caption": "Table 3: Pretraining hyperparameters for CSBrain.",
      "page": 18
    },
    {
      "caption": "Table 4: lists the default hyperparameter settings",
      "page": 18
    },
    {
      "caption": "Table 4: Default hyperparameter settings for downstream finetuning.",
      "page": 19
    },
    {
      "caption": "Table 5: summarizes",
      "page": 22
    },
    {
      "caption": "Table 5: Results on Motor Imagery Classification (BCIC-IV-2a, 4-class).",
      "page": 22
    },
    {
      "caption": "Table 6: presents the results on SHU-MI. CSBrain again achieves the best performance",
      "page": 22
    },
    {
      "caption": "Table 6: Results on Motor Imagery Classification (SHU-MI, 2-class).",
      "page": 22
    },
    {
      "caption": "Table 7: , our CSBrain model achieves top performance on all three metrics: 0.6304",
      "page": 22
    },
    {
      "caption": "Table 7: Results on Motor Imagery Classification (PhysioNet-MI, 4-class).",
      "page": 23
    },
    {
      "caption": "Table 8: shows the performance on SEED-V. CSBrain again surpasses",
      "page": 23
    },
    {
      "caption": "Table 8: Results on Emotion Recognition (SEED-V, 5-class).",
      "page": 23
    },
    {
      "caption": "Table 9: Results on Emotion Recognition (FACED, 9-class).",
      "page": 24
    },
    {
      "caption": "Table 10: shows the performance on CHB-MIT. Our CSBrain achieves the best performance in",
      "page": 24
    },
    {
      "caption": "Table 11: summarizes the results on the Siena dataset. CSBrain achieves",
      "page": 24
    },
    {
      "caption": "Table 10: Results on Seizure Detection (CHB-MIT, 2-class).",
      "page": 25
    },
    {
      "caption": "Table 11: Results on Seizure Detection (Siena, 2-class).",
      "page": 25
    },
    {
      "caption": "Table 12: reports the performance on ISRUC. CSBrain achieves the highest balanced accuracy of",
      "page": 25
    },
    {
      "caption": "Table 12: Results on Sleep Staging (ISRUC, 5-class).",
      "page": 26
    },
    {
      "caption": "Table 13: summarizes the performance of all methods on HMC. CSBrain achieves the highest balanced accuracy",
      "page": 26
    },
    {
      "caption": "Table 13: Results on Sleep Staging (HMC, 5-class).",
      "page": 26
    },
    {
      "caption": "Table 14: reports the performance on BCIC2020-3. CSBrain achieves",
      "page": 26
    },
    {
      "caption": "Table 14: Results on Imagined Speech Classification (BCIC2020-3, 5-class).",
      "page": 27
    },
    {
      "caption": "Table 15: , CSBrain achieves the",
      "page": 27
    },
    {
      "caption": "Table 15: Results on Vigilance Estimation (SEED-VIG, regression).",
      "page": 27
    },
    {
      "caption": "Table 16: reports the performance on MentalArithmetic. CSBrain achieves the highest balanced",
      "page": 28
    },
    {
      "caption": "Table 16: Results on Mental Stress Detection (MentalArithmetic, 2-class).",
      "page": 28
    },
    {
      "caption": "Table 17: Results on Mental Disorder Diagnosis (Mumtaz2016, 2-class).",
      "page": 29
    },
    {
      "caption": "Table 18: reports the performance comparison on TUEV. CSBrain achieves the",
      "page": 29
    },
    {
      "caption": "Table 18: Results on Event Type Classification (TUEV, 6-class).",
      "page": 29
    },
    {
      "caption": "Table 19: , CSBrain achieves the highest balanced accuracy of 0.8172 and the best AUC-PR of",
      "page": 30
    },
    {
      "caption": "Table 19: Results on Abnormal Detection (TUAB, 2-class).",
      "page": 30
    },
    {
      "caption": "Table 20: presents the performance comparison on TUSL. CSBrain outperforms all baselines by a",
      "page": 30
    },
    {
      "caption": "Table 20: Results on Slowing Event Classification (TUSL, 3-class).",
      "page": 30
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Motor imagery and direct brain-computer communication",
      "authors": [
        "Gert Pfurtscheller",
        "Christa Neuper"
      ],
      "year": "2001",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "2",
      "title": "Decoding mental states from brain activity in humans",
      "authors": [
        "John-Dylan Haynes",
        "Geraint Rees"
      ],
      "year": "2006",
      "venue": "Nature reviews neuroscience"
    },
    {
      "citation_id": "3",
      "title": "Beyond the brain-computer interface: Decoding brain activity as a tool to understand neuronal mechanisms subtending cognition and behavior",
      "authors": [
        "Célia Loriette",
        "Julian Amengual",
        "Suliann Ben"
      ],
      "year": "2022",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "4",
      "title": "Explicit brain functional alignment for cross-subject visual decoding from limited fmri data",
      "authors": [
        "Yuqin Dai",
        "Zhouheng Yao",
        "Chunfeng Song",
        "Qihao Zheng",
        "Weijian Mai",
        "Kunyu Peng",
        "Shuai Lu",
        "Wanli Ouyang",
        "Jian Yang",
        "Jiamin Wu",
        "Mindaligner"
      ],
      "year": "2025",
      "venue": "Explicit brain functional alignment for cross-subject visual decoding from limited fmri data",
      "arxiv": "arXiv:2502.05034"
    },
    {
      "citation_id": "5",
      "title": "Handbook of clinical neurology",
      "authors": [
        "Gernot R Müller-Putz",
        "Electroencephalography"
      ],
      "year": "2020",
      "venue": "Handbook of clinical neurology"
    },
    {
      "citation_id": "6",
      "title": "A braincomputer interface that evokes tactile sensations improves robotic arm control",
      "authors": [
        "Sharlene Flesher",
        "John Downey",
        "Jeffrey Weiss",
        "Christopher Hughes",
        "Angelica Herrera",
        "Elizabeth Tyler-Kabara",
        "L Michael",
        "Jennifer Boninger",
        "Robert Collinger",
        "Gaunt"
      ],
      "year": "2021",
      "venue": "Science"
    },
    {
      "citation_id": "7",
      "title": "Combining vr with electroencephalography as a frontier of brain-computer interfaces",
      "authors": [
        "Hongbian Li",
        "Hyonyoung Shin",
        "Luis Sentis",
        "Ka-Chun Siu",
        "José Del R Millán",
        "Nanshu Lu"
      ],
      "venue": "Device"
    },
    {
      "citation_id": "8",
      "title": "Chrononet: A deep recurrent neural network for abnormal eeg identification",
      "authors": [
        "Subhrajit Roy",
        "Isabell Kiral-Kornek",
        "Stefan Harrer"
      ],
      "year": "2019",
      "venue": "Artificial Intelligence in Medicine: Conference on Artificial Intelligence in Medicine"
    },
    {
      "citation_id": "9",
      "title": "Automatic signal abnormality detection using time-frequency features and machine learning: A newborn eeg seizure case study",
      "authors": [
        "Boualem Boashash",
        "Samir Ouelha"
      ],
      "year": "2016",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "10",
      "title": "Deep learning for motor imagery eeg-based classification: A review",
      "authors": [
        "Ali Al-Saegh",
        "Shefa Dawwd"
      ],
      "year": "2021",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "11",
      "title": "A spatial filter temporal graph convolutional network for decoding motor imagery eeg signals",
      "authors": [
        "Xianlun Tang",
        "Jing Zhang",
        "Yidan Qi",
        "Ke Liu",
        "Rui Li",
        "Huiming Wang"
      ],
      "year": "2024",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "12",
      "title": "Eeg-based emotion recognition: a stateof-the-art review of current trends and opportunities",
      "authors": [
        "James Nazmi Sofian Suhaimi",
        "Jason Mountstephens",
        "Teo"
      ],
      "year": "2020",
      "venue": "Computational intelligence and neuroscience"
    },
    {
      "citation_id": "13",
      "title": "Contrastive learning of eeg representation of brain area for emotion recognition",
      "authors": [
        "Sheng Dai",
        "Ming Li",
        "Xu Wu",
        "Xiangyu Ju",
        "Xinyu Li",
        "Jun Yang",
        "Dewen Hu"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "14",
      "title": "Self-supervised eeg representation learning for automatic sleep staging",
      "authors": [
        "Chaoqi Yang",
        "Danica Xiao",
        "M Brandon Westover",
        "Jimeng Sun"
      ],
      "year": "2023",
      "venue": "JMIR AI"
    },
    {
      "citation_id": "15",
      "title": "Generalizable sleep staging via multi-level domain alignment",
      "authors": [
        "Jiquan Wang",
        "Sha Zhao",
        "Haiteng Jiang",
        "Shijian Li",
        "Tao Li",
        "Gang Pan"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "16",
      "title": "A hybrid deep learning approach for epileptic seizure detection in eeg signals",
      "authors": [
        "Ijaz Ahmad",
        "Xin Wang",
        "Danish Javeed",
        "Prabhat Kumar",
        "Oluwarotimi Samuel",
        "Shixiong Chen"
      ],
      "year": "2023",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "17",
      "title": "Eeg datasets for seizure detection and prediction-a review",
      "authors": [
        "Sheng Wong",
        "Anj Simmons",
        "Jessica Rivera-Villicana",
        "Scott Barnett",
        "Shobi Sivathamboo",
        "Piero Perucca",
        "Zongyuan Ge",
        "Patrick Kwan",
        "Levin Kuhlmann",
        "Rajesh Vasa"
      ],
      "year": "2023",
      "venue": "Epilepsia Open"
    },
    {
      "citation_id": "18",
      "title": "Automatic and efficient framework for identifying multiple neurological disorders from eeg signals",
      "authors": [
        "Md Nurul",
        "Ahad Tawhid",
        "Siuly Siuly",
        "Kate Wang",
        "Hua Wang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Technology and Society"
    },
    {
      "citation_id": "19",
      "title": "A review of graph theory-based diagnosis of neurological disorders based on eeg and mri",
      "authors": [
        "Ying Yan",
        "Guanting Liu",
        "Haoyang Cai",
        "Edmond Wu",
        "Jun Cai",
        "Adrian Cheok",
        "Na Liu",
        "Tao Li",
        "Zhiyong Fan"
      ],
      "year": "2024",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "20",
      "title": "Medformer: A multi-granularity patching transformer for medical time-series classification",
      "authors": [
        "Yihe Wang",
        "Nan Huang",
        "Taida Li",
        "Yujun Yan",
        "Xiang Zhang"
      ],
      "year": "2024",
      "venue": "The Annual Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "21",
      "title": "Eegnet: a compact convolutional neural network for eeg-based brain-computer interfaces",
      "authors": [
        "Amelia Vernon J Lawhern",
        "Nicholas Solon",
        "Waytowich",
        "Stephen M Gordon",
        "P Chou",
        "Brent Hung",
        "Lance"
      ],
      "year": "2018",
      "venue": "Journal of neural engineering"
    },
    {
      "citation_id": "22",
      "title": "Development of expert-level classification of seizures and rhythmic and periodic patterns during eeg interpretation",
      "authors": [
        "Jin Jing",
        "Wendong Ge",
        "Shenda Hong",
        "Marta Fernandes",
        "Zhen Lin",
        "Chaoqi Yang",
        "Sungtae An",
        "Aaron Struck",
        "Aline Herlopian",
        "Ioannis Karakis"
      ],
      "year": "2023",
      "venue": "Neurology"
    },
    {
      "citation_id": "23",
      "title": "Self-supervised electroencephalogram representation learning for automatic sleep staging: Model development and evaluation study",
      "authors": [
        "Chaoqi Yang",
        "Cao Xiao",
        "M Brandon Westover",
        "Jimeng Sun"
      ],
      "year": "2023",
      "venue": "JMIR AI"
    },
    {
      "citation_id": "24",
      "title": "Motor imagery eeg classification algorithm based on cnn-lstm feature fusion network",
      "authors": [
        "Hongli Li",
        "Man Ding",
        "Ronghua Zhang",
        "Chunbo Xiu"
      ],
      "year": "2022",
      "venue": "Biomedical signal processing and control"
    },
    {
      "citation_id": "25",
      "title": "Emotion recognition based on eeg using lstm recurrent neural network",
      "authors": [
        "Salma Alhagry",
        "Aly Aly Fahmy",
        "Reda El-Khoribi"
      ],
      "year": "2017",
      "venue": "International Journal of Advanced Computer Science and Applications"
    },
    {
      "citation_id": "26",
      "title": "An efficient lstm network for emotion recognition from multichannel eeg signals",
      "authors": [
        "Xiaobing Du",
        "Cuixia Ma",
        "Guanhua Zhang",
        "Jinyao Li",
        "Yu-Kun Lai",
        "Guozhen Zhao",
        "Xiaoming Deng",
        "Yong-Jin Liu",
        "Hongan Wang"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "27",
      "title": "Graph neural network-based eeg classification: A survey",
      "authors": [
        "Dominik Klepl",
        "Min Wu",
        "Fei He"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "28",
      "title": "Eeg-gnn: Graph neural networks for classification of electroencephalogram (eeg) signals",
      "authors": [
        "Andac Demir",
        "Toshiaki Koike-Akino",
        "Ye Wang",
        "Masaki Haruna",
        "Deniz Erdogmus"
      ],
      "year": "2021",
      "venue": "Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)"
    },
    {
      "citation_id": "29",
      "title": "Eeg-based emotion recognition using regularized graph neural networks",
      "authors": [
        "Peixiang Zhong",
        "Di Wang",
        "Chunyan Miao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "30",
      "title": "Eeg conformer: Convolutional transformer for eeg decoding and visualization",
      "authors": [
        "Yonghao Song",
        "Qingqing Zheng",
        "Bingchuan Liu",
        "Xiaorong Gao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "31",
      "title": "Transformer-based spatial-temporal feature learning for eeg decoding",
      "authors": [
        "Yonghao Song",
        "Xueyu Jia",
        "Lie Yang",
        "Longhan Xie"
      ],
      "year": "2021",
      "venue": "Transformer-based spatial-temporal feature learning for eeg decoding",
      "arxiv": "arXiv:2106.11170"
    },
    {
      "citation_id": "32",
      "title": "Transformer convolutional neural networks for automated artifact detection in scalp eeg",
      "authors": [
        "Wei Yan Peh",
        "Yuanyuan Yao",
        "Justin Dauwels"
      ],
      "year": "2022",
      "venue": "Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)"
    },
    {
      "citation_id": "33",
      "title": "Toward reliable signals decoding for electroencephalogram: A benchmark study to eegnex",
      "authors": [
        "Xia Chen",
        "Xiangbin Teng",
        "Han Chen",
        "Yafeng Pan",
        "Philipp Geyer"
      ],
      "year": "2024",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "34",
      "title": "Biot: Biosignal transformer for cross-data learning in the wild",
      "authors": [
        "Chaoqi Yang",
        "M Westover",
        "Jimeng Sun"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "35",
      "title": "Large brain model for learning generic representations with tremendous eeg data in bci",
      "authors": [
        "Wei-Bang Jiang",
        "Li-Ming Zhao",
        "Bao-Liang Lu"
      ],
      "year": "2024",
      "venue": "The International Conference on Learning Representations"
    },
    {
      "citation_id": "36",
      "title": "Eegpt: Pretrained transformer for universal and reliable representation of eeg signals",
      "authors": [
        "Guangyu Wang",
        "Wenchao Liu",
        "Yuhong He",
        "Cong Xu",
        "Lin Ma",
        "Haifeng Li"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "37",
      "title": "Neuro-gpt: Towards a foundation model for eeg",
      "authors": [
        "Wenhui Cui",
        "Woojae Jeong",
        "Philipp Thölke",
        "Takfarinas Medani",
        "Karim Jerbi",
        "Anand Joshi",
        "Richard Leahy"
      ],
      "year": "2024",
      "venue": "IEEE International Symposium on Biomedical Imaging (ISBI)"
    },
    {
      "citation_id": "38",
      "title": "Brainwave: A brain signal foundation model for clinical applications",
      "authors": [
        "Zhizhang Yuan",
        "Fanqi Shen",
        "Meng Li",
        "Yuguo Yu",
        "Chenhao Tan",
        "Yang Yang"
      ],
      "year": "2024",
      "venue": "Brainwave: A brain signal foundation model for clinical applications",
      "arxiv": "arXiv:2402.10251"
    },
    {
      "citation_id": "39",
      "title": "A simple review of eeg foundation models: Datasets, advancements and future perspectives",
      "authors": [
        "Junhong Lai",
        "Jiyu Wei",
        "Lin Yao",
        "Yueming Wang"
      ],
      "year": "2025",
      "venue": "A simple review of eeg foundation models: Datasets, advancements and future perspectives",
      "arxiv": "arXiv:2504.20069"
    },
    {
      "citation_id": "40",
      "title": "CBramod: A criss-cross brain foundation model for EEG decoding",
      "authors": [
        "Jiquan Wang",
        "Sha Zhao",
        "Zhiling Luo",
        "Yangxuan Zhou",
        "Haiteng Jiang",
        "Shijian Li",
        "Tao Li",
        "Gang Pan"
      ],
      "year": "2025",
      "venue": "The International Conference on Learning Representations"
    },
    {
      "citation_id": "41",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "42",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer",
        "Alexander Kolesnikov",
        "Dirk Weissenborn",
        "Xiaohua Zhai",
        "Thomas Unterthiner",
        "Mostafa Dehghani",
        "Matthias Minderer",
        "Georg Heigold",
        "Sylvain Gelly",
        "Jakob Uszkoreit",
        "Neil Houlsby"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "43",
      "title": "Self-supervised learning: Generative or contrastive",
      "authors": [
        "Xiao Liu",
        "Fanjin Zhang",
        "Zhenyu Hou",
        "Li Mian",
        "Zhaoyu Wang",
        "Jing Zhang",
        "Jie Tang"
      ],
      "year": "2021",
      "venue": "IEEE transactions on knowledge and data engineering"
    },
    {
      "citation_id": "44",
      "title": "Bendr: Using transformers and a contrastive self-supervised learning task to learn from massive amounts of eeg data",
      "authors": [
        "Demetres Kostas",
        "Stephane Aroca-Ouellette",
        "Frank Rudzicz"
      ],
      "year": "2021",
      "venue": "Frontiers in Human Neuroscience"
    },
    {
      "citation_id": "45",
      "title": "Masked autoencoders are scalable vision learners",
      "authors": [
        "Kaiming He",
        "Xinlei Chen",
        "Saining Xie",
        "Yanghao Li",
        "Piotr Dollár",
        "Ross Girshick"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "46",
      "title": "Cswin transformer: A general vision transformer backbone with cross-shaped windows",
      "authors": [
        "Xiaoyi Dong",
        "Jianmin Bao",
        "Dongdong Chen",
        "Weiming Zhang",
        "Nenghai Yu",
        "Lu Yuan",
        "Dong Chen",
        "Baining Guo"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "47",
      "title": "Principles of neural science",
      "authors": [
        "James Eric R Kandel",
        "Thomas Schwartz",
        "Steven Jessell",
        "James Siegelbaum",
        "Sarah Hudspeth",
        "Mack"
      ],
      "year": "2000",
      "venue": "Principles of neural science"
    },
    {
      "citation_id": "48",
      "title": "Rhythms of the Brain",
      "authors": [
        "György Buzsáki"
      ],
      "year": "2006",
      "venue": "Rhythms of the Brain"
    },
    {
      "citation_id": "49",
      "title": "A weighted and directed interareal connectivity matrix for macaque cerebral cortex",
      "authors": [
        "Maria Nikola T Markov",
        "Ercsey-Ravasz",
        "Ribeiro Ar",
        "Camille Gomes",
        "Loic Lamy",
        "Julien Magrou",
        "Pierre Vezoli",
        "Arnaud Misery",
        "Rene Falchier",
        "Marie-Alice Quilodran",
        "Gariel"
      ],
      "year": "2014",
      "venue": "Cerebral cortex"
    },
    {
      "citation_id": "50",
      "title": "2020 international brain-computer interface competition: A review",
      "authors": [
        "Ji-Hoon Jeong",
        "Jeong-Hyun Cho",
        "Young-Eun Lee",
        "Seo-Hyun Lee",
        "Gi-Hwan Shin",
        "Young-Seok Kweon",
        "José Del R Millán",
        "Klaus-Robert Müller",
        "Seong-Whan Lee"
      ],
      "year": "2022",
      "venue": "Frontiers in human neuroscience"
    },
    {
      "citation_id": "51",
      "title": "The sleep slow oscillation as a traveling wave",
      "authors": [
        "Marcello Massimini",
        "Reto Huber",
        "Fabio Ferrarelli",
        "Sean Hill",
        "Giulio Tononi"
      ],
      "year": "2004",
      "venue": "Journal of Neuroscience"
    },
    {
      "citation_id": "52",
      "title": "The wu-minn human connectome project: an overview",
      "authors": [
        "Stephen David C Van Essen",
        "Deanna Smith",
        "Timothy Barch",
        "Essa Behrens",
        "Kamil Yacoub",
        "Ugurbil",
        "Hcp Wu-Minn",
        "Consortium"
      ],
      "year": "2013",
      "venue": "Neuroimage"
    },
    {
      "citation_id": "53",
      "title": "Cognitive motor processes: the role of motor imagery in the study of motor representations",
      "authors": [
        "Jörn Munzert",
        "Britta Lorey",
        "Karen Zentgraf"
      ],
      "year": "2009",
      "venue": "Brain research reviews"
    },
    {
      "citation_id": "54",
      "title": "Eeg based emotion recognition: A tutorial and review",
      "authors": [
        "Xiang Li",
        "Yazhou Zhang",
        "Prayag Tiwari",
        "Dawei Song",
        "Bin Hu",
        "Meihong Yang",
        "Zhigang Zhao",
        "Neeraj Kumar",
        "Pekka Marttinen"
      ],
      "year": "2022",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "55",
      "title": "Eeg artifact removal-state-of-the-art and guidelines",
      "authors": [
        "Jose Antonio",
        "Begoña Garcia-Zapirain"
      ],
      "year": "2015",
      "venue": "Journal of neural engineering"
    },
    {
      "citation_id": "56",
      "title": "Simmim: A simple framework for masked image modeling",
      "authors": [
        "Zhenda Xie",
        "Zheng Zhang",
        "Yue Cao",
        "Yutong Lin",
        "Jianmin Bao",
        "Zhuliang Yao",
        "Qi Dai",
        "Han Hu"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "57",
      "title": "NeuroLM: A universal multi-task foundation model for bridging the gap between language and EEG signals",
      "authors": [
        "Weibang Jiang",
        "Yansen Wang",
        "Bao Liang Lu",
        "Dongsheng Li"
      ],
      "year": "2025",
      "venue": "The International Conference on Learning Representations"
    },
    {
      "citation_id": "58",
      "title": "Bci competition 2008-graz data set a. Institute for knowledge discovery (laboratory of brain-computer interfaces",
      "authors": [
        "Clemens Brunner",
        "Robert Leeb",
        "Gernot Müller-Putz",
        "Alois Schlögl",
        "Gert Pfurtscheller"
      ],
      "year": "2008",
      "venue": "Bci competition 2008-graz data set a. Institute for knowledge discovery (laboratory of brain-computer interfaces"
    },
    {
      "citation_id": "59",
      "title": "Bci2000: a general-purpose brain-computer interface (bci) system",
      "authors": [
        "Gerwin Schalk",
        "Dennis Mcfarland",
        "Thilo Hinterberger",
        "Niels Birbaumer",
        "Jonathan Wolpaw"
      ],
      "year": "2004",
      "venue": "IEEE Transactions on biomedical engineering"
    },
    {
      "citation_id": "60",
      "title": "A large eeg dataset for studying cross-session variability in motor imagery brain-computer interface",
      "authors": [
        "Jun Ma",
        "Banghua Yang",
        "Wenzheng Qiu",
        "Yunzhe Li",
        "Shouwei Gao",
        "Xinxing Xia"
      ],
      "year": "2022",
      "venue": "Scientific Data"
    },
    {
      "citation_id": "61",
      "title": "Gnn4eeg: A benchmark and toolkit for electroencephalography classification with graph neural network",
      "authors": [
        "Kaiyuan Zhang",
        "Ziyi Ye",
        "Qingyao Ai",
        "Xiaohui Xie",
        "Yiqun Liu"
      ],
      "year": "2024",
      "venue": "Companion of the ACM International Joint Conference on Pervasive and Ubiquitous Computing"
    },
    {
      "citation_id": "62",
      "title": "Comparing recognition performance and robustness of multimodal deep learning models for multimodal emotion recognition",
      "authors": [
        "Wei Liu",
        "Jie-Lin Qiu",
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "63",
      "title": "Physiobank, physiotoolkit, and physionet: components of a new research resource for complex physiologic signals",
      "authors": [
        "Ary L Goldberger",
        "A Luis",
        "Leon Amaral",
        "Jeffrey Glass",
        "Plamen Hausdorff",
        "Roger Ch Ivanov",
        "Joseph Mark",
        "George Mietus",
        "Chung-Kang Moody",
        "H Eugene Peng",
        "Stanley"
      ],
      "year": "2000",
      "venue": "Physiobank, physiotoolkit, and physionet: components of a new research resource for complex physiologic signals"
    },
    {
      "citation_id": "64",
      "title": "Application of machine learning to epileptic seizure onset detection and treatment",
      "authors": [
        "Ali Hossam"
      ],
      "year": "2009",
      "venue": "Application of machine learning to epileptic seizure onset detection and treatment"
    },
    {
      "citation_id": "65",
      "title": "Eeg synchronization analysis for seizure prediction: A study on data of noninvasive recordings",
      "authors": [
        "Paolo Detti",
        "Giampaolo Vatti",
        "Garazi Zabalo",
        "Manrique De"
      ],
      "year": "2020",
      "venue": "Processes"
    },
    {
      "citation_id": "66",
      "title": "Isruc-sleep: A comprehensive public dataset for sleep researchers",
      "authors": [
        "Sirvan Khalighi",
        "Teresa Sousa",
        "José Moutinho Santos",
        "Urbano Nunes"
      ],
      "year": "2016",
      "venue": "Computer methods and programs in biomedicine"
    },
    {
      "citation_id": "67",
      "title": "Inter-database validation of a deep learning approach for automatic sleep scoring",
      "authors": [
        "Diego Alvarez-Estevez",
        "Roselyne Rijsman"
      ],
      "year": "2021",
      "venue": "PloS one"
    },
    {
      "citation_id": "68",
      "title": "A multimodal approach to estimating vigilance using eeg and forehead eog",
      "authors": [
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2017",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "69",
      "title": "Electroencephalograms during mental arithmetic task performance",
      "authors": [
        "Igor Zyma",
        "Sergii Tukaev",
        "Ivan Seleznov",
        "Ken Kiyono",
        "Anton Popov",
        "Mariia Chernykh",
        "Oleksii Shpenkov"
      ],
      "year": "2019",
      "venue": "Data"
    },
    {
      "citation_id": "70",
      "title": "MDD Patients and Healthy Controls EEG Data (New)",
      "authors": [
        "Wajid Mumtaz"
      ],
      "year": "2016",
      "venue": "MDD Patients and Healthy Controls EEG Data (New)"
    },
    {
      "citation_id": "71",
      "title": "The temple university hospital eeg data corpus",
      "authors": [
        "Iyad Obeid",
        "Joseph Picone"
      ],
      "year": "2016",
      "venue": "Frontiers in neuroscience"
    },
    {
      "citation_id": "72",
      "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "authors": [
        "Michael Ramprasaath R Selvaraju",
        "Abhishek Cogswell",
        "Ramakrishna Das",
        "Devi Vedantam",
        "Dhruv Parikh",
        "Batra"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "73",
      "title": "Event-related cortical desynchronization detected by power measurements of scalp eeg",
      "authors": [
        "Gert Pfurtscheller",
        "Aranibar"
      ],
      "year": "1977",
      "venue": "Electroencephalography and clinical neurophysiology"
    },
    {
      "citation_id": "74",
      "title": "Speech production: Wernicke, broca and beyond",
      "authors": [
        "S Catrin",
        "Sophie Blank",
        "Kevin Scott",
        "Elizabeth Murphy",
        "Richard Js Warburton",
        "Wise"
      ],
      "year": "2002",
      "venue": "Brain"
    },
    {
      "citation_id": "75",
      "title": "Revealing the spatiotemporal brain dynamics of covert speech compared with overt speech: A simultaneous eeg-fmri study",
      "authors": [
        "Wei Zhang",
        "Muyun Jiang",
        "Kok Ann",
        "Colin Teo",
        "Raghavan Bhuvanakantham",
        "Laiguan Fong",
        "Wei Khang",
        "Jeremy Sim",
        "Zhiwei Guo",
        "Chuan Huat",
        "Vince Foo",
        "Rong Hui",
        "Jonathan Chua",
        "Parasuraman Padmanabhan"
      ],
      "year": "2024",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "76",
      "title": "Fft-based deep feature learning method for eeg classification",
      "authors": [
        "Mingyang Li",
        "Wanzhong Chen"
      ],
      "year": "2021",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "77",
      "title": "Epileptic seizure classification of eeg time-series using rational discrete short-time fourier transform",
      "authors": [
        "Kaveh Samiee",
        "Peter Kovacs",
        "Moncef Gabbouj"
      ],
      "year": "2014",
      "venue": "IEEE transactions on Biomedical Engineering"
    },
    {
      "citation_id": "78",
      "title": "An eeg based real-time epilepsy seizure detection approach using discrete wavelet transform and machine learning methods",
      "authors": [
        "Mingkan Shen",
        "Peng Wen",
        "Bo Song",
        "Yan Li"
      ],
      "year": "2022",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "79",
      "title": "Optimizing spatial filters for robust eeg single-trial analysis",
      "authors": [
        "Benjamin Blankertz",
        "Ryota Tomioka",
        "Steven Lemm",
        "Motoaki Kawanabe",
        "Klaus-Robert Muller"
      ],
      "year": "2007",
      "venue": "IEEE Signal processing magazine"
    },
    {
      "citation_id": "80",
      "title": "Multiclass braincomputer interface classification by riemannian geometry",
      "authors": [
        "Alexandre Barachant",
        "Stéphane Bonnet",
        "Marco Congedo",
        "Christian Jutten"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "81",
      "title": "Differential evolution algorithm as a tool for optimal feature subset selection in motor imagery eeg",
      "authors": [
        "Nauman Muhammad Zeeshan Baig",
        "Aslam",
        "P Hubert",
        "Li Shum",
        "Zhang"
      ],
      "year": "2017",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "82",
      "title": "Multiclass support vector machines for eeg-signals classification",
      "authors": [
        "Inan Guler",
        "Elif Derya"
      ],
      "year": "2007",
      "venue": "IEEE transactions on information technology in biomedicine"
    },
    {
      "citation_id": "83",
      "title": "Comparison of signal decomposition methods in classification of eeg signals for motor-imagery bci system",
      "authors": [
        "Jasmin Kevric",
        "Abdulhamit Subasi"
      ],
      "year": "2017",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "84",
      "title": "Logistic regression for single trial eeg classification",
      "authors": [
        "Ryota Tomioka",
        "Kazuyuki Aihara",
        "Klaus-Robert Müller"
      ],
      "year": "2006",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "85",
      "title": "Deep learning with convolutional neural networks for eeg decoding and visualization",
      "authors": [
        "Robin Tibor Schirrmeister",
        "Jost Tobias Springenberg",
        "Lukas Dominique",
        "Josef Fiederer",
        "Martin Glasstetter",
        "Katharina Eggensperger",
        "Michael Tangermann",
        "Frank Hutter",
        "Wolfram Burgard",
        "Tonio Ball"
      ],
      "year": "2017",
      "venue": "Human brain mapping"
    },
    {
      "citation_id": "86",
      "title": "Lmda-net: A lightweight multi-dimensional attention network for general eeg-based brain-computer interfaces and interpretability",
      "authors": [
        "Zhengqing Miao",
        "Meirong Zhao",
        "Xin Zhang",
        "Dong Ming"
      ],
      "year": "2023",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "87",
      "title": "Lstm-based eeg classification in motor imagery tasks",
      "authors": [
        "Ping Wang",
        "Aimin Jiang",
        "Xiaofeng Liu",
        "Jing Shang",
        "Li Zhang"
      ],
      "year": "2018",
      "venue": "IEEE transactions on neural systems and rehabilitation engineering"
    },
    {
      "citation_id": "88",
      "title": "An efficient lstm network for emotion recognition from multichannel eeg signals",
      "authors": [
        "Xiaobing Du",
        "Cuixia Ma",
        "Guanhua Zhang",
        "Jinyao Li",
        "Yu-Kun Lai",
        "Guozhen Zhao",
        "Xiaoming Deng",
        "Yong-Jin Liu",
        "Hongan Wang"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "89",
      "title": "Sleeptransformer: Automatic sleep staging with interpretability and uncertainty quantification",
      "authors": [
        "Huy Phan",
        "Kaare Mikkelsen",
        "Philipp Oliver Y Chén",
        "Alfred Koch",
        "Maarten Mertins",
        "Vos"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "90",
      "title": "Graphsleepnet: Adaptive spatial-temporal graph convolutional networks for sleep stage classification",
      "authors": [
        "Ziyu Jia",
        "Youfang Lin",
        "Jing Wang",
        "Ronghao Zhou",
        "Xiaojun Ning",
        "Yuanlai He",
        "Yaoshuai Zhao"
      ],
      "year": "2020",
      "venue": "Ijcai"
    },
    {
      "citation_id": "91",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies"
    },
    {
      "citation_id": "92",
      "title": "Language models are few-shot learners",
      "authors": [
        "Tom Brown",
        "Benjamin Mann",
        "Nick Ryder",
        "Melanie Subbiah",
        "Jared Kaplan",
        "Prafulla Dhariwal",
        "Arvind Neelakantan",
        "Pranav Shyam",
        "Girish Sastry",
        "Amanda Askell"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "93",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "94",
      "title": "FlexCAD: Unified and versatile controllable CAD generation with fine-tuned large language models",
      "authors": [
        "Zhanwei Zhang",
        "Shizhao Sun",
        "Wenxiao Wang",
        "Deng Cai",
        "Jiang Bian"
      ],
      "year": "2025",
      "venue": "The Thirteenth International Conference on Learning Representations"
    },
    {
      "citation_id": "95",
      "title": "Learning from observer gaze: Zero-shot attention prediction oriented by human-object interaction recognition",
      "authors": [
        "Yuchen Zhou",
        "Linkai Liu",
        "Chao Gou"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "96",
      "title": "Skeleton-incontext: Unified skeleton sequence modeling with in-context learning",
      "authors": [
        "Xinshun Wang",
        "Zhongbin Fang",
        "Xia Li",
        "Xiangtai Li",
        "Chen Chen",
        "Mengyuan Liu"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "97",
      "title": "Drivinggen: Efficient safety-critical driving video generation with latent diffusion models",
      "authors": [
        "Zipeng Guo",
        "Yuchen Zhou",
        "Chao Gou"
      ],
      "year": "2024",
      "venue": "2024 IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "98",
      "title": "Brainbert: Self-supervised representation learning for intracranial recordings",
      "authors": [
        "Christopher Wang",
        "Vighnesh Subramaniam",
        "Uri Adam",
        "Gabriel Yaari",
        "Boris Kreiman",
        "Ignacio Katz",
        "Andrei Cases",
        "Barbu"
      ],
      "year": "2023",
      "venue": "Brainbert: Self-supervised representation learning for intracranial recordings",
      "arxiv": "arXiv:2302.14367"
    },
    {
      "citation_id": "99",
      "title": "Foundation model for intracranial neural signal",
      "authors": [
        "Daoze Zhang",
        "Zhizhang Yuan",
        "Yang Yang",
        "Junru Chen",
        "Jingjing Wang",
        "Yafeng Li",
        "Brant"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "100",
      "title": "Eeg2rep: enhancing self-supervised eeg representation through informative masked inputs",
      "authors": [
        "Navid Mohammadi Foumani",
        "Geoffrey Mackellar",
        "Soheila Ghane",
        "Saad Irtza",
        "Nam Nguyen",
        "Mahsa Salehi"
      ],
      "year": "2024",
      "venue": "Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining"
    },
    {
      "citation_id": "101",
      "title": "Ten-twenty electrode system of the international federation",
      "authors": [
        "H Herbert",
        "Jasper"
      ],
      "year": "1958",
      "venue": "Electroencephalogr Clin Neurophysiol"
    },
    {
      "citation_id": "102",
      "title": "Ten percent electrode system for topographic studies of spontaneous and evoked eeg activities",
      "authors": [
        "Gian Emilio Chatrian",
        "Ettore Lettich",
        "Paula Nelson"
      ],
      "year": "1985",
      "venue": "American Journal of EEG technology"
    },
    {
      "citation_id": "103",
      "title": "Guideline thirteen: guidelines for standard electrode position nomenclature",
      "year": "1994",
      "venue": "J Clin Neurophysiol"
    },
    {
      "citation_id": "104",
      "title": "Three-dimensional probabilistic anatomical cranio-cerebral correlation via the international 10-20 system oriented for transcranial functional brain mapping",
      "authors": [
        "Masako Okamoto",
        "Haruka Dan",
        "Kuniko Sakamoto",
        "Kazuhiro Takeo",
        "Koji Shimizu",
        "Satoru Kohno",
        "Ichiro Oda",
        "Seiichiro Isobe",
        "Tateo Suzuki",
        "Kaoru Kohyama"
      ],
      "year": "2004",
      "venue": "Neuroimage"
    },
    {
      "citation_id": "105",
      "title": "Electroencephalography: basic principles, clinical applications, and related fields",
      "authors": [
        "Henry Craig"
      ],
      "year": "2006",
      "venue": "Neurology"
    },
    {
      "citation_id": "106",
      "title": "The american academy of sleep medicine manual for the scoring of sleep and associated events",
      "authors": [
        "S Iber",
        "C Ancoli-Israel",
        "Chesson",
        "Quan"
      ],
      "year": "2007",
      "venue": "The american academy of sleep medicine manual for the scoring of sleep and associated events"
    },
    {
      "citation_id": "107",
      "title": "Automatic sleep staging of eeg signals: recent development, challenges, and future directions",
      "authors": [
        "Huy Phan",
        "Kaare Mikkelsen"
      ],
      "year": "2022",
      "venue": "Physiological Measurement"
    },
    {
      "citation_id": "108",
      "title": "NeuroLM: A universal multi-task foundation model for bridging the gap between language and EEG signals",
      "authors": [
        "Weibang Jiang",
        "Yansen Wang",
        "Bao Liang Lu",
        "Dongsheng Li"
      ],
      "year": "2025",
      "venue": "The International Conference on Learning Representations"
    },
    {
      "citation_id": "109",
      "title": "Scaling laws for neural language models",
      "authors": [
        "Jared Kaplan",
        "Sam Mccandlish",
        "Tom Henighan",
        "Tom Brown",
        "Benjamin Chess",
        "Rewon Child",
        "Scott Gray",
        "Alec Radford",
        "Jeffrey Wu",
        "Dario Amodei"
      ],
      "year": "2020",
      "venue": "Scaling laws for neural language models",
      "arxiv": "arXiv:2001.08361"
    },
    {
      "citation_id": "110",
      "title": "Visualizing data using t-sne",
      "authors": [
        "Laurens Van Der Maaten",
        "Geoffrey Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    }
  ]
}