{
  "paper_id": "2203.00456v4",
  "title": "Wemac: Women And Emotion Multi-Modal Affective Computing Dataset",
  "published": "2022-03-01T13:39:54Z",
  "authors": [
    "Jose A. Miranda",
    "Esther Rituerto-González",
    "Laura Gutiérrez-Martín",
    "Clara Luis-Mingueza",
    "Manuel F. Canabal",
    "Alberto Ramírez Bárcenas",
    "Jose M. Lanza-Gutiérrez",
    "Carmen Peláez-Moreno",
    "Celia López-Ongil"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "WEMAC is a unique open multi-modal dataset that comprises physiological, speech, and self-reported emotional data records of 100 women, targeting Gender-based Violence detection. Realistic emotions were elicited through visualizing a validated video set using an immersive virtual reality headset. The physiological signals captured during the experiment include blood volume pulse, galvanic skin response, and skin temperature. The speech was acquired right after the stimuli visualization to capture the final traces of the perceived emotion. Subjects were asked to annotate among 12 categorical emotions, several dimensional emotions with a modified version of the Self-Assessment Manikin, and liking and familiarity labels. The technical validation proves that all the targeted categorical emotions show a strong statistically significant positive correlation with their corresponding reported ones. That means that the videos elicit the desired emotions in the users in most cases. Specifically, a negative correlation is found when comparing fear and not-fear emotions, indicating that this is a well-portrayed emotional dimension, a specific, though not exclusive, purpose of WEMAC towards detecting gender violence.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Background & Summary",
      "text": "Gender-based Violence (GBV) is a violation of human rights and fundamental liberties as declared by the United Nations in 1993  [1] . This declaration states that GBV is any act of physical, sexual, or psychological violence directed toward the female gender. A datum that helps ponder its impact on society shows that more than 27% of ever-partnered women aged between 15 and 49 experienced physical or sexual violence by intimate partners from 2000 to 2018  [2] . Another worrying statistic is that, in 2020, approximately 47, 000 women and girls were killed worldwide by their intimate partners or other family members, meaning a woman or girl is killed by someone in her own family every 11 minute  [3] . On this basis, it can be understood that GBV is an urgent problem worldwide that should be addressed by science and society. From a sociological point of view, particular emphasis on education is essential to eradicate, combat, and prevent GBV, but it requires several generations to produce a change in society. In the meantime, technology can be fundamental for helping prevent and combat GBV  [4]  and empowering women. With this focus, the multidisciplinary team UC3M4Safety 1 was created in 2017 to propose Bindi, an inconspicuous autonomous system powered by artificial intelligence and deployed under the Internet of Things (IoT) paradigm. The goal of Bindi is to automatically report when a woman is in a GBV-related risk situation to trigger a protection protocol  [5] . This risk situation identification is performed by detecting fear-related emotions in the user through a multi-modal intelligent engine feeding with physiological and audio data captured by a pair of wearable devices: a bracelet and a pendant.\n\nOne of the main shortcomings of training this specific fear detection system for Bindi is the lack of adequate datasets. Over the last decades, several datasets were published providing emotional labels together with auditory and physiological variables, such as DEAP  [6] , MAHNOB  [7] , WESAD  [8] , AMIGOS  [9] , FAU, Reg, and Ulm TSST Corpora  [10] , and BioSpeech  [11] .\n\nHowever, the target of these datasets is the classification of a generic set of emotions instead of focusing on fear detection. This strategy makes it hard to obtain a robust model due to the lack of fear samples. Moreover, gender perspective was not considered either, in spite of stimuli interpretation being strongly affected by gender  [12] . Instead, a sufficient number of women volunteers and a balanced fear and non-fear set of emotions should be considered to fit the target users of the GBV application.\n\nTo fill this gap, UC3M4Safety generated the UC3M4Safety Database  [13] , which includes the Women and Emotion Multi-modal Affective Computing (WEMAC) and audiovisual stimuli datasets, as listed in Table  1 . This paper presents WEMAC, a collection of physiological and audio data captured in a virtual reality set-up where women volunteers are exposed to a subset of audiovisual stimuli previously rated by experts judges and an online crowd-sourcing procedure  [14]    [15] . For each volunteer, WEMAC includes the answers to an initial questionnaire  [16] , physiological signals  [17] , features extracted from the volunteer's speech data to ensure confidentiality  [18] , and self-reported emotional labels  [19] .",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Database",
      "text": "Datasets Conditions Participants",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Uc3M4Safety",
      "text": "Database  [13]  Audiovisual Stimuli: Videos  [14]  Online crowd-sourcing General public and Audiovisual Stimuli: Emotional Ratings  [15]  expert judges WEMAC: Biopsychosocial Questionnaire  [16]  Laboratory Women volunteers WEMAC: Physiological Signals  [17]  WEMAC: Audio Features  [18]  WEMAC: Self-reported Emotional Annotations  [19]  Table  1 . Hierarchy and subdivisions of the UC3M4Safety Database datasets.\n\nWEMAC offers several advantages over other state-of-the-art datasets: 1) the use of immersive technology (virtual reality) to elicit emotions, which offers a high degree of correlation between the research conditions and the emotional phenomenon under study; 2) a high number of volunteers (100), higher than any other public dataset of this type to the best of our knowledge; 3) a specifically designed modification of the labeling methodology to consider gender perspective by changing the design of the original Self-Assessment Manikins (SAMs)  [20] ; 4) the implementation of an online recovery process to ensure a physiological stabilization between stimuli, and 5) the usage of different sensory systems to provide a heterogeneous set-up approach.\n\nWEMAC is intended, but not limited, to address research questions related to 1) affective computing using multi-modal information, 2) the design of solutions to the very challenging problem of GBV, 3) the understanding of subjective and self-reported emotional labeling, and 4) fear classification in women.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Methods",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Ethics Statement",
      "text": "The experimentation was approved by and performed following the guidelines and regulations of the Ethics in Research Committee of University Carlos III Madrid. The approval was granted considering the circumstances of the research project entitled Integral protection of gender-based violence victims using multimodal affective computing  2  .\n\nThe submission to the Ethical Committee covered essential topics for the development of the experiments. Among others, the adequacy of the volunteers' informed consent, the research goals and plans, the data management and de-identification procedures, and the compliance with the European General Data Protection Regulation (GDPR). The aforementioned written informed consent asserts that the volunteers 1) were aware of the research, its objectives, purposes, and how their data was planned to be used; 2) were informed regarding the experimental procedure, the possibility of refusing to participate in the research at any point, and the right to request data erasure; 3) could ask any question during the experiments. Moreover, they granted permission for processing their personal data to the extent necessary for the implementation of the research project, including sharing with other researchers their physiological recording and speech features, as well as the initial questionnaires and self-reported annotations.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Participants",
      "text": "The volunteers were recruited by using different communication channels, including social networks (such as Facebook, Twitter, or Instagram), Women's Associations, and the collaboration of the municipality of Leganés and Getafe (Madrid, Spain), resulting in an initial selection of 144 Spanish-speaking women volunteers. This initial set was reduced due to technical problems during the recording or sickness caused by the virtual reality set-up involving the interruption of the experiment. The final set includes 100 women volunteers aged between 20 and 77 (with an average of 39.92 and a standard deviation of 14.26). For the purpose of covering a wide age range, the recruitment process requested a balanced number of volunteers in five age groups defined as G1 (18 -24) for which we recruited 22 volunteers, G2 (25 -34) with 18 volunteers and G3  (35 -44) , G4 (45 -54), and G5 (≥ 55) that include 20 volunteers each.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Stimuli",
      "text": "The selection of audiovisual clips to record WEMAC is the result of a thorough study done with the purpose of collecting a high-quality set of audiovisual stimuli able to trigger realistic emotions under a controlled scenario  [21] . To this end, 370 samples with emotional content were initially collected on the Internet from commercial films, series, documentaries, short films, commercials, and video clips. The set was labeled with the advice of a panel of experts seeking to elicit the following 12 discrete emotions: joy, sadness, surprise, contempt, hope, fear, attraction, disgust, tenderness, anger, calm, and tedium. After that, the team discarded those clips over two minutes long, those needing context to be understood, or that elicited more than one target emotion. Afterward, the resulting 162 clips were surveyed in an online crowd-sourcing poll to be labeled with discrete emotion categories after its visualization, getting 1, 520 volunteer annotators (929 women and 591 men). A further selection was performed by considering two conditions: 1) at least 50% of the volunteers (considering both genders or at least 50% of one gender individually) labeled the clip with the same discrete emotion; 2) the rest of the emotions labeled only reached a maximum of 30% agreement to avoid confusion with the target emotion. After this selection, none of the videos previously selected for attraction, contempt, hope, and tedium had greater than 50% agreement among the responses obtained, so the final set of videos covers a list of 8 target emotions joy, sadness, surprise, fear, attraction, disgust, tenderness, anger, and calm. Finally, some videos were discarded to obtain an even distribution between fear and non-fear emotions. It resulted in the Audiovisual Stimuli dataset  [14]  with 42 clips and a distribution of 44.44% for fear and 55.55% for no-fear  [15] .\n\nApplying the whole Audiovisual Stimuli dataset in WEMAC would be unfeasible due to the excessive duration of the resulting experimentation. Thus, two batches of videos are generated so that the experimental procedure lasts 1 to 1.5 hours per volunteer. The selection criterion for the videos in these two batches is based on three premises: the emotional highest discrete labeling agreement, targeting for an adequate laboratory experiment duration, and a balanced distribution of fear and non-fear categories within the four quadrants in the Pleasure-Arousal space  [22] . Table  2  shows details about the 14 selected videos for each batch in WEMAC, including the stimuli identification (Stimuli ID, with the same notation as followed in the Audiovisual Stimuli dataset  [14] ), the visualization order in the experimentation, the emotion label reported in the online crowd-sourcing study, the video duration, and the visualization format.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Measures",
      "text": "Volunteers' annotations are collected in two instants: prior to the experiment and during the experimentation. Before the experiment, each volunteer is provided with informed consent, a personal data form, and a general questionnaire to supply additional information related to cognition, appraisal, attention, personality traits, gender, and age. In this regard, the general questionnaire collects age group, recent physical activity or medication that can alter the physiological response of the participant, self-identified emotional burdens due to work, economic and personal situation, and mood biases (fears, phobias, and previous traumatic experiences).\n\nDuring the experimental protocol, in addition to physiological information, the following self-assessment annotations are obtained after each visualized stimulus:\n\n• Speech-based labeling: two questions regarding each video stimulus are asked immediately after its visualization. Their goal is to make the user relive the emotions felt during viewing. Some examples are \"What did you feel during the visualization of the video?\" or \"Could you describe what happened in the video using your own words?\". The answer is stored as an audio signal. Note that both questions and answers are in Spanish.\n\n• Valence, Arousal, and Dominance: annotated using a 9-point Likert scale supported by modified SAMs. Details about the redesign process of the SAMs can be found in  [20] .\n\n• Familiarity with the emotion felt, the situation displayed in the clip, and the specific clip: annotated in three different questions. The two first consider a 9-point Likert scale, whereas the last one considers a binary yes-no option.\n\n• Liking of the video: annotated through a binary yes-no question. • A discrete emotion out of a total of 12 (joy, sadness, surprise, contempt, hope, fear, attraction, disgust, tenderness, anger, calm, and tedium)  [21] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Apparatus",
      "text": "The equipment employed to capture the physiological and speech data is as follows:\n\n• The BioSignalPlux 3  research toolkit system. It is a commonly used device to acquire different physiological signals in the literature  [23, 24, 25, 26] . More specifically, we capture finger Blood Volume Pulse (BVP), ventral wrist Galvanic Skin Response (GSR), forearm Skin Temperature (SKT), trapezoidal Electromyography (EMG), chest respiration (RESP), and inertial wrist movement through an accelerometer.\n\n• The Oculus Rift® S VR Headset  4  . Its embedded microphone captures the speech signal produced during the speech-based labeling at a sampling rate of 48 kHz mono and a depth of 16 bits. This device also guides volunteers through the study. To this end, an interactive virtual reality environment developed in Unity software  5  presents stimuli and collects self-assessments. More details about this virtual environment can be found in  [27] .\n\n• Additionally, two in-house sensory systems are employed. On the one hand, the Bindi's bracelet  [28]  measures dorsal wrist BVP, ventral wrist GSR, and forearm SKT. The hardware and software particularities of this device are detailed in  [29, 30, 31] . The previously mentioned BioSignalPlux toolkit is employed as a golden standard to analyze the performance of its sensors due to its experimental nature. BVP and GSR signals from BioSignalPlux and Bindi were successfully compared and correlated with Bindi in  [30]  and  [31] . On the other hand, a GSR sensor to be integrated into the next version of the Bindi bracelet is used. Its hardware and software particularities are detailed in  [32] .\n\nNote that the synchronization of all the different sensors acquisition together with the stages of the experiment is performed using a laptop (MSI GE75 Raider 8SE-034ES) running a Unity® framework-based program. On the one hand, the BiosignalPlux device connection is configured using the OpenSignals (r)evolution software  6  , and its TCP/IP module is used to facilitate the data exchange between this platform and the Unity® framework. On the other hand, the additional in-house sensory systems are wirelessly connected to the laptop using Bluetooth Low Energy dongles. The information storage is divided by scenes and marked individually with a timestamp set by the environment employed. The sampling frequency for all physiological signals is 200 Hz.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Procedure",
      "text": "The study was conducted between October 2020 and April 2021. It took place in the Electronics Technology Department at the School of Engineering of the Universidad Carlos III de Madrid, Spain. The experimental methodology designed to be applied for each volunteer is schematized in Figure  1 . During this experiment, at least one researcher and one psychologist remained in the room at the disposal of the participants in case they needed any help. Upon arrival, participants were informed about the experimental procedure. Then, they signed the informed consent, filled out the personal data form, and answered the general questionnaire. Next, participants listened to instructions regarding the experiment. The participants were requested to avoid unnecessary actions or movements during the experiment (e.g., turning the wrist). They were also informed that they could skip any clip or quit the experiment at any time. Once the procedure was clear to the participants, the sensors were set up, as well as the Virtual Reality Headset. Next, the participants followed a tutorial to get used to the headset, joystick, interactive screens, and the particularities of the different annotations. The main part of the experiment consisted of fourteen iterations (one per stimulus) of i) a neutral clip sampled from  [33] , ii) the visualization of the emotional film clip, iii) different interactive screens of self-assessment annotations, and iv) 3D recovery landscape scenes.\n\nThe full experiment, including documentation reading, equipment set-up, tutorial, and visualization of the clips, lasted around 80 minutes per participant, depending on the time spent on the questionnaires.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Data Processing And Cleaning",
      "text": "The whole set of physiological signals captured for the 100 volunteers has a total duration of 34:51:50 (HH:MM:SS), whereas the audio signals altogether last 12:19:25 since they are only saved during the speech-based annotation.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Physiological Signals",
      "text": "The signals being released are the ones acquired by the BioSignalPlux research toolkit. Specifically, the raw and filtered BVP, GSR, and SKT signals captured during every video visualization are provided. The preprocessing is as follows:\n\n1. Two main filters are applied to the BVP signal due to the noise problems observed. On the one hand, high-frequency noise is filtered out using a direct-form low-pass Finite Impulse Response (FIR) filter with 6 dB at 3.5 Hz. Note that a\n\nHamming window is used during the filter design process to minimize the first side lobe properly. On the other hand, the residual baseline-wander or low-frequency drift effect presented in the signal is removed using a forward-backwards low-pass Butterworth Infinite Impulse Response (IIR) filtering stage. Specifically, the forward-backwards technique handles the non-linear phase of such filters.\n\n2. For the GSR and SKT signals, a basic FIR filtering with 2 Hz cut-off frequency is applied. After that, this filtered output is downsampled to 10 Hz and also processed with both a moving average and a moving median filter. The former used a 1 s long window and helped to reduce the high noise residual after the initial FIR, whereas the latter employed a 0.5 s window and dealt with the rapid transients.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Speech Signals",
      "text": "The speech signals recorded have a duration ranging from 20 s to 60 s and mostly contain speech. Since the release of the raw speech signals is not possible due to ethics and privacy issues  7  , we preprocessed the speech signals and extracted both low and high-level features so that the research community can analyze and work with them.\n\nThe preprocessing is as follows. A low pass filter at 8 kHz is applied, where most of the energy is concentrated. A high pass filter with a cut-off frequency of 50 Hz is also applied to remove the electrical power line interference. Afterward, the audio amplitude is normalized per participant to fit the range [-1, 1]. That means the normalization per participant was performed using all her audio signals. Next, downsampling at 16 kHz is performed with the librosa Python library  [34]  to facilitate the handling of the signals. Finally, the signals are padded with zeros to fill in the incomplete last second.\n\nDifferent Python toolkits are used to extract information from the preprocessed signal at a window size of 1 s without overlapping. We follow a similar approach to the one followed in the MuSe Challenge 2021  [35]  for the feature and embedding extraction of the audio signals. That is:\n\n1. librosa  [34]  [36]: 19 features are extracted by the librosa Python toolkit (13 Mel-Frequency Cepstral Coefficients, Root Mean Square or Energy, Zero Crossing Rate, Spectral Centroid, Spectral Roll-off, Spectral Flatness, and Pitch) at a window size of 20 ms and a hop size of 10 ms. Also, the mean and standard deviation for each of the 19 features are computed for every 1 s window. As a result, 38 speech features are computed.\n\n2. eGeMAPS  [37] : 88 Low-Level Descriptors (LLDs) related to speech and audio are extracted through the openSMILE Python toolkit  [38]  on its default configuration, i.e., F0, harmonic features, HNR, jitter, and shimmer are computed with a 60 ms window. Loudness, spectral slope, formants, harmonics, Hammarberg Index, and Alpha ratio are computed with a 25 ms window. They are all further averaged in 1 s windows.\n\n3. ComParE: 6, 373 features used in the ComParE 2016 challenge  [39]  are extracted by using the openSMILE Python toolkit. The default window and step sizes are used as in  [40] , i.e., F0, jitter, and shimmer are computed with a window size of 60 ms and a step of 10 ms. All other LLDs are computed with a window size of 20 ms and a step of 10 ms. Then all features are further averaged in 1 s windows.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Deepspectrum [41]:",
      "text": "This toolkit is used for the extraction of audio embeddings based on different deep neural network architectures trained with ImageNet  [42] . Specifically, two different configurations are considered, and therefore two embeddings sets are extracted, i.e., ResNet50 and the output of its last Average Pooling layer (avg_pool), resulting in 2048-dimensional embeddings, and VGG-19 and its last Fully Connected layer (fc2), resulting in 4096-dimensional embeddings.\n\n5. VGGish: The 128-dimensional embeddings from the output layer of the VGG-19 network trained for AudioSet  [43]  are also included.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Pase+:",
      "text": "The 256-dimensional features from the PASE+ (Problem Agnostic Speech Encoder+)  [44]  encoder network, used as speech feature extractor, are also provided.\n\n1. Bio-psycho-social questionnaire and informed consent  [16] : a tabular data CSV (Comma-Separated Values) file with 100 rows corresponding to each of the volunteers and 14 columns corresponding to the information completed in the biopsychosocial questionnaire at the beginning of the experiment. This information includes:  (1)   2. Self-reported annotations  [19] : They contain the emotional labeling reported by the participants after watching each of the 14 videos in the experiment. The data are stored in one CSV file, that contains 14 columns and 1, 400 rows (100 volunteers × 14 clips). Regarding the columns, 5 of them refer to information about the volunteer (identifier and age group) and the video (batch, display position, and video code number), whereas 9 of them refers to the self-assessment provided (arousal, valence, dominance, liking, reported and target discrete emotions, and familiarity scores for the emotion, the situation, and the clip).\n\n3. Physiological signals  [17] : BVP, GSR, and SKT physiological signals captured during the experimentation by the BioSignalPlux research toolkit are provided in a binary MATLAB® file (.mat). It contains a cell array with 100 rows (one per volunteer) and 14 columns (one per video). Each cell contains four fields: volunteer identifier, clip or trial identifier, filtering indicator, and an inner cell array (with the physiological data associated with that specific clip and volunteer).\n\n4. Speech features  [18] : The speech features contain 6 folders: librosa, eGeMAPS, Compare, Deepspectrum-Resnet50, Deepspectrum-VGG19, and VGGish. They correspond to the six feature sets described in the Data Processing and Cleaning section for Speech Signals. Each folder includes a CSV file per audiovisual stimuli and volunteer. Each CSV has as many columns as the number of features calculated, where an additional first column is also included referring to the timestamp (in seconds). The number of rows fits with the number of seconds the speech signal lasts.\n\nBesides data collection, additional informative documents regarding, for instance, sensor placement during the experiment or the self-assessment instructions, are included in each folder of the data repository.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Technical Validation",
      "text": "Emotional elicitation and labeling is a complex task, and sometimes the expected (or targeted) emotions are not the ones the volunteers experienced (or reported). The agreement between the target class and the self-reported discrete emotion annotations by the volunteers in this experiment is shown in the matrix in Figure  2 , where it is observed as the ratio of times a targeted emotion is identified and felt as such by the volunteers. Thus, a value of 1.00 means a perfect agreement between the targeted emotion and the emotion felt, and 0.00 means no agreement. As introduced before, only 8 of the 12 emotions initially selected were included in WEMAC (see the Stimuli Section), although the 12 emotions were considered for the discrete emotion labeling (see the Measures Section). It means that the number of targeted emotions is smaller than the reported ones in this matrix. Analyzing this figure it can be found that the non-included emotions (attraction, contempt, hope and tedium) are very scarcely selected with the exception of the 17% of times a stimulus expected to represent anger is taken as contempt. It is also observed that sadness, calm, joy and fear are the emotions best identified, being the agreement in the fear emotion especially relevant for the use case. Tenderness and disgust are also quite well portrayed by the stimuli while anger is often taken as disgust or contempt, and amusement as joy or disgust.\n\nTo statistically confirm the associations observed between the target and self-reported discrete emotions, a Pearson's chi-squared test is used as implemented in the chisq.test() function in the R software. In this test, the null hypothesis (H 0 ) states that the variables (i.e., the target and reported emotions) are independent, meaning there is no relationship between categorical variables. As a result of this test, a p-value equaling 0.0005 is obtained, resulting in the dependency between categories is confirmed, with a 0.01 significance level. Then, we perform a study regarding the relationships between the different target and reported individual emotions. To this end, a posthoc analysis with Bonferroni correction for multiple comparisons is performed for each combination of target and reported emotion. This analysis is performed by means of the Pearson standardized residuals (Z-factor) implemented in the stdres component of the result of the previous chisq.test() function in the R software. The Z-factors appear in Table  3 , where a strong dependence occurs when the absolute value of the Z-factor is higher than the reference Z-factor  (3.34) .\n\nIn such a case, a positive Z-factor means a strong direct dependence and a negative one means a strong inverse dependence.\n\nThe following conclusions are extracted from analyzing Table  3 : (1) all the target emotions show a strong positive dependence with their corresponding reported, so the videos are especially eliciting the desired emotion in the users; (2) all  videos originally classified as non-fear have a significant negative dependence with such emotion, which implies fear is clearly recognizable; (3) it is statistically confirmed that the clips classified as disgust, tenderness and anger show positive dependence with other emotions in addition to their own. Focusing on this last conclusion, it is remarkable the dependence between disgust and amusement since they are far from each other in the PAD space, meaning they should be easy to differentiate. For the videos classified as anger, positive correlations are observed with the emotions of disgust and contempt, which fits with the behavior already obtained in previous studies  [20, 21]  when it comes to clips that show gender-based violence scenes.\n\nThe continuous PAD annotations are also examined by means of the Intra-class Correlation Coefficient (ICC) based on a single-rating, absolute-agreement, two-way mixed effects model, as implemented in the icc() function from the library irr in the R software. This study is used to evaluate the inter-rater consistency of the continuous annotations (targeted or reported ones) when classifying stimuli with respect to the corresponding discrete annotations (targeted or reported ones). Based on the 95% confidence interval of the ICC estimate, ICC index values less than 0.5, between 0.5 and 0.75, between 0.75 and 0.9, and greater than 0.90 are considered poor, moderate, good, and excellent reliability, respectively  [45] .\n\nTable  4  shows the reliability ICC metrics analyzing the targeted continuous annotations regarding the targeted discrete ones (see the Targeted field). Analyzing this table it is found poor consistency for 4 out of 8 of the emotions, which are amusement, anger, disgust, and sadness. It highlights the case of disgust with an even almost zero reliability. However, a moderated reliability (close to good) is obtained for fear and tenderness, getting good reliability for calm. This table also shows the reliability ICC metrics analyzing the reported continuous annotations regarding the reported discrete ones (see the Reported field). Analyzing this table, it is found that the reliability obtained is slightly better than in the targeted case with poor consistency for 5 out of 12 of the emotions (amusement, attraction, contempt, disgust, and sadness), a moderated reliability for anger, hope, joy, tedium, and tenderness, and good reliability for calm and fear. From this study, we conclude that  (1)",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: During this experiment, at least one researcher and one psychologist remained in",
      "page": 5
    },
    {
      "caption": "Figure 1: Experimental methodology followed during the development of the WEMAC dataset. Prior to and during the",
      "page": 5
    },
    {
      "caption": "Figure 2: , where it is observed as the ratio of times a targeted",
      "page": 7
    },
    {
      "caption": "Figure 2: Self-reported emotion labeling distribution (0.00-1.00) comparing the target discrete emotions to the reported ones.",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table 1: This paper presents",
      "data": [
        {
          "Database": "UC3M4Safety\nDatabase [13]",
          "Datasets": "Audiovisual Stimuli: Videos [14]\nAudiovisual Stimuli: Emotional Ratings [15]",
          "Conditions": "Online crowd-sourcing",
          "Participants": "General public and\nexpert judges"
        },
        {
          "Database": "",
          "Datasets": "WEMAC: Biopsychosocial Questionnaire [16]\nWEMAC: Physiological Signals [17]\nWEMAC: Audio Features [18]\nWEMAC: Self-reported Emotional Annotations [19]",
          "Conditions": "Laboratory",
          "Participants": "Women volunteers"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.86\n0.05\n0.03\n0.00\n0.04\n0.00\n0.00\n0.00\n0.01\n0.01\n0.00\n0.00\n0.00\n0.83\n0.01\n0.06\n0.02\n0.00\n0.00\n0.00\n0.00\n0.01\n0.03\n0.04\n0.02\n0.02\n0.76\n0.00\n0.00\n0.03\n0.03\n0.04\n0.04\n0.03\n0.02\n0.01\n0.01\n0.07\n0.00\n0.76\n0.03\n0.07\n0.00\n0.00\n0.01\n0.01\n0.00\n0.03\n0.00\n0.10\n0.00\n0.20\n0.60\n0.00\n0.00\n0.00\n0.01\n0.01\n0.01\n0.07\n0.00\n0.00\n0.00\n0.22": ""
        },
        {
          "0.86\n0.05\n0.03\n0.00\n0.04\n0.00\n0.00\n0.00\n0.01\n0.01\n0.00\n0.00\n0.00\n0.83\n0.01\n0.06\n0.02\n0.00\n0.00\n0.00\n0.00\n0.01\n0.03\n0.04\n0.02\n0.02\n0.76\n0.00\n0.00\n0.03\n0.03\n0.04\n0.04\n0.03\n0.02\n0.01\n0.01\n0.07\n0.00\n0.76\n0.03\n0.07\n0.00\n0.00\n0.01\n0.01\n0.00\n0.03\n0.00\n0.10\n0.00\n0.20\n0.60\n0.00\n0.00\n0.00\n0.01\n0.01\n0.01\n0.07\n0.00\n0.00\n0.00\n0.22": ""
        },
        {
          "0.86\n0.05\n0.03\n0.00\n0.04\n0.00\n0.00\n0.00\n0.01\n0.01\n0.00\n0.00\n0.00\n0.83\n0.01\n0.06\n0.02\n0.00\n0.00\n0.00\n0.00\n0.01\n0.03\n0.04\n0.02\n0.02\n0.76\n0.00\n0.00\n0.03\n0.03\n0.04\n0.04\n0.03\n0.02\n0.01\n0.01\n0.07\n0.00\n0.76\n0.03\n0.07\n0.00\n0.00\n0.01\n0.01\n0.00\n0.03\n0.00\n0.10\n0.00\n0.20\n0.60\n0.00\n0.00\n0.00\n0.01\n0.01\n0.01\n0.07\n0.00\n0.00\n0.00\n0.22": "0.02\n0.01\n0.00\n0.05\n0.02\n0.04\n0.11\n0.03"
        },
        {
          "0.86\n0.05\n0.03\n0.00\n0.04\n0.00\n0.00\n0.00\n0.01\n0.01\n0.00\n0.00\n0.00\n0.83\n0.01\n0.06\n0.02\n0.00\n0.00\n0.00\n0.00\n0.01\n0.03\n0.04\n0.02\n0.02\n0.76\n0.00\n0.00\n0.03\n0.03\n0.04\n0.04\n0.03\n0.02\n0.01\n0.01\n0.07\n0.00\n0.76\n0.03\n0.07\n0.00\n0.00\n0.01\n0.01\n0.00\n0.03\n0.00\n0.10\n0.00\n0.20\n0.60\n0.00\n0.00\n0.00\n0.01\n0.01\n0.01\n0.07\n0.00\n0.00\n0.00\n0.22": ""
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "United Nations. Declaration on the elimination of violence against women",
      "year": "1993",
      "venue": "United Nations. Declaration on the elimination of violence against women"
    },
    {
      "citation_id": "2",
      "title": "Global, regional, and national prevalence estimates of physical or sexual, or both, intimate partner violence against women in 2018",
      "authors": [
        "L Sardinha"
      ],
      "year": "2022",
      "venue": "Global, regional, and national prevalence estimates of physical or sexual, or both, intimate partner violence against women in 2018",
      "doi": "10.1016/S0140-6736(21)02664-7"
    },
    {
      "citation_id": "3",
      "title": "Killings of women and girls by their intimate partner or other family members -global estimates 2020 -world",
      "authors": [
        "Trend Analysis Research",
        "U Branch",
        "C Unodc)"
      ],
      "year": "2021",
      "venue": "Killings of women and girls by their intimate partner or other family members -global estimates 2020 -world"
    },
    {
      "citation_id": "4",
      "title": "Gender, Technology and Violence. Routledge Studies in Crime and Society",
      "year": "2017",
      "venue": "Gender, Technology and Violence. Routledge Studies in Crime and Society"
    },
    {
      "citation_id": "5",
      "title": "Bindi: Affective internet of things to combat gender-based violence",
      "authors": [
        "J Miranda"
      ],
      "year": "2022",
      "venue": "IEEE Internet Things J"
    },
    {
      "citation_id": "6",
      "title": "Deap: A database for emotion analysis ;using physiological signals",
      "authors": [
        "S Koelstra"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affect. Comput"
    },
    {
      "citation_id": "7",
      "title": "A multimodal database for affect recognition and implicit tagging",
      "authors": [
        "M Soleymani",
        "J Lichtenauer",
        "T Pun",
        "M Pantic"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affect. Comput"
    },
    {
      "citation_id": "8",
      "title": "Introducing wesad, a multimodal dataset for wearable stress and affect detection",
      "authors": [
        "P Schmidt",
        "A Reiss",
        "R Duerichen",
        "C Marberger",
        "K Van Laerhoven"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM International Conference on Multimodal Interaction, ICMI '18",
      "doi": "10.1145/3242969.3242985"
    },
    {
      "citation_id": "9",
      "title": "Amigos: A dataset for affect, personality and mood research on individuals and groups",
      "authors": [
        "J Correa",
        "M Abadi",
        "N Sebe",
        "I Patras"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affect. Comput"
    },
    {
      "citation_id": "10",
      "title": "An evaluation of speech-based recognition of emotional and physiological markers of stress",
      "authors": [
        "A Baird"
      ],
      "year": "2021",
      "venue": "Front. Comput. Sci",
      "doi": "10.3389/fcomp.2021.750284"
    },
    {
      "citation_id": "11",
      "title": "Predicting biological signals from speech: Introducing a novel multimodal dataset and results",
      "authors": [
        "A Baird",
        "S Amiriparian",
        "M Berschneider",
        "M Schmitt",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "IEEE 21st International Workshop on Multimedia Signal Processing (MMSP)",
      "doi": "10.1109/MMSP.2019.8901758"
    },
    {
      "citation_id": "12",
      "title": "Brain function, emotional experience and personality",
      "authors": [
        "D Robinson"
      ],
      "year": "2008",
      "venue": "Neth. J. Psychol",
      "doi": "10.1007/BF03076418"
    },
    {
      "citation_id": "13",
      "title": "UC3M4Safety Database description",
      "authors": [
        "M Blanco Ruiz"
      ],
      "year": "2021",
      "venue": "UC3M4Safety Database description"
    },
    {
      "citation_id": "14",
      "title": "UC3M4Safety Database -List of Audiovisual Stimuli",
      "authors": [
        "M Blanco Ruiz"
      ],
      "year": "2021",
      "venue": "UC3M4Safety Database -List of Audiovisual Stimuli",
      "doi": "10.21950/LUO1IZ"
    },
    {
      "citation_id": "15",
      "title": "UC3M4Safety Database -List of Audiovisual Stimuli",
      "authors": [
        "M Blanco Ruiz"
      ],
      "year": "2021",
      "venue": "UC3M4Safety Database -List of Audiovisual Stimuli",
      "doi": "10.21950/CXAAHR"
    },
    {
      "citation_id": "16",
      "title": "UC3M4Safety Database -WEMAC: Biopsychosocial questionnaire and informed consent",
      "authors": [
        "J Miranda Calero"
      ],
      "year": "2022",
      "venue": "UC3M4Safety Database -WEMAC: Biopsychosocial questionnaire and informed consent",
      "doi": "10.21950/U5DXJR"
    },
    {
      "citation_id": "17",
      "title": "UC3M4Safety Database -WEMAC: Physiological signals",
      "authors": [
        "J Miranda Calero"
      ],
      "year": "2022",
      "venue": "UC3M4Safety Database -WEMAC: Physiological signals",
      "doi": "10.21950/FNUHKE"
    },
    {
      "citation_id": "18",
      "title": "UC3M4Safety Database -WEMAC: Audio features",
      "authors": [
        "E Rituerto González"
      ],
      "year": "2022",
      "venue": "UC3M4Safety Database -WEMAC: Audio features",
      "doi": "10.21950/XKHCCW"
    },
    {
      "citation_id": "19",
      "title": "UC3M4Safety Database -WEMAC: Emotional labelling",
      "authors": [
        "J Miranda Calero"
      ],
      "year": "2022",
      "venue": "UC3M4Safety Database -WEMAC: Emotional labelling",
      "doi": "10.21950/RYUCLV"
    },
    {
      "citation_id": "20",
      "title": "Gender biases in the training methods of affective computing: Redesign and validation of the self-assessment manikin in measuring emotions via audiovisual clips",
      "authors": [
        "C Sainz-De Baranda Andujar",
        "L Gutiérrez-Martín",
        "J Miranda-Calero",
        "M Blanco-Ruiz",
        "C López-Ongil"
      ],
      "year": "2022",
      "venue": "Front. Psychol"
    },
    {
      "citation_id": "21",
      "title": "Emotion elicitation under audiovisual stimuli reception: Should artificial intelligence consider the gender perspective?",
      "authors": [
        "M Blanco-Ruiz",
        "C Sainz-De Baranda",
        "L Gutiérrez-Martín",
        "E Romero-Perales",
        "C López-Ongil"
      ],
      "year": "2020",
      "venue": "Int. J. Environ. Res. Public Heal",
      "doi": "10.3390/ijerph17228534"
    },
    {
      "citation_id": "22",
      "title": "The world of emotions is not two-dimensional",
      "authors": [
        "J Fontaine",
        "K Scherer",
        "E Roesch",
        "P Ellsworth"
      ],
      "year": "2007",
      "venue": "Psychol. Sci",
      "doi": "10.1111/j.1467-9280.2007.02024.x"
    },
    {
      "citation_id": "23",
      "title": "Csl-share: A multimodal wearable sensor-based human activity dataset",
      "authors": [
        "H Liu",
        "Y Hartmann",
        "T Schultz"
      ],
      "year": "2021",
      "venue": "Csl-share: A multimodal wearable sensor-based human activity dataset"
    },
    {
      "citation_id": "24",
      "title": "A practical wearable sensor-based human activity recognition research pipeline",
      "authors": [
        "H Liu",
        "Y Hartmann",
        "T Schultz"
      ],
      "year": "2022",
      "venue": "HEALTHINF"
    },
    {
      "citation_id": "25",
      "title": "Heartbeat selection based on outlier removal",
      "authors": [
        "M Carvalho",
        "S Brás"
      ],
      "year": "2022",
      "venue": "Iberian Conference on Pattern Recognition and Image Analysis"
    },
    {
      "citation_id": "26",
      "title": "Analysis of various machine learning algorithm for cardiac pulse prediction",
      "authors": [
        "M Harjani",
        "M Grover",
        "N Sharma",
        "I Kaushik"
      ],
      "year": "2019",
      "venue": "2019 International Conference on Computing, Communication, and Intelligent Systems (ICCCIS)"
    },
    {
      "citation_id": "27",
      "title": "Entorno de entrenamiento para detección de emociones en víctimas de Violencia de Género mediante realidad virtual",
      "authors": [
        "L Gutiérrez Martín"
      ],
      "year": "2019",
      "venue": "Entorno de entrenamiento para detección de emociones en víctimas de Violencia de Género mediante realidad virtual"
    },
    {
      "citation_id": "28",
      "title": "Bindi: Affective internet of things to combat gender-based violence",
      "authors": [
        "J Miranda Calero"
      ],
      "year": "2022",
      "venue": "IEEE Internet Things J",
      "doi": "10.1109/JIOT.2022.3177256"
    },
    {
      "citation_id": "29",
      "title": "Embedded emotion recognition: Autonomous multimodal affective internet of things",
      "authors": [
        "J Miranda",
        "M Canabal",
        "M Portela García",
        "C Lopez-Ongil"
      ],
      "year": "2018",
      "venue": "Proceedings of the cyber-physical systems workshop"
    },
    {
      "citation_id": "30",
      "title": "A design space exploration for heart rate variability in a wearable smart device",
      "authors": [
        "J Miranda",
        "M Canabal",
        "L Gutiérrez-Martín",
        "J Lanza-Gutiérrez",
        "C López-Ongil"
      ],
      "year": "2020",
      "venue": "2020 XXXV Conference on Design of Circuits and Integrated Systems (DCIS)",
      "doi": "10.1109/DCIS51330.2020.9268628"
    },
    {
      "citation_id": "31",
      "title": "Electrodermal activity smart sensor integration in a wearable affective computing system",
      "authors": [
        "M Canabal",
        "J Miranda",
        "J Lanza-Gutiérrez",
        "A Pérez Garcilópez",
        "C López-Ongil"
      ],
      "year": "2020",
      "venue": "2020 XXXV Conference on Design of Circuits and Integrated Systems (DCIS)",
      "doi": "10.1109/DCIS51330.2020.9268662"
    },
    {
      "citation_id": "32",
      "title": "Design and validation of an efficient and adjustable gsr sensor for emotion monitoring",
      "authors": [
        "M Canabal"
      ],
      "year": "2022",
      "venue": "Design and validation of an efficient and adjustable gsr sensor for emotion monitoring"
    },
    {
      "citation_id": "33",
      "title": "Emotion elicitation using films in: Coan ja, allen jjb, editors. the handbook of emotion elicitation and assessment",
      "authors": [
        "J Rottenberg",
        "R Ray",
        "J Gross"
      ],
      "year": "2007",
      "venue": "Emotion elicitation using films in: Coan ja, allen jjb, editors. the handbook of emotion elicitation and assessment"
    },
    {
      "citation_id": "34",
      "title": "librosa, audio and music signal analysis in python",
      "authors": [
        "B Mcfee"
      ],
      "year": "2015",
      "venue": "Proceedings of the 14th python in science conference",
      "doi": "10.25080/Majora-7b98e3ed-003"
    },
    {
      "citation_id": "35",
      "title": "The muse 2021 multimodal sentiment analysis challenge: Sentiment, emotion, physiological-emotion, and stress",
      "authors": [
        "L Stappen"
      ],
      "year": "2021",
      "venue": "The muse 2021 multimodal sentiment analysis challenge: Sentiment, emotion, physiological-emotion, and stress"
    },
    {
      "citation_id": "36",
      "title": "",
      "authors": [
        "B Mcfee"
      ],
      "year": "2022",
      "venue": "",
      "doi": "10.5281/zenodo.6097378"
    },
    {
      "citation_id": "37",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "F Eyben"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affect. Comput",
      "doi": "10.1109/TAFFC.2015.2457417"
    },
    {
      "citation_id": "38",
      "title": "opensmile -the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia",
      "doi": "10.1145/1873951.1874246"
    },
    {
      "citation_id": "39",
      "title": "The interspeech 2016 computational paralinguistics challenge: Deception, sincerity & native language",
      "authors": [
        "B Schuller"
      ],
      "year": "2001",
      "venue": "The interspeech 2016 computational paralinguistics challenge: Deception, sincerity & native language",
      "doi": "10.21437/Interspeech.2016-129"
    },
    {
      "citation_id": "40",
      "title": "Real-time Speech and Music Classification by Large Audio Feature Space Extraction",
      "authors": [
        "F Eyben"
      ],
      "year": "2015",
      "venue": "Real-time Speech and Music Classification by Large Audio Feature Space Extraction"
    },
    {
      "citation_id": "41",
      "title": "Snore sound classification using image-based deep spectrum features",
      "authors": [
        "S Amiriparian"
      ],
      "year": "2017",
      "venue": "Snore sound classification using image-based deep spectrum features"
    },
    {
      "citation_id": "42",
      "title": "ImageNet Large Scale Visual Recognition Challenge",
      "authors": [
        "O Russakovsky"
      ],
      "year": "2015",
      "venue": "Int. J. Comput. Vis. (IJCV)",
      "doi": "10.1007/s11263-015-0816-y"
    },
    {
      "citation_id": "43",
      "title": "Audio set: An ontology and human-labeled dataset for audio events",
      "authors": [
        "J Gemmeke"
      ],
      "year": "2017",
      "venue": "Proc. IEEE ICASSP 2017"
    },
    {
      "citation_id": "44",
      "title": "Multi-task self-supervised learning for robust speech recognition",
      "authors": [
        "M Ravanelli"
      ],
      "year": "2020",
      "venue": "Multi-task self-supervised learning for robust speech recognition",
      "doi": "10.48550/ARXIV.2001.09239"
    },
    {
      "citation_id": "45",
      "title": "A guideline of selecting and reporting intraclass correlation coefficients for reliability research",
      "authors": [
        "T Koo",
        "M Li"
      ],
      "year": "2016",
      "venue": "J. Chiropr. Medicine",
      "doi": "10.1016/j.jcm.2016.02.012"
    }
  ]
}