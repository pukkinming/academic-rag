{
  "paper_id": "2412.13541v4",
  "title": "Spatio-Temporal Fuzzy-Oriented Multi-Modal Meta-Learning For Fine-Grained Emotion Recognition",
  "published": "2024-12-18T06:40:53Z",
  "authors": [
    "Jingyao Wang",
    "Wenwen Qiang",
    "Changwen Zheng",
    "Fuchun Sun"
  ],
  "keywords": [
    "Fine-grained Emotion Recognition",
    "Multi-modal Learning",
    "Meta-learning",
    "Generalized Fuzzy Rules"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Fine-grained emotion recognition (FER) plays a vital role in various fields, such as disease diagnosis, personalized recommendations, and multimedia mining. However, existing FER methods face three key challenges in real-world applications: (i) they rely on large amounts of continuously annotated data to ensure accuracy since emotions are complex and ambiguous in reality, which is costly and time-consuming; (ii) they cannot capture the temporal heterogeneity caused by changing emotion patterns, because they usually assume that the temporal correlation within sampling periods is the same; (iii) they do not consider the spatial heterogeneity of different FER scenarios, that is, the distribution of emotion information in different data may have bias or interference. To address these challenges, we propose a Spatio-Temporal Fuzzy-oriented Multi-modal Meta-learning framework (ST-F2M). Specifically, ST-F2M first divides the multimodal videos into multiple views, and each view corresponds to one modality of one emotion. Multiple randomly selected views for the same emotion form a meta-training task. Next, ST-F2M uses an integrated module with spatial and temporal convolutions to encode the data of each task, reflecting the spatial and temporal heterogeneity. Then it adds fuzzy semantic information to each task based on generalized fuzzy rules, which helps handle the complexity and ambiguity of emotions. Finally, ST-F2M learns emotion-related general meta-knowledge through meta-recurrent neural networks to achieve fast and robust fine-grained emotion recognition. Extensive experiments show that ST-F2M outperforms various state-of-the-art methods in terms of accuracy and model efficiency. In addition, we construct ablation studies and further analysis to explore why ST-F2M performs well.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Fine-Grained Emotion Recognition",
      "text": "Fine-grained emotion recognition is a key task with many practical applications. It has evolved from physics-based models  [26] ,  [27]  to data-driven models  [28] ,  [29] , and from coarse-grained  [21] ,  [30]  to fine-grained analysis  [22] . Different types of data can be used to detect fine-grained emotions, such as body movements  [31] , blood pressure  [32] , textual information  [33]  and facial expressions  [34] . Among them, facial expressions and textual information are the most convenient and important sources for emotion recognition  [28] , as they are easily captured and abundant in social media.\n\nTraditional methods mainly use handcrafted features which can be divided into two categories, i.e., appearance-based  [35] -  [37]  and geometric-based  [38] -  [40] . They contain emotional information, but can only reflect shallow features and are not suitable for structured images. In contrast, deep learningbased methods  [41] ,  [42]  can extract more advanced features, and have been a breakthrough in recent decades. Among the most widely used for static emotion recognition are CNNs  [22] ,  [41] ,  [42] . These approaches can perform optimal graph reasoning, while providing long-range context information. Considering the dynamic change of emotions, some researchers have attempted to exploit recurrent neural networks (RNNs) in emotion recognition  [15] ,  [43] ,  [44] . However, all these methods still face the three challenges as mentioned in Section I. Besides, existing methods may have large errors in the process of information transfer when the scenarios are challenging and complex, as shown in Figure  1(c ). In contrast, our ST-F2M utilizes multi-modal meta-learning architecture-driven optimization to eliminate these challenges, achieving robust fine-grained emotion recognition.",
      "page_start": 2,
      "page_end": 11
    },
    {
      "section_name": "B. Generalized Fuzzy Emotion",
      "text": "Human emotions are divided into six basic categories, including happiness, anger, sadness, disgust, fear, and surprise  [45] . This is used as the foundation for emotion recognition  [46] -  [48] . However, human emotions often do not exist in isolation, but influence and are intertwined with each other. Existing studies ignore the cross-combination of emotions that are widely present in real life.\n\nThe application of generalized fuzzy mathematics provides solutions to the above problem. It can better handle the complexity and diversity of emotions by introducing fuzzy sets and fuzzy rules. Priya et al.  [49]  present a novel geometrical fuzzybased approach to accurately recognize emotions using fuzzy membership functions. Sreenivas et al.  [50]  propose a recurrent fuzzy neural network (RFNN) for emotion classification. Liliana et al.  [51]  display the degree of fuzzy cluster on eight face emotions based on semi-supervised Fuzzy C-means (FCM). Although these works proposed a definition of generalized fuzzy emotions, they only clustered emotions of different intensities without considering the diversity of emotions. In this paper, we aim to utilize fuzzy math and conduct generalized fuzzy rules to help quickly identify mixed emotions.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Meta-Learning For Emotion Recognition",
      "text": "Humans can learn from a few samples and easily develop an understanding of new tasks, thus interacting with the external world. For example, humans can quickly identify different types of apples in real life after only seeing a picture of an apple. Meta-learning aims at equipping machines with this ability to adapt to new scenarios effectively  [52] . Meta-learning methods can be categorized into three types: optimization-based methods  [53] -  [55] , model-based methods  [56] ,  [57] , and metric-based methods  [58] -  [60] . They all rely on shared structures to extract meta-knowledge, enabling excellent performance on new tasks, and are widely applied in few-shot learning.\n\nRecent studies have attempted to employ meta-learning for emotion recognition  [28] . Tang et al.  [61]  proposed a meta-transfer learning model aimed at extracting emotional information from electroencephalogram (EEG) data in different environments. Kuruvayil et al.  [62]  leveraged the rapid adaptation capability of prototype networks for emotion recognition while mitigating the severe overfitting issue caused by limited samples. Nguyen et al.  [63]  introduced an optimization-based PathNet to facilitate the transfer of emotional knowledge from visual to audio domains. Although these works have all employed meta-learning methods to address the challenge of limited training data, they have overlooked the unique challenges posed by real-world data  [9] ,  [24] , such as sample defects, complex environments, and subtle differences in finegrained emotions. In this study, we are the first to consider and explore the spatio-temporal heterogeneity of fine-grained emotions in the real world, while utilizing meta-learning to unearth the intrinsic factors influencing recognition.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Problem Definition",
      "text": "In this section, we first introduce the problem settings and notations of meta-learning for FER.\n\nWith a given task distribution p(T ), both the meta-training set D tr and the meta-testing set D te are randomly drawn from p(T ) without any class-level overlap. Within a single training batch, we denote the N tr training tasks as\n\n) represents an individual sample and its corresponding label respectively, while N • i indicates the number of samples in each set. Meta-learning can be conceptualized as a two-level optimization problem. In the first level (inner loop), we learn the task-specific model f i θ for task τ i with the support set D s i utilizing the meta-learning model f θ . The objective is:\n\nwhere α is the learning rate. In the second level (outer loop), we update the meta-learning model f θ by minimizing the average loss computed across multiple tasks, incorporating the query sets D q and leveraging the knowledge gained from the previously learned task-specific models. The objective is:\n\nwhere β is the learning rate. Note that f i θ is derived by computing the derivative of f θ , making ∇ f θ 1 Ntr Ntr i=1 L(X q i , Y q i , f i θ ) be interpreted as the second-order derivative of f θ .\n\nIn this study, we will mine meta-knowledge in fine-grained emotion recognition based on this paradigm to improve generalization and transferability.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Generalized Fuzzy-Based Rules",
      "text": "To quantify emotional intensity, we propose a set of generalized fuzzy-based rules to describe emotional states. By incorporating fuzzy inference into machine learning, we are better equipped to handle the inherent uncertainty in emotions, while also achieving higher computational efficiency in fewshot tasks  [64] . We develop two systems based on these proposed rules: fuzzy component inference system (FCIS), and fuzzy knowledge inference system (FKIS).\n\nFCIS aims to convert representations into a set of fuzzy linguistic conditions. Each condition describes a sequence of changes in a physiological attribute, where the elements of the sequence represent the state of that attribute. For instance, facial components have multiple attributes like eyebrows and mouth corners. These components change differently when humans express different emotions. Taking the eyebrow as an example, the values corresponding to low, normal, and raised are respectively -1, 0, and 1. When describing sad, the value of the eyebrows is -1, i.e., low, and when happy, the value of the eyebrows is 1, i.e., raised. A linguistic condition describing a sequence of changes in the mouth could be represented as (-1)(-1)(-1)011000. Table  I  presents various attributes of facial components, while Figure  2  provides the membership functions for all the physiological attributes. The conditions generated by FCIS are then passed to FKIS.\n\nFKIS aims to obtain sequences describing emotional intensity. The fuzzy inference rules within FKIS are constructed based on psychological knowledge  [65] ,  [66] , and stored in a knowledge-based engine. The engine takes the changes in a set of physiological attributes as input and computes fuzzy sets based on the membership functions for each attribute. Subsequently, the engine generates 18 classification values for (composite) emotional mappings by combining trigger rules and performing de-fuzzification based on the fuzzy sets. Each basic emotion has three intensity levels, i.e., low, medium, and high. Table  II  provides a set of FKIS rules for determining emotional intensity, with each element representing the current state of a physiological attribute. Figure  3  illustrates the membership functions for the intensity value of six base emotions. Take Figure  3  (a) as an example, when \"Eccentricity=0.2\", the membership degree of \"Angry-Low\" is 0, and the membership degree of \"Angry-Medium\" is 0.34. The coding of the emotions refers to representing emotions as numerical or symbolic values, while membership functions are used to describe the degree of belonging of each data point to each fuzzy set, i.e., the extent of each data point in each emotion category, or its degree in each facial component. We weight the emotional intensity encoding (shown in Table  II ) based on the membership value and introduce it into the model calculation as fuzzy information. The fuzzy membership functions calculate the coding of",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotions Coding Of F C I",
      "text": "Angry-High 0010010(-1)(-1)(-1)(-1)(-1) Angry-Medium (-1)0(-1)000101011 Angry-Low 0001101(-1)0000 Happy-High 110110100111 Happy-Medium 011011000(-1)00 Happy-Low (-1)010100010(-1)(-1) Disgust-High 11(-1)000001(-1)00 Disgust-Medium 110000000(-1)00 Fear-Low 000(-1)(-1)0000011 Sad-High (-1)(-1)0(-1)(-1)100(-1)(-1)00 Sad-Medium (-1)(-1)0001000(-1)00 Surprise-Low 000110000011 the emotions for each view and introduce one-dimensional information with fuzzy semantics, which better captures the complexity of emotions. Note that different emotions have varying numbers of rules; for instance, anger has 42 rules, while disgust only has 34 rules. In this study, the above systems, i.e., FCIS and FKIS, are used in two goals: (i) for the decision-making process of ST-F2M: followed by the spatio-temporal integrated module to assign fuzzy semantic information containing emotional intensity to the representation; (ii) for dataset construction: assign emotion and intensity information to benchmark datasets in areas such as facial expression recognition to build benchmark datasets for fine-grained emotion recognition.\n\nFor the first objective, the introduction of fuzzy rules helps deal with the complexity and ambiguity of emotions in reality and improve model generalization. Specifically, the proposed emotional reasoning system is grounded in psychological theories of human affect, wherein complex emotional states are decomposed into a set of elementary characteristic components. These components serve as the minimal semantic units of emotion, analogous to \"building blocks\", and enable a compositional representation of emotional expressions. This formulation allows emotional data, regardless of context or modality, to be interpreted as permutations of these basic units, thus supporting generalizability and reusability across different scenarios. Further, the fuzzy membership mechanism allows each component to be activated with varying intensities depending on context and subject, addressing the intrinsic individual variability and ambiguity in emotional expression. This enables the system to model the uncertainty and continuity inherent in human emotions more robustly than discrete categorical methods. The specific implementation is illustrated in Section V.\n\nFor the second objective, our system significantly reduces data maintenance overhead compared with traditional annotation-based approaches, which demand costly manual labeling for every new video or sample. First, both building a fuzzy inference system and data annotation for the first time require human knowledge as a prior. However, the systems we built are based on fuzzy emotional reasoning rules constructed from human characteristic components, which are universal and applicable to all emotions once and for all. For new data, the features can still be extracted by the feature extractor and then input into the inference system for fuzzy inference without the need to reconstruct the rules. In contrast, data annotation is to identify and annotate videos from the pixel level. New video data needs to be re-annotated, which will incur new costs. In addition, our fuzzy system contains a built-in inference function. We only need to write down the encoding -1, 0, 1 of the characteristic components required for the emotion, and then we can perform inference based on the built-in functions in FCIS and FKIS without requiring a separate model training process. Thus, we can generate new fuzzy rules based on the inference function at any time without additional cost. Therefore, the introduction of fuzzy rules greatly improves the model effect and speeds up reasoning, while the cost of first construction is negligible for the model. We have constructed two FER datasets, called DISFA-FER and WFLW-FER, based on DISFA  [67]  and WFLW  [68] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "V. Methodology",
      "text": "To address the three key challenges in FER, we propose a spatio-temporal fuzzy-oriented multi-modal meta-learning framework (ST-F2M). During the design process of the framework, we aim to solve four key issues: (i) how to construct multi-modal meta-learning tasks, (ii) how to consider spatiotemporal heterogeneity, (iii) how to deal with the complexity and ambiguity of emotions, and (iv) how to learn general prior knowledge from limited data and achieve rapid convergence.\n\nIn this section, we first describe how to partition the limited training data into several meta-learning tasks and then provide details about the model training process. The overall framework of ST-F2M is illustrated in Figure  4 , and the pseudo-code of ST-F2M is shown in Algorithm 1.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Meta-Learning Task Construction",
      "text": "Before implementing meta-learning, we first build metalearning tasks for fine-grained emotion recognition. Our goal in this subsection is to solve the first issue mentioned above, i.e., how to construct multi-modal meta-learning tasks.\n\nSpecifically, we first separate different modalities from the input long video data and add location labels to each modal data. The location label refers to the relative position (appearance order) between different elements of each modality. It is obtained by the positional encoding of different modal data in the long video through an MLP, without requiring an additional manual annotation process. Next, we segment the data of different modalities according to the emotional label sequences, and each segment corresponds to an emotional view. Each view reflects a modal data of an emotion. Then, views from the same location are combined into a group based on the location labels. Finally, we randomly select multiple groups of views with the same emotion labels to form a metalearning task. Each modal data for each emotion corresponds to a perspective.\n\nFormally, given the FER videos (X , Y) where X a and Y a are the a-th long video and the corresponding label sequence respectively, we first segment the visual modality and text modality data represented as (X v a , Y a , pos a ) and (X t a , Y a , pos a ), where pos i is the positional encoding of different modal data in the long video. Next, we segment the data based on the emotional label sequence Y a and obtain\n\n, where N Ya is the number of emotions contained in the long video, that is, the number of elements in Y a . Next, we aggregate views with the same position to obtain x a,b = x v a,b , x t a,b . Finally, a meta-learning task T i is formed by randomly selecting x a,b with the same emotional label.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "B. Meta-Learning Training",
      "text": "Next, we will perform meta-learning based on the constructed tasks mentioned above. Our goal in this section is to address the last three issues mentioned above, and propose a spatiotemporal convolution integrated module, a generalized fuzzy inference module, and a meta-optimation module. Next, we will introduce the training process of ST-F2M, as well as the details of these three modules.\n\n1) Overview of meta-training: The training process of ST-F2M is mainly divided into four steps: (i) encoding through the spatio-temporal convolution integrated module, which helps consider spatio-temporal heterogeneity; (ii) adding fuzzy emotional semantic information using generalized fuzzy inference, which helps deal with the complexity and ambiguity of emotions; (iii) emotion category determination based on classification head, which obtains FER results; and (iv) bilevel optimization through the meta-optimation module, which improves model efficiency and robustness. Through this process, ST-F2M can effectively cope with the three challenges of real-life FER and achieve rapid convergence from small and complex multi-modal emotion data.\n\nFormally, ST-F2M can be defined as f θ = h • F • g, where g is the shared encoder, F denotes the fuzzy inference, h is the classifier. We first construct the training set D tr and the test set D te based on the method in Subsection V-A, which are obtained from the distribution p(T ). For each task T i , we first input it into the encoder g composed of a spatio-temporal convolution integrated module, thereby obtaining the representation Z i = g(T i ). Next, Z i is passed into the fuzzy inference system F to obtain the representation Ẑi = F (Z i ) that introduces fuzzy semantic information. Then, Ẑi will be input into the classification header to generate specific classification results. Ultimately, this process will undergo a bi-level optimization mentioned in Subsection III to learn common meta-knowledge for rapid adaptation to new tasks.\n\n2) Spatio-temporal encoding: first introduce the process of spatio-temporal encoding, i.e., Z i = g(T i ).\n\nTo capture the temporal correlation, we adopt a onedimensional convolution following  [69]  along the temporal dimension with a gating mechanism. Specifically, our temporal convolution (C te ) takes the task T i composed of FER video tensors as input, while outputting a time-aware embedding of each emotional state:\n\nwhere t out denotes the length of the output embedding following the convolutional operations in the encoder, Z t i ∈ R Ns×d denotes the embedding matrix at the time step t, N s and d represent the number of samples in the support set and embedding dimensions. The n-th row z t,n i ∈ R d corresponds to the embedding of view x s i,j . Further, in order to capture the spatial correlation, we construct a spatial convolution module (C sp ) based on the message-passing mechanism  [70] , and integrate the temporal information:\n\nwhere A i is the region adjacency matrix of task T i , Z i denotes the refined embedding of task T i which takes into account Algorithm 1: Pseudo-Code of ST-F2M Update the ST-F2M model f θ using Eq.7, i.e.,\n\nspatio-temporal heterogeneity. This spatio-temporal encoder effectively captures emotional changes in long videos, allowing us to model the connection between emotional states across time and various spatial regions, which solves the second issue.\n\n3) Generalized fuzzy inference: For generalized fuzzy inference, we perform secondary encoding based on the fuzzy systems described in Subsection IV, that is, Ẑi = F (Z i ).\n\nIn brief, after spatio-temporal encoding, the feature embedding Z i is input into FCIS, which directly calculates the values of f c i based on the designed membership function (as shown in Table  I  and Figure  2 ). Next, Z i is input into FKIS, which assigns a one-dimensional fuzzy semantics to Ẑi based on the designed membership function of FKIS (as shown in Table  II  and Figure    3 ) and the result of FCIS. The optimization of the generalized fuzzy inference module is performed when building the inference system before meta-training, and its fitness function  [71] ,  [72]  is expressed as:\n\nwhere λ 1 and λ 2 represent the range of membership functions in FCIS and FKIS, which affects the final meta-learning model performance. The calculation of this loss is obtained by evaluating the final effect of changing the fuzzy rules through a fixed meta-learning model. Compared with Z i , one-dimensional information containing fuzzy semantics is introduced into Ẑi , and its element value is the encoding information of f c i . Next, the embedding Ẑi containing fuzzy information of each view will be input to the classifier h for fine-grained emotion recognition to obtain the emotion change sequence of task T i . The generalized fuzzy inference module we built can give the data fuzzy semantic information to cope with the differences in emotional expressions between different individuals and the similarities between different emotions. It addresses the third issue regarding emotional complexity and ambiguity.\n\n4) Overall optimization: Finally, we utilize meta-recurrent neural networks to capture general emotion-related metaknowledge and achieve fast convergence for FER, i.e., updating the model\n\nSpecifically, we perform bi-level optimization as described in Subsection III. The objective of the first level optimization (inner loop) is updated to:\n\nwhere α is the learning rate, N s i is the number of samples in the support set D s i , ẑs i,j ∈ Ẑs i is the embedding of the view x s i,j calculated through g, F , and h. Then, the objective of the second level (outer loop) becomes:\n\nwhere β is the learning rate, N q i represents the number of samples in the query set D q i , N tr is the number of training tasks, ẑq i,j ∈ Ẑq i denotes the embedding of the view x q i,j in the query set D q i , which is calculated through g, F , and h. Through this bi-level optimization mechanism, we can learn emotion-related general meta-knowledge from training tasks and quickly adapt to new tasks based on this knowledge. This meta-optimization module solves the fourth issue in the reallife FER problem mentioned above, which is to learn key knowledge from limited data and achieve rapid convergence.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Vi. Experiments",
      "text": "In this section, we first introduce the experimental setup in Subsection VI-A. Then, we present the results of performance comparison, computational efficiency comparison, and robustness analysis in Subsections VI-D to VI-C, demonstrating the superiority of ST-F2M. Next, we conduct ablation studies and parameter investigation to further explore how ST-F2M works and what makes ST-F2M work well in Subsections VI-E to VI-F respectively. Finally, we evaluate the practical application effect of ST-F2M in Subsection VI-G.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Experimental Setup",
      "text": "In this subsection, we introduce implementation details, datasets, and baselines of our experiments in turn.\n\n1) Implementation Details: We opt for the most commonly used ResNet-18 as the backbone. After the convolution and filtering stages, we sequentially apply batch normalization, ReLU non-linear activation, and 2 × 2 max pooling (achieved through stride convolutions). The last layer ultimately produces two separate channels as its final output. The first channel is fed into the proposed fuzzy systems, i.e., FCIS and FKIS, to extract fuzzy semantic information, while the second channel is directed towards a softmax layer. These network architectures undergo an initial pre-training phase and remain unaltered throughout the training process. As we transition to the optimization stage, we utilize the Adam optimizer  [73]  for training our model. We set the values of momentum and weight decay to 0.9 and 10 -4 , respectively. The initial learning rate for all experiments is set at 0.2, with the flexibility for linear scaling as needed. All experiments are executed on 10 NVIDIA V100 GPUs.\n\n2) Datasets: We evaluate our ST-F2M on five benchmark datasets, including: (i) CK+  [74] , which contains 593 video sequences from 123 different subjects. The videos are recorded at 30 frames per second (FPS) with 640 × 490 or 640 × 480 pixel resolution; (ii) DISFA  [67]  contains 27 videos of 4844 frames, which is mainly used for facial expression-based FER; (iii) WFLW  [68] , which contains 10,000 samples and the sentiment intensity is added by manual annotation. The rich attribute annotations of this dataset, such as occlusion and blur, are significantly helpful in simulating defects in real-life; (iv) CMU-MOSEI  [75] , which contains 22,856 video clips from 1000 different speakers; and (v) CREMA-D  [76]  consists of 7,442 video clips that employ both facial and vocal data to categorize six fundamental emotional states. Following  [77] , we pre-extract 35-dimensional visual features from video frames through FACET  [78] , and pre-extract 300-dimensional text features from video text through Glove  [79]  to supplement the missing modal information. Note that for the facial expression datasets, DISFA and WFLW, we assign emotion and intensity information to the training data based on generalized fuzzy rules, and named the datasets DISFA-FER and WFLW-FER which will be open sourced.\n\n3) Baselines: We introduce four types of baselines: (i) stateof-the-art methods (before 2024) of each dataset, including FN2EN  [80] , VGG-F  [81] , PropNet-CNN  [82] , UniMSE  [18] , and SepTr  [83] ; (ii) meta-learning methods, including MAML  [53] , Reptile  [84] , ProtoNet  [85] , RelationNet  [86] , MetaCRL  [87] , and ANIL  [55] ; and (iii) prominent and recently proposed multi-modal FER methods, including PATHOSnet-v2  [88] , COGMEN  [89] , and MARLIN  [90] , BMHP  [91] , FPM-Net  [92] , TAE  [93] , and CBERL  [94] . Note that for single-modal methods under multi-modal data sets, we use the bagging  [95]      method to integrate after solving the single modality. For the meta-learning method, we adopt the same task construction method as ST-F2M for multi-modal emotion recognition.",
      "page_start": 7,
      "page_end": 9
    },
    {
      "section_name": "B. Performance Comparison",
      "text": "To evaluate the effectiveness of ST-F2M, we conduct comparative experiments with the baselines on the five datasets mentioned in Section VI-A, containing six basic emotions, i.e., happiness, anger, sadness, disgust, fear, and surprise, and each emotion consists of three intensity levels, i.e., low, medium and high. In this experiment, we set two metrics, including accuracy (ACC) and recall (RECALL). We record the average value of 10 rounds to obtain the results of these two metrics.\n\nThe results are shown in Table  III . From the results, we can observe that across all datasets, our approach consistently outperforms both meta-learning methods and multi-modal facial expression recognition methods. Moreover, our ST-F2M achieves results similar to state-of-the-art (SOTA) baselines which rely on large datasets, while our ST-F2M only has limited data support. This indicates that ST-F2M can achieve similar or even superior generalization improvements compared to baselines, without the need for extensive data reliance. It is worth noting that our method is always the SOTA method on the Recall, which illustrates its superiority in fields such as disease diagnosis that require high recall rates  [96] .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "C. Computational Efficiency Comparison",
      "text": "To analyze the computational efficiency of ST-F2M, we construct comparative experiments between ST-F2M and baselines to evaluate their trade-off between performance and efficiency. Specifically, we built an edge computing platform, where the host is NVIDIA Jetson AGX Xavier, including a 512core NVIDIA Volta GPU with 64 Tensor cores, two NVIDIA deep learning gas pedals, two vision gas pedals, and an eightcore NVIDIA Carmel Arm CPU. We record the running time (FPS) of each model under different accuracy effects.\n\nFigure  5  illustrates the results of the trade-off between performance and efficiency. The center of each circle in the figure represents the average result for the respective models, and the circle's area represents a 90% confidence interval. It's important to note that our method doesn't require additional preprocessing steps and achieves a real-time speed of 20.3 FPS (0.02 seconds per frame). On the same platform, ST-F2M achieves nearly the fastest detection while maintaining a high level of accuracy. This result highlights the superiority of ST-F2M in terms of computational efficiency.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "D. Robustness Analysis",
      "text": "1) When Facing Distortion and Noise: Emotion recognition in real life can encounter various interferences. Take medical environments as an example, the emotional data of the patients may have large portions obscured due to wearing masks or bandages. A robust method should still achieve reliable emotion recognition in the presence of various interferences. To assess the robustness of ST-F2M, we manually introduce multiple noises into the data to simulate real-world interferences, including adding fog  [97] , masks  [98] , and distortion  [99]  for visual modality and adding masks for textual modality. All types of noise are introduced into the data with the levels of 10%, 30%, and 50%, respectively. Specifically, in the case of fogging, these levels also correspond to increasing fog density; in the case of masking, these levels indicate the proportion of the sample area that is masked; in the case of distortion, these levels refer to the proportion of samples that affected.\n\nThe results are shown in Figure  6 . We record the results of ST-F2M on CMU-MOSEI. The accuracy fluctuates within 4%, demonstrating excellent stability. Table  IV  shows the comparison results of robustness analysis. The results show that ST-F2M obtains relatively stable results under all data sets and scenarios, while other methods suffer from data noises. Therefore, ST-F2M shows superior robustness on real-life FER.\n\nTo gain a more intuitive understanding of ST-F2M's performance, we randomly select 2000 cases for each emotion from video data containing 20% masks. Each case comprises an image and a text message. Figure  7  displays the confusion matrix for the six basic emotions (without intensity), while Figure  8  presents the confusion matrix for the six emotions with intensity. It is evident that the results on the diagonal show a significant advantage, indicating the robustness of ST-F2M.\n\n2) When Facing Mislabeling with Few Samples: To further assess the robustness of the proposed method, we conduct an experiment targeting scenarios with limited samples and mislabeled data. Using CK+ as the benchmark dataset, we follow the same experimental setup as in Subsection VI-B, except that each training batch contains only 20% of the original data to simulate a few-shot setting. Additionally, 30% of the sampled training data have their emotion labels randomly shuffled to mimic real-world weak supervision with label noise. We report both the average and worst-case test performance of different models in Table  V . The results show that our method consistently achieves superior performance under these challenging conditions, demonstrating its effectiveness. This also confirms that the fuzzy system accurately models fundamental emotional components and effectively mitigates the impact of mislabeled data during training.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "E. Ablation Study",
      "text": "We conduct an ablation study to explore the impact of different modules in ST-F2M, i.e., the spatio-temporal convolution module M st , the generalized fuzzy inference module M gf , and the meta-optimization module M mt . We evaluate their influence on the ST-F2M performance by replacing or redesigning these modules. For instance, M st is replaced with a Conv4-based encoder without considering spatio-temporal features, M gf is disabled, and M mt is substituted with singlelevel training from scratch. We conduct experiments in three different environments, including CMU-MOSEI (Group A), CMU-MOSEI with 30% interference (Group B), and CMU-MOSEI with randomly altered speeds (Group C). The latter two are used to assess the impact of ST-F2M on spatial and temporal heterogeneity.\n\nThe results are shown in Figure  9 . The results indicate that all three modules significantly enhance the model's performance. Furthermore, they bring noticeable performance improvements, especially in Groups B and C, with M st being particularly prominent. Therefore, our design demonstrates foresight, while highlighting our concern for the issue of spatial and temporal heterogeneity in fine-grained emotion recognition.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "F. Parameter Investigation",
      "text": "The impact of hyperparameters on performance cannot be overlooked. In the case of ST-F2M, the description of membership functions in FCIS and FKIS directly affects emotion recognition results. Therefore, we conduct experiments specifically focusing on the attribute membership range λ 1 in FCIS and the membership range λ 2 in FKIS to find the optimal settings. The example results are presented in a 3D histogram in Figure  10 . Example results are presented as a 3D histogram in figure  10 . Specifically, we take the high-intensity value sample of angry emotion as an example to calculate the membership range of the generalized fuzzy inference module when the accuracy and recall rates are the highest. The best effect is when λ 1 and λ 2 are both 0.4, which is also the setting of our proposed fuzzy systems. The results indicate that the membership functions we constructed for the system in Subsection IV deliver the best performance. This demonstrates the rigor of our work.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "G. Practical Case Study",
      "text": "Although current FER methods achieve good results on benchmark datasets, they have not been deployed and tested in real-life applications  [1] ,  [3] ,  [5] . There is a big difference between the real scenarios and the benchmark datasets, and it is interfered with by various aspects  [20] ,  [28] . Although the aforementioned experiments have proven the outstanding effect and robustness of ST-F2M in different scenarios, to evaluate the effect of the model in actual deployment, we further select more complex medical robot scenarios for experiments.\n\nWe use the same implementation details as Subsection VI-A but a different deployment platform, i.e., a self-built robot platform 1 . The robot host we use is NVIDIA Jetson AGX Xavier, including a 512-core NVIDIA Volta GPU with 64 Tensor cores, two NVIDIA deep learning accelerator pedals, two visual accelerator pedals, and an eight-core NVIDIA Carmel Arm CPU. Our ST-F2M is experimented on a unified model edge computing platform and ultimately embedded on a medical robotics platform. We set up the communication and underlying control algorithms to achieve a complete systemclosed loop. In this experiment, we use the built robot entity to conduct 5 groups of experiments in the laboratory simulation environment. The data of each group contains 20 pieces of data with different angles. Figure  11(a)  shows the data collection. Each piece of data is a 30-second video, labeled with the emotional sequence contained in the video. The robot's goal is to recognize the full range of emotions contained in video data. Note that the data in this scene may have various interferences, such as camera distortion and object occlusion  [100] ,  [101] . Figure  1(c ) provides examples of real-world FER samples.\n\nFrom Figure  11 , we can observe that: (i) ST-F2M still achieves superior results in real-life deployment; (ii) other FER methods have unsatisfactory results in the robot entity test environment, possibly due to various reasons like camera distortion, object occlusion, etc.; and (iii) ST-F2M can still obtain relatively stable FER results when facing many problems caused by the robot entity testing environment. These observations prove the advantages of ST-F2M in practical applications 1 More details on the robot build, please see: https://github.com/WangJingy ao07/PickBallBot-5mao/tree/main and illustrate the effectiveness and forward-lookingness of our design for the real-life FER field.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Vii. Conclusion",
      "text": "In this paper, we propose a novel Spatio-Temporal Fuzzyoriented Multi-modal Meta-learning (ST-F2M) framework for real-life fine-grained emotion recognition in videos. It addresses three key challenges in this field. Specifically, considering spatiotemporal heterogeneity, we integrate temporal and spatial convolutions to encode multi-modal emotional states in long videos. Then, we design i) a fuzzy emotion and fuzzy knowledge inference system based on generalized fuzzy rules; and ii) a fuzzy-oriented multi-modal meta-learning paradigm to achieve fast and robust fine-grained emotion recognition. Comprehensive experiments on five FER datasets demonstrate the robustness and generalization of ST-F2M. In practical applications, our method has been successfully deployed on edge computing platforms such as medical robots, achieving robust fine-grained emotion recognition. In addition, we construct two FER datasets, namely DISFA-FER and WFLW-FER, based on the constructed generalized fuzzy inference system. We hope that ST-F2M can provide valuable insights into other application areas of FER.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Illustration of our motivation, i.e., the three key challenges in fine-",
      "page": 1
    },
    {
      "caption": "Figure 1: (a) shows the results of six data groups",
      "page": 1
    },
    {
      "caption": "Figure 1: (b). Some prior work",
      "page": 2
    },
    {
      "caption": "Figure 1: (c), we can observe",
      "page": 2
    },
    {
      "caption": "Figure 1: (c). In contrast, our ST-",
      "page": 2
    },
    {
      "caption": "Figure 2: provides the membership",
      "page": 3
    },
    {
      "caption": "Figure 3: illustrates the membership",
      "page": 3
    },
    {
      "caption": "Figure 3: (a) as an example, when “Eccentricity=0.2”, the",
      "page": 3
    },
    {
      "caption": "Figure 2: The membership functions for all the physiological attributes. The horizontal axis represents the input, such as the feature encoding result of the neural",
      "page": 4
    },
    {
      "caption": "Figure 3: The membership functions example for the intensity value of six emotions. The horizontal axis represents the converted eccentricity value of the",
      "page": 4
    },
    {
      "caption": "Figure 4: The framework of ST-F2M. ST-F2M first constructs multi-modal meta-learning tasks based on the input videos. Next, it performs encoding through a",
      "page": 5
    },
    {
      "caption": "Figure 4: , and the pseudo-code of",
      "page": 5
    },
    {
      "caption": "Figure 2: ). Next, Zi is input into FKIS, which",
      "page": 6
    },
    {
      "caption": "Figure 3: ) and the result of FCIS. The optimization of",
      "page": 6
    },
    {
      "caption": "Figure 5: Trade-off performance. It provides the comparative results of models’",
      "page": 8
    },
    {
      "caption": "Figure 6: Robust analysis on CMU-MOSEI. (%) represents the proportion of",
      "page": 8
    },
    {
      "caption": "Figure 7: The confusion matrix of six emotions.",
      "page": 9
    },
    {
      "caption": "Figure 8: The confusion matrices of the intensities of six emotions.",
      "page": 9
    },
    {
      "caption": "Figure 9: Ablation study of ST-F2M.",
      "page": 9
    },
    {
      "caption": "Figure 10: Parameter investigations about {λ1, λ2}.",
      "page": 9
    },
    {
      "caption": "Figure 11: Comparison results of practical case study. Group A ∼Group E respectively represent the five groups of data collected in the robot platform. For",
      "page": 9
    },
    {
      "caption": "Figure 5: illustrates the results of the trade-off between",
      "page": 10
    },
    {
      "caption": "Figure 6: We record the results of",
      "page": 10
    },
    {
      "caption": "Figure 7: displays the confusion",
      "page": 10
    },
    {
      "caption": "Figure 8: presents the confusion matrix for the six emotions",
      "page": 10
    },
    {
      "caption": "Figure 9: The results indicate that all",
      "page": 10
    },
    {
      "caption": "Figure 10: Example results are presented as a 3D",
      "page": 11
    },
    {
      "caption": "Figure 10: Specifically, we take the high-intensity",
      "page": 11
    },
    {
      "caption": "Figure 11: (a) shows the data collection.",
      "page": 11
    },
    {
      "caption": "Figure 1: (c) provides examples of real-world FER samples.",
      "page": 11
    },
    {
      "caption": "Figure 11: , we can observe that: (i) ST-F2M still",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "C:80% 0%\nD:80% 10%\nE:90% 20%\nF:100% 30%": "(a",
          "Column_3": ")",
          "Column_4": "Da",
          "Column_5": "t",
          "Column_6": "ade",
          "Column_7": "",
          "Column_8": "pen",
          "Column_9": "d",
          "Column_10": "enc",
          "Column_11": "y",
          "Column_12": "",
          "Column_13": ""
        },
        {
          "Column_1": "r\nope",
          "C:80% 0%\nD:80% 10%\nE:90% 20%\nF:100% 30%": "eal-life\nn sour",
          "Column_3": "d\nc",
          "Column_4": "ata\ne data",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": ""
        },
        {
          "Column_1": "",
          "C:80% 0%\nD:80% 10%\nE:90% 20%\nF:100% 30%": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "real",
          "Column_13": "-life data open source data"
        },
        {
          "Column_1": "",
          "C:80% 0%\nD:80% 10%\nE:90% 20%\nF:100% 30%": ". Ill\ndem\nete\nns\nre\nfin\n[1\nracy\nma\nthre",
          "Column_3": "",
          "Column_4": "stra\notio\nem\nsi\nhe\n-gr\n].\nth\net\ne k",
          "Column_5": "",
          "Column_6": "ion\nrec\ntio\ngle\natu\nain\nThe\nn\nim\ny c",
          "Column_7": "",
          "Column_8": "ofo\ngni\nr\ndis\ne\nd s\nref\nE\npro\nhal",
          "Column_9": "",
          "Column_10": "(c\nrm\nion.\nco\nret\nf h\ngm\nre,\n[1\ne\neng",
          "Column_11": "",
          "Column_12": "Sp\notiv\nnit\ne\nma\nnt\nFE\n6].\nER\ns,",
          "Column_13": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "Beginning",
          "Column_3": "",
          "Column_4": "Later",
          "Column_5": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "CK+\nACC RECALL": "96.13±0.61 94.56±0.78\n94.67±0.23 95.01±0.91\n95.07±0.89 94.54±0.07\n89.28±0.74 90.45±0.13\n90.30±0.47 88.31±0.11",
          "DISFA-FER\nACC RECALL": "81.12±0.75 84.35±0.84\n80.64±0.58 86.19±0.17\n77.51±0.93 83.10±0.42\n79.57±0.14 84.17±0.55\n82.94±0.27 82.97±0.21",
          "WFLW-FER\nACC RECALL": "78.91±0.24 80.85±0.37\n79.68±0.28 80.39±0.47\n81.80±0.73 82.69±0.57\n80.20±0.83 79.73±0.12\n78.06±0.77 77.54±0.05",
          "CMU-MOSEI\nACC RECALL": "84.77±0.20 85.09±0.51\n75.56±0.73 75.94±0.25\n72.21±0.34 70.90±0.46\n86.97±0.56 87.49±0.36\n80.41±0.50 81.67±0.08"
        },
        {
          "CK+\nACC RECALL": "85.70±0.64 87.51±0.08\n85.15±0.59 87.99±0.30\n86.40±0.67 86.65±0.43\n87.34±0.54 88.76±0.65\n92.23±0.47 93.42±0.31\n89.34±0.57 89.90±0.44",
          "DISFA-FER\nACC RECALL": "76.94±0.30 77.07±0.18\n77.56±0.29 76.56±0.35\n78.43±0.43 78.24±0.63\n77.65±0.93 77.85±0.36\n82.36±0.64 82.42±0.57\n77.81±0.39 79.60±0.37",
          "WFLW-FER\nACC RECALL": "74.67±0.29 77.83±0.55\n75.51±0.33 78.04±0.40\n73.76±0.43 78.38±0.53\n74.27±0.54 76.93±0.74\n77.24±0.52 78.23±0.43\n74.97±0.03 75.47±0.11",
          "CMU-MOSEI\nACC RECALL": "81.38±0.72 81.64±0.86\n83.51±0.70 82.56±0.56\n83.43±0.46 82.76±0.45\n82.32±0.75 80.73±0.29\n84.24±0.54 86.36±0.72\n79.40±0.61 82.17±0.27"
        },
        {
          "CK+\nACC RECALL": "92.74±0.64 91.12±0.99\n91.73±0.93 90.31±0.11\n94.76±0.14 94.34±0.44\n94.15±0.19 93.81±0.34\n95.02±0.56 93.99±0.27\n95.19±0.77 92.15±0.45\n94.30±0.25 93.41±0.56\n96.96±0.52 96.66±0.17",
          "DISFA-FER\nACC RECALL": "80.18±0.35 84.34±0.47\n79.14±0.37 80.66±0.58\n80.31±0.74 79.69±0.48\n81.09±0.22 80.87±0.31\n79.23±0.50 78.44±0.46\n83.51±0.37 89.13±0.56\n86.41±0.65 89.88±0.72\n87.32±0.36 93.96±0.17",
          "WFLW-FER\nACC RECALL": "81.94±0.50 80.37±0.24\n82.01±0.97 80.65±0.04\n76.19±0.73 78.54±0.08\n79.25±0.24 80.25±0.36\n83.15±0.56 81.09±0.28\n84.53±0.39 87.20±0.21\n86.64±0.51 87.65±0.55\n86.79±0.72 92.37±0.20",
          "CMU-MOSEI\nACC RECALL": "85.19±0.98 82.46±0.36\n81.60±0.28 84.24±0.95\n77.67±0.81 79.96±0.20\n85.21±0.49 85.91±0.53\n84.31±0.56 82.12±0.65\n86.68±0.45 90.14±0.68\n87.84±0.59 87.01±0.52\n89.86±0.39 91.96±0.38"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fog\n0% 10% 30% 50%": "84.8±0.2 83.6±0.3 79.5±0.6 76.5±0.3\n75.6±0.7 73.8±0.5 70.4±0.5 68.4±0.3\n72.2±0.3 71.3±0.4 68.8±0.7 65.3±0.2\n87.0±0.6 86.2±0.8 82.3±0.6 76.4±0.5\n80.4±0.5 78.6±0.3 76.4±0.5 71.4±0.7",
          "Mask\n0% 10% 30% 50%": "84.8±0.2 83.5±0.4 80.1±0.5 77.4±0.3\n75.6±0.7 73.8±0.5 71.6±0.7 68.7±0.4\n72.2±0.3 71.6±0.7 69.5±0.7 65.6±0.7\n87.0±0.6 86.1±0.5 82.3±0.4 77.1±0.3\n80.4±0.5 79.7±0.2 77.0±0.6 73.9±0.4"
        },
        {
          "Fog\n0% 10% 30% 50%": "81.4±0.7 79.2±0.4 76.7±0.5 73.6±0.5\n83.5±0.7 82.5±0.7 80.4±0.6 77.5±0.4\n83.4±0.5 81.4±0.6 80.8±0.4 77.8±0.3\n82.3±0.8 81.4±0.5 78.1±0.6 74.4±0.6\n84.2±0.5 83.4±0.7 81.7±0.5 78.6±0.7\n79.4±0.6 78.3±0.4 75.5±0.4 72.5±0.7",
          "Mask\n0% 10% 30% 50%": "81.4±0.7 80.8±0.5 77.8±0.5 74.4±0.2\n83.5±0.7 82.4±0.3 80.0±0.4 76.8±0.6\n83.4±0.5 82.5±0.3 79.7±0.4 75.4±0.5\n82.3±0.8 81.4±0.6 79.6±0.7 76.3±0.4\n84.2±0.5 83.5±0.4 82.4±0.6 79.4±0.3\n79.4±0.6 78.1±0.5 75.9±0.4 72.6±0.3"
        },
        {
          "Fog\n0% 10% 30% 50%": "85.2±1.0 84.1±0.2 81.8±0.3 76.4±0.4\n81.6±0.3 79.6±0.4 75.6±0.3 71.3±0.2\n77.7±0.8 76.3±0.7 73.2±0.4 71.4±0.3\n85.2±0.5 84.0±0.4 82.3±0.4 79.2±0.5\n84.3±0.6 83.2±0.5 81.9±0.5 78.6±0.4\n86.6±0.5 85.0±0.5 83.7±0.4 81.6±0.7\n87.8±0.6 87.2±0.4 86.7±0.6 84.3±0.4\n89.8±0.4 89.3±0.3 87.9±0.4 85.1±0.4",
          "Mask\n0% 10% 30% 50%": "85.2±1.0 83.7±0.7 80.5±0.4 75.4±0.3\n81.6±0.3 79.8±0.4 76.4±0.6 72.5±0.7\n77.7±0.8 76.5±0.6 73.6±0.4 70.6±0.4\n85.2±0.5 83.9±0.5 81.6±0.4 75.2±0.4\n84.3±0.6 82.8±0.5 79.7±0.6 75.0±0.5\n84.3±0.6 83.2±0.3 81.9±0.7 78.0±0.6\n84.3±0.6 82.7±0.9 80.1±0.2 76.1±0.5\n89.8±0.4 87.3±0.4 84.6±0.2 80.4±0.4"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "F",
          "Column_3": "og",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "M",
          "Column_8": "as",
          "Column_9": "k",
          "Column_10": "",
          "Column_11": "D",
          "0%\n10%\n30%": "is",
          "Column_13": "tort",
          "Column_14": "ion"
        },
        {
          "Column_1": "ust\ntrod",
          "Column_2": "ana\nuce",
          "Column_3": "lys\ndn",
          "Column_4": "iso\nois",
          "Column_5": "",
          "Column_6": "MO\nata",
          "Column_7": "SEI\nset.",
          "Column_8": ".(",
          "Column_9": "%)r",
          "Column_10": "",
          "Column_11": "the",
          "0%\n10%\n30%": "pr",
          "Column_13": "opo",
          "Column_14": "rtio"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "9": "llaceR\n9"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "2M-\n2M-": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "Mgf",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "",
          "Column_24": "",
          "Column_25": "",
          "Column_26": "",
          "Column_27": "",
          "Column_28": "",
          "Column_29": "",
          "Column_30": "",
          "Column_31": "",
          "Column_32": "",
          "Column_33": "",
          "Column_34": "",
          "Column_35": "",
          "Column_36": "",
          "Column_37": "",
          "Column_38": "",
          "Column_39": "",
          "Column_40": "",
          "Column_41": "",
          "Column_42": "",
          "Column_43": "",
          "Column_44": "",
          "Column_45": "",
          "Column_46": "",
          "Column_47": "",
          "Column_48": "",
          "Column_49": "",
          "Column_50": "",
          "Column_51": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "Gr",
          "Column_4": "ou",
          "Column_5": "p A",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "Gr",
          "Column_9": "oup",
          "Column_10": "B",
          "Column_11": "",
          "2M-\n2M-": "",
          "Column_13": "",
          "Column_14": "Gr",
          "Column_15": "oup",
          "Column_16": "",
          "Column_17": "C",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "",
          "Column_24": "",
          "Column_25": "",
          "Column_26": "",
          "Column_27": "",
          "Column_28": "",
          "Column_29": "",
          "Column_30": "",
          "Column_31": "",
          "Column_32": "",
          "Column_33": "",
          "Column_34": "",
          "Column_35": "",
          "Column_36": "",
          "Column_37": "",
          "Column_38": "",
          "Column_39": "",
          "Column_40": "",
          "Column_41": "",
          "Column_42": "",
          "Column_43": "",
          "Column_44": "",
          "Column_45": "",
          "Column_46": "",
          "Column_47": "",
          "Column_48": "",
          "Column_49": "",
          "Column_50": "",
          "Column_51": ""
        },
        {
          "Column_1": "",
          "Column_2": "blat",
          "Column_3": "ion",
          "Column_4": "stu",
          "Column_5": "dyo",
          "Column_6": "fST-F2M",
          "Column_7": ".",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "2M-\n2M-": "up\nren\nmpa",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "",
          "Column_24": "",
          "Column_25": "",
          "Column_26": "",
          "Column_27": "",
          "Column_28": "",
          "Column_29": "",
          "Column_30": "",
          "Column_31": "",
          "Column_32": "",
          "Column_33": "",
          "Column_34": "",
          "Column_35": "",
          "Column_36": "",
          "Column_37": "",
          "Column_38": "",
          "Column_39": "",
          "Column_40": "",
          "Column_41": "",
          "Column_42": "",
          "Column_43": "",
          "Column_44": "",
          "Column_45": "",
          "Column_46": "",
          "Column_47": "",
          "Column_48": "",
          "Column_49": "",
          "Column_50": "",
          "Column_51": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "2M-\n2M-": "",
          "Column_13": "",
          "Column_14": "10\n9\n8\nycaruccA\n7\n6\n5\n4",
          "Column_15": "0\n0\n0\n0\n0\n0\n0",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "FN2EN\nUniMSE\nSepTr\nCOGMEN\nMARLIN\nMAML\nANIL\nST-F2M\nGroup A Group B Group C Group D Group E",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "",
          "Column_24": "",
          "Column_25": "",
          "Column_26": "",
          "Column_27": "",
          "Column_28": "",
          "Column_29": "",
          "Column_30": "",
          "Column_31": "",
          "Column_32": "",
          "Column_33": "",
          "Column_34": "",
          "Column_35": "",
          "Column_36": "",
          "Column_37": "",
          "Column_38": "",
          "Column_39": "",
          "Column_40": "",
          "Column_41": "",
          "Column_42": "",
          "Column_43": "",
          "Column_44": "",
          "Column_45": "",
          "Column_46": "",
          "Column_47": "",
          "Column_48": "",
          "Column_49": "",
          "Column_50": "",
          "Column_51": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "2M-\n2M-": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "FN2EN\nUniMSE\nSepTr\nCOGMEN\nMARLIN\nMAML\nANIL\nST-F2M",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "",
          "Column_24": "",
          "Column_25": "",
          "Column_26": "",
          "Column_27": "",
          "Column_28": "",
          "Column_29": "",
          "Column_30": "",
          "Column_31": "",
          "Column_32": "",
          "Column_33": "",
          "Column_34": "",
          "Column_35": "",
          "Column_36": "",
          "Column_37": "",
          "Column_38": "",
          "Column_39": "",
          "Column_40": "",
          "Column_41": "",
          "Column_42": "",
          "Column_43": "",
          "Column_44": "",
          "Column_45": "",
          "Column_46": "",
          "Column_47": "",
          "Column_48": "",
          "Column_49": "",
          "Column_50": "",
          "Column_51": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "2M-\n2M-": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "",
          "Column_24": "",
          "Column_25": "",
          "Column_26": "",
          "Column_27": "",
          "Column_28": "",
          "Column_29": "",
          "Column_30": "",
          "Column_31": "",
          "Column_32": "",
          "Column_33": "",
          "Column_34": "",
          "Column_35": "",
          "Column_36": "",
          "Column_37": "",
          "Column_38": "",
          "Column_39": "",
          "Column_40": "",
          "Column_41": "",
          "Column_42": "",
          "Column_43": "",
          "Column_44": "",
          "Column_45": "",
          "Column_46": "",
          "Column_47": "",
          "Column_48": "",
          "Column_49": "",
          "Column_50": "AN\nST",
          "Column_51": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "2M-\n2M-": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "G",
          "Column_20": "ro",
          "Column_21": "up",
          "Column_22": "A",
          "Column_23": "",
          "Column_24": "",
          "Column_25": "",
          "Column_26": "G",
          "Column_27": "rou",
          "Column_28": "p B",
          "Column_29": "",
          "Column_30": "",
          "Column_31": "",
          "Column_32": "",
          "Column_33": "G",
          "Column_34": "ro",
          "Column_35": "up",
          "Column_36": "C",
          "Column_37": "",
          "Column_38": "",
          "Column_39": "",
          "Column_40": "",
          "Column_41": "G",
          "Column_42": "ro",
          "Column_43": "up",
          "Column_44": "D",
          "Column_45": "",
          "Column_46": "",
          "Column_47": "",
          "Column_48": "G",
          "Column_49": "ro",
          "Column_50": "up",
          "Column_51": "E"
        },
        {
          "Column_1": "",
          "Column_2": "om\nda\nolle",
          "Column_3": "par\nta,t\nctio",
          "Column_4": "(a)\niso\nhe\nne",
          "Column_5": "Dat\nnre\nrobo\nnvir",
          "Column_6": "",
          "Column_7": "on\nract\nand\nnd",
          "Column_8": "ical\nide\n(b)",
          "Column_9": "ca\nntifi\npro",
          "Column_10": "se\nes\nvid",
          "Column_11": "",
          "2M-\n2M-": "",
          "Column_13": "",
          "Column_14": "A∼\ntan\nriso",
          "Column_15": "G\ngles\nnre",
          "Column_16": "",
          "Column_17": "rou\n,t\nsu",
          "Column_18": "p\nha\nlts",
          "Column_19": "E\nt\n.",
          "Column_20": "r\nis,",
          "Column_21": "esp\nthe",
          "Column_22": "e\ns",
          "Column_23": "ctiv\nam",
          "Column_24": "",
          "Column_25": "re\nir",
          "Column_26": "p\nec",
          "Column_27": "res\ntion",
          "Column_28": "(b\nent\nas",
          "Column_29": ")\nth\nt",
          "Column_30": "C\ne\nhe",
          "Column_31": "",
          "Column_32": "p\ne\nte",
          "Column_33": "ar\ngr\nr",
          "Column_34": "iso\nou\nfa",
          "Column_35": "n\nps\nces",
          "Column_36": "re\no",
          "Column_37": "su\nf\n−",
          "Column_38": "lt\nd\n60",
          "Column_39": "",
          "Column_40": "c\n−",
          "Column_41": "ol\n3",
          "Column_42": "lec\n0◦",
          "Column_43": "ted\n,0",
          "Column_44": "i\n◦",
          "Column_45": "n\n,",
          "Column_46": "t\n30",
          "Column_47": "",
          "Column_48": "ob\nnd",
          "Column_49": "ot\n6",
          "Column_50": "plat\n0◦.",
          "Column_51": "for\n(a)"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Standard\nAverageACC WorstACC": "85.70±0.64 83.16±0.59\n86.40±0.67 84.21±0.58\n92.74±0.64 91.30±0.44\n94.76±0.14 92.8±0.36\n94.15±0.19 90.69±0.25\n95.02±0.56 93.46±0.54\n95.19±0.77 91.49±0.84\n94.30±0.25 91.76±0.18\n96.96±0.52 95.01±0.39"
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Multimodal emotion recognition with temporal and semantic consistency",
      "authors": [
        "B Chen",
        "Q Cao",
        "M Hou",
        "Z Zhang",
        "G Lu",
        "D Zhang"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "2",
      "title": "Graphcfc: A directed graph based cross-modal feature complementation approach for multimodal conversational emotion recognition",
      "authors": [
        "J Li",
        "X Wang",
        "G Lv",
        "Z Zeng"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "3",
      "title": "Emocov: Machine learning for emotion detection, analysis and visualization using covid-19 tweets",
      "authors": [
        "M Kabir",
        "S Madria"
      ],
      "year": "2021",
      "venue": "Online Social Networks and Media"
    },
    {
      "citation_id": "4",
      "title": "Recognition of emotions in user-generated videos through frame-level adaptation and emotion intensity learning",
      "authors": [
        "H Zhang",
        "M Xu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "5",
      "title": "Customer preferences extraction for air purifiers based on fine-grained sentiment analysis of online reviews",
      "authors": [
        "J Zhang",
        "A Zhang",
        "D Liu",
        "Y Bian"
      ],
      "year": "2021",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "6",
      "title": "Self-organizing double function-link fuzzy brain emotional control system design for uncertain nonlinear systems",
      "authors": [
        "T.-T Huynh",
        "C.-M Lin",
        "T.-L Le",
        "N.-Q.-K Le",
        "V.-P Vu",
        "F Chao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics: Systems"
    },
    {
      "citation_id": "7",
      "title": "Intelligent cockpit for intelligent vehicle in metaverse: A case study of empathetic auditory regulation of human emotion",
      "authors": [
        "W Li",
        "L Wu",
        "C Wang",
        "J Xue",
        "W Hu",
        "S Li",
        "G Guo",
        "D Cao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics: Systems"
    },
    {
      "citation_id": "8",
      "title": "Subtype-aware unsupervised domain adaptation for medical diagnosis",
      "authors": [
        "X Liu",
        "X Liu",
        "B Hu",
        "W Ji",
        "F Xing",
        "J Lu",
        "J You",
        "C.-C Kuo",
        "G Fakhri",
        "J Woo"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "9",
      "title": "Fine-grained image analysis with deep learning: A survey",
      "authors": [
        "X.-S Wei",
        "Y.-Z Song",
        "O Mac Aodha",
        "J Wu",
        "Y Peng",
        "J Tang",
        "J Yang",
        "S Belongie"
      ],
      "year": "2021",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "10",
      "title": "Disturbance rejection in mimo systems with emotional-learning-based controller: Application to variable rotor-speed helicopters",
      "authors": [
        "B Debnath",
        "S Mija"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics: Systems"
    },
    {
      "citation_id": "11",
      "title": "Brain network manifold learned by cognition-inspired graph embedding model for emotion recognition",
      "authors": [
        "C Li",
        "P Li",
        "Z Chen",
        "L Yang",
        "F Li",
        "F Wan",
        "Z Cao",
        "D Yao",
        "B.-L Lu",
        "P Xu"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics: Systems"
    },
    {
      "citation_id": "12",
      "title": "Combining a parallel 2d cnn with a self-attention dilated residual network for ctc-based discrete speech emotion recognition",
      "authors": [
        "Z Zhao",
        "Q Li",
        "Z Zhang",
        "N Cummins",
        "H Wang",
        "J Tao",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "13",
      "title": "Mlg-ncs: Multimodal local-global neuromorphic computing system for affective video content analysis",
      "authors": [
        "X Ji",
        "Z Dong",
        "G Zhou",
        "C Lai",
        "D Qi"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics: Systems"
    },
    {
      "citation_id": "14",
      "title": "User preference mining based on fine-grained sentiment analysis",
      "authors": [
        "Y Xiao",
        "C Li",
        "M Thürer",
        "Y Liu",
        "T Qu"
      ],
      "year": "2022",
      "venue": "Journal of Retailing and Consumer Services"
    },
    {
      "citation_id": "15",
      "title": "Few-shot learning for fine-grained emotion recognition using physiological signals",
      "authors": [
        "T Zhang",
        "A Ali",
        "A Hanjalic",
        "P Cesar"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "16",
      "title": "Challenges in the diagnosis of parkinson's disease",
      "authors": [
        "E Tolosa",
        "A Garrido",
        "S Scholz",
        "W Poewe"
      ],
      "year": "2021",
      "venue": "The Lancet Neurology"
    },
    {
      "citation_id": "17",
      "title": "Multimodal emotion classification with multi-level semantic reasoning network",
      "authors": [
        "T Zhu",
        "L Li",
        "J Yang",
        "S Zhao",
        "X Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "18",
      "title": "Unimse: Towards unified multimodal sentiment analysis and emotion recognition",
      "authors": [
        "G Hu",
        "T.-E Lin",
        "Y Zhao",
        "G Lu",
        "Y Wu",
        "Y Li"
      ],
      "year": "2022",
      "venue": "Unimse: Towards unified multimodal sentiment analysis and emotion recognition",
      "arxiv": "arXiv:2211.11256"
    },
    {
      "citation_id": "19",
      "title": "Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "authors": [
        "A Zadeh",
        "R Zellers",
        "E Pincus",
        "L.-P Morency"
      ],
      "year": "2016",
      "venue": "Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "arxiv": "arXiv:1606.06259"
    },
    {
      "citation_id": "20",
      "title": "Learning to augment expressions for few-shot finegrained facial expression recognition",
      "authors": [
        "W Wang",
        "Y Fu",
        "Q Sun",
        "T Chen",
        "C Cao",
        "Z Zheng",
        "G Xu",
        "H Qiu",
        "Y.-G Jiang",
        "X Xue"
      ],
      "year": "2020",
      "venue": "Learning to augment expressions for few-shot finegrained facial expression recognition",
      "arxiv": "arXiv:2001.06144"
    },
    {
      "citation_id": "21",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "S Poria",
        "N Majumder",
        "R Mihalcea",
        "E Hovy"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "22",
      "title": "Feda: Finegrained emotion difference analysis for facial expression recognition",
      "authors": [
        "H Liu",
        "H Cai",
        "Q Lin",
        "X Zhang",
        "X Li",
        "H Xiao"
      ],
      "year": "2023",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "23",
      "title": "Fine-grained facial expression recognition in the wild",
      "authors": [
        "L Liang",
        "C Lang",
        "Y Li",
        "S Feng",
        "J Zhao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Information Forensics and Security"
    },
    {
      "citation_id": "24",
      "title": "Training deep networks for facial expression recognition with crowd-sourced label distribution",
      "authors": [
        "E Barsoum",
        "C Zhang",
        "C Ferrer",
        "Z Zhang"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "25",
      "title": "Adaptive weighting of handcrafted feature losses for facial expression recognition",
      "authors": [
        "W Xie",
        "L Shen",
        "J Duan"
      ],
      "year": "2019",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "26",
      "title": "Mixed feelings: expression of non-basic emotions in a muscle-based talking head",
      "authors": [
        "I Albrecht",
        "M Schröder",
        "J Haber",
        "H.-P Seidel"
      ],
      "year": "2005",
      "venue": "Virtual Reality"
    },
    {
      "citation_id": "27",
      "title": "Convolutional features-based broad learning with lstm for multidimensional facial emotion recognition in human-robot interaction",
      "authors": [
        "L Chen",
        "M Li",
        "M Wu",
        "W Pedrycz",
        "K Hirota"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics: Systems"
    },
    {
      "citation_id": "28",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "29",
      "title": "Design and analysis of a closed-loop emotion regulation system based on multimodal affective computing and emotional markov chain",
      "authors": [
        "X Wang",
        "C.-Z Li",
        "Z Sun",
        "Y Xu"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics: Systems"
    },
    {
      "citation_id": "30",
      "title": "Long dialogue emotion detection based on commonsense knowledge graph guidance",
      "authors": [
        "W Nie",
        "Y Bao",
        "Y Zhao",
        "A Liu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "31",
      "title": "Emotional expression: Advances in basic emotion theory",
      "authors": [
        "D Keltner",
        "D Sauter",
        "J Tracy",
        "A Cowen"
      ],
      "year": "2019",
      "venue": "Journal of nonverbal behavior"
    },
    {
      "citation_id": "32",
      "title": "Multimodal spontaneous emotion corpus for human behavior analysis",
      "authors": [
        "Z Zhang",
        "J Girard",
        "Y Wu",
        "X Zhang",
        "P Liu",
        "U Ciftci",
        "S Canavan",
        "M Reale",
        "A Horowitz",
        "H Yang"
      ],
      "year": "2016",
      "venue": "Proceedings"
    },
    {
      "citation_id": "33",
      "title": "A survey of textual emotion detection",
      "authors": [
        "S Al-Saqqa",
        "H Abdel-Nabi",
        "A Awajan"
      ],
      "year": "2018",
      "venue": "2018 8th International Conference on Computer Science and Information Technology (CSIT)"
    },
    {
      "citation_id": "34",
      "title": "Facial expression recognition based on deep learning",
      "authors": [
        "H Ge",
        "Z Zhu",
        "Y Dai",
        "B Wang",
        "X Wu"
      ],
      "year": "2022",
      "venue": "Computer Methods and Programs in Biomedicine"
    },
    {
      "citation_id": "35",
      "title": "A real time facial expression classification system using local binary patterns",
      "authors": [
        "S Happy",
        "A George",
        "A Routray"
      ],
      "year": "2012",
      "venue": "2012 4th International conference on intelligent human computer interaction (IHCI)"
    },
    {
      "citation_id": "36",
      "title": "Human facial expression recognition using stepwise linear discriminant analysis and hidden conditional random fields",
      "authors": [
        "M Siddiqi",
        "R Ali",
        "A Khan",
        "Y.-T Park",
        "S Lee"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "37",
      "title": "Facial expression recognition based on local region specific features and support vector machines",
      "authors": [
        "D Ghimire",
        "S Jeong",
        "J Lee",
        "S Park"
      ],
      "year": "2017",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "38",
      "title": "A brief review of facial emotion recognition based on visual information",
      "authors": [
        "B Ko"
      ],
      "year": "2018",
      "venue": "sensors"
    },
    {
      "citation_id": "39",
      "title": "Geometric-convolutional feature fusion based on learning propagation for facial expression recognition",
      "authors": [
        "Y Tang",
        "X Zhang",
        "H Wang"
      ],
      "year": "2018",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "40",
      "title": "Ga-svm-based facial emotion recognition using facial geometric features",
      "authors": [
        "X Liu",
        "X Cheng",
        "K Lee"
      ],
      "year": "2020",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "41",
      "title": "Graph based feature extraction and hybrid classification approach for facial expression recognition",
      "authors": [
        "L Krithika",
        "G Priya"
      ],
      "year": "2021",
      "venue": "Journal of ambient intelligence and humanized computing"
    },
    {
      "citation_id": "42",
      "title": "Mer-gcn: Microexpression recognition based on relation modeling with graph convolutional networks",
      "authors": [
        "L Lo",
        "H.-X Xie",
        "H.-H Shuai",
        "W.-H Cheng"
      ],
      "year": "2020",
      "venue": "2020 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)"
    },
    {
      "citation_id": "43",
      "title": "Facial expression recognition based on deep evolutional spatial-temporal networks",
      "authors": [
        "K Zhang",
        "Y Huang",
        "Y Du",
        "L Wang"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "44",
      "title": "Fg-agr: Finegrained associative graph representation for facial expression recognition in the wild",
      "authors": [
        "C Li",
        "X Li",
        "X Wang",
        "D Huang",
        "Z Liu",
        "L Liao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "45",
      "title": "Are there basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Are there basic emotions"
    },
    {
      "citation_id": "46",
      "title": "Eeg-based emotion recognition: A state-of-the-art review of current trends and opportunities",
      "authors": [
        "N Suhaimi",
        "J Mountstephens",
        "J Teo"
      ],
      "year": "2020",
      "venue": "Computational intelligence and neuroscience"
    },
    {
      "citation_id": "47",
      "title": "Gut microbiota and fear processing in women affected by obesity: An exploratory pilot study",
      "authors": [
        "F Scarpina",
        "S Turroni",
        "S Mambrini",
        "M Barone",
        "S Cattaldo",
        "S Mai",
        "E Prina",
        "I Bastoni",
        "S Cappelli",
        "G Castelnuovo"
      ],
      "year": "2022",
      "venue": "Nutrients"
    },
    {
      "citation_id": "48",
      "title": "Emo-sensory communication, emo-sensory intelligence and gender",
      "authors": [
        "E Naji Meidani",
        "H Makiabadi",
        "M Zabetipour",
        "H Abbasnejad",
        "A Pooresfehani",
        "S Shayesteh"
      ],
      "year": "2022",
      "venue": "Journal of Business, Communication & Technology"
    },
    {
      "citation_id": "49",
      "title": "Emotion recognition from geometric fuzzy membership functions",
      "authors": [
        "R Vishnu Priya"
      ],
      "year": "2019",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "50",
      "title": "Group based emotion recognition from video sequence with hybrid optimization based recurrent fuzzy neural network",
      "authors": [
        "V Sreenivas",
        "V Namdeo",
        "E Kumar"
      ],
      "year": "2020",
      "venue": "Journal of Big Data"
    },
    {
      "citation_id": "51",
      "title": "Human emotion recognition based on active appearance model and semi-supervised fuzzy c-means",
      "authors": [
        "D Liliana",
        "M Widyanto",
        "T Basaruddin"
      ],
      "year": "2016",
      "venue": "2016 international conference on advanced computer science and information systems (ICACSIS)"
    },
    {
      "citation_id": "52",
      "title": "A review on machine learning styles in computer vision-techniques and future directions",
      "authors": [
        "S Mahadevkar",
        "B Khemani",
        "S Patil",
        "K Kotecha",
        "D Vora",
        "A Abraham",
        "L Gabralla"
      ],
      "year": "2022",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "53",
      "title": "Model-agnostic meta-learning for fast adaptation of deep networks",
      "authors": [
        "C Finn",
        "P Abbeel",
        "S Levine"
      ],
      "year": "2017",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "54",
      "title": "Reptile: a scalable metalearning algorithm",
      "authors": [
        "A Nichol",
        "J Schulman"
      ],
      "year": "2018",
      "venue": "Reptile: a scalable metalearning algorithm",
      "arxiv": "arXiv:1803.02999"
    },
    {
      "citation_id": "55",
      "title": "Rapid learning or feature reuse? towards understanding the effectiveness of maml",
      "authors": [
        "A Raghu",
        "M Raghu",
        "S Bengio",
        "O Vinyals"
      ],
      "year": "2019",
      "venue": "Rapid learning or feature reuse? towards understanding the effectiveness of maml",
      "arxiv": "arXiv:1909.09157"
    },
    {
      "citation_id": "56",
      "title": "Meta-learning with memory-augmented neural networks",
      "authors": [
        "A Santoro",
        "S Bartunov",
        "M Botvinick",
        "D Wierstra",
        "T Lillicrap"
      ],
      "year": "2016",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "57",
      "title": "Meta-learning improves lifelong relation extraction",
      "authors": [
        "A Obamuyide",
        "A Vlachos"
      ],
      "year": "2019",
      "venue": "Proceedings of the 4th Workshop on Representation Learning for NLP"
    },
    {
      "citation_id": "58",
      "title": "Prototypical networks for few-shot learning",
      "authors": [
        "J Snell",
        "K Swersky",
        "R Zemel"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "59",
      "title": "Learning to compare: Relation network for few-shot learning",
      "authors": [
        "F Sung",
        "Y Yang",
        "L Zhang",
        "T Xiang",
        "P Torr",
        "T Hospedales"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "60",
      "title": "Variational metric scaling for metric-based meta-learning",
      "authors": [
        "J Chen",
        "L.-M Zhan",
        "X.-M Wu",
        "F.-L Chung"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "61",
      "title": "Deep neural network for emotion recognition based on meta-transfer learning",
      "authors": [
        "H Tang",
        "G Jiang",
        "Q Wang"
      ],
      "year": "2022",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "62",
      "title": "Emotion recognition from facial images with simultaneous occlusion, pose and illumination variations using meta-learning",
      "authors": [
        "S Kuruvayil",
        "S Palaniswamy"
      ],
      "year": "2022",
      "venue": "Journal of King Saud University-Computer and Information Sciences"
    },
    {
      "citation_id": "63",
      "title": "Meta-transfer learning for emotion recognition",
      "authors": [
        "D Nguyen",
        "D Nguyen",
        "S Sridharan",
        "S Denman",
        "T Nguyen",
        "D Dean",
        "C Fookes"
      ],
      "year": "2023",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "64",
      "title": "Cluster-level contrastive learning for emotion recognition in conversations",
      "authors": [
        "K Yang",
        "T Zhang",
        "H Alhuzali",
        "S Ananiadou"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "65",
      "title": "The psychology of emotion regulation: An integrative review",
      "authors": [
        "S Koole"
      ],
      "year": "2009",
      "venue": "Cognition and emotion"
    },
    {
      "citation_id": "66",
      "title": "Comparison of mamdani-type and sugeno-type fuzzy inference systems for air conditioning system",
      "authors": [
        "A Kaur",
        "A Kaur"
      ],
      "year": "2012",
      "venue": "International Journal of Soft Computing and Engineering (IJSCE)"
    },
    {
      "citation_id": "67",
      "title": "Disfa: A spontaneous facial action intensity database",
      "authors": [
        "S Mavadati",
        "M Mahoor",
        "K Bartlett",
        "P Trinh",
        "J Cohn"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "68",
      "title": "Look at boundary: A boundary-aware face alignment algorithm",
      "authors": [
        "W Wu",
        "C Qian",
        "S Yang",
        "Q Wang",
        "Y Cai",
        "Q Zhou"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "69",
      "title": "Spatio-temporal graph convolutional networks: A deep learning framework for traffic forecasting",
      "authors": [
        "B Yu",
        "H Yin",
        "Z Zhu"
      ],
      "year": "2017",
      "venue": "Spatio-temporal graph convolutional networks: A deep learning framework for traffic forecasting",
      "arxiv": "arXiv:1709.04875"
    },
    {
      "citation_id": "70",
      "title": "Rethinking graph neural architecture search from message-passing",
      "authors": [
        "S Cai",
        "L Li",
        "J Deng",
        "B Zhang",
        "Z.-J Zha",
        "L Su",
        "Q Huang"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "71",
      "title": "Fitness functions in evolutionary robotics: A survey and analysis",
      "authors": [
        "A Nelson",
        "G Barlow",
        "L Doitsidis"
      ],
      "year": "2009",
      "venue": "Robotics and Autonomous Systems"
    },
    {
      "citation_id": "72",
      "title": "Fitness function design to improve evolutionary structural testing",
      "authors": [
        "A Baresel",
        "H Sthamer",
        "M Schmidt"
      ],
      "year": "2002",
      "venue": "Proceedings of the 4th Annual Conference on Genetic and Evolutionary Computation"
    },
    {
      "citation_id": "73",
      "title": "Improving adam optimizer",
      "authors": [
        "A Tato",
        "R Nkambou"
      ],
      "year": "2018",
      "venue": "Improving adam optimizer"
    },
    {
      "citation_id": "74",
      "title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "Z Ambadar",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "2010 ieee computer society conference on computer vision and pattern recognition-workshops"
    },
    {
      "citation_id": "75",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "76",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "77",
      "title": "Tailor versatile multi-modal learning for multi-label emotion recognition",
      "authors": [
        "Y Zhang",
        "M Chen",
        "J Shen",
        "C Wang"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "78",
      "title": "Openface: an open source facial behavior analysis toolkit",
      "authors": [
        "T Baltrušaitis",
        "P Robinson",
        "L.-P Morency"
      ],
      "year": "2016",
      "venue": "2016 IEEE winter conference on applications of computer vision"
    },
    {
      "citation_id": "79",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "J Pennington",
        "R Socher",
        "C Manning"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "80",
      "title": "Facenet2expnet: Regularizing a deep face recognition net for expression recognition",
      "authors": [
        "H Ding",
        "S Zhou",
        "R Chellappa"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "81",
      "title": "Pre-training strategies and datasets for facial representation learning",
      "authors": [
        "A Bulat",
        "S Cheng",
        "J Yang",
        "A Garbett",
        "E Sanchez",
        "G Tzimiropoulos"
      ],
      "year": "2022",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "82",
      "title": "Propagationnet: Propagate points to curve to learn structure information",
      "authors": [
        "X Huang",
        "W Deng",
        "H Shen",
        "X Zhang",
        "J Ye"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "83",
      "title": "Septr: Separable transformer for audio spectrogram processing",
      "authors": [
        "N.-C Ristea",
        "R Ionescu",
        "F Khan"
      ],
      "year": "2022",
      "venue": "Septr: Separable transformer for audio spectrogram processing",
      "arxiv": "arXiv:2203.09581"
    },
    {
      "citation_id": "84",
      "title": "Reptile: a scalable metalearning algorithm",
      "authors": [
        "A Nichol",
        "J Schulman"
      ],
      "year": "2018",
      "venue": "Reptile: a scalable metalearning algorithm",
      "arxiv": "arXiv:1803.02999"
    },
    {
      "citation_id": "85",
      "title": "Prototypical networks for few-shot learning",
      "authors": [
        "J Snell",
        "K Swersky",
        "R Zemel"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "86",
      "title": "Learning to compare: Relation network for few-shot learning",
      "authors": [
        "F Sung",
        "Y Yang",
        "L Zhang",
        "T Xiang",
        "P Torr",
        "T Hospedales"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "87",
      "title": "Hacking task confounder in meta-learning",
      "authors": [
        "J Wang",
        "W Qiang",
        "Y Ren",
        "Z Song",
        "X Su",
        "C Zheng"
      ],
      "year": "2023",
      "venue": "Hacking task confounder in meta-learning",
      "arxiv": "arXiv:2312.05771"
    },
    {
      "citation_id": "88",
      "title": "Combining deep and unsupervised features for multilingual speech emotion recognition",
      "authors": [
        "V Scotti",
        "F Galati",
        "L Sbattella",
        "R Tedesco"
      ],
      "year": "2021",
      "venue": "International Conference on Pattern Recognition"
    },
    {
      "citation_id": "89",
      "title": "Cogmen: Contextualized gnn based multimodal emotion recognition",
      "authors": [
        "A Joshi",
        "A Bhat",
        "A Jain",
        "A Singh",
        "A Modi"
      ],
      "year": "2022",
      "venue": "Cogmen: Contextualized gnn based multimodal emotion recognition",
      "arxiv": "arXiv:2205.02455"
    },
    {
      "citation_id": "90",
      "title": "Marlin: Masked autoencoder for facial video representation learning",
      "authors": [
        "Z Cai",
        "S Ghosh",
        "K Stefanov",
        "A Dhall",
        "J Cai",
        "H Rezatofighi",
        "R Haffari",
        "M Hayat"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "91",
      "title": "Emotion recognition based on brain-like multimodal hierarchical perception",
      "authors": [
        "X Zhu",
        "Y Huang",
        "X Wang",
        "R Wang"
      ],
      "year": "2024",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "92",
      "title": "Fusing pairwise modalities for emotion recognition in conversations",
      "authors": [
        "C Fan",
        "J Lin",
        "R Mao",
        "E Cambria"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "93",
      "title": "A novel transformer autoencoder for multi-modal emotion recognition with incomplete data",
      "authors": [
        "C Cheng",
        "W Liu",
        "Z Fan",
        "L Feng",
        "Z Jia"
      ],
      "year": "2024",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "94",
      "title": "Deep imbalanced learning for multimodal emotion recognition in conversations",
      "authors": [
        "T Meng",
        "Y Shou",
        "W Ai",
        "N Yin",
        "K Li"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Artificial Intelligence"
    },
    {
      "citation_id": "95",
      "title": "Online bagging and boosting",
      "authors": [
        "N Oza",
        "S Russell"
      ],
      "year": "2001",
      "venue": "International Workshop on Artificial Intelligence and Statistics"
    },
    {
      "citation_id": "96",
      "title": "Applications of machine learning predictive models in the chronic disease diagnosis",
      "authors": [
        "G Battineni",
        "G Sagaro",
        "N Chinatalapudi",
        "F Amenta"
      ],
      "year": "2020",
      "venue": "Journal of personalized medicine"
    },
    {
      "citation_id": "97",
      "title": "Effective image enhancement techniques for fog-affected indoor and outdoor images",
      "authors": [
        "K Kim",
        "S Kim",
        "K.-S Kim"
      ],
      "year": "2018",
      "venue": "IET Image Processing"
    },
    {
      "citation_id": "98",
      "title": "A new application of fractional atanganabaleanu derivatives: designing abc-fractional masks in image processing",
      "authors": [
        "B Ghanbari",
        "A Atangana"
      ],
      "year": "2020",
      "venue": "Physica A: Statistical Mechanics and its Applications"
    },
    {
      "citation_id": "99",
      "title": "Distortion agnostic deep watermarking",
      "authors": [
        "X Luo",
        "R Zhan",
        "H Chang",
        "F Yang",
        "P Milanfar"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "100",
      "title": "A precision analysis of camera distortion models",
      "authors": [
        "Z Tang",
        "R Von Gioi",
        "P Monasse",
        "J.-M Morel"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "101",
      "title": "Occlusionaware real-time object tracking",
      "authors": [
        "X Dong",
        "J Shen",
        "D Yu",
        "W Wang",
        "J Liu",
        "H Huang"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Multimedia"
    }
  ]
}