{
  "paper_id": "2309.08108v1",
  "title": "Foundation Model Assisted Automatic Speech Emotion Recognition: Transcribing, Annotating, And Augmenting",
  "published": "2023-09-15T02:19:03Z",
  "authors": [
    "Tiantian Feng",
    "Shrikanth Narayanan"
  ],
  "keywords": [
    "Speech",
    "Emotion recognition",
    "Foundation model",
    "Large Language Model"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Significant advances are being made in speech emotion recognition (SER) using deep learning models. Nonetheless, training SER systems remains challenging, requiring both time and costly resources. Like many other machine learning tasks, acquiring datasets for SER requires substantial data annotation efforts, including transcription and labeling. These annotation processes present challenges when attempting to scale up conventional SER systems. Recent developments in foundational models have had a tremendous impact, giving rise to applications such as ChatGPT. These models have enhanced human-computer interactions including bringing unique possibilities for streamlining data collection in fields like SER. In this research, we explore the use of foundational models to assist in automating SER from transcription and annotation to augmentation. Our study demonstrates that these models can generate transcriptions to enhance the performance of SER systems that rely solely on speech data. Furthermore, we note that annotating emotions from transcribed speech remains a challenging task. However, combining outputs from multiple LLMs enhances the quality of annotations. Lastly, our findings suggest the feasibility of augmenting existing speech emotion datasets by annotating unlabeled speech samples.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) has benefited considerably from using large-scale pre-trained speech models  [1, 2, 3, 4] , offering substantial performance improvements over conventional SER systems that primarily depend on low-level acoustic descriptors (e.g., speech prosody and spectral information). These advances in emotion recognition open up opportunities for widespread applications in healthcare and virtual assistants, transforming our ways of connecting, engaging, and interacting with the world. However, success in deploying SER models in real-world applications requires the acquisition of high-quality annotations to speech samples, which is often expensive, time-consuming, and privacy-unfriendly.\n\nOne typical labeling step in SER datasets involves transcribing the speech content. For example, IEMOCAP  [5] , one of the most popular SER testbeds, had obtained the professional transcriptions of the audio dialogues using a commercial service. Such a process often requires training transcribers on transcription guidelines, creating considerable R&D costs. The advent of Amazon's Mechanical Turk  [6]  (MTurk) had substantially increased the efficiency of transcribing services by providing the marketplace for human workers to perform such tasks for pay. However, it still demands many MTurk hours to transcribe the audio conversations, leading to significant costs. In addition, MTurk may not be a viable option when the data collection poses significant privacy risks and must be annotated inhouse, which is a standard practice mandated by Institutional Review Boards (IRBs) involving sensitive human subject data  [7] .\n\nFurthermore, SER dataset often requires emotion labeling. A standard emotion labeling process involves instructing multiple human annotators to assess the emotional content of the speech sample in terms of emotional descriptors. Similar to transcribing, the emotion annotation procedure yields substantial costs in hiring multiple annotators to ensure authentic appraisal of a speech sample. Moreover, utilizing services such as MTurk for emotion annotation would raise notable privacy risks. Therefore, curating the SER dataset remains a challenging task, particularly for institutions that encounter resource constraints and comply with strict regulatory guidelines.\n\nThe emergence of foundation models  [8]  delivered promising speech recognition and language reasoning performances, bringing unique opportunities to facilitate SER data curation. For example, Whisper  [4]  is designed for automatic speech recognition (ASR), trained on thousands of hours of audio data from the Internet. This model delivers remarkable zero-shot ASR performance, demonstrating its enormous potential for deployment as a transcription service. Along with the advancements in automatic transcription, large language models (LLMs) like GPT4  [9]  offer human-level text reasoning and comprehension capabilities, positioning them as candidates for reducing the involvement of human emotion annotation.\n\nIn this paper, we report comprehensive experiments on the use of foundation models in assisting curation of the speech emotion recognition dataset in transcribing, emotion annotation, and augmentation. Our study focuses on exploring modeling approaches that require a single V100-equivalent GPU, ensuring the ease of reproducibility. In summary, our contributions are listed as follows:\n\n• Our work represents one of the early studies on the use of the foundation model to assist SER dataset curation covering three critical factors: transcribing, emotion annotation, and augmentation. Fig.  1 : Our proposed foundation model-assisted automatic SER framework. The speech is first transcribed to text and is subsequently fed to LLMs to annotate categorical emotions. Our SER modeling framework involves a text and speech backbone to extract corresponding embeddings, which are then passed through a cross-attention layer to obtain the multimodal representations to predict emotion labels.\n\nthe pre-trained speech models, which are then trained with labeled speech samples for speech-related tasks. One recent popular model in this category is the Massively Multilingual Speech (MMS)  [10]  model released by Meta, which is pre-trained on 491K hours of speech. In contrast, Whisper by OpenAI  [4]  adopts a weakly supervised learning approach, with objectives to perform tasks such as voice activity detection, language identification, and speech recognition. The training of this model is conducted using a dataset comprising 680k hours of labeled speech data.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Large Language Models",
      "text": "Large language models like ChatGPT have demonstrated remarkable performance in language reasoning tasks. However, GPT4 or ChatGPT requires the user to upload the speech content to the remote server for prompting. This creates considerable privacy risks in sensitive settings and applications. Instead, we decided to explore foundation models that can operate on a single GPU, including LLaMa 2 families  [11] , Falcon families  [12] , and Flan-T5 XXL  [13] . We want to highlight that several prior works  [14, 15]  have investigated the ability of LLMs to annotate ground-truth transcriptions or ASR-generated transcription. However, most of these works consider conventional SER modeling architecture (e.g., ResNet-50). Moreover, they do not incorporate ASR-generated transcription in SER modeling and experiment with a limited set of LLMs.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Method",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Foundation Model Assisted Annotation",
      "text": "Our automatic annotation framework is presented in Fig 1 . Given an unlabeled speech sample, we first propose to obtain the speech content using foundation speech recognition models. This work investigates two recent ASR models, Whisper-Large V2 and MMS, that offer the most competitive results. After obtaining the ASRgenerated transcripts, we directly send them to the large language models. Our LLMs include LLaMa 2 families, Falcon families, and Flan-T5 XXL. The details about the foundation models used in this study and their approximate model size can be found in Table  1 . The obtained emotion labels and transcripts are used for SER training.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A Bag Of Tricks In Prompt Engineering",
      "text": "We investigate and compare several tricks in prompt engineering. Base Prompt Our prompt design is similar to  [15] , where instructing the LLMs to annotate the spoken utterance delivers decent zero-shot performance. In addition, we instruct the LLMs to choose emotions from five categories: neutral, sad, happy, angry, and other. This strategy constrains the LLMs to output more determined labels, and we introduce the option of \"other\" to filter out unconfident responses to include in SER modeling. In summary, our prompt template is: What is the emotion of this utterance? \"Everything is not working!\" Options: -neutral -sad -angry -happy -other ANSWER: sad Multiple-LLMs Agreement It is known that relying on the response from one LLM could yield biased language reasoning  [16] . To mitigate this concern, we propose ensemble the output from multiple LLMs, collecting the wisdom from multiple reasoners.\n\nLLMs + Human Feedback One critical lesson we learned from prior research is that LLMs exhibit limited zero-shot capabilities in annotating emotions from speech. Consequently, we contend that human evaluation may remain essential. However, instead of relying on multiple human raters for a majority agreement, we propose that assessing the agreement between the LLM annotations and one human feedback is sufficient for quality control.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Emotion Recognition Modeling",
      "text": "The complete model architecture is illustrated in Fig 1 . Our SER includes speech and text backbones to extract the corresponding embeddings. Specifically, we utilize Whisper-Small  [4]  and MMS-300M  [10]  as the speech backbone and Roberta as the text backbone.\n\nWe intend not to experiment with Whisper-Large as the speech backbone as it requires prohibitively large GPU capacities for our setting. The output of backbone models is subsequently fed into weighted averaging layers to combine the hidden outputs from all encoder layers. The weighted output is then passed through a cross-attention layer to obtain the multimodal representation for SER.   [17, 18, 19, 20] . We acknowledge that this inclusion criterion trivializes the automatic emotion annotation, but it ensures fair comparisons when having multiple datasets with different emotions. The emotion annotation results reported in our experiments will likely decrease in practice.\n\nIEMOCAP  [5]  contains multi-modal recordings of human interactions from 10 subjects evenly distributed between males and females.\n\nMultimodal EmotionLines Dataset (MELD)  [21]  contains more than 13000 utterances from the Friends TV series. Each utterance is labeled with seven emotions, -Anger, Disgust, Sadness, Joy, Neutral, Surprise, and Fear. We map the Joy to happy emotion and keep Anger, Sadness, and Neutral in the experiments.  [22]  corpus is developed to investigate naturalistic emotions elicited from improvised situations. The corpus comprises audio and visual data collected from 12 individuals, with an equal number of subjects from both male and female participants.  [23]  is collected from podcast recordings, with 610 speakers in the training, 30 in the development, and 50 in the test.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Msp-Improv",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Msp-Podcast",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiment Details",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Foundation Model Assisted Annotation",
      "text": "We apply MMS-300M and Whisper Large V2 to obtain the ASR output. Since LLMs with more than 10B parameters exceed most GPU memory capacities, we decided to load LLMs over 10B using float 16 instead of float 32. In addition, we load Falcon-40B with 8-bit. We use a temperature of 0.02 in all prompting experiments, as a lower temperature results in more deterministic output. We use the checkpoints of all foundation models from HuggingFace  [24] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion Recognition Modeling",
      "text": "We apply a 5-fold and 6-fold evaluation on IEMOCAP and MSP-Improv datasets respectively, where each session is regarded as a unique test fold. In contrast, we use the standard splits for training, validation, and testing from the MELD and MSP-Podcast datasets. We use the RoBERTa  [25]  model as the text backbone while we compare the speech backbones between MMS-300M and Whisper-Small. We choose MMS-300M along with MMS-1B ASR output and Whisper-Small along with Whisper Large V2 ASR output in SER modeling. Specifically, we set the batch size to 32, the learning rate to 0.0001, the max training epoch to 30, and truncated utterances to 15 seconds in baseline emotion recognition training. We use the ground-truth transcriptions in the test set for fair comparisons. We use the checkpoints of backbone models from HuggingFace  [24] . Fig.  2 : Comparisons between two foundation models in transcribing.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Transcription Results",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Does Ser Benefit From Asr Using Foundation Model?",
      "text": "This section compares the SER training using ASR-generated with ground-truth transcriptions (human transcriptions). As both MSP-Improv and MSP-Podcast datasets do not have transcriptions from human experts, we conduct SER training using only ASR output from selected foundation models. The results in Table  3  demonstrate that the foundation model provides transcriptions that lead to consistent performance increases compared to speech-only modeling. Moreover, we can identify that ASR-generated output delivers competitive SER performance compared to ground-truth transcripts.\n\nIt is worth noting that our proposed SER training using ASR output from foundation models considerably outperforms conventional SER systems such as Dialogue RNN  [26]  and CNN-attention  [27] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Does Ser Vary With Different Foundation Models?",
      "text": "We further compare SER performance using ASR output between Whisper-Large V2 and MMS-1B, as illustrated in Figure  2 . The findings indicate that SER performance using ASR output provides consistent benefits to speech-only modeling approaches. However, we have noticed that SER with Whisper-Large V2 transcripts consistently outperforms using the MMS-1B transcripts. To identify the cause that may contribute to this performance difference, we inspect the WER of these two models on IEMOCAP and MELD datasets with ground-truth transcription shown in Tabel 5. The WER indicates that Whisper Large V2 yields better speech recognition than MMS-1B in our experimental datasets. However, we can observe that WER is still fairly large in both datasets, complying with the findings in  [28] . Therefore, we proceeded with the remaining experiments for LLM emotion annotation using Whisper Large V2.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion Annotations",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "How Does Base Prompt Perform Compared To Prior Works?",
      "text": "Table  4  shows the SER training performance leveraging the emotion annotations using each individual LLM. Similar to previous work, Based on the individual performance of emotional annotation shown in Table  4 , we decide to apply the majority votes of emotion annotations from Flan-T5 XXL, LLaMa2-13B, and Falcon-40B as the emotion labels. The results indicate that aggregating majority votes from multi-LLMs enhances the quality of emotion annotation. However, this improvement is only marginal, leading to a 1-2% increase in SER performance. This observation suggests that relying on LLMs alone, even when considering input from multi-LLMs, yields unsatisfactory labels compared to conventional human labeling methods.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Would Adding Limited Involvement Of Human Annotation Benefit Emotion Annotation?",
      "text": "The last column in Table  4  involves the performance of SER adding human feedback (HF) in the annotation process. As MELD does not provide individual annotator labels, we exclude this dataset in this experiment. It is obvious that integrating limited human feedback can lead to substantial improvement in SER training. Our hypothesis is that text modality may often provide ambiguous information in determining the emotion labels, thus LLMs are prone to give erroneous estimations of the expressed emotion given limited modalities. Limited inspections on audio samples with human annotators offer a disambiguation process that increases the label quality.\n\n7.4. How different are emotion annotations using transcriptions between ground-truth and ASR output?\n\nTable  6  reveals the SER training comparisons using emotion labels inferred from ground truth and ASR transcriptions. We report results with datasets that include the ground truth transcriptions. Interestingly, results in Table  6  show that ASR transcriptions, even with fairly large WER, lead to comparable SER performance to ground truth transcriptions. Moreover, LLMs with HF consistently outperform LLMs-only annotation. In future studies, it is worth studying why erroneous ASR output can yield comparable emotion reasoning using clean ground-truth transcriptions.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Augmentation",
      "text": "This section explores the ability to use our proposed automated labeling framework to augment an existing training dataset. We choose the multiple-LLMs agreement and LLMs with human feedback to provide emotion labels from ASR transcriptions, as these two approaches yield higher SER. We select MSP-Podcast and MELD as the augmentation datasets as these two datasets originate from Internet sources. This experiment setup is similar to the previous work in  [15] . The comparison aligns with the prior work  [15]  that augmenting IEMOCAP data with MELD using multi-LLMs labeling improves the performance. However, this finding does not hold when the training data is MSP-Improv. Moreover, augmenting SER training with MSP-Podcast using multi-LLM labeling consistently decreases the SER performance. On the other hand, we discover that augmenting data using LLM labeling with even limited human feedback consistently improves the SER performance, highlighting the importance of human feedback in emotional reasoning.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we explore the use of the foundation model in assisting curation of the SER datasets in transcribing, emotion annotation, and augmentation. Our study focuses on exploring open-source models that require a single V100-equivalent GPU that is widely accessible. Our study demonstrates that foundational models can generate transcriptions to enhance the performance of SER systems that rely solely on speech data. However, WERs are fairly large. Furthermore, we observe that annotating emotions from transcribed speech remains a challenging task, even when combining outputs from multiple LLMs. Lastly, our findings suggest the feasibility of augmenting existing speech emotion datasets by annotating unlabeled speech samples using a two-stage annotation process that includes limited human feedback. In summary, our results highlight the importance of human-in-the-loop for annotating emotion labels from speech signals. Our future work would use multi-modal approaches to assist automatic emotion annotation instead of only LLMs.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Our proposed foundation model-assisted automatic SER framework. The speech is first transcribed to text and is subsequently fed",
      "page": 2
    },
    {
      "caption": "Figure 2: Comparisons between two foundation models in transcribing.",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "University of Southern California, Los Angeles, USA": "ABSTRACT"
        },
        {
          "University of Southern California, Los Angeles, USA": ""
        },
        {
          "University of Southern California, Los Angeles, USA": "Significant advances are being made in speech emotion recognition"
        },
        {
          "University of Southern California, Los Angeles, USA": ""
        },
        {
          "University of Southern California, Los Angeles, USA": "(SER) using deep learning models. Nonetheless,\ntraining SER sys-"
        },
        {
          "University of Southern California, Los Angeles, USA": ""
        },
        {
          "University of Southern California, Los Angeles, USA": "tems remains challenging, requiring both time and costly resources."
        },
        {
          "University of Southern California, Los Angeles, USA": ""
        },
        {
          "University of Southern California, Los Angeles, USA": "Like many other machine learning tasks, acquiring datasets for SER"
        },
        {
          "University of Southern California, Los Angeles, USA": ""
        },
        {
          "University of Southern California, Los Angeles, USA": "requires substantial data annotation efforts,\nincluding transcription"
        },
        {
          "University of Southern California, Los Angeles, USA": ""
        },
        {
          "University of Southern California, Los Angeles, USA": "and labeling. These annotation processes present challenges when"
        },
        {
          "University of Southern California, Los Angeles, USA": ""
        },
        {
          "University of Southern California, Los Angeles, USA": "attempting to scale up conventional SER systems. Recent develop-"
        },
        {
          "University of Southern California, Los Angeles, USA": ""
        },
        {
          "University of Southern California, Los Angeles, USA": "ments in foundational models have had a tremendous impact, giv-"
        },
        {
          "University of Southern California, Los Angeles, USA": ""
        },
        {
          "University of Southern California, Los Angeles, USA": "ing rise to applications such as ChatGPT. These models have en-"
        },
        {
          "University of Southern California, Los Angeles, USA": ""
        },
        {
          "University of Southern California, Los Angeles, USA": "hanced human-computer interactions including bringing unique pos-"
        },
        {
          "University of Southern California, Los Angeles, USA": ""
        },
        {
          "University of Southern California, Los Angeles, USA": "sibilities for streamlining data collection in fields like SER. In this"
        },
        {
          "University of Southern California, Los Angeles, USA": ""
        },
        {
          "University of Southern California, Los Angeles, USA": "research, we explore the use of foundational models to assist\nin au-"
        },
        {
          "University of Southern California, Los Angeles, USA": ""
        },
        {
          "University of Southern California, Los Angeles, USA": "tomating SER from transcription and annotation to augmentation."
        },
        {
          "University of Southern California, Los Angeles, USA": ""
        },
        {
          "University of Southern California, Los Angeles, USA": "Our\nstudy demonstrates\nthat\nthese models can generate transcrip-"
        },
        {
          "University of Southern California, Los Angeles, USA": ""
        },
        {
          "University of Southern California, Los Angeles, USA": "tions to enhance the performance of SER systems that rely solely on"
        },
        {
          "University of Southern California, Los Angeles, USA": ""
        },
        {
          "University of Southern California, Los Angeles, USA": "speech data.\nFurthermore, we note that annotating emotions from"
        },
        {
          "University of Southern California, Los Angeles, USA": ""
        },
        {
          "University of Southern California, Los Angeles, USA": "transcribed speech remains a challenging task. However, combining"
        },
        {
          "University of Southern California, Los Angeles, USA": ""
        },
        {
          "University of Southern California, Los Angeles, USA": "outputs from multiple LLMs enhances the quality of annotations."
        },
        {
          "University of Southern California, Los Angeles, USA": ""
        },
        {
          "University of Southern California, Los Angeles, USA": "Lastly, our findings suggest\nthe feasibility of augmenting existing"
        },
        {
          "University of Southern California, Los Angeles, USA": ""
        },
        {
          "University of Southern California, Los Angeles, USA": "speech emotion datasets by annotating unlabeled speech samples."
        },
        {
          "University of Southern California, Los Angeles, USA": ""
        },
        {
          "University of Southern California, Los Angeles, USA": "Index\nTerms— Speech,\nEmotion\nrecognition,\nFoundation"
        },
        {
          "University of Southern California, Los Angeles, USA": "model, Large Language Model"
        },
        {
          "University of Southern California, Los Angeles, USA": ""
        },
        {
          "University of Southern California, Los Angeles, USA": ""
        },
        {
          "University of Southern California, Los Angeles, USA": "1.\nINTRODUCTION"
        },
        {
          "University of Southern California, Los Angeles, USA": ""
        },
        {
          "University of Southern California, Los Angeles, USA": "Speech emotion recognition (SER) has benefited considerably from"
        },
        {
          "University of Southern California, Los Angeles, USA": "using large-scale pre-trained speech models\n[1, 2, 3, 4], offering"
        },
        {
          "University of Southern California, Los Angeles, USA": "substantial performance improvements over conventional SER sys-"
        },
        {
          "University of Southern California, Los Angeles, USA": "tems that primarily depend on low-level acoustic descriptors (e.g.,"
        },
        {
          "University of Southern California, Los Angeles, USA": "speech prosody and spectral\ninformation). These advances in emo-"
        },
        {
          "University of Southern California, Los Angeles, USA": "tion recognition open up opportunities for widespread applications in"
        },
        {
          "University of Southern California, Los Angeles, USA": "healthcare and virtual assistants, transforming our ways of connect-"
        },
        {
          "University of Southern California, Los Angeles, USA": ""
        },
        {
          "University of Southern California, Los Angeles, USA": "ing, engaging, and interacting with the world. However, success in"
        },
        {
          "University of Southern California, Los Angeles, USA": ""
        },
        {
          "University of Southern California, Los Angeles, USA": "deploying SER models in real-world applications requires the acqui-"
        },
        {
          "University of Southern California, Los Angeles, USA": ""
        },
        {
          "University of Southern California, Los Angeles, USA": "sition of high-quality annotations to speech samples, which is often"
        },
        {
          "University of Southern California, Los Angeles, USA": ""
        },
        {
          "University of Southern California, Los Angeles, USA": "expensive, time-consuming, and privacy-unfriendly."
        },
        {
          "University of Southern California, Los Angeles, USA": ""
        },
        {
          "University of Southern California, Los Angeles, USA": "One typical\nlabeling step in SER datasets involves transcribing"
        },
        {
          "University of Southern California, Los Angeles, USA": ""
        },
        {
          "University of Southern California, Los Angeles, USA": "the speech content.\nFor example,\nIEMOCAP [5], one of\nthe most"
        },
        {
          "University of Southern California, Los Angeles, USA": ""
        },
        {
          "University of Southern California, Los Angeles, USA": "popular SER testbeds, had obtained the professional\ntranscriptions"
        },
        {
          "University of Southern California, Los Angeles, USA": ""
        },
        {
          "University of Southern California, Los Angeles, USA": "of the audio dialogues using a commercial service. Such a process"
        },
        {
          "University of Southern California, Los Angeles, USA": ""
        },
        {
          "University of Southern California, Los Angeles, USA": "often requires training transcribers on transcription guidelines, cre-"
        },
        {
          "University of Southern California, Los Angeles, USA": "ating considerable R&D costs. The advent of Amazon’s Mechanical"
        },
        {
          "University of Southern California, Los Angeles, USA": ""
        },
        {
          "University of Southern California, Los Angeles, USA": "Turk[6] (MTurk) had substantially increased the efficiency of tran-"
        },
        {
          "University of Southern California, Los Angeles, USA": "scribing services by providing the marketplace for human workers to"
        },
        {
          "University of Southern California, Los Angeles, USA": ""
        },
        {
          "University of Southern California, Los Angeles, USA": "perform such tasks for pay. However,\nit still demands many MTurk"
        },
        {
          "University of Southern California, Los Angeles, USA": "hours\nto transcribe the audio conversations,\nleading to significant"
        },
        {
          "University of Southern California, Los Angeles, USA": "costs.\nIn addition, MTurk may not be a viable option when the data"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Attention": "Embedding"
        },
        {
          "Attention": "CNNs\nTransformers"
        },
        {
          "Attention": "Emotion Recognition Modeling"
        },
        {
          "Attention": "Fig. 1: Our proposed foundation model-assisted automatic SER framework. The speech is first\ntranscribed to text and is subsequently fed"
        },
        {
          "Attention": "to LLMs to annotate categorical emotions. Our SER modeling framework involves a text and speech backbone to extract corresponding"
        },
        {
          "Attention": "embeddings, which are then passed through a cross-attention layer to obtain the multimodal representations to predict emotion labels."
        },
        {
          "Attention": "Table 1: Summary of foundation models used in this work."
        },
        {
          "Attention": "the pre-trained speech models, which are then trained with labeled"
        },
        {
          "Attention": "speech samples for speech-related tasks. One recent popular model"
        },
        {
          "Attention": "Foundation Model\nInput\nAnnotation\n# Parameters"
        },
        {
          "Attention": "in this category is the Massively Multilingual Speech (MMS)\n[10]"
        },
        {
          "Attention": "MMS-1B\nSpeech\nTranscription\n1000M\nmodel\nreleased by Meta, which is pre-trained on 491K hours of"
        },
        {
          "Attention": "Whisper Large V2\nSpeech\nTranscription\n1.550M\nspeech.\nIn contrast, Whisper by OpenAI\n[4] adopts a weakly su-"
        },
        {
          "Attention": "pervised learning approach, with objectives to perform tasks such as"
        },
        {
          "Attention": "LLaMa 2-7B\nText\nEmotion\n7B"
        },
        {
          "Attention": "voice activity detection,\nlanguage identification, and speech recog-"
        },
        {
          "Attention": "LLaMa 2-13B\nText\nEmotion\n13B"
        },
        {
          "Attention": "nition. The training of this model is conducted using a dataset com-"
        },
        {
          "Attention": "Falcon-7B\nText\nEmotion\n7B"
        },
        {
          "Attention": "prising 680k hours of labeled speech data."
        },
        {
          "Attention": "Falcon-40B\nText\nEmotion\n40B"
        },
        {
          "Attention": "Flan-T5 XXL\nText\nEmotion\n11B"
        },
        {
          "Attention": "2.2. Large Language Models"
        },
        {
          "Attention": "Large language models like ChatGPT have demonstrated remark-"
        },
        {
          "Attention": "performance.\nIn addition, we instruct the LLMs to choose emotions\nable performance in language reasoning tasks. However, GPT4 or"
        },
        {
          "Attention": "from five categories:\nneutral,\nsad, happy, angry, and other.\nThis\nChatGPT requires the user\nto upload the speech content\nto the re-"
        },
        {
          "Attention": "strategy constrains the LLMs to output more determined labels, and\nmote server for prompting. This creates considerable privacy risks"
        },
        {
          "Attention": "we introduce the option of ”other” to filter out unconfident responses\nin sensitive settings and applications.\nInstead, we decided to ex-"
        },
        {
          "Attention": "to include in SER modeling. In summary, our prompt template is:\nplore foundation models that can operate on a single GPU,\ninclud-"
        },
        {
          "Attention": "What is the emotion of this utterance? ”Everything is not working!”\ning LLaMa 2 families [11], Falcon families [12], and Flan-T5 XXL"
        },
        {
          "Attention": "sad\nOptions: -neutral -sad -angry -happy -other ANSWER:\n[13]. We want\nto highlight\nthat several prior works [14, 15] have"
        },
        {
          "Attention": "investigated the ability of LLMs to annotate ground-truth transcrip-"
        },
        {
          "Attention": "Multiple-LLMs Agreement It is known that relying on the response"
        },
        {
          "Attention": "tions or ASR-generated transcription. However, most of these works"
        },
        {
          "Attention": "from one LLM could yield biased language reasoning [16]. To mit-"
        },
        {
          "Attention": "consider conventional SER modeling architecture (e.g., ResNet-50)."
        },
        {
          "Attention": "igate this concern, we propose ensemble the output\nfrom multiple"
        },
        {
          "Attention": "Moreover,\nthey do not\nincorporate ASR-generated transcription in"
        },
        {
          "Attention": "LLMs, collecting the wisdom from multiple reasoners."
        },
        {
          "Attention": "SER modeling and experiment with a limited set of LLMs."
        },
        {
          "Attention": "LLMs + Human Feedback One critical\nlesson we learned from"
        },
        {
          "Attention": "prior research is that LLMs exhibit\nlimited zero-shot capabilities in\n3. METHOD"
        },
        {
          "Attention": "annotating emotions from speech. Consequently, we contend that"
        },
        {
          "Attention": "3.1. Foundation Model Assisted Annotation"
        },
        {
          "Attention": "human evaluation may remain essential. However,\ninstead of rely-"
        },
        {
          "Attention": "Our automatic annotation framework is presented in Fig 1. Given\ning on multiple human raters for a majority agreement, we propose"
        },
        {
          "Attention": "an unlabeled speech sample, we first propose to obtain the speech\nthat assessing the agreement between the LLM annotations and one"
        },
        {
          "Attention": "content using foundation speech recognition models. This work in-\nhuman feedback is sufficient for quality control."
        },
        {
          "Attention": "vestigates two recent ASR models, Whisper-Large V2 and MMS,"
        },
        {
          "Attention": "that offer\nthe most competitive results. After obtaining the ASR-"
        },
        {
          "Attention": "3.3. Emotion Recognition Modeling"
        },
        {
          "Attention": "generated transcripts, we directly send them to the large language"
        },
        {
          "Attention": "models. Our LLMs include LLaMa 2 families, Falcon families, and"
        },
        {
          "Attention": "The complete model architecture is illustrated in Fig 1. Our SER"
        },
        {
          "Attention": "Flan-T5 XXL. The details about\nthe foundation models used in this"
        },
        {
          "Attention": "includes speech and text backbones to extract the corresponding em-"
        },
        {
          "Attention": "study and their approximate model size can be found in Table 1. The"
        },
        {
          "Attention": "beddings.\nSpecifically, we utilize Whisper-Small\n[4] and MMS-"
        },
        {
          "Attention": "obtained emotion labels and transcripts are used for SER training."
        },
        {
          "Attention": "300M [10] as the speech backbone and Roberta as the text backbone."
        },
        {
          "Attention": "We intend not to experiment with Whisper-Large as the speech back-"
        },
        {
          "Attention": "3.2. A Bag of Tricks in Prompt Engineering"
        },
        {
          "Attention": "bone as it requires prohibitively large GPU capacities for our setting."
        },
        {
          "Attention": "The output of backbone models is subsequently fed into weighted"
        },
        {
          "Attention": "We investigate and compare several tricks in prompt engineering."
        },
        {
          "Attention": "averaging layers\nto combine the hidden outputs\nfrom all encoder"
        },
        {
          "Attention": "Base Prompt Our prompt design is similar to [15], where instructing"
        },
        {
          "Attention": "layers. The weighted output is then passed through a cross-attention"
        },
        {
          "Attention": "the LLMs to annotate the spoken utterance delivers decent zero-shot"
        },
        {
          "Attention": "layer to obtain the multimodal representation for SER."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 2: Summary of dataset statistics used in this work.": "Datasets\nNeutral\nHappy\nSad\nAngry\nTotal",
          "Table 3: SER performances using transcriptions.": "Datasets\nInput\nTranscription\nUAR(%)"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "IEMOCAP\n1,708\n1,636\n1,084\n1,103\n5,531",
          "Table 3: SER performances using transcriptions.": "Speech\n-\n67.45"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "MELD\n6,436\n2,308\n1,002\n1,607\n9045",
          "Table 3: SER performances using transcriptions.": "IEMOCAP\n73.87\nSpeech+Text\nGround-truth"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "MSP-Improv\n3,477\n2,644\n885\n792\n7,798",
          "Table 3: SER performances using transcriptions.": "Speech+Text\nWhisper-Large V2\n71.78"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "MSP-Podcast\n20,986\n12,060\n2,166\n2,712\n37,924",
          "Table 3: SER performances using transcriptions.": ""
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "",
          "Table 3: SER performances using transcriptions.": "Speech\n-\n48.55"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "",
          "Table 3: SER performances using transcriptions.": "MELD\n56.31\nSpeech+Text\nGround-truth"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "",
          "Table 3: SER performances using transcriptions.": "Speech+Text\nWhisper-Large V2\n54.32"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "4. DATASETS",
          "Table 3: SER performances using transcriptions.": ""
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "",
          "Table 3: SER performances using transcriptions.": "Speech\n-\n63.23"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "",
          "Table 3: SER performances using transcriptions.": "MSP-Improv"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "Table 2 displays data statistics for the four datasets included in our",
          "Table 3: SER performances using transcriptions.": "65.44\nSpeech+Text\nWhisper-Large V2"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "work. Due to the existence of imbalanced label distribution within",
          "Table 3: SER performances using transcriptions.": ""
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "",
          "Table 3: SER performances using transcriptions.": "Speech\n-\n60.82"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "the dataset, we decided to keep the four most frequently presented",
          "Table 3: SER performances using transcriptions.": "MSP-Podcast"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "",
          "Table 3: SER performances using transcriptions.": "63.19\nSpeech+Text\nWhisper-Large V2"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "emotions for all\nthe datasets, as recommended in [17, 18, 19, 20].",
          "Table 3: SER performances using transcriptions.": ""
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "We acknowledge that this inclusion criterion trivializes the automatic",
          "Table 3: SER performances using transcriptions.": "Whisper Large V2 and MMS-1B"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "emotion annotation, but\nit ensures\nfair comparisons when having",
          "Table 3: SER performances using transcriptions.": "Whisper Large V2\nMMS-1B"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "multiple datasets with different emotions. The emotion annotation",
          "Table 3: SER performances using transcriptions.": ""
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "",
          "Table 3: SER performances using transcriptions.": "70"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "results reported in our experiments will likely decrease in practice.",
          "Table 3: SER performances using transcriptions.": ""
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "",
          "Table 3: SER performances using transcriptions.": "UAR\n60"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "IEMOCAP [5] contains multi-modal recordings of human interac-",
          "Table 3: SER performances using transcriptions.": ""
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "tions from 10 subjects evenly distributed between males and females.",
          "Table 3: SER performances using transcriptions.": ""
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "",
          "Table 3: SER performances using transcriptions.": "50"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "",
          "Table 3: SER performances using transcriptions.": "IEMOCAP\nMELD\nMSP-Improv\nMSP-Podcast"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "Multimodal EmotionLines Dataset\n(MELD)\n[21] contains more",
          "Table 3: SER performances using transcriptions.": ""
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "than 13000 utterances from the Friends TV series. Each utterance is",
          "Table 3: SER performances using transcriptions.": "Fig. 2: Comparisons between two foundation models in transcribing."
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "labeled with seven emotions, – Anger, Disgust, Sadness, Joy, Neu-",
          "Table 3: SER performances using transcriptions.": ""
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "tral, Surprise, and Fear. We map the Joy to happy emotion and keep",
          "Table 3: SER performances using transcriptions.": "6. TRANSCRIPTION RESULTS"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "Anger, Sadness, and Neutral in the experiments.",
          "Table 3: SER performances using transcriptions.": ""
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "",
          "Table 3: SER performances using transcriptions.": "6.1. Does SER benefit from ASR using Foundation Model?"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "MSP-Improv [22] corpus\nis developed to investigate naturalistic",
          "Table 3: SER performances using transcriptions.": ""
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "emotions elicited from improvised situations. The corpus comprises",
          "Table 3: SER performances using transcriptions.": "This section compares the SER training using ASR-generated with"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "audio and visual data collected from 12 individuals, with an equal",
          "Table 3: SER performances using transcriptions.": "ground-truth transcriptions (human transcriptions). As both MSP-"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "number of subjects from both male and female participants.",
          "Table 3: SER performances using transcriptions.": "Improv and MSP-Podcast datasets do not have transcriptions from"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "",
          "Table 3: SER performances using transcriptions.": "human experts, we conduct SER training using only ASR output"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "MSP-Podcast\n[23]\nis collected from podcast\nrecordings, with 610",
          "Table 3: SER performances using transcriptions.": ""
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "",
          "Table 3: SER performances using transcriptions.": "from selected foundation models.\nThe results\nin Table 3 demon-"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "speakers in the training, 30 in the development, and 50 in the test.",
          "Table 3: SER performances using transcriptions.": ""
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "",
          "Table 3: SER performances using transcriptions.": "strate that\nthe foundation model provides transcriptions that\nlead to"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "",
          "Table 3: SER performances using transcriptions.": "consistent performance increases compared to speech-only model-"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "",
          "Table 3: SER performances using transcriptions.": "ing. Moreover, we can identify that ASR-generated output delivers"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "5. EXPERIMENT DETAILS",
          "Table 3: SER performances using transcriptions.": ""
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "",
          "Table 3: SER performances using transcriptions.": "competitive SER performance compared to ground-truth transcripts."
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "5.1. Foundation Model Assisted Annotation",
          "Table 3: SER performances using transcriptions.": "It\nis worth noting that our proposed SER training using ASR out-"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "",
          "Table 3: SER performances using transcriptions.": "put from foundation models considerably outperforms conventional"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "We apply MMS-300M and Whisper Large V2 to obtain the ASR",
          "Table 3: SER performances using transcriptions.": "SER systems such as Dialogue RNN [26] and CNN-attention [27]."
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "output.\nSince LLMs with more than 10B parameters exceed most",
          "Table 3: SER performances using transcriptions.": ""
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "",
          "Table 3: SER performances using transcriptions.": "6.2. Does SER vary with different Foundation Models?"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "GPU memory capacities, we decided to load LLMs over 10B using",
          "Table 3: SER performances using transcriptions.": ""
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "float 16 instead of float 32.\nIn addition, we load Falcon-40B with",
          "Table 3: SER performances using transcriptions.": ""
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "",
          "Table 3: SER performances using transcriptions.": "We further compare SER performance using ASR output between"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "8-bit. We use a temperature of 0.02 in all prompting experiments, as",
          "Table 3: SER performances using transcriptions.": ""
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "",
          "Table 3: SER performances using transcriptions.": "Whisper-Large V2 and MMS-1B, as\nillustrated in Figure 2.\nThe"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "a lower temperature results in more deterministic output. We use the",
          "Table 3: SER performances using transcriptions.": ""
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "",
          "Table 3: SER performances using transcriptions.": "findings indicate that SER performance using ASR output provides"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "checkpoints of all foundation models from HuggingFace [24].",
          "Table 3: SER performances using transcriptions.": ""
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "",
          "Table 3: SER performances using transcriptions.": "consistent benefits to speech-only modeling approaches. However,"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "",
          "Table 3: SER performances using transcriptions.": "we have noticed that SER with Whisper-Large V2 transcripts con-"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "",
          "Table 3: SER performances using transcriptions.": "sistently outperforms using the MMS-1B transcripts. To identify the"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "5.2. Emotion Recognition Modeling",
          "Table 3: SER performances using transcriptions.": ""
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "",
          "Table 3: SER performances using transcriptions.": "cause that may contribute to this performance difference, we inspect"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "",
          "Table 3: SER performances using transcriptions.": "the WER of\nthese two models on IEMOCAP and MELD datasets"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "We apply a 5-fold and 6-fold evaluation on IEMOCAP and MSP-",
          "Table 3: SER performances using transcriptions.": ""
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "",
          "Table 3: SER performances using transcriptions.": "with ground-truth transcription shown in Tabel 5. The WER indi-"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "Improv datasets\nrespectively, where each session is regarded as a",
          "Table 3: SER performances using transcriptions.": ""
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "",
          "Table 3: SER performances using transcriptions.": "cates that Whisper Large V2 yields better speech recognition than"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "unique test fold.\nIn contrast, we use the standard splits for training,",
          "Table 3: SER performances using transcriptions.": ""
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "",
          "Table 3: SER performances using transcriptions.": "MMS-1B in our experimental datasets. However, we can observe"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "validation, and testing from the MELD and MSP-Podcast datasets.",
          "Table 3: SER performances using transcriptions.": ""
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "",
          "Table 3: SER performances using transcriptions.": "that WER is still\nfairly large in both datasets, complying with the"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "We use the RoBERTa [25] model as\nthe text backbone while we",
          "Table 3: SER performances using transcriptions.": ""
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "",
          "Table 3: SER performances using transcriptions.": "findings in [28]. Therefore, we proceeded with the remaining exper-"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "compare the speech backbones between MMS-300M and Whisper-",
          "Table 3: SER performances using transcriptions.": ""
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "",
          "Table 3: SER performances using transcriptions.": "iments for LLM emotion annotation using Whisper Large V2."
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "Small. We choose MMS-300M along with MMS-1B ASR output",
          "Table 3: SER performances using transcriptions.": ""
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "and Whisper-Small along with Whisper Large V2 ASR output\nin",
          "Table 3: SER performances using transcriptions.": ""
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "",
          "Table 3: SER performances using transcriptions.": "7. EMOTION ANNOTATIONS"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "SER modeling. Specifically, we set the batch size to 32, the learning",
          "Table 3: SER performances using transcriptions.": ""
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "rate to 0.0001, the max training epoch to 30, and truncated utterances",
          "Table 3: SER performances using transcriptions.": ""
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "",
          "Table 3: SER performances using transcriptions.": "7.1. How does base prompt perform compared to prior works?"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "to 15 seconds in baseline emotion recognition training. We use the",
          "Table 3: SER performances using transcriptions.": ""
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "ground-truth transcriptions in the test set for fair comparisons. We",
          "Table 3: SER performances using transcriptions.": "Table 4 shows the SER training performance leveraging the emotion"
        },
        {
          "Table 2: Summary of dataset statistics used in this work.": "use the checkpoints of backbone models from HuggingFace [24].",
          "Table 3: SER performances using transcriptions.": "annotations using each individual LLM. Similar\nto previous work,"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 7: SER performance with augmentation. ↑ indicates an in-",
      "data": [
        {
          "Table 4: SER (UAR) with emotion annotation from LLMs. The transcription is ASR output from Whisper Large V2. HF is human feedback.": "Datasets"
        },
        {
          "Table 4: SER (UAR) with emotion annotation from LLMs. The transcription is ASR output from Whisper Large V2. HF is human feedback.": "IEMOCAP"
        },
        {
          "Table 4: SER (UAR) with emotion annotation from LLMs. The transcription is ASR output from Whisper Large V2. HF is human feedback.": "MELD"
        },
        {
          "Table 4: SER (UAR) with emotion annotation from LLMs. The transcription is ASR output from Whisper Large V2. HF is human feedback.": "MSP-Improv"
        },
        {
          "Table 4: SER (UAR) with emotion annotation from LLMs. The transcription is ASR output from Whisper Large V2. HF is human feedback.": "MSP-Podcast"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 7: SER performance with augmentation. ↑ indicates an in-",
      "data": [
        {
          "Meld": ""
        },
        {
          "Meld": ""
        },
        {
          "Meld": ""
        },
        {
          "Meld": ""
        },
        {
          "Meld": ""
        },
        {
          "Meld": "Datasets"
        },
        {
          "Meld": ""
        },
        {
          "Meld": ""
        },
        {
          "Meld": "IEMOCAP"
        },
        {
          "Meld": ""
        },
        {
          "Meld": ""
        },
        {
          "Meld": "MSP-Improv"
        },
        {
          "Meld": ""
        },
        {
          "Meld": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 7: SER performance with augmentation. ↑ indicates an in-",
      "data": [
        {
          "MSP-Podcast": "Table 5: WER (word error rate) in transcriptions. Processed tran-",
          "51.20": "",
          "47.23": "",
          "48.12": "",
          "43.25": "",
          "48.11": "Table 6: SER (UAR) comparisons with annotations using ground-",
          "52.59": "",
          "53.54": ""
        },
        {
          "MSP-Podcast": "scripts consider only lowercase and remove punctuation.",
          "51.20": "",
          "47.23": "",
          "48.12": "",
          "43.25": "",
          "48.11": "truth and Whisper transcriptions. HF means human feedback.",
          "52.59": "",
          "53.54": ""
        },
        {
          "MSP-Podcast": "",
          "51.20": "Whisper Large V2",
          "47.23": "MMS-1B",
          "48.12": "",
          "43.25": "Datasets",
          "48.11": "Transcription",
          "52.59": "Multi-LLMs",
          "53.54": "LLMs+HF"
        },
        {
          "MSP-Podcast": "Datasets",
          "51.20": "",
          "47.23": "",
          "48.12": "",
          "43.25": "",
          "48.11": "",
          "52.59": "",
          "53.54": ""
        },
        {
          "MSP-Podcast": "",
          "51.20": "Processed",
          "47.23": "Processed",
          "48.12": "Original",
          "43.25": "",
          "48.11": "",
          "52.59": "",
          "53.54": ""
        },
        {
          "MSP-Podcast": "",
          "51.20": "",
          "47.23": "",
          "48.12": "",
          "43.25": "",
          "48.11": "Ground truth",
          "52.59": "50.36",
          "53.54": "59.08"
        },
        {
          "MSP-Podcast": "",
          "51.20": "",
          "47.23": "",
          "48.12": "",
          "43.25": "IEMOCAP",
          "48.11": "",
          "52.59": "",
          "53.54": ""
        },
        {
          "MSP-Podcast": "",
          "51.20": "",
          "47.23": "",
          "48.12": "",
          "43.25": "",
          "48.11": "Whisper Large V2",
          "52.59": "51.60",
          "53.54": "60.19"
        },
        {
          "MSP-Podcast": "IEMOCAP",
          "51.20": "12.21",
          "47.23": "26.76",
          "48.12": "51.46",
          "43.25": "",
          "48.11": "",
          "52.59": "",
          "53.54": ""
        },
        {
          "MSP-Podcast": "MELD",
          "51.20": "37.87",
          "47.23": "55.78",
          "48.12": "71.28",
          "43.25": "",
          "48.11": "Ground truth",
          "52.59": "55.69",
          "53.54": "N.A."
        },
        {
          "MSP-Podcast": "",
          "51.20": "",
          "47.23": "",
          "48.12": "",
          "43.25": "Meld",
          "48.11": "",
          "52.59": "",
          "53.54": ""
        },
        {
          "MSP-Podcast": "",
          "51.20": "",
          "47.23": "",
          "48.12": "",
          "43.25": "",
          "48.11": "Whisper Large V2",
          "52.59": "53.90",
          "53.54": "N.A."
        },
        {
          "MSP-Podcast": "",
          "51.20": "",
          "47.23": "",
          "48.12": "",
          "43.25": "",
          "48.11": "Table 7: SER performance with augmentation.",
          "52.59": "",
          "53.54": "↑ indicates an in-"
        },
        {
          "MSP-Podcast": "we identify that LLMs struggle to provide correct emotion labels for",
          "51.20": "",
          "47.23": "",
          "48.12": "",
          "43.25": "",
          "48.11": "",
          "52.59": "",
          "53.54": ""
        },
        {
          "MSP-Podcast": "",
          "51.20": "",
          "47.23": "",
          "48.12": "",
          "43.25": "",
          "48.11": "crease in SER performance using augmentation.",
          "52.59": "",
          "53.54": ""
        },
        {
          "MSP-Podcast": "SER training,",
          "51.20": "leading to a 10-20% decrease in performance com-",
          "47.23": "",
          "48.12": "",
          "43.25": "",
          "48.11": "",
          "52.59": "",
          "53.54": ""
        },
        {
          "MSP-Podcast": "pared to SER training using ground-truth emotion labels. Moreover,",
          "51.20": "",
          "47.23": "",
          "48.12": "",
          "43.25": "Datasets",
          "48.11": "Augmentation",
          "52.59": "Multi-LLMs",
          "53.54": "LLMs+Human"
        },
        {
          "MSP-Podcast": "larger LLMs provide better emotion labels, with Falcon-40B yield-",
          "51.20": "",
          "47.23": "",
          "48.12": "",
          "43.25": "",
          "48.11": "",
          "52.59": "",
          "53.54": ""
        },
        {
          "MSP-Podcast": "",
          "51.20": "",
          "47.23": "",
          "48.12": "",
          "43.25": "",
          "48.11": "MELD",
          "52.59": "72.60 ↑",
          "53.54": "N.A."
        },
        {
          "MSP-Podcast": "ing the best overall emotion annotations for SER training.",
          "51.20": "",
          "47.23": "",
          "48.12": "",
          "43.25": "IEMOCAP",
          "48.11": "",
          "52.59": "",
          "53.54": ""
        },
        {
          "MSP-Podcast": "",
          "51.20": "",
          "47.23": "",
          "48.12": "",
          "43.25": "",
          "48.11": "MSP-Podcast",
          "52.59": "69.29 ↓",
          "53.54": "72.62 ↑"
        },
        {
          "MSP-Podcast": "",
          "51.20": "",
          "47.23": "",
          "48.12": "",
          "43.25": "",
          "48.11": "MELD",
          "52.59": "65.05 ↓",
          "53.54": "N.A."
        },
        {
          "MSP-Podcast": "",
          "51.20": "",
          "47.23": "",
          "48.12": "",
          "43.25": "MSP-Improv",
          "48.11": "",
          "52.59": "",
          "53.54": ""
        },
        {
          "MSP-Podcast": "",
          "51.20": "",
          "47.23": "",
          "48.12": "",
          "43.25": "",
          "48.11": "MSP-Podcast",
          "52.59": "64.31 ↓",
          "53.54": "66.68 ↑"
        },
        {
          "MSP-Podcast": "7.2. Can majority vote of multi-LLMs improve annotation?",
          "51.20": "",
          "47.23": "",
          "48.12": "",
          "43.25": "",
          "48.11": "",
          "52.59": "",
          "53.54": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "10. REFERENCES": "",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "mentieva, F. Fischer, U. Gasser, G. Groh, S. G¨unnemann,"
        },
        {
          "10. REFERENCES": "[1] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0:",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "E. H¨ullermeier et al., “Chatgpt for good? on opportunities and"
        },
        {
          "10. REFERENCES": "A framework for self-supervised learning of speech represen-",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "challenges of large language models for education,” Learning"
        },
        {
          "10. REFERENCES": "tations,” Advances in neural\ninformation processing systems,",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "and individual differences, vol. 103, p. 102274, 2023."
        },
        {
          "10. REFERENCES": "vol. 33, pp. 12 449–12 460, 2020.",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": ""
        },
        {
          "10. REFERENCES": "",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "[17] T. Feng, H. Hashemi, M. Annavaram, and S. S. Narayanan,"
        },
        {
          "10. REFERENCES": "[2]\nS.\nSchneider,\nA.\nBaevski,\nR.\nCollobert,\nand M. Auli,",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "“Enhancing\nprivacy\nthrough\ndomain\nadaptive\nnoise\ninjec-"
        },
        {
          "10. REFERENCES": "“wav2vec: Unsupervised Pre-Training for Speech Recogni-",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "tion for speech emotion recognition,” in ICASSP 2022-2022."
        },
        {
          "10. REFERENCES": "tion,” in Proc. Interspeech 2019, 2019, pp. 3465–3469.",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "IEEE, 2022, pp. 7702–7706."
        },
        {
          "10. REFERENCES": "[3]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen,\nJ. Li,",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": ""
        },
        {
          "10. REFERENCES": "",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "[18] T. Feng and S. Narayanan, “Privacy and utility preserving data"
        },
        {
          "10. REFERENCES": "N. Kanda, T. Yoshioka, X. Xiao et al., “Wavlm: Large-scale",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": ""
        },
        {
          "10. REFERENCES": "",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "transformation for speech emotion recognition,” in 2021 9th"
        },
        {
          "10. REFERENCES": "self-supervised pre-training for full stack speech processing,”",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": ""
        },
        {
          "10. REFERENCES": "",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "International Conference on Affective Computing and Intelli-"
        },
        {
          "10. REFERENCES": "IEEE Journal of Selected Topics in Signal Processing, vol. 16,",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": ""
        },
        {
          "10. REFERENCES": "",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "gent Interaction (ACII).\nIEEE, 2021, pp. 1–7."
        },
        {
          "10. REFERENCES": "no. 6, pp. 1505–1518, 2022.",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": ""
        },
        {
          "10. REFERENCES": "",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "[19] L.-W. Chen and A. Rudnicky, “Exploring wav2vec 2.0 fine-"
        },
        {
          "10. REFERENCES": "[4] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": ""
        },
        {
          "10. REFERENCES": "",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "arXiv\ntuning\nfor\nimproved\nspeech\nemotion\nrecognition,”"
        },
        {
          "10. REFERENCES": "I. Sutskever, “Robust speech recognition via large-scale weak",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": ""
        },
        {
          "10. REFERENCES": "",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "preprint arXiv:2110.06309, 2021."
        },
        {
          "10. REFERENCES": "supervision,” arXiv preprint arXiv:2212.04356, 2022.",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": ""
        },
        {
          "10. REFERENCES": "",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "[20] L. Pepino, P. Riera, and L. Ferrer, “Emotion Recognition from"
        },
        {
          "10. REFERENCES": "[5] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower,",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": ""
        },
        {
          "10. REFERENCES": "",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "Speech Using wav2vec 2.0 Embeddings,” in Proc. Interspeech"
        },
        {
          "10. REFERENCES": "S. Kim,\nJ. N. Chang, S. Lee, and S. S. Narayanan, “IEMO-",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": ""
        },
        {
          "10. REFERENCES": "",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "2021, 2021, pp. 3400–3404."
        },
        {
          "10. REFERENCES": "CAP: Interactive emotional dyadic motion capture database,”",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": ""
        },
        {
          "10. REFERENCES": "",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "[21]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,"
        },
        {
          "10. REFERENCES": "Language resources and evaluation, vol. 42, no. 4, pp. 335–",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": ""
        },
        {
          "10. REFERENCES": "",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "and R. Mihalcea,\n“Meld: A multimodal multi-party dataset"
        },
        {
          "10. REFERENCES": "359, 2008.",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": ""
        },
        {
          "10. REFERENCES": "",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "arXiv\npreprint\nfor\nemotion\nrecognition\nin\nconversations,”"
        },
        {
          "10. REFERENCES": "[6] M. Marge, S. Banerjee, and A. I. Rudnicky, “Using the ama-",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": ""
        },
        {
          "10. REFERENCES": "",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "arXiv:1810.02508, 2018."
        },
        {
          "10. REFERENCES": "zon mechanical\nturk for transcription of spoken language,” in",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": ""
        },
        {
          "10. REFERENCES": "2010 IEEE International Conference on Acoustics, Speech and",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "[22] C. Busso, S. Parthasarathy, A. Burmania, M. AbdelWahab,"
        },
        {
          "10. REFERENCES": "",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "N. Sadoughi, and E. M. Provost, “Msp-improv: An acted cor-"
        },
        {
          "10. REFERENCES": "Signal Processing.\nIEEE, 2010, pp. 5270–5273.",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": ""
        },
        {
          "10. REFERENCES": "",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "pus of dyadic interactions to study emotion perception,” IEEE"
        },
        {
          "10. REFERENCES": "[7] T. Feng, R. Hebbar, N. Mehlman, X. Shi, A. Kommineni, and",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": ""
        },
        {
          "10. REFERENCES": "",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "Trans. on Affective Computing, vol. 8, no. 1, pp. 67–80, 2016."
        },
        {
          "10. REFERENCES": "S. Narayanan,\n“A review of\nspeech-centric trustworthy ma-",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": ""
        },
        {
          "10. REFERENCES": "chine learning: Privacy, safety, and fairness,” APSIPA Trans.",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "[23] R. Lotfian and C. Busso,\n“Building naturalistic emotionally"
        },
        {
          "10. REFERENCES": "on Signal and Information Processing, vol. 12, no. 3, 2023.",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "balanced speech corpus by retrieving emotional speech from"
        },
        {
          "10. REFERENCES": "",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "existing podcast\nrecordings,” IEEE Transactions on Affective"
        },
        {
          "10. REFERENCES": "[8] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora,",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": ""
        },
        {
          "10. REFERENCES": "",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "Computing, vol. 10, no. 4, pp. 471–483, 2017."
        },
        {
          "10. REFERENCES": "S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": ""
        },
        {
          "10. REFERENCES": "et al., “On the opportunities and risks of foundation models,”",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "[24] T. Wolf, L. Debut, V. Sanh,\nJ. Chaumond, C. Delangue,"
        },
        {
          "10. REFERENCES": "arXiv preprint arXiv:2108.07258, 2021.",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davi-"
        },
        {
          "10. REFERENCES": "",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "son, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu,"
        },
        {
          "10. REFERENCES": "[9] OpenAI, “Gpt-4 technical report,” ArXiv, vol. abs/2303.08774,",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": ""
        },
        {
          "10. REFERENCES": "",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "T. L. Scao, S. Gugger, M. Drame, Q. Lhoest, and A. M. Rush,"
        },
        {
          "10. REFERENCES": "2023.\n[Online]. Available:\nhttps://api.semanticscholar.org/",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": ""
        },
        {
          "10. REFERENCES": "",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "“Transformers: State-of-the-art natural\nlanguage processing,”"
        },
        {
          "10. REFERENCES": "CorpusID:257532815",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": ""
        },
        {
          "10. REFERENCES": "",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "in Proceedings of the 2020 Conf. on Empirical Methods in Nat-"
        },
        {
          "10. REFERENCES": "[10] V. Pratap, A. Tjandra, B. Shi, P. Tomasello, A. Babu, S. Kundu,",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": ""
        },
        {
          "10. REFERENCES": "",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "ural Language Processing, Oct. 2020, pp. 38–45."
        },
        {
          "10. REFERENCES": "A. Elkahky, Z. Ni, A. Vyas, M. Fazel-Zarandi et al., “Scal-",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": ""
        },
        {
          "10. REFERENCES": "",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "[25] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy,"
        },
        {
          "10. REFERENCES": "ing speech technology to 1,000+ languages,” arXiv preprint",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": ""
        },
        {
          "10. REFERENCES": "",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "M. Lewis, L. Zettlemoyer, and V. Stoyanov, “Roberta: A ro-"
        },
        {
          "10. REFERENCES": "arXiv:2305.13516, 2023.",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": ""
        },
        {
          "10. REFERENCES": "",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "arXiv preprint\nbustly optimized bert pretraining approach,”"
        },
        {
          "10. REFERENCES": "[11] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi,",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": ""
        },
        {
          "10. REFERENCES": "",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "arXiv:1907.11692, 2019."
        },
        {
          "10. REFERENCES": "Y\n. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": ""
        },
        {
          "10. REFERENCES": "",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "[26] N. Majumder, S. Poria, D. Hazarika, R. Mihalcea, A. Gelbukh,"
        },
        {
          "10. REFERENCES": "et al., “Llama 2: Open foundation and fine-tuned chat mod-",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": ""
        },
        {
          "10. REFERENCES": "",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "and E. Cambria, “Dialoguernn: An attentive rnn for emotion"
        },
        {
          "10. REFERENCES": "els,” arXiv preprint arXiv:2307.09288, 2023.",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": ""
        },
        {
          "10. REFERENCES": "",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "detection in conversations,” in Proceedings of the AAAI confer-"
        },
        {
          "10. REFERENCES": "[12]\n“Falcon llm,” https://falconllm.tii.ae/falcon.html.",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": ""
        },
        {
          "10. REFERENCES": "",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "ence on artificial intelligence, vol. 33, no. 01, 2019, pp. 6818–"
        },
        {
          "10. REFERENCES": "[13] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fe-",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "6825."
        },
        {
          "10. REFERENCES": "dus, E. Li, X. Wang, M. Dehghani, S. Brahma et al., “Scal-",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": ""
        },
        {
          "10. REFERENCES": "",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "[27] Z.\nPeng, Y.\nLu,\nS.\nPan,\nand Y.\nLiu,\n“Efficient\nspeech"
        },
        {
          "10. REFERENCES": "arXiv\npreprint\ning\ninstruction-finetuned\nlanguage models,”",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": ""
        },
        {
          "10. REFERENCES": "",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "emotion recognition using multi-scale cnn and attention,” in"
        },
        {
          "10. REFERENCES": "arXiv:2210.11416, 2022.",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": ""
        },
        {
          "10. REFERENCES": "",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "ICASSP 2021-2021 IEEE International Conference on Acous-"
        },
        {
          "10. REFERENCES": "[14] T. Gong, J. Belanich, K. Somandepalli, A. Nagrani, B. Eoff,",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "tics, Speech and Signal Processing (ICASSP).\nIEEE, 2021,"
        },
        {
          "10. REFERENCES": "and B.\nJou,\n“LanSER: Language-Model Supported Speech",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "pp. 3020–3024."
        },
        {
          "10. REFERENCES": "Emotion Recognition,” in Proc.\nINTERSPEECH 2023, 2023,",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": ""
        },
        {
          "10. REFERENCES": "",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "[28] Y. Li, Z. Zhao, O. Klejch, P. Bell,\nand C. Lai,\n“Asr\nand"
        },
        {
          "10. REFERENCES": "pp. 2408–2412.",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": ""
        },
        {
          "10. REFERENCES": "",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "emotional\nspeech: A word-level\ninvestigation of\nthe mutual"
        },
        {
          "10. REFERENCES": "[15]\nS. Latif, M. Usama, M. I. Malik, and B. W. Schuller, “Can large",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "arXiv preprint"
        },
        {
          "10. REFERENCES": "",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "impact of\nspeech and emotion recognition,”"
        },
        {
          "10. REFERENCES": "language models\naid in annotating speech emotional data?",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": ""
        },
        {
          "10. REFERENCES": "",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": "arXiv:2305.16065, 2023."
        },
        {
          "10. REFERENCES": "uncovering new frontiers,” arXiv preprint arXiv:2307.06090,",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": ""
        },
        {
          "10. REFERENCES": "2023.",
          "[16] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. De-": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "2",
      "title": "wav2vec: Unsupervised Pre-Training for Speech Recognition",
      "authors": [
        "S Schneider",
        "A Baevski",
        "R Collobert",
        "M Auli"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "3",
      "title": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2022",
      "venue": "Robust speech recognition via large-scale weak supervision",
      "arxiv": "arXiv:2212.04356"
    },
    {
      "citation_id": "5",
      "title": "IEMO-CAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "6",
      "title": "Using the amazon mechanical turk for transcription of spoken language",
      "authors": [
        "M Marge",
        "S Banerjee",
        "A Rudnicky"
      ],
      "year": "2010",
      "venue": "2010 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "7",
      "title": "A review of speech-centric trustworthy machine learning: Privacy, safety, and fairness",
      "authors": [
        "T Feng",
        "R Hebbar",
        "N Mehlman",
        "X Shi",
        "A Kommineni",
        "S Narayanan"
      ],
      "year": "2023",
      "venue": "APSIPA Trans. on Signal and Information Processing"
    },
    {
      "citation_id": "8",
      "title": "On the opportunities and risks of foundation models",
      "authors": [
        "R Bommasani",
        "D Hudson",
        "E Adeli",
        "R Altman",
        "S Arora",
        "S Arx",
        "M Bernstein",
        "J Bohg",
        "A Bosselut",
        "E Brunskill"
      ],
      "year": "2021",
      "venue": "On the opportunities and risks of foundation models",
      "arxiv": "arXiv:2108.07258"
    },
    {
      "citation_id": "9",
      "title": "Gpt-4 technical report",
      "authors": [
        "Openai"
      ],
      "year": "2023",
      "venue": "Gpt-4 technical report"
    },
    {
      "citation_id": "10",
      "title": "Scaling speech technology to 1,000+ languages",
      "authors": [
        "V Pratap",
        "A Tjandra",
        "B Shi",
        "P Tomasello",
        "A Babu",
        "S Kundu",
        "A Elkahky",
        "Z Ni",
        "A Vyas",
        "M Fazel-Zarandi"
      ],
      "year": "2023",
      "venue": "Scaling speech technology to 1,000+ languages",
      "arxiv": "arXiv:2305.13516"
    },
    {
      "citation_id": "11",
      "title": "Llama 2: Open foundation and fine-tuned chat models",
      "authors": [
        "H Touvron",
        "L Martin",
        "K Stone",
        "P Albert",
        "A Almahairi",
        "Y Babaei",
        "N Bashlykov",
        "S Batra",
        "P Bhargava",
        "S Bhosale"
      ],
      "year": "2023",
      "venue": "Llama 2: Open foundation and fine-tuned chat models",
      "arxiv": "arXiv:2307.09288"
    },
    {
      "citation_id": "12",
      "title": "Falcon llm",
      "venue": "Falcon llm"
    },
    {
      "citation_id": "13",
      "title": "Scaling instruction-finetuned language models",
      "authors": [
        "H Chung",
        "L Hou",
        "S Longpre",
        "B Zoph",
        "Y Tay",
        "W Fedus",
        "E Li",
        "X Wang",
        "M Dehghani",
        "S Brahma"
      ],
      "year": "2022",
      "venue": "Scaling instruction-finetuned language models",
      "arxiv": "arXiv:2210.11416"
    },
    {
      "citation_id": "14",
      "title": "LanSER: Language-Model Supported Speech Emotion Recognition",
      "authors": [
        "T Gong",
        "J Belanich",
        "K Somandepalli",
        "A Nagrani",
        "B Eoff",
        "B Jou"
      ],
      "year": "2023",
      "venue": "Proc. INTERSPEECH 2023"
    },
    {
      "citation_id": "15",
      "title": "Can large language models aid in annotating speech emotional data? uncovering new frontiers",
      "authors": [
        "S Latif",
        "M Usama",
        "M Malik",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "Can large language models aid in annotating speech emotional data? uncovering new frontiers",
      "arxiv": "arXiv:2307.06090"
    },
    {
      "citation_id": "16",
      "title": "Chatgpt for good? on opportunities and challenges of large language models for education",
      "authors": [
        "E Kasneci",
        "K Seßler",
        "S Küchemann",
        "M Bannert",
        "D Dementieva",
        "F Fischer",
        "U Gasser",
        "G Groh",
        "S Günnemann",
        "E Hüllermeier"
      ],
      "year": "2023",
      "venue": "Learning and individual differences"
    },
    {
      "citation_id": "17",
      "title": "Enhancing privacy through domain adaptive noise injection for speech emotion recognition",
      "authors": [
        "T Feng",
        "H Hashemi",
        "M Annavaram",
        "S Narayanan"
      ],
      "year": "2022",
      "venue": "Enhancing privacy through domain adaptive noise injection for speech emotion recognition"
    },
    {
      "citation_id": "18",
      "title": "Privacy and utility preserving data transformation for speech emotion recognition",
      "authors": [
        "T Feng",
        "S Narayanan"
      ],
      "year": "2021",
      "venue": "2021 9th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "19",
      "title": "Exploring wav2vec 2.0 finetuning for improved speech emotion recognition",
      "authors": [
        "L.-W Chen",
        "A Rudnicky"
      ],
      "year": "2021",
      "venue": "Exploring wav2vec 2.0 finetuning for improved speech emotion recognition",
      "arxiv": "arXiv:2110.06309"
    },
    {
      "citation_id": "20",
      "title": "Emotion Recognition from Speech Using wav2vec 2.0 Embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "21",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "22",
      "title": "Msp-improv: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "C Busso",
        "S Parthasarathy",
        "A Burmania",
        "M Abdelwahab",
        "N Sadoughi",
        "E Provost"
      ],
      "year": "2016",
      "venue": "IEEE Trans. on Affective Computing"
    },
    {
      "citation_id": "23",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "24",
      "title": "Transformers: State-of-the-art natural language processing",
      "authors": [
        "T Wolf",
        "L Debut",
        "V Sanh",
        "J Chaumond",
        "C Delangue",
        "A Moi",
        "P Cistac",
        "T Rault",
        "R Louf",
        "M Funtowicz",
        "J Davison",
        "S Shleifer",
        "P Von Platen",
        "C Ma",
        "Y Jernite",
        "J Plu",
        "C Xu",
        "T Scao",
        "S Gugger",
        "M Drame",
        "Q Lhoest",
        "A Rush"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conf. on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "25",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "26",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "27",
      "title": "Efficient speech emotion recognition using multi-scale cnn and attention",
      "authors": [
        "Z Peng",
        "Y Lu",
        "S Pan",
        "Y Liu"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "28",
      "title": "Asr and emotional speech: A word-level investigation of the mutual impact of speech and emotion recognition",
      "authors": [
        "Y Li",
        "Z Zhao",
        "O Klejch",
        "P Bell",
        "C Lai"
      ],
      "year": "2023",
      "venue": "Asr and emotional speech: A word-level investigation of the mutual impact of speech and emotion recognition",
      "arxiv": "arXiv:2305.16065"
    }
  ]
}