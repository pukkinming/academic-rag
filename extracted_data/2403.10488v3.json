{
  "paper_id": "2403.10488v3",
  "title": "Joint Multimodal Transformer For Emotion Recognition In The Wild",
  "published": "2024-03-15T17:23:38Z",
  "authors": [
    "Paul Waligora",
    "Haseeb Aslam",
    "Osama Zeeshan",
    "Soufiane Belharbi",
    "Alessandro Lameiras Koerich",
    "Marco Pedersoli",
    "Simon Bacon",
    "Eric Granger"
  ],
  "keywords": [
    "Affective Computing",
    "Multi-Modal Fusion",
    "Audio-Visual Fusion",
    "Transformers",
    "Cross-Attention",
    "Valence-Arousal Estimation"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal emotion recognition (MMER) systems typically outperform unimodal systems by leveraging the inter-and intra-modal relationships between, e.g., visual, textual, physiological, and auditory modalities. This paper proposes an MMER method that relies on a joint multimodal transformer (JMT) for fusion with key-based cross-attention. This framework can exploit the complementary nature of diverse modalities to improve predictive accuracy. Separate backbones capture intra-modal spatiotemporal dependencies within each modality over video sequences. Subsequently, our JMT fusion architecture integrates the individual modality embeddings, allowing the model to effectively capture inter-and intra-modal relationships. Extensive experiments 1 on two challenging expression recognition tasks -(1) dimensional emotion recognition on the Affwild2 dataset (with face and voice) and (2) pain estimation on the Biovid dataset (with face and biosensors) -indicate that our JMT fusion can provide a cost-effective solution for MMER. Empirical results show that MMER systems with our proposed fusion allow us to outperform relevant baseline and state-of-the-art methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Human-computer interaction is applied in a wide range of real-world scenarios, e.g., health care, the Internet of Things (IoT), and autonomous driving. Researchers have classified human emotions in different ways, most notably according to discrete categories, ordinal intensity levels, and the valence/arousal circumplex  [1] . With the recent advancements in deep learning and sensor technologies, research in affective computing has evolved from lab-controlled to real-world (in the wild) scenarios. In the latter, human emotions are usually expressed over a broader spectrum beyond the six basic categorical expressions -anger, disgust, fear, happy, sad, and surprise  [9] . Therefore, there is much interest in analyzing 1 Code: https://github.com/PoloWlg/Joint-Multimodal-Transformer-6th-ABAW. and modeling complex and subtle expressions of emotions in real-world scenarios. For instance, the spectrum of emotions can be formulated as dimensional emotional recognition (ER), where complex human emotions are represented along arousal (intensity) and valence (positiveness) axes.\n\nMultimodal fusion has been widely explored for the problem of video-based ER in the literature  [50, 28] . For instance, audio and visual modalities may provide complementary and redundant information over a video sequence. These relationships must be captured to model the intricacies of human emotions effectively. Furthermore, effectively capturing both the intra-modal temporal dependencies within the audio and visual modalities and the inter-modal association across the audio and visual modalities is crucial in developing an effective AER system  [47] .\n\nSeveral methods have been proposed for video-based ER and recurrent networks have been employed to capture the intra-modal temporal dependencies from video sequences  [50, 28, 5] . Recently, attention-based methods have been introduced to extract features that are the most relevant to downstream tasks. Cross-attention-based methods have also been  [48]  employed to capture the inter-modal association between the audio, visual, and other modalities. Lu et al.  [35]  proposed ViLBERT, the seminal work in multimodal co-attention. Since then, many transformer-based cross-attention methods have been proposed  [43, 57] . These methods, however, cannot effectively capture the intra-modal temporal dynamics. Further, they specialize in capturing the complementary information among the modalities but do not include a mechanism to explicitly capture the redundant information.\n\nThe proposed method introduces a third branch with the joint representation of the multiple modalities, as shown in Figure  1 . By incorporating a joint representation branch, the model can access additional contextual information that may not be fully captured by cross-attention alone. Such a joint representation branch can help improve the model's understanding of complex relationships between the input sequences. Further, the proposed method becomes more robust to noise or irrelevant information present in individual sequences, which helps mitigate the sensitivity of cross-attention to noisy inputs and improves the system's overall performance.\n\nOur main contributions are summarized as follows.\n\n(1) This paper proposes a joint multimodal transformer (JMT) fusion architecture that leverages joint modality representations. It captures inter-and intra-modal information in videos using key-based cross-attention, and exploits the redundant and complementary associations among modalities. (2) An extensive set of experiments on two challenging emotion recognition datasets (pain estimation on BioVid and dimension valence-arousal assessment on Affwild2) indicate that our proposed JMT fusion architecture can outperform relevant baseline and state-of-the-art methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work In Emotion Recognition",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multimodal Methods",
      "text": "MMER refers to integrating multiple sources of information (modalities) to improve the accuracy and robustness of automated emotion recognition systems at the expense of complexity. These modalities typically include visual, audio, textual, and physiological. The seminal work in multimodal deep learning was proposed by Ngiam et al.  [39] , where the features from the audio and visual modalities were extracted separately, and then autoencoders and Restricted Boltzmann Machines were used to feature fusion. Tzirakis et al.  [55]  proposed one of the early approaches for A-V fusion for dimensional emotion recognition, in which the visual features were extracted using a ResNet50 and the audio features were obtained using a 1D convolutional neural network (CNN). The modality-specific features were concatenated and fed to a recurrent net for simultaneous temporal modeling and modality fusion. An empirical study was presented by Juan et al.  [42] , where the authors studied the impact of fine-tuning multiple layers in a pretrained CNN for the visual modality.\n\nA two-stream autoencoder with a long short-term memory (LSTM) network was proposed by Nguyen et al.  [40]  to jointly learn and compact representative features from the visual and audio modalities. A knowledge distillation-based approach was investigated by Schonevald et al.  [50]  for visual modality. For the audio modality, spectrograms were obtained and fed to a CNN model, and the two modalities were fused using a recurrent net. A novel self-distillation scheme was put forward by Deng et al.  [6]  to overcome the problem of noisy labels in a multitasking setting. A two-stream aural visual (TSAV) network was proposed by Kuhnke et al.  [28] , in which the audio features were extracted using a ResNet18, and the visual features were extracted using a 3D-CNN. The obtained embeddings were fed to a specially designed TSAV network for information fusion.\n\nPain estimation is one of the primary problems in affective computing. Researchers have proposed many multimodal datasets for the pain estimation task. The facial activity descriptors method for pain estimation was introduced by Werner et al.  [59] . Dragomir et al.  [7]  propose a subjectindependent method from facial images with a residual learning technique. A Sparse LSTM-based method was proposed by Zhi et al.  [65]  to solve the problem of vanishing gradients in temporal learning. Morabit et al.  [38]  proposed a data-efficient image transformer. To process multiscale electrodermal activity signals, a SE-Net-based network was proposed by Lu et al.  [36] . Multimodal solutions to fuse the physiological and visual modalities were proposed by Werner et al.  [59] , Kachele et al.  [13] , and Zhu et al.  [65] . Physiological signals are more discriminative for pain classification than the visual modality.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Attention-Based And Transformer Methods",
      "text": "Since its inception, attention models have shown extraordinary performance in many applications. These models have been extensively investigated for capturing the inter and intramodal associations between the audio and visual modalities for tasks like action localization  [32] , A-V event localization  [8] , and multimodal emotion recognition  [43] . An attentionbased fusion mechanism was proposed by Zhang et al.  [64] , 3D-CNNs and 2D-CNNs were used to extract multi-features in the visual modality, and for the audio modality, a 2D-CNN was used to learn representation from spectrograms. Specialized scoring functions were used to re-weight the audio and visual features.\n\nRecently, cross-modal attention has shown promising results because of its ability to model inter-modal relationships. Srinivas et al.  [43]  explored a transformer network with encoder layers, where cross-modal attention is used to fuse audio and visual features for continuous arousal/valence prediction in the wild. Tzirakis et al.  [54]  explored the idea of cross-attention in conjunction with self-attention. The authors proposed a transformer-based fusion architecture. Although the methods mentioned above have used cross-modal attention with transformers, they do not have any explicit mechanism to capture semantic relevance between the A-V features, particularly the intra-modal correlations. Zhang et al.  [63]  proposed a method for A-V fusion using leader-follower attentive fusion for continuous arousal/valence prediction. Attention weights are combined with the encoded visual and audio features. Cross attention presented in Praveen et al.  [48]  has shown a substantial increase in performance by using cross-correlation across the individual features. In contrast, our proposed method uses key-based cross-attention in multimodal transformers and explores the idea of feeding the joint A-V feature vector. By feeding the joint A-V feature representation, the proposed method effectively captures the inter-and intra-modal relationships simultaneously by interacting across itself and the other modalities.\n\nHuang et al.  [11]  investigated the idea of multi-head attention in transformer-based fusion architecture, which was further combined with LSTM to capture the high-level representations. Tran et al.  [53]  proposed a cross-modal transformer architecture that consisted of a multimodal cross-modal attention block, where the Queries were generated from one modality and the key values were generated from the other modality. Le et al.  [31]  put forward an end-to-end transformerbased fusion mechanism for multilabel multimodal emotion classification; the model consisted of three parts: i) three backbone networks for visual, audio, and textual feature extractor, ii) a transformer network for information fusion, and iii) classification network. Zhou et al.  [66]  proposed a transformerbased fusion scheme along with the temporal convolutional network (TCN); the audio and visual features were extracted using pretrained backbones followed by a TCN, the output of TCN was concatenated and fed to a transformer encoder block. A multilayer perceptron (MLP) was then used for the final prediction.\n\nAll the aforementioned transformer-based fusion architectures primarily focus on intermodality correlation. In contrast, in addition to modeling the intermodality relationships to capture the complementarity between modalities, the proposed method explicitly feeds the joint (combined) features to the multimodal transformer to introduce redundancy. By incorporating this third joint representation branch, the proposed model can access enhanced contextual information that crossattention might only partially capture. Doing this improves the model's understanding of complex relationships between the input sequences. Further, the proposed method becomes more robust to noise or irrelevant information present in individual sequences. This third joint representation allows the model to dynamically focus on this newly introduced information in sequences where both modalities are simultaneously noisy. This helps mitigate the sensitivity of cross-attention to noisy inputs and improves the system's overall performance.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Proposed Approach",
      "text": "The proposed method is a hierarchical fusion mechanism, where the intra-modality features are combined using transformer-based self-attention, and cross-modality features are fused using transformer-based cross-attention. Further, we feed a third joint representation to the joint transformer module (JTM) to enhance robustness. The K (key matrix), V (value matrix), and Q (query matrix) vectors are shared among the six transformer blocks. In the end, the output of these six blocks is again fed to a transformer self-attention block to weigh the most relevant representations dynamically. The final prediction is made using fully connected (FC) layers.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Modality Specific Feature Extraction",
      "text": "In the first step, modality-specific features are extracted using backbones. The proposed method allows combining multiple backbones for each modality to improve system robustness. The extracted feature vectors from each backbone are fed to a transformer self-attention block. The combined feature vector represents the particular modality. For example, to capture information about a person's emotional state, we can use an R(2+1)D CNN pretrained on the Kinetics-400 dataset  [29]  to extract visual features. For the audio modality, we could extract features using a ResNet18  [10]  CNN with a GRU  [4] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multi-Transformer Attention Fusion",
      "text": "We define f A as the deep features extracted from backbone A, and f B as the deep features extracted from the backbone B in Figure  2 . Given f B and f A , the joint feature representation is obtained by concatenating f B and f A feature vectors:\n\nwhere [•; •] denotes a concatenation operation. The concatenated features are then fed to an FC layer for dimensionality The output of the cross-attended features is concatenated, and an FC layer is used for valence/arousal prediction.\n\nreduction of the joint feature representation to yield f J . We now have three key sources of information: f B , f A and f J , which have the same dimensionality.\n\nEach representation is then fed to a specific encoder. Our model is composed of three different encoders, one for each type of feature f B , f A , and f J . Each encoder consists of a multi-head-self-attention (Eq. 2) followed by a fully connected feed-forward network. Residual connection and layer normalization are performed around both of these layers. The key is used to associate a sequence to a key value, the value matrix holds information that is ultimately used to compute the output of the attention mechanism, and the query matrix represents a set of vectors used to query the key-value pairs. The K, V , Q matrix are calculated this way:\n\nX corresponds to one of the sources of information and W K , W Q , W V are the weights of the key, query, value matrices respectively. The output values of the self-attention layers are given by:\n\nWith self-attention layers, each encoder focuses independently on important cues related to its respective source of information. Afterward, each encoder embedding is combined by utilizing six cross-modal attention layers where the query matrix Q is shared with the key K and value V matrix of the other source of information. Sharing this matrix between each source of information helps the model add redundancy and complement the visual and audio modalities, thus improving its performance. At the output of each of the six cross-attention modules, the feature vector of dimension 512 is output. These six feature vectors are then stacked to form a sequence, which is then fed to a transformer self-attention block. This block dynamically selects and weighs these feature vectors. The final attended features are fed to an FC layer for final prediction.\n\nThe model aims to maximize the Concordance Correlation Coefficient (CCC)  [30] , which is common in dimensional emotion recognition. To achieve this, we minimize the following loss:\n\nwhere ρ 2 xy is the covariance between the predictions and the ground truth, ρ 2\n\nx and ρ 2 y the variance of the prediction and the ground truth, µ x and µ y the mean of prediction and the ground truth.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Methodology",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Datasets",
      "text": "Affwild2 was put forward by  [23] . It is one of the largest and most comprehensive datasets for affective computing focused in-the-wild scenarios. The dataset is also a part of the Affective Behavior Analysis in the Wild (ABAW) challenge  [2, 22, 21, 17, 16, 3, 15, 27, 26, 20, 18, 25, 24, 19, 61] . The dataset comprises 564 videos that were collected from YouTube. These videos are labeled for three main affective computing tasks: i) categorical expression recognition, ii) continuous valence arousal prediction, and iii) action unit detection. The dataset comes with a train, validation, and test split with 341, 71, and 152 videos, respectively. The continuous valence/arousal annotation set is used to validate the proposed method.\n\nBiovid Heat Pain Database: The BioVid Heat Pain Database  [60]  comprises 87 subjects where heat pain was induced experimentally at the right arm with four different intensities. The data comprises video and depth map video from a Kinect camera, galvanic skin response (EDA), electromyograph (EMG) on trapezius muscle, and electrocardiogram (ECG). The Biovid dataset has various partitions from Part A through E. These partitions differ in the modalities, annotations, and tasks. We use Part A of the dataset and utilize the video and raw EDA data to validate our proposed method.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Implementation Details",
      "text": "Affwild2: In the visual modality, cropped and aligned facial images provided with the dataset are used  [27] . Black frames (zero pixels) replace missing frames in the visual modality. These images are then fed to a 3D network that takes the input size of 224×224. A clip length of 8 is used, which makes up a sub-sequence of 64 frames. Each sub-sequence contains eight clips. A dropout with a value of 0.8 was used in the linear layers for network regularization. The network was optimized using the stochastic gradient descent (SGD) iterative method with an initial learning rate (LR) of 10 -3 . The batch size used for the visual modality was 8. For further generalization, random cropping and random horizontal flips were added as data augmentation. The maximum number of epochs is 50, with early stopping for model selection. In the audio modality, the audio from each video is separated and resampled for 44 kHz. Following the segmentation in the visual modality, small vocal segments are extracted corresponding to the length of the clip. A ResNet18 is used to extract the features from the spectrograms. A Discrete Fourier transform with a length of 1024, a hop length of 10 msec, and a window length of 20 msec is used to obtain spectrograms of each vocal segment. The spectrograms have a resolution of 64×107 pixels, corresponding to a single clip in the visual modality. Other preprocessing of spectrograms include mean and variance normalization, as well as conversion to log-power spectrum. The first convolutional layer in the pretrained ResNet model is adapted to take in the single-channel spectrograms. The learning rate of 1 × 10 -2 is used and Adam optimizer is used to optimize the network. 64 batch size is set for the audio modality.\n\nThe audio and visual backbones are frozen to train the A-V fusion network and only train the whole transformer-based fusion model. Each of these backbones outputs deep feature vectors of dimension 512. These features are concatenated (to obtain the joint feature representation, a feature vector of dimension 1024) and then fed to an FC layer to reduce dimensionality to 512. Each of these features is fed to the fusion model, as we can see in Figure  2 . At the output of each of the six cross-attention modules, the feature vector of dimension 512 is outputted. These six feature vectors are then stacked to form a sequence, which is then fed to a transformer self-attention block. This block dynamically selects and weighs these feature vectors. The final attended features are fed to an FC layer for final prediction. We perform a grid search to find the optimal learning rate and batch size for our fusion network. Thus, we use a list of learning rates: [8 × 10 -4 , 6 × 10 -4 , 3 × 10 -4 ] and an SGD optimizer to optimize the fusion network. We use a batch size of 32, and the maximum number of epochs is 5, with early stopping for fusion model selection. We select the best model among the learning rates listed before.\n\nBiovid: In the visual modality, the faces are cropped and aligned using an MTCNN. We apply the frame retention strategy for missing frames where the face is not visible, and the MTCNN cannot capture any frame. Further, to ensure noisefree input to the visual model, we clip the first 2 seconds of the video and the last 0.5 seconds at the end because the subjects show no signs of pain during this duration. The total number of frames is 75. The extracted faces are fed to an R3D model for a visual feature extractor. The batch size is set to 64. The network is separately optimized using the SGD optimizer and the learning rate of 10 -3 .\n\nFor the physiological modality, the EDA is used. The signal is clipped to correspond to the visual modality and fed to a 1D CNN. The architecture of the custom 1D-CNN is shown\n\nin Table  1 . The CNN outputs a 512-dimensional feature vector. The model is optimized using the SGD optimizer with a learning rate of 10 -4 . The batch size for the physiological backbone is set to 1024.\n\nFor the modality fusion, the two feature vectors and the joint representation are fed to the joint transformer block, as shown in Figure  3 . The FC layers are removed from both backbones that were added in the backbone training phase. 512dimensional feature vectors from visual and physiological backbones are obtained and fed to the joint multimodal transformer module. The backbones are frozen, and the joint transformer block is optimized using the ADAM optimizer with a learning rate of 5 × 10 -6 , and the batch size is set to 128.\n\n5 Results and Discussion",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Comparison With The State-Of-The-Art",
      "text": "This section compares the proposed method with the baseline and state-of-the-art on the Affwild2 and Biovid datasets. Table  2  presents the performance of our fusion model on the Biovid Heat Pain Database. We performed 5-fold crossvalidation to pick up the best average fusion model. It can be seen from the table that the proposed model can achieve stateof-the-art performance with multimodal input while using 5fold cross-validation. We used 5-fold cross-validation instead of LOSO validation method due to the computational cost of processing video. The physiological modality is stronger in the Biovid database. Many studies have validated the models on physiological modality. Our empirical results show that the EDA-only accuracy is 77.2%, whereas the visual-only accuracy is 72.9%. The proposed model can improve over unimodal performance and achieves state-of-the-art performance on the Biovid dataset. We also compare it with standard fusion techniques like feature concatenation. For a fair comparison, we keep all the parameters the same. The proposed model improves 6% over simple feature concatenation and 1.3% over a vanilla multimodal transformer i.e. without a joint representation.\n\nFigure  4  shows the visualization of the attention weights generated by the joint transformer model. It can be seen that the model gives more weightage to the physiological modality.\n\nTable  3  shows the valence and arousal CCC values on the Affwild2 official validation set and custom-defined folds to increase generalizability. On the official validation set, the proposed method achieves a 0.666 average with 0.717 valence and 0.614 arousal. In addition, we performed ensembling by considering a dual modeling. We observed that different training configurations lead to models that can be best at once case: 'Valence', or 'Arousal'. Therefore, we used a dual model for prediction by taking the best at each category. This lead to performance increase.  The average of valence and arousal is 0.458. The proposed method significantly improves over the baseline (provided by the challenge organizers). It is essential to mention here that the other methods that achieve higher performance are due to extensive pertaining, the use of additional modalities like text, and the use of more robust backbones. For a fair comparison, we use a similar setting to Joint Cross Attention  [46] , which includes similar pretraining and identical backbones for audio and visual modalities. They used joint cross-attention to fuse the two modalities and achieve a 0.369 average. On the other hand, the proposed model uses JMT fusion and can achieve a 0.443 average. The proposed method improves 7% over the joint cross-attention-based method.  ) improves the performance compared to using a single vision backbone.\n\nTable  7  shows the results of the proposed method with and without the joint representation. On the Biovid dataset, the joint multimodal transformer improves by 1.3% over the vanilla multimodal transformer.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "Multimodal emotion recognition systems outperform their unimodal counterparts, especially in the wild environment. The missing and noisy modality is a prevalent issue with inthe-wild emotion recognition systems. Many attention-based methods have been proposed in the literature to overcome this problem. These methods aim to weigh the modalities dynamically. This paper introduces a joint multimodal transformer for emotional recognition. This transformer-based architecture introduces a joint feature representation to add more redundancy and complementary between audio and visual data. The two modalities are first encoded using separate backbones to extract intra-modal spatiotemporal dependencies. The feature vectors of the two modalities are joint and the joint feature vector is also fed into the Joint Multimodal Transformer module. This joint representation provides more fine-grained information about the inter-modal association between the two modalities. The proposed model outperforms state-of-the-art methods on the Biovid dataset and improves over the vanilla multimodal transformer by 6% on the Af-fwild2 dataset. Our future work includes introducing more modalities and sophisticated backbones for effective feature extraction.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (Top) An illustration of the vanilla multimodal trans-",
      "page": 1
    },
    {
      "caption": "Figure 1: By incorporating a joint representation branch, the model",
      "page": 2
    },
    {
      "caption": "Figure 2: Given fB and fA, the joint feature representation",
      "page": 3
    },
    {
      "caption": "Figure 2: An overview of the proposed joint multimodal transformer model for A-V fusion. The audio and visual modalities",
      "page": 4
    },
    {
      "caption": "Figure 2: At the output of",
      "page": 5
    },
    {
      "caption": "Figure 3: The FC layers are removed from both back-",
      "page": 6
    },
    {
      "caption": "Figure 3: Illustration of the proposed joint multimodal trans-",
      "page": 6
    },
    {
      "caption": "Figure 4: shows the visualization of the attention weights gen-",
      "page": 6
    },
    {
      "caption": "Figure 4: Visualization of attention weights for visual and",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Description of the architecture of the custom 1D-",
      "data": [
        {
          "Input": "1st Conv\nReLU\nMax Pooling",
          "-": "2\n-\n-",
          "2816 × 1": "1406 × 32\n1406 × 32\n703 × 32"
        },
        {
          "Input": "2nd Conv\nReLU\nMax Pooling",
          "-": "1\n-\n-",
          "2816 × 1": "699 × 64\n699 × 64\n349 × 64"
        },
        {
          "Input": "1st FC\nReLU",
          "-": "-\n-",
          "2816 × 1": "512\n512"
        },
        {
          "Input": "2nd FC",
          "-": "-",
          "2816 × 1": "2"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 3: CCC for valence and arousal of the fusion model 0.443average. Theproposedmethodimproves7%overthe",
      "data": [
        {
          "Werner et al. [60] ICPR 2014": "Werner et al. [58] IEEE TAC 2016",
          "EDA, ECG, EMG + Video": "Video",
          "5-FOLD": "LOSO",
          "80.6": "72.4"
        },
        {
          "Werner et al. [60] ICPR 2014": "Kachele et al. [12] IEEE IJSTSP 2016",
          "EDA, ECG, EMG + Video": "EDA, ECG, EMG",
          "5-FOLD": "LOSO",
          "80.6": "82.73"
        },
        {
          "Werner et al. [60] ICPR 2014": "Lopez et al. [33] ACIIW 2017",
          "EDA, ECG, EMG + Video": "EDA, ECG",
          "5-FOLD": "10-FOLD",
          "80.6": "82.75"
        },
        {
          "Werner et al. [60] ICPR 2014": "Lopez et al. [34] EMBC 2018",
          "EDA, ECG, EMG + Video": "EDA",
          "5-FOLD": "LOSO",
          "80.6": "74.21"
        },
        {
          "Werner et al. [60] ICPR 2014": "Thiam et al. [51] Sensors 2019",
          "EDA, ECG, EMG + Video": "EDA",
          "5-FOLD": "LOSO",
          "80.6": "84.57"
        },
        {
          "Werner et al. [60] ICPR 2014": "Wang et al. [56] EMBC 2020",
          "EDA, ECG, EMG + Video": "EDA, ECG, EMG",
          "5-FOLD": "LOSO",
          "80.6": "83.3"
        },
        {
          "Werner et al. [60] ICPR 2014": "Pouromran et al. [45] PLoS ONE 2021",
          "EDA, ECG, EMG + Video": "EDA",
          "5-FOLD": "LOSO",
          "80.6": "83.3"
        },
        {
          "Werner et al. [60] ICPR 2014": "Thiam et al. [52] Frontiers 2021",
          "EDA, ECG, EMG + Video": "EDA, ECG, EMG",
          "5-FOLD": "LOSO",
          "80.6": "84.25"
        },
        {
          "Werner et al. [60] ICPR 2014": "Phan et al. [44] IEEE Access 2023",
          "EDA, ECG, EMG + Video": "EDA, ECG, EMG",
          "5-FOLD": "LOSO",
          "80.6": "84.8"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 3: CCC for valence and arousal of the fusion model 0.443average. Theproposedmethodimproves7%overthe",
      "data": [
        {
          "Audio backbone: 1D CNN only": "Visual backbone: R3D model only",
          "EDA": "Video",
          "5-FOLD": "5-FOLD",
          "77.2": "72.9"
        },
        {
          "Audio backbone: 1D CNN only": "Fusion: feature concatenation",
          "EDA": "EDA + Video",
          "5-FOLD": "5-FOLD",
          "77.2": "83.5"
        },
        {
          "Audio backbone: 1D CNN only": "Fusion: vanilla transformer",
          "EDA": "EDA + Video",
          "5-FOLD": "5-FOLD",
          "77.2": "87.8"
        },
        {
          "Audio backbone: 1D CNN only": "Fusion: JMT (ours)",
          "EDA": "EDA + Video",
          "5-FOLD": "5-FOLD",
          "77.2": "89.1"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 3: CCC for valence and arousal of the fusion model 0.443average. Theproposedmethodimproves7%overthe",
      "data": [
        {
          "Official": "fold-1",
          "0.717": "0.705",
          "0.614": "0.683",
          "0.666": "0.694"
        },
        {
          "Official": "fold-2",
          "0.717": "0.741",
          "0.614": "0.623",
          "0.666": "0.682"
        },
        {
          "Official": "fold-3",
          "0.717": "0.657",
          "0.614": "0.637",
          "0.666": "0.647"
        },
        {
          "Official": "fold-4",
          "0.717": "0.760",
          "0.614": "0.666",
          "0.666": "0.713"
        },
        {
          "Official": "fold-5",
          "0.717": "0.684",
          "0.614": "0.629",
          "0.666": "0.657"
        },
        {
          "Official": "Ensembling",
          "0.717": "0.769",
          "0.614": "0.692",
          "0.666": "0.731"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 3: CCC for valence and arousal of the fusion model 0.443average. Theproposedmethodimproves7%overthe",
      "data": [
        {
          "Baseline": "Joint Cross\nAttention [46]",
          "0.180": "0.374",
          "0.170": "0.363",
          "0.175": "0.369"
        },
        {
          "Baseline": "AU-NO [14]",
          "0.180": "0.418",
          "0.170": "0.407",
          "0.175": "0.413"
        },
        {
          "Baseline": "HSE-NN [49]",
          "0.180": "0.417",
          "0.170": "0.454",
          "0.175": "0.436"
        },
        {
          "Baseline": "PRL [41]",
          "0.180": "0.450",
          "0.170": "0.445",
          "0.175": "0.448"
        },
        {
          "Baseline": "Joint Multimodal\nTransformer (ours)",
          "0.180": "0.472",
          "0.170": "0.443",
          "0.175": "0.458"
        },
        {
          "Baseline": "FlyingPigs [62]",
          "0.180": "0.520",
          "0.170": "0.602",
          "0.175": "0.561"
        },
        {
          "Baseline": "Situ-RUCAIM3 [37]",
          "0.180": "0.606",
          "0.170": "0.596",
          "0.175": "0.601"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 6: CCC performance of visual and audio backbones 87.8",
      "data": [
        {
          "R2D1 model only\n(visual)": "ResNet18 model only\n(audio)",
          "0.194": "0.273",
          "0.310": "0.246",
          "0.252": "0.260"
        },
        {
          "R2D1 model only\n(visual)": "Concat + FC layers",
          "0.194": "0.320",
          "0.310": "0.327",
          "0.252": "0.323"
        },
        {
          "R2D1 model only\n(visual)": "Vanilla Transformer",
          "0.194": "0.376",
          "0.310": "0.334",
          "0.252": "0.355"
        },
        {
          "R2D1 model only\n(visual)": "JMT (ours)",
          "0.194": "0.366",
          "0.310": "0.379",
          "0.252": "0.373"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 6: CCC performance of visual and audio backbones 87.8",
      "data": [
        {
          "Affwild2": "",
          "Vanilla Multimodal\nTransformer": "Joint Multimodal\nTransformer",
          "V: 0.432\nA: 0.410\nAvg: 0.421": "V: 0.425\nA: 0.450\nAvg: 0.438"
        },
        {
          "Affwild2": "Biovid",
          "Vanilla Multimodal\nTransformer": "Vanilla Multimodal\nTransformer",
          "V: 0.432\nA: 0.410\nAvg: 0.421": "87.8"
        },
        {
          "Affwild2": "",
          "Vanilla Multimodal\nTransformer": "Joint Multimodal\nTransformer",
          "V: 0.432\nA: 0.410\nAvg: 0.421": "89.1"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 6: CCC performance of visual and audio backbones 87.8",
      "data": [
        {
          "Joint Cross Attention [46]": "I3D model only\n(visual)",
          "0.374": "0.336",
          "0.363": "0.422",
          "0.369": "0.379"
        },
        {
          "Joint Cross Attention [46]": "ResNet18 model only\n(audio)",
          "0.374": "0.273",
          "0.363": "0.246",
          "0.369": "0.260"
        },
        {
          "Joint Cross Attention [46]": "Concatenation + FC",
          "0.374": "0.387",
          "0.363": "0.453",
          "0.369": "0.420"
        },
        {
          "Joint Cross Attention [46]": "Vanilla Transformer\n- I3D (visual)\n- ResNet18 (audio)",
          "0.374": "0.432",
          "0.363": "0.410",
          "0.369": "0.421"
        },
        {
          "Joint Cross Attention [46]": "JMT (ours)\n- I3D (visual)\n- ResNet18 (audio)",
          "0.374": "0.425",
          "0.363": "0.450",
          "0.369": "0.438"
        },
        {
          "Joint Cross Attention [46]": "JMT (ours)\n- I3D+R2D1 (visual, FC)\n- ResNet18 (audio)",
          "0.374": "0.472",
          "0.363": "0.443",
          "0.369": "0.458"
        },
        {
          "Joint Cross Attention [46]": "JMT (ours)\n- I3D+R2D1 (visual, TR)\n- ResNet18 (audio)",
          "0.374": "0.458",
          "0.363": "0.445",
          "0.369": "0.452"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Features and classifiers for emotion recognition from speech: a survey from 2000 to 2011",
      "authors": [
        "C Anagnostopoulos",
        "T Iliou",
        "I Giannoukos"
      ],
      "year": "2015",
      "venue": "Artif Intell Rev"
    },
    {
      "citation_id": "2",
      "title": "Distilling privileged multimodal information for expression recognition using optimal transport",
      "authors": [
        "M Aslam",
        "M Zeeshan",
        "S Belharbi",
        "M Pedersoli",
        "A Koerich",
        "S Bacon",
        "E Granger"
      ],
      "year": "2024",
      "venue": "18th IEEE International Conference on Automatic Face and Gesture Recognition",
      "arxiv": "arXiv:2401.15489"
    },
    {
      "citation_id": "3",
      "title": "Privileged knowledge distillation for dimensional emotion recognition in the wild",
      "authors": [
        "M Aslam",
        "O Zeeshan",
        "M Pedersoli",
        "A Koerich",
        "S Bacon",
        "E Granger"
      ],
      "year": "2023",
      "venue": "CVPRw 2023: IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "4",
      "title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "authors": [
        "K Cho",
        "B Van Merriënboer",
        "C Gulcehre",
        "D Bahdanau",
        "F Bougares",
        "H Schwenk",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "arxiv": "arXiv:1406.1078"
    },
    {
      "citation_id": "5",
      "title": "Mdn: A deep maximization-differentiation network for spatio-temporal depression detection",
      "authors": [
        "W De Melo",
        "E Granger",
        "M Lopez"
      ],
      "year": "2021",
      "venue": "IEEE Tran. on Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "Iterative distillation for better uncertainty estimates in multitask emotion recognition",
      "authors": [
        "D Deng",
        "L Wu",
        "B Shi"
      ],
      "year": "2021",
      "venue": "ICCVW"
    },
    {
      "citation_id": "7",
      "title": "Automatic subject independent pain intensity estimation using a deep learning approach",
      "authors": [
        "M.-C Dragomir",
        "C Florea",
        "V Pupezescu"
      ],
      "year": "2020",
      "venue": "2020 International Conference on e-Health and Bioengineering (EHB)"
    },
    {
      "citation_id": "8",
      "title": "Audio-visual event localization via recursive fusion by joint co-attention",
      "authors": [
        "B Duan",
        "H Tang",
        "W Wang",
        "Z Zong",
        "G Yang",
        "Y Yan"
      ],
      "year": "2021",
      "venue": "WACV"
    },
    {
      "citation_id": "9",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cognition and Emotion"
    },
    {
      "citation_id": "10",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "11",
      "title": "Multimodal transformer fusion for continuous emotion recognition",
      "authors": [
        "J Huang",
        "J Tao",
        "B Liu",
        "Z Lian",
        "M Niu"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "Methods for person-centered continuous pain intensity assessment from bio-physiological channels",
      "authors": [
        "M Kächele",
        "P Thiam",
        "M Amirian",
        "F Schwenker",
        "G Palm"
      ],
      "year": "2016",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "Bio-visual fusion for personindependent recognition of pain intensity",
      "authors": [
        "M Kächele",
        "P Werner",
        "A Al-Hamadi",
        "G Palm",
        "S Walter",
        "F Schwenker"
      ],
      "year": "2015",
      "venue": "International Workshop on Multiple Classifier Systems"
    },
    {
      "citation_id": "14",
      "title": "Continuous-time audiovisual fusion with recurrence vs. attention for in-the-wild affect recognition",
      "authors": [
        "V Karas",
        "M Tellamekala",
        "A Mallol-Ragolta",
        "M Valstar",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "Continuous-time audiovisual fusion with recurrence vs. attention for in-the-wild affect recognition",
      "arxiv": "arXiv:2203.13285"
    },
    {
      "citation_id": "15",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & multi-task learning challenges",
      "authors": [
        "D Kollias"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "16",
      "title": "Abaw: learning from synthetic data & multi-task learning challenges",
      "authors": [
        "D Kollias"
      ],
      "year": "2023",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "17",
      "title": "Multi-label compound expression recognition: C-expr database & network",
      "authors": [
        "D Kollias"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "18",
      "title": "Analysing affective behavior in the first abaw competition",
      "authors": [
        "D Kollias",
        "A Schulc",
        "E Hajiyev",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "Analysing affective behavior in the first abaw competition"
    },
    {
      "citation_id": "19",
      "title": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "authors": [
        "D Kollias",
        "V Sharmanska",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "arxiv": "arXiv:1910.11111"
    },
    {
      "citation_id": "20",
      "title": "Distribution matching for heterogeneous multi-task learning: a large-scale face study",
      "authors": [
        "D Kollias",
        "V Sharmanska",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "Distribution matching for heterogeneous multi-task learning: a large-scale face study",
      "arxiv": "arXiv:2105.03790"
    },
    {
      "citation_id": "21",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & emotional reaction intensity estimation challenges",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "A Baird",
        "A Cowen",
        "S Zafeiriou"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "22",
      "title": "The 6th affective behavior analysis in-the-wild (abaw) competition",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "A Cowen",
        "S Zafeiriou",
        "C Shao",
        "G Hu"
      ],
      "year": "2024",
      "venue": "The 6th affective behavior analysis in-the-wild (abaw) competition",
      "arxiv": "arXiv:2402.19344"
    },
    {
      "citation_id": "23",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond. IJCV",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "B Schuller",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond. IJCV"
    },
    {
      "citation_id": "24",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "B Schuller",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "25",
      "title": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "26",
      "title": "Affect analysis in-thewild: Valence-arousal, expressions, action units and a unified framework",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "Affect analysis in-thewild: Valence-arousal, expressions, action units and a unified framework",
      "arxiv": "arXiv:2103.15792"
    },
    {
      "citation_id": "27",
      "title": "Analysing affective behavior in the second abaw2 competition",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "ICCVW"
    },
    {
      "citation_id": "28",
      "title": "Twostream aural-visual affect analysis in the wild",
      "authors": [
        "F Kuhnke",
        "L Rumberg",
        "J Ostermann"
      ],
      "year": "2020",
      "venue": "FG Workshop"
    },
    {
      "citation_id": "29",
      "title": "Twostream aural-visual affect analysis in the wild",
      "authors": [
        "F Kuhnke",
        "L Rumberg",
        "J Ostermann"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)"
    },
    {
      "citation_id": "30",
      "title": "A concordance correlation coefficient to evaluate reproducibility",
      "authors": [
        "I Lawrence",
        "K Lin"
      ],
      "year": "1989",
      "venue": "Biometrics"
    },
    {
      "citation_id": "31",
      "title": "Multi-label multimodal emotion recognition with transformer-based fusion and emotion-level representation learning",
      "authors": [
        "H.-D Le",
        "G.-S Lee",
        "S.-H Kim",
        "S Kim",
        "H.-J Yang"
      ],
      "year": "2023",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "32",
      "title": "Crossattentional audio-visual fusion for weakly-supervised action localization",
      "authors": [
        "J.-T Lee",
        "M Jain",
        "H Park",
        "S Yun"
      ],
      "year": "2021",
      "venue": "ICLR"
    },
    {
      "citation_id": "33",
      "title": "Multi-task neural networks for personalized pain recognition from physiological signals",
      "authors": [
        "D Lopez-Martinez",
        "R Picard"
      ],
      "year": "2017",
      "venue": "2017 Seventh International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)"
    },
    {
      "citation_id": "34",
      "title": "Continuous pain intensity estimation from autonomic signals with recurrent neural networks",
      "authors": [
        "D Lopez-Martinez",
        "R Picard"
      ],
      "year": "2018",
      "venue": "2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)"
    },
    {
      "citation_id": "35",
      "title": "Vilbert: Pretraining task-agnostic visiolinguistic representations for visionand-language tasks",
      "authors": [
        "J Lu",
        "D Batra",
        "D Parikh",
        "S Lee"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "36",
      "title": "Transformer encoder with multiscale deep learning for pain classification using physiological signals",
      "authors": [
        "Z Lu",
        "B Ozek",
        "S Kamarthi"
      ],
      "year": "2023",
      "venue": "Transformer encoder with multiscale deep learning for pain classification using physiological signals",
      "arxiv": "arXiv:2303.06845"
    },
    {
      "citation_id": "37",
      "title": "Multimodal emotion estimation for in-the-wild videos",
      "authors": [
        "L Meng",
        "Y Liu",
        "X Liu",
        "Z Huang",
        "W Jiang",
        "T Zhang",
        "Y Deng",
        "R Li",
        "Y Wu",
        "J Zhao"
      ],
      "year": "2022",
      "venue": "Multimodal emotion estimation for in-the-wild videos",
      "arxiv": "arXiv:2203.13032"
    },
    {
      "citation_id": "38",
      "title": "Pain detection from facial expressions based on transformers and distillation",
      "authors": [
        "S Morabit",
        "A Rivenq"
      ],
      "year": "2022",
      "venue": "2022 11th International Symposium on Signal, Image, Video and Communications (ISIVC)"
    },
    {
      "citation_id": "39",
      "title": "Multimodal deep learning",
      "authors": [
        "J Ngiam",
        "A Khosla",
        "M Kim",
        "J Nam",
        "H Lee",
        "A Ng"
      ],
      "year": "2011",
      "venue": "Proceedings of the 28th International Conference on International Conference on Machine Learning, ICML'11"
    },
    {
      "citation_id": "40",
      "title": "Deep autoencoders with sequential learning for multimodal dimensional emotion recognition",
      "authors": [
        "D Nguyen",
        "D Nguyen",
        "R Zeng",
        "T Nguyen",
        "S Tran",
        "T Nguyen",
        "S Sridharan",
        "C Fookes"
      ],
      "year": "2021",
      "venue": "IEEE Trans. on Multimedia"
    },
    {
      "citation_id": "41",
      "title": "An ensemble approach for facial expression analysis in video",
      "authors": [
        "H.-H Nguyen",
        "V.-T Huynh",
        "S.-H Kim"
      ],
      "year": "2022",
      "venue": "An ensemble approach for facial expression analysis in video",
      "arxiv": "arXiv:2203.12891"
    },
    {
      "citation_id": "42",
      "title": "Emotion recognition using fusion of audio and video features",
      "authors": [
        "J Ortega",
        "P Cardinal",
        "A Koerich"
      ],
      "year": "2019",
      "venue": "SMC"
    },
    {
      "citation_id": "43",
      "title": "Training strategies to handle missing modalities for audio-visual expression recognition",
      "authors": [
        "S Parthasarathy",
        "S Sundaram"
      ],
      "year": "2020",
      "venue": "ICMI"
    },
    {
      "citation_id": "44",
      "title": "Pain recognition with physiological signals using multi-level context information",
      "authors": [
        "K Phan",
        "N Iyortsuun",
        "S Pant",
        "H.-J Yang",
        "S.-H Kim"
      ],
      "year": "2023",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "45",
      "title": "Exploration of physiological sensors, features, and machine learning models for pain intensity estimation",
      "authors": [
        "F Pouromran",
        "S Radhakrishnan",
        "S Kamarthi"
      ],
      "year": "2021",
      "venue": "Pone Journal"
    },
    {
      "citation_id": "46",
      "title": "A joint cross-attention model for audio-visual fusion in dimensional emotion recognition",
      "authors": [
        "R Praveen",
        "W De Melo",
        "N Ullah",
        "H Aslam",
        "O Zeeshan",
        "T Denorme",
        "M Pedersoli",
        "A Koerich",
        "S Bacon",
        "P Cardinal"
      ],
      "year": "2022",
      "venue": "IEEE/CVF CVPR"
    },
    {
      "citation_id": "47",
      "title": "A joint cross-attention model for audio-visual fusion in dimensional emotion recognition",
      "authors": [
        "R Praveen",
        "W De Melo",
        "N Ullah",
        "H Aslam",
        "O Zeeshan",
        "T Denorme",
        "M Pedersoli",
        "A Koerich",
        "S Bacon",
        "P Cardinal",
        "E Granger"
      ],
      "year": "2022",
      "venue": "CVPRW"
    },
    {
      "citation_id": "48",
      "title": "Cross attentional audio-visual fusion for dimensional emotion recognition",
      "authors": [
        "G Rajasekhar",
        "E Granger",
        "P Cardinal"
      ],
      "year": "2021",
      "venue": "FG"
    },
    {
      "citation_id": "49",
      "title": "Frame-level prediction of facial expressions, valence, arousal and action units for mobile devices",
      "authors": [
        "A Savchenko"
      ],
      "year": "2022",
      "venue": "Frame-level prediction of facial expressions, valence, arousal and action units for mobile devices",
      "arxiv": "arXiv:2203.13436"
    },
    {
      "citation_id": "50",
      "title": "Leveraging recent advances in deep learning for audio-visual emotion recognition",
      "authors": [
        "L Schoneveld",
        "A Othmani",
        "H Abdelkawy"
      ],
      "year": "2021",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "51",
      "title": "Exploring deep physiological models for nociceptive pain recognition",
      "authors": [
        "P Thiam",
        "P Bellmann",
        "H Kestler",
        "F Schwenker"
      ],
      "year": "2019",
      "venue": "Sensors"
    },
    {
      "citation_id": "52",
      "title": "Multi-modal pain intensity assessment based on physiological signals: A deep learning perspective",
      "authors": [
        "P Thiam",
        "H Hihn",
        "D Braun",
        "H Kestler",
        "F Schwenker"
      ],
      "year": "2021",
      "venue": "Frontiers in Physiology"
    },
    {
      "citation_id": "53",
      "title": "A pre-trained audio-visual transformer for emotion recognition",
      "authors": [
        "M Tran",
        "M Soleymani"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "54",
      "title": "End-to-end multimodal affect recognition in real-world environments",
      "authors": [
        "P Tzirakis",
        "J Chen",
        "S Zafeiriou",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "55",
      "title": "End-to-end multimodal emotion recognition using deep neural networks",
      "authors": [
        "P Tzirakis",
        "G Trigeorgis",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2017",
      "venue": "IEEE J. of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "56",
      "title": "Hybrid rnn-ann based deep physiological network for pain recognition",
      "authors": [
        "R Wang",
        "K Xu",
        "H Feng",
        "W Chen"
      ],
      "year": "2020",
      "venue": "2020 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)"
    },
    {
      "citation_id": "57",
      "title": "Multi-modality cross attention network for image and sentence matching",
      "authors": [
        "X Wei",
        "T Zhang",
        "Y Li",
        "Y Zhang",
        "F Wu"
      ],
      "year": "2020",
      "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "58",
      "title": "Automatic pain assessment with facial activity descriptors",
      "authors": [
        "P Werner",
        "A Al-Hamadi",
        "K Limbrecht-Ecklundt",
        "S Walter",
        "S Gruss",
        "H Traue"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "59",
      "title": "Automatic pain recognition from video and biomedical signals",
      "authors": [
        "P Werner",
        "A Al-Hamadi",
        "R Niese",
        "S Walter",
        "S Gruss",
        "H Traue"
      ],
      "year": "2014",
      "venue": "22nd International Conference on Pattern Recognition"
    },
    {
      "citation_id": "60",
      "title": "Automatic pain recognition from video and biomedical signals",
      "authors": [
        "P Werner",
        "A Al-Hamadi",
        "R Niese",
        "S Walter",
        "S Gruss",
        "H Traue"
      ],
      "year": "2014",
      "venue": "2014 22nd international conference on pattern recognition"
    },
    {
      "citation_id": "61",
      "title": "Aff-wild: Valence and arousal 'inthe-wild'challenge",
      "authors": [
        "S Zafeiriou",
        "D Kollias",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "I Kotsia"
      ],
      "year": "2017",
      "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference on"
    },
    {
      "citation_id": "62",
      "title": "Continuous emotion recognition using visual-audio-linguistic information: A technical report for abaw3",
      "authors": [
        "S Zhang",
        "R An",
        "Y Ding",
        "C Guan"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "63",
      "title": "Continuous emotion recognition with audio-visual leader-follower attentive fusion",
      "authors": [
        "S Zhang",
        "Y Ding",
        "Z Wei",
        "C Guan"
      ],
      "year": "2021",
      "venue": "ICCVW"
    },
    {
      "citation_id": "64",
      "title": "Multimodal continuous valence-arousal estimation in the wild",
      "authors": [
        "Y.-H Zhang",
        "R Huang",
        "J Zeng",
        "S Shan"
      ],
      "year": "2020",
      "venue": "IEEE FG"
    },
    {
      "citation_id": "65",
      "title": "Multimodal-based stream integrated neural networks for pain assessment",
      "authors": [
        "R Zhi",
        "C Zhou",
        "J Yu",
        "T Li",
        "G Zamzmi"
      ],
      "year": "2021",
      "venue": "IEICE Trans. Inf. Syst"
    },
    {
      "citation_id": "66",
      "title": "Leveraging tcn and transformer for effective visual-audio fusion in continuous emotion recognition",
      "authors": [
        "W Zhou",
        "J Lu",
        "Z Xiong",
        "W Wang"
      ],
      "year": "2023",
      "venue": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    }
  ]
}