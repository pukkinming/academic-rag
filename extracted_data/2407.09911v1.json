{
  "paper_id": "2407.09911v1",
  "title": "Sensemo: Enabling Affective Learning Through Real-Time Emotion Recognition With Smartwatches",
  "published": "2024-07-13T15:10:58Z",
  "authors": [
    "Kushan Choksi",
    "Hongkai Chen",
    "Karan Joshi",
    "Sukrutha Jade",
    "Shahriar Nirjon",
    "Shan Lin"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recent research has demonstrated the capability of physiological signals to infer both user emotional and attention responses. This presents an opportunity for leveraging widely available physiological sensors in smartwatches, to detect real-time emotional cues in users, such as stress and excitement. In this paper, we introduce SensEmo, a smartwatch-based system designed for affective learning. SensEmo utilizes multiple physiological sensor data, including heart rate and galvanic skin response, to recognize a student's motivation and concentration levels during class. This recognition is facilitated by a personalized emotion recognition model that predicts emotional states based on degrees of valence and arousal. With real-time emotion and attention feedback from students, we design a Markov decision process-based algorithm to enhance student learning effectiveness and experience by offering suggestions to the teacher regarding teaching content and pacing. We evaluate SensEmo with 22 participants in real-world classroom environments. Evaluation results show that SensEmo recognizes student emotion with an average of 88.9% accuracy. More importantly, SensEmo assists students to achieve better online learning outcomes, e.g., an average of 40.0% higher grades in quizzes, over the traditional learning without student emotional feedback.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotion impacts learning, with curiosity and motivation being crucial for effective learning and academic success  [1] . However, students often struggle to maintain motivation and concentration during class, especially when the difficulty level and teaching pace do not match their needs. Experienced teachers use strategies like interactive discussions to boost engagement, but there is a lack of effective methods to monitor students' emotions. This problem is more significant in online classrooms, where interactions are limited, making it difficult to get emotional feedback from students. Thus, an affective learning system that accurately identifies a learner's emotional state is needed.\n\nAffective learning involves emotional engagement in acquiring knowledge, skills, and attitudes, recognizing emotions' role in cognitive processes, memory, and decision-making. Research has used methods like eye tracking  [2] , facial recognition  [3] , speech recognition  [4] ,\n\ntext recognition  [5] , and gesture movement  [6]  to detect emotions. However, these methods are not suitable for classroom settings. For instance, camera-based methods raise privacy concerns and struggle to capture all students at once. Additionally, in online learning, factors like lighting, network conditions, and privacy further limit these methods.\n\nTo address these challenges, we use off-the-shelf smartwatches with physiological sensing capabilities for emotion recognition. Studies have showed strong links between physiological signals and emotional states  [7] . We develop a personalized emotion model by analyzing physiological signals from different users, identifying changes related to motivation and concentration. Leveraging this model, we develop SensEmo, which helps teachers monitor students' emotions and adjust teaching methods. For example, SensEmo suggests providing concrete examples to clarify key concepts when it detects confusion among multiple students. Similarly, it advises increasing the pace when most students appear bored. It is important to note that various factors can influence a student's motivation level in real-world scenarios, but this paper focuses on classroom learning situations.\n\nTo assess our emotion recognition solution, we implemented SensEmo on commercial smartwatches and tested it with 22 volunteers using International Affective Picture System (IAPS) emotion image dataset  [8] . Physiological signal features collected by smartwatches were mapped to the valence-arousal space and classified into four states: curious, bored, confused, and satisfied. Additionally, we conducted simulated and realworld experiments to evaluate the system's impact on affective learning. Results indicate that SensEmo helps maintain students' emotional states and enhances learning outcomes.\n\nIn summary, our main contributions are the following.\n\n• We developed SensEmo, the first affective learning system that utilizes real-time emotion sensing and recognition with a smartwatch to provide students' feedback. • We proposed a personalized emotion model that classifies physiological signals into specific levels of motivation and concentration tailored to the learning environment.\n\n• We introduced an affective learning model using a reinforcement learning-based controller, which adapts teaching content and pace according to real-time emotion and individual learning preferences of students. This model is applicable to both online and classroom learning systems.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Emotion recognition has been explored in many domains. A KNN algorithm classifies emotions using physiological and subjective components  [9] . StressSense uses smartphonedetected voice for stress recognition  [10] . Text analysis studies emotions via semantic labels, attributes  [11] , and typing rhythms  [12] . Wearable devices detect emotions using respiratory frequency, heart rate variability, skin conductance, and accelerometer data  [13] . However, limited research focuses on emotion recognition in learning environments. Studies explore wearable biosensors to enhance learning by monitoring physiological signals  [14] , assessing cognitive load during problem-solving  [15] , and predicting depression in students  [16] . However, few have examined emotion recognition's role in improving learning performance.\n\nAdaptive systems have been proposed to adjust game difficulty based on users' emotions to maintain engagement  [17] , and e-learning systems have used neurofuzzy networks to estimate user behavior and adapt content presentation  [18] . However, they are usually limited by realworld implementation and validation. In contrast, SensEmo utilizes physiological signals from smartwatches to guide teaching content and pace in real-world settings.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. System Design",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Application Scenarios",
      "text": "SensEmo can be implemented in both in-person and online remote classroom settings, where students wear a smartwatch, and a mobile app collects real-time physiological data. This data is then sent to a central server for emotion recognition, adapting the learning process for students. In online remote learning, teaching content and pace can be automatically adjusted for each individual student. In an in-person classroom setting, the instructor focuses on maintaining positive emotional responses among students. This paper explores SensEmo's use in affective learning, with potential applications in driving monitoring  [19] , health monitoring  [20] , and interviewing/consultation assistance  [21] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. System Overview",
      "text": "SensEmo collects real-time emotional feedback from students' physiological signals during engagement with teaching content. This feedback is used to adapt the course automatically. The components of SensEmo are shown in Figure  1 .  Sensor system. SensEmo's sensor system uses a smartwatch to capture real-time physiological signals, including heart rate, skin resistance, and skin temperature. Physiological data collection. A smartphone application communicates with smartwatches to retrieve sensor data, which is sent to the cloud for processing and feature extraction. Cloud computing. A cloud platform is used due to storage and computational requirements, facilitating feature mapping, emotion recognition, and controller computations. Affective learning controller. The learning controller uses real-time feedback on emotions and concentration to adjust teaching content and pace. It employs a Markov Decision Process (MDP) trained via reinforcement learning, creating a loop that informs the instructor of students' emotions and preferences. The design, shown in Figure  2 , includes emotion recognition and reinforcement learning-based control.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Emotion Recognition",
      "text": "During the students' engagement in learning, SensEmo uses smartwatches to capture physiological data. To interpret this data, it is crucial to correlate physiological signals with emotional states. In this section, we describe our approach to emotion recognition. Physiological feature selection. Emotional responses trigger autonomic nervous system reactions, captured through physiological signals. SensEmo uses the following features to deduce emotions. (1) Electrodermal activity: Continuous changes in skin's electrical characteristics, influenced by moisture. Skin conductance response, increasing with stress and perspiration, and skin conductance level can be obtained from electrodermal activity. (2) Blood volume pulse: Captures changes in blood volume, indicating blood flow. The inter-beat interval, which represents the time duration between two heartbeats, is derived from the blood volume pulse signal. This interval is utilized to determine features such as heart rate and heart rate variability. (3) Skin temperature: Linked to the autonomic nervous system. Excitement can raise skin temperature. We measure both skin temperature response and level. Personalized emotion model calibration. Emotion recognition using physiological signals varies among individuals  [22] . For instance, one person's heart rate may rise to 100 bps when excited, another's to 120 bps. To address the user-specific differences, we developed a personalized calibration technique to normalize emotion-reflecting physiological changes. Our local min-max normalization maps physiological signals to a feature space from -1 to 1, calculated as\n\n, where f is the feature, f max and f min are local maxima and minima. This normalization has been used to analyze different physiological features consistently, disregarding individual emotional responses  [23] .\n\nValence/arousal scales from physiological features. SensEmo predicts students' valence and arousal scales from calibrated features. We use the IAPS image dataset, which elicits emotional responses. Images are categorized as pleasant, neutral, or unpleasant, and rated on valence (pleasant to unpleasant) and arousal (calm to excited) scales. These scales are target values for mapping physiological features into the valence-arousal space, widely used in psychological research  [24] . During our study, users' physiological signals were collected while viewing these images. This collected dataset is used to train a model to translate physiological signals into valence and arousal scales. Learning emotion recognition from valence-arousal space. The valence-arousal space categorizes emotions in a two-dimensional space  [25] . Existing theories label emotions within this space  [26] . SensEmo maps valence and arousal scales to learning-related emotions like boredom, confusion, curiosity, and satisfaction  [27] . Figure  3  shows the valencearousal space highlighting learning-relevant emotions. This approach, allowing continuous emotion modeling, has been validated by clinical data  [28] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Reinforcement Learning-Based Control",
      "text": "After processing student emotion data, the system identifies the current emotional state, S t , and preferences for content difficulty and pace. These inputs are fed to a reinforcement learning-based controller with two main components: a decision-making policy lookup and an MDP controller. The lookup retrieves information about optimal and sub-optimal decisions made by the MDP controller in the current and previous steps. Sub-optimal decisions are fallback options if the optimal policy isn't feasible. For instance, lectures may not able to change content or adjust pace as suggested by  the MDP controller. The MDP controller optimizes decision sequences based on the current emotional state S t and student preferences, aiming to keep the user in a state of curiosity, characterized by relaxed alertness  [29] . The MDP structure is shown in Figure  4 , comprising states (bored, satisfied, curious, confused) and actions (increase/decrease pace, simplify/no change in content). Transition probabilities determine the likelihood of moving from one state to another, influenced by the current state and the student's preferences. However, determining the optimal decision that maintains the user in the desired emotional state of curiosity relies on value iteration of the MDP model.\n\nSensEmo faces uncertainties in both the students' emotional states and learning performance, which are influenced by the instructor's decisions. The system uses a MDP represented by a 4-tuple: states S, actions A, transition probabilities P , and rewards R. The goal is to find an optimal policy π * that maximizes cumulative rewards over time, guiding decisions to induce curiosity. The MDP is defined as follows.\n\n• States: S = {s 0 ,s 1 ,s 2 ,s 3 }. Each represents emotion state bored, satisfied, curious, and confused, respectively. Additionally, s ′ denotes the state that follows after s. • Actions: A = {a 0 , a 1 , a 2 , a 3 }. Each represents action increase pace, decrease pace, simplify content, and no change in content. Note that a is the action taken during each iteration, as indicated in equations (  1 ) and (2). • Transition Probabilities: Determined by a Markov chain asymptotic analysis  [30]  with our experimental data. • Reward: R a (s|s ′ ). It represents the incentives for state transition. The initial rewards were defined based on the observed differences in asymptotic state transition probabilities. These values were obtained empirically, considering the variations in state transition probabilities. The optimization problem for the MDP aims at maximizing the accumulated rewards based on current emotional states of the students, in order to determine the optimal policy π * . The optimal policy π * (s) is determined asymptotically using the value iteration optimization function, which is defined by the following equations.\n\nThe policy π(s) maps states s to actions a and the value function V i+1 (s) at iteration i + 1 represents the maximum expected return from state s, subject to the optimal action and the initialization of the value function. Initial values for states are assigned randomly. The discount factor γ ∈ [0, 1] weighs the importance of future rewards, empirically set to guide actions towards curious or satisfied states. In our system, the discount factors for s 1 ,...,s 4 are set to 0.1, 0.45, 0.35, and 0.1, respectively. Furthermore, the decision-making process takes into account the student's preferences, such as desirable learning pace or preference between illustrations and descriptions. This preference data is collected through a prelecture questionnaire that can be conveniently stored online.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iv. System Implementation",
      "text": "In this section, we describe the implementation and realization of both the hardware and algorithms of SensEmo.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Sensor System",
      "text": "Data Acquisition. We implemented SensEmo on Microsoft Band 2 smartwatches, which can detect electrodermal activity, blood volume pulse, and skin temperature. Each sensor has a default sampling rate of 1 Hz. Proper sensor placement is crucial to avoid motion artifacts that can affect measurement accuracy. An Android app was installed on users' smartphones to receive sensor data from the smartwatches and facilitate data collection. Feature extraction. SensEmo extracts six features from the sensor data: skin conductance response (SCR), skin conductance level (SCL), heart rate (HR), heart rate variability (HRV), skin temperature response (STR), and skin temperature level (STL). It computes moving averages of HR, STR, and SCL, and running deviations of HRV, SCR, and STR using a 50-sample moving window. HRV is extracted from the RR-interval data stream using the Root-mean-square of successive differences (RMSSD) method.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Emotion Recognition",
      "text": "We describe two major components of emotion recognition: personalized model calibration and emotion recognition. Personalized model calibration. First, SensEmo calculates the maximum and minimum values of the six physiological sensor features (i.e., SCR, SCL, HR, HRV, STR, and STL) within a 50-sample window for each user. Then, it computes the normalized features for all users. Emotion recognition. We use the IAPS image dataset to map normalized features to valence and arousal scales. During data collection, users' physiological signals were recorded while viewing images, with the valence and arousal scales of those images considered as the ground truth. A Fine Gaussian Support Vector Machine (SVM) was chosen for predicting valence and arousal scales due to its memory efficiency. The dataset was split into a 70:15:15 ratio for training, validation, and testing. Finally, a fuzzy emotion modeling approach  [28]  mapped the valence and arousal scales onto the valence-arousal space, identifying specific emotions with nuanced representation by considering fuzzy boundaries between emotional states.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Mdp Control With Collective Emotion",
      "text": "The input to the MDP is the collective emotion state from individual student emotions, which is defined as the weighted average of the individual emotions. It is computed as\n\nwhere n i is the number of students in emotional state s i , and N is total number of emotions. To ensure continuous operation, we analyzed the MDP structure's stability, irreducibility, aperiodicity, ergodicity, and control process consistency, involving asymptotic behavior and eigenvalue analysis. Figure  5  shows a convergence sample of the Markov chain's asymptotic behavior. By analyzing asymptotics with our data, we determined state transition values and computed the stationary state distribution from a uniform distribution.\n\nTable  I  shows an optimal and a sub-optimal policy for SensEmo, helping it adjust decisions based on value iteration results. Note that Table I represents one optimal policy, which can vary with data, iterations, reward schemes, and student preferences. SensEmo stores sub-optimal solutions with lower rewards than the optimal policy for alternative suggestions. These are used when optimal suggestions cannot be implemented. For instance, if confusion is the collective emotion, SensEmo may suggest content simplification. If infeasible, it proposes sub-optimal adjustments like reducing the teaching pace. We observe that the sub-optimal policy can change depending on the prevailing conditions, while the optimal policy remains stable.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Discussion Of System Generalization",
      "text": "In SensEmo, personalized calibration enables our approach to be applied to diverse groups of individuals with varying baselines, ensuring its adaptability. In addition, the MDP-based decision-making framework can be easily extended to accommodate different numbers of states and objectives, allowing for flexible customization of the state transition probabilities in a generalized system function. For example, SensEmo can be utilized for monitoring mental states in medical systems  [31] .\n\nThe methodology employed for emotion recognition in our approach is also capable of incorporating a broader range of emotion states within the valence-arousal space. While we utilized a simple four-quadrant classification of emotional space in our case, the number of emotions can be expanded, allowing for further classification of emotional subspaces.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "V. System Evaluation",
      "text": "In this section, we evaluate the performance of SensEmo in emotion recognition and its impact on improving learning outcomes. To conduct the evaluation, we recruited 22 graduate students between the ages of 22 and 27. Additionally, all experiments described in this paper received approval/waiver from the IRB.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Emotion Recognition Evaluation",
      "text": "Experimental Setup. To collect data for different emotion states, it is crucial to use stimuli that are consistent and reliable throughout the experiment. In order to establish a neutral emotional state for volunteers, we utilizes the IAPS image dataset. We present the user with a series of five images that have medium valence and low arousal, lasting for 15 minutes. Following this, the desired images are displayed for data collection purposes. Between subsequent data collection intervals, we again show the user images with medium valence and low arousal. It is important to note that images with low valence and high arousal, which may contain negative content such as mutilation, can have a long-lasting emotional impact and consequently influence subsequent data collection. For this reason, data collection involving such images is performed towards the end of the experiments. System Accuracy. To create the dataset, we carefully select IAPS images that span a wide range of valence and arousal levels, encompassing both very low and very high values. From the dataset, we gather data for a total of 200 images. Among these, 100 images are shared among the volunteers, while the remaining 100 images are distinct, covering various ranges of valence and arousal. This compilation forms our dataset, which includes recorded physiological sensor data along with rated arousal and valence scales. Following personalized calibration, we train a Fine Gaussian SVM with supervised learning. To evaluate the classification accuracy, we conduct a 10-fold cross-validation. The resulting confusion matrix is shown in in Figure  6  (left). The experimental findings indicate that the Fine Gaussian SVM achieves an overall accuracy of 88.9%. In comparison, the k-nearest neighbors algorithm (KNN) yields an accuracy of 75.2%. System Reliability. In this evaluation, we examine the reliability of SensEmo when the users are presented with instantaneous stimuli, i.e., video and music  [32] . Specifically, we gather physiological data while users are exposed to video stimuli in the form of music videos that incorporate elements of strong emotional stimuli. It is important to note that these videos are compilations of multiple graphics sourced from the IAPS dataset, which elicit various emotions at different times. The same set of videos is utilized for all users in order to evaluate if similar patterns of physiological signals emerge across different individuals in response to the same stimuli. Our findings reveal that the response patterns of GSR and HRV exhibit similarities among different users when experiencing the same emotion. These results demonstrate that not only can we rely on patterns of physiological signals to infer emotions, but SensEmo also reliably captures and records such data. Furthermore, through personalized calibration, we mitigate differences in the physiological signal responses of different users and enhance the accuracy of emotion recognition. Ablation study on personalized calibration. Figures 6 shows the confusion matrices obtained with and without personalized calibration, respectively. Across all emotions, there is a significant and noticeable increase in the true positive rate. Additionally, the overall recognition accuracy improves from 27.4% to 88.9% with the implementation of personalized calibration. Throughout our experiments, we observed instances of incorrect and unstable classification of user emotional states when personalized calibration was not applied, even when the user's emotions remained consistent. These findings suggest that personalized calibration improves robustness of SensEmo. It is worth noting that similar outcomes were observed for all the selected features. These results underscore the importance of personalized calibration in standardizing feature extraction and enhancing emotional recognition.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Reinforcement Learning-Based Controller Evaluation",
      "text": "The role of the reinforcement learning-based controller is to adjust the system according to the students' current emotional state. It is essential for the controller to respond promptly and efficiently, ensuring it can adapt to the right moment. Consequently, the response time of the feedback control system, and consequently, the entire system, should be minimized. In addition to response time, another crucial factor is the acceptance of the feedback system by the students. It is important that the students perceive the feedback system positively and find it valuable in their learning experience. Response Time. We evaluate the response time for the adaptation decision inference using the emotion recognition results. In this analysis, we also take into consideration the time required by the emotion recognition model. By examining the response time, we gain insights into how long it takes for the system to provide a useful suggestion or adaptation once the user's data becomes available. To measure the response time, we utilize software timers. These experiments involve using real lecture slides obtained from the MIT OpenCourse-Ware  [33] , where no adaptation is provided to users but only recorded for the purpose of analysis. After analyzing the results, we find that the average response time of the feedback system is approximately 3.1 seconds, which is significantly shorter than the duration of any individual lecture slide. System Acceptance. The purpose of the feedback is to enhance the student's learning experience with minimal effort on their part. To evaluate the effectiveness of the feedback system, we introduce another parameter: the number of manual interventions made by the student to adjust the pace or content. This parameter serves as a measure of feedback acceptance, where a lower number of interventions indicates better performance of the feedback system. We track these interventions using a software counter. Our experiments show that, on average, students only make approximately 2.2 interventions per minute. This suggests that the feedback system effectively assists students in maintaining a suitable teaching pace and content, minimizing the need for frequent manual interventions.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "C. Resource Utilization Study",
      "text": "We evaluate the energy consumption and memory usage of SensEmo. In our experiments, the estimated battery life for a Microsoft Band 2 with the SensEmo app connected is 5 hours, which exceeds the duration of a typical class session. Regarding memory usage, the Android application requires only 3.28 MB of internal memory space. Furthermore, the average runtime memory space utilized by the application is a mere 215 KB. In addition, Table  II  shows a detailed breakdown of power usage while the app runs on Android devices.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "D. Real-World Learning Scenarios",
      "text": "In this section, we conducted real-world experiments to evaluate SensEmo 's impact on learning outcomes in both online remote learning and in-person classroom settings. For the online remote learning, 22 graduate students were randomly divided into two groups of 11. One group used SensEmo while the other did not. All students wore identical smartwatches and viewed lecture slides from four different lectures (30-50 minutes each) on topics ranging from biology to geography, without any prior knowledge. Their physiological data was recorded, and comprehension was tested with a quiz of 10 concept questions at the end. In the in-person classroom setting, 10 graduate students participated in two sessions, wearing identical smartwatches. In the first session, the instructor did not receive real-time feedback. In the second session, the same instructor received real-time feedback from SensEmo and adjusted the lecture content and pace based on students' emotions. Both sessions concluded with a quiz of 5 conceptual questions. Students also completed a survey about their emotional states during the sessions. Learning outcome. For the online setting, students using SensEmo scored 40.0% higher on the quiz compared to those who did not. Emotion recognition analysis showed longer durations of curiosity among SensEmo users, suggesting significant potential learning impact, shown in Figure  7  (left). This suggests that even a modest increase in the time spent experiencing the desired emotion can significantly influence learning outcomes. Also, SensEmo's adaptability in teaching pace and content further contributed to improved learning performance. In the classroom setting, students exhibited more curiosity and less boredom in the session with real-time feedback, shown in Figure  7  (middle). Although confusion increased, it might indicate active learning. Quiz scores, shown in Figure  7  (right), were higher in the second session, hinting at SensEmo's positive impact despite non-randomization and material control issues. Students also reported that wearing the smartwatch was neither distracting nor inconvenient.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "In this paper, we present SensEmo, the first affective learning system that uses real-time data from physiological sensors available in commercial smartwatches. It achieves an average of 88.9% accuracy in inferring users' emotions based on valence and arousal scales, within learning environments. By utilizing sensed emotional states, SensEmo enables the adaptation of teaching materials and pace. The effectiveness of this adaptability has been evaluated in both online remote learning for individual users and in-person classroom settings with multiple users. Our findings suggest that integrating wearable sensing into affective learning systems has the potential to enhance learning outcomes when compared to traditional approaches.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Fig. 1: SensEmo uses physiological signals to personalize the",
      "page": 2
    },
    {
      "caption": "Figure 2: Overview of the affective learning feedback controller,",
      "page": 2
    },
    {
      "caption": "Figure 2: , includes emotion",
      "page": 2
    },
    {
      "caption": "Figure 3: shows the valence-",
      "page": 3
    },
    {
      "caption": "Figure 3: The 2-dimensional valence-arousal space and mapping",
      "page": 3
    },
    {
      "caption": "Figure 4: The discrete MDP representation of SensEmo.",
      "page": 3
    },
    {
      "caption": "Figure 4: , comprising states (bored,",
      "page": 3
    },
    {
      "caption": "Figure 5: Convergent asymptotic behavior of the MDP.",
      "page": 4
    },
    {
      "caption": "Figure 5: shows a convergence sample of the Markov chain’s",
      "page": 4
    },
    {
      "caption": "Figure 6: Emotion recognition confusion matrices of SensEmo",
      "page": 5
    },
    {
      "caption": "Figure 6: (left). The experimental",
      "page": 5
    },
    {
      "caption": "Figure 7: Comparison of various aspects in online and in-person classroom settings. (Left) Students using SensEmo exhibit a",
      "page": 6
    },
    {
      "caption": "Figure 7: (middle). Although confusion",
      "page": 6
    },
    {
      "caption": "Figure 7: (right), were higher in the second session, hinting",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Policy Based on Value Iteration": "Optimal Policy π(s)\nSub-optimal Policy π∗(s)"
        },
        {
          "Policy Based on Value Iteration": "Enriching content\nSimplifying content"
        },
        {
          "Policy Based on Value Iteration": "Making no change\nDecreasing pace"
        },
        {
          "Policy Based on Value Iteration": "Simplifying content\nDecreasing pace"
        },
        {
          "Policy Based on Value Iteration": "Decreasing pace\nEnriching content"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A mobile gamification learning system for improving the learning motivation and achievements",
      "authors": [
        "C.-H Su",
        "C.-H Cheng"
      ],
      "year": "2015",
      "venue": "Journal of Assisted Learning"
    },
    {
      "citation_id": "2",
      "title": "AdELE: A framework for adaptive e-learning through eye tracking",
      "authors": [
        "V Barrios",
        "C Gütl",
        "A Preis",
        "K Andrews",
        "M Pivec",
        "F Mödritscher",
        "C Trummer"
      ],
      "year": "2004",
      "venue": "Proceedings of IKNOW"
    },
    {
      "citation_id": "3",
      "title": "Emotion recognition using PHOG and LPQ features",
      "authors": [
        "A Dhall",
        "A Asthana",
        "R Goecke",
        "T Gedeon"
      ],
      "year": "2011",
      "venue": "2011 IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "4",
      "title": "Emotion recognition through speech",
      "authors": [
        "A Utane",
        "S Nalbalwar"
      ],
      "year": "2013",
      "venue": "International Journal of Applied Information Systems (IJAIS)"
    },
    {
      "citation_id": "5",
      "title": "mDB: Monitoring dysfunctional behaviors for patients with bipolar disorder",
      "authors": [
        "K Choksi",
        "S Nagaraj",
        "R Thielke",
        "S Lin"
      ],
      "year": "2020",
      "venue": "42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society"
    },
    {
      "citation_id": "6",
      "title": "EDBL-algorithm for detection and analysis of emotion using body language",
      "authors": [
        "S Singh",
        "V Sharma",
        "K Jain",
        "R Bhall"
      ],
      "year": "2015",
      "venue": "1st International Conference on Next Generation Computing Technologies (NGCT)"
    },
    {
      "citation_id": "7",
      "title": "The autonomic nervous system and emotion",
      "authors": [
        "R Levenson"
      ],
      "venue": "Emotion Review"
    },
    {
      "citation_id": "8",
      "title": "International affective picture system (IAPS): Technical manual and affective ratings",
      "authors": [
        "P Lang"
      ],
      "year": "1995",
      "venue": "The Center for Research in Psychophysiology"
    },
    {
      "citation_id": "9",
      "title": "Emotion recognition from physiological signals using wireless sensors for presence technologies",
      "authors": [
        "F Nasoz",
        "K Alvarez",
        "C Lisetti",
        "N Finkelstein"
      ],
      "year": "2004",
      "venue": "Cognition, Technology & Work"
    },
    {
      "citation_id": "10",
      "title": "StressSense: Detecting stress in unconstrained acoustic environments using smartphones",
      "authors": [
        "H Lu",
        "D Frauendorfer",
        "M Rabbi",
        "M Mast",
        "G Chittaranjan",
        "A Campbell",
        "D Gatica-Perez",
        "T Choudhury"
      ],
      "year": "2012",
      "venue": "Proceedings of the 2012 ACM Conference on Ubiquitous Computing"
    },
    {
      "citation_id": "11",
      "title": "Emotion recognition from text using semantic labels and separable mixture models",
      "authors": [
        "C.-H Wu",
        "Z.-J Chuang",
        "Y.-C Lin"
      ],
      "year": "2006",
      "venue": "ACM transactions on Asian language information processing"
    },
    {
      "citation_id": "12",
      "title": "Identifying emotional states using keystroke dynamics",
      "authors": [
        "C Epp",
        "M Lippold",
        "R Mandryk"
      ],
      "year": "2011",
      "venue": "Proceedings of the SIGCHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "13",
      "title": "Inclusion of respiratory frequency information in heart rate variability analysis for stress assessment",
      "authors": [
        "A Hernando",
        "J Lazaro",
        "E Gil",
        "A Arza",
        "J Garzón",
        "R Lopez-Anton",
        "C De La Camara",
        "P Laguna",
        "J Aguiló",
        "R Bailón"
      ],
      "year": "2016",
      "venue": "IEEE journal of biomedical and health informatics"
    },
    {
      "citation_id": "14",
      "title": "Wearable biosensor technology in education: A systematic review",
      "authors": [
        "M Hernández-Mustieles",
        "Y Lima-Carmona",
        "M Pacheco-Ramírez",
        "A Mendoza-Armenta",
        "J Romero-Gómez",
        "C Cruz-Gómez",
        "D Rodríguez-Alvarado",
        "A Arceo",
        "J Cruz-Garza",
        "M Ramírez-Moreno"
      ],
      "year": "2024",
      "venue": "Sensors"
    },
    {
      "citation_id": "15",
      "title": "Using machine learning to train a wearable device for measuring students' cognitive load during problem-solving activities based on electrodermal activity, body temperature, and heart rate: Development of a cognitive load tracker for both personal and classroom use",
      "authors": [
        "W Romine",
        "N Schroeder",
        "J Graft",
        "F Yang",
        "R Sadeghi",
        "M Zabihimayvan",
        "D Kadariya",
        "T Banerjee"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "16",
      "title": "Tracking depression dynamics in college students using mobile phone and wearable sensing",
      "authors": [
        "R Wang",
        "W Wang",
        "A Dasilva",
        "J Huckins",
        "W Kelley",
        "T Heatherton",
        "A Campbell"
      ],
      "year": "2018",
      "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
    },
    {
      "citation_id": "17",
      "title": "Emotion assessment from physiological signals for adaptation of game difficulty",
      "authors": [
        "G Chanel",
        "C Rebetez",
        "M Bétrancourt",
        "T Pun"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems and Humans"
    },
    {
      "citation_id": "18",
      "title": "Estimation of behavioral user state based on eye gaze and head pose-application in an e-learning environment",
      "authors": [
        "S Asteriadis",
        "P Tzouveli",
        "K Karpouzis",
        "S Kollias"
      ],
      "year": "2009",
      "venue": "Estimation of behavioral user state based on eye gaze and head pose-application in an e-learning environment"
    },
    {
      "citation_id": "19",
      "title": "Magtrack: Enabling safe driving monitoring with wearable magnetics",
      "authors": [
        "H Huang",
        "H Chen",
        "S Lin"
      ],
      "year": "2019",
      "venue": "Proceedings of the 17th annual international conference on mobile systems, applications, and services"
    },
    {
      "citation_id": "20",
      "title": "EmoMarker: A privacy-preserving, multi-modal sensing system for dyadic digital biomarkers of expressed emotions for patients with dementia",
      "authors": [
        "Y Li",
        "D Yu",
        "S Chen",
        "G Xing",
        "H Chen"
      ],
      "year": "2024",
      "venue": "Proceedings of the 22nd Annual International Conference on Mobile Systems, Applications and Services"
    },
    {
      "citation_id": "21",
      "title": "DrHouse: An LLM-empowered diagnostic reasoning system through harnessing outcomes from sensor data and expert knowledge",
      "authors": [
        "B Yang",
        "S Jiang",
        "L Xu",
        "K Liu",
        "H Li",
        "G Xing",
        "H Chen",
        "X Jiang",
        "Z Yan"
      ],
      "year": "2024",
      "venue": "DrHouse: An LLM-empowered diagnostic reasoning system through harnessing outcomes from sensor data and expert knowledge",
      "arxiv": "arXiv:2405.12541"
    },
    {
      "citation_id": "22",
      "title": "The relationship between heart rate reactivity, emotionally aggressive behavior, and general violence in batterers",
      "authors": [
        "J Gottman",
        "N Jacobson",
        "R Rushe",
        "J Shortt"
      ],
      "year": "1995",
      "venue": "Journal of family psychology"
    },
    {
      "citation_id": "23",
      "title": "Comparison of Min-Max normalization and Z-score normalization in the K-nearest neighbor (kNN) algorithm to test the accuracy of types of breast cancer",
      "authors": [
        "H Henderi",
        "T Wahyuningsih",
        "E Rahwanto"
      ],
      "year": "2021",
      "venue": "International Journal of Informatics and Information Systems"
    },
    {
      "citation_id": "24",
      "title": "Is heart rate variability (HRV) an adequate tool for evaluating human emotions?-a focus on the use of the International Affective Picture System (IAPS)",
      "authors": [
        "K.-H Choi",
        "J Kim",
        "O Kwon",
        "M Kim",
        "Y Ryu",
        "J.-E Park"
      ],
      "year": "2017",
      "venue": "Psychiatry research"
    },
    {
      "citation_id": "25",
      "title": "The emotion probe: studies of motivation and attention",
      "authors": [
        "P Lang"
      ],
      "year": "1995",
      "venue": "American psychologist"
    },
    {
      "citation_id": "26",
      "title": "The circumplex model of affect: An integrative approach to affective neuroscience, cognitive development, and psychopathology",
      "authors": [
        "J Posner",
        "J Russell",
        "B Peterson"
      ],
      "year": "2005",
      "venue": "Development and psychopathology"
    },
    {
      "citation_id": "27",
      "title": "An affective model of interplay between emotions and learning: Reengineering educational pedagogy-building a learning companion",
      "authors": [
        "B Kort",
        "R Reilly",
        "R Picard"
      ],
      "year": "2001",
      "venue": "An affective model of interplay between emotions and learning: Reengineering educational pedagogy-building a learning companion"
    },
    {
      "citation_id": "28",
      "title": "A fuzzy physiological approach for continuously modeling emotion during interaction with play technologies",
      "authors": [
        "R Mandryk",
        "M Atkins"
      ],
      "year": "2007",
      "venue": "International journal of human-computer studies"
    },
    {
      "citation_id": "29",
      "title": "12 brain/mind learning principles in action: Teach for the development of higher-order thinking and executive function",
      "authors": [
        "R Caine",
        "G Caine",
        "C Mcclintic",
        "K Klimek"
      ],
      "year": "2015",
      "venue": "12 brain/mind learning principles in action: Teach for the development of higher-order thinking and executive function"
    },
    {
      "citation_id": "30",
      "title": "Non-negative matrices and Markov chains",
      "authors": [
        "E Seneta"
      ],
      "venue": "Non-negative matrices and Markov chains"
    },
    {
      "citation_id": "31",
      "title": "Science & Business Media",
      "year": "2006",
      "venue": "Science & Business Media"
    },
    {
      "citation_id": "32",
      "title": "An analytical system for user emotion extraction, mental state modeling, and rating",
      "authors": [
        "Z Shao",
        "R Chandramouli",
        "K Subbalakshmi",
        "C Boyadjiev"
      ],
      "year": "2019",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "33",
      "title": "Predicting audience responses to movie content from electro-dermal activity signals",
      "authors": [
        "F Silveira",
        "B Eriksson",
        "A Sheth",
        "A Sheppard"
      ],
      "year": "2013",
      "venue": "Proceedings of the 2013 ACM international joint conference on Pervasive and ubiquitous computing"
    },
    {
      "citation_id": "34",
      "title": "",
      "authors": [
        "\" Mit Opencourseware"
      ],
      "venue": ""
    }
  ]
}