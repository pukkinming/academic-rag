{
  "paper_id": "2501.08809v1",
  "title": "Xmusic: Towards A Generalized And Controllable Symbolic Music Generation Framework",
  "published": "2025-01-15T14:08:44Z",
  "authors": [
    "Sida Tian",
    "Can Zhang",
    "Wei Yuan",
    "Wei Tan",
    "Wenjie Zhu"
  ],
  "keywords": [
    "AIGC",
    "Music Generation",
    "Multi-Modal Parsing",
    "Music Quality Assessment",
    "Large-Scale Dataset"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In recent years, remarkable advancements in artificial intelligence-generated content (AIGC) have been achieved in the fields of image synthesis and text generation, generating content comparable to that produced by humans. However, the quality of AI-generated music has not yet reached this standard, primarily due to the challenge of effectively controlling musical emotions and ensuring high-quality outputs. This paper presents a generalized symbolic music generation framework, XMusic, which supports flexible prompts (i.e., images, videos, texts, tags, and humming) to generate emotionally controllable and high-quality symbolic music. XMusic consists of two core components, XProjector and XComposer. XProjector parses the prompts of various modalities into symbolic music elements (i.e., emotions, genres, rhythms and notes) within the projection space to generate matching music. XComposer contains a Generator and a Selector. The Generator generates emotionally controllable and melodious music based on our innovative symbolic music representation, whereas the Selector identifies high-quality symbolic music by constructing a multi-task learning scheme involving quality assessment, emotion recognition, and genre recognition tasks. In addition, we build XMIDI, a large-scale symbolic music dataset that contains 108,023 MIDI files annotated with precise emotion and genre labels. Objective and subjective evaluations show that XMusic significantly outperforms the current stateof-the-art methods with impressive music quality. Our XMusic has been awarded as one of the nine Highlights of Collectibles at WAIC 2023. The project homepage of XMusic is: https://xmusicproject.github.io.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "A RTIFICIAL intelligence (AI) techniques have signif- icantly advanced the field of AI Generated Content (AIGC), making it a prominent research area in recent years. AIGC fosters creativity, exploration, and innovation across diverse artistic domains. As an art form centered on sound, music is a significant component of AIGC. Automatic music generation has numerous potential applications, including adaptive soundtracks, video background music generation, music transcription, and royalty-free music creation, etc. Although recent studies (such as AudioLM  [1] , MusicLM  [2] , Riffusion  [3] , MusicGen  [4]  and Noise2Music  [5] , etc) have achieved success in terms of generating music within the audio domain, editing such generated music in its audio format remains challenging and unintuitive. In contrast, symbolic music, typically represented in MIDI format, offers greater flexibility, enabling users to modify specific musical elements explicitly. Thus, this paper focuses on music generation within the symbolic domain.\n\nAll the authors are with Tencent. ˇ\" ) : Equal contribution. It contains two essential components: XProjector and XComposer. XProjector parses various input prompts into specific symbolic music elements. These elements then serve as control signals, guiding the music generation process within the Generator of XComposer. Additionally, XComposer includes a Selector that evaluates and identifies high-quality generated music. The Generator is trained on our large-scale dataset, XMIDI, which includes precise emotion and genre labels.\n\nSymbolic music generation methods primarily aim to model the temporal dependencies within music, predicting subsequent musical events based on prior ones. The Transformer is a natural fit for this sequence-to-sequence task while handling long-range dependencies. Recent advancements have demonstrated the remarkable potential of Transformer models  [6] ,  [7]  in symbolic music generation. The Music Transformer by Huang et al.  [8]  shows the first successful application of the self-attention mechanism for generating long symbolic music. The Pop Music Transformer  [9]  incorporates the Transformer-XL  [7]  architecture to generate symbolic pop music with an enhanced rhythmic structure. Another influential contribution is the Compound Word Transformer  [10] , which explores novel and efficient tokenization techniques for symbolic music training.\n\nConditional symbolic music generation has attracted significant attention due to its ability to leverage user-supplied information as a \"prompt\" for producing unique musical compositions. Existing conditional methods explore various types of prompts, including attribute tags (e.g., emotions  [11] ,  [12] , genres  [13] , and instruments  [14] ), sequential data (e.g., lead sheets  [15] , motifs  [16] , and melodies  [8] ), and multimedia data (e.g., performance footage  [17] ,  [18]  and general videos  [19] ,  [20] ). Despite considerable advancements in the field of conditional symbolic music generation, the integration of diverse prompt types (such as images, videos, texts, tags, and humming) within a single generative model remains unexplored.\n\nIn this work, we aim to build a generalized, controllable and high-quality framework, referred to as XMusic, for symbolic music generation. We address the challenges associated with this goal from four perspectives: input, representation, assessment and data, each described as follows:\n\n1) Input: multi-modal parsing. A versatile framework should support various multi-modal prompts as inputs. Given the inherent differences among multi-modal data, the primary challenge in multi-modal prompt parsing lies in effectively processing and extracting musical information from heterogeneous data sources. To address this, we propose a multimodal prompt parsing method, termed XProjector. This projector contains a novel projection space for symbolic music elements, serving as a bridge between diverse multi-modal prompts and core musical elements. In XProjector, multiple prompt types (i.e., images, videos, texts, tags, and humming) are analyzed and mapped to specific musical elements (i.e., emotions, genres, rhythms, and notes), as shown in Fig.  1 . For instance, temporally-related prompts, such as videos and humming, are translated into rhythm elements to maintain temporal consistency. Emotional prompts, such as images, videos, and texts, are mapped to corresponding emotional elements to ensure the generated music accurately conveys the intended emotions. This approach harmonizes multi-modal prompts by translating them into a unified projection space of musical elements.\n\n2) Representation: precise control. An optimal representation should contain fine-grained, type-specific musical elements (such as emotions, rhythms, and genres), facilitating accurate, efficient, and controllable music generation. In this paper, we build our music representation based on compound words  [10]  with enhancements to musical elements. Specifically, the family tokens of our representation include note-related, rhythm-related, tag-related and instrument-related tokens. The tag-related tokens provide control over emotions and genres, while the instrument-related tokens (program) distinguish between different instrument tracks. With this representation, our music generator can efficiently produce coherent, melodious, and harmonious compositions aligned with the control signals generated by XProjector.\n\n3) Assessment: high-quality music selection. Prevailing methods generate final music outputs in a single pass, often resulting in inconsistent quality. Post-hoc music quality assessment is crucial yet overlooked in existing approaches. Automatically evaluating and selecting high-quality generated music is essential for ensuring superior outcomes. To this end, we propose a Selector that identifies high-quality music via a multi-task learning scheme, as shown in Fig.  1 . Recognizing that emotions, genres, and quality are global semantic concepts, we concurrently train the model on emotion recognition, genre recognition, and quality assessment tasks. This multitask approach promotes knowledge transfer across these tasks, enhancing the model's ability to assess quality by leveraging insights from emotion and genre recognition. As a result, our approach achieves reliable quality assessment performance with a minimal number of annotated samples.\n\n4) Data: large-scale dataset. High-quality, large-scale sym-bolic music datasets with fine-grained emotion and genre annotations are scarce and challenging to collect. To address this gap, we construct XMIDI, a large-scale dataset comprising 108,023 MIDI files with precise and diverse emotion and genre labels. The XMIDI dataset is approximately 10 times larger than the previous largest emotion-labeled dataset ELMG  [12]  in terms of song size, as shown in Table  III .\n\nOur main contributions are summarized as follows:\n\n• We introduce a multi-modal controllable framework, termed XMusic, for symbolic music generation. XMusic supports various types of prompts (i.e., images, videos, texts, tags, and humming) as inputs and generates emotionally controllable, high-quality music tailored to the provided prompts.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Artificial Intelligence Generated Content (Aigc)",
      "text": "AIGC aims to utilize AI technology to automate content production while addressing human individual requirements. Recently, AIGC has demonstrated significant potential in generating high-quality content that closely resembles humangenerated content (HGC), particularly in the areas of text generation  [21] -  [23]  and image synthesis  [24] -  [26] . Despite recent progress, the field of music generation remains relatively underexplored within the AIGC community. AIgenerated music still lacks the emotional depth and melodic richness typically found in human-composed music pieces.\n\nRecent studies  [1] -  [5] ,  [27]  have focused on generating audiobased music from textual inputs. However, audio-based music generation faces challenges, such as limited editability and the inability to finely control attributes like tempo, pitch, duration, and rhythm. In contrast, symbolic music, which represents musical ideas through notation, offers more flexible and precise control over these attributes. Therefore, this paper focuses on music generation in symbolic domain. We present XMusic, a universal symbolic music generation framework that supports flexible prompts, and XMIDI, a large-scale symbolic music dataset annotated with precise emotion and genre labels.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Symbolic Music Representations",
      "text": "Conventional symbolic music representations can be classified into two main categories: image-like  [28] ,  [29]  and MIDI-like  [8] ,  [30] ,  [31]  representations. The image-like representation utilizes a 2D matrix, such as a binary piano roll  [32] ,  [33] , to indicate the presence of notes at each time position. Subsequently, convolutional operations are applied for music generation. In contrast, the MIDI-like representation encodes music as a sequence of events that evolve over time. Transformers  [6] ,  [7]  are then utilized to capture the temporal dependencies between musical events. Nonetheless, MIDIlike representations possess inherent limitations in modeling music rhythm structure. REMI  [9]  addresses this by organizing input data into a metrical structure, i.e., introducing positional elements such as bar and beat events, along with supportive musical information like tempo and chord events. Empirical evidence suggests that this approach improves the rhythmic regularity of the generated music. Compound Words  [10]  further groups the REMI tokens by note type and metric type, significantly reducing token sequence length, thereby accelerating training and inference times. A recent study, SDMuse  [34] , demonstrates the effectiveness of hybrid representations, leveraging the complementary strengths of imagelike and MIDI-like representations for music editing and generation.\n\nIn this paper, we construct our music representation following the Compound Words structure with several crucial enhancements. We introduce two new family tokens: i) tagrelated tokens (emotion, genre) to control emotional expression and musical style; and ii) instrument-related tokens (program) to facilitate the generation of multi-track music featuring diverse instruments.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "C. Symbolic Music Generation",
      "text": "Given our interest in improving the versatility, controllability, and quality of symbolic music generation, it is crucial to review and compare various generation methods. We categorize these methods into five groups: unconditioned, tagconditioned, sequence-conditioned, video-conditioned, and Xconditioned (our approach).\n\n1) Unconditioned Symbolic Music Generation: Unconditioned methods generate music from scratch using a random seed, without any specific constraints or additional input. The primary challenge lies in ensuring long-term structural coherence as the music length increases. Researchers focus on enhancing the overall repetitive structure of the generated music using various models, such as Transformer-based architectures  [16] ,  [35] , RNN-based models  [36] ,  [37] , or optimization-based approaches  [38] . In contrast, conditional music generation has gained popularity in recent years, as it enables users to guide the generation process to produce unique musical compositions.\n\n2) Tag-conditioned Symbolic Music Generation: Tagconditioned methods involve conditioning the generation on high-level tags such as instrument, genre or emotion. For instance, MuseNet  [13]  can generate music based on a specific set of instruments and a particular musical style. GTR-CTRL  [14]  presents methods to condition Transformer-based models to generate guitar tabs based on the desired instrument and genre. EMOPIA  [11]  is an emotion-labeled symbolic music dataset comprising 1,078 music clips from 387 songs, facilitating research on emotion-conditioned symbolic music generation  [12] ,  [15] ,  [39] .\n\n3) Sequence-conditioned Symbolic Music Generation: These methods typically employ a conditioning sequence as a prior to generate a continuation or extension accordingly. Standard sequence prompts for music generation include lead sheets, motifs, melodies, themes, and lyrics, which can be directly extracted from musical pieces to form training pairs. Huang et al.  [8]  demonstrate the first successful application of Transformers to produce accompaniments conditioned on melodies. MELONS  [16]  is another transformerbased framework that generates full-song melodies with longterm structures given motifs. MGM  [40]  learns motif-level repetitions and integrates them into the music generation process. Theme Transformer  [41]  introduces a theme-based conditioning approach that compels the model to manifest the given theme multiple times in its resulting generation. UP-Transformer  [42]  focuses on user preference-based music transfer, utilizing a single piece of a user's favorite music as the condition for transferring musical styles. The relationship between melody and lyrics is crucial for symbolic music generation. Yu et al.  [43]  propose a conditional LSTM-GAN model that generates melodies from lyrics by leveraging the syntactic structures of the lyrics through LSTM networks. This approach ensures the generated sequences align with the lyrics and mimic the distribution of real melody samples. Zhang et al.  [44]  present a novel Transformer-based approach to generate syllable-level lyrics from melodies, employing an explicit n-gram (EXPLING) loss function and a prior attention mechanism to improve sequence alignment and controllable lyric generation. Duan et al.  [45]  address the challenge of interpretability in melody generation from lyrics by integrating mutual information constraints and Transformer-based semantic feature extraction.\n\n4) Video-conditioned Symbolic Music Generation: Most previous methods for video-conditioned music generation focus on composing music from silent performance videos  [17] ,  [18] , a process akin to visual music transcription. The instrument type and rhythm can be inferred from visual cues (e.g., performance venue, musician's actions, etc), which limits music diversity to some extend. Recent methods  [46] ,  [47]  take dance or human action videos as input, generating music pieces that plausibly match the corresponding visual input. However, these methods cannot be applied to general videos as they rely on additional keypoint annotations. CMT  [19]  generates background music for general videos by establishing rule-based rhythmic video-music relationships. To mitigate potential style conflicts caused by this rule-based design, V-MusProd  [20]  introduces semantic-level correspondence. This approach decouples music generation into three progressive stages (chords, melody, and accompaniment) and extracts video-music relational features (semantic, color, motion) for guidance.\n\n5) X-conditioned Symbolic Music Generation: This paper introduces the X-conditioned framework, where X represents various types of prompts, such as images, videos, text, tags, and humming. Our XMusic is a multi-modal controllable symbolic music generation framework designed to support versatile prompts.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Method",
      "text": "Our proposed XMusic supports various types of content as prompts for generating high-quality music. As shown in Fig.  2 , the process is divided into three stages: parsing, control, and selection. First, XProjector (Sec. III-A) analyzes the input content and parses it into symbolic music elements within the projection space. Second, XComposer (Sec. III-B) maps these elements to token sequences and controls the Generator to generate corresponding music. Finally, the Selector evaluates the quality of the generated music batches and selects the one with the highest quality score. The symbolic music dataset XMIDI is introduced in Sec. III-C.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Xprojector",
      "text": "The projection space of symbolic music elements, denoted as P, acts as a bridge between multi-modal content and symbolic music. This space includes four types of symbolic music elements, emotions (P E ), genres (P G ), rhythms (P R ), and notes (P N ), represented as {P E , P G , P R , P N } ∈ P.\n\nThe emotion element P E ∈ R D E and the genre element P G ∈ R D G are expressed as one-hot vectors, where D E and D G denote the number of emotion and genre categories, respectively. In this paper, P E can be chosen from 11 emotions: exciting, warm, happy, romantic, funny, sad, angry, lazy, quiet, fear, and magnificent. Similarly, P G offers 6 common genre choices: rock, pop, country, jazz, classical, and folk.\n\nThe rhythm element spans the range of bars and is represented as P R = {p r i } N Bar i=1 , where N Bar denotes the total number of bars. p r i represents the rhythmic component of the i-th bar and can be expanded as {p bar i , p beat i1 , ..., p beat in }, with i n indicating the number of beats within the bar. Specifically, the bar element p bar = (bar, density) ∈ R 2 records the starting position and note density of the current bar, while the beat element p beat = (beat, tempo, strength) ∈ R 3 captures the starting position, tempo and intensity of the beat.\n\nThe note element covers the range of a single note and is defined as\n\n, where N N ote denotes the number of notes in the sequence. Each note element p n = (pitch, duration, velocity) ∈ R 3 represents the pitch, duration, and velocity of the note.\n\nWhen prompts from various modalities are input, specific symbolic music elements are activated, guiding the matching music generation process. As shown in Table  I , inputs such as videos, images, or text with emotional tendencies activate the emotion element. Similarly, inputs containing temporal information, like videos or humming, activate the rhythm element.\n\nXProjector, as the core component of XMusic, analyzes multi-modal content and maps it into symbolic music elements within the projection space. The associated mapping function is denoted as F XP .\n\nImage prompts, characterized by their non-sequential nature, guide the music generation process by controlling the overall properties of the sequence. Specifically, XProjector performs sentiment analysis on the input image to determine its dominant emotion category and activates the corresponding emotion element within the projection space. This mechanism guides the generation of music aligned with the detected emotion. The image sentiment analysis module computes an emotion score S e (image) for the input image and selects the emotion with the highest score. The calculation is as follows:\n\nwhere E denotes the set of emotion categories. XProjector employs two models for this calculation. The first model is the well-established deep convolutional neural network ResNet  [48] . Specifically, we utilize the ResNet-50 architecture to train an image emotion classifier on our large-scale image emotion dataset (details in Sec. IV-A). The classifier outputs the probability S e ResNet (image) for each emotion e. The second model leverages CLIP  [49] , a prominent imagetext pre-training model. We compute the embedding similarity between the input image and synonymous textual descriptions of each emotion e. These similarities are then normalized via the Softmax function to derive S e CLIP (image). Weight factors λ 1 and λ 2 balance the contributions of the two models, with values set to λ 1 = 1 and λ 2 = 2 in our implementation.\n\nText prompts, inherently sequential data, are processed similarly to image-conditioned inputs. XProjector performs sentiment analysis to identify the dominant emotion in the input text and activates the corresponding element within the projection space. The text sentiment analysis module employs the SentenceTransformer  [50]  model to calculate embedding similarities between the input text and synonymous descriptions of each emotion e. These similarities are then normalized via the Softmax function to produce an emotion score S e (text) as follows:\n\nIn tag-conditioned music generation, users can select from 11 emotion tags and 6 genre tags to guide the process. Once a tag is chosen, XProjector activates the corresponding emotion or genre element within the projection space:\n\nVideo prompts, which are spatio-temporal data, guide both global and local attributes of the music sequence. For videoconditioned music generation, XProjector analyzes sentiment, motion, and scene transitions within the input video, mapping these factors to the appropriate rhythm and emotion elements in the projection space. This ensures a high degree of synchronization between the generated music and the video content.\n\nWe observe a significant correlation between video background music tempo and scene transition frequency. For example, montage videos with rapid scene transitions typically feature fast-paced music, while peaceful scenery videos are often paired with slower tempos. To formalize this relationship, we introduce a scene transition rate metric R scene to control the music tempo t music :\n\nHere, N scene denotes the total number of scene transitions (computed using PySceneDetect  [51] ), while T video represents the video duration in seconds. The music tempo t music (measured in bpm) is derived from R scene , with an initial tempo t init and incremental tempo t inc . The tanh activation function ensures that the coefficient for t inc remains between 0 and 1. Our analysis of tempo distributions in the training dataset shows that 98.6% of musical tempos fall within the 60∼130 bpm range. Accordingly, we set t init = 60 and t inc = 70 to keep generated tempos within this range.\n\nRegarding emotions, we determine the emotional category of the input video through sentiment analysis and activate the corresponding emotion element within the projection space to control the emotional style of the music. The video sentiment analysis module computes an emotion score S e (video) for the video and selects the emotion with the highest score as the analysis result. The calculation formula is as follows:\n\nHere, we uniformly sample N ipb frames per bar and compute an emotion score S e (image) for each frame. These scores are averaged to obtain a bar-level emotion score S e (bar). Given N bpb , the number of beats per bar, and using the video duration T video along with the music tempo t music , we calculate the total number of music bars N bar . Averaging S e (bar) across all N bar bars yields the final video emotion score S e (video). In this paper, N bpb is set to 4, and N ipb is set to 8.\n\nInspired by CMT  [19] , which establishes a correlation between fast motion and dense notes, we control the local music rhythm using video motion information. Specifically, XProjector extracts video motion information by calculating the optical flow flow t (x, y). Due to the high computational complexity of optical flow, we use a more efficient PA  [52]  to model video motion. This average optical flow intensity within each bar, along with the visual beat saliency  [53]  for each beat, is mapped to note density and beat strength, respectively. Note that we also preserve their percentile distribution (based on the training set statistics)  [19]  within the projection space. The calculation formula is:\n\nThus, the complete mapping relationship is expressed as follows:\n\nwhere i = 1, 2, ..., N bar and j = 1, 2, ..., N bpb .\n\nHumming prompts, which are sequential data, guide the generation process of complete music by first being transcribed into an initial MIDI sequence. XProjector employs the VOCANO  [54]  algorithm to transcribe input humming audio into an original MIDI sequence, i.e., M origin = VOCANO(humming). This sequence is then processed using standardization operations (beat processing and note quantization) to create a standard prior MIDI sequence, i.e., M std = Standardize(M origin ). For beat processing, the tempo of each beat is derived from the time intervals between adjacent beats, adjusting the default tempo of 120 bpm in the transcribed sequence to the actual tempo. Note quantization adjusts the onset and offset positions of each note to the nearest 32nd note position. Finally, information from M std is organized and mapped to the corresponding note and rhythm elements within the projection space to generate the subsequent music sequences. The detailed formulas are as follows:\n\nwhere i = 1, 2, ..., N bar , j = 1, 2, ..., N bpb , and D and S denote the calculation formulas for note density and beat strength, respectively. T M std beat is the fixed beat length in M std , measured in seconds. M bari std , M beati;j std\n\n, and M tempo i;j std represent the i-th bar, the j-th beat within the i-th bar, and the tempo value for that beat in M std , respectively. These values contribute to calculating the rhythm elements.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Xcomposer",
      "text": "Our enhanced symbolic music representation, as shown in Fig.  3 , maps MIDI files and the elements within the projection space to token sequences representing symbolic music, thereby assisting XComposer in the subsequent music generation and selection processes.\n\nXComposer follows the Compound Word (CP)  [10]  architecture, where tokens belonging to the same family (representing the same event) are grouped into a supertoken and positioned at the same time step. As illustrated in Fig.  3 , XComposer introduces three key improvements:\n\n• First, we introduce a new family token named \"Tag\", along with two corresponding grouped tokens: \"Emotion\" and \"Genre\". These tokens enable control over the music generation process by specifying emotion and genre. • Second, we add a new family token called \"Instrument\", with its corresponding grouped token, \"Program\", ensuring the generation of multi-track music. • Third, within the \"Rhythm\" family token, we incorporate the grouped tokens \"Density\" and \"Strength\" into bar and beat events, respectively, allowing control over note density and beat strength. In this paper, we utilize the Tag token to capture the overall semantic information of the music. Unlike methods that specify the Tag token solely at the beginning of the entire music piece (e.g., EMOPIA  [11] ), our approach places the Tag token at the beginning of each bar. This strategy offers two key advantages: it generates music that adheres more closely to the specified tag and facilitates bar-by-bar fine-tuning of emotion categories in video-conditioned music generation scenarios. The Emotion and Genre tokens represent the emotional and stylistic characteristics of the music, offering 11 and 6 options, respectively.\n\nThe Instrument token, positioned at the beginning of the note sequence, indicates the instrument information of the subsequent note sequence. This token enables track-level modeling for 17 instruments (detailed in Sec. III-C), resulting in music enriched by diverse instrumental ensembles.\n\nFor video-conditioned music generation, the local rhythm of the generated music is synchronized with the motion information of the video. Inspired by CMT  [19] , we incorporate the Density and Strength tokens at bar and beat event positions, respectively, to control the note density and beat strength of each bar. This method ensures that the generated rhythm closely aligns with the video content.\n\nOur representation chronologically encodes symbolic music events (e.g., Tag, Bar, Beat, Instrument, and Note) in each bar of MIDI files to form token sequences. This design supports the generation of emotionally expressive and melodically coherent music.\n\nImplementation details of our representation are provided in Table  II . For melodic instrument notes, we utilize 128 tokens to represent pitch, following the standard MIDI format. For percussion instrument notes, which lack pitch information, we employ 128 pseudo-pitch tokens to denote different percussion types. To reduce the vocabulary size, tempos are quantized into 65 values (ranging from 32 to 224), and velocities into 44 values (ranging from 40 to 126). We also add additional \"Tag\" and \"Instrument\" family tokens to represent emotion/genre and instrument information, respectively. To capture shorter note durations, we use a higher resolution (32nd notes) instead of the 16th notes used in CP. Furthermore, to accommodate largescale data training, we have increased the embedding size for each token.\n\nThe Generator, serving as the core component of XComposer, conditionally generates symbolic music based on our enhanced representation. It employs a Transformer Decoder  [6]  as the backbone network to effectively model the dependencies among tokens.\n\nSpecifically, given the first t tokens and aiming to predict the next token, the process is structured as follows. Initially, the token sequences are transformed into a two-dimensional event matrix, where each element event j i represents the j-th attribute of the i-th token. Each event i contains 12 dimensions: family type, emotion, genre, bar-beat, tempo, chord, density, strength, program, pitch, duration, and velocity. Subsequently, at time i, a linear projection is applied to each one-hot vector event j i , producing a dense vector embed j i . These dense vectors are then concatenated to form the dense representation concat i for the current token. Following this, linear projection and positional encoding are applied to concat i to obtain the input feature input i for the Transformer network at time i. The input features from the first t tokens are fed into the Transformer network to compute the hidden state H t at the current time step. The next event event t+1 is then predicted by applying multiple linear projections to H t . In line with the approach proposed by  [10] , the next family type is predicted first, followed by the prediction of the other attributes based on H t and the one-hot vector of the predicted family type. Crossentropy loss is utilized to optimize this prediction process. The overall procedure can be expressed as follows:\n\nfor i = 1, ..., t and j = 1, ..., 12.\n\nfor j = 2, ..., 12.\n\nDuring inference, a stochastic temperature-controlled sampling method is employed to enhance the diversity of the generated tokens.\n\nThe Selector, as another core component of XComposer, identifies high-quality symbolic music through a multi-task learning scheme. It leverages the Transformer Encoder  [6]  as its backbone network to evaluate the quality of symbolic music.\n\nOnly a subset of the music generated by the Generator achieves human-level quality, which is characterized by beautiful and coherent melodies, distinct tune variations, and alternating rhythmic structures. Our objective is to accurately identify these high-quality pieces using supervised learning. To this end, we generate batches of symbolic music under diverse control conditions via the Generator. This is followed by manual annotations for each piece to determine whether it meets human-level standards. Then a classification model is trained using this annotated dataset to evaluate the quality of symbolic music accurately.\n\nSpecifically, we design a multi-task learning scheme comprising quality assessment, emotion recognition, and genre recognition tasks to select high-quality music. The Selector represents each MIDI file as a token sequence using our representation and translates this sequence into a corresponding event matrix. The same transformation is applied to each event vector within the matrix as in the Generator, yielding the input features for the Transformer network. Since the Selector analyzes entire MIDI files rather than predicting subsequent events, the T input features from all moments are fed into the encoder, producing output features F i at each time step. We then apply Global Average Pooling along the temporal dimension to derive the global feature vector F encoder . This global representation is subsequently passed through three fully connected layers, with output activations normalized to produce class probabilities for each task. The process can be expressed as follows:\n\nfor i = 1, ..., T and j = 1, ..., 12.\n\n{Fi} T i=1 = TransformerEncoder({input ′ i } T i=1 ) F encoder = GAPtime({Fi} T i=1 ) Probgenre = Softmax(FCgenre(F encoder )) Probemotion = Softmax(FCemotion(F encoder )) Prob quality = Softmax(FC quality (F encoder ))  (12)  The rationale for employing a multi-task learning scheme lies in the subtle differences in quality assessment criteria across various music types. By integrating emotion and genre recognition tasks, the network gains a more holistic understanding of the music, thereby enhancing its overall selection accuracy. Moreover, although the training dataset originates from the Generator, the Selector demonstrates remarkable generalizability, effectively assessing the quality, emotion, and genre of symbolic music from unknown sources.\n\nDuring inference, the Selector employs Prob quality as the quality score. It identifies the sample within a batch that achieves the highest quality score surpassing a predefined threshold, thereby selecting the most promising high-quality music piece.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Xmidi",
      "text": "In this section, we introduce our constructed symbolic music dataset, i.e., XMIDI. Existing publicly available symbolic music datasets suffer from limitations in both scale and label completeness, making it nearly impossible to train a music generation model that meets the requirements of this study. To address this gap, we built XMIDI, the largest known For data collection and cleaning, we first crawled MIDI files from various online sources, including the Internet Archive, GitHub, and Reddit. To ensure dataset quality, we carried out the following data cleaning steps. i) Automatic Cleaning: We removed corrupted or empty files and performed basic deduplication based on MD5 file hashes. ii) Data Deduplication: We rendered the remaining MIDI files into audio format, extracted chroma features, and calculated cosine similarities to identify and eliminate duplicates more effectively. iii) Manual Cleaning: During the annotation phase, trained annotators discarded files with evident abnormalities (e.g., those with large missing note segments or excessively short sound effects), ensuring that only musically normal files remained. Following common practice  [29] , we addressed data imbalance issue by merging instrument tracks. Specifically, we grouped the 128 melodic instruments into their parent categories. For example, program IDs 0∼7 (such as Acoustic Piano and Electric Piano) were grouped under Piano, program IDs 24∼31 (including Acoustic and Electric Guitars) under Guitar, and all 61 percussion instruments into the Drum category. This consolidation resulted in 17 distinct instrument types: piano, xylophone, organ, guitar, bass, violin, harp, string, trumpet, tuba, sax, flute, lead, pad, pipa, guzheng, and drum.\n\nFor data annotation, we established a comprehensive labeling system for emotions and genres and hired ten professional annotators (five males and five females) to ensure accurate labeling of each MIDI file. The annotators worked independently but maintained regular communication with the organizers to uphold consistent standards. To maintain high accuracy, we adopted several measures: i) Standardization: Detailed descriptions and representative music demos were provided for each emotion and genre label to ensure a uniform understanding. ii) Cross-checking Mechanism: Each annotation was independently verified by at least three experts. iii) Random Quality Checks: The dataset was divided into batches of 500 files. Random samples were drawn from each batch for accuracy assessment, with batches failing to meet the 95% accuracy threshold returned for revision. iv) Regular Training for the Annotators: Weekly meetings were held to review frequently misclassified cases, reinforcing consistency among annotators. v) Discussion of Controversial Cases: Controversial cases were deliberated upon by a panel consisting of all annotators and organizers.   The data statistics of our XMIDI dataset are shown in Fig.  4 . In terms of emotion distribution (Fig.  4a ), categories such as exciting, warm, happy, romantic, funny, sad, and angry dominate, whereas emotion like lazy, quiet, fear, and magnificent are less frequent. The genre distribution (Fig.  4b ) is relatively balanced, with Rock representing the largest share and folk music the smallest. Regarding music length (Fig.  4c ), most compositions span between 1 and 5 minutes, with shorter and longer pieces being less common.\n\nWe compared XMIDI dataset with existing emotion-labeled MIDI datasets in terms of emotion categories, genre types and data size. As shown in Table  III , previous emotion datasets  [11] ,  [55] ,  [56]  are relatively small, typically containing only a few hundred songs. Bao et al.  [12]  recently built a large-scale paired lyric-melody dataset annotated using deep learning models. However, their emotion labels are coarsegrained (positive/negative), and automated labeling introduces the risk of mis-classification. In contrast, our XMIDI offers finer emotion categories (11 distinct emotions), more precise annotations (annotated by experts and cross-validated) and a nearly 10-times-larger data size.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Iv. Experiments",
      "text": "A. Datasets 1) Symbolic Music Generation: We developed XMIDI, the largest symbolic music dataset to date with precise emotion and genre labels. Each piece of music has an average duration of approximately 176 seconds, contributing to a cumulative dataset length of 5,278 hours.\n\n2) Image Emotion Recognition: We collected a new image emotion recognition dataset containing 269,793 images, among which 40,000 images are selected as the test set, and the remaining images are used for training. The images were gathered from various sources, including the WebEMO  [57]  image sentiment dataset, the Places365  [58]  scene recognition dataset, and web searches from Baidu and Google. Given that the labels of internet images are inherently noisy, we employed human annotators to filter out samples that did not clearly correspond to their assigned labels.\n\n3) Text Emotion Recognition: We constructed a standard test set for text emotion classification, consisting of 1,100 sentences distributed evenly across 11 emotion categories.\n\nTo generate emotionally nuanced text, we instructed GPT-4  [22]  to produce sentences that conveyed specific emotions without explicitly including emotion words. We applied Sen-tenceTransformer  [50]  to compute text embedding similarities and removed redundant samples to ensure diversity and distinctiveness within the dataset.\n\n4) Music Quality Assessment: We utilized the Generator of XComposer to generate 9,540 music pieces by applying random combinations of emotion and genre tags as conditions. Subsequently, we conducted a manual quality evaluation of the generated pieces. High-quality music was characterized by coherent melodies, distinct tune fluctuations, and dynamic rhythmic variations. In contrast, pieces failing to meet these criteria did not achieve human-level quality standards.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "B. Implementation Details",
      "text": "We employ the Transformer architecture  [6]  as the backbone of XComposer. For the Generator, we utilize Transformer Decoder to predict subsequent symbolic music event based on the previous events. The model consists of 30 self-attention layers, each containing 16 attention heads, with a hidden size set to 1,024. During training, the Adam optimizer is employed with an initial learning rate of 3e-5. When the loss saturates (specifically at values of 0.053, 0.049, and 0.045), we halve the learning rate and resume training. The overall training procedure spans 24 days, using a batch size of 40 across 8 NVIDIA A800 GPUs. To mitigate gradient explosion, we set the maximum gradient to 0.5. For the Selector, we use Transformer Encoder to encode the token sequences and predict global attributes such as quality levels, emotions, and genres. We use 3 self-attention layers, 8 attention heads, and a hidden size of 512. During training, the Adam optimizer is employed with a learning rate of 1e-5, and the model training process is completed in 10 hours on a single NVIDIA Tesla V100 GPU.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "C. Objective Evaluation",
      "text": "1) Metrics: We selected three typical objective metrics: Pitch Class Histogram Entropy (PCE)  [59] , Grooving Pattern Similarity (GS)  [59]  and Empty Beat Rate (EBR)  [60] . Specifically, the PCE evaluates the distribution and uniformity of pitch classes within a musical piece or segment. A lower PCE indicates a more concentrated pitch class distribution, usually implying clearer tonality. Conversely, a higher PCE represents a more uniform distribution, reflecting unstable tonality. The GS metric assesses the resemblance between rhythmic patterns in musical bars. A high GS score means that the grooving patterns of the analyzed pairs are similar, indicating a strong rhythm structure. In contrast, a low GS score means dissimilar grooving patterns, indicating a unstable rhythm structure. In addition, the EBR measures the proportion of empty or silent beats in a music piece. A higher EBR score suggests frequent gaps or pauses, indicating sparse note distribution and a lack of richness in the music. All three objective metrics are computed using the MusPy Toolkit  [61] .\n\n2) Comparison with Symbolic Music Generation Methods: We compared XMusic with the current state-of-the-art symbolic music generation methods: CP  [10]  and EMOPIA  [11] . For fair comparison, we used their official pre-trained models directly for inference. The comparative results are listed in Table  IV -(a). Our XMusic outperforms both methods, achieving the lowest PCE and EBR scores and the highest GS score. This suggests that our method exhibits clear tonality, rich note distribution and distinct rhythmicity, respectively.\n\n3) Comparison with Video-conditioned Symbolic Music Generation Method: To further evaluate our method, we compared XMusic with CMT  [19] , the state-of-the-art videoconditioned symbolic music generation method. CMT is the first and only open-source method capable of generating background music for general videos. We did not include V-MusProd  [20]  in this comparison because it had not been open-sourced at the submission time of this manuscript. We randomly selected videos covering various scenes, including landscapes, animations, weddings, performances, sports, and games. These videos varied in duration (from 30 seconds to 2 minutes) and conveyed diverse sentiments, such as exciting, romantic, fear, happy emotions, etc. We generated background music for each video using both CMT and our XMusic. The results are summarized in Table IV-(b). Compared with CMT, XMusic has clear advantages across all three objective metrics, demonstrating the effectiveness of our method.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "D. Subjective Evaluation",
      "text": "For music generation, subjective human evaluations are essential, as they offer a comprehensive understanding of music quality. We designed online questionnaires for subjective evaluation and invited 31 participants to participate. To ensure blind evaluation, the music results within each question were randomly shuffled.\n\n1) Comparison with Symbolic Music Generation Methods: Following common practice  [10] ,  [11] , we generated MIDI files for each method (CP  [10] , EMOPIA  [11]  and our proposed XMusic) and rendered these files in audio format using the same soundfont. In the questionnaire, each question contained three randomly ordered audio samples generated via the three aforementioned methods. The participants were required to carefully listen to and rank the audio samples based on the following metrics: i) Richness: The diversity of musical elements, such as melody, harmony, rhythm, and timbre. ii) Correctness: The absence of errors or unnatural musical elements, such as odd chords or sudden silences. iii) Structuredness: The presence of repetitive structures, such as memorable melodies. The questionnaire took an average of 47 minutes to complete. We averaged the ranking results from the 31 participants to obtain the final results, presented in Table V-(a). As shown, our method achieved the highest average rank across all three evaluation metrics, indicating that XMusic surpassed the existing state-of-the-art approaches in generating impressive and high-quality music.\n\n2) Comparison with Emotion-conditioned Symbolic Music Generation Method: To evaluate the efficacy of our method in emotion control, we compared our XMusic with the state-of-the-art emotion-conditioned method EMOPIA  [11] . To our best knowledge, EMOPIA is currently the only open-source emotion-conditioned symbolic music generation method. EMOPIA and XMusic utilize differing levels of emotion granularity. EMOPIA adopts Russell's Circumplex model, conceptualizing emotions in a two-dimensional space defined by valence and arousal, resulting in four classes (quadrants): PVPA (positive valence positive arousal), NVPA (negative valence positive arousal), NVNA (negative valence negative arousal), and PVNA (positive valence negative arousal). In contrast, XMusic employs 11 specific emotion classes, including happy, funny, sad, exciting, etc. Since there are no clear correspondences between the 4 EMOPIA quadrants and the 11 XMusic classes, we aligned the emotion categories by merging adjacent EMOPIA quadrants and mapping corresponding XMusic categories to these combined classes. For example, PVPA and PVNA were merged to form a new Positive Valence (PV) class, with the \"happy\" and \"funny\" classes mapped to this category. The other new categories were defined as follows: NV (combining NVPA & NVNA, mapped to sad), PA (PVPA & NVPA, mapped to exciting) and NA (PVNA & NVNA, mapped to quiet). We generated and rendered music files for each method using these new categories as input prompts. Participants were instructed to count the number of music pieces that they perceived as fitting the new labels. This task took an average of 112 minutes to complete. We computed the average number of correctly classified pieces per class as determined by the 31 participants. As shown in Table  VI , the music generated by our method better matched the input emotion prompts, demonstrating that our approach has superior emotional controllability.\n\n3) Comparison with Video-conditioned Symbolic Music Generation Method: Additionally, we compared XMusic with  CMT  [19]  in a video-conditioned evaluation. While the latest work, V-MusProd  [20] , also focuses on generating music for general videos, the source code for V-MusProd was not publicly available at the time of our evaluation. Therefore, a direct and fair comparison could only be conducted between XMusic and CMT. In addition to evaluating richness, correctness, and structuredness, we assessed the degree of videomusic alignment, focusing on both emotional and rhythmic correspondences. Using the same videos selected for the objective evaluation, we paired each video with background music generated by CMT and XMusic. These were presented in a random order for blindness. The questionnaire took about 25 minutes to complete. The participants were required to carefully listen and rank the two background music pieces in terms of five aspects. The average rankings from the 31 participants are summarized in Table V-(b). XMusic consistently outperformed CMT across all five metrics, demonstrating its superior performance in handling video prompts. We attribute this success to the powerful emotion analysis and control capabilities of XMusic. In contrast, CMT relies on rule-based rhythm control, which lacks perception and control of emotions, resulting in poor emotional alignment. For instance, it may even generate cheerful music for a sorrowful video. By effectively understanding and controlling emotions, XMusic processes semantic information to generate music that harmonizes both rhythmically and emotionally. Video demos are available on our demo website.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "4) Comparison With",
      "text": "Text-conditioned Symbolic Music Generation Methods: We compared XMusic with two existing text-conditioned methods: BART-base  [62]  and GPT-4  [22]  (instructed to produce ABC notation, following  [64] ). For fair comparison, the ABC notation outputs were converted to MIDI format and rendered using the same soundfont. Participants evaluated music generated from identical text inputs via a structured questionnaire, ranking the results based on four evaluation metrics. The average questionnaire completion time was 34 minutes. As listed in Table V-(c), XMusic outperformed both comparative methods across all metrics, demonstrating that the idea of explicitly analyzing emotions in text and using this analysis to control music generation is effective.\n\n5) Comparison with Image-conditioned Symbolic Music Generation Methods: Symbolic music generation using images is relatively under-explored, with only a few notable methods  [63] ,  [65] ,  [66]  aiming to discover visual-musical associations. Among these, we could only compare XMusic with Synesthesia  [63] , as the source codes and demos for the other methods were unavailable. Specifically, we used the same images provided in the official Synesthesia repository 2  as inputs to our XMusic and then created a questionnaire to rank the two methods given the same input image. The questionnaire took about 13 minutes to complete. Table  V   -(d)  shows the average rankings from 31 participants. In contrast to Synesthesia, which implicitly models emotional information using paired image-music data, our XMusic explicitly decouples the emotion analysis and control processes, offering a more intuitive and effective solution.\n\n6) Evaluation of the Controllability of Humming to Generate Music: Although some Apps, such as HumBeatz and ZhiQu, offer the capability to generate accompaniment based on user humming, few research works have focused on generating melodies from humming input. A recent work, Hum-ming2Music  [67] , is most relevant to the humming controllability of XMusic. However, a direct comparison was not feasible because the authors did not release their open-source code or demonstrations. Through a subjective evaluation following  [67] , we observed that XMusic has the following advantages: i) Accurate transcription. The transcription results were wellaligned with the original input humming melody. ii) Smooth transition. The transitions between transcribed humming notes and the subsequent composition were natural, benefiting from the long-term dependencies captured by our XComposer. iii) Consistent rhythm. The overall rhythm remained coherent, with no noticeable interruptions or abrupt changes. This consistency supports the effectiveness of our approach in parsing  user humming and mapping it to notes and rhythmic elements.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "7) Evaluations On Public Datasets:",
      "text": "We conducted experiments to compare XMusic with CP  [10]  and EMOPIA  [11]  on two widely used symbolic music datasets: AILabs1k7  [10]  and EMOPIA  [11] . i) AILabs1k7 contains 1,748 pop piano MIDI files, each with an average duration of 4 minutes, totaling around 108 hours. Since this dataset lacks emotion and genre annotations, the emotion token in the EMOPIA model and the emotion and genre tokens in XMusic were set to [ignore]. ii) EMOPIA consists of 1,087 MIDI clips extracted from 387 popular piano music pieces and includes emotion labels at the clip level. As genre labels are absent in this dataset, the genre token in the XMusic model was also set to [ignore]. Following  [11] , we first pre-trained our model on the AILabs1k7 dataset due to the relatively small scale of EMOPIA dataset. The subjective evaluation results are listed in Table  VIII . With identical training data, our XMusic outperformed both CP and EMOPIA, demonstrating the effectiveness and generalizability of the proposed method. As discussed in Sec. IV-E1 and Sec. IV-E5, the performance gains are primarily attributed to the enhanced music representation and the effective Selector model.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "E. Ablation Study",
      "text": "1) The Effectiveness of the Selector: To validate the effectiveness of our quality assessment model, we investigated whether incorporating the Selector improved music quality. Specifically, we employed two models, one with the Selector and one without, to generate 5 music pieces for each emotion. We then randomly selected two pieces belonging to the same emotion but originating from different models to create comparative pairs, yielding a total of 55 pairs. The participants were required to rank the music pieces in each pair from 4 perspectives: richness, correctness, structuredness and emotional matching. The completion of this questionnaire . As shown, the music generated using the Selector consistently received higher average rankings across all four metrics, demonstrating the effectiveness of our Selector model and the necessity of conducting post-hoc music quality assessments. We also conducted an objective ablation study on the Selector. As shown in Table  IX , incorporating the Selector improved the GS score and reduced the PCE and EBR scores, objectively demonstrating the effectiveness of the proposed Selector. In summary, the effectiveness of the Selector has been validated through both subjective and objective evaluations.\n\nAs described in Sec. III-B, our Selector leverages a multitask learning scheme involving three sub-tasks: music quality assessment, emotion recognition, and genre recognition. We conducted ablation experiments to validate the effectiveness of the multi-task learning scheme. As shown in Table  X , the accuracy of the music quality assessment task significantly improved as when additional sub-tasks were incrementally added in joint learning. We conjecture that this improvement stemmed from the fact that music quality, emotion, and genre are subjective attributes influenced by human perception. Joint learning across these tasks helps the Selector align more closely with human perceptual standards. The Selector achieved a music quality assessment accuracy of 94.8% on our self-constructed evaluation benchmark, demonstrating its robust quality control capabilities.\n\nIn summary, subjective evaluations, objective results, and multi-head ablation studies collectively demonstrate the effectiveness of our Selector.\n\n2) The Effectiveness of Fine-Grained Emotion Control: As mentioned in Sec. III-B, we introduce an emotion token before each bar to enable fine-grained emotion fine-tuning at the bar level. To verify the impact of this enhancement on video-conditioned music generation, we designed three comparative settings: i) no control, i.e., without specifying the  Notably, the fine-grained emotion control at the bar level achieved the best results across all five metrics, especially for the Emotion-Matching metric, demonstrating the effectiveness of conducting fine-grained emotion control at the bar level. This strategy can capture subtle emotional changes on the fly and generate music results that are more emotionally aligned with the input video.\n\n3) The Controllability of Text and Image Prompts: To quantitatively showcase the controllability of text and image prompts, we evaluated the emotion classification accuracy on the self-constructed test sets described in Sec. IV-A. The image-based emotion classification task achieved an accuracy of 85.2%, while the text-based task reached 87.7%. This high performance can be attributed to the integration of general emotional knowledge from large-scale models such as CLIP  [49]  and SentenceTransformer  [50] . As demonstrated in Sec. IV-D2, given emotion tags, our XComposer surpasses the current methods in emotion control capabilities. Thus, XProjector accurately analyzes emotions from text and images, while XComposer effectively generates emotion-specific music, demonstrating the strong controllability of XMusic for text and image inputs.\n\n4) Ablation on the Weighting Factors of Image Emotion Recognition Task: The weighting factors λ 1 and λ 2 in Eqn. 1 balance the contributions of two models (i.e., ResNet and CLIP). We explored various numerical configurations to evaluate their impact on the image emotion recognition task. As shown in Table XI, we observed that the single model settings (λ 1 = 0 or λ 2 = 0) were inferior to the dual-model consensus settings. Moreover, giving higher weight to the CLIP model (λ 1 < λ 2 ) led to better results. Therefore, we set λ 1 = 1 and λ 2 = 2 as the default settings because they yielded the best overall performance.\n\n5) The Effectiveness of the Proposed Music Representation: As described in Sec. III-B, we designed an enhanced symbolic music representation based on the CP  [10]  representation, which includes three key family token improvements: Tag, Instrument, and Rhythm. To evaluate the efficacy of these new family tokens, we conducted a series of ablation studies on the XMIDI dataset. Starting from the baseline CP representation, we incrementally added Tag, Instrument, and Rhythm family tokens to investigate their impact on music generation. The participants in the subjective evaluation ranked the generated music based on three aspects: richness, correctness, and structuredness. The average time to complete the questionnaire was 83 minutes, and the results are summarized in Table VII-(c). We can conclude that each added family token significantly enhanced performance, demonstrating the effectiveness of the proposed symbolic music representation.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "F. Discussion",
      "text": "1) Data Distribution of the XMIDI Dataset: We designed a comparative experiment to investigate whether the imbalance issue in XMIDI dataset affects the quality of the generated music. The two common strategies for addressing imbalanced data are undersampling common classes and oversampling rare classes through duplication. We applied these two sampling strategies to balance the data categories and trained models with an equal number of iterations. We then conducted a subjective evaluation comparing the performance of these models with a baseline model trained on the original, imbalanced dataset. The participants ranked the generated music pieces from these three models, and the questionnaire took approximately 61 minutes to complete. As shown in Table VII-(d), we have the following observations: i) The model trained with the undersampling strategy performed worse than the one trained on the original XMIDI dataset, demonstrating that data diversity is more critical than balance for music generation; ii) The oversampling strategy was also inferior to the baseline, likely due to overfitting issue caused by simple data duplication; iii) The model trained on the original XMIDI dataset achieved the highest performance. We conjecture that the diverse data sources of our XMIDI reflects the longtailed distribution of real-world music categories, better aligns with human auditory preferences. In future work, we will expand the number of rare categories or explore more effective augmentation strategies to further address the data imbalance challenge effectively.\n\n2) Limitations and Future Work: Our XMusic supports five common input modalities for controllable music generation: videos, images, texts, tags, and humming. Other modalities, such as human skeletons, gestures, and depth, are worth further exploration. Additionally, this paper focuses on a subset of symbolic music elements, while a broader range of elements, such as time signatures, music lengths, and keys, could be incorporated for more comprehensive control. Currently, XMusic analyzes only the global emotion expressed in the text prompt and does not explicitly consider specific music elements mentioned within the text. To address this, we plan to train a text classifier in the future to better extract the music elements contained in the text for more precise control. Moreover, we aim to further expand the XMIDI dataset, particularly for rare emotion and genre categories, to build a more balanced and larger-scale symbolic music dataset.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "V. Conclusion",
      "text": "In this paper, we propose a multi-modal controllable symbolic music generation framework called XMusic. This framework supports versatile prompts, such as videos, images, texts, tags, and humming. Music elements act as connectors between the prompt parsing and generation controlling processes, explicitly decoupling the control signal analysis task from the music generation pipeline. This design enjoys strong scalability, facilitating the integration of new modalities in a plug-andplay manner. Specifically, our XProjector parses multi-modal prompts into symbolic music elements within the projection space, while XComposer generates high-quality music aligned with the control conditions based on our enhanced symbolic music representation. Furthermore, we construct a large-scale symbolic music dataset called XMIDI with precise emotion and genre annotations for training the music generation model. Compared to the current state-of-the-art methods, XMusic achieves superior performance across all utilized objective and subjective evaluation metrics.",
      "page_start": 14,
      "page_end": 14
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The architectural overview of our XMusic framework.",
      "page": 1
    },
    {
      "caption": "Figure 1: For instance, temporally-related prompts, such as videos and",
      "page": 2
    },
    {
      "caption": "Figure 1: Recognizing",
      "page": 2
    },
    {
      "caption": "Figure 2: Illustration of the proposed XMusic, which supports flexible (a) X-Prompts to guide the generation of high-quality",
      "page": 4
    },
    {
      "caption": "Figure 2: , the process is divided into three stages: parsing, control,",
      "page": 4
    },
    {
      "caption": "Figure 3: Comparison between our representation and Compound",
      "page": 6
    },
    {
      "caption": "Figure 3: , maps MIDI files and the elements within the",
      "page": 6
    },
    {
      "caption": "Figure 4: Data statistics of our XMIDI dataset.",
      "page": 9
    },
    {
      "caption": "Figure 4: In terms of emotion distribution (Fig. 4a), categories",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          ")\n)": ", Can Zhang\nSida Tian",
          ")": ", Wei Tan, Wenjie Zhu\n, Wei Yuan"
        },
        {
          ")\n)": "Abstract—In recent years, remarkable advancements\nin arti-",
          ")": "X-Prompts\nXMIDI Dataset"
        },
        {
          ")\n)": "ficial\nintelligence-generated content\n(AIGC) have been achieved",
          ")": ""
        },
        {
          ")\n)": "",
          ")": "I have passed \ntrain"
        },
        {
          ")\n)": "in the fields of\nimage synthesis and text generation, generating",
          ")": "the exam"
        },
        {
          ")\n)": "content\ncomparable\nto\nthat\nproduced\nby\nhumans. However,",
          ")": "image\nvideo\ntag\ntext\nhumming"
        },
        {
          ")\n)": "",
          ")": "Token"
        },
        {
          ")\n)": "the\nquality\nof AI-generated music\nhas\nnot\nyet\nreached\nthis",
          ")": ""
        },
        {
          ")\n)": "",
          ")": "Sequences"
        },
        {
          ")\n)": "standard, primarily due to the challenge of effectively controlling",
          ")": ""
        },
        {
          ")\n)": "",
          ")": "Transformer\ncontrol"
        },
        {
          ")\n)": "",
          ")": "select"
        },
        {
          ")\n)": "musical emotions and ensuring high-quality outputs. This paper",
          ")": "Decoder"
        },
        {
          ")\n)": "presents\na\ngeneralized symbolic music\ngeneration framework,",
          ")": "Symbolic Music"
        },
        {
          ")\n)": "",
          ")": "Generator\nSelector"
        },
        {
          ")\n)": "XMusic, which supports flexible prompts\n(i.e.,\nimages,\nvideos,",
          ")": "Elements"
        },
        {
          ")\n)": "texts,\ntags, and humming)\nto generate\nemotionally controllable",
          ")": ""
        },
        {
          ")\n)": "",
          ")": "XProjector\nXComposer"
        },
        {
          ")\n)": "and high-quality symbolic music. XMusic\nconsists of\ntwo core",
          ")": ""
        },
        {
          ")\n)": "components, XProjector and XComposer. XProjector parses the",
          ")": ""
        },
        {
          ")\n)": "prompts of various modalities into symbolic music elements (i.e.,",
          ")": "Fig. 1: The architectural overview of our XMusic framework."
        },
        {
          ")\n)": "emotions, genres, rhythms and notes) within the projection space",
          ")": ""
        },
        {
          ")\n)": "",
          ")": "It contains two essential components: XProjector and XCom-"
        },
        {
          ")\n)": "to generate matching music. XComposer\ncontains a Generator",
          ")": ""
        },
        {
          ")\n)": "",
          ")": "poser. XProjector parses various\ninput prompts\ninto specific"
        },
        {
          ")\n)": "and a Selector. The Generator generates emotionally controllable",
          ")": ""
        },
        {
          ")\n)": "",
          ")": "symbolic music elements. These elements then serve as con-"
        },
        {
          ")\n)": "and melodious music based on our innovative symbolic music rep-",
          ")": ""
        },
        {
          ")\n)": "resentation, whereas the Selector identifies high-quality symbolic",
          ")": "trol\nsignals, guiding the music generation process within the"
        },
        {
          ")\n)": "music by\nconstructing\na multi-task learning\nscheme\ninvolving",
          ")": "Generator of XComposer. Additionally, XComposer\nincludes"
        },
        {
          ")\n)": "quality assessment,\nemotion recognition, and genre\nrecognition",
          ")": ""
        },
        {
          ")\n)": "",
          ")": "a Selector\nthat evaluates and identifies high-quality generated"
        },
        {
          ")\n)": "tasks. In addition, we build XMIDI, a large-scale symbolic music",
          ")": ""
        },
        {
          ")\n)": "",
          ")": "music. The Generator\nis\ntrained on our\nlarge-scale dataset,"
        },
        {
          ")\n)": "dataset\nthat contains 108,023 MIDI files annotated with precise",
          ")": ""
        },
        {
          ")\n)": "",
          ")": "XMIDI, which includes precise emotion and genre labels."
        },
        {
          ")\n)": "emotion and genre\nlabels. Objective and subjective\nevaluations",
          ")": ""
        },
        {
          ")\n)": "show that XMusic\nsignificantly outperforms\nthe\ncurrent\nstate-",
          ")": ""
        },
        {
          ")\n)": "of-the-art methods with impressive music quality. Our XMusic",
          ")": ""
        },
        {
          ")\n)": "has been awarded as one of\nthe nine Highlights of Collectibles at",
          ")": "Symbolic music generation methods primarily aim to model"
        },
        {
          ")\n)": "WAIC 2023. The project homepage of XMusic is: https://xmusic-",
          ")": ""
        },
        {
          ")\n)": "",
          ")": "the\ntemporal\ndependencies within music,\npredicting\nsubse-"
        },
        {
          ")\n)": "project.github.io.",
          ")": ""
        },
        {
          ")\n)": "",
          ")": "quent musical events based on prior ones. The Transformer is"
        },
        {
          ")\n)": "Index Terms—AIGC, Music Generation, Multi-Modal Parsing,",
          ")": ""
        },
        {
          ")\n)": "",
          ")": "a natural fit for this sequence-to-sequence task while handling"
        },
        {
          ")\n)": "Music Quality Assessment, Large-Scale Dataset.",
          ")": ""
        },
        {
          ")\n)": "",
          ")": "long-range dependencies. Recent advancements have demon-"
        },
        {
          ")\n)": "",
          ")": "strated the\nremarkable potential of Transformer models\n[6],"
        },
        {
          ")\n)": "I.\nINTRODUCTION",
          ")": "[7]\nin symbolic music generation. The Music Transformer by"
        },
        {
          ")\n)": "",
          ")": "Huang et al.\n[8] shows the first successful application of\nthe"
        },
        {
          ")\n)": "intelligence\n(AI)\ntechniques\nhave\nsignif-",
          ")": ""
        },
        {
          ")\n)": "A RTIFICIAL",
          ")": "self-attention mechanism for generating long symbolic music."
        },
        {
          ")\n)": "advanced\nthe\nfield\nof AI Generated Content",
          ")": ""
        },
        {
          ")\n)": "",
          ")": "The Pop Music Transformer [9] incorporates the Transformer-"
        },
        {
          ")\n)": "(AIGC), making it a prominent\nresearch area in recent years.",
          ")": ""
        },
        {
          ")\n)": "",
          ")": "XL [7] architecture to generate symbolic pop music with an"
        },
        {
          ")\n)": "AIGC fosters\ncreativity,\nexploration,\nand\ninnovation\nacross",
          ")": ""
        },
        {
          ")\n)": "",
          ")": "enhanced rhythmic structure. Another\ninfluential contribution"
        },
        {
          ")\n)": "diverse artistic domains. As an art\nform centered on sound,",
          ")": ""
        },
        {
          ")\n)": "",
          ")": "is\nthe Compound Word Transformer\n[10], which\nexplores"
        },
        {
          ")\n)": "music\nis\na\nsignificant\ncomponent of AIGC. Automatic mu-",
          ")": ""
        },
        {
          ")\n)": "",
          ")": "novel and efficient tokenization techniques for symbolic music"
        },
        {
          ")\n)": "sic generation has numerous potential applications,\nincluding",
          ")": ""
        },
        {
          ")\n)": "",
          ")": "training."
        },
        {
          ")\n)": "adaptive\nsoundtracks,\nvideo\nbackground music\ngeneration,",
          ")": ""
        },
        {
          ")\n)": "",
          ")": "Conditional\nsymbolic music generation has\nattracted sig-"
        },
        {
          ")\n)": "music transcription, and royalty-free music creation, etc. Al-",
          ")": ""
        },
        {
          ")\n)": "",
          ")": "nificant attention due to its ability to leverage user-supplied"
        },
        {
          ")\n)": "though recent\nstudies\n(such as AudioLM [1], MusicLM [2],",
          ")": ""
        },
        {
          ")\n)": "",
          ")": "information\nas\na\n“prompt”\nfor\nproducing\nunique musical"
        },
        {
          ")\n)": "Riffusion [3], MusicGen [4] and Noise2Music [5], etc) have",
          ")": ""
        },
        {
          ")\n)": "",
          ")": "compositions. Existing conditional methods\nexplore various"
        },
        {
          ")\n)": "achieved success in terms of generating music within the audio",
          ")": ""
        },
        {
          ")\n)": "",
          ")": "types of prompts,\nincluding attribute tags (e.g., emotions [11],"
        },
        {
          ")\n)": "domain,\nediting\nsuch\ngenerated music\nin\nits\naudio\nformat",
          ")": ""
        },
        {
          ")\n)": "",
          ")": "[12], genres [13], and instruments [14]), sequential data (e.g.,"
        },
        {
          ")\n)": "remains\nchallenging\nand\nunintuitive.\nIn\ncontrast,\nsymbolic",
          ")": ""
        },
        {
          ")\n)": "",
          ")": "lead sheets\n[15], motifs\n[16],\nand melodies\n[8]),\nand multi-"
        },
        {
          ")\n)": "music,\ntypically represented in MIDI\nformat, offers greater",
          ")": ""
        },
        {
          ")\n)": "",
          ")": "media data (e.g., performance footage [17],\n[18] and general"
        },
        {
          ")\n)": "flexibility, enabling users to modify specific musical elements",
          ")": ""
        },
        {
          ")\n)": "",
          ")": "videos [19],\n[20]). Despite considerable advancements in the"
        },
        {
          ")\n)": "explicitly. Thus, this paper focuses on music generation within",
          ")": ""
        },
        {
          ")\n)": "",
          ")": "field of conditional symbolic music generation,\nthe integration"
        },
        {
          ")\n)": "the symbolic domain.",
          ")": ""
        },
        {
          ")\n)": "",
          ")": "of diverse prompt\ntypes\n(such as\nimages, videos,\ntexts,\ntags,"
        },
        {
          ")\n)": "ˇ\n)",
          ")": ""
        },
        {
          ")\n)": "All\nthe authors are with Tencent.\n: Equal contribution.",
          ")": "and\nhumming) within\na\nsingle\ngenerative model\nremains"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "assessment and data, each described as follows:": "1)\nInput: multi-modal\nparsing. A versatile\nframework",
          "than the previous largest emotion-labeled dataset ELMG [12]": "in terms of song size, as shown in Table III."
        },
        {
          "assessment and data, each described as follows:": "should support various multi-modal prompts as inputs. Given",
          "than the previous largest emotion-labeled dataset ELMG [12]": "Our main contributions are summarized as follows:"
        },
        {
          "assessment and data, each described as follows:": "the inherent differences among multi-modal data,\nthe primary",
          "than the previous largest emotion-labeled dataset ELMG [12]": ""
        },
        {
          "assessment and data, each described as follows:": "",
          "than the previous largest emotion-labeled dataset ELMG [12]": "• We\nintroduce\na multi-modal\ncontrollable\nframework,"
        },
        {
          "assessment and data, each described as follows:": "challenge\nin multi-modal prompt parsing lies\nin effectively",
          "than the previous largest emotion-labeled dataset ELMG [12]": ""
        },
        {
          "assessment and data, each described as follows:": "",
          "than the previous largest emotion-labeled dataset ELMG [12]": "termed XMusic,\nfor symbolic music generation. XMusic"
        },
        {
          "assessment and data, each described as follows:": "processing and extracting musical\ninformation from hetero-",
          "than the previous largest emotion-labeled dataset ELMG [12]": ""
        },
        {
          "assessment and data, each described as follows:": "",
          "than the previous largest emotion-labeled dataset ELMG [12]": "supports various\ntypes of prompts\n(i.e.,\nimages, videos,"
        },
        {
          "assessment and data, each described as follows:": "geneous data\nsources. To address\nthis, we propose\na multi-",
          "than the previous largest emotion-labeled dataset ELMG [12]": ""
        },
        {
          "assessment and data, each described as follows:": "",
          "than the previous largest emotion-labeled dataset ELMG [12]": "texts,\ntags, and humming) as inputs and generates emo-"
        },
        {
          "assessment and data, each described as follows:": "modal prompt parsing method,\ntermed XProjector. This pro-",
          "than the previous largest emotion-labeled dataset ELMG [12]": ""
        },
        {
          "assessment and data, each described as follows:": "",
          "than the previous largest emotion-labeled dataset ELMG [12]": "tionally controllable, high-quality music\ntailored to the"
        },
        {
          "assessment and data, each described as follows:": "jector contains a novel projection space for\nsymbolic music",
          "than the previous largest emotion-labeled dataset ELMG [12]": ""
        },
        {
          "assessment and data, each described as follows:": "",
          "than the previous largest emotion-labeled dataset ELMG [12]": "provided prompts."
        },
        {
          "assessment and data, each described as follows:": "elements,\nserving as\na\nbridge between diverse multi-modal",
          "than the previous largest emotion-labeled dataset ELMG [12]": ""
        },
        {
          "assessment and data, each described as follows:": "",
          "than the previous largest emotion-labeled dataset ELMG [12]": "• We propose XProjector\nto parse various\ninput prompts"
        },
        {
          "assessment and data, each described as follows:": "prompts and core musical elements.\nIn XProjector, multiple",
          "than the previous largest emotion-labeled dataset ELMG [12]": ""
        },
        {
          "assessment and data, each described as follows:": "",
          "than the previous largest emotion-labeled dataset ELMG [12]": "into specific\nsymbolic music\nelements. These\nelements"
        },
        {
          "assessment and data, each described as follows:": "prompt\ntypes (i.e.,\nimages, videos,\ntexts,\ntags, and humming)",
          "than the previous largest emotion-labeled dataset ELMG [12]": ""
        },
        {
          "assessment and data, each described as follows:": "",
          "than the previous largest emotion-labeled dataset ELMG [12]": "then serve as control signals that guide the music gener-"
        },
        {
          "assessment and data, each described as follows:": "are analyzed and mapped to specific musical elements\n(i.e.,",
          "than the previous largest emotion-labeled dataset ELMG [12]": ""
        },
        {
          "assessment and data, each described as follows:": "",
          "than the previous largest emotion-labeled dataset ELMG [12]": "ation process."
        },
        {
          "assessment and data, each described as follows:": "emotions, genres,\nrhythms,\nand notes),\nas\nshown in Fig. 1.",
          "than the previous largest emotion-labeled dataset ELMG [12]": ""
        },
        {
          "assessment and data, each described as follows:": "",
          "than the previous largest emotion-labeled dataset ELMG [12]": "• We design a music composer called XComposer, which"
        },
        {
          "assessment and data, each described as follows:": "For\ninstance,\ntemporally-related prompts, such as videos and",
          "than the previous largest emotion-labeled dataset ELMG [12]": ""
        },
        {
          "assessment and data, each described as follows:": "",
          "than the previous largest emotion-labeled dataset ELMG [12]": "includes\na Generator\nthat\ncreates music\nby\nfollowing"
        },
        {
          "assessment and data, each described as follows:": "humming,\nare\ntranslated\ninto\nrhythm elements\nto maintain",
          "than the previous largest emotion-labeled dataset ELMG [12]": ""
        },
        {
          "assessment and data, each described as follows:": "",
          "than the previous largest emotion-labeled dataset ELMG [12]": "control\nsignals, and a Selector\nthat evaluates and filters"
        },
        {
          "assessment and data, each described as follows:": "temporal\nconsistency. Emotional\nprompts,\nsuch\nas\nimages,",
          "than the previous largest emotion-labeled dataset ELMG [12]": ""
        },
        {
          "assessment and data, each described as follows:": "",
          "than the previous largest emotion-labeled dataset ELMG [12]": "the generated music via a multi-task learning scheme."
        },
        {
          "assessment and data, each described as follows:": "videos,\nand\ntexts,\nare mapped\nto\ncorresponding\nemotional",
          "than the previous largest emotion-labeled dataset ELMG [12]": ""
        },
        {
          "assessment and data, each described as follows:": "",
          "than the previous largest emotion-labeled dataset ELMG [12]": "• We build XMIDI,\nthe largest\nsymbolic music dataset\nto"
        },
        {
          "assessment and data, each described as follows:": "elements\nto ensure\nthe generated music\naccurately conveys",
          "than the previous largest emotion-labeled dataset ELMG [12]": ""
        },
        {
          "assessment and data, each described as follows:": "",
          "than the previous largest emotion-labeled dataset ELMG [12]": "date.\nIt\nis manually\nannotated\nby\nexperts\nto\nfacilitate"
        },
        {
          "assessment and data, each described as follows:": "the intended emotions. This approach harmonizes multi-modal",
          "than the previous largest emotion-labeled dataset ELMG [12]": ""
        },
        {
          "assessment and data, each described as follows:": "",
          "than the previous largest emotion-labeled dataset ELMG [12]": "automatic music\ngeneration with\nprecise\nemotion\nand"
        },
        {
          "assessment and data, each described as follows:": "prompts by translating them into a unified projection space of",
          "than the previous largest emotion-labeled dataset ELMG [12]": ""
        },
        {
          "assessment and data, each described as follows:": "",
          "than the previous largest emotion-labeled dataset ELMG [12]": "genre labels. The XMIDI dataset will be made publicly"
        },
        {
          "assessment and data, each described as follows:": "musical elements.",
          "than the previous largest emotion-labeled dataset ELMG [12]": ""
        },
        {
          "assessment and data, each described as follows:": "",
          "than the previous largest emotion-labeled dataset ELMG [12]": "available."
        },
        {
          "assessment and data, each described as follows:": "2) Representation:\nprecise\ncontrol. An\noptimal\nrepre-",
          "than the previous largest emotion-labeled dataset ELMG [12]": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2": "bolic music\ndatasets with\nfine-grained\nemotion\nand\ngenre"
        },
        {
          "2": "annotations are scarce and challenging to collect. To address"
        },
        {
          "2": "this gap, we construct XMIDI, a large-scale dataset comprising"
        },
        {
          "2": "108,023 MIDI files with precise and diverse emotion and genre"
        },
        {
          "2": "labels. The XMIDI dataset\nis approximately 10 times\nlarger"
        },
        {
          "2": "than the previous largest emotion-labeled dataset ELMG [12]"
        },
        {
          "2": "in terms of song size, as shown in Table III."
        },
        {
          "2": "Our main contributions are summarized as follows:"
        },
        {
          "2": ""
        },
        {
          "2": "• We\nintroduce\na multi-modal\ncontrollable\nframework,"
        },
        {
          "2": ""
        },
        {
          "2": "termed XMusic,\nfor symbolic music generation. XMusic"
        },
        {
          "2": ""
        },
        {
          "2": "supports various\ntypes of prompts\n(i.e.,\nimages, videos,"
        },
        {
          "2": ""
        },
        {
          "2": "texts,\ntags, and humming) as inputs and generates emo-"
        },
        {
          "2": ""
        },
        {
          "2": "tionally controllable, high-quality music\ntailored to the"
        },
        {
          "2": ""
        },
        {
          "2": "provided prompts."
        },
        {
          "2": ""
        },
        {
          "2": "• We propose XProjector\nto parse various\ninput prompts"
        },
        {
          "2": ""
        },
        {
          "2": "into specific\nsymbolic music\nelements. These\nelements"
        },
        {
          "2": ""
        },
        {
          "2": "then serve as control signals that guide the music gener-"
        },
        {
          "2": ""
        },
        {
          "2": "ation process."
        },
        {
          "2": ""
        },
        {
          "2": "• We design a music composer called XComposer, which"
        },
        {
          "2": ""
        },
        {
          "2": "includes\na Generator\nthat\ncreates music\nby\nfollowing"
        },
        {
          "2": ""
        },
        {
          "2": "control\nsignals, and a Selector\nthat evaluates and filters"
        },
        {
          "2": ""
        },
        {
          "2": "the generated music via a multi-task learning scheme."
        },
        {
          "2": ""
        },
        {
          "2": "• We build XMIDI,\nthe largest\nsymbolic music dataset\nto"
        },
        {
          "2": ""
        },
        {
          "2": "date.\nIt\nis manually\nannotated\nby\nexperts\nto\nfacilitate"
        },
        {
          "2": ""
        },
        {
          "2": "automatic music\ngeneration with\nprecise\nemotion\nand"
        },
        {
          "2": ""
        },
        {
          "2": "genre labels. The XMIDI dataset will be made publicly"
        },
        {
          "2": ""
        },
        {
          "2": "available."
        },
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": "II. RELATED WORK"
        },
        {
          "2": ""
        },
        {
          "2": "A. Artificial\nIntelligence Generated Content\n(AIGC)"
        },
        {
          "2": ""
        },
        {
          "2": "AIGC aims\nto utilize AI\ntechnology to automate\ncontent"
        },
        {
          "2": ""
        },
        {
          "2": "production while addressing human individual\nrequirements."
        },
        {
          "2": ""
        },
        {
          "2": "Recently, AIGC has\ndemonstrated\nsignificant\npotential\nin"
        },
        {
          "2": ""
        },
        {
          "2": "generating high-quality content\nthat closely resembles human-"
        },
        {
          "2": ""
        },
        {
          "2": "generated\ncontent\n(HGC),\nparticularly\nin\nthe\nareas\nof\ntext"
        },
        {
          "2": ""
        },
        {
          "2": "generation\n[21]–[23]\nand\nimage\nsynthesis\n[24]–[26]. De-"
        },
        {
          "2": ""
        },
        {
          "2": "spite\nrecent progress,\nthe field of music generation remains"
        },
        {
          "2": ""
        },
        {
          "2": "relatively\nunderexplored within\nthe AIGC community. AI-"
        },
        {
          "2": ""
        },
        {
          "2": "generated music still\nlacks\nthe emotional depth and melodic"
        },
        {
          "2": ""
        },
        {
          "2": "richness\ntypically\nfound\nin\nhuman-composed music\npieces."
        },
        {
          "2": ""
        },
        {
          "2": "Recent studies [1]–[5], [27] have focused on generating audio-"
        },
        {
          "2": ""
        },
        {
          "2": "based music from textual inputs. However, audio-based music"
        },
        {
          "2": ""
        },
        {
          "2": "generation faces challenges, such as limited editability and the"
        },
        {
          "2": ""
        },
        {
          "2": "inability to finely control attributes like tempo, pitch, duration,"
        },
        {
          "2": ""
        },
        {
          "2": "and rhythm. In contrast, symbolic music, which represents mu-"
        },
        {
          "2": ""
        },
        {
          "2": "sical\nideas through notation, offers more flexible and precise"
        },
        {
          "2": ""
        },
        {
          "2": "control over\nthese attributes. Therefore,\nthis paper\nfocuses on"
        },
        {
          "2": ""
        },
        {
          "2": "music generation in symbolic domain. We present XMusic, a"
        },
        {
          "2": ""
        },
        {
          "2": "universal symbolic music generation framework that supports"
        },
        {
          "2": ""
        },
        {
          "2": "flexible prompts,\nand XMIDI,\na\nlarge-scale\nsymbolic music"
        },
        {
          "2": ""
        },
        {
          "2": "dataset annotated with precise emotion and genre labels."
        },
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": "B.\nSymbolic Music Representations"
        },
        {
          "2": ""
        },
        {
          "2": "Conventional\nsymbolic music representations can be clas-"
        },
        {
          "2": "sified\ninto\ntwo main\ncategories:\nimage-like\n[28],\n[29]\nand"
        },
        {
          "2": "MIDI-like\n[8],\n[30],\n[31]\nrepresentations.\nThe\nimage-like"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3": "facilitating research on emotion-conditioned symbolic music"
        },
        {
          "3": "generation [12],\n[15],\n[39]."
        },
        {
          "3": "3)\nSequence-conditioned\nSymbolic Music\nGeneration:"
        },
        {
          "3": "These methods\ntypically employ a conditioning sequence as"
        },
        {
          "3": "a prior\nto generate\na\ncontinuation or\nextension accordingly."
        },
        {
          "3": "Standard\nsequence\nprompts\nfor music\ngeneration\ninclude"
        },
        {
          "3": "lead sheets, motifs, melodies,\nthemes, and lyrics, which can"
        },
        {
          "3": "be\ndirectly\nextracted\nfrom musical\npieces\nto\nform training"
        },
        {
          "3": "pairs. Huang et al.\n[8] demonstrate\nthe first\nsuccessful\nap-"
        },
        {
          "3": "plication of Transformers to produce accompaniments condi-"
        },
        {
          "3": "tioned on melodies. MELONS [16]\nis\nanother\ntransformer-"
        },
        {
          "3": "based framework that generates full-song melodies with long-"
        },
        {
          "3": "term structures\ngiven motifs. MGM [40]\nlearns motif-level"
        },
        {
          "3": "repetitions\nand\nintegrates\nthem into\nthe music\ngeneration"
        },
        {
          "3": "process. Theme Transformer\n[41]\nintroduces\na\ntheme-based"
        },
        {
          "3": "conditioning\napproach\nthat\ncompels\nthe model\nto manifest"
        },
        {
          "3": "the\ngiven\ntheme multiple\ntimes\nin\nits\nresulting\ngeneration."
        },
        {
          "3": "UP-Transformer\n[42]\nfocuses on user preference-based music"
        },
        {
          "3": "transfer, utilizing a single piece of a user’s favorite music as"
        },
        {
          "3": "the condition for\ntransferring musical styles. The relationship"
        },
        {
          "3": "between melody\nand\nlyrics\nis\ncrucial\nfor\nsymbolic music"
        },
        {
          "3": "generation. Yu et al.\n[43] propose a conditional LSTM-GAN"
        },
        {
          "3": "model\nthat generates melodies\nfrom lyrics by leveraging the"
        },
        {
          "3": "syntactic structures of the lyrics through LSTM networks. This"
        },
        {
          "3": "approach ensures the generated sequences align with the lyrics"
        },
        {
          "3": "and mimic\nthe\ndistribution\nof\nreal melody\nsamples. Zhang"
        },
        {
          "3": "et\nal.\n[44]\npresent\na\nnovel Transformer-based\napproach\nto"
        },
        {
          "3": "generate\nsyllable-level\nlyrics\nfrom melodies,\nemploying\nan"
        },
        {
          "3": "explicit n-gram (EXPLING) loss function and a prior attention"
        },
        {
          "3": "mechanism to improve\nsequence\nalignment\nand controllable"
        },
        {
          "3": ""
        },
        {
          "3": "lyric generation. Duan et al.\n[45]\naddress\nthe\nchallenge of"
        },
        {
          "3": "interpretability in melody generation from lyrics by integrating"
        },
        {
          "3": "mutual\ninformation constraints and Transformer-based seman-"
        },
        {
          "3": "tic feature extraction."
        },
        {
          "3": "4) Video-conditioned\nSymbolic Music Generation: Most"
        },
        {
          "3": "previous methods for video-conditioned music generation fo-"
        },
        {
          "3": "cus on composing music from silent performance videos [17],"
        },
        {
          "3": "[18],\na\nprocess\nakin\nto\nvisual music\ntranscription. The\nin-"
        },
        {
          "3": "strument\ntype\nand rhythm can be\ninferred from visual\ncues"
        },
        {
          "3": "(e.g., performance venue, musician’s actions, etc), which limits"
        },
        {
          "3": "music diversity to some\nextend. Recent methods\n[46],\n[47]"
        },
        {
          "3": "take dance or human action videos as input, generating music"
        },
        {
          "3": "pieces\nthat plausibly match the\ncorresponding visual\ninput."
        },
        {
          "3": "However,\nthese methods cannot be applied to general videos"
        },
        {
          "3": "as\nthey rely on additional keypoint\nannotations. CMT [19]"
        },
        {
          "3": "generates background music for general videos by establishing"
        },
        {
          "3": "rule-based\nrhythmic\nvideo-music\nrelationships. To mitigate"
        },
        {
          "3": "potential\nstyle conflicts caused by this\nrule-based design, V-"
        },
        {
          "3": "MusProd [20] introduces semantic-level correspondence. This"
        },
        {
          "3": "approach decouples music generation into three progressive"
        },
        {
          "3": "stages\n(chords, melody,\nand\naccompaniment)\nand\nextracts"
        },
        {
          "3": "video-music relational\nfeatures\n(semantic, color, motion)\nfor"
        },
        {
          "3": "guidance."
        },
        {
          "3": "5) X-conditioned Symbolic Music Generation: This paper"
        },
        {
          "3": "introduces the X-conditioned framework, where X represents"
        },
        {
          "3": "various\ntypes of prompts,\nsuch as\nimages, videos,\ntext,\ntags,"
        },
        {
          "3": "and\nhumming. Our XMusic\nis\na multi-modal\ncontrollable"
        },
        {
          "3": "symbolic music\ngeneration\nframework\ndesigned\nto\nsupport"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Notes\nRepresentation\nRhythm\nGeneration": "Fig. 2:\nIllustration of\nthe proposed XMusic, which supports flexible (a) X-Prompts\nto guide the generation of high-quality"
        },
        {
          "Notes\nRepresentation\nRhythm\nGeneration": "symbolic music. The XProjector analyzes these prompts, mapping them to symbolic music elements within the (b) Projection"
        },
        {
          "Notes\nRepresentation\nRhythm\nGeneration": "Space. Subsequently, the (c) Generator of XComposer transforms these symbolic music elements into token sequences based on"
        },
        {
          "Notes\nRepresentation\nRhythm\nGeneration": "our enhanced representation. It employs a Transformer Decoder as the generative model\nto predict successive events iteratively,"
        },
        {
          "Notes\nRepresentation\nRhythm\nGeneration": "thereby creating complete musical compositions. Finally,\nthe (d) Selector of XComposer utilizes a Transformer Encoder\nto"
        },
        {
          "Notes\nRepresentation\nRhythm\nGeneration": "encode the complete token sequences and employs a multi-task learning scheme to evaluate the quality of the generated music."
        },
        {
          "Notes\nRepresentation\nRhythm\nGeneration": "versatile prompts.\nmusic elements, emotions (P E), genres (P G),\nrhythms (P R),"
        },
        {
          "Notes\nRepresentation\nRhythm\nGeneration": "and notes (P N ),\nrepresented as {P E, P G, P R, P N } ∈ P."
        },
        {
          "Notes\nRepresentation\nRhythm\nGeneration": "III. METHOD"
        },
        {
          "Notes\nRepresentation\nRhythm\nGeneration": "The\nemotion element P E ∈ RDE\nand the genre\nelement"
        },
        {
          "Notes\nRepresentation\nRhythm\nGeneration": "Our\nproposed XMusic\nsupports\nvarious\ntypes\nof\ncontent\nP G ∈ RDG are expressed as one-hot vectors, where DE and"
        },
        {
          "Notes\nRepresentation\nRhythm\nGeneration": "re-\nas prompts\nfor generating high-quality music. As\nshown in\nDG denote the number of emotion and genre categories,"
        },
        {
          "Notes\nRepresentation\nRhythm\nGeneration": "spectively. In this paper, P E can be chosen from 11 emotions:\nFig. 2, the process is divided into three stages: parsing, control,"
        },
        {
          "Notes\nRepresentation\nRhythm\nGeneration": "exciting, warm, happy, romantic, funny, sad, angry, lazy, quiet,\nand selection. First, XProjector (Sec. III-A) analyzes the input"
        },
        {
          "Notes\nRepresentation\nRhythm\nGeneration": "fear, and magnificent. Similarly, P G offers 6 common genre\ncontent and parses it\ninto symbolic music elements within the"
        },
        {
          "Notes\nRepresentation\nRhythm\nGeneration": "choices:\nrock, pop, country,\njazz, classical, and folk.\nprojection space. Second, XComposer (Sec. III-B) maps these"
        },
        {
          "Notes\nRepresentation\nRhythm\nGeneration": "elements\nto token sequences\nand controls\nthe Generator\nto"
        },
        {
          "Notes\nRepresentation\nRhythm\nGeneration": "The\nrhythm element\nspans\nthe\nrange of bars\nand is\nrep-"
        },
        {
          "Notes\nRepresentation\nRhythm\nGeneration": "generate corresponding music. Finally,\nthe Selector evaluates"
        },
        {
          "Notes\nRepresentation\nRhythm\nGeneration": "resented as P R = {pr\nthe total\n, where NBar denotes\ni }NBar"
        },
        {
          "Notes\nRepresentation\nRhythm\nGeneration": "the quality of the generated music batches and selects the one"
        },
        {
          "Notes\nRepresentation\nRhythm\nGeneration": "number of bars. pr\nrepresents the rhythmic component of\nthe\ni"
        },
        {
          "Notes\nRepresentation\nRhythm\nGeneration": "with the highest quality score. The\nsymbolic music dataset"
        },
        {
          "Notes\nRepresentation\nRhythm\nGeneration": ", pbeat\n, ..., pbeat\ni-th bar and can be expanded as {pbar\n}, with in"
        },
        {
          "Notes\nRepresentation\nRhythm\nGeneration": "i\ni1\nin"
        },
        {
          "Notes\nRepresentation\nRhythm\nGeneration": "XMIDI\nis introduced in Sec.\nIII-C."
        },
        {
          "Notes\nRepresentation\nRhythm\nGeneration": "indicating the number of beats within the bar. Specifically,\nthe"
        },
        {
          "Notes\nRepresentation\nRhythm\nGeneration": "bar element pbar = (bar, density) ∈ R2\nrecords\nthe starting"
        },
        {
          "Notes\nRepresentation\nRhythm\nGeneration": "A. XProjector\nposition and note density of\nthe\ncurrent bar, while\nthe beat"
        },
        {
          "Notes\nRepresentation\nRhythm\nGeneration": "element pbeat = (beat, tempo, strength) ∈ R3\ncaptures\nthe"
        },
        {
          "Notes\nRepresentation\nRhythm\nGeneration": "The projection space\nof\nsymbolic music\nelements, de-"
        },
        {
          "Notes\nRepresentation\nRhythm\nGeneration": "starting position,\ntempo and intensity of\nthe beat."
        },
        {
          "Notes\nRepresentation\nRhythm\nGeneration": "noted as P, acts as a bridge between multi-modal content and"
        },
        {
          "Notes\nRepresentation\nRhythm\nGeneration": "symbolic music. This\nspace includes\nfour\ntypes of\nsymbolic\nThe note\nelement\ncovers\nthe\nrange of\na\nsingle note\nand"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5": ""
        },
        {
          "5": "similarities between the input\ntext and synonymous descrip-"
        },
        {
          "5": ""
        },
        {
          "5": "tions of each emotion e. These similarities are then normalized"
        },
        {
          "5": "via the Softmax function to produce an emotion score Se(text)"
        },
        {
          "5": "as follows:"
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": "Se(text)}\n(2)\nFXP (text) = {P E} = {argmax"
        },
        {
          "5": "e∈E"
        },
        {
          "5": ""
        },
        {
          "5": "In tag-conditioned music generation, users can select from"
        },
        {
          "5": "11 emotion tags and 6 genre tags to guide the process. Once a"
        },
        {
          "5": ""
        },
        {
          "5": "tag is chosen, XProjector activates the corresponding emotion"
        },
        {
          "5": "or genre element within the projection space:"
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": "FXP (tage) = {P E} = {tage}"
        },
        {
          "5": "(3)"
        },
        {
          "5": "FXP (tagg) = {P G} = {tagg}"
        },
        {
          "5": ""
        },
        {
          "5": "Video prompts, which are spatio-temporal data, guide both"
        },
        {
          "5": "global and local attributes of\nthe music sequence. For video-"
        },
        {
          "5": "conditioned music generation, XProjector analyzes sentiment,"
        },
        {
          "5": "motion, and scene transitions within the input video, mapping"
        },
        {
          "5": "these factors to the appropriate rhythm and emotion elements"
        },
        {
          "5": "in the projection space. This ensures a high degree of synchro-"
        },
        {
          "5": "nization between the generated music and the video content."
        },
        {
          "5": "We observe a significant correlation between video back-"
        },
        {
          "5": "ground music\ntempo\nand\nscene\ntransition\nfrequency.\nFor"
        },
        {
          "5": "example, montage videos with rapid scene transitions typically"
        },
        {
          "5": "feature fast-paced music, while peaceful scenery videos are of-"
        },
        {
          "5": "ten paired with slower tempos. To formalize this relationship,"
        },
        {
          "5": ""
        },
        {
          "5": "to control\nwe introduce a scene transition rate metric Rscene"
        },
        {
          "5": ""
        },
        {
          "5": "the music tempo tmusic:"
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": "Nscene"
        },
        {
          "5": "Rscene ="
        },
        {
          "5": ""
        },
        {
          "5": "(4)\nTvideo"
        },
        {
          "5": "tmusic = tinit + tinc ∗ tanh(Rscene)"
        },
        {
          "5": ""
        },
        {
          "5": "Here, Nscene denotes the total number of scene transitions"
        },
        {
          "5": "(computed using PySceneDetect [51]), while Tvideo represents"
        },
        {
          "5": ""
        },
        {
          "5": "(mea-\nthe video duration in seconds. The music tempo tmusic"
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": "sured in bpm)\ntempo\nis derived from Rscene, with an initial"
        },
        {
          "5": "tinit and incremental\ntempo tinc. The tanh activation function"
        },
        {
          "5": ""
        },
        {
          "5": "ensures\nthat\nthe\ncoefficient\nfor\nremains between 0 and\ntinc"
        },
        {
          "5": ""
        },
        {
          "5": "1. Our analysis of\ntempo distributions in the training dataset"
        },
        {
          "5": ""
        },
        {
          "5": "shows that 98.6% of musical\ntempos fall within the 60∼130"
        },
        {
          "5": ""
        },
        {
          "5": "bpm range. Accordingly, we set\ntinit = 60 and tinc = 70 to"
        },
        {
          "5": ""
        },
        {
          "5": "keep generated tempos within this range."
        },
        {
          "5": ""
        },
        {
          "5": "Regarding emotions, we determine the emotional category"
        },
        {
          "5": ""
        },
        {
          "5": "of the input video through sentiment analysis and activate the"
        },
        {
          "5": ""
        },
        {
          "5": "corresponding emotion element within the projection space to"
        },
        {
          "5": ""
        },
        {
          "5": "control\nthe emotional style of the music. The video sentiment"
        },
        {
          "5": ""
        },
        {
          "5": "analysis module\ncomputes\nan emotion score Se(video)\nfor"
        },
        {
          "5": ""
        },
        {
          "5": "the video and selects\nthe emotion with the highest\nscore as"
        },
        {
          "5": ""
        },
        {
          "5": "the analysis result. The calculation formula is as follows:"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6": "Token Sequences on the i-th Bar"
        },
        {
          "6": "Time\nTime"
        },
        {
          "6": "… …\n… …"
        },
        {
          "6": "Tempo\nChord\nStrength\nBeat\nTempo\nChord"
        },
        {
          "6": "Beat\nRhythm\nRhythm\n…\n…"
        },
        {
          "6": "Pitch\nPitch\nDuration Velocity\nDuration Velocity\nNote\nNote"
        },
        {
          "6": "Instrument\nProgram"
        },
        {
          "6": "…\n…"
        },
        {
          "6": "Pitch\nPitch\nDuration Velocity\nDuration Velocity\nNote\nNote"
        },
        {
          "6": "Instrument\nProgram"
        },
        {
          "6": ""
        },
        {
          "6": "Tempo\nChord\nStrength\nBeat\nTempo\nChord\nBeat\nRhythm\nRhythm"
        },
        {
          "6": "Bar\nDensity\nBar\nRhythm\nRhythm"
        },
        {
          "6": "Tag\nEmotion\nGenre"
        },
        {
          "6": "(a) Our Representation\n(b) CP Representation"
        },
        {
          "6": ""
        },
        {
          "6": "Fig. 3: Comparison between our representation and Compound"
        },
        {
          "6": "Word (CP)\n[10] representation. The dotted boxes represent our"
        },
        {
          "6": "new tokens in comparison with those of the CP representation."
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": "within the projection space to generate the subsequent music"
        },
        {
          "6": ""
        },
        {
          "6": "sequences. The detailed formulas are as follows:"
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": "pbar\n))\n∗ Nbpb ∗ (i − 1), D(M bari\n= (bari, densityi) = (T Mstd"
        },
        {
          "6": "std"
        },
        {
          "6": "pbeat\n= (beati;j, tempoi;j, strengthi;j)"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Humming prompts, which are sequential data, guide the": "generation\nprocess\nof\ncomplete music\nby first\nbeing\ntran-",
          "tecture, where": "senting the",
          "tokens belonging to the\nsame\nfamily (repre-": "same\nevent)\nare grouped into a\nsupertoken and"
        },
        {
          "Humming prompts, which are sequential data, guide the": "scribed\ninto\nan\ninitial MIDI\nsequence. XProjector\nemploys",
          "tecture, where": "positioned",
          "tokens belonging to the\nsame\nfamily (repre-": "at\nthe\nsame\ntime\nstep. As\nillustrated\nin Fig.\n3,"
        },
        {
          "Humming prompts, which are sequential data, guide the": "the VOCANO [54]\nalgorithm to\ntranscribe\ninput\nhumming",
          "tecture, where": "XComposer",
          "tokens belonging to the\nsame\nfamily (repre-": "introduces three key improvements:"
        },
        {
          "Humming prompts, which are sequential data, guide the": "=\naudio\ninto\nan\noriginal MIDI\nsequence,\ni.e., Morigin",
          "tecture, where": "•",
          "tokens belonging to the\nsame\nfamily (repre-": "First, we\nintroduce\na new family token named “Tag”,"
        },
        {
          "Humming prompts, which are sequential data, guide the": "VOCANO(humming). This sequence is then processed using",
          "tecture, where": "",
          "tokens belonging to the\nsame\nfamily (repre-": "along with two corresponding grouped tokens: “Emotion”"
        },
        {
          "Humming prompts, which are sequential data, guide the": "standardization operations (beat processing and note quantiza-",
          "tecture, where": "",
          "tokens belonging to the\nsame\nfamily (repre-": "and “Genre”. These tokens enable control over the music"
        },
        {
          "Humming prompts, which are sequential data, guide the": "tion)\nto create a standard prior MIDI\nsequence,\ni.e., Mstd =",
          "tecture, where": "",
          "tokens belonging to the\nsame\nfamily (repre-": "generation process by specifying emotion and genre."
        },
        {
          "Humming prompts, which are sequential data, guide the": "the tempo of each\nStandardize(Morigin). For beat processing,",
          "tecture, where": "•",
          "tokens belonging to the\nsame\nfamily (repre-": "Second, we add a new family token called “Instrument”,"
        },
        {
          "Humming prompts, which are sequential data, guide the": "beat\nis derived from the time intervals between adjacent beats,",
          "tecture, where": "",
          "tokens belonging to the\nsame\nfamily (repre-": "with its corresponding grouped token, “Program”, ensur-"
        },
        {
          "Humming prompts, which are sequential data, guide the": "adjusting the default\ntempo of 120 bpm in the\ntranscribed",
          "tecture, where": "",
          "tokens belonging to the\nsame\nfamily (repre-": "ing the generation of multi-track music."
        },
        {
          "Humming prompts, which are sequential data, guide the": "sequence to the actual\ntempo. Note quantization adjusts\nthe",
          "tecture, where": "",
          "tokens belonging to the\nsame\nfamily (repre-": "• Third, within the “Rhythm” family token, we incorporate"
        },
        {
          "Humming prompts, which are sequential data, guide the": "onset\nand offset positions of\neach note\nto the nearest 32nd",
          "tecture, where": "",
          "tokens belonging to the\nsame\nfamily (repre-": "the\ngrouped\ntokens\n“Density”\nand\n“Strength”\ninto\nbar"
        },
        {
          "Humming prompts, which are sequential data, guide the": "note\nposition. Finally,\ninformation\nis\norganized\nfrom Mstd",
          "tecture, where": "",
          "tokens belonging to the\nsame\nfamily (repre-": "and beat events,\nrespectively, allowing control over note"
        },
        {
          "Humming prompts, which are sequential data, guide the": "and mapped to the corresponding note and rhythm elements",
          "tecture, where": "",
          "tokens belonging to the\nsame\nfamily (repre-": "density and beat strength."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "7": ""
        },
        {
          "7": "the 16th notes used in CP. Furthermore, to accommodate large-"
        },
        {
          "7": ""
        },
        {
          "7": "scale data training, we have increased the embedding size for"
        },
        {
          "7": ""
        },
        {
          "7": "The Generator, serving as the core component of XCom-"
        },
        {
          "7": ""
        },
        {
          "7": "conditionally\ngenerates\nsymbolic music\nbased\non"
        },
        {
          "7": "enhanced representation.\nIt\nemploys\na Transformer De-"
        },
        {
          "7": ""
        },
        {
          "7": "[6] as\nthe backbone network to effectively model\nthe"
        },
        {
          "7": ""
        },
        {
          "7": ""
        },
        {
          "7": "Specifically, given the first t tokens and aiming to predict the"
        },
        {
          "7": ""
        },
        {
          "7": "token,\nthe process\nis\nstructured as\nfollows.\nInitially,\nthe"
        },
        {
          "7": ""
        },
        {
          "7": "token sequences are transformed into a two-dimensional event"
        },
        {
          "7": ""
        },
        {
          "7": "represents the j-th attribute"
        },
        {
          "7": "i"
        },
        {
          "7": "of the i-th token. Each eventi contains 12 dimensions: family"
        },
        {
          "7": "type, emotion, genre, bar-beat, tempo, chord, density, strength,"
        },
        {
          "7": ""
        },
        {
          "7": "program, pitch, duration, and velocity. Subsequently, at\ntime"
        },
        {
          "7": ""
        },
        {
          "7": "i, a linear projection is applied to each one-hot vector eventj\ni ,"
        },
        {
          "7": ""
        },
        {
          "7": "embedj"
        },
        {
          "7": "i . These dense vectors"
        },
        {
          "7": "representation concati"
        },
        {
          "7": "the\ncurrent\ntoken. Following this,\nlinear projection and"
        },
        {
          "7": ""
        },
        {
          "7": "to obtain the input"
        },
        {
          "7": ""
        },
        {
          "7": "feature inputi for the Transformer network at time i. The input"
        },
        {
          "7": ""
        },
        {
          "7": "t\nfrom the first\ntokens are fed into the Transformer"
        },
        {
          "7": ""
        },
        {
          "7": "at\nthe current\ntime"
        },
        {
          "7": ""
        },
        {
          "7": "is\nthen predicted by applying"
        },
        {
          "7": ""
        },
        {
          "7": "In line with the approach\nto Ht."
        },
        {
          "7": ""
        },
        {
          "7": "by\n[10],\nthe\nnext\nfamily\ntype\nis\npredicted\nfirst,"
        },
        {
          "7": ""
        },
        {
          "7": "the other\nattributes based on"
        },
        {
          "7": ""
        },
        {
          "7": "Ht and the one-hot vector of the predicted family type. Cross-"
        },
        {
          "7": ""
        },
        {
          "7": "entropy loss is utilized to optimize this prediction process. The"
        },
        {
          "7": ""
        },
        {
          "7": "overall procedure can be expressed as follows:"
        },
        {
          "7": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "8": "TABLE III: Comparison\nbetween\nexisting\nemotion-labeled"
        },
        {
          "8": ""
        },
        {
          "8": "MIDI datasets and the proposed XMIDI dataset."
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": "Dataset\nEmotion Type\nGenre Type\nSize (Songs)"
        },
        {
          "8": ""
        },
        {
          "8": "MIREX-like [55]\n5 classes\nmultiple\n193"
        },
        {
          "8": "VGMIDI\n[56]\nvalence\nvideo game\n95"
        },
        {
          "8": ""
        },
        {
          "8": "EMOPIA [11]\nRussell’s 4Q\npop\n387"
        },
        {
          "8": ""
        },
        {
          "8": "ELMG [12]\n2 classes\nmultiple\n11,528"
        },
        {
          "8": ""
        },
        {
          "8": "XMIDI (Ours)\n108,023\n11 classes\n6 classes"
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": "symbolic music dataset with precise emotion and genre labels,"
        },
        {
          "8": ""
        },
        {
          "8": "comprising 108,023 MIDI files. The average duration of\nthe"
        },
        {
          "8": ""
        },
        {
          "8": "music pieces is around 176 seconds, resulting in a total dataset"
        },
        {
          "8": ""
        },
        {
          "8": "length of around 5,278 hours."
        },
        {
          "8": ""
        },
        {
          "8": "For data collection and cleaning, we first crawled MIDI files"
        },
        {
          "8": ""
        },
        {
          "8": "from various online sources,\nincluding the Internet Archive,"
        },
        {
          "8": ""
        },
        {
          "8": "GitHub, and Reddit. To ensure dataset quality, we carried out"
        },
        {
          "8": ""
        },
        {
          "8": "the following data cleaning steps.\ni) Automatic Cleaning: We"
        },
        {
          "8": ""
        },
        {
          "8": "removed corrupted or\nempty files\nand performed basic de-"
        },
        {
          "8": ""
        },
        {
          "8": "duplication based on MD5 file hashes.\nii) Data Deduplication:"
        },
        {
          "8": ""
        },
        {
          "8": "We\nrendered\nthe\nremaining MIDI\nfiles\ninto\naudio\nformat,"
        },
        {
          "8": ""
        },
        {
          "8": "extracted chroma features, and calculated cosine similarities to"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "i }12\nj=1)": ""
        },
        {
          "i }12\nj=1)": "input′\ni = Linearinput′ (PositionalEncoding(concat′"
        },
        {
          "i }12\nj=1)": ""
        },
        {
          "i }12\nj=1)": ""
        },
        {
          "i }12\nj=1)": "i = 1, ..., T and j = 1, ..., 12."
        },
        {
          "i }12\nj=1)": ""
        },
        {
          "i }12\nj=1)": ""
        },
        {
          "i }12\nj=1)": "{Fi}T\ni=1 = TransformerEncoder({input′"
        },
        {
          "i }12\nj=1)": ""
        },
        {
          "i }12\nj=1)": "Fencoder = GAPtime({Fi}T\ni=1)"
        },
        {
          "i }12\nj=1)": ""
        },
        {
          "i }12\nj=1)": "Probgenre = Softmax(FCgenre(Fencoder))"
        },
        {
          "i }12\nj=1)": ""
        },
        {
          "i }12\nj=1)": "Probemotion = Softmax(FCemotion(Fencoder))"
        },
        {
          "i }12\nj=1)": "Probquality = Softmax(FCquality(Fencoder))"
        },
        {
          "i }12\nj=1)": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": "The\ndata\nstatistics\nof\nour XMIDI\ndataset\nare\nshown\nin"
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": "Fig. 4.\nIn terms of emotion distribution (Fig. 4a), categories"
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": "such\nas\nexciting, warm,\nhappy,\nromantic,\nfunny,\nsad,\nand"
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": "angry dominate, whereas\nemotion like\nlazy, quiet,\nfear,\nand"
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": "magnificent are less frequent. The genre distribution (Fig. 4b)"
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": "is relatively balanced, with Rock representing the largest share"
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": "and folk music the smallest. Regarding music length (Fig. 4c),"
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": "most compositions span between 1 and 5 minutes, with shorter"
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": "and longer pieces being less common."
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": "We compared XMIDI dataset with existing emotion-labeled"
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": "MIDI\ndatasets\nin\nterms\nof\nemotion\ncategories,\ngenre\ntypes"
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": "and\ndata\nsize. As\nshown\nin\nTable\nIII,\nprevious\nemotion"
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": "datasets [11], [55], [56] are relatively small,\ntypically contain-"
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": "ing only a few hundred songs. Bao et al. [12] recently built a"
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": "large-scale paired lyric-melody dataset annotated using deep"
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": "learning models. However,\ntheir\nemotion labels\nare\ncoarse-"
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": "grained (positive/negative), and automated labeling introduces"
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": ""
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": "the risk of mis-classification.\nIn contrast, our XMIDI offers"
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": ""
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": "finer emotion categories (11 distinct emotions), more precise"
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": ""
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": "annotations\n(annotated by experts and cross-validated) and a"
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": ""
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": "nearly 10-times-larger data size."
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": ""
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": ""
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": ""
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": "IV. EXPERIMENTS"
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": ""
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": "A. Datasets"
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": ""
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": ""
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": "1)\nSymbolic Music Generation: We developed XMIDI,\nthe"
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": ""
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": "largest\nsymbolic music dataset\nto date with precise emotion"
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": ""
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": "and genre labels. Each piece of music has an average duration"
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": ""
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": "of\napproximately 176 seconds,\ncontributing to a\ncumulative"
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": ""
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": "dataset\nlength of 5,278 hours."
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": ""
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": "2)\nImage Emotion Recognition: We\ncollected a new im-"
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": ""
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": "age emotion recognition dataset containing 269,793 images,"
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": ""
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": "among which 40,000 images are selected as the test set, and"
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": ""
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": "the remaining images are used for\ntraining. The images were"
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": ""
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": "gathered from various\nsources,\nincluding the WebEMO [57]"
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": "image sentiment dataset,\nthe Places365 [58] scene recognition"
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": ""
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": "dataset, and web searches from Baidu and Google. Given that"
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": "the labels of internet images are inherently noisy, we employed"
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": "human annotators\nto filter out\nsamples\nthat did not\nclearly"
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": "correspond to their assigned labels."
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": "3) Text Emotion Recognition: We\nconstructed a\nstandard"
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": "test\nset\nfor\ntext\nemotion\nclassification,\nconsisting\nof\n1,100"
        },
        {
          "Fig. 4: Data statistics of our XMIDI dataset.": "sentences\ndistributed\nevenly\nacross\n11\nemotion\ncategories."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "10": ""
        },
        {
          "10": "blind evaluation,\nthe music results within each question were"
        },
        {
          "10": ""
        },
        {
          "10": "randomly shuffled."
        },
        {
          "10": "1) Comparison with Symbolic Music Generation Methods:"
        },
        {
          "10": ""
        },
        {
          "10": "Following common practice\n[10],\n[11], we generated MIDI"
        },
        {
          "10": "files\nfor\neach method\n(CP\n[10],\nEMOPIA [11]\nand\nour"
        },
        {
          "10": ""
        },
        {
          "10": "proposed XMusic)\nand rendered these files\nin audio format"
        },
        {
          "10": ""
        },
        {
          "10": "using the same soundfont. In the questionnaire, each question"
        },
        {
          "10": ""
        },
        {
          "10": "contained\nthree\nrandomly\nordered\naudio\nsamples\ngenerated"
        },
        {
          "10": ""
        },
        {
          "10": "via the three aforementioned methods. The participants were"
        },
        {
          "10": "required\nto\ncarefully\nlisten\nto\nand\nrank\nthe\naudio\nsamples"
        },
        {
          "10": ""
        },
        {
          "10": "based\non\nthe\nfollowing metrics:\ni) Richness: The\ndiversity"
        },
        {
          "10": "of musical elements,\nsuch as melody, harmony,\nrhythm, and"
        },
        {
          "10": "timbre.\nii) Correctness: The\nabsence of\nerrors or unnatural"
        },
        {
          "10": ""
        },
        {
          "10": "musical elements, such as odd chords or sudden silences.\niii)"
        },
        {
          "10": ""
        },
        {
          "10": "Structuredness: The presence of\nrepetitive structures, such as"
        },
        {
          "10": ""
        },
        {
          "10": "memorable melodies. The questionnaire took an average of 47"
        },
        {
          "10": ""
        },
        {
          "10": "minutes to complete. We averaged the ranking results from the"
        },
        {
          "10": ""
        },
        {
          "10": "31 participants to obtain the final results, presented in Table V-"
        },
        {
          "10": ""
        },
        {
          "10": "(a). As shown, our method achieved the highest average rank"
        },
        {
          "10": ""
        },
        {
          "10": "across\nall\nthree\nevaluation metrics,\nindicating\nthat XMusic"
        },
        {
          "10": ""
        },
        {
          "10": "surpassed the existing state-of-the-art approaches in generating"
        },
        {
          "10": ""
        },
        {
          "10": "impressive and high-quality music."
        },
        {
          "10": ""
        },
        {
          "10": "2) Comparison with Emotion-conditioned Symbolic Music"
        },
        {
          "10": ""
        },
        {
          "10": "Generation Method: To evaluate the efficacy of our method"
        },
        {
          "10": ""
        },
        {
          "10": "in\nemotion\ncontrol, we\ncompared\nour XMusic with\nthe"
        },
        {
          "10": ""
        },
        {
          "10": "state-of-the-art\nemotion-conditioned method EMOPIA [11]."
        },
        {
          "10": ""
        },
        {
          "10": "To\nour\nbest\nknowledge,\nEMOPIA is\ncurrently\nthe\nonly"
        },
        {
          "10": ""
        },
        {
          "10": "open-source emotion-conditioned symbolic music generation"
        },
        {
          "10": ""
        },
        {
          "10": "method. EMOPIA and XMusic utilize differing levels of emo-"
        },
        {
          "10": ""
        },
        {
          "10": "tion granularity. EMOPIA adopts Russell’s Circumplex model,"
        },
        {
          "10": ""
        },
        {
          "10": "conceptualizing emotions in a two-dimensional space defined"
        },
        {
          "10": ""
        },
        {
          "10": "by valence and arousal,\nresulting in four classes (quadrants):"
        },
        {
          "10": ""
        },
        {
          "10": "PVPA (positive\nvalence\npositive\narousal), NVPA (negative"
        },
        {
          "10": ""
        },
        {
          "10": "valence positive\narousal), NVNA (negative valence negative"
        },
        {
          "10": ""
        },
        {
          "10": "arousal),\nand PVNA (positive valence negative\narousal).\nIn"
        },
        {
          "10": ""
        },
        {
          "10": "contrast, XMusic employs 11 specific emotion classes,\ninclud-"
        },
        {
          "10": ""
        },
        {
          "10": "ing happy,\nfunny,\nsad, exciting, etc. Since there are no clear"
        },
        {
          "10": ""
        },
        {
          "10": "correspondences between the 4 EMOPIA quadrants and the 11"
        },
        {
          "10": ""
        },
        {
          "10": "XMusic classes, we aligned the emotion categories by merg-"
        },
        {
          "10": ""
        },
        {
          "10": "ing adjacent EMOPIA quadrants and mapping corresponding"
        },
        {
          "10": ""
        },
        {
          "10": "XMusic\ncategories\nto these\ncombined classes. For\nexample,"
        },
        {
          "10": ""
        },
        {
          "10": "PVPA and PVNA were merged to form a new Positive Valence"
        },
        {
          "10": ""
        },
        {
          "10": "(PV)\nclass, with\nthe\n“happy”\nand\n“funny”\nclasses mapped"
        },
        {
          "10": ""
        },
        {
          "10": "to this\ncategory. The other new categories were defined as"
        },
        {
          "10": ""
        },
        {
          "10": "follows: NV (combining NVPA & NVNA, mapped to sad),"
        },
        {
          "10": ""
        },
        {
          "10": "PA (PVPA & NVPA, mapped to exciting) and NA (PVNA &"
        },
        {
          "10": ""
        },
        {
          "10": "NVNA, mapped to quiet). We generated and rendered music"
        },
        {
          "10": ""
        },
        {
          "10": "files\nfor\neach method\nusing\nthese\nnew categories\nas\ninput"
        },
        {
          "10": ""
        },
        {
          "10": "prompts. Participants were instructed to count\nthe number of"
        },
        {
          "10": ""
        },
        {
          "10": "music\npieces\nthat\nthey\nperceived\nas fitting\nthe\nnew labels."
        },
        {
          "10": ""
        },
        {
          "10": "This\ntask took an average of 112 minutes\nto complete. We"
        },
        {
          "10": "computed the\naverage number of\ncorrectly classified pieces"
        },
        {
          "10": "per class as determined by the 31 participants. As\nshown in"
        },
        {
          "10": ""
        },
        {
          "10": "Table VI,\nthe music generated by our method better matched"
        },
        {
          "10": "the input emotion prompts, demonstrating that our approach"
        },
        {
          "10": "has superior emotional controllability."
        },
        {
          "10": "3) Comparison\nwith\nVideo-conditioned\nSymbolic Music"
        },
        {
          "10": "Generation Method: Additionally, we compared XMusic with"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "11": ""
        },
        {
          "11": "Overall Rank"
        },
        {
          "11": "2.1452"
        },
        {
          "11": "2.3549"
        },
        {
          "11": "1.5000"
        },
        {
          "11": "1.6923"
        },
        {
          "11": ""
        },
        {
          "11": "1.3077"
        },
        {
          "11": "2.4290"
        },
        {
          "11": "2.0484"
        },
        {
          "11": "1.5226"
        },
        {
          "11": "1.7258"
        },
        {
          "11": ""
        },
        {
          "11": "1.2742"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE\nVI:\nSubjective": "",
          "comparison": "",
          "with\nstate-of-the-art": ""
        },
        {
          "TABLE\nVI:\nSubjective": "emotion-conditioned symbolic music generation method.",
          "comparison": "",
          "with\nstate-of-the-art": ""
        },
        {
          "TABLE\nVI:\nSubjective": "",
          "comparison": "",
          "with\nstate-of-the-art": ""
        },
        {
          "TABLE\nVI:\nSubjective": "",
          "comparison": "",
          "with\nstate-of-the-art": ""
        },
        {
          "TABLE\nVI:\nSubjective": "Classes",
          "comparison": "EMOPIA [11]",
          "with\nstate-of-the-art": "XMusic (Ours)"
        },
        {
          "TABLE\nVI:\nSubjective": "",
          "comparison": "",
          "with\nstate-of-the-art": ""
        },
        {
          "TABLE\nVI:\nSubjective": "Positive Valence (PV) ↑",
          "comparison": "38%",
          "with\nstate-of-the-art": "76%"
        },
        {
          "TABLE\nVI:\nSubjective": "",
          "comparison": "",
          "with\nstate-of-the-art": ""
        },
        {
          "TABLE\nVI:\nSubjective": "Negative Valence (NV) ↑",
          "comparison": "38%",
          "with\nstate-of-the-art": "70%"
        },
        {
          "TABLE\nVI:\nSubjective": "Positive Arousal\n(PA) ↑",
          "comparison": "39%",
          "with\nstate-of-the-art": "66%"
        },
        {
          "TABLE\nVI:\nSubjective": "Negative Arousal\n(NA) ↑",
          "comparison": "51%",
          "with\nstate-of-the-art": "84%"
        },
        {
          "TABLE\nVI:\nSubjective": "",
          "comparison": "",
          "with\nstate-of-the-art": ""
        },
        {
          "TABLE\nVI:\nSubjective": "",
          "comparison": "",
          "with\nstate-of-the-art": ""
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "CMT [19]\nin a video-conditioned evaluation. While the latest": "work, V-MusProd\n[20],\nalso\nfocuses\non\ngenerating music",
          "ages\nis\nrelatively\nunder-explored, with\nonly\na\nfew notable": "methods\n[63],\n[65],\n[66]\naiming\nto\ndiscover\nvisual-musical"
        },
        {
          "CMT [19]\nin a video-conditioned evaluation. While the latest": "for general videos,\nthe source code for V-MusProd was not",
          "ages\nis\nrelatively\nunder-explored, with\nonly\na\nfew notable": "associations. Among these, we could only compare XMusic"
        },
        {
          "CMT [19]\nin a video-conditioned evaluation. While the latest": "publicly available at\nthe time of our evaluation. Therefore, a",
          "ages\nis\nrelatively\nunder-explored, with\nonly\na\nfew notable": "with Synesthesia\n[63],\nas\nthe\nsource\ncodes\nand\ndemos\nfor"
        },
        {
          "CMT [19]\nin a video-conditioned evaluation. While the latest": "direct and fair comparison could only be conducted between",
          "ages\nis\nrelatively\nunder-explored, with\nonly\na\nfew notable": "the other methods were unavailable. Specifically, we used the"
        },
        {
          "CMT [19]\nin a video-conditioned evaluation. While the latest": "XMusic and CMT. In addition to evaluating richness, correct-",
          "ages\nis\nrelatively\nunder-explored, with\nonly\na\nfew notable": "same images provided in the official Synesthesia repository 2"
        },
        {
          "CMT [19]\nin a video-conditioned evaluation. While the latest": "ness,\nand\nstructuredness, we\nassessed\nthe\ndegree\nof\nvideo-",
          "ages\nis\nrelatively\nunder-explored, with\nonly\na\nfew notable": "as\ninputs\nto\nour XMusic\nand\nthen\ncreated\na\nquestionnaire"
        },
        {
          "CMT [19]\nin a video-conditioned evaluation. While the latest": "music\nalignment,\nfocusing on both emotional\nand rhythmic",
          "ages\nis\nrelatively\nunder-explored, with\nonly\na\nfew notable": "to rank the\ntwo methods given the\nsame\ninput\nimage. The"
        },
        {
          "CMT [19]\nin a video-conditioned evaluation. While the latest": "correspondences. Using\nthe\nsame\nvideos\nselected\nfor\nthe",
          "ages\nis\nrelatively\nunder-explored, with\nonly\na\nfew notable": "questionnaire took about 13 minutes to complete. Table V-(d)"
        },
        {
          "CMT [19]\nin a video-conditioned evaluation. While the latest": "objective evaluation, we paired each video with background",
          "ages\nis\nrelatively\nunder-explored, with\nonly\na\nfew notable": "shows\nthe average rankings\nfrom 31 participants.\nIn contrast"
        },
        {
          "CMT [19]\nin a video-conditioned evaluation. While the latest": "music generated by CMT and XMusic. These were presented",
          "ages\nis\nrelatively\nunder-explored, with\nonly\na\nfew notable": "to Synesthesia, which implicitly models emotional information"
        },
        {
          "CMT [19]\nin a video-conditioned evaluation. While the latest": "in a random order for blindness. The questionnaire took about",
          "ages\nis\nrelatively\nunder-explored, with\nonly\na\nfew notable": "using paired image-music data, our XMusic explicitly decou-"
        },
        {
          "CMT [19]\nin a video-conditioned evaluation. While the latest": "25 minutes\nto\ncomplete. The\nparticipants were\nrequired\nto",
          "ages\nis\nrelatively\nunder-explored, with\nonly\na\nfew notable": "ples\nthe\nemotion analysis\nand control processes, offering a"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE VIII: Comparison with state-of-the-art symbolic music": "generation methods on public datasets.",
          "TABLE IX: Objective ablation study on the proposed Selector.": ""
        },
        {
          "TABLE VIII: Comparison with state-of-the-art symbolic music": "",
          "TABLE IX: Objective ablation study on the proposed Selector.": "Setting"
        },
        {
          "TABLE VIII: Comparison with state-of-the-art symbolic music": "Dataset\nMethod",
          "TABLE IX: Objective ablation study on the proposed Selector.": ""
        },
        {
          "TABLE VIII: Comparison with state-of-the-art symbolic music": "CP [10]",
          "TABLE IX: Objective ablation study on the proposed Selector.": "w/o (✗) Selector"
        },
        {
          "TABLE VIII: Comparison with state-of-the-art symbolic music": "AILabs1k7 [10]\nEMOPIA [11]",
          "TABLE IX: Objective ablation study on the proposed Selector.": "with (✓) Selector"
        },
        {
          "TABLE VIII: Comparison with state-of-the-art symbolic music": "XMusic (Ours)",
          "TABLE IX: Objective ablation study on the proposed Selector.": ""
        },
        {
          "TABLE VIII: Comparison with state-of-the-art symbolic music": "CP [10]",
          "TABLE IX: Objective ablation study on the proposed Selector.": ""
        },
        {
          "TABLE VIII: Comparison with state-of-the-art symbolic music": "AILabs1k7 [10]",
          "TABLE IX: Objective ablation study on the proposed Selector.": ""
        },
        {
          "TABLE VIII: Comparison with state-of-the-art symbolic music": "EMOPIA [11]",
          "TABLE IX: Objective ablation study on the proposed Selector.": ""
        },
        {
          "TABLE VIII: Comparison with state-of-the-art symbolic music": "+EMOPIA [11]\nXMusic (Ours)",
          "TABLE IX: Objective ablation study on the proposed Selector.": ""
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "13": ""
        },
        {
          "13": "led to better\n(λ1 < λ2)\nresults. Therefore, we set λ1 = 1 and"
        },
        {
          "13": ""
        },
        {
          "13": "the default\nsettings because they yielded the best\nλ2 = 2 as"
        },
        {
          "13": "overall performance."
        },
        {
          "13": ""
        },
        {
          "13": "5) The Effectiveness of the Proposed Music Representation:"
        },
        {
          "13": ""
        },
        {
          "13": "As described in Sec. III-B, we designed an enhanced symbolic"
        },
        {
          "13": "music\nrepresentation\nbased\non\nthe CP [10]\nrepresentation,"
        },
        {
          "13": ""
        },
        {
          "13": "which\nincludes\nthree\nkey family\ntoken\nimprovements: Tag,"
        },
        {
          "13": ""
        },
        {
          "13": "Instrument, and Rhythm. To evaluate the efficacy of these new"
        },
        {
          "13": "family tokens, we conducted a series of ablation studies on the"
        },
        {
          "13": "XMIDI dataset. Starting from the baseline CP representation,"
        },
        {
          "13": ""
        },
        {
          "13": "we incrementally added Tag,\nInstrument, and Rhythm family"
        },
        {
          "13": ""
        },
        {
          "13": "tokens\nto investigate\ntheir\nimpact on music generation. The"
        },
        {
          "13": "participants in the subjective evaluation ranked the generated"
        },
        {
          "13": "music based on three aspects: richness, correctness, and struc-"
        },
        {
          "13": "turedness. The average time to complete the questionnaire was"
        },
        {
          "13": "83 minutes, and the results are summarized in Table VII-(c)."
        },
        {
          "13": "We can conclude that each added family token significantly"
        },
        {
          "13": "enhanced performance, demonstrating the effectiveness of the"
        },
        {
          "13": "proposed symbolic music representation."
        },
        {
          "13": ""
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "14": "[11] H.-T. Hung,\nJ. Ching,\nS. Doh, N. Kim,\nJ. Nam,\nand Y.-H. Yang,"
        },
        {
          "14": "“EMOPIA: A multi-modal pop piano dataset\nfor emotion recognition"
        },
        {
          "14": ""
        },
        {
          "14": "Int. Society for Music\nand emotion-based music generation,” in Proc."
        },
        {
          "14": ""
        },
        {
          "14": "Information Retrieval Conf., 2021."
        },
        {
          "14": "[12] C. Bao and Q. Sun, “Generating music with emotions,” IEEE Transac-"
        },
        {
          "14": "tions on Multimedia, 2022."
        },
        {
          "14": ""
        },
        {
          "14": "[13] C. Payne, “Musenet,” OpenAI Blog, vol. 3, 2019."
        },
        {
          "14": ""
        },
        {
          "14": "[14]\nP. Sarmento, A. Kumar, Y.-H. Chen, C. Carr, Z. Zukowski, and M. Bar-"
        },
        {
          "14": "thet,\n“Gtr-ctrl:\nInstrument\nand\ngenre\nconditioning\nfor\nguitar-focused"
        },
        {
          "14": "music generation with transformers,” in Artificial Intelligence in Music,"
        },
        {
          "14": "Sound, Art and Design: 12th International Conference, EvoMUSART"
        },
        {
          "14": "2023, Held as Part of EvoStar 2023, Brno, Czech Republic, April 12–"
        },
        {
          "14": ""
        },
        {
          "14": "14, 2023, Proceedings.\nSpringer, 2023, pp. 260–275."
        },
        {
          "14": "[15]\nS. Ji and X. Yang, “Emomusictv: Emotion-conditioned symbolic music"
        },
        {
          "14": ""
        },
        {
          "14": "IEEE Transactions\non\ngeneration with\nhierarchical\ntransformer\nvae,”"
        },
        {
          "14": "Multimedia, pp. 1–13, 2023."
        },
        {
          "14": "[16] Y. Zou, P. Zou, Y. Zhao, K. Zhang, R. Zhang, and X. Wang, “Melons:"
        },
        {
          "14": ""
        },
        {
          "14": "Generating melody with\nlong-term structure\nusing\ntransformers\nand"
        },
        {
          "14": ""
        },
        {
          "14": "structure graph,” ICASSP 2022 - 2022 IEEE International Conference"
        },
        {
          "14": "on Acoustics, Speech and Signal Processing (ICASSP), pp. 191–195,"
        },
        {
          "14": "2021."
        },
        {
          "14": ""
        },
        {
          "14": "[17] C. Gan, D. Huang, P. Chen, J. B. Tenenbaum, and A. Torralba, “Foley"
        },
        {
          "14": ""
        },
        {
          "14": "music: Learning to generate music from videos,” in Computer Vision–"
        },
        {
          "14": "ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28,"
        },
        {
          "14": "2020, Proceedings, Part XI 16.\nSpringer, 2020, pp. 758–775."
        },
        {
          "14": ""
        },
        {
          "14": "[18] K. Su, X. Liu,\nand E. Shlizerman,\n“Audeo: Audio\ngeneration\nfor\na"
        },
        {
          "14": ""
        },
        {
          "14": "Information Processing\nsilent performance video,” Advances in Neural"
        },
        {
          "14": "Systems, vol. 33, pp. 3325–3337, 2020."
        },
        {
          "14": "[19]\nS. Di,\nZ.\nJiang,\nS.\nLiu,\nZ. Wang,\nL.\nZhu,\nZ. He, H.\nLiu,\nand"
        },
        {
          "14": ""
        },
        {
          "14": "S. Yan, “Video background music generation with controllable music"
        },
        {
          "14": ""
        },
        {
          "14": "the 29th ACM International Conference\ntransformer,” in Proceedings of"
        },
        {
          "14": "on Multimedia, 2021, pp. 2037–2045."
        },
        {
          "14": "[20]\nL. Zhuo, Z. Wang, B. Wang, Y. Liao, C. Bao, S. Peng, S. Han, A. Zhang,"
        },
        {
          "14": ""
        },
        {
          "14": "F. Fang,\nand S. Liu,\n“Video\nbackground music\ngeneration: Dataset,"
        },
        {
          "14": ""
        },
        {
          "14": "the IEEE/CVF International\nmethod and evaluation,” in Proceedings of"
        },
        {
          "14": "Conference on Computer Vision, 2023, pp. 15 637–15 647."
        },
        {
          "14": "[21]\nL. Ouyang,\nJ. Wu, X.\nJiang, D. Almeida, C. Wainwright, P. Mishkin,"
        },
        {
          "14": ""
        },
        {
          "14": "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language"
        },
        {
          "14": "models to follow instructions with human feedback,” Advances in Neural"
        },
        {
          "14": "Information Processing Systems, vol. 35, pp. 27 730–27 744, 2022."
        },
        {
          "14": ""
        },
        {
          "14": "[22] OpenAI, “Gpt-4 technical\nreport,” ArXiv, vol. abs/2303.08774, 2023."
        },
        {
          "14": "[23] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T."
        },
        {
          "14": ""
        },
        {
          "14": "Cheng, A. Jin, T. Bos, L. Baker, Y. Du et al., “Lamda: Language models"
        },
        {
          "14": ""
        },
        {
          "14": "for dialog applications,” arXiv preprint arXiv:2201.08239, 2022."
        },
        {
          "14": ""
        },
        {
          "14": "[24]\nT. Karras, S. Laine, and T. Aila, “A style-based generator architecture"
        },
        {
          "14": ""
        },
        {
          "14": "the IEEE/CVF\nfor generative adversarial networks,” in Proceedings of"
        },
        {
          "14": ""
        },
        {
          "14": "conference on computer vision and pattern recognition, 2019, pp. 4401–"
        },
        {
          "14": ""
        },
        {
          "14": "4410."
        },
        {
          "14": ""
        },
        {
          "14": "[25] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, “Hierarchical"
        },
        {
          "14": ""
        },
        {
          "14": "arXiv\npreprint\ntext-conditional\nimage\ngeneration with\nclip\nlatents,”"
        },
        {
          "14": ""
        },
        {
          "14": "arXiv:2204.06125, 2022."
        },
        {
          "14": ""
        },
        {
          "14": "[26] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, “High-"
        },
        {
          "14": ""
        },
        {
          "14": "resolution image synthesis with latent diffusion models,” in Proceedings"
        },
        {
          "14": "of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recogni-"
        },
        {
          "14": "tion, 2022, pp. 10 684–10 695."
        },
        {
          "14": "[27]\nF.\nSchneider,\nZ.\nJin,\nand\nB.\nSch¨olkopf,\n“Moˆusai:\nText-to-music"
        },
        {
          "14": "arXiv\npreprint\ngeneration\nwith\nlong-context\nlatent\ndiffusion,”"
        },
        {
          "14": "arXiv:2301.11757, 2023."
        },
        {
          "14": "[28]\nL.-C. Yang, S.-Y. Chou,\nand Y.-H. Yang,\n“Midinet: A convolutional"
        },
        {
          "14": "generative adversarial network for symbolic-domain music generation,”"
        },
        {
          "14": "for Music\nin International Society\nInformation Retrieval Conference,"
        },
        {
          "14": "2017."
        },
        {
          "14": "[29] H.-W. Dong, W.-Y. Hsiao, L.-C. Yang,\nand Y.-H. Yang,\n“Musegan:"
        },
        {
          "14": "Multi-track\nsequential\ngenerative\nadversarial\nnetworks\nfor\nsymbolic"
        },
        {
          "14": "of\nthe AAAI\nmusic\ngeneration\nand\naccompaniment,”\nin Proceedings"
        },
        {
          "14": "Conference on Artificial\nIntelligence, vol. 32, no. 1, 2018."
        },
        {
          "14": "[30] K. Choi, C. Hawthorne,\nI. Simon, M. Dinculescu, and J. Engel, “En-"
        },
        {
          "14": "coding musical\nstyle with transformer autoencoders,” in International"
        },
        {
          "14": "Conference on Machine Learning.\nPMLR, 2020, pp. 1899–1908."
        },
        {
          "14": "[31]\nJ. Jiang, G. G. Xia, D. B. Carlton, C. N. Anderson, and R. H. Miyakawa,"
        },
        {
          "14": "“Transformer vae: A hierarchical model\nfor\nstructure-aware and inter-"
        },
        {
          "14": "ICASSP 2020-2020\nIEEE\npretable music\nrepresentation\nlearning,”\nin"
        },
        {
          "14": "International Conference on Acoustics, Speech and Signal Processing"
        },
        {
          "14": "(ICASSP).\nIEEE, 2020, pp. 516–520."
        },
        {
          "14": "[32] G. Brunner, Y. Wang, R. Wattenhofer, and S. Zhao, “Symbolic music"
        },
        {
          "14": "genre transfer with cyclegan,” in 2018 ieee 30th international conference"
        },
        {
          "14": "on tools with artificial\nintelligence (ictai).\nIEEE, 2018, pp. 786–793."
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": "models to follow instructions with human feedback,” Advances in Neural"
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": "Information Processing Systems, vol. 35, pp. 27 730–27 744, 2022."
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": ""
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": "[22] OpenAI, “Gpt-4 technical\nreport,” ArXiv, vol. abs/2303.08774, 2023."
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": "[23] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T."
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": ""
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": "Cheng, A. Jin, T. Bos, L. Baker, Y. Du et al., “Lamda: Language models"
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": ""
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": "for dialog applications,” arXiv preprint arXiv:2201.08239, 2022."
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": ""
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": "[24]\nT. Karras, S. Laine, and T. Aila, “A style-based generator architecture"
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": ""
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": "the IEEE/CVF\nfor generative adversarial networks,” in Proceedings of"
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": ""
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": "conference on computer vision and pattern recognition, 2019, pp. 4401–"
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": ""
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": "4410."
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": ""
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": "[25] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, “Hierarchical"
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": ""
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": "arXiv\npreprint\ntext-conditional\nimage\ngeneration with\nclip\nlatents,”"
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": ""
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": "arXiv:2204.06125, 2022."
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": ""
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": "[26] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, “High-"
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": ""
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": "resolution image synthesis with latent diffusion models,” in Proceedings"
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": "of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recogni-"
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": "tion, 2022, pp. 10 684–10 695."
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": "[27]\nF.\nSchneider,\nZ.\nJin,\nand\nB.\nSch¨olkopf,\n“Moˆusai:\nText-to-music"
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": "arXiv\npreprint\ngeneration\nwith\nlong-context\nlatent\ndiffusion,”"
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": "arXiv:2301.11757, 2023."
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": "[28]\nL.-C. Yang, S.-Y. Chou,\nand Y.-H. Yang,\n“Midinet: A convolutional"
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": "generative adversarial network for symbolic-domain music generation,”"
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": "for Music\nin International Society\nInformation Retrieval Conference,"
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": "2017."
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": "[29] H.-W. Dong, W.-Y. Hsiao, L.-C. Yang,\nand Y.-H. Yang,\n“Musegan:"
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": "Multi-track\nsequential\ngenerative\nadversarial\nnetworks\nfor\nsymbolic"
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": "of\nthe AAAI\nmusic\ngeneration\nand\naccompaniment,”\nin Proceedings"
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": "Conference on Artificial\nIntelligence, vol. 32, no. 1, 2018."
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": "[30] K. Choi, C. Hawthorne,\nI. Simon, M. Dinculescu, and J. Engel, “En-"
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": "coding musical\nstyle with transformer autoencoders,” in International"
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": "Conference on Machine Learning.\nPMLR, 2020, pp. 1899–1908."
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": "[31]\nJ. Jiang, G. G. Xia, D. B. Carlton, C. N. Anderson, and R. H. Miyakawa,"
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": "“Transformer vae: A hierarchical model\nfor\nstructure-aware and inter-"
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": "ICASSP 2020-2020\nIEEE\npretable music\nrepresentation\nlearning,”\nin"
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": "International Conference on Acoustics, Speech and Signal Processing"
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": "(ICASSP).\nIEEE, 2020, pp. 516–520."
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": "[32] G. Brunner, Y. Wang, R. Wattenhofer, and S. Zhao, “Symbolic music"
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": "genre transfer with cyclegan,” in 2018 ieee 30th international conference"
        },
        {
          "C. Zhang, S. Agarwal, K. Slama, A. Ray et al.,\n“Training language": "on tools with artificial\nintelligence (ictai).\nIEEE, 2018, pp. 786–793."
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "15": "[56]\nL. Ferreira\nand J. Whitehead,\n“Learning to generate music with sen-"
        },
        {
          "15": "of\nthe\n20th\nInternational\nSociety\nfor Music\ntiment,”\nin Proceedings"
        },
        {
          "15": "Information Retrieval Conference, ISMIR 2019, Delft, The Netherlands,"
        },
        {
          "15": "November 4-8, 2019, 2019, pp. 384–390."
        },
        {
          "15": "[57] R. Panda, J. Zhang, H. Li, J.-Y. Lee, X. Lu, and A. K. Roy-Chowdhury,"
        },
        {
          "15": "“Contemplating visual emotions: Understanding and overcoming dataset"
        },
        {
          "15": "bias,” in European Conference on Computer Vision, 2018."
        },
        {
          "15": "[58] B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, and A. Torralba, “Places:"
        },
        {
          "15": "A 10 million image database for scene recognition,” IEEE Transactions"
        },
        {
          "15": "on Pattern Analysis and Machine Intelligence, 2017."
        },
        {
          "15": "[59]\nS.-L. Wu\nand Y.-H. Yang,\n“The\njazz\ntransformer\non\nthe\nfront\nline:"
        },
        {
          "15": "Exploring the shortcomings of ai-composed music through quantitative"
        },
        {
          "15": "International\nSociety\nfor Music\nInformation Retrieval\nmeasures,”\nin"
        },
        {
          "15": "Conference, 2020."
        },
        {
          "15": "[60] H.-W. Dong, W.-Y. Hsiao, and Y.-H. Yang, “Pypianoroll: Open source"
        },
        {
          "15": "ISMIR. Late-\npython package for handling multitrack pianoroll,” Proc."
        },
        {
          "15": "breaking paper, 2018."
        },
        {
          "15": ""
        },
        {
          "15": "[61] H.-W.\nDong,\nK.\nChen,\nJ. McAuley,\nand\nT.\nBerg-Kirkpatrick,"
        },
        {
          "15": ""
        },
        {
          "15": "arXiv\npreprint\n“Muspy: A toolkit\nfor\nsymbolic music\ngeneration,”"
        },
        {
          "15": ""
        },
        {
          "15": "arXiv:2008.01951, 2020."
        },
        {
          "15": ""
        },
        {
          "15": "[62]\nS. Wu and M. Sun, “Exploring the efficacy of pre-trained checkpoints in"
        },
        {
          "15": ""
        },
        {
          "15": "text-to-music generation task,” arXiv preprint arXiv:2211.11216, 2022."
        },
        {
          "15": ""
        },
        {
          "15": "[63] X. Tan, M. Antony,\nand H. Kong,\n“Automated music generation for"
        },
        {
          "15": ""
        },
        {
          "15": "visual art\nthrough emotion.” in ICCC, 2020, pp. 247–250."
        },
        {
          "15": ""
        },
        {
          "15": "[64]\nS. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Ka-"
        },
        {
          "15": ""
        },
        {
          "15": "mar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg et al., “Sparks of artificial"
        },
        {
          "15": ""
        },
        {
          "15": "arXiv\npreprint\ngeneral\nintelligence:\nEarly\nexperiments with\ngpt-4,”"
        },
        {
          "15": ""
        },
        {
          "15": "arXiv:2303.12712, 2023."
        },
        {
          "15": ""
        },
        {
          "15": "[65] X. Wu, “A study on image-based music generation,” 2008."
        },
        {
          "15": ""
        },
        {
          "15": "[66] R. Madhok, S. Goel, and S. Garg, “Sentimozart: Music generation based"
        },
        {
          "15": ""
        },
        {
          "15": "on emotions.” in ICAART (2), 2018, pp. 501–506."
        },
        {
          "15": ""
        },
        {
          "15": "[67] Y. Qiu,\nJ. Zhang, H. Ren, Y. Shan,\nand J. Zhou,\n“Humming2music:"
        },
        {
          "15": ""
        },
        {
          "15": "the\nbeing a composer as long as you can humming,” in Proceedings of"
        },
        {
          "15": ""
        },
        {
          "15": "Thirty-Second International Joint Conference on Artificial\nIntelligence,"
        },
        {
          "15": ""
        },
        {
          "15": "2023, pp. 7163–7166."
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        }
      ],
      "page": 15
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Audiolm: a language modeling approach to audio generation",
      "authors": [
        "Z Borsos",
        "R Marinier",
        "D Vincent",
        "E Kharitonov",
        "O Pietquin",
        "M Sharifi",
        "D Roblek",
        "O Teboul",
        "D Grangier",
        "M Tagliasacchi"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "2",
      "title": "Musiclm: Generating music from text",
      "authors": [
        "A Agostinelli",
        "T Denk",
        "Z Borsos",
        "J Engel",
        "M Verzetti",
        "A Caillon",
        "Q Huang",
        "A Jansen",
        "A Roberts",
        "M Tagliasacchi"
      ],
      "year": "2023",
      "venue": "Musiclm: Generating music from text",
      "arxiv": "arXiv:2301.11325"
    },
    {
      "citation_id": "3",
      "title": "Riffusion -Stable diffusion for real-time music generation",
      "authors": [
        "S Forsgren",
        "H Martiros"
      ],
      "year": "2022",
      "venue": "Riffusion -Stable diffusion for real-time music generation"
    },
    {
      "citation_id": "4",
      "title": "Simple and controllable music generation",
      "authors": [
        "J Copet",
        "F Kreuk",
        "I Gat",
        "T Remez",
        "D Kant",
        "G Synnaeve",
        "Y Adi",
        "A Défossez"
      ],
      "year": "2023",
      "venue": "Simple and controllable music generation",
      "arxiv": "arXiv:2306.05284"
    },
    {
      "citation_id": "5",
      "title": "Noise2music: Text-conditioned music generation with diffusion models",
      "authors": [
        "Q Huang",
        "D Park",
        "T Wang",
        "T Denk",
        "A Ly",
        "N Chen",
        "Z Zhang",
        "Z Zhang",
        "J Yu",
        "C Frank"
      ],
      "year": "2023",
      "venue": "Noise2music: Text-conditioned music generation with diffusion models",
      "arxiv": "arXiv:2302.03917"
    },
    {
      "citation_id": "6",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "7",
      "title": "Transformer-xl: Attentive language models beyond a fixed-length context",
      "authors": [
        "Z Dai",
        "Z Yang",
        "Y Yang",
        "J Carbonell",
        "Q Le",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "8",
      "title": "Music transformer: Generating music with long-term structure",
      "authors": [
        "C.-Z Huang",
        "A Vaswani",
        "J Uszkoreit",
        "I Simon",
        "C Hawthorne",
        "N Shazeer",
        "A Dai",
        "M Hoffman",
        "M Dinculescu",
        "D Eck"
      ],
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "9",
      "title": "Pop music transformer: Beat-based modeling and generation of expressive pop piano compositions",
      "authors": [
        "Y.-S Huang",
        "Y.-H Yang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "10",
      "title": "Compound word transformer: Learning to compose full-song music over dynamic directed hypergraphs",
      "authors": [
        "W.-Y Hsiao",
        "J.-Y Liu",
        "Y.-C Yeh",
        "Y.-H Yang"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "11",
      "title": "EMOPIA: A multi-modal pop piano dataset for emotion recognition and emotion-based music generation",
      "authors": [
        "H.-T Hung",
        "J Ching",
        "S Doh",
        "N Kim",
        "J Nam",
        "Y.-H Yang"
      ],
      "year": "2021",
      "venue": "Proc. Int. Society for Music Information Retrieval Conf"
    },
    {
      "citation_id": "12",
      "title": "Generating music with emotions",
      "authors": [
        "C Bao",
        "Q Sun"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "13",
      "title": "Musenet",
      "authors": [
        "C Payne"
      ],
      "year": "2019",
      "venue": "OpenAI Blog"
    },
    {
      "citation_id": "14",
      "title": "Gtr-ctrl: Instrument and genre conditioning for guitar-focused music generation with transformers",
      "authors": [
        "P Sarmento",
        "A Kumar",
        "Y.-H Chen",
        "C Carr",
        "Z Zukowski",
        "M Barthet"
      ],
      "year": "2023",
      "venue": "Artificial Intelligence in Music, Sound, Art and Design: 12th International Conference"
    },
    {
      "citation_id": "15",
      "title": "Emomusictv: Emotion-conditioned symbolic music generation with hierarchical transformer vae",
      "authors": [
        "S Ji",
        "X Yang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "16",
      "title": "Melons: Generating melody with long-term structure using transformers and structure graph",
      "authors": [
        "Y Zou",
        "P Zou",
        "Y Zhao",
        "K Zhang",
        "R Zhang",
        "X Wang"
      ],
      "year": "2021",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "Foley music: Learning to generate music from videos",
      "authors": [
        "C Gan",
        "D Huang",
        "P Chen",
        "J Tenenbaum",
        "A Torralba"
      ],
      "year": "2020",
      "venue": "Computer Vision-ECCV 2020: 16th European Conference"
    },
    {
      "citation_id": "18",
      "title": "Audeo: Audio generation for a silent performance video",
      "authors": [
        "K Su",
        "X Liu",
        "E Shlizerman"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "19",
      "title": "Video background music generation with controllable music transformer",
      "authors": [
        "S Di",
        "Z Jiang",
        "S Liu",
        "Z Wang",
        "L Zhu",
        "Z He",
        "H Liu",
        "S Yan"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "20",
      "title": "Video background music generation: Dataset, method and evaluation",
      "authors": [
        "L Zhuo",
        "Z Wang",
        "B Wang",
        "Y Liao",
        "C Bao",
        "S Peng",
        "S Han",
        "A Zhang",
        "F Fang",
        "S Liu"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "21",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "L Ouyang",
        "J Wu",
        "X Jiang",
        "D Almeida",
        "C Wainwright",
        "P Mishkin",
        "C Zhang",
        "S Agarwal",
        "K Slama",
        "A Ray"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "22",
      "title": "Gpt-4 technical report",
      "authors": [
        "Openai"
      ],
      "year": "2023",
      "venue": "ArXiv"
    },
    {
      "citation_id": "23",
      "title": "Lamda: Language models for dialog applications",
      "authors": [
        "R Thoppilan",
        "D Freitas",
        "J Hall",
        "N Shazeer",
        "A Kulshreshtha",
        "H.-T Cheng",
        "A Jin",
        "T Bos",
        "L Baker",
        "Y Du"
      ],
      "year": "2022",
      "venue": "Lamda: Language models for dialog applications",
      "arxiv": "arXiv:2201.08239"
    },
    {
      "citation_id": "24",
      "title": "A style-based generator architecture for generative adversarial networks",
      "authors": [
        "T Karras",
        "S Laine",
        "T Aila"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "25",
      "title": "Hierarchical text-conditional image generation with clip latents",
      "authors": [
        "A Ramesh",
        "P Dhariwal",
        "A Nichol",
        "C Chu",
        "M Chen"
      ],
      "year": "2022",
      "venue": "Hierarchical text-conditional image generation with clip latents",
      "arxiv": "arXiv:2204.06125"
    },
    {
      "citation_id": "26",
      "title": "Highresolution image synthesis with latent diffusion models",
      "authors": [
        "R Rombach",
        "A Blattmann",
        "D Lorenz",
        "P Esser",
        "B Ommer"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "27",
      "title": "Moûsai: Text-to-music generation with long-context latent diffusion",
      "authors": [
        "F Schneider",
        "Z Jin",
        "B Schölkopf"
      ],
      "year": "2023",
      "venue": "Moûsai: Text-to-music generation with long-context latent diffusion",
      "arxiv": "arXiv:2301.11757"
    },
    {
      "citation_id": "28",
      "title": "Midinet: A convolutional generative adversarial network for symbolic-domain music generation",
      "authors": [
        "L.-C Yang",
        "S.-Y Chou",
        "Y.-H Yang"
      ],
      "year": "2017",
      "venue": "International Society for Music Information Retrieval Conference"
    },
    {
      "citation_id": "29",
      "title": "Musegan: Multi-track sequential generative adversarial networks for symbolic music generation and accompaniment",
      "authors": [
        "H.-W Dong",
        "W.-Y Hsiao",
        "L.-C Yang",
        "Y.-H Yang"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "30",
      "title": "Encoding musical style with transformer autoencoders",
      "authors": [
        "K Choi",
        "C Hawthorne",
        "I Simon",
        "M Dinculescu",
        "J Engel"
      ],
      "year": "2020",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "31",
      "title": "Transformer vae: A hierarchical model for structure-aware and interpretable music representation learning",
      "authors": [
        "J Jiang",
        "G Xia",
        "D Carlton",
        "C Anderson",
        "R Miyakawa"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "32",
      "title": "Symbolic music genre transfer with cyclegan",
      "authors": [
        "G Brunner",
        "Y Wang",
        "R Wattenhofer",
        "S Zhao"
      ],
      "year": "2018",
      "venue": "2018 ieee 30th international conference on tools with artificial intelligence (ictai)"
    },
    {
      "citation_id": "33",
      "title": "Convolutional generative adversarial networks with binary neurons for polyphonic music generation",
      "authors": [
        "H.-W Dong",
        "Y.-H Yang"
      ],
      "year": "2018",
      "venue": "International Society for Music Information Retrieval Conference"
    },
    {
      "citation_id": "34",
      "title": "Sdmuse: Stochastic differential music editing and generation via hybrid representation",
      "authors": [
        "C Zhang",
        "Y Ren",
        "K Zhang",
        "S Yan"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "35",
      "title": "Controllable deep melody generation via hierarchical music structure representation",
      "authors": [
        "S Dai",
        "Z Jin",
        "C Gomes",
        "R Dannenberg"
      ],
      "year": "2021",
      "venue": "International Society for Music Information Retrieval Conference"
    },
    {
      "citation_id": "36",
      "title": "Structurenet: Inducing structure in generated melodies",
      "authors": [
        "G Medeot",
        "S Cherla",
        "K Kosta",
        "M Mcvicar",
        "S Abdallah",
        "M Selvi",
        "E Newton-Rex",
        "K Webster"
      ],
      "year": "2018",
      "venue": "International Society for Music Information Retrieval Conference"
    },
    {
      "citation_id": "37",
      "title": "Modeling self-repetition in music generation using generative adversarial networks",
      "authors": [
        "H Jhamtani",
        "T Berg-Kirkpatrick"
      ],
      "year": "2019",
      "venue": "Machine Learning for Music Discovery Workshop, ICML"
    },
    {
      "citation_id": "38",
      "title": "Morpheus: generating structured music with constrained patterns and tension",
      "authors": [
        "D Herremans",
        "E Chew"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "39",
      "title": "Generating music with sentiment using transformer-gans",
      "authors": [
        "P Neves",
        "J Fornari",
        "J Florindo"
      ],
      "year": "2022",
      "venue": "Generating music with sentiment using transformer-gans",
      "arxiv": "arXiv:2212.11134"
    },
    {
      "citation_id": "40",
      "title": "The beauty of repetition: an algorithmic composition model with motif-level repetition generator and outline-to-music generator in symbolic music generation",
      "authors": [
        "Z Hu",
        "X Ma",
        "Y Liu",
        "G Chen",
        "Y Liu",
        "R Dannenberg"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "41",
      "title": "Theme transformer: Symbolic music generation with theme-conditioned transformer",
      "authors": [
        "Y.-J Shih",
        "S.-L Wu",
        "F Zalkow",
        "M Muller",
        "Y.-H Yang"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "42",
      "title": "Can machines generate personalized music? a hybrid favorite-aware method for user preference music transfer",
      "authors": [
        "Z Hu",
        "Y Liu",
        "G Chen",
        "Y Liu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "43",
      "title": "Conditional lstm-gan for melody generation from lyrics",
      "authors": [
        "Y Yu",
        "A Srivastava",
        "S Canales"
      ],
      "year": "2021",
      "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)"
    },
    {
      "citation_id": "44",
      "title": "Controllable syllable-level lyrics generation from melody with prior attention",
      "authors": [
        "Z Zhang",
        "Y Yu",
        "A Takasu"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "45",
      "title": "Melody generation from lyrics with local interpretability",
      "authors": [
        "W Duan",
        "Y Yu",
        "X Zhang",
        "S Tang",
        "W Li",
        "K Oyama"
      ],
      "year": "2023",
      "venue": "ACM Transactions on Multimedia Computing, Communications and Applications"
    },
    {
      "citation_id": "46",
      "title": "How does it sound?",
      "authors": [
        "K Su",
        "X Liu",
        "E Shlizerman"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "47",
      "title": "Quantized gan for complex music generation from dance videos",
      "authors": [
        "Y Zhu",
        "K Olszewski",
        "Y Wu",
        "P Achlioptas",
        "M Chai",
        "Y Yan",
        "S Tulyakov"
      ],
      "year": "2022",
      "venue": "Computer Vision-ECCV 2022: 17th European Conference"
    },
    {
      "citation_id": "48",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "49",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "C Hallacy",
        "A Ramesh",
        "G Goh",
        "S Agarwal",
        "G Sastry",
        "A Askell",
        "P Mishkin",
        "J Clark"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "50",
      "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
      "authors": [
        "N Reimers",
        "I Gurevych"
      ],
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "51",
      "title": "Pyscenedetect documentation",
      "venue": "Pyscenedetect documentation"
    },
    {
      "citation_id": "52",
      "title": "Pan: Persistent appearance network with an efficient motion cue for fast action recognition",
      "authors": [
        "C Zhang",
        "Y Zou",
        "G Chen",
        "L Gan"
      ],
      "year": "2019",
      "venue": "Proceedings of the 27th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "53",
      "title": "Visual rhythm and beat",
      "authors": [
        "A Davis",
        "M Agrawala"
      ],
      "year": "2018",
      "venue": "ACM Transactions on Graphics (TOG)"
    },
    {
      "citation_id": "54",
      "title": "Vocano: A note transcription framework for singing voice in polyphonic music",
      "authors": [
        "J.-Y Hsu",
        "L Su"
      ],
      "year": "2021",
      "venue": "ISMIR"
    },
    {
      "citation_id": "55",
      "title": "Multimodal music emotion recognition: A new dataset, methodology and comparative analysis",
      "authors": [
        "R Panda",
        "R Malheiro",
        "B Rocha",
        "A Oliveira",
        "R Paiva"
      ],
      "year": "2013",
      "venue": "International symposium on computer music multidisciplinary research"
    },
    {
      "citation_id": "56",
      "title": "Learning to generate music with sentiment",
      "authors": [
        "L Ferreira",
        "J Whitehead"
      ],
      "year": "2019",
      "venue": "Proceedings of the 20th International Society for Music Information Retrieval Conference"
    },
    {
      "citation_id": "57",
      "title": "Contemplating visual emotions: Understanding and overcoming dataset bias",
      "authors": [
        "R Panda",
        "J Zhang",
        "H Li",
        "J.-Y Lee",
        "X Lu",
        "A Roy-Chowdhury"
      ],
      "year": "2018",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "58",
      "title": "Places: A 10 million image database for scene recognition",
      "authors": [
        "B Zhou",
        "A Lapedriza",
        "A Khosla",
        "A Oliva",
        "A Torralba"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "59",
      "title": "The jazz transformer on the front line: Exploring the shortcomings of ai-composed music through quantitative measures",
      "authors": [
        "S.-L Wu",
        "Y.-H Yang"
      ],
      "year": "2020",
      "venue": "International Society for Music Information Retrieval Conference"
    },
    {
      "citation_id": "60",
      "title": "Pypianoroll: Open source python package for handling multitrack pianoroll",
      "authors": [
        "H.-W Dong",
        "W.-Y Hsiao",
        "Y.-H Yang"
      ],
      "year": "2018",
      "venue": "Proc. ISMIR. Latebreaking paper"
    },
    {
      "citation_id": "61",
      "title": "Muspy: A toolkit for symbolic music generation",
      "authors": [
        "H.-W Dong",
        "K Chen",
        "J Mcauley",
        "T Berg-Kirkpatrick"
      ],
      "year": "2020",
      "venue": "Muspy: A toolkit for symbolic music generation",
      "arxiv": "arXiv:2008.01951"
    },
    {
      "citation_id": "62",
      "title": "Exploring the efficacy of pre-trained checkpoints in text-to-music generation task",
      "authors": [
        "S Wu",
        "M Sun"
      ],
      "year": "2022",
      "venue": "Exploring the efficacy of pre-trained checkpoints in text-to-music generation task",
      "arxiv": "arXiv:2211.11216"
    },
    {
      "citation_id": "63",
      "title": "Automated music generation for visual art through emotion",
      "authors": [
        "X Tan",
        "M Antony",
        "H Kong"
      ],
      "year": "2020",
      "venue": "ICCC"
    },
    {
      "citation_id": "64",
      "title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
      "authors": [
        "S Bubeck",
        "V Chandrasekaran",
        "R Eldan",
        "J Gehrke",
        "E Horvitz",
        "E Kamar",
        "P Lee",
        "Y Lee",
        "Y Li",
        "S Lundberg"
      ],
      "year": "2023",
      "venue": "Sparks of artificial general intelligence: Early experiments with gpt-4",
      "arxiv": "arXiv:2303.12712"
    },
    {
      "citation_id": "65",
      "title": "A study on image-based music generation",
      "authors": [
        "X Wu"
      ],
      "year": "2008",
      "venue": "A study on image-based music generation"
    },
    {
      "citation_id": "66",
      "title": "Sentimozart: Music generation based on emotions",
      "authors": [
        "R Madhok",
        "S Goel",
        "S Garg"
      ],
      "year": "2018",
      "venue": "ICAART"
    },
    {
      "citation_id": "67",
      "title": "Humming2music: being a composer as long as you can humming",
      "authors": [
        "Y Qiu",
        "J Zhang",
        "H Ren",
        "Y Shan",
        "J Zhou"
      ],
      "year": "2023",
      "venue": "Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence"
    }
  ]
}