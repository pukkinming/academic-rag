{
  "paper_id": "2109.08043v1",
  "title": "Generating Dataset For Large-Scale 3D Facial Emotion Recognition",
  "published": "2021-09-16T15:12:41Z",
  "authors": [
    "Faizan Farooq Khan",
    "Syed Zulqarnain Gilani"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The tremendous development in deep learning has led facial expression recognition (FER) to receive much attention in the past few years. Although 3D FER has an inherent edge over its 2D counterpart, work on 2D images has dominated the field. The main reason for the slow development of 3D FER is the unavailability of large training and large test datasets. Recognition accuracies have already saturated on existing 3D emotion recognition datasets due to their small gallery sizes. Unlike 2D photographs, 3D facial scans are not easy to collect, causing a bottleneck in the development of deep 3D FER networks and datasets. In this work, we propose a method for generating a large dataset of 3D faces with labeled emotions. We also develop a deep convolutional neural network(CNN) for 3D FER trained on 624,000 3D facial scans. The test data comprises 208,000 3D facial scans. * Use footnote for providing further information about author (webpage, alternative address)-not for acknowledging funding agencies. Preprint. Under review.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotional expressions are the behaviors that communicate our emotional state or attitude to others. They have always been used and form an essential part of interpersonal communication. Emotions are expressed in different forms, often missed by the naked eye. Recently deep learning has become the mainstream method to promote FER. FER has gained traction in the past few years, and reliable FER systems are required in affect-aware machines and devices. Such systems can understand human emotion and interact with users more naturally. Most of the work has been done in the 2D modality, resulting in a significant gap between 2D and 3D FER. The lack of availability of a large 3D data set is responsible for the slow development of the FER in 3D modality.\n\nThe 3D facial scans have inherent advantages over 2D images. The 2D images suffer from variations of head-pose, illumination, occlusion, and identity, whereas 3D facial scans are uninfluenced by these factors  (1) . The ability to capture all the muscle movement accurately, regardless of the lighting and pose variations, by using 3D instruments provides high resolution 3D facial scans where the muscle activities that change with expressions are visually evident, which is beneficial for FER.\n\nIn this paper, we present a technique for data augmentation that introduces non-linear heterogeneous variations in 3D shape and facial expressions to generate a dataset of 832K 3D scans of 100 unique identities. Existing 3D datasets for FER are several orders of magnitude lower than ours, as shown in Table  1 .\n\nApart from data, the expression recognition algorithm itself is a critical component. Using networks trained on 2D images to perform 3D face recognition is simplistic and sub-optimal as 3D data has its peculiarities defined by the underlying shape and geometry. To the best of our knowledge, there is no deep network trained on a dataset comparable to ours in size, which is designed specifically for 3D",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Works",
      "text": "Facial Expression recognition has been vastly studied, but the studies have focused majorly on 2D data for the most part. 3D FER has recently started to become an extensive research field, and some early attempts include (2; 3; 4; 5; 6), and the most recent (7; 8; 9). There have been efforts made  (8; 11)  to try methods that tend to both 2D and 3D modalities.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Manually-Crafted Feature-Based Method",
      "text": "The use of curvature-based features from 3D facial models to detect areas with high curvatures, such as the nose tip and eye cavities is followed in  (12; 13) . A segmentation-based approach using K-means to discard the background and locate the candidate faces using edge and ellipsoid detection is used in  (14) . Random Forests are used by  (15) , each pixel is assigned a body part label, including the face. The approach used by (  15 ) is extended in (16) which uses Graph Cuts to optimize the Random Forest probabilities.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Deep Learning Based 3D Facial Expression Recognition",
      "text": "Deep CNN's for FER of six basic expressions is proposed in  (17) . The CNN's are trained on the 2D facial appearance and the 3D face shape, respectively, obtained from the BU-3DFE database. The use of hand-crafted descriptors in capturing the local shape and texture information is introduced in (18); this work is extended by  (8) , where deep learning techniques are introduced in which they achieve state-of-the-art performances on the BU-3DFE database at that time.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multi Modal Techniques",
      "text": "The use of 2D texture information incorporated along with 3D features is done in  (23) . After learning the SVM models from the 2D and 3D data separately, classification is performed. In 2015, (9), proposed to fuse the key features obtained from the geometric and textured domains, to investigate how the overall performance is affected. They conducted experiments on the BU-3DFE database, demonstrating the effectiveness of combing texture and depth cues.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Data Generation Process",
      "text": "We use 3D facial scans of 100 individuals from the BU3DFE dataset to train and test our deep network. For an individual, we have 6 basic expressions of 4 levels and a neutral face, which gives us 25 facial scans for each identity. Out of all 25 facial scans, we make use of level 2, level 3, and the   29 ), we establish dense correspondence over 15K 3D vertices on the faces from this dataset, using the keypoints-based algorithm. The goal now is to grow the dataset by generating faces from the space spanned by the trio of densely corresponding real 3D faces of distinct identities. We make use of the idea put forward by  (34)  and expand on it for facial expressions. To ensure that the identities in the trio are as \"distinct\" as possible, we select the face trio with the maximum non-rigid shape difference and the trio having the same expression to ensure the generated face has the same expression as that of original faces.\n\nLet the faces be represented by F i = [x p , y p , x p ] T , where i = 1, ..., N, p = 1, ..., P ; N = 100 and P = 15, 000. The shape difference between faces F i , F j and F k is defined as\n\nwhere, γ ij is the amount of bending energy required to deform 3D face F i to face F j . Extending the 2D thin-plate spline model  (10)  to our case, we calculate the bending energy as, γ ij = x T Bx + y T By + z T Bz where x, y and z are the vectors containing the x, y and z coordinates of P points in face F j and B is the bending matrix, which is defined as the P × P upper left matrix of\n\nand 0 is a P × 4 matrix of zeros.We select 832,000 trios of 3D faces with maximum shape difference D(i, j) from the possible 2,102,100 trios. The possible trios are calculated as 100 3 for each expression, times the number of expression, i.e, 13 . Since the 3D faces in each trio are in dense correspondence to each other, a new face F is generated from the linear space of each trio (i, j, k) as\n\n. The 3D pointcloud generated from each trio is then used to generate a three channel image following the steps by  (34) . The three channels consist of depth, azimuth and elevation. The depth channel is generated by fitting a surface of the form z(x, y) to the 3D pointcloud using the gridfit algorithm  (13) . The surface normals of the original point-cloud are calculated in spherical coordinates (θ, ϕ) where θ, ϕ are the azimuth and elevation angles of the normal vector. The surfaces of the form θ(x, y) and ϕ(x, y) are fitted to the azimuth and elevation angles using a similar x, y grid to the depth channel to make the second and third channels. The three channels are normalized on the 0-255 range and can be rendered as an RGB image. This image is passed through a landmark identification network  (18)  to detect the nosetip. With the face centered at the nosetip, we crop a square of 224 × 224 pixels.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Network Architecture",
      "text": "The success of the recent deep network in 2D FER inspired us to propose a deep convolutional neural network that is suited to 3D data. 2D images exhibit significant texture variations over small regions and (  35 ) network was designed for such case. In contrast, 3D facial surfaces are generally smooth, and hence filters with larger kernel sizes would better suit this type of data. This claim is strengthened by the results shown in Table  2 .\n\nThe skeleton architecture of our network follows  (35)  but with a change in the convolution layers (see Figure  1  for details). We aim to minimize the average prediction binary cross-entropy loss after the softmax layer by learning the parameters of a network designed to classify the six basic emotions and the neutral face. After the network is trained, dropout layers are removed before testing.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Limitations",
      "text": "The limitations of our work can be divided into two parts. First, the synthetic dataset is generated using only 100 unique identities. Although the synthetic faces were generated from the space spanned by a trio of densely corresponding real 3D faces of distinct identities, the low number of identities lead to the generation of faces that are similar to other faces. This claim is further strengthened when the network is tested on the unseen test set. The set consists of identities not used during the training process. Training on similar faces prevents the generalization of the network leading to poor results. Second, the different available 3D datasets have been collected in controlled environments with every dataset captured by different instruments. The scans captured from different types of equipment result in different properties between two datasets, making it harder for a network trained on one dataset to perform on the other dataset.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "Our work aims to bridge the vast gap between research advancements in 2D and 3D FER algorithms especially in the context of deep learning. The results are not able to beat the state-of-the-art algorithms but show promise by achieving 71.4% accuracy on the complete Bosphporus dataset which is not used for training. A combination of 3D datasets can provide more variation with a greater number of identities, the dataset generation technique described in the paper can yield better results in this case. In this work, we also show the difference between 2D and 3D facial data and how larger kernels perform better on 3D data while the case is the opposite for 2D datasets.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Architecture of our proposed network.",
      "page": 4
    },
    {
      "caption": "Figure 1: for details). We aim to minimize the average prediction binary cross-entropy loss after",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract": "The tremendous development in deep learning has led facial expression recognition"
        },
        {
          "Abstract": "(FER) to receive much attention in the past few years. Although 3D FER has an"
        },
        {
          "Abstract": "inherent edge over its 2D counterpart, work on 2D images has dominated the ﬁeld."
        },
        {
          "Abstract": "The main reason for the slow development of 3D FER is the unavailability of large"
        },
        {
          "Abstract": "training and large test datasets. Recognition accuracies have already saturated on"
        },
        {
          "Abstract": "existing 3D emotion recognition datasets due to their small gallery sizes. Unlike"
        },
        {
          "Abstract": "2D photographs, 3D facial scans are not easy to collect, causing a bottleneck in"
        },
        {
          "Abstract": "the development of deep 3D FER networks and datasets. In this work, we propose"
        },
        {
          "Abstract": "a method for generating a large dataset of 3D faces with labeled emotions. We"
        },
        {
          "Abstract": "also develop a deep convolutional neural network(CNN) for 3D FER trained on"
        },
        {
          "Abstract": "624,000 3D facial scans. The test data comprises 208,000 3D facial scans."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "624,000 3D facial scans. The test data comprises 208,000 3D facial scans.": ""
        },
        {
          "624,000 3D facial scans. The test data comprises 208,000 3D facial scans.": ""
        },
        {
          "624,000 3D facial scans. The test data comprises 208,000 3D facial scans.": ""
        },
        {
          "624,000 3D facial scans. The test data comprises 208,000 3D facial scans.": ""
        },
        {
          "624,000 3D facial scans. The test data comprises 208,000 3D facial scans.": ""
        },
        {
          "624,000 3D facial scans. The test data comprises 208,000 3D facial scans.": ""
        },
        {
          "624,000 3D facial scans. The test data comprises 208,000 3D facial scans.": ""
        },
        {
          "624,000 3D facial scans. The test data comprises 208,000 3D facial scans.": ""
        },
        {
          "624,000 3D facial scans. The test data comprises 208,000 3D facial scans.": ""
        },
        {
          "624,000 3D facial scans. The test data comprises 208,000 3D facial scans.": ""
        },
        {
          "624,000 3D facial scans. The test data comprises 208,000 3D facial scans.": ""
        },
        {
          "624,000 3D facial scans. The test data comprises 208,000 3D facial scans.": ""
        },
        {
          "624,000 3D facial scans. The test data comprises 208,000 3D facial scans.": ""
        },
        {
          "624,000 3D facial scans. The test data comprises 208,000 3D facial scans.": ""
        },
        {
          "624,000 3D facial scans. The test data comprises 208,000 3D facial scans.": ""
        },
        {
          "624,000 3D facial scans. The test data comprises 208,000 3D facial scans.": ""
        },
        {
          "624,000 3D facial scans. The test data comprises 208,000 3D facial scans.": ""
        },
        {
          "624,000 3D facial scans. The test data comprises 208,000 3D facial scans.": ""
        },
        {
          "624,000 3D facial scans. The test data comprises 208,000 3D facial scans.": ""
        },
        {
          "624,000 3D facial scans. The test data comprises 208,000 3D facial scans.": ""
        },
        {
          "624,000 3D facial scans. The test data comprises 208,000 3D facial scans.": ""
        },
        {
          "624,000 3D facial scans. The test data comprises 208,000 3D facial scans.": ""
        },
        {
          "624,000 3D facial scans. The test data comprises 208,000 3D facial scans.": "information about author"
        },
        {
          "624,000 3D facial scans. The test data comprises 208,000 3D facial scans.": ""
        },
        {
          "624,000 3D facial scans. The test data comprises 208,000 3D facial scans.": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1: Details of 3D Datasets for FER": "Expressions"
        },
        {
          "Table 1: Details of 3D Datasets for FER": "7"
        },
        {
          "Table 1: Details of 3D Datasets for FER": "6"
        },
        {
          "Table 1: Details of 3D Datasets for FER": "6"
        },
        {
          "Table 1: Details of 3D Datasets for FER": "7"
        },
        {
          "Table 1: Details of 3D Datasets for FER": "7"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FRGC v.2 (22)": "Bosphous (21)",
          "6": "7",
          "466": "105",
          "4007": "4652"
        },
        {
          "FRGC v.2 (22)": "Ours",
          "6": "7",
          "466": "100",
          "4007": "832,000"
        },
        {
          "FRGC v.2 (22)": "FER. We cover this research gap and propose a Deep 3D expression recognition network suited for",
          "6": "",
          "466": "",
          "4007": ""
        },
        {
          "FRGC v.2 (22)": "3D face data and trained from scratch on 624K 3D faces.",
          "6": "",
          "466": "",
          "4007": ""
        },
        {
          "FRGC v.2 (22)": "In a nutshell, our contributions are as follows:",
          "6": "",
          "466": "",
          "4007": "(1) Large scale dataset: We present a method for"
        },
        {
          "FRGC v.2 (22)": "generating a large corpus of labeled 3D facial expression data for training CNNs. Our dataset contains",
          "6": "",
          "466": "",
          "4007": ""
        },
        {
          "FRGC v.2 (22)": "832K 3D facial scans of 100 identities, each consisting of 6 basic expressions and a neutral face,",
          "6": "",
          "466": "",
          "4007": ""
        },
        {
          "FRGC v.2 (22)": "highly rich in shape variations. (2) Deep 3D Facial Emotion Recognition Network. We propose a",
          "6": "",
          "466": "",
          "4007": ""
        },
        {
          "FRGC v.2 (22)": "deep CNN explicitly designed for 3D FER and trained on 832k 3D faces. We show its effectiveness",
          "6": "",
          "466": "",
          "4007": ""
        },
        {
          "FRGC v.2 (22)": "by achieving an end-to-end Rank-1 recognition rate of 88% on the BU3DFE dataset.",
          "6": "",
          "466": "",
          "4007": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 2: Comparison of emotion recognition accuracy (%) on BU3DFE AND Bosphorus dataset.": "Method"
        },
        {
          "Table 2: Comparison of emotion recognition accuracy (%) on BU3DFE AND Bosphorus dataset.": "Li et al. (30)"
        },
        {
          "Table 2: Comparison of emotion recognition accuracy (%) on BU3DFE AND Bosphorus dataset.": "Jan et al. (9)"
        },
        {
          "Table 2: Comparison of emotion recognition accuracy (%) on BU3DFE AND Bosphorus dataset.": "Sheng et al. (32)"
        },
        {
          "Table 2: Comparison of emotion recognition accuracy (%) on BU3DFE AND Bosphorus dataset.": "Khashman et al. (33)"
        },
        {
          "Table 2: Comparison of emotion recognition accuracy (%) on BU3DFE AND Bosphorus dataset.": "Ours(Kernel=7)"
        },
        {
          "Table 2: Comparison of emotion recognition accuracy (%) on BU3DFE AND Bosphorus dataset.": "Ours(Kernel=5)"
        },
        {
          "Table 2: Comparison of emotion recognition accuracy (%) on BU3DFE AND Bosphorus dataset.": "Ours(Kernel=3)"
        },
        {
          "Table 2: Comparison of emotion recognition accuracy (%) on BU3DFE AND Bosphorus dataset.": "Azazi et al. (10)"
        },
        {
          "Table 2: Comparison of emotion recognition accuracy (%) on BU3DFE AND Bosphorus dataset.": "Hariri et al. (31)"
        },
        {
          "Table 2: Comparison of emotion recognition accuracy (%) on BU3DFE AND Bosphorus dataset.": "Zhang et al. (24)"
        },
        {
          "Table 2: Comparison of emotion recognition accuracy (%) on BU3DFE AND Bosphorus dataset.": "Ours(Kernel=7)"
        },
        {
          "Table 2: Comparison of emotion recognition accuracy (%) on BU3DFE AND Bosphorus dataset.": "Ours(Kernel=5)"
        },
        {
          "Table 2: Comparison of emotion recognition accuracy (%) on BU3DFE AND Bosphorus dataset.": "Ours(Kernel=3)"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Ours(Kernel=5)": "Ours(Kernel=3)",
          "Bosphorus": "Bosphorus",
          "7E": "7E",
          "0:100": "0:100",
          "3D": "3D",
          "70.8%": "70.4%"
        },
        {
          "Ours(Kernel=5)": "",
          "Bosphorus": "neutral expression, giving us 13 facial scans for each identity.",
          "7E": "",
          "0:100": "",
          "3D": "Inspired by the work done by (29),",
          "70.8%": ""
        },
        {
          "Ours(Kernel=5)": "we establish dense correspondence over 15K 3D vertices on the faces from this dataset, using the",
          "Bosphorus": "",
          "7E": "",
          "0:100": "",
          "3D": "",
          "70.8%": ""
        },
        {
          "Ours(Kernel=5)": "",
          "Bosphorus": "keypoints-based algorithm. The goal now is to grow the dataset by generating faces from the space",
          "7E": "",
          "0:100": "",
          "3D": "",
          "70.8%": ""
        },
        {
          "Ours(Kernel=5)": "",
          "Bosphorus": "spanned by the trio of densely corresponding real 3D faces of distinct identities. We make use of the",
          "7E": "",
          "0:100": "",
          "3D": "",
          "70.8%": ""
        },
        {
          "Ours(Kernel=5)": "",
          "Bosphorus": "idea put forward by (34) and expand on it for facial expressions. To ensure that the identities in the",
          "7E": "",
          "0:100": "",
          "3D": "",
          "70.8%": ""
        },
        {
          "Ours(Kernel=5)": "",
          "Bosphorus": "trio are as “distinct” as possible, we select the face trio with the maximum non-rigid shape difference",
          "7E": "",
          "0:100": "",
          "3D": "",
          "70.8%": ""
        },
        {
          "Ours(Kernel=5)": "",
          "Bosphorus": "and the trio having the same expression to ensure the generated face has the same expression as that",
          "7E": "",
          "0:100": "",
          "3D": "",
          "70.8%": ""
        },
        {
          "Ours(Kernel=5)": "of original faces.",
          "Bosphorus": "",
          "7E": "",
          "0:100": "",
          "3D": "",
          "70.8%": ""
        },
        {
          "Ours(Kernel=5)": "",
          "Bosphorus": "Let the faces be represented by Fi = [xp, yp, xp]T , where i = 1, ..., N, p = 1, ..., P ; N = 100 and",
          "7E": "",
          "0:100": "",
          "3D": "",
          "70.8%": ""
        },
        {
          "Ours(Kernel=5)": "",
          "Bosphorus": "P = 15, 000. The shape difference between faces Fi, Fj and Fk is deﬁned as",
          "7E": "",
          "0:100": "",
          "3D": "",
          "70.8%": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4": "The success of the recent deep network in 2D FER inspired us to propose a deep convolutional neural",
          "Network Architecture": ""
        },
        {
          "4": "",
          "Network Architecture": "network that is suited to 3D data. 2D images exhibit signiﬁcant texture variations over small regions"
        },
        {
          "4": "",
          "Network Architecture": "and (35) network was designed for such case. In contrast, 3D facial surfaces are generally smooth,"
        },
        {
          "4": "",
          "Network Architecture": "and hence ﬁlters with larger kernel sizes would better suit this type of data. This claim is strengthened"
        },
        {
          "4": "by the results shown in Table 2.",
          "Network Architecture": ""
        },
        {
          "4": "The skeleton architecture of our network follows (35) but with a change in the convolution layers",
          "Network Architecture": ""
        },
        {
          "4": "(see Figure 1 for details). We aim to minimize the average prediction binary cross-entropy loss after",
          "Network Architecture": ""
        },
        {
          "4": "",
          "Network Architecture": "the softmax layer by learning the parameters of a network designed to classify the six basic emotions"
        },
        {
          "4": "and the neutral face. After the network is trained, dropout layers are removed before testing.",
          "Network Architecture": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "[1] Abate, Andrea & Nappi, Michele & Riccio, Daniel & Sabatino, Gabriele. “2D and 3D Face"
        },
        {
          "References": "Recognition: A Survey. Pattern Recognition Letters.” (2006) 28. 1885-1906."
        },
        {
          "References": "[2] Wang, Jun et al. “3D Facial Expression Recognition Based on Primitive Surface Feature Distribu-"
        },
        {
          "References": ""
        },
        {
          "References": "(2006): 1399-1406."
        },
        {
          "References": "[3] H. Soyel and H. Demirel, “Facial expression recognition using 3D facial feature distances,” 2007"
        },
        {
          "References": "Image Analysis and Recognition, pp. 1–13,2007."
        },
        {
          "References": "[4] Boqing Gong, Yueming Wang, Jianzhuang Liu, and Xiaoou Tang, “Automatic facial expression"
        },
        {
          "References": ""
        },
        {
          "References": "ACM international conference on Multimedia (MM ’09)."
        },
        {
          "References": "[5] S. Berretti, A. D. Bimbo, P. Pala, B. B. Amor and M. Daoudi, “A Set of Selected SIFT Features"
        },
        {
          "References": ""
        },
        {
          "References": "2010, pp. 4125-4128."
        },
        {
          "References": "[6] P. Lemaire, M. Ardabilian, L. Chen, and M. Daoudi, “Fully automatic 3d facial expression"
        },
        {
          "References": ""
        },
        {
          "References": ""
        },
        {
          "References": "Workshops on, April 2013, pp. 1–7."
        },
        {
          "References": "[7] H. Li, H. Ding, D. Huang, Y. Wang, X. Zhao, J. M. Morvan, and L. Chen, “An efﬁcient multi-"
        },
        {
          "References": ""
        },
        {
          "References": "Vision and Image Understanding, vol. 140, pp. 83–92, 2015."
        },
        {
          "References": "[8] H. LI, J. Sun, X. Zongben, and L. Chen, “Multimodal 2d+3d facial expression recognition with"
        },
        {
          "References": ""
        },
        {
          "References": "[9] A. Jan and H. Meng, “Automatic 3d facial expression recognition using geometric and textured"
        },
        {
          "References": ""
        },
        {
          "References": "and Gesture Recognition (FG), vol. 05, May 2015, pp. 1–6."
        },
        {
          "References": "[10] A. Azazi, S.L. Lutﬁ, I. Venkat, “Identifying universal facial emotion markers for automatic 3D"
        },
        {
          "References": ""
        },
        {
          "References": "Sciences (ICCOINS), IEEE, 2014, pp. 1–6."
        },
        {
          "References": "[11] Son Thai Ly, Nhu-Tai Do, Guee-Sang Lee, Soo-Hyung Kim and Hyung-Jeong Yan, “Multi-"
        },
        {
          "References": ""
        },
        {
          "References": "Computer Vision and Pattern Recognition (CVPR) Workshops, 2019."
        },
        {
          "References": "[12] Colombo, Alessandro & Cusano, Claudio & Schettini, Raimondo. (2006). “3D face detection"
        },
        {
          "References": ""
        },
        {
          "References": "[13] P. Nair and A. Cavallaro, “3-d face detection, landmark localization, and registration using a"
        },
        {
          "References": "point distribution model,” T.Multimedia, vol. 11, no. 4, pp. 611–623, 2009."
        },
        {
          "References": "[14] M. Pamplona Segundo, L. Silva, O. R. P. Bellon, and C. C.Queirolo, “Automatic face seg-"
        },
        {
          "References": "mentation and facial"
        },
        {
          "References": "2010."
        },
        {
          "References": "[15] A. Hernández, N. Zlateva, A. Marinov, M. Reyes, P. Radeva,D. Dimov, and S. Escalera, “Graph"
        },
        {
          "References": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "Expression Recognition Using BU-3DFE Database.” Singapore: Springer Singapore, 2016, pp."
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "441–450"
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "[18] H. Li, H. Ding, D. Huang, Y. Wang, X. Zhao, J. M. Morvan, andL. Chen, “An efﬁcient multi-"
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "modal 2D + 3D feature-based approach to automatic facial expression recognition,”Computer"
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "Vision and Image Understanding, vol. 140, pp. 83–92, 2015."
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "[19]\n\"A 3D Facial Expression Database For Facial Behavior Research” by Lijun Yin; Xiaozhou Wei;"
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "Yi Sun; Jun Wang; Matthew J. Rosato, 7th International Conference on Automatic Face and"
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "Gesture Recognition, 10-12 April 2006 P:211 - 216"
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "”A High-Resolution 3D Dynamic Facial Expression Database” by Lijun Yin; Xiaochen Chen;\n[20]"
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "Yi Sun; Tony Worm; Michael Reale, The 8th International Conference on Automatic Face and"
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "Gesture Recognition, 17-19 September 2008 (Tracking Number: 66)"
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "[21] Savran A. et al. (2008) Bosphorus Database for 3D Face Analysis. In: Schouten B., Juul N.C.,"
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "Drygajlo A., Tistarelli M. (eds) Biometrics and Identity Management. BioID 2008. Lecture Notes"
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "in Computer Science, vol 5372. Springer, Berlin, Heidelberg. https://doi.org/10.1007/978-3-540-"
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "89991-4_6"
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "[22] P. J. Phillips et al., \"Overview of the face recognition grand challenge,\" 2005 IEEE Computer"
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "Society Conference on Computer Vision and Pattern Recognition (CVPR’05), 2005, pp. 947-954"
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "vol. 1, doi: 10.1109/CVPR.2005.268."
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "[23] M. Hayat and M. Bennamoun, \"An Automatic Framework for Textured 3D Video-Based Facial"
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "Expression Recognition,\" in IEEE Transactions on Affective Computing, vol. 5, no. 3, pp."
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "301-313, 1 July-Sept. 2014, doi: 10.1109/TAFFC.2014.2330580."
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "[24] Y. Zhang, L. Zhang, M. Hossain, “Adaptive 3D facial action intensity estimation and emotion"
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "recognition”, Expert Syst. Appl. 42 (3) (2015) 1446–1464, doi: 10. 1016/j.eswa.2014.08.042 ."
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "[25] Chang, Feng-Ju & Tran, Anh & Hassner, Tal & Masi, Iacopo & Nevatia, Ram & Medioni,"
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "Gérard. (2019). “Deep, Landmark-Free FAME: Face Alignment, Modeling, and Expression"
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "Estimation”. International Journal of Computer Vision. 127. 10.1007/s11263-019-01151-x."
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "[26] V. Blanz, S. Romdhani, and T. Vetter, “Face identiﬁcation across different poses and illumina-"
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "tions with a 3d morphable model.” In International Conference on Automatic Face and Gesture"
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "Recognition, pages 192–197,2002."
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "[27] Blanz, V. and T. Vetter. “Face Recognition Based on Fitting a 3D Morphable Model.” IEEE"
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "Trans. Pattern Anal. Mach. Intell. 25 (2003): 1063-1074."
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "to\n[28] Baptiste Chu, Sami Romdhani, and Liming Chen, “3D-Aided Face Recognition Robust"
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "Expression and Pose Variations”, In Proceedings of the 2014 IEEE Conference on Computer"
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "Vision and Pattern Recognition (CVPR ’14). IEEE Computer Society, USA, 1907–1914."
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "[29] Gilani, S. Z. et al. “Dense 3D Face Correspondence.” IEEE Transactions on Pattern Analysis"
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "and Machine Intelligence 40 (2018): 1584-1598."
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "[30] W. Li, D. Huang, H. Li, Y. Wang, “Automatic 4D facial expression recognition using dynamic"
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "geometrical image network”, in: 2018 13th IEEE International Conference on Automatic Face &"
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "Gesture Recognition (FG 2018), IEEE, 2018,pp. 24–30."
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "[31] Walid Hariri, Hedi Tabia, Nadir Farah, Abdallah Benouareth, and David Declercq. 2017. “3D"
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "facial expression recognition using kernel methods on Riemannian manifold”. Eng. Appl. Artif."
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "Intell. 64, C (September 2017), 25–32."
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "[32] N. Sheng, Y. Cai, C. Zhan, Changyan Qiu, Yize Cui, Xurong Gao, “3D facial expression"
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "recognition using distance features and LBP features based on automatically detected keypoints”,"
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "in: 2016 9th International Congress on Image and Signal Processing, Bio Medical Engineering"
        },
        {
          "[17] X.-P. Huynh, T.-D. Tran, and Y.-G. Kim, “Convolutional Neural Network Models for Facial": "and Informatics (CISP-BMEI), IEEE, Datong, 2016, pp. 396–401."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[33] A. Khashman, F.O. Conkbayir, “Intelligent recognition of emotional expressions in 3D face": ""
        },
        {
          "[33] A. Khashman, F.O. Conkbayir, “Intelligent recognition of emotional expressions in 3D face": "IEEE, 2013, pp. 1–4."
        },
        {
          "[33] A. Khashman, F.O. Conkbayir, “Intelligent recognition of emotional expressions in 3D face": "[34] Zulqarnain Gilani, Syed and Mian, Ajmal. “Learning from Millions of 3D Scans for Large-"
        },
        {
          "[33] A. Khashman, F.O. Conkbayir, “Intelligent recognition of emotional expressions in 3D face": ""
        },
        {
          "[33] A. Khashman, F.O. Conkbayir, “Intelligent recognition of emotional expressions in 3D face": "Recognition (CVPR), IEEE, doi: 10.1109/cvpr.2018.00203"
        },
        {
          "[33] A. Khashman, F.O. Conkbayir, “Intelligent recognition of emotional expressions in 3D face": "[35] Simonyan, Karen & Zisserman, Andrew."
        },
        {
          "[33] A. Khashman, F.O. Conkbayir, “Intelligent recognition of emotional expressions in 3D face": "Large-Scale Image Recognition. arXiv 1409.1556."
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "2D and 3D Face Recognition: A Survey",
      "authors": [
        "Andrea Abate",
        "Michele Nappi",
        "Riccio",
        "& Daniel",
        "Gabriele Sabatino"
      ],
      "year": "2006",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "2",
      "title": "3D Facial Expression Recognition Based on Primitive Surface Feature Distribution",
      "authors": [
        "Jun Wang"
      ],
      "year": "2006",
      "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "3",
      "title": "Facial expression recognition using 3D facial feature distances",
      "authors": [
        "H Soyel",
        "H Demirel"
      ],
      "year": "2007",
      "venue": "2007 Image Analysis and Recognition"
    },
    {
      "citation_id": "4",
      "title": "Automatic facial expression recognition on a single 3D face by exploring shape deformation",
      "authors": [
        "Boqing Gong",
        "Yueming Wang",
        "Jianzhuang Liu",
        "Xiaoou Tang"
      ],
      "venue": "Proceedings of the 17th ACM international conference on Multimedia (MM '09)"
    },
    {
      "citation_id": "5",
      "title": "A Set of Selected SIFT Features for 3D Facial Expression Recognition",
      "authors": [
        "S Berretti",
        "A Bimbo",
        "P Pala",
        "B Amor",
        "M Daoudi"
      ],
      "year": "2010",
      "venue": "20th International Conference on Pattern Recognition"
    },
    {
      "citation_id": "6",
      "title": "Fully automatic 3d facial expression recognition using differential mean curvature maps and histograms of oriented gradients",
      "authors": [
        "P Lemaire",
        "M Ardabilian",
        "L Chen",
        "M Daoudi"
      ],
      "year": "2013",
      "venue": "Automatic Face and Gesture Recognition (FG), 2013 10th IEEE International Conference and Workshops on"
    },
    {
      "citation_id": "7",
      "title": "An efficient multimodal 2D + 3D feature-based approach to automatic facial expression recognition",
      "authors": [
        "H Li",
        "H Ding",
        "D Huang",
        "Y Wang",
        "X Zhao",
        "J Morvan",
        "L Chen"
      ],
      "year": "2015",
      "venue": "Computer Vision and Image Understanding"
    },
    {
      "citation_id": "8",
      "title": "Multimodal 2d+3d facial expression recognition with deep fusion convolutional neural network",
      "authors": [
        "H Li",
        "J Sun",
        "X Zongben",
        "L Chen"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "9",
      "title": "Automatic 3d facial expression recognition using geometric and textured feature fusion",
      "authors": [
        "A Jan",
        "H Meng"
      ],
      "year": "2015",
      "venue": "2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "10",
      "title": "Identifying universal facial emotion markers for automatic 3D facial expression recognition",
      "authors": [
        "A Azazi",
        "S Lutfi",
        "I Venkat"
      ],
      "year": "2014",
      "venue": "2014 International Conference on Computer and Information Sciences (ICCOINS)"
    },
    {
      "citation_id": "11",
      "title": "Multimodal 2D and 3D for In-The-Wild Facial Expression Recognition",
      "authors": [
        "Son Thai Ly",
        "Nhu-Tai Do",
        "Guee-Sang Lee",
        "Soo-Hyung Kim",
        "Hyung-Jeong Yan"
      ],
      "year": "2019",
      "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops"
    },
    {
      "citation_id": "12",
      "title": "3D face detection using curvature analysis",
      "authors": [
        "Alessandro Colombo",
        "Claudio Cusano",
        "Raimondo Schettini"
      ],
      "year": "2006",
      "venue": "Pattern Recognition",
      "doi": "10.1016/j.patcog.2005.09.009"
    },
    {
      "citation_id": "13",
      "title": "3-d face detection, landmark localization, and registration using a point distribution model",
      "authors": [
        "P Nair",
        "A Cavallaro"
      ],
      "year": "2009",
      "venue": "T.Multimedia"
    },
    {
      "citation_id": "14",
      "title": "Automatic face segmentation and facial landmark detection in range images",
      "authors": [
        "M Segundo",
        "L Silva",
        "O Bellon",
        "C Queirolo"
      ],
      "year": "2010",
      "venue": "SMC-B"
    },
    {
      "citation_id": "15",
      "title": "Graph cuts optimization for multilimb human segmentation in depth maps",
      "authors": [
        "A Hernández",
        "N Zlateva",
        "A Marinov",
        "M Reyes",
        "P Radeva",
        "D Dimov",
        "S Escalera"
      ],
      "year": "2012",
      "venue": "CVPR"
    },
    {
      "citation_id": "16",
      "title": "Real-time human pose recognition in parts from single depth images",
      "authors": [
        "J Shotton",
        "T Sharp",
        "A Kipman",
        "A Fitzgibbon",
        "M Finocchio",
        "A Blake",
        "M Cook",
        "R Moore"
      ],
      "year": "2013",
      "venue": "CACM"
    },
    {
      "citation_id": "17",
      "title": "Convolutional Neural Network Models for Facial Expression Recognition Using BU-3DFE Database",
      "authors": [
        "X.-P Huynh",
        "T.-D Tran",
        "Y.-G Kim"
      ],
      "year": "2016",
      "venue": "Convolutional Neural Network Models for Facial Expression Recognition Using BU-3DFE Database"
    },
    {
      "citation_id": "18",
      "title": "An efficient multimodal 2D + 3D feature-based approach to automatic facial expression recognition",
      "authors": [
        "H Li",
        "H Ding",
        "D Huang",
        "Y Wang",
        "X Zhao",
        "J Morvan",
        "Chen"
      ],
      "year": "2015",
      "venue": "Computer Vision and Image Understanding"
    },
    {
      "citation_id": "19",
      "title": "A 3D Facial Expression Database For Facial Behavior Research",
      "authors": [
        "Lijun Yin",
        "; Xiaozhou",
        "Yi Sun",
        "; Jun Wang",
        "; Matthew",
        "J Rosato"
      ],
      "year": "2006",
      "venue": "th International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "20",
      "title": "A High-Resolution 3D Dynamic Facial Expression Database",
      "authors": [
        "Lijun Yin",
        "; Xiaochen",
        "Yi Sun",
        "Tony Worm",
        "; Michael Reale"
      ],
      "year": "2008",
      "venue": "The 8th International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "21",
      "title": "Bosphorus Database for 3D Face Analysis",
      "authors": [
        "A Savran"
      ],
      "year": "2008",
      "venue": "Biometrics and Identity Management",
      "doi": "10.1007/978-3-540-89991-4_6"
    },
    {
      "citation_id": "22",
      "title": "Overview of the face recognition grand challenge",
      "authors": [
        "P Phillips"
      ],
      "year": "2005",
      "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)",
      "doi": "10.1109/CVPR.2005.268"
    },
    {
      "citation_id": "23",
      "title": "An Automatic Framework for Textured 3D Video-Based Facial Expression Recognition",
      "authors": [
        "M Hayat",
        "M Bennamoun"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2014.2330580"
    },
    {
      "citation_id": "24",
      "title": "Adaptive 3D facial action intensity estimation and emotion recognition",
      "authors": [
        "Y Zhang",
        "L Zhang",
        "M Hossain"
      ],
      "year": "2015",
      "venue": "Expert Syst. Appl",
      "doi": "10.1016/j.eswa.2014.08.042"
    },
    {
      "citation_id": "25",
      "title": "Deep, Landmark-Free FAME: Face Alignment, Modeling, and Expression Estimation",
      "authors": [
        "Feng-Ju & Chang",
        "Anh Tran",
        "Hassner",
        "Masi",
        "& Iacopo",
        "Ram Nevatia",
        "Gérard Medioni"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision",
      "doi": "127.10.1007/s11263-019-01151-x"
    },
    {
      "citation_id": "26",
      "title": "Face identification across different poses and illuminations with a 3d morphable model",
      "authors": [
        "V Blanz",
        "S Romdhani",
        "T Vetter"
      ],
      "year": "2002",
      "venue": "International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "27",
      "title": "Face Recognition Based on Fitting a 3D Morphable Model",
      "authors": [
        "V Blanz",
        "T Vetter"
      ],
      "year": "2003",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "28",
      "title": "3D-Aided Face Recognition Robust to Expression and Pose Variations",
      "authors": [
        "Baptiste Chu",
        "Sami Romdhani",
        "Liming Chen"
      ],
      "venue": "Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR '14)"
    },
    {
      "citation_id": "29",
      "title": "Dense 3D Face Correspondence",
      "authors": [
        "S Gilani"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "30",
      "title": "Automatic 4D facial expression recognition using dynamic geometrical image network",
      "authors": [
        "W Li",
        "D Huang",
        "H Li",
        "Y Wang"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)"
    },
    {
      "citation_id": "31",
      "title": "3D facial expression recognition using kernel methods on Riemannian manifold",
      "authors": [
        "Walid Hariri",
        "Hedi Tabia",
        "Nadir Farah",
        "Abdallah Benouareth",
        "David Declercq"
      ],
      "year": "2017",
      "venue": "Eng. Appl. Artif. Intell"
    },
    {
      "citation_id": "32",
      "title": "3D facial expression recognition using distance features and LBP features based on automatically detected keypoints",
      "authors": [
        "N Sheng",
        "Y Cai",
        "C Zhan",
        "Changyan Qiu",
        "Yize Cui",
        "Xurong Gao"
      ],
      "year": "2016",
      "venue": "2016 9th International Congress on Image and Signal Processing"
    },
    {
      "citation_id": "33",
      "title": "Intelligent recognition of emotional expressions in 3D face images",
      "authors": [
        "A Khashman",
        "F Conkbayir"
      ],
      "year": "2013",
      "venue": "2013 21st Signal Processing and Communications Applications Conference (SIU)"
    },
    {
      "citation_id": "34",
      "title": "Learning from Millions of 3D Scans for Large-Scale 3D Face Recognition",
      "authors": [
        "Zulqarnain Gilani",
        "Syed And Mian"
      ],
      "year": "2018",
      "venue": "Learning from Millions of 3D Scans for Large-Scale 3D Face Recognition",
      "doi": "10.1109/cvpr.2018.00203"
    },
    {
      "citation_id": "35",
      "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "authors": [
        "Karen Simonyan",
        "Andrew Zisserman"
      ],
      "year": "2014",
      "venue": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
    }
  ]
}