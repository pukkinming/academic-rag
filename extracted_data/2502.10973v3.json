{
  "paper_id": "2502.10973v3",
  "title": "Akan Cinematic Emotions (Akace): A Multimodal Multi-Party Dataset For Emotion Recognition In Movie Dialogues",
  "published": "2025-02-16T03:24:33Z",
  "authors": [
    "David Sasu",
    "Zehui Wu",
    "Ziwei Gong",
    "Run Chen",
    "Pengyuan Shi",
    "Lin Ai",
    "Julia Hirschberg",
    "Natalie Schluter"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper, we introduce the Akan Conversation Emotion (AkaCE) dataset, the first multimodal emotion dialogue dataset for an African language, addressing the significant lack of resources for low-resource languages in emotion recognition research. AkaCE, developed for the Akan language, contains 385 emotion-labeled dialogues and 6,162 utterances across audio, visual, and textual modalities, along with word-level prosodic prominence annotations. The presence of prosodic labels in this dataset also makes it the first prosodically annotated African language dataset. We demonstrate the quality and utility of AkaCE through experiments using state-of-the-art emotion recognition methods, establishing solid baselines for future research. We hope AkaCE inspires further work on inclusive, linguistically and culturally diverse NLP resources. * Currently at Apple.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion Recognition in Conversation (ERC) is a rapidly evolving subfield of Natural Language Processing (NLP) that focuses on detecting or classifying the emotional states expressed by speakers in multi-turn conversations  (Poria et al., 2019) . Unlike traditional emotion recognition tasks that aim to identify emotions from isolated text or speech snippets or speech utterances such as  (Zahiri and Choi, 2018) , ERC seeks to leverage contextual cues from prior dialogue, speaker relationships, and conversational flow to infer emotional states more accurately  (Poria et al., 2019) .\n\nIn recent years, ERC has garnered significant attention within the NLP community, driven by its growing relevance to a range of real-world applications. Notable examples include empathetic chatbot systems  (Fragopanagos and Taylor, 2005) , call-center dialogue systems  (Danieli et al., 2015) , and mental health support tools  (Ringeval et al. ,  ). These systems rely on ERC to capture the evolving emotional dynamics of conversations, enabling more contextually appropriate and emotionally aware responses. Developing robust ERC systems often requires multimodal data integration  (Poria et al., 2018) , which is challenging due to the need to jointly model diverse inputs like scene context, discussion topics, conversational history, and speaker personalities  (Shen et al., 2020; Hazarika et al., 2018a; Wu et al., 2024b) . However, comprehensive multimodal ERC dialogue datasets remain scarce, with benchmark resources like IEMOCAP  (Busso et al., 2008) , MSP-IMPROV  (Busso et al., 2016) , MELD  (Poria et al., 2018) , and M³ED  (Zhao et al., 2022a)  being notable exceptions.\n\nA major limitation of existing ERC datasets is their focus on high-resource languages, particularly English (IEMOCAP, MSP-IMPROV, MELD) and Chinese (M³ED). This lack of linguistic diversity hinders the development of ERC systems for lowresource languages, especially in Africa. To our knowledge, no multimodal ERC dataset exists for any African language, despite the continent being home to approximately 3,000 of the world's 7,000 languages (Leben, 2018) and 18.3% of the global population  (Mo Ibrahim Foundation, 2023) .\n\nTo address this gap, we introduce the Akan Con-versation Emotion (AkaCE) dataset, a multimodal emotion dialogue dataset for Akan, a major West African language spoken by about 20 million people  (Peterson et al., n.d.) . Akan is the most widely spoken language in Ghana, with around 80% of the population using it as a first or second language, and approximately 44% identifying as native Akan speakers. It is also natively spoken in parts of Ivory Coast and Togo. The language primarily comprises three main dialects: Asante, Akuapem, and Fante. AkaCE contains 385 emotion-labeled dialogues from 21 Akan movies, covering diverse scenes and topics. It includes 6,162 utterances from 308 speakers (155 male, 153 female), ensuring a genderbalanced dataset. As a tonal language, Akan's prosodic features are crucial for emotion recognition, so AkaCE includes word-level prosodic prominence annotations to support research on prosody in ERC. Our baseline experiments validate the dataset's quality and utility for low-resource and cross-cultural emotion recognition research. Our main contributions are:\n\n1. We introduce AkaCE, the first multimodal emotion dialogue dataset for an African language, enabling cross-cultural emotion research 1 . 2. We validate AkaCE through experiments with state-of-the-art ERC methods, establishing a strong baseline and detailed analysis. 3. We provide word-level prosodic prominence annotations, making AkaCE the first prosodically annotated dataset for an African language, facilitating research on prosody's role in ERC and tonal language processing.\n\n2 Related work",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Datasets",
      "text": "ERC and Speech Emotion Recognition (SER) datasets are essential for developing models to understand and classify emotions in speech. These datasets can be grouped into three main categories.\n\nTable  1  compares AkaCE with other discussed datasets.\n\nSingle-modality datasets focus exclusively on a single modality, such as text, including EmoryNLP  (Zahiri and Choi, 2018), EmotionLines (Chen et al., 2018) , and DailyDialog  (Li et al., 2017) . These are useful for text-based emotion recognition but lack the multi-modal richness needed for comprehensive analysis involving vocal or visual cues.\n\n1 Our data and code are available at this GitHub repo.\n\nMulti-modal datasets combine text, audio, or video, such as CMU-MOSEI  (Zadeh et al., 2018) , AFEW  (Dhall et al., 2012) , MEC  (Li et al., 2018) , and CH-SIMS  (Yu et al., 2020) . While offering richer features, they are not conversational and miss the dynamic, context-dependent expressions seen in natural dialogues.\n\nConversational multi-modal datasets integrate text, audio, and video with conversational context, such as IEMOCAP  (Busso et al., 2008) , MSP-IMPROV  (Busso et al., 2016) , MELD  (Poria et al., 2018) , and M³ED  (Zhao et al., 2022a) . But these datasets mainly focus on high-resource languages, leaving gaps for low-resource languages.\n\nExisting datasets lack resources for low-resource languages, such as Akan, and prosodic annotations critical for tonal languages. AkaCE fills this gap by offering a multi-modal resource with prosodic labels and conversational data, enabling robust SER for Akan and advancing cross-cultural SER research.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Methods",
      "text": "Conversational emotion recognition (ERC) has evolved through various approaches addressing contextual modeling, multimodal integration, and speaker dependencies. Early works used hierarchical LSTMs  (Poria et al., 2017)  and Conversational Memory Networks (CMN)  (Hazarika et al., 2018b)  to capture context and inter-speaker influences, improving sentiment classification but struggling with generalization and sparse contexts.\n\nDialogueRNN  (Majumder et al., 2019)  and Hi-GRU  (Jiao et al., 2019)  refined speaker-specific emotion tracking and attention-based modeling but faced challenges with subtle distinctions and multimodal integration. Knowledge-enriched models  (Zhong et al., 2019)  leveraged commonsense knowledge for emotion detection but struggled with closely related emotions and low-resource settings.\n\nGraph-based methods such as ConGCN  (Zhang et al., 2019)  and DialogueGCN  (Ghosal et al., 2019)  modeled multi-speaker dependencies effectively but relied heavily on textual features. Multimodal transformers like MulT  (Tsai et al., 2019)  and MMGCN  (Hu et al., 2021)  advanced crossmodal fusion but faced scalability issues due to dataset alignment and computational demands.\n\nRecent transformer-based models like DialogXL  (Shen et al., 2021)  and EmoBERTa  (Kim and Vossen, 2021)     (Lei et al., 2023) , incorporating acoustic features  (Wu et al., 2024a)  and contextual information  (Xue et al., 2024; Fu, 2024; Zhang et al., 2023) . Despite these advancements, existing methods often lack robust solutions for underrepresented languages and datasets. Our work bridges these gaps by introducing a multimodal dataset and a focus on low-resource settings, enabling more comprehensive and inclusive ERC research.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Akace Dataset",
      "text": "We construct the AkaCE dataset by collecting and annotating dialogues from Akan-language movies, with examples illustrated in Figure  1 . The dataset includes transcriptions with speaker identifications, emotion labels, and word-level prosodic prominence annotations. Table  1  compares AkaCE with other discussed datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Data Selection",
      "text": "The dataset consists of 21 Akan movies that were downloaded from the Internet Archive. To ensure that the movies included within this dataset were of high quality we ensured that each of the movies selected to be a part of the dataset fulfilled the following criteria: (1) the movie must be complete and not truncated in any section, (2) the speech of the actors within the movie should be intelligible, (3) the facial expressions of the actors within movie should be clear.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Annotators And Annotation Process",
      "text": "The annotation task was carried out by Akan data annotation professionals contracted through an in-stitute of linguistics and bible translation in Ghana. The annotators consisted of five men and two women, all native Akan speakers. Of these seven annotators, three were employed to work full-time while the rest worked part-time. One of the fulltime annotators opted to annotate seven movies, whereas the other two full-time annotators each chose to annotate five movies. The remaining four part-time annotators annotated one movie each. The movies were randomly assigned to their respective annotators.\n\nThe data annotators recorded the desired data by watching the movies and simultaneously recording the necessary information into Microsoft Excel sheets. All resulting sheets were then collated into one Excel sheet, where all redundant entries were eliminated and annotation errors were corrected.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Text And Speaker Annotation",
      "text": "Even though there have been recent advances in Akan Automatic Speech Recognition (ASR), most modern Akan ASR systems still generate many recognition errors as a result of the dearth of training data available  (Dossou, 2024) . As a result of this, all the speech utterances for each movie were manually transcribed by the annotators before any emotion labelling was performed.\n\nAdditionally, due to the lack of acoustic models for Akan that could facilitate audio alignment and automatically generate timestamps for each utterance in a movie, annotators manually tracked and recorded the timestamps for all utterances.\n\nFor the speaker annotations, the speaker for each utterance was identified by a unique identifier which consists of a combination of the order in which the speaker first appeared in the current dialogue and their gender. For instance, a possible label that would be assigned to a man who is the first speaker in the current dialogue of a scene within a movie is 'speaker one man'.\n\nTo ensure the high quality of the utterance transcriptions, a professional Akan linguist from the same institute was employed to peruse all of the transcriptions provided by the annotators and correct any identified errors.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Annotation",
      "text": "The emotion demonstrated for each utterance within a dialogue was annotated using one of seven possible emotion labels: Sadness, Fear, Anger, Surprise, Disgust, Happy and Neutral. Six out of these emotions (i.e Sadness, Fear, Anger, Surprise, Happy and Disgust) were proposed by Paul  Ekman (1992)  as the six universal human emotions. Following previous works  (Poria et al., 2018; Busso et al., 2008; Gong et al., 2024) , a neutral emotion label was added to identify utterances within the dataset that did not carry any pronounced emotional undertone. This set of seven emotion labels is further supported by Ahmad et al. (  2025 ), who employed the same categories in their study of cultural variation in emotion perception across 15 African languages.\n\nThe annotators were instructed to assign emotion labels to each utterance while simultaneously viewing their assigned movies. To ensure the accuracy and reliability of annotations, a preliminary information session was held by a research coordinator at the aforementioned institute in Ghana. This session provided a comprehensive overview of the annotation task, clarified expectations, and included illustrative examples of how each target emotion might manifest in various scenarios. In cases of uncertainty, annotators were guided to select the emotion label they deemed most appropriate for the utterance. The emotion annotation tutorial was designed with inspiration and reference to established emotion annotation guidelines, such as  Gong et al. (2024) .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Annotation Finalization",
      "text": "Following the preliminary emotion annotation process, two annotators who demonstrated the highest quality in utterance transcriptions were selected to provide second-opinion emotion labels for utterances they had not annotated during the initial labelling phase. After this second round of labelling, the final emotion label for each utterance in the dataset was determined through a majority voting procedure. In cases of inter-annotator disagreement regarding the appropriate emotion label, the final decision was made by an external Akan-speaking consultant, recognized as an expert in Akan Emotion Analysis. Notably, an analysis of inter-annotator agreement yielded an overall Fleiss' Kappa statistic  (Fleiss, 1971 ) of k = 0.488 0.488 0.488 which is comparable to the inter-annotator agreement scores of several other popular high-quality speech emotion datasets such as MELD  (Poria et al., 2018)  which has a score of 0.43  (Poria et al., 2018) , IEMOCAP which has a score of 0.48  (Busso et al., 2008) , MSP-IMPROV which has a score of 0.49  (Busso et al., 2016)  and M³ED which has a score of 0.59  (Zhao et al., 2022b) .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Prosodic Prominence Annotation",
      "text": "The annotation strategy used for prosodic prominence closely mirrored the approach employed for emotion labelling. The same two annotators responsible for assigning emotion labels to the utterances were selected for this task. Before starting the prosodic prominence annotations, they received detailed instructions outlining the concept of prosodic prominence and the steps involved in performing the task. Additionally, they were presented with examples of prosodic prominence annotations deemed accurate by consulted linguists to ensure a clear understanding of the expectations.\n\nFor the annotation task, the annotators were instructed to listen to the audio of each utterance in the dataset and assign a value of 1 to words they deemed prosodically prominent and 0 to words they considered non-prominent. All annotations were conducted using Excel sheets. This approach to prosodic prominence annotation was inspired by a similar approach leveraged in  Cole et al. (2017) . An analysis of the inter-annotator agreement for prosodic annotation between the two annotators yielded an overall Fleiss' Kappa statistic  (Fleiss, 1971)  of k = 1.0 1.0 1.0. The observed perfect agreement may be attributed to the tonal and expressive nature of Akan, which provides listeners with clear prosodic cues for identifying prominent words. In particular, prior work by  Kügler and Genzel (2012)  shows that Akan speakers signal pragmatic prominence through consistent acoustic patterns such as pitch lowering, even on high-toned syllables. This systematic use of prosody to express information structure likely contributes to the high salience of prominent words in speech, facilitating consistent annotation across raters.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Dataset Statistics",
      "text": "General Dataset Statistics Table  2  presents basic statistics of the Akan Cinematic Emotions (AkaCE) Dataset. It contains 385 dialogues, 4477 turns and 6162 utterances, which contain an average of 19 words. With respect to prosodic prominence, 37314 words were annotated to be prosodically prominent whereas 79991 words were annotated to be non-prominent.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Distribution",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Experiments And Analysis",
      "text": "We conduct a series of experiments to establish baseline performance for emotion recognition on AkaCE using unimodal and multimodal approaches. We first evaluate text, audio, and vision separately with state-of-the-art models, then explore modality combinations through feature concatenation and transformer-based fusion. These results serve as a foundation for future research on multimodal emotion recognition in Akan.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Experiment Setup",
      "text": "Each movie in our dataset is segmented into training, testing, and validation sets using a 7:1.5:1.5 ratio. Following a comprehensive data cleaning process that removed invalid utterances -specifically those with erroneous timestamps or annotations -the final dataset comprised 3,888 utterances for training, 816 for validation, and 834 for testing.\n\nSegmentation for both audio and video modalities is based on the timestamps associated with each utterance. The audio recordings, originally sampled at 44 kHz, are resampled to 16 kHz to meet the input requirements of the Whisper  (Radford et al., 2022)  model. Video frames are extracted at two distinct rates -1 frame per second and 5 frames per second -to evaluate the impact of temporal resolution on emotion detection. Additionally, MTCNN  (Zhang et al., 2016)  is employed to extract faces from each frame, capturing crucial facial cues essential for effective emotion recognition.\n\nWe conduct our experiments on an RTX A6000 GPU. To ensure a reliable assessment of model performance, we use weighted F1 and macro F1 scores instead of accuracy, as the latter can be misleading in imbalanced scenarios.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Text Experiments",
      "text": "For our text experiments, we employ the Ghana-NLP/abena-base-asante-twi-uncased (Alabi et al., 2020) model from Hugging Face, a variant of multilingual BERT (mBERT) fine-tuned specifically for the Akan language. The model is initially trained on the Twi subset of the JW300  (  Vulić, 2019) dataset, which primarily consists of the Akuapem dialect of Twi, and is later fine-tuned on Asante Twi Bible data to specialize in Asante Twi. To our knowledge, this remains the only available language model trained on this language. We investigate the impact of context by comparing two settings: one incorporating the previous utterance as context and another without contextual information. Following the context modeling approach from MMML  (Wu et al., 2024b) , we process the context and current utterance separately before concatenating their feature representations, rather than simply merging them at the input level. The concatenated features are then passed to the classifier layer. We use a learning rate of 1e-5 and a batch size of 16 in both settings.\n\nAs shown in Table  5 , our results indicate that incorporating context improves performance. Specifically, the model without context achieves a weighted F1 score of 43.12 and a macro F1 score of 18.85, while the context-aware model yields a weighted F1 score of 44.58 and a macro F1 score of 22.29. These findings highlight the benefits of incorporating contextual information for emotion detection in Akan text.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Audio Experiments",
      "text": "We conduct audio experiments using three different encoding methods: Whisper 2 , spectrogrambased features, and openSMILE 3 , where Whisper achieves the highest performance (Table  6 ). We set the learning rate to 1e-5 and use a batch size of 16 for all three methods.\n\nOpenSMILE features are extracted using the ComParE 2016 feature set, incorporating Low-Level Descriptors (LLDs) and Functionals, resulting in a 130-dimensional feature vector with a maximum sequence length of 3000. These features are then used to train an audio transformer encoder, following the approach of  Wu et al. (2024b) . The model consists of three transformer encoder layers, each with two attention heads, and positional encoding, followed by a fully connected linear classifier for prediction. The model reaches a weighted F1 score of 13.80 and a macro F1 score 6.58. This low performance can be attributed to the absence of pretraining.\n\nSpectrogram features are computed with 128 Mel-frequency bins, normalized, and truncated 2 https://huggingface.co/openai/whisper-small 3 https://audeering.github.io/opensmile-python/",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Model",
      "text": "Weighted or padded to a maximum length of 1024 frames. These features are then used to fine-tune a pretrained Audio Spectrogram Transformer (AST)  (Gong et al., 2021)  with an additional linear classifier layer. The model achieves a weighted F1 score of 47.89 and a macro F1 score of 23.36. We select AST due to its pretraining on a diverse auditory data, encompassing both human speech and nonhuman sounds, such as music and environmental noises. This broad training enables AST to effectively capture complex acoustic patterns, making it particularly well-suited for our AkaCE dataset, which consists of movie scenes containing a mix of dialogue, background music, and ambient sounds. Finally, we fine-tune a Whisper-Small encoder without freezing its parameters, achieving the best performance with a weighted F1 score of 52.38 and a macro F1 score of 29.51. These results indicate that pretraining audio models on multiple languages benefits speech emotion recognition in low-resource target languages. However, due to the imbalance in training samples for Whisper, the improvement remains relatively small. This further underscores the necessity of our dataset collection, as it represents the first multimodal emotion dialogue dataset for an African language, addressing the significant resource gap in emotion recognition research for low-resource languages.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Vision Experiments",
      "text": "For the vision modality, we explore two main approaches for encoding visual information. First, we use ResNet18 and ResNet50  (He et al., 2015)  to extract feature representations from entire video frames. To evaluate the impact of temporal resolution on emotion detection, we experiment with frame sampling rates of 1 frame per second (1 fps) and 5 frames per second (5 fps). In addition, we investigate a face-based approach where faces are extracted from each frame using MTCNN and then encoded with InceptionResnetV1, a model pre-trained on VGGFace2  (Cao et al., 2018) . All vision experiments are conducted using a learning rate of 1e-4 and a batch size of 16.\n\nAs shown in The face-based approach using InceptionRes-NetV1 underperforms compared to whole-frame models, achieving only 39.96 weighted F1 and 16.53 macro F1, suggesting that facial expressions alone may not provide sufficient information for robust emotion detection in our dataset. Unlike datasets such as CMU-MOSEI  (Zadeh et al., 2018)  that enforce a single visible face in close-up shots, our dataset does not impose such constraints. As a result, videos may contain multiple faces, and the primary speaker's face may be distant from the camera, adding challenges for models relying solely on facial features. These findings highlight the importance of frame selection strategies and suggest that balancing temporal resolution with model capacity is crucial for optimal vision-based emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multimodal Experiments",
      "text": "We evaluate modality combinations using the bestperforming unimodal models. Starting with simple feature concatenation as a baseline, we then apply transformer-based fusion to enhance cross-modal interactions. These experiments assess the impact of multimodal integration on emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Modality Features Concatenation",
      "text": "For the fusion experiments, we evaluate all possible combinations of the three modalities to understand how multimodal integration impacts ERC. We use the best-performing unimodal models: the contextual text model (Ghana-NLP/abena-base-asantetwi-uncased) for text, Whisper-small for audio, and ResNet18-5fps for vision. Feature representations from each modality are concatenated and passed through a classifier layer to compute logits for emotion prediction. To consider distinct characteristics of each modality, we experiment with different learning rates but find that using a single learning rate of 1e-5 yields the most stable results.\n\nOur results in Table  8  show that combining modalities improves emotion recognition performance, with the best results achieved when integrating all three modalities. The multimodal model using text, audio, and vision achieves the highest weighted F1 (55.81) and macro F1 (30.97), outperforming all unimodal and bimodal models.\n\nAmong the unimodal models, audio performs best (52.38 weighted F1, 29.51 macro F1), indicating that speech features carry the most discriminative information for emotion recognition in our dataset. Interestingly, text alone (44.58 weighted F1, 22.29 macro F1) underperforms compared to audio, contrasting with trends in high-resource languages where text embeddings often yield the best results  (Zadeh et al., 2018; Yu et al., 2020) . This gap is likely due to the limited availability of large and diverse pretraining corpora for Akan, restricting the effectiveness of text embeddings. Vision alone performs worst (40.57 weighted F1), suggesting that visual cues are less reliable, possibly due to variations in facial visibility and camera angles.\n\nIn bimodal settings, text + audio (55.51 weighted F1) and audio + vision (53.84 weighted F1) show substantial improvements over their unimodal counterparts, reinforcing the importance of speech information in multimodal emotion recognition. However, text + vision (43.33 weighted F1) provides only a marginal improvement over vision alone, suggesting that textual and visual features may not be as complementary as text and audio. Overall, these results highlight the advantages of multimodal fusion, particularly the strong synergy be- tween textual and auditory features, while also emphasizing the challenges posed by the limited availability of high-quality pretraining data for Akan.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Transformer Fusion",
      "text": "To further enhance multimodal fusion, we employ a transformer-based cross-attention encoder to capture interdependencies between different modalities. This approach enables a more nuanced integration of modality-specific features by projecting information from one modality into the representational space of another. Given our bimodal results indicating that text and vision do not complement each other effectively, we structure our fusion process around audio-centric interactions. Specifically, we use a cross-attention encoder to fuse text and audio (audio-text fusion) as well as audio and vision (audio-vision fusion).\n\nIn this framework, we first extract features from each modality using the best-performing unimodal models. The cross-attention encoder is designed such that the query comes from one modality while the keys and values are derived from another. This mechanism allows each modality to selectively attend to the most relevant aspects of the other, facilitating effective multimodal alignment. The encoder projects the hidden representations of one modality into the representational space of another, enhancing cross-modal interactions.\n\nTo structure the fusion process, we prepend a CLS token to the hidden states of each modality before applying the cross-attention mechanism. After audio-text fusion, we obtain two new hidden representations: T a , where text features are projected into the audio space, and A t , where audio features are projected into the text space. Similarly, for audio-vision fusion, we obtain A v and V a , corresponding to audio projected into the vision space and vice versa. For classification, we use the CLS token from each fused representation as features.\n\nAdditionally, we incorporate the context feature from the text encoder to enrich the final representation. These features are concatenated and passed through a classifier layer to predict emotion labels.\n\nAs shown in Table  9 , Transformer fusion outperforms simple concatenation in both weighted and macro F1 scores, achieving 56.13 and 31.68, respectively. This improvement highlights the effectiveness of advanced fusion mechanisms in integrating multimodal features for emotion recognition. The higher macro F1 score suggests that Transformer Fusion provides better balance across emotion classes, likely due to its ability to capture cross-modal dependencies more effectively. These findings underscore the potential of attention-based fusion techniques for enhancing multimodal ERC, particularly in low-resource settings like Akan.\n\nTable  10  presents the per-emotion F1 scores and support. As expected, the model performs best on classes with higher frequency in the training set, such as Neutral, Anger, and Sadness, while performance on low-resource classes like Disgust and Fear remains limited. These findings underscore the importance of addressing class imbalance in multimodal emotion recognition, especially in low-resource language contexts such as Akan.\n\nModel Complexity. To contextualize the scale of our proposed transformer fusion system, we report in Table  11  the parameter counts for each of its core components. For the vision backbone, we use ResNet-18 (convolutional and fully connected layers), totalling approximately 11.7 million parameters. Text features are extracted using the Ghana-NLP abena-base-asante-twi-uncased model, a BERT-base variant with 12 transformer layers of 768 hidden units, contributing about 178 million parameters. Audio features are encoded with Ope-nAI's Whisper-small model, which consists of 12layer encoder-decoder stacks with 768-dimensional hidden representations, accounting for 244 million parameters. The transformer fusion module itself is composed of five cross-attention blocks, each using 768-dimensional queries, keys, and values with 12 attention heads, adding roughly 12 million parameters. Altogether, the complete model comprises approximately 450 million parameters, with the majority allocated to the unimodal backbones and only a small fraction to the fusion mechanism.  Looking ahead, we aim to expand to additional African languages and develop culturally adaptive ERC systems. Multimodal emotion recognition can be improved by speech enhancement techniques and pretraining models on African languages. Integrating vision-language models for scene descriptions can also provide richer context. Advanced fusion techniques like graph neural networks (GNNs) and hypergraphs may further refine cross-modal integration. We hope AkaCE inspires further research toward culturally adaptive, linguistically diverse NLP resources.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Limitations",
      "text": "While the Akan Cinematic Emotions (AkaCE) dataset represents a significant advancement in multimodal emotion recognition research, particularly for African languages, there are several limitations to acknowledge.\n\nOne limitation of this work is that the dataset focuses exclusively on the Akan language. While this contributes to the representation of low-resource languages in emotion recognition research, the findings may not generalize to other African languages or cultural contexts without further adaptation and testing. The emotional expressions and prosodic characteristics in Akan may differ substantially from those in other languages, limiting cross-linguistic applicability.\n\nAnother limitation lies in the domain of the dataset, which is derived from movie dialogues. While this ensures the presence of diverse emotions and rich multimodal interactions, it is likely that a portion of the data contains acted emotions rather than naturally occurring ones. Acted emotions may differ in intensity, expression, and prosodic features from emotions encountered in real-world scenarios, potentially introducing a bias in models trained on this dataset. This could affect the generalizability of such models to real-life applications, where emotional expressions might be less exaggerated or contextually different.\n\nAdditionally, while the inclusion of prosodic annotations is a novel feature, the labelling process may be subject to subjective interpretations, particularly for ambiguous emotional expressions. The quality and consistency of these annotations could impact the performance of models relying on prosodic features. Further efforts to standardize prosodic annotation practices would benefit future iterations of this dataset.\n\nAnother challenge is related to visual data. Although the dataset incorporates visual modalities, the quality and consistency of visual features in movie dialogues may vary due to differences in lighting, camera angles, and actor positioning. These variations could impact the reliability of visual emotion recognition models trained on this dataset. Moreover, further exploration of vision features, including fine-tuned embeddings and advanced visual annotations, may reveal additional insights but was not the focus of this study.\n\nDespite these limitations, we believe that AkaCE provides an essential foundation for advancing speech emotion recognition in low-resource languages and encourages further exploration in this area.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Ethical Considerations",
      "text": "The potential for misuse of the AkaCE dataset must be carefully acknowledged. While the dataset is intended for research purposes, deploying models trained on AkaCE in real-world applications without proper domain adaptation and validation could result in inaccurate emotion predictions, particularly in scenarios that deviate from cinematic dialogues. As such, researchers and practitioners should exercise caution when extending the use of this dataset to other applications.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An example of a dialogue showing conversa-",
      "page": 1
    },
    {
      "caption": "Figure 1: The dataset",
      "page": 3
    },
    {
      "caption": "Figure 2: Illustration of the transformer fusion model.",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "inspires further work on inclusive, linguistically": ""
        },
        {
          "inspires further work on inclusive, linguistically": "and culturally diverse NLP resources."
        },
        {
          "inspires further work on inclusive, linguistically": ""
        },
        {
          "inspires further work on inclusive, linguistically": ""
        },
        {
          "inspires further work on inclusive, linguistically": "1\nIntroduction"
        },
        {
          "inspires further work on inclusive, linguistically": ""
        },
        {
          "inspires further work on inclusive, linguistically": "Emotion Recognition in Conversation (ERC)\nis"
        },
        {
          "inspires further work on inclusive, linguistically": "a rapidly evolving subfield of Natural Language"
        },
        {
          "inspires further work on inclusive, linguistically": "Processing (NLP) that focuses on detecting or clas-"
        },
        {
          "inspires further work on inclusive, linguistically": "sifying the emotional states expressed by speakers"
        },
        {
          "inspires further work on inclusive, linguistically": "in multi-turn conversations (Poria et al., 2019). Un-"
        },
        {
          "inspires further work on inclusive, linguistically": "like traditional emotion recognition tasks that aim"
        },
        {
          "inspires further work on inclusive, linguistically": "to identify emotions from isolated text or speech"
        },
        {
          "inspires further work on inclusive, linguistically": "snippets or speech utterances such as (Zahiri and"
        },
        {
          "inspires further work on inclusive, linguistically": "Choi, 2018), ERC seeks to leverage contextual cues"
        },
        {
          "inspires further work on inclusive, linguistically": ""
        },
        {
          "inspires further work on inclusive, linguistically": "from prior dialogue, speaker relationships, and con-"
        },
        {
          "inspires further work on inclusive, linguistically": ""
        },
        {
          "inspires further work on inclusive, linguistically": "versational flow to infer emotional states more ac-"
        },
        {
          "inspires further work on inclusive, linguistically": ""
        },
        {
          "inspires further work on inclusive, linguistically": "curately (Poria et al., 2019)."
        },
        {
          "inspires further work on inclusive, linguistically": ""
        },
        {
          "inspires further work on inclusive, linguistically": "In recent years, ERC has garnered significant"
        },
        {
          "inspires further work on inclusive, linguistically": ""
        },
        {
          "inspires further work on inclusive, linguistically": "attention within the NLP community, driven by"
        },
        {
          "inspires further work on inclusive, linguistically": ""
        },
        {
          "inspires further work on inclusive, linguistically": "its growing relevance to a range of real-world ap-"
        },
        {
          "inspires further work on inclusive, linguistically": ""
        },
        {
          "inspires further work on inclusive, linguistically": "plications. Notable examples include empathetic"
        },
        {
          "inspires further work on inclusive, linguistically": ""
        },
        {
          "inspires further work on inclusive, linguistically": "chatbot systems (Fragopanagos and Taylor, 2005),"
        },
        {
          "inspires further work on inclusive, linguistically": ""
        },
        {
          "inspires further work on inclusive, linguistically": "call-center dialogue systems (Danieli et al., 2015),"
        },
        {
          "inspires further work on inclusive, linguistically": ""
        },
        {
          "inspires further work on inclusive, linguistically": "and mental health support\ntools (Ringeval et al.,"
        },
        {
          "inspires further work on inclusive, linguistically": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract": "In this paper, we\nintroduce\nthe Akan Con-"
        },
        {
          "Abstract": "versation Emotion (AkaCE) dataset,\nthe first"
        },
        {
          "Abstract": "multimodal emotion dialogue dataset\nfor an"
        },
        {
          "Abstract": "African language, addressing the significant"
        },
        {
          "Abstract": "lack of resources for low-resource languages"
        },
        {
          "Abstract": "in emotion recognition research. AkaCE, de-"
        },
        {
          "Abstract": "veloped for the Akan language, contains 385"
        },
        {
          "Abstract": "emotion-labeled dialogues and 6,162 utterances"
        },
        {
          "Abstract": "across audio, visual, and textual modalities,"
        },
        {
          "Abstract": "along with word-level prosodic prominence"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "annotations. The presence of prosodic labels"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "in this dataset also makes it\nthe first prosodi-"
        },
        {
          "Abstract": "cally annotated African language dataset. We"
        },
        {
          "Abstract": "demonstrate the quality and utility of AkaCE"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "through experiments using state-of-the-art emo-"
        },
        {
          "Abstract": "tion recognition methods,\nestablishing solid"
        },
        {
          "Abstract": "baselines for future research. We hope AkaCE"
        },
        {
          "Abstract": "inspires further work on inclusive, linguistically"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "and culturally diverse NLP resources."
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "1\nIntroduction"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "Emotion Recognition in Conversation (ERC)\nis"
        },
        {
          "Abstract": "a rapidly evolving subfield of Natural Language"
        },
        {
          "Abstract": "Processing (NLP) that focuses on detecting or clas-"
        },
        {
          "Abstract": "sifying the emotional states expressed by speakers"
        },
        {
          "Abstract": "in multi-turn conversations (Poria et al., 2019). Un-"
        },
        {
          "Abstract": "like traditional emotion recognition tasks that aim"
        },
        {
          "Abstract": "to identify emotions from isolated text or speech"
        },
        {
          "Abstract": "snippets or speech utterances such as (Zahiri and"
        },
        {
          "Abstract": "Choi, 2018), ERC seeks to leverage contextual cues"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "from prior dialogue, speaker relationships, and con-"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "versational flow to infer emotional states more ac-"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "curately (Poria et al., 2019)."
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "In recent years, ERC has garnered significant"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "attention within the NLP community, driven by"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "its growing relevance to a range of real-world ap-"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "plications. Notable examples include empathetic"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "chatbot systems (Fragopanagos and Taylor, 2005),"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "call-center dialogue systems (Danieli et al., 2015),"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "and mental health support\ntools (Ringeval et al.,"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "*Currently at Apple."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table 1: compares AkaCE with other discussed",
      "data": [
        {
          "the dataset’s quality and utility for low-resource": "and cross-cultural emotion recognition research.",
          "for Akan and advancing cross-cultural SER re-": "search."
        },
        {
          "the dataset’s quality and utility for low-resource": "Our main contributions are:",
          "for Akan and advancing cross-cultural SER re-": ""
        },
        {
          "the dataset’s quality and utility for low-resource": "",
          "for Akan and advancing cross-cultural SER re-": "2.2\nRelated Methods"
        },
        {
          "the dataset’s quality and utility for low-resource": "the first multimodal\n1. We introduce AkaCE,",
          "for Akan and advancing cross-cultural SER re-": ""
        },
        {
          "the dataset’s quality and utility for low-resource": "emotion dialogue dataset for an African lan-",
          "for Akan and advancing cross-cultural SER re-": "Conversational\nemotion recognition (ERC) has"
        },
        {
          "the dataset’s quality and utility for low-resource": "guage,\nenabling cross-cultural\nemotion re-",
          "for Akan and advancing cross-cultural SER re-": "evolved through various approaches addressing"
        },
        {
          "the dataset’s quality and utility for low-resource": "search1.",
          "for Akan and advancing cross-cultural SER re-": "contextual modeling, multimodal integration, and"
        },
        {
          "the dataset’s quality and utility for low-resource": "2. We validate AkaCE through experiments with",
          "for Akan and advancing cross-cultural SER re-": "speaker dependencies. Early works used hierarchi-"
        },
        {
          "the dataset’s quality and utility for low-resource": "state-of-the-art ERC methods, establishing a",
          "for Akan and advancing cross-cultural SER re-": "cal LSTMs (Poria et al., 2017) and Conversational"
        },
        {
          "the dataset’s quality and utility for low-resource": "strong baseline and detailed analysis.",
          "for Akan and advancing cross-cultural SER re-": "Memory Networks (CMN) (Hazarika et al., 2018b)"
        },
        {
          "the dataset’s quality and utility for low-resource": "3. We provide word-level prosodic prominence",
          "for Akan and advancing cross-cultural SER re-": "to capture context and inter-speaker influences, im-"
        },
        {
          "the dataset’s quality and utility for low-resource": "annotations, making AkaCE the first prosod-",
          "for Akan and advancing cross-cultural SER re-": "proving sentiment classification but struggling with"
        },
        {
          "the dataset’s quality and utility for low-resource": "ically annotated dataset\nfor an African lan-",
          "for Akan and advancing cross-cultural SER re-": "generalization and sparse contexts."
        },
        {
          "the dataset’s quality and utility for low-resource": "guage, facilitating research on prosody’s role",
          "for Akan and advancing cross-cultural SER re-": "DialogueRNN (Majumder et al., 2019) and Hi-"
        },
        {
          "the dataset’s quality and utility for low-resource": "in ERC and tonal language processing.",
          "for Akan and advancing cross-cultural SER re-": "GRU (Jiao et al., 2019)\nrefined speaker-specific"
        },
        {
          "the dataset’s quality and utility for low-resource": "",
          "for Akan and advancing cross-cultural SER re-": "emotion tracking and attention-based modeling"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: compares AkaCE with other discussed",
      "data": [
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "emotion dialogue dataset for Akan, a major West",
          "Multi-modal datasets combine text, audio, or": "video, such as CMU-MOSEI (Zadeh et al., 2018),"
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "African language spoken by about 20 million peo-",
          "Multi-modal datasets combine text, audio, or": "AFEW (Dhall et al., 2012), MEC (Li et al., 2018),"
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "ple (Peterson et al., n.d.). Akan is the most widely",
          "Multi-modal datasets combine text, audio, or": "and CH-SIMS (Yu et al., 2020). While offering"
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "spoken language in Ghana, with around 80% of the",
          "Multi-modal datasets combine text, audio, or": "richer features, they are not conversational and miss"
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "population using it as a first or second language,",
          "Multi-modal datasets combine text, audio, or": "the dynamic, context-dependent expressions seen"
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "and approximately 44% identifying as native Akan",
          "Multi-modal datasets combine text, audio, or": "in natural dialogues."
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "speakers. It is also natively spoken in parts of Ivory",
          "Multi-modal datasets combine text, audio, or": "Conversational multi-modal datasets integrate"
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "Coast and Togo. The language primarily comprises",
          "Multi-modal datasets combine text, audio, or": "text, audio, and video with conversational context,"
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "three main dialects: Asante, Akuapem, and Fante.",
          "Multi-modal datasets combine text, audio, or": "such as\nIEMOCAP (Busso et al., 2008), MSP-"
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "AkaCE contains 385 emotion-labeled dialogues",
          "Multi-modal datasets combine text, audio, or": "IMPROV (Busso et al., 2016), MELD (Poria et al.,"
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "from 21 Akan movies, covering diverse scenes and",
          "Multi-modal datasets combine text, audio, or": "2018), and M³ED (Zhao et al., 2022a). But these"
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "topics. It includes 6,162 utterances from 308 speak-",
          "Multi-modal datasets combine text, audio, or": "datasets mainly focus on high-resource languages,"
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "ers\n(155 male, 153 female), ensuring a gender-",
          "Multi-modal datasets combine text, audio, or": "leaving gaps for low-resource languages."
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "balanced dataset.\nAs a tonal\nlanguage, Akan’s",
          "Multi-modal datasets combine text, audio, or": "Existing datasets lack resources for low-resource"
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "prosodic features are crucial\nfor emotion recog-",
          "Multi-modal datasets combine text, audio, or": "languages, such as Akan, and prosodic annotations"
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "nition,\nso AkaCE includes word-level prosodic",
          "Multi-modal datasets combine text, audio, or": "critical for tonal languages. AkaCE fills this gap by"
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "prominence annotations\nto support\nresearch on",
          "Multi-modal datasets combine text, audio, or": "offering a multi-modal resource with prosodic la-"
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "prosody in ERC. Our baseline experiments validate",
          "Multi-modal datasets combine text, audio, or": "bels and conversational data, enabling robust SER"
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "the dataset’s quality and utility for low-resource",
          "Multi-modal datasets combine text, audio, or": "for Akan and advancing cross-cultural SER re-"
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "and cross-cultural emotion recognition research.",
          "Multi-modal datasets combine text, audio, or": "search."
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "Our main contributions are:",
          "Multi-modal datasets combine text, audio, or": ""
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "",
          "Multi-modal datasets combine text, audio, or": "2.2\nRelated Methods"
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "the first multimodal\n1. We introduce AkaCE,",
          "Multi-modal datasets combine text, audio, or": ""
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "emotion dialogue dataset for an African lan-",
          "Multi-modal datasets combine text, audio, or": "Conversational\nemotion recognition (ERC) has"
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "guage,\nenabling cross-cultural\nemotion re-",
          "Multi-modal datasets combine text, audio, or": "evolved through various approaches addressing"
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "search1.",
          "Multi-modal datasets combine text, audio, or": "contextual modeling, multimodal integration, and"
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "2. We validate AkaCE through experiments with",
          "Multi-modal datasets combine text, audio, or": "speaker dependencies. Early works used hierarchi-"
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "state-of-the-art ERC methods, establishing a",
          "Multi-modal datasets combine text, audio, or": "cal LSTMs (Poria et al., 2017) and Conversational"
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "strong baseline and detailed analysis.",
          "Multi-modal datasets combine text, audio, or": "Memory Networks (CMN) (Hazarika et al., 2018b)"
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "3. We provide word-level prosodic prominence",
          "Multi-modal datasets combine text, audio, or": "to capture context and inter-speaker influences, im-"
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "annotations, making AkaCE the first prosod-",
          "Multi-modal datasets combine text, audio, or": "proving sentiment classification but struggling with"
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "ically annotated dataset\nfor an African lan-",
          "Multi-modal datasets combine text, audio, or": "generalization and sparse contexts."
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "guage, facilitating research on prosody’s role",
          "Multi-modal datasets combine text, audio, or": "DialogueRNN (Majumder et al., 2019) and Hi-"
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "in ERC and tonal language processing.",
          "Multi-modal datasets combine text, audio, or": "GRU (Jiao et al., 2019)\nrefined speaker-specific"
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "",
          "Multi-modal datasets combine text, audio, or": "emotion tracking and attention-based modeling"
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "2\nRelated work",
          "Multi-modal datasets combine text, audio, or": ""
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "",
          "Multi-modal datasets combine text, audio, or": "but faced challenges with subtle distinctions and"
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "",
          "Multi-modal datasets combine text, audio, or": "multimodal integration. Knowledge-enriched mod-"
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "2.1\nRelated Datasets",
          "Multi-modal datasets combine text, audio, or": ""
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "",
          "Multi-modal datasets combine text, audio, or": "els (Zhong et al., 2019) leveraged commonsense"
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "ERC and Speech Emotion Recognition\n(SER)",
          "Multi-modal datasets combine text, audio, or": ""
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "",
          "Multi-modal datasets combine text, audio, or": "knowledge for emotion detection but struggled with"
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "datasets are essential for developing models to un-",
          "Multi-modal datasets combine text, audio, or": ""
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "",
          "Multi-modal datasets combine text, audio, or": "closely related emotions and low-resource settings."
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "derstand and classify emotions in speech. These",
          "Multi-modal datasets combine text, audio, or": ""
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "",
          "Multi-modal datasets combine text, audio, or": "Graph-based methods such as ConGCN (Zhang"
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "datasets can be grouped into three main categories.",
          "Multi-modal datasets combine text, audio, or": ""
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "",
          "Multi-modal datasets combine text, audio, or": "et\nal., 2019)\nand DialogueGCN (Ghosal\net\nal.,"
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "Table 1 compares AkaCE with other discussed",
          "Multi-modal datasets combine text, audio, or": ""
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "",
          "Multi-modal datasets combine text, audio, or": "2019) modeled multi-speaker dependencies effec-"
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "datasets.",
          "Multi-modal datasets combine text, audio, or": ""
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "",
          "Multi-modal datasets combine text, audio, or": "tively but relied heavily on textual features. Mul-"
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "Single-modality datasets focus exclusively on a",
          "Multi-modal datasets combine text, audio, or": ""
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "",
          "Multi-modal datasets combine text, audio, or": "timodal transformers like MulT (Tsai et al., 2019)"
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "single modality, such as text, including EmoryNLP",
          "Multi-modal datasets combine text, audio, or": ""
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "",
          "Multi-modal datasets combine text, audio, or": "and MMGCN (Hu et al., 2021) advanced cross-"
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "(Zahiri and Choi, 2018), EmotionLines (Chen et al.,",
          "Multi-modal datasets combine text, audio, or": ""
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "",
          "Multi-modal datasets combine text, audio, or": "modal\nfusion but\nfaced scalability issues due to"
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "2018), and DailyDialog (Li et al., 2017). These are",
          "Multi-modal datasets combine text, audio, or": ""
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "",
          "Multi-modal datasets combine text, audio, or": "dataset alignment and computational demands."
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "useful for text-based emotion recognition but lack",
          "Multi-modal datasets combine text, audio, or": ""
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "",
          "Multi-modal datasets combine text, audio, or": "Recent transformer-based models like DialogXL"
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "the multi-modal richness needed for comprehen-",
          "Multi-modal datasets combine text, audio, or": ""
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "",
          "Multi-modal datasets combine text, audio, or": "(Shen et\nal., 2021)\nand EmoBERTa\n(Kim and"
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "sive analysis involving vocal or visual cues.",
          "Multi-modal datasets combine text, audio, or": ""
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "",
          "Multi-modal datasets combine text, audio, or": "Vossen, 2021) improved ERC with dialog-aware at-"
        },
        {
          "versation Emotion (AkaCE) dataset, a multimodal": "1Our data and code are available at this GitHub repo.",
          "Multi-modal datasets combine text, audio, or": "tention and speaker-aware features but lacked mul-"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "EmoryNLP (Zahiri and Choi, 2018)",
          "Dialogue": "Yes",
          "Modalities": "t",
          "Prosodic Annotations": "No",
          "Sources": "Friends TV",
          "Mul-label": "Yes",
          "Emos": "9",
          "Spks": "–",
          "Language": "English",
          "Utts": "12,606"
        },
        {
          "Dataset": "EmotionLines (Chen et al., 2018)",
          "Dialogue": "Yes",
          "Modalities": "t",
          "Prosodic Annotations": "No",
          "Sources": "Friends TV",
          "Mul-label": "No",
          "Emos": "7",
          "Spks": "–",
          "Language": "English",
          "Utts": "29,245"
        },
        {
          "Dataset": "DailyDialog (Li et al., 2017)",
          "Dialogue": "Yes",
          "Modalities": "t",
          "Prosodic Annotations": "No",
          "Sources": "Daily",
          "Mul-label": "No",
          "Emos": "7",
          "Spks": "–",
          "Language": "English",
          "Utts": "102,979"
        },
        {
          "Dataset": "CMU-MOSEI (Zadeh et al., 2018)",
          "Dialogue": "No",
          "Modalities": "a, v, t",
          "Prosodic Annotations": "No",
          "Sources": "YouTube",
          "Mul-label": "No",
          "Emos": "7",
          "Spks": "1000",
          "Language": "English",
          "Utts": "23,453"
        },
        {
          "Dataset": "AFEW (Dhall et al., 2012)",
          "Dialogue": "No",
          "Modalities": "a, v",
          "Prosodic Annotations": "No",
          "Sources": "Movies",
          "Mul-label": "No",
          "Emos": "7",
          "Spks": "330",
          "Language": "English",
          "Utts": "1,645"
        },
        {
          "Dataset": "MEC (Li et al., 2018)",
          "Dialogue": "No",
          "Modalities": "a, v",
          "Prosodic Annotations": "No",
          "Sources": "Movies, TVs",
          "Mul-label": "No",
          "Emos": "8",
          "Spks": "–",
          "Language": "Mandarin",
          "Utts": "7,030"
        },
        {
          "Dataset": "CH-SIMS (Yu et al., 2020)",
          "Dialogue": "No",
          "Modalities": "a, v, t",
          "Prosodic Annotations": "No",
          "Sources": "Movies, TVs",
          "Mul-label": "No",
          "Emos": "5",
          "Spks": "474",
          "Language": "Mandarin",
          "Utts": "2,281"
        },
        {
          "Dataset": "IEMOCAP (Busso et al., 2008)",
          "Dialogue": "Yes",
          "Modalities": "a, v, t",
          "Prosodic Annotations": "No",
          "Sources": "Act",
          "Mul-label": "No",
          "Emos": "5",
          "Spks": "10",
          "Language": "English",
          "Utts": "7,433"
        },
        {
          "Dataset": "MSP-IMPROV (Busso et al., 2016)",
          "Dialogue": "Yes",
          "Modalities": "a, v, t",
          "Prosodic Annotations": "No",
          "Sources": "Act",
          "Mul-label": "No",
          "Emos": "5",
          "Spks": "12",
          "Language": "English",
          "Utts": "8,438"
        },
        {
          "Dataset": "MELD (Poria et al., 2018)",
          "Dialogue": "Yes",
          "Modalities": "a, v, t",
          "Prosodic Annotations": "No",
          "Sources": "Friends TV",
          "Mul-label": "No",
          "Emos": "7",
          "Spks": "407",
          "Language": "English",
          "Utts": "13,708"
        },
        {
          "Dataset": "M³ED (Zhao et al., 2022a)",
          "Dialogue": "Yes",
          "Modalities": "a, v, t",
          "Prosodic Annotations": "No",
          "Sources": "56 TVs",
          "Mul-label": "Yes",
          "Emos": "7",
          "Spks": "626",
          "Language": "Mandarin",
          "Utts": "24,449"
        },
        {
          "Dataset": "AkaCE (Ours)",
          "Dialogue": "Yes",
          "Modalities": "a, v, t",
          "Prosodic Annotations": "Yes",
          "Sources": "21 Movies",
          "Mul-label": "No",
          "Emos": "7",
          "Spks": "308",
          "Language": "Akan",
          "Utts": "6,162"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "a, v, t\nMELD (Poria et al., 2018)\nYes\nNo",
          "Act\nNo\n5\n12\nEnglish\n8,438": "Friends TV\nNo\n7\n407\nEnglish\n13,708"
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "a, v, t\nM³ED (Zhao et al., 2022a)\nYes\nNo",
          "Act\nNo\n5\n12\nEnglish\n8,438": "56 TVs\nYes\n7\n626\nMandarin\n24,449"
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "a, v, t\nAkaCE (Ours)\nYes\nYes",
          "Act\nNo\n5\n12\nEnglish\n8,438": "21 Movies\nNo\n7\n308\nAkan\n6,162"
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "Table 1: Comparison of existing benchmark datasets. a, v, t refer to audio, visual, and text modalities respectively.",
          "Act\nNo\n5\n12\nEnglish\n8,438": ""
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "timodal capabilities. M2FNet (Chudasama et al.,",
          "Act\nNo\n5\n12\nEnglish\n8,438": "stitute of linguistics and bible translation in Ghana."
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "2022) addressed multimodal fusion, effectively in-",
          "Act\nNo\n5\n12\nEnglish\n8,438": "The\nannotators\nconsisted of five men and two"
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "tegrating text, audio, and visual data,\nthough it",
          "Act\nNo\n5\n12\nEnglish\n8,438": "women, all native Akan speakers. Of these seven"
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "struggled with imbalanced datasets. Recent meth-",
          "Act\nNo\n5\n12\nEnglish\n8,438": "annotators, three were employed to work full-time"
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "ods leverage LLMs to enhance performance, refor-",
          "Act\nNo\n5\n12\nEnglish\n8,438": "while the rest worked part-time. One of the full-"
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "mulating emotion recognition as a generative task",
          "Act\nNo\n5\n12\nEnglish\n8,438": "time annotators opted to annotate seven movies,"
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "(Lei et al., 2023), incorporating acoustic features",
          "Act\nNo\n5\n12\nEnglish\n8,438": "whereas the other\ntwo full-time annotators each"
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "(Wu et al., 2024a) and contextual information (Xue",
          "Act\nNo\n5\n12\nEnglish\n8,438": "chose to annotate five movies. The remaining four"
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "et al., 2024; Fu, 2024; Zhang et al., 2023).",
          "Act\nNo\n5\n12\nEnglish\n8,438": "part-time annotators annotated one movie each."
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "Despite these advancements, existing methods",
          "Act\nNo\n5\n12\nEnglish\n8,438": "The movies were randomly assigned to their\nre-"
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "often lack robust solutions for underrepresented",
          "Act\nNo\n5\n12\nEnglish\n8,438": "spective annotators."
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "languages and datasets. Our work bridges these",
          "Act\nNo\n5\n12\nEnglish\n8,438": "The data annotators recorded the desired data"
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "gaps by introducing a multimodal dataset and a",
          "Act\nNo\n5\n12\nEnglish\n8,438": "by watching the movies and simultaneously record-"
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "focus on low-resource settings, enabling more com-",
          "Act\nNo\n5\n12\nEnglish\n8,438": "ing the necessary information into Microsoft Excel"
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "prehensive and inclusive ERC research.",
          "Act\nNo\n5\n12\nEnglish\n8,438": "sheets. All resulting sheets were then collated into"
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "",
          "Act\nNo\n5\n12\nEnglish\n8,438": "one Excel sheet, where all redundant entries were"
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "3\nAkaCE Dataset",
          "Act\nNo\n5\n12\nEnglish\n8,438": ""
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "",
          "Act\nNo\n5\n12\nEnglish\n8,438": "eliminated and annotation errors were corrected."
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "We construct the AkaCE dataset by collecting and",
          "Act\nNo\n5\n12\nEnglish\n8,438": ""
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "",
          "Act\nNo\n5\n12\nEnglish\n8,438": "3.3\nText and Speaker Annotation"
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "annotating dialogues from Akan-language movies,",
          "Act\nNo\n5\n12\nEnglish\n8,438": ""
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "with examples illustrated in Figure 1. The dataset",
          "Act\nNo\n5\n12\nEnglish\n8,438": "Even though there have been recent advances in"
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "includes transcriptions with speaker identifications,",
          "Act\nNo\n5\n12\nEnglish\n8,438": "Akan Automatic Speech Recognition (ASR), most"
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "emotion labels, and word-level prosodic promi-",
          "Act\nNo\n5\n12\nEnglish\n8,438": "modern Akan ASR systems still generate many"
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "nence annotations. Table 1 compares AkaCE with",
          "Act\nNo\n5\n12\nEnglish\n8,438": "recognition errors as a result of the dearth of train-"
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "other discussed datasets.",
          "Act\nNo\n5\n12\nEnglish\n8,438": "ing data available (Dossou, 2024). As a result of"
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "",
          "Act\nNo\n5\n12\nEnglish\n8,438": "this, all the speech utterances for each movie were"
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "3.1\nData Selection",
          "Act\nNo\n5\n12\nEnglish\n8,438": ""
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "",
          "Act\nNo\n5\n12\nEnglish\n8,438": "manually transcribed by the annotators before any"
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "The dataset consists of 21 Akan movies that were",
          "Act\nNo\n5\n12\nEnglish\n8,438": "emotion labelling was performed."
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "downloaded from the Internet Archive. To ensure",
          "Act\nNo\n5\n12\nEnglish\n8,438": "Additionally, due to the lack of acoustic models"
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "that the movies included within this dataset were",
          "Act\nNo\n5\n12\nEnglish\n8,438": "for Akan that could facilitate audio alignment and"
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "of high quality we ensured that each of the movies",
          "Act\nNo\n5\n12\nEnglish\n8,438": "automatically generate timestamps for each utter-"
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "selected to be a part of\nthe dataset\nfulfilled the",
          "Act\nNo\n5\n12\nEnglish\n8,438": "ance in a movie, annotators manually tracked and"
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "following criteria: (1) the movie must be complete",
          "Act\nNo\n5\n12\nEnglish\n8,438": "recorded the timestamps for all utterances."
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "and not truncated in any section, (2) the speech of",
          "Act\nNo\n5\n12\nEnglish\n8,438": "For\nthe\nspeaker\nannotations,\nthe\nspeaker\nfor"
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "the actors within the movie should be intelligible,",
          "Act\nNo\n5\n12\nEnglish\n8,438": "each utterance was identified by a unique identi-"
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "(3) the facial expressions of the actors within movie",
          "Act\nNo\n5\n12\nEnglish\n8,438": "fier which consists of a combination of the order"
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "should be clear.",
          "Act\nNo\n5\n12\nEnglish\n8,438": "in which the speaker first appeared in the current"
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "",
          "Act\nNo\n5\n12\nEnglish\n8,438": "dialogue and their gender. For instance, a possi-"
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "3.2\nAnnotators and Annotation Process",
          "Act\nNo\n5\n12\nEnglish\n8,438": ""
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "",
          "Act\nNo\n5\n12\nEnglish\n8,438": "ble label that would be assigned to a man who is"
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "The annotation task was carried out by Akan data",
          "Act\nNo\n5\n12\nEnglish\n8,438": "the first speaker in the current dialogue of a scene"
        },
        {
          "a, v, t\nMSP-IMPROV (Busso et al., 2016)\nYes\nNo": "annotation professionals contracted through an in-",
          "Act\nNo\n5\n12\nEnglish\n8,438": "one\nwithin a movie is ‘speaker\nman’."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "General Statistics\nValues": ""
        },
        {
          "General Statistics\nValues": "Total number of seconds\n87441"
        },
        {
          "General Statistics\nValues": ""
        },
        {
          "General Statistics\nValues": "Avg. number of seconds per movie\n4163.4"
        },
        {
          "General Statistics\nValues": ""
        },
        {
          "General Statistics\nValues": "Total number of movies\n21"
        },
        {
          "General Statistics\nValues": ""
        },
        {
          "General Statistics\nValues": "Total number of dialogs\n385"
        },
        {
          "General Statistics\nValues": ""
        },
        {
          "General Statistics\nValues": "Total number of words\n117305"
        },
        {
          "General Statistics\nValues": "Total number of utterances\n6162"
        },
        {
          "General Statistics\nValues": ""
        },
        {
          "General Statistics\nValues": "Total number of turns\n4477"
        },
        {
          "General Statistics\nValues": ""
        },
        {
          "General Statistics\nValues": "Number of prominence words\n37314"
        },
        {
          "General Statistics\nValues": ""
        },
        {
          "General Statistics\nValues": "Number of non-prominence words\n79991"
        },
        {
          "General Statistics\nValues": "Average number of turns per dialog\n11.62"
        },
        {
          "General Statistics\nValues": "Average number of utterances per dialog\n16"
        },
        {
          "General Statistics\nValues": "Average number of words per dialog\n305"
        },
        {
          "General Statistics\nValues": "Average utterance length in seconds\n6.701"
        },
        {
          "General Statistics\nValues": ""
        },
        {
          "General Statistics\nValues": "Average number of words per utterance\n19"
        },
        {
          "General Statistics\nValues": ""
        },
        {
          "General Statistics\nValues": "Average duration per dialog in seconds\n227.1"
        },
        {
          "General Statistics\nValues": ""
        },
        {
          "General Statistics\nValues": ""
        },
        {
          "General Statistics\nValues": "Table 2: General statistics of the AkaCE Dataset"
        },
        {
          "General Statistics\nValues": ""
        },
        {
          "General Statistics\nValues": ""
        },
        {
          "General Statistics\nValues": "Akan-speaking consultant,\nrecognized as an ex-"
        },
        {
          "General Statistics\nValues": "pert in Akan Emotion Analysis. Notably, an analy-"
        },
        {
          "General Statistics\nValues": "sis of inter-annotator agreement yielded an overall"
        },
        {
          "General Statistics\nValues": "Fleiss’ Kappa statistic (Fleiss, 1971) of k = 0.488"
        },
        {
          "General Statistics\nValues": "which is comparable to the inter-annotator agree-"
        },
        {
          "General Statistics\nValues": "ment scores of several other popular high-quality"
        },
        {
          "General Statistics\nValues": "speech emotion datasets\nsuch as MELD (Poria"
        },
        {
          "General Statistics\nValues": "et al., 2018) which has a score of 0.43 (Poria et al.,"
        },
        {
          "General Statistics\nValues": "2018), IEMOCAP which has a score of 0.48 (Busso"
        },
        {
          "General Statistics\nValues": "et al., 2008), MSP-IMPROV which has a score of"
        },
        {
          "General Statistics\nValues": "0.49 (Busso et al., 2016) and M³ED which has a"
        },
        {
          "General Statistics\nValues": "score of 0.59 (Zhao et al., 2022b)."
        },
        {
          "General Statistics\nValues": ""
        },
        {
          "General Statistics\nValues": "3.6\nProsodic Prominence Annotation"
        },
        {
          "General Statistics\nValues": ""
        },
        {
          "General Statistics\nValues": "The annotation strategy used for prosodic promi-"
        },
        {
          "General Statistics\nValues": "nence closely mirrored the approach employed for"
        },
        {
          "General Statistics\nValues": "emotion labelling.\nThe same two annotators re-"
        },
        {
          "General Statistics\nValues": "sponsible for assigning emotion labels to the ut-"
        },
        {
          "General Statistics\nValues": "terances were selected for this task. Before start-"
        },
        {
          "General Statistics\nValues": "ing the prosodic prominence annotations, they re-"
        },
        {
          "General Statistics\nValues": "ceived detailed instructions outlining the concept of"
        },
        {
          "General Statistics\nValues": "prosodic prominence and the steps involved in per-"
        },
        {
          "General Statistics\nValues": ""
        },
        {
          "General Statistics\nValues": "forming the task. Additionally, they were presented"
        },
        {
          "General Statistics\nValues": "with examples of prosodic prominence annotations"
        },
        {
          "General Statistics\nValues": "deemed accurate by consulted linguists to ensure a"
        },
        {
          "General Statistics\nValues": "clear understanding of the expectations."
        },
        {
          "General Statistics\nValues": "For the annotation task, the annotators were in-"
        },
        {
          "General Statistics\nValues": "structed to listen to the audio of each utterance in"
        },
        {
          "General Statistics\nValues": "the dataset and assign a value of 1 to words they"
        },
        {
          "General Statistics\nValues": "deemed prosodically prominent and 0 to words they"
        },
        {
          "General Statistics\nValues": "considered non-prominent. All annotations were"
        },
        {
          "General Statistics\nValues": "conducted using Excel sheets. This approach to"
        },
        {
          "General Statistics\nValues": "prosodic prominence annotation was inspired by a"
        },
        {
          "General Statistics\nValues": "similar approach leveraged in Cole et al. (2017)."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotion Labels\nValues": ""
        },
        {
          "Emotion Labels\nValues": "Neutral\n2941"
        },
        {
          "Emotion Labels\nValues": "Sadness\n806"
        },
        {
          "Emotion Labels\nValues": "Anger\n1107"
        },
        {
          "Emotion Labels\nValues": "Fear\n134"
        },
        {
          "Emotion Labels\nValues": "Surprise\n364"
        },
        {
          "Emotion Labels\nValues": "Disgust\n162"
        },
        {
          "Emotion Labels\nValues": "Happy\n568"
        },
        {
          "Emotion Labels\nValues": ""
        },
        {
          "Emotion Labels\nValues": ""
        },
        {
          "Emotion Labels\nValues": "Table 3: Distribution of emotions in the AkaCE dataset"
        },
        {
          "Emotion Labels\nValues": ""
        },
        {
          "Emotion Labels\nValues": ""
        },
        {
          "Emotion Labels\nValues": "Speaker statistics\nValues"
        },
        {
          "Emotion Labels\nValues": "Number of speakers\n308"
        },
        {
          "Emotion Labels\nValues": ""
        },
        {
          "Emotion Labels\nValues": "Number of male speakers\n155"
        },
        {
          "Emotion Labels\nValues": "Number of female speakers\n153"
        },
        {
          "Emotion Labels\nValues": ""
        },
        {
          "Emotion Labels\nValues": ""
        },
        {
          "Emotion Labels\nValues": "Table 4: Distribution of speakers in the AkaCE Dataset"
        },
        {
          "Emotion Labels\nValues": ""
        },
        {
          "Emotion Labels\nValues": ""
        },
        {
          "Emotion Labels\nValues": "An analysis of the inter-annotator agreement for"
        },
        {
          "Emotion Labels\nValues": "prosodic annotation between the two annotators"
        },
        {
          "Emotion Labels\nValues": "yielded an overall Fleiss’ Kappa statistic (Fleiss,"
        },
        {
          "Emotion Labels\nValues": "1971) of k = 1.01.01.0. The observed perfect agreement"
        },
        {
          "Emotion Labels\nValues": "may be attributed to the tonal and expressive na-"
        },
        {
          "Emotion Labels\nValues": "ture of Akan, which provides listeners with clear"
        },
        {
          "Emotion Labels\nValues": "prosodic cues for identifying prominent words. In"
        },
        {
          "Emotion Labels\nValues": "particular, prior work by Kügler and Genzel (2012)"
        },
        {
          "Emotion Labels\nValues": "shows that Akan speakers signal pragmatic promi-"
        },
        {
          "Emotion Labels\nValues": "nence through consistent acoustic patterns such as"
        },
        {
          "Emotion Labels\nValues": "pitch lowering, even on high-toned syllables. This"
        },
        {
          "Emotion Labels\nValues": "systematic use of prosody to express information"
        },
        {
          "Emotion Labels\nValues": "structure likely contributes to the high salience of"
        },
        {
          "Emotion Labels\nValues": "prominent words in speech, facilitating consistent"
        },
        {
          "Emotion Labels\nValues": "annotation across raters."
        },
        {
          "Emotion Labels\nValues": ""
        },
        {
          "Emotion Labels\nValues": "3.7\nDataset Statistics"
        },
        {
          "Emotion Labels\nValues": ""
        },
        {
          "Emotion Labels\nValues": "General Dataset Statistics\nTable 2 presents ba-"
        },
        {
          "Emotion Labels\nValues": ""
        },
        {
          "Emotion Labels\nValues": "sic\nstatistics of\nthe Akan Cinematic Emotions"
        },
        {
          "Emotion Labels\nValues": ""
        },
        {
          "Emotion Labels\nValues": "(AkaCE) Dataset. It contains 385 dialogues, 4477"
        },
        {
          "Emotion Labels\nValues": "turns and 6162 utterances, which contain an aver-"
        },
        {
          "Emotion Labels\nValues": ""
        },
        {
          "Emotion Labels\nValues": "age of 19 words. With respect to prosodic promi-"
        },
        {
          "Emotion Labels\nValues": ""
        },
        {
          "Emotion Labels\nValues": "nence, 37314 words were annotated to be prosodi-"
        },
        {
          "Emotion Labels\nValues": ""
        },
        {
          "Emotion Labels\nValues": "cally prominent whereas 79991 words were anno-"
        },
        {
          "Emotion Labels\nValues": ""
        },
        {
          "Emotion Labels\nValues": "tated to be non-prominent."
        },
        {
          "Emotion Labels\nValues": ""
        },
        {
          "Emotion Labels\nValues": "Emotion Distribution\nTable 3 illustrates the dis-"
        },
        {
          "Emotion Labels\nValues": "tribution of emotions in the AkaCE Dataset. Neu-"
        },
        {
          "Emotion Labels\nValues": "tral emotion had the highest frequency, appearing"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 5: , our results indicate",
      "data": [
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "openSMILE\n13.80\n6.58"
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "Spectrogram\n47.89\n23.36"
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "52.38\n29.51\nWhisper-small"
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "Table 6: Audio-based emotion detection results."
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "or padded to a maximum length of 1024 frames."
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "These features are then used to fine-tune a pre-"
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "trained Audio Spectrogram Transformer\n(AST)"
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "(Gong et al., 2021) with an additional linear classi-"
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "fier layer. The model achieves a weighted F1 score"
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "of 47.89 and a macro F1 score of 23.36. We select"
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "AST due to its pretraining on a diverse auditory"
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "data, encompassing both human speech and non-"
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "human sounds, such as music and environmental"
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "noises. This broad training enables AST to effec-"
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "tively capture complex acoustic patterns, making"
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "it particularly well-suited for our AkaCE dataset,"
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "which consists of movie scenes containing a mix of"
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "dialogue, background music, and ambient sounds."
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "Finally, we fine-tune a Whisper-Small encoder"
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "without freezing its parameters, achieving the best"
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "performance with a weighted F1 score of 52.38"
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "and a macro F1 score of 29.51. These results in-"
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "dicate that pretraining audio models on multiple"
        },
        {
          "Model\nWeighted F1\nMacro F1": "languages benefits speech emotion recognition in"
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "low-resource target\nlanguages. However, due to"
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "the imbalance in training samples for Whisper, the"
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "improvement remains relatively small. This further"
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "underscores the necessity of our dataset collection,"
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "as it represents the first multimodal emotion dia-"
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "logue dataset for an African language, addressing"
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "the significant resource gap in emotion recognition"
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "research for low-resource languages."
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "4.4\nVision Experiments"
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "For the vision modality, we explore two main ap-"
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "proaches for encoding visual\ninformation.\nFirst,"
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "we use ResNet18 and ResNet50 (He et al., 2015)"
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "to extract feature representations from entire video"
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "frames. To evaluate the impact of temporal reso-"
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "lution on emotion detection, we experiment with"
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "frame sampling rates of 1 frame per\nsecond (1"
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "fps) and 5 frames per second (5 fps).\nIn addition,"
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "we investigate a face-based approach where faces"
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "are extracted from each frame using MTCNN and"
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "then encoded with InceptionResnetV1, a model"
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "pre-trained on VGGFace2 (Cao et al., 2018). All"
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "vision experiments are conducted using a learning"
        },
        {
          "Model\nWeighted F1\nMacro F1": "rate of 1e-4 and a batch size of 16."
        },
        {
          "Model\nWeighted F1\nMacro F1": ""
        },
        {
          "Model\nWeighted F1\nMacro F1": "As shown in Table 7, our results indicate that"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 8: show that combining",
      "data": [
        {
          "Model": "ResNet18-1fps",
          "Weighted F1": "44.58",
          "Macro F1": "22.29",
          "Modality": "Text"
        },
        {
          "Model": "ResNet50-1fps",
          "Weighted F1": "52.38",
          "Macro F1": "29.51",
          "Modality": "Audio"
        },
        {
          "Model": "ResNet18-5fps",
          "Weighted F1": "40.57",
          "Macro F1": "20.02",
          "Modality": "Vision"
        },
        {
          "Model": "ResNet50-5fps",
          "Weighted F1": "55.51",
          "Macro F1": "30.15",
          "Modality": "Text + Audio"
        },
        {
          "Model": "Inception-Face-5fps",
          "Weighted F1": "43.33",
          "Macro F1": "21.15",
          "Modality": "Text + Vision"
        },
        {
          "Model": "",
          "Weighted F1": "53.84",
          "Macro F1": "30.42",
          "Modality": "Audio + Vision"
        },
        {
          "Model": "",
          "Weighted F1": "55.81",
          "Macro F1": "30.97",
          "Modality": "Text + Audio + Vision"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 8: show that combining",
      "data": [
        {
          "ResNet50-5fps\n41.76\n19": "Inception-Face-5fps\n39.96\n16.53",
          "Text + Audio\n55.51\n30.15": "Text + Vision\n43.33\n21.15"
        },
        {
          "ResNet50-5fps\n41.76\n19": "",
          "Text + Audio\n55.51\n30.15": "Audio + Vision\n53.84\n30.42"
        },
        {
          "ResNet50-5fps\n41.76\n19": "Table 7: Vision-based emotion detection results.",
          "Text + Audio\n55.51\n30.15": "55.81\n30.97\nText + Audio + Vision"
        },
        {
          "ResNet50-5fps\n41.76\n19": "",
          "Text + Audio\n55.51\n30.15": "Table 8: Results of modality concatenation experiments"
        },
        {
          "ResNet50-5fps\n41.76\n19": "ResNet18 with a 5 fps sampling rate achieves the",
          "Text + Audio\n55.51\n30.15": ""
        },
        {
          "ResNet50-5fps\n41.76\n19": "",
          "Text + Audio\n55.51\n30.15": "using the best unimodal models."
        },
        {
          "ResNet50-5fps\n41.76\n19": "highest weighted F1 score (42.04), suggesting that",
          "Text + Audio\n55.51\n30.15": ""
        },
        {
          "ResNet50-5fps\n41.76\n19": "increasing temporal resolution enhances emotion",
          "Text + Audio\n55.51\n30.15": ""
        },
        {
          "ResNet50-5fps\n41.76\n19": "recognition. However, the highest macro F1 score",
          "Text + Audio\n55.51\n30.15": "ResNet18-5fps for vision. Feature representations"
        },
        {
          "ResNet50-5fps\n41.76\n19": "(20.02) is observed with ResNet18 at 1 fps,\nindi-",
          "Text + Audio\n55.51\n30.15": "from each modality are concatenated and passed"
        },
        {
          "ResNet50-5fps\n41.76\n19": "cating that this setting may better capture underrep-",
          "Text + Audio\n55.51\n30.15": "through a classifier layer to compute logits for emo-"
        },
        {
          "ResNet50-5fps\n41.76\n19": "resented emotion classes. Interestingly, ResNet50,",
          "Text + Audio\n55.51\n30.15": "tion prediction. To consider distinct characteristics"
        },
        {
          "ResNet50-5fps\n41.76\n19": "despite being a larger model, does not consistently",
          "Text + Audio\n55.51\n30.15": "of each modality, we experiment with different"
        },
        {
          "ResNet50-5fps\n41.76\n19": "outperform ResNet18, possibly due to overfitting.",
          "Text + Audio\n55.51\n30.15": "learning rates but find that using a single learning"
        },
        {
          "ResNet50-5fps\n41.76\n19": "Its best weighted F1 score (41.76 at 5 fps) slightly",
          "Text + Audio\n55.51\n30.15": "rate of 1e-5 yields the most stable results."
        },
        {
          "ResNet50-5fps\n41.76\n19": "trails that of ResNet18-5fps.",
          "Text + Audio\n55.51\n30.15": ""
        },
        {
          "ResNet50-5fps\n41.76\n19": "",
          "Text + Audio\n55.51\n30.15": "Our\nresults\nin Table 8 show that\ncombining"
        },
        {
          "ResNet50-5fps\n41.76\n19": "The face-based approach using InceptionRes-",
          "Text + Audio\n55.51\n30.15": ""
        },
        {
          "ResNet50-5fps\n41.76\n19": "",
          "Text + Audio\n55.51\n30.15": "modalities improves emotion recognition perfor-"
        },
        {
          "ResNet50-5fps\n41.76\n19": "NetV1 underperforms compared to whole-frame",
          "Text + Audio\n55.51\n30.15": ""
        },
        {
          "ResNet50-5fps\n41.76\n19": "",
          "Text + Audio\n55.51\n30.15": "mance, with the best results achieved when inte-"
        },
        {
          "ResNet50-5fps\n41.76\n19": "models, achieving only 39.96 weighted F1 and",
          "Text + Audio\n55.51\n30.15": ""
        },
        {
          "ResNet50-5fps\n41.76\n19": "",
          "Text + Audio\n55.51\n30.15": "grating all three modalities. The multimodal model"
        },
        {
          "ResNet50-5fps\n41.76\n19": "16.53 macro F1, suggesting that facial expressions",
          "Text + Audio\n55.51\n30.15": ""
        },
        {
          "ResNet50-5fps\n41.76\n19": "",
          "Text + Audio\n55.51\n30.15": "using text, audio, and vision achieves the highest"
        },
        {
          "ResNet50-5fps\n41.76\n19": "alone may not provide sufficient\ninformation for",
          "Text + Audio\n55.51\n30.15": ""
        },
        {
          "ResNet50-5fps\n41.76\n19": "",
          "Text + Audio\n55.51\n30.15": "weighted F1 (55.81) and macro F1 (30.97), outper-"
        },
        {
          "ResNet50-5fps\n41.76\n19": "robust emotion detection in our dataset. Unlike",
          "Text + Audio\n55.51\n30.15": ""
        },
        {
          "ResNet50-5fps\n41.76\n19": "",
          "Text + Audio\n55.51\n30.15": "forming all unimodal and bimodal models."
        },
        {
          "ResNet50-5fps\n41.76\n19": "datasets such as CMU-MOSEI (Zadeh et al., 2018)",
          "Text + Audio\n55.51\n30.15": ""
        },
        {
          "ResNet50-5fps\n41.76\n19": "",
          "Text + Audio\n55.51\n30.15": "Among the unimodal models, audio performs"
        },
        {
          "ResNet50-5fps\n41.76\n19": "that enforce a single visible face in close-up shots,",
          "Text + Audio\n55.51\n30.15": ""
        },
        {
          "ResNet50-5fps\n41.76\n19": "",
          "Text + Audio\n55.51\n30.15": "best (52.38 weighted F1, 29.51 macro F1),\nindi-"
        },
        {
          "ResNet50-5fps\n41.76\n19": "our dataset does not impose such constraints. As",
          "Text + Audio\n55.51\n30.15": ""
        },
        {
          "ResNet50-5fps\n41.76\n19": "",
          "Text + Audio\n55.51\n30.15": "cating that speech features carry the most discrimi-"
        },
        {
          "ResNet50-5fps\n41.76\n19": "a result, videos may contain multiple faces, and",
          "Text + Audio\n55.51\n30.15": ""
        },
        {
          "ResNet50-5fps\n41.76\n19": "",
          "Text + Audio\n55.51\n30.15": "native information for emotion recognition in our"
        },
        {
          "ResNet50-5fps\n41.76\n19": "the primary speaker’s face may be distant\nfrom",
          "Text + Audio\n55.51\n30.15": ""
        },
        {
          "ResNet50-5fps\n41.76\n19": "",
          "Text + Audio\n55.51\n30.15": "dataset.\nInterestingly, text alone (44.58 weighted"
        },
        {
          "ResNet50-5fps\n41.76\n19": "the camera, adding challenges for models relying",
          "Text + Audio\n55.51\n30.15": ""
        },
        {
          "ResNet50-5fps\n41.76\n19": "",
          "Text + Audio\n55.51\n30.15": "F1, 22.29 macro F1) underperforms compared to"
        },
        {
          "ResNet50-5fps\n41.76\n19": "solely on facial features. These findings highlight",
          "Text + Audio\n55.51\n30.15": ""
        },
        {
          "ResNet50-5fps\n41.76\n19": "",
          "Text + Audio\n55.51\n30.15": "audio, contrasting with trends in high-resource lan-"
        },
        {
          "ResNet50-5fps\n41.76\n19": "the importance of frame selection strategies and",
          "Text + Audio\n55.51\n30.15": ""
        },
        {
          "ResNet50-5fps\n41.76\n19": "",
          "Text + Audio\n55.51\n30.15": "guages where text embeddings often yield the best"
        },
        {
          "ResNet50-5fps\n41.76\n19": "suggest\nthat balancing temporal\nresolution with",
          "Text + Audio\n55.51\n30.15": ""
        },
        {
          "ResNet50-5fps\n41.76\n19": "",
          "Text + Audio\n55.51\n30.15": "results (Zadeh et al., 2018; Yu et al., 2020). This"
        },
        {
          "ResNet50-5fps\n41.76\n19": "model capacity is crucial for optimal vision-based",
          "Text + Audio\n55.51\n30.15": ""
        },
        {
          "ResNet50-5fps\n41.76\n19": "",
          "Text + Audio\n55.51\n30.15": "gap is likely due to the limited availability of large"
        },
        {
          "ResNet50-5fps\n41.76\n19": "emotion recognition.",
          "Text + Audio\n55.51\n30.15": ""
        },
        {
          "ResNet50-5fps\n41.76\n19": "",
          "Text + Audio\n55.51\n30.15": "and diverse pretraining corpora for Akan, restrict-"
        },
        {
          "ResNet50-5fps\n41.76\n19": "",
          "Text + Audio\n55.51\n30.15": "ing the effectiveness of text embeddings. Vision"
        },
        {
          "ResNet50-5fps\n41.76\n19": "4.5\nMultimodal Experiments",
          "Text + Audio\n55.51\n30.15": ""
        },
        {
          "ResNet50-5fps\n41.76\n19": "",
          "Text + Audio\n55.51\n30.15": "alone performs worst (40.57 weighted F1), suggest-"
        },
        {
          "ResNet50-5fps\n41.76\n19": "We evaluate modality combinations using the best-",
          "Text + Audio\n55.51\n30.15": ""
        },
        {
          "ResNet50-5fps\n41.76\n19": "",
          "Text + Audio\n55.51\n30.15": "ing that visual cues are less reliable, possibly due"
        },
        {
          "ResNet50-5fps\n41.76\n19": "performing unimodal models. Starting with simple",
          "Text + Audio\n55.51\n30.15": ""
        },
        {
          "ResNet50-5fps\n41.76\n19": "",
          "Text + Audio\n55.51\n30.15": "to variations in facial visibility and camera angles."
        },
        {
          "ResNet50-5fps\n41.76\n19": "feature concatenation as a baseline, we then apply",
          "Text + Audio\n55.51\n30.15": ""
        },
        {
          "ResNet50-5fps\n41.76\n19": "",
          "Text + Audio\n55.51\n30.15": "In bimodal settings, text + audio (55.51 weighted"
        },
        {
          "ResNet50-5fps\n41.76\n19": "transformer-based fusion to enhance cross-modal",
          "Text + Audio\n55.51\n30.15": ""
        },
        {
          "ResNet50-5fps\n41.76\n19": "",
          "Text + Audio\n55.51\n30.15": "F1) and audio + vision (53.84 weighted F1) show"
        },
        {
          "ResNet50-5fps\n41.76\n19": "interactions. These experiments assess the impact",
          "Text + Audio\n55.51\n30.15": ""
        },
        {
          "ResNet50-5fps\n41.76\n19": "",
          "Text + Audio\n55.51\n30.15": "substantial improvements over their unimodal coun-"
        },
        {
          "ResNet50-5fps\n41.76\n19": "of multimodal integration on emotion recognition.",
          "Text + Audio\n55.51\n30.15": ""
        },
        {
          "ResNet50-5fps\n41.76\n19": "",
          "Text + Audio\n55.51\n30.15": "terparts, reinforcing the importance of speech infor-"
        },
        {
          "ResNet50-5fps\n41.76\n19": "4.5.1\nModality Features Concatenation",
          "Text + Audio\n55.51\n30.15": ""
        },
        {
          "ResNet50-5fps\n41.76\n19": "",
          "Text + Audio\n55.51\n30.15": "mation in multimodal emotion recognition. How-"
        },
        {
          "ResNet50-5fps\n41.76\n19": "For the fusion experiments, we evaluate all possible",
          "Text + Audio\n55.51\n30.15": "ever,\ntext + vision (43.33 weighted F1) provides"
        },
        {
          "ResNet50-5fps\n41.76\n19": "combinations of the three modalities to understand",
          "Text + Audio\n55.51\n30.15": "only a marginal\nimprovement over vision alone,"
        },
        {
          "ResNet50-5fps\n41.76\n19": "how multimodal integration impacts ERC. We use",
          "Text + Audio\n55.51\n30.15": "suggesting that\ntextual and visual\nfeatures may"
        },
        {
          "ResNet50-5fps\n41.76\n19": "the best-performing unimodal models:\nthe con-",
          "Text + Audio\n55.51\n30.15": "not be as complementary as text and audio. Over-"
        },
        {
          "ResNet50-5fps\n41.76\n19": "textual text model (Ghana-NLP/abena-base-asante-",
          "Text + Audio\n55.51\n30.15": "all, these results highlight the advantages of mul-"
        },
        {
          "ResNet50-5fps\n41.76\n19": "twi-uncased) for text, Whisper-small for audio, and",
          "Text + Audio\n55.51\n30.15": "timodal fusion, particularly the strong synergy be-"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 9: , Transformer fusion out-",
      "data": [
        {
          "Figure 2: Illustration of the transformer fusion model.": ""
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": "tween textual and auditory features, while also em-"
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": ""
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": "phasizing the challenges posed by the limited avail-"
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": ""
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": "ability of high-quality pretraining data for Akan."
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": ""
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": "4.5.2\nTransformer Fusion"
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": "To further enhance multimodal fusion, we employ"
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": ""
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": "a transformer-based cross-attention encoder to cap-"
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": ""
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": "ture interdependencies between different modali-"
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": ""
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": "ties. This approach enables a more nuanced inte-"
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": ""
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": "gration of modality-specific features by projecting"
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": "information from one modality into the representa-"
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": "tional space of another. Given our bimodal results"
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": "indicating that text and vision do not complement"
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": "each other effectively, we structure our fusion pro-"
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": ""
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": "cess around audio-centric interactions. Specifically,"
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": ""
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": "we use a cross-attention encoder to fuse text and au-"
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": "dio (audio-text fusion) as well as audio and vision"
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": "(audio-vision fusion)."
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": "In this framework, we first extract features from"
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": "each modality using the best-performing unimodal"
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": "models. The cross-attention encoder is designed"
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": "such that the query comes from one modality while"
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": "the keys and values are derived from another. This"
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": "mechanism allows each modality to selectively at-"
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": "tend to the most relevant aspects of the other, facili-"
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": "tating effective multimodal alignment. The encoder"
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": "projects the hidden representations of one modality"
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": "into the representational space of another, enhanc-"
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": "ing cross-modal interactions."
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": "To structure the fusion process, we prepend a"
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": "CLS token to the hidden states of each modality"
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": "before applying the cross-attention mechanism. Af-"
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": "ter audio-text fusion, we obtain two new hidden"
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": "features are pro-\nrepresentations: Ta, where text"
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": "jected into the audio space, and At, where audio"
        },
        {
          "Figure 2: Illustration of the transformer fusion model.": "features are projected into the text space. Similarly,"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "fusion techniques for enhancing multimodal ERC,": "particularly in low-resource settings like Akan.",
          "prosody research. Our experiments with state-of-": "the-art ERC methods validate AkaCE’s quality and"
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "Table 10 presents the per-emotion F1 scores and",
          "prosody research. Our experiments with state-of-": "establish a strong baseline for future research."
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "support. As expected,\nthe model performs best",
          "prosody research. Our experiments with state-of-": "Looking ahead, we aim to expand to additional"
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "on classes with higher\nfrequency in the training",
          "prosody research. Our experiments with state-of-": "African languages and develop culturally adaptive"
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "set, such as Neutral, Anger, and Sadness, while",
          "prosody research. Our experiments with state-of-": "ERC systems. Multimodal emotion recognition can"
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "performance on low-resource classes like Disgust",
          "prosody research. Our experiments with state-of-": "be improved by speech enhancement\ntechniques"
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "and Fear remains limited. These findings under-",
          "prosody research. Our experiments with state-of-": "and pretraining models on African languages. Inte-"
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "score the importance of addressing class imbalance",
          "prosody research. Our experiments with state-of-": "grating vision-language models for scene descrip-"
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "in multimodal emotion recognition, especially in",
          "prosody research. Our experiments with state-of-": "tions can also provide richer context. Advanced fu-"
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "low-resource language contexts such as Akan.",
          "prosody research. Our experiments with state-of-": "sion techniques like graph neural networks (GNNs)"
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "",
          "prosody research. Our experiments with state-of-": "and hypergraphs may further refine cross-modal"
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "Model Complexity.\nTo contextualize the scale",
          "prosody research. Our experiments with state-of-": ""
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "",
          "prosody research. Our experiments with state-of-": "integration. We hope AkaCE inspires further re-"
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "of our proposed transformer fusion system, we re-",
          "prosody research. Our experiments with state-of-": ""
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "",
          "prosody research. Our experiments with state-of-": "search toward culturally adaptive, linguistically di-"
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "port in Table 11 the parameter counts for each of",
          "prosody research. Our experiments with state-of-": ""
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "",
          "prosody research. Our experiments with state-of-": "verse NLP resources."
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "its core components. For the vision backbone, we",
          "prosody research. Our experiments with state-of-": ""
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "use ResNet-18 (convolutional and fully connected",
          "prosody research. Our experiments with state-of-": ""
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "",
          "prosody research. Our experiments with state-of-": "Acknowledgements"
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "layers), totalling approximately 11.7 million param-",
          "prosody research. Our experiments with state-of-": ""
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "eters. Text features are extracted using the Ghana-",
          "prosody research. Our experiments with state-of-": "This\nresearch\nis\nsupported\nin\npart\nby\nthe Na-"
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "NLP abena-base-asante-twi-uncased model,",
          "prosody research. Our experiments with state-of-": "tional Science Foundation via ARNI\n(The NSF"
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "a BERT-base variant with 12 transformer\nlayers",
          "prosody research. Our experiments with state-of-": "AI\nInstitute\nfor Artificial\nand Natural\nIntelli-"
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "of 768 hidden units, contributing about 178 million",
          "prosody research. Our experiments with state-of-": "gence), under the Columbia 2025 Research Project"
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "parameters. Audio features are encoded with Ope-",
          "prosody research. Our experiments with state-of-": "(\"Towards Safe, Robust,\nInterpretable Dialogue"
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "nAI’s Whisper-small model, which consists of 12-",
          "prosody research. Our experiments with state-of-": "Agents for Democratized Medical Care\"), and in"
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "layer encoder-decoder stacks with 768-dimensional",
          "prosody research. Our experiments with state-of-": "part by the Defense Advanced Research Projects"
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "hidden representations, accounting for 244 million",
          "prosody research. Our experiments with state-of-": "Agency (DARPA), via the CCU Program contract"
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "parameters. The transformer fusion module itself is",
          "prosody research. Our experiments with state-of-": "HR001122C0034. The views, opinions and/or find-"
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "composed of five cross-attention blocks, each using",
          "prosody research. Our experiments with state-of-": "ings expressed are those of the authors and should"
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "768-dimensional queries, keys, and values with 12",
          "prosody research. Our experiments with state-of-": "not be interpreted as representing the official views"
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "attention heads, adding roughly 12 million param-",
          "prosody research. Our experiments with state-of-": "or policies of the National Science Foundation or"
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "eters. Altogether,\nthe complete model comprises",
          "prosody research. Our experiments with state-of-": "the U.S. Government."
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "approximately 450 million parameters, with the",
          "prosody research. Our experiments with state-of-": ""
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "",
          "prosody research. Our experiments with state-of-": "Limitations"
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "majority allocated to the unimodal backbones and",
          "prosody research. Our experiments with state-of-": ""
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "only a small fraction to the fusion mechanism.",
          "prosody research. Our experiments with state-of-": ""
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "",
          "prosody research. Our experiments with state-of-": "While\nthe Akan Cinematic Emotions\n(AkaCE)"
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "Component\nConfiguration\nParameters",
          "prosody research. Our experiments with state-of-": "dataset represents a significant advancement in mul-"
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "ResNet-18 (Vision)\nConv + FC layers\n11.7M",
          "prosody research. Our experiments with state-of-": ""
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "",
          "prosody research. Our experiments with state-of-": "timodal emotion recognition research, particularly"
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "BERT-base (Text)\n12×768 (Abena-Twi)\n178M",
          "prosody research. Our experiments with state-of-": ""
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "Whisper-small (Audio)\n12×768 Encoder-Decoder\n244M",
          "prosody research. Our experiments with state-of-": "for African languages, there are several limitations"
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "Cross-Attention Blocks\n5×(768-d, 12 heads)\n12M",
          "prosody research. Our experiments with state-of-": ""
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "",
          "prosody research. Our experiments with state-of-": "to acknowledge."
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "≈ 450M\nTotal",
          "prosody research. Our experiments with state-of-": ""
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "",
          "prosody research. Our experiments with state-of-": "One limitation of this work is that the dataset fo-"
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "Table 11: Model parameter breakdown across modali-",
          "prosody research. Our experiments with state-of-": "cuses exclusively on the Akan language. While this"
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "ties.",
          "prosody research. Our experiments with state-of-": ""
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "",
          "prosody research. Our experiments with state-of-": "contributes to the representation of low-resource"
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "",
          "prosody research. Our experiments with state-of-": "languages\nin emotion recognition research,\nthe"
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "",
          "prosody research. Our experiments with state-of-": "findings may not generalize to other African lan-"
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "5\nConclusion and Future Directions",
          "prosody research. Our experiments with state-of-": ""
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "",
          "prosody research. Our experiments with state-of-": "guages or cultural contexts without further adap-"
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "We\nintroduce\nthe Akan Conversation Emotion",
          "prosody research. Our experiments with state-of-": "tation and testing. The emotional expressions and"
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "(AkaCE) dataset, the first multimodal emotion dia-",
          "prosody research. Our experiments with state-of-": "prosodic characteristics in Akan may differ sub-"
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "logue dataset for an African language, addressing",
          "prosody research. Our experiments with state-of-": "stantially from those in other languages,\nlimiting"
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "the resource gap in ERC research for low-resource",
          "prosody research. Our experiments with state-of-": "cross-linguistic applicability."
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "languages. AkaCE comprises 385 emotion-labeled",
          "prosody research. Our experiments with state-of-": "Another\nlimitation lies\nin the domain of\nthe"
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "dialogues and word-level prosodic prominence an-",
          "prosody research. Our experiments with state-of-": "dataset, which is derived from movie dialogues."
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "notations, making it a valuable resource for cross-",
          "prosody research. Our experiments with state-of-": "While this ensures the presence of diverse emotions"
        },
        {
          "fusion techniques for enhancing multimodal ERC,": "cultural emotion recognition and tonal\nlanguage",
          "prosody research. Our experiments with state-of-": "and rich multimodal interactions, it is likely that a"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "portion of the data contains acted emotions rather": "",
          "In Proceedings of the 57th Annual Meeting of the As-": "sociation for Computational Linguistics, pages 3204–"
        },
        {
          "portion of the data contains acted emotions rather": "than naturally occurring ones. Acted emotions may",
          "In Proceedings of the 57th Annual Meeting of the As-": ""
        },
        {
          "portion of the data contains acted emotions rather": "",
          "In Proceedings of the 57th Annual Meeting of the As-": "3210, Florence, Italy. Association for Computational"
        },
        {
          "portion of the data contains acted emotions rather": "differ in intensity, expression, and prosodic features",
          "In Proceedings of the 57th Annual Meeting of the As-": ""
        },
        {
          "portion of the data contains acted emotions rather": "",
          "In Proceedings of the 57th Annual Meeting of the As-": "Linguistics."
        },
        {
          "portion of the data contains acted emotions rather": "from emotions encountered in real-world scenarios,",
          "In Proceedings of the 57th Annual Meeting of the As-": ""
        },
        {
          "portion of the data contains acted emotions rather": "potentially introducing a bias in models trained on",
          "In Proceedings of the 57th Annual Meeting of the As-": "Ibrahim Said Ahmad, Shiran Dudy, Tadesse Destaw Be-"
        },
        {
          "portion of the data contains acted emotions rather": "",
          "In Proceedings of the 57th Annual Meeting of the As-": "lay, Idris Abdulmumin, Seid Muhie Yimam, Sham-"
        },
        {
          "portion of the data contains acted emotions rather": "this dataset. This could affect\nthe generalizabil-",
          "In Proceedings of the 57th Annual Meeting of the As-": ""
        },
        {
          "portion of the data contains acted emotions rather": "",
          "In Proceedings of the 57th Annual Meeting of the As-": "suddeen Hassan Muhammad, and Kenneth Church."
        },
        {
          "portion of the data contains acted emotions rather": "ity of such models to real-life applications, where",
          "In Proceedings of the 57th Annual Meeting of the As-": ""
        },
        {
          "portion of the data contains acted emotions rather": "",
          "In Proceedings of the 57th Annual Meeting of the As-": "2025.\nExploring cultural nuances in emotion per-"
        },
        {
          "portion of the data contains acted emotions rather": "emotional expressions might be less exaggerated",
          "In Proceedings of the 57th Annual Meeting of the As-": ""
        },
        {
          "portion of the data contains acted emotions rather": "",
          "In Proceedings of the 57th Annual Meeting of the As-": "ception across 15 african languages. arXiv preprint"
        },
        {
          "portion of the data contains acted emotions rather": "or contextually different.",
          "In Proceedings of the 57th Annual Meeting of the As-": "arXiv:2503.19642."
        },
        {
          "portion of the data contains acted emotions rather": "Additionally, while the inclusion of prosodic",
          "In Proceedings of the 57th Annual Meeting of the As-": ""
        },
        {
          "portion of the data contains acted emotions rather": "",
          "In Proceedings of the 57th Annual Meeting of the As-": "Jesujoba O. Alabi, Kwabena Amponsah-Kaakyire,"
        },
        {
          "portion of the data contains acted emotions rather": "annotations is a novel\nfeature,\nthe labelling pro-",
          "In Proceedings of the 57th Annual Meeting of the As-": ""
        },
        {
          "portion of the data contains acted emotions rather": "",
          "In Proceedings of the 57th Annual Meeting of the As-": "David I. Adelani, and Cristina España-Bonet. 2020."
        },
        {
          "portion of the data contains acted emotions rather": "cess may be subject\nto subjective interpretations,",
          "In Proceedings of the 57th Annual Meeting of the As-": "Massive vs. curated embeddings for low-resourced"
        },
        {
          "portion of the data contains acted emotions rather": "particularly for ambiguous emotional expressions.",
          "In Proceedings of the 57th Annual Meeting of the As-": "languages:\nthe case of Yorùbá and Twi.\nIn Proceed-"
        },
        {
          "portion of the data contains acted emotions rather": "",
          "In Proceedings of the 57th Annual Meeting of the As-": "ings of the Twelfth Language Resources and Evalua-"
        },
        {
          "portion of the data contains acted emotions rather": "The quality and consistency of these annotations",
          "In Proceedings of the 57th Annual Meeting of the As-": ""
        },
        {
          "portion of the data contains acted emotions rather": "",
          "In Proceedings of the 57th Annual Meeting of the As-": "tion Conference, pages 2754–2762, Marseille, France."
        },
        {
          "portion of the data contains acted emotions rather": "could impact\nthe performance of models relying",
          "In Proceedings of the 57th Annual Meeting of the As-": ""
        },
        {
          "portion of the data contains acted emotions rather": "",
          "In Proceedings of the 57th Annual Meeting of the As-": "European Language Resources Association."
        },
        {
          "portion of the data contains acted emotions rather": "on prosodic features. Further efforts to standardize",
          "In Proceedings of the 57th Annual Meeting of the As-": ""
        },
        {
          "portion of the data contains acted emotions rather": "prosodic annotation practices would benefit future",
          "In Proceedings of the 57th Annual Meeting of the As-": "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe"
        },
        {
          "portion of the data contains acted emotions rather": "",
          "In Proceedings of the 57th Annual Meeting of the As-": "Kazemzadeh, Emily Mower, Samuel Kim,\nJean-"
        },
        {
          "portion of the data contains acted emotions rather": "iterations of this dataset.",
          "In Proceedings of the 57th Annual Meeting of the As-": ""
        },
        {
          "portion of the data contains acted emotions rather": "",
          "In Proceedings of the 57th Annual Meeting of the As-": "nette N Chang,\nSungbok Lee,\nand Shrikanth S"
        },
        {
          "portion of the data contains acted emotions rather": "Another challenge is related to visual data. Al-",
          "In Proceedings of the 57th Annual Meeting of the As-": ""
        },
        {
          "portion of the data contains acted emotions rather": "",
          "In Proceedings of the 57th Annual Meeting of the As-": "Narayanan. 2008.\nIemocap:\nInteractive emotional"
        },
        {
          "portion of the data contains acted emotions rather": "though the dataset incorporates visual modalities,",
          "In Proceedings of the 57th Annual Meeting of the As-": "dyadic motion capture database. Language resources"
        },
        {
          "portion of the data contains acted emotions rather": "the quality and consistency of visual\nfeatures in",
          "In Proceedings of the 57th Annual Meeting of the As-": "and evaluation, 42:335–359."
        },
        {
          "portion of the data contains acted emotions rather": "movie dialogues may vary due to differences in",
          "In Proceedings of the 57th Annual Meeting of the As-": ""
        },
        {
          "portion of the data contains acted emotions rather": "",
          "In Proceedings of the 57th Annual Meeting of the As-": "Carlos Busso, Srinivas Parthasarathy, Alec Burmania,"
        },
        {
          "portion of the data contains acted emotions rather": "lighting,\ncamera\nangles,\nand actor positioning.",
          "In Proceedings of the 57th Annual Meeting of the As-": ""
        },
        {
          "portion of the data contains acted emotions rather": "",
          "In Proceedings of the 57th Annual Meeting of the As-": "Mohammed AbdelWahab, Najmeh Sadoughi, and"
        },
        {
          "portion of the data contains acted emotions rather": "These variations could impact the reliability of vi-",
          "In Proceedings of the 57th Annual Meeting of the As-": "Emily Mower Provost. 2016. Msp-improv: An acted"
        },
        {
          "portion of the data contains acted emotions rather": "",
          "In Proceedings of the 57th Annual Meeting of the As-": "corpus of dyadic interactions to study emotion per-"
        },
        {
          "portion of the data contains acted emotions rather": "sual emotion recognition models trained on this",
          "In Proceedings of the 57th Annual Meeting of the As-": ""
        },
        {
          "portion of the data contains acted emotions rather": "",
          "In Proceedings of the 57th Annual Meeting of the As-": "ception.\nIEEE Transactions on Affective Computing,"
        },
        {
          "portion of the data contains acted emotions rather": "dataset. Moreover,\nfurther exploration of vision",
          "In Proceedings of the 57th Annual Meeting of the As-": ""
        },
        {
          "portion of the data contains acted emotions rather": "",
          "In Proceedings of the 57th Annual Meeting of the As-": "8(1):67–80."
        },
        {
          "portion of the data contains acted emotions rather": "features, including fine-tuned embeddings and ad-",
          "In Proceedings of the 57th Annual Meeting of the As-": ""
        },
        {
          "portion of the data contains acted emotions rather": "vanced visual annotations, may reveal additional",
          "In Proceedings of the 57th Annual Meeting of the As-": "Qiong Cao, Li Shen, Weidi Xie, Omkar M. Parkhi, and"
        },
        {
          "portion of the data contains acted emotions rather": "",
          "In Proceedings of the 57th Annual Meeting of the As-": "Andrew Zisserman. 2018. Vggface2: A dataset for"
        },
        {
          "portion of the data contains acted emotions rather": "insights but was not the focus of this study.",
          "In Proceedings of the 57th Annual Meeting of the As-": ""
        },
        {
          "portion of the data contains acted emotions rather": "",
          "In Proceedings of the 57th Annual Meeting of the As-": "recognising faces across pose and age.\nPreprint,"
        },
        {
          "portion of the data contains acted emotions rather": "Despite these limitations, we believe that AkaCE",
          "In Proceedings of the 57th Annual Meeting of the As-": ""
        },
        {
          "portion of the data contains acted emotions rather": "",
          "In Proceedings of the 57th Annual Meeting of the As-": "arXiv:1710.08092."
        },
        {
          "portion of the data contains acted emotions rather": "provides an essential\nfoundation for advancing",
          "In Proceedings of the 57th Annual Meeting of the As-": ""
        },
        {
          "portion of the data contains acted emotions rather": "speech emotion recognition in low-resource lan-",
          "In Proceedings of the 57th Annual Meeting of the As-": "SY Chen, CC Hsu, CC Kuo, and LW Ku. 2018. Emo-"
        },
        {
          "portion of the data contains acted emotions rather": "",
          "In Proceedings of the 57th Annual Meeting of the As-": "tionlines: An emotion corpus of multi-party conversa-"
        },
        {
          "portion of the data contains acted emotions rather": "guages and encourages further exploration in this",
          "In Proceedings of the 57th Annual Meeting of the As-": ""
        },
        {
          "portion of the data contains acted emotions rather": "",
          "In Proceedings of the 57th Annual Meeting of the As-": "tions. arxiv 2018. arXiv preprint arXiv:1802.08379."
        },
        {
          "portion of the data contains acted emotions rather": "area.",
          "In Proceedings of the 57th Annual Meeting of the As-": ""
        },
        {
          "portion of the data contains acted emotions rather": "",
          "In Proceedings of the 57th Annual Meeting of the As-": "Vishal Chudasama, Purbayan Kar, Ashish Gudmalwar,"
        },
        {
          "portion of the data contains acted emotions rather": "Ethical Considerations",
          "In Proceedings of the 57th Annual Meeting of the As-": "Nirmesh Shah, Pankaj Wasnik, and Naoyuki Onoe."
        },
        {
          "portion of the data contains acted emotions rather": "",
          "In Proceedings of the 57th Annual Meeting of the As-": "2022. M2fnet: Multi-modal fusion network for emo-"
        },
        {
          "portion of the data contains acted emotions rather": "",
          "In Proceedings of the 57th Annual Meeting of the As-": "tion recognition in conversation.\nIn Proceedings of"
        },
        {
          "portion of the data contains acted emotions rather": "The potential for misuse of the AkaCE dataset must",
          "In Proceedings of the 57th Annual Meeting of the As-": ""
        },
        {
          "portion of the data contains acted emotions rather": "",
          "In Proceedings of the 57th Annual Meeting of the As-": "the IEEE/CVF Conference on Computer Vision and"
        },
        {
          "portion of the data contains acted emotions rather": "be carefully acknowledged. While the dataset\nis",
          "In Proceedings of the 57th Annual Meeting of the As-": ""
        },
        {
          "portion of the data contains acted emotions rather": "",
          "In Proceedings of the 57th Annual Meeting of the As-": "Pattern Recognition, pages 4652–4661."
        },
        {
          "portion of the data contains acted emotions rather": "intended for\nresearch purposes, deploying mod-",
          "In Proceedings of the 57th Annual Meeting of the As-": ""
        },
        {
          "portion of the data contains acted emotions rather": "els trained on AkaCE in real-world applications",
          "In Proceedings of the 57th Annual Meeting of the As-": "Jennifer Cole, Timothy Mahrt, and Joseph Roy. 2017."
        },
        {
          "portion of the data contains acted emotions rather": "",
          "In Proceedings of the 57th Annual Meeting of the As-": "Computer\nCrowd-sourcing prosodic\nannotation."
        },
        {
          "portion of the data contains acted emotions rather": "without proper domain adaptation and validation",
          "In Proceedings of the 57th Annual Meeting of the As-": ""
        },
        {
          "portion of the data contains acted emotions rather": "",
          "In Proceedings of the 57th Annual Meeting of the As-": "Speech & Language, 45:300–325."
        },
        {
          "portion of the data contains acted emotions rather": "could result in inaccurate emotion predictions, par-",
          "In Proceedings of the 57th Annual Meeting of the As-": ""
        },
        {
          "portion of the data contains acted emotions rather": "ticularly in scenarios that deviate from cinematic",
          "In Proceedings of the 57th Annual Meeting of the As-": "Morena Danieli, Giuseppe Riccardi, and Firoj Alam."
        },
        {
          "portion of the data contains acted emotions rather": "dialogues. As such, researchers and practitioners",
          "In Proceedings of the 57th Annual Meeting of the As-": "2015. Emotion unfolding and affective scenes: A"
        },
        {
          "portion of the data contains acted emotions rather": "",
          "In Proceedings of the 57th Annual Meeting of the As-": "case study in spoken conversations.\nIn Proceedings"
        },
        {
          "portion of the data contains acted emotions rather": "should exercise caution when extending the use of",
          "In Proceedings of the 57th Annual Meeting of the As-": ""
        },
        {
          "portion of the data contains acted emotions rather": "",
          "In Proceedings of the 57th Annual Meeting of the As-": "of the International Workshop on Emotion Represen-"
        },
        {
          "portion of the data contains acted emotions rather": "this dataset to other applications.",
          "In Proceedings of the 57th Annual Meeting of the As-": ""
        },
        {
          "portion of the data contains acted emotions rather": "",
          "In Proceedings of the 57th Annual Meeting of the As-": "tations and Modelling for Companion Technologies,"
        },
        {
          "portion of the data contains acted emotions rather": "",
          "In Proceedings of the 57th Annual Meeting of the As-": "pages 5–11."
        },
        {
          "portion of the data contains acted emotions rather": "",
          "In Proceedings of the 57th Annual Meeting of the As-": "Abhinav Dhall, Roland Goecke, Simon Lucey, and Tom"
        },
        {
          "portion of the data contains acted emotions rather": "References",
          "In Proceedings of the 57th Annual Meeting of the As-": ""
        },
        {
          "portion of the data contains acted emotions rather": "",
          "In Proceedings of the 57th Annual Meeting of the As-": "Gedoen. 2012.\nCollecting large,\nrichly annotated"
        },
        {
          "portion of the data contains acted emotions rather": "Željko Agi´c and Ivan Vuli´c. 2019.\nJW300: A wide-",
          "In Proceedings of the 57th Annual Meeting of the As-": "IEEE Mul-\nfacial-expression databases from movies."
        },
        {
          "portion of the data contains acted emotions rather": "coverage parallel corpus for low-resource languages.",
          "In Proceedings of the 57th Annual Meeting of the As-": "timedia, pages 34–41."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "accented speech recognition: Epistemic uncertainty-",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Lyu. 2019.\nHigru: Hierarchical gated recurrent"
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "driven data selection for generalizable asr models.",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "units for utterance-level emotion recognition. arXiv"
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "Preprint, arXiv:2306.02105.",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "preprint arXiv:1904.04446."
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "Paul Ekman. 1992. Are there basic emotions? Psycho-",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Taewoon Kim and Piek Vossen. 2021.\nEmoberta:"
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "logical review, 99 (3).",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Speaker-aware emotion recognition in conversation"
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "with roberta. arXiv preprint arXiv:2108.12009."
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "Joseph L. Fleiss. 1971. Measuring nominal scale agree-",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": ""
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "ment among many raters.\nPsychological Bulletin,",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": ""
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Frank Kügler\nand Susanne Genzel. 2012.\nOn the"
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "76(5):378–382.",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": ""
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "prosodic expression of pragmatic prominence: The"
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Language\ncase of pitch register lowering in akan."
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "Nickolaos Fragopanagos\nand John G Taylor. 2005.",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": ""
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "and speech, 55(3):331–359."
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "Emotion recognition in human–computer interaction.",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": ""
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "Neural Networks, 18(4):389–405.",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": ""
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "William R Leben. 2018. Languages of the world.\nIn"
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Oxford Research Encyclopedia of Linguistics."
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "Yumeng Fu.\n2024.\nCkerc:\nJoint\nlarge\nlanguage",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": ""
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "models with\ncommonsense\nknowledge\nfor\nemo-",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": ""
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Shanglin Lei, Guanting Dong, Xiaoping Wang, Keheng"
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "arXiv preprint\ntion recognition in conversation.",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": ""
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Wang, and Sirui Wang. 2023.\nInstructerc: Reform-"
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "arXiv:2403.07260.",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": ""
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "ing emotion recognition in conversation with a re-"
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "arXiv preprint\ntrieval multi-task llms framework."
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "Deepanway Ghosal, Navonil Majumder, Soujanya Poria,",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": ""
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "arXiv:2309.11911."
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "Niyati Chhaya, and Alexander Gelbukh. 2019. Dia-",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": ""
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "loguegcn: A graph convolutional neural network for",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": ""
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "emotion recognition in conversation. arXiv preprint",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Ya Li,\nJianhua Tao, Björn Schuller, Shiguang Shan,"
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "arXiv:1908.11540.",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Dongmei Jiang, and Jia Jia. 2018. Mec 2017: Multi-"
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "modal emotion recognition challenge.\nIn 2018 First"
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "Yuan Gong, Yu-An Chung, and James Glass. 2021. Ast:",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Asian Conference on Affective Computing and Intelli-"
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "Audio spectrogram transformer.\nIn Interspeech 2021,",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "gent Interaction (ACII Asia), pages 1–5. IEEE."
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "pages 571–575.",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": ""
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang"
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "Ziwei Gong, Muyin Yao, Xinyi Hu, Xiaoning Zhu, and",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Cao, and Shuzi Niu. 2017. Dailydialog: A manually"
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "Julia Hirschberg. 2024. A mapping on current classi-",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "labelled multi-turn dialogue dataset. arXiv preprint"
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "fying categories of emotions used in multimodal mod-",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "arXiv:1710.03957."
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "els for emotion recognition.\nIn Proceedings of The",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": ""
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "18th Linguistic Annotation Workshop (LAW-XVIII),",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": ""
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "N Majumder, S Poria, D Hazarika, R Mihalcea, A Gel-"
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "pages 19–28, St. Julians, Malta. Association for Com-",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": ""
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "bukh, and E Cambria DialogueRNN. 2019. An at-"
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "putational Linguistics.",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": ""
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "tentive rnn for emotion detection in conversations."
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Association for the Advancement of Artificial Intelli-"
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "Devamanyu Hazarika, Soujanya Poria, Rada Mihal-",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": ""
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "gence, pages 6818–6825."
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "cea, Erik Cambria, and Roger Zimmermann. 2018a.",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": ""
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "ICON: Interactive conversational memory network",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": ""
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Mo Ibrahim Foundation. 2023. Africa in the World and"
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "for multimodal emotion detection.\nIn Proceedings of",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": ""
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "the World in Africa: Facts & Figures, April 2023."
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "the 2018 Conference on Empirical Methods in Nat-",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": ""
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Accessed: 15-Feb-2025."
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "ural Language Processing, pages 2594–2604, Brus-",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": ""
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "sels, Belgium. Association for Computational Lin-",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": ""
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Angeline Peterson, Danya Al-Saleh, Sam Allen, Alex"
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "guistics.",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": ""
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Fochios, Olivia Mulford, Kaden Paulson-Smith, and"
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Resources\nfor Self-\nLauren Parnell Marino. n.d."
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "Devamanyu Hazarika, Soujanya Poria, Amir Zadeh,",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": ""
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Instructional Learners of Less Commonly Taught"
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "Erik Cambria, Louis-Philippe Morency, and Roger",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": ""
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Languages. Accessed: 15-Feb-2025."
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "Zimmermann. 2018b. Conversational memory net-",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": ""
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "work for emotion recognition in dyadic dialogue",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": ""
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Soujanya Poria, Erik Cambria, Devamanyu Hazarika,"
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "the conference. Associ-\nvideos.\nIn Proceedings of",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": ""
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Navonil Majumder, Amir Zadeh, and Louis-Philippe"
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "ation for Computational Linguistics. North American",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": ""
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Morency. 2017. Context-dependent sentiment anal-"
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "Chapter. Meeting, volume 2018, page 2122. NIH",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": ""
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "ysis in user-generated videos.\nIn Proceedings of the"
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "Public Access.",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": ""
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "55th annual meeting of\nthe association for compu-"
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "tational linguistics (volume 1: Long papers), pages"
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "Sun. 2015. Deep residual learning for image recogni-",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "873–883."
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "tion. Preprint, arXiv:1512.03385.",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": ""
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-"
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "Jingwen Hu, Yuchen Liu, Jinming Zhao, and Qin Jin.",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "jumder, Gautam Naik, Erik Cambria, and Rada Mi-"
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "2021. Mmgcn: Multimodal fusion via deep graph",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "halcea. 2018.\nMeld:\nA multimodal multi-party"
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "convolution network for emotion recognition in con-",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "dataset\nfor\nemotion recognition in conversations."
        },
        {
          "Bonaventure F. P. Dossou. 2024. Advancing african-": "versation. arXiv preprint arXiv:2107.06779.",
          "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R": "arXiv preprint arXiv:1810.02508."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "and Eduard Hovy. 2019. Emotion recognition in con-",
          "of modality.\nIn Proceedings of the 58th annual meet-": "ing of the association for computational linguistics,"
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "versation: Research challenges, datasets, and recent",
          "of modality.\nIn Proceedings of the 58th annual meet-": "pages 3718–3727."
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "advances.\nIEEE access, 7:100943–100953.",
          "of modality.\nIn Proceedings of the 58th annual meet-": ""
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "",
          "of modality.\nIn Proceedings of the 58th annual meet-": "AmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Poria,"
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "Alec Radford, Jong Wook Kim, Tao Xu, Greg Brock-",
          "of modality.\nIn Proceedings of the 58th annual meet-": "Erik Cambria, and Louis-Philippe Morency. 2018."
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "man, Christine McLeavey, and Ilya Sutskever. 2022.",
          "of modality.\nIn Proceedings of the 58th annual meet-": "Multimodal\nlanguage analysis\nin the wild: Cmu-"
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "Robust speech recognition via large-scale weak su-",
          "of modality.\nIn Proceedings of the 58th annual meet-": "mosei dataset and interpretable dynamic fusion graph."
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "pervision. Preprint, arXiv:2212.04356.",
          "of modality.\nIn Proceedings of the 58th annual meet-": "In Proceedings of the 56th Annual Meeting of the As-"
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "",
          "of modality.\nIn Proceedings of the 58th annual meet-": "sociation for Computational Linguistics (Volume 1:"
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "Fabien Ringeval, Björn Schuller, Michel Valstar, Roddy",
          "of modality.\nIn Proceedings of the 58th annual meet-": "Long Papers), pages 2236–2246."
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "Cowie, Heysem Kaya, Maximilian Schmitt, Shahin",
          "of modality.\nIn Proceedings of the 58th annual meet-": ""
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "Amiriparian, Nicholas Cummins, Denis Lalanne,",
          "of modality.\nIn Proceedings of the 58th annual meet-": "Sayyed M Zahiri and Jinho D Choi. 2018.\nEmotion"
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "Adrien Michaud, et al. 2018. Avec 2018 workshop",
          "of modality.\nIn Proceedings of the 58th annual meet-": "detection on tv show transcripts with sequence-based"
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "and challenge: Bipolar disorder and cross-cultural",
          "of modality.\nIn Proceedings of the 58th annual meet-": "convolutional neural networks.\nIn Workshops at the"
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "the 2018 on\naffect recognition.\nIn Proceedings of",
          "of modality.\nIn Proceedings of the 58th annual meet-": "thirty-second aaai conference on artificial\nintelli-"
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "audio/visual emotion challenge and workshop, pages",
          "of modality.\nIn Proceedings of the 58th annual meet-": "gence."
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "3–13.",
          "of modality.\nIn Proceedings of the 58th annual meet-": ""
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "",
          "of modality.\nIn Proceedings of the 58th annual meet-": "Dong Zhang, Liangqing Wu, Changlong Sun, Shoushan"
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "",
          "of modality.\nIn Proceedings of the 58th annual meet-": "Li, Qiaoming Zhu, and Guodong Zhou. 2019. Mod-"
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "Guangyao Shen, Xin Wang, Xuguang Duan, Hongzhi",
          "of modality.\nIn Proceedings of the 58th annual meet-": ""
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "",
          "of modality.\nIn Proceedings of the 58th annual meet-": "eling both context-and speaker-sensitive dependence"
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "Li, and Wenwu Zhu. 2020. Memor: A dataset for",
          "of modality.\nIn Proceedings of the 58th annual meet-": ""
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "",
          "of modality.\nIn Proceedings of the 58th annual meet-": "for emotion detection in multi-speaker conversations."
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "multimodal emotion reasoning in videos.\nIn Proceed-",
          "of modality.\nIn Proceedings of the 58th annual meet-": ""
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "",
          "of modality.\nIn Proceedings of the 58th annual meet-": "In IJCAI, pages 5415–5421. Macao."
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "ings of\nthe 28th ACM International Conference on",
          "of modality.\nIn Proceedings of the 58th annual meet-": ""
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "Multimedia, MM ’20, page 493–502, New York, NY,",
          "of modality.\nIn Proceedings of the 58th annual meet-": ""
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "",
          "of modality.\nIn Proceedings of the 58th annual meet-": "Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li,\nand"
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "USA. Association for Computing Machinery.",
          "of modality.\nIn Proceedings of the 58th annual meet-": ""
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "",
          "of modality.\nIn Proceedings of the 58th annual meet-": "Yu Qiao. 2016.\nJoint face detection and alignment"
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "",
          "of modality.\nIn Proceedings of the 58th annual meet-": "using multitask cascaded convolutional networks."
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "Weizhou Shen, Junqing Chen, Xiaojun Quan, and Zhix-",
          "of modality.\nIn Proceedings of the 58th annual meet-": ""
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "",
          "of modality.\nIn Proceedings of the 58th annual meet-": "IEEE Signal Processing Letters, 23(10):1499–1503."
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "ian Xie. 2021. Dialogxl: All-in-one xlnet for multi-",
          "of modality.\nIn Proceedings of the 58th annual meet-": ""
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "party conversation emotion recognition.\nIn Proceed-",
          "of modality.\nIn Proceedings of the 58th annual meet-": ""
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "",
          "of modality.\nIn Proceedings of the 58th annual meet-": "Yazhou Zhang, Mengyao Wang, Prayag Tiwari, Qiuchi"
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "ings of the AAAI Conference on Artificial Intelligence,",
          "of modality.\nIn Proceedings of the 58th annual meet-": ""
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "",
          "of modality.\nIn Proceedings of the 58th annual meet-": "Li, Benyou Wang, and Jing Qin. 2023. Dialoguellm:"
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "volume 35, pages 13789–13797.",
          "of modality.\nIn Proceedings of the 58th annual meet-": ""
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "",
          "of modality.\nIn Proceedings of the 58th annual meet-": "Context and emotion knowledge-tuned llama mod-"
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "",
          "of modality.\nIn Proceedings of the 58th annual meet-": "els for emotion recognition in conversations. arXiv"
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang,",
          "of modality.\nIn Proceedings of the 58th annual meet-": ""
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "",
          "of modality.\nIn Proceedings of the 58th annual meet-": "preprint arXiv:2310.11374."
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "J Zico Kolter, Louis-Philippe Morency, and Ruslan",
          "of modality.\nIn Proceedings of the 58th annual meet-": ""
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "Salakhutdinov. 2019. Multimodal transformer for un-",
          "of modality.\nIn Proceedings of the 58th annual meet-": ""
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "",
          "of modality.\nIn Proceedings of the 58th annual meet-": "Jinming Zhao, Tenggan Zhang, Jingwen Hu, Yuchen"
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "aligned multimodal language sequences.\nIn Proceed-",
          "of modality.\nIn Proceedings of the 58th annual meet-": ""
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "",
          "of modality.\nIn Proceedings of the 58th annual meet-": "Liu, Qin\nJin, Xinchao Wang,\nand Haizhou Li."
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "ings of the conference. Association for computational",
          "of modality.\nIn Proceedings of the 58th annual meet-": ""
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "",
          "of modality.\nIn Proceedings of the 58th annual meet-": "2022a.\nM3ed: Multi-modal multi-scene multi-"
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "linguistics. Meeting, volume 2019, page 6558. NIH",
          "of modality.\nIn Proceedings of the 58th annual meet-": ""
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "",
          "of modality.\nIn Proceedings of the 58th annual meet-": "arXiv preprint\nlabel emotional dialogue database."
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "Public Access.",
          "of modality.\nIn Proceedings of the 58th annual meet-": ""
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "",
          "of modality.\nIn Proceedings of the 58th annual meet-": "arXiv:2205.10237."
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "Zehui Wu,\nZiwei Gong,\nLin Ai,\nPengyuan\nShi,",
          "of modality.\nIn Proceedings of the 58th annual meet-": ""
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "",
          "of modality.\nIn Proceedings of the 58th annual meet-": "Jinming Zhao, Tenggan Zhang, Jingwen Hu, Yuchen"
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "Kaan Donbekci, and Julia Hirschberg. 2024a. Be-",
          "of modality.\nIn Proceedings of the 58th annual meet-": ""
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "",
          "of modality.\nIn Proceedings of the 58th annual meet-": "Liu, Qin Jin, Xinchao Wang, and Haizhou Li. 2022b."
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "yond silent\nletters:\nAmplifying llms\nin emotion",
          "of modality.\nIn Proceedings of the 58th annual meet-": ""
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "",
          "of modality.\nIn Proceedings of the 58th annual meet-": "M3ED: Multi-modal multi-scene multi-label emo-"
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "arXiv preprint\nrecognition with vocal nuances.",
          "of modality.\nIn Proceedings of the 58th annual meet-": ""
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "",
          "of modality.\nIn Proceedings of the 58th annual meet-": "tional dialogue database.\nIn Proceedings of the 60th"
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "arXiv:2407.21315.",
          "of modality.\nIn Proceedings of the 58th annual meet-": ""
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "",
          "of modality.\nIn Proceedings of the 58th annual meet-": "Annual Meeting of the Association for Computational"
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "",
          "of modality.\nIn Proceedings of the 58th annual meet-": "Linguistics (Volume 1: Long Papers), pages 5699–"
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "Zehui Wu,\nZiwei Gong,\nJaywon Koo,\nand\nJulia",
          "of modality.\nIn Proceedings of the 58th annual meet-": ""
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "",
          "of modality.\nIn Proceedings of the 58th annual meet-": "5710, Dublin, Ireland. Association for Computational"
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "Hirschberg. 2024b. Multimodal multi-loss fusion",
          "of modality.\nIn Proceedings of the 58th annual meet-": ""
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "",
          "of modality.\nIn Proceedings of the 58th annual meet-": "Linguistics."
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "network for sentiment analysis.\nIn Proceedings of",
          "of modality.\nIn Proceedings of the 58th annual meet-": ""
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "the 2024 Conference of\nthe North American Chap-",
          "of modality.\nIn Proceedings of the 58th annual meet-": ""
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "",
          "of modality.\nIn Proceedings of the 58th annual meet-": "Peixiang Zhong, Di Wang, and Chunyan Miao. 2019."
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "ter of the Association for Computational Linguistics:",
          "of modality.\nIn Proceedings of the 58th annual meet-": ""
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "",
          "of modality.\nIn Proceedings of the 58th annual meet-": "Knowledge-enriched transformer\nfor\nemotion de-"
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "Human Language Technologies (Volume 1:\nLong",
          "of modality.\nIn Proceedings of the 58th annual meet-": ""
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "",
          "of modality.\nIn Proceedings of the 58th annual meet-": "arXiv preprint\ntection in textual\nconversations."
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "Papers), pages 3588–3602, Mexico City, Mexico. As-",
          "of modality.\nIn Proceedings of the 58th annual meet-": ""
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "",
          "of modality.\nIn Proceedings of the 58th annual meet-": "arXiv:1909.10681."
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "sociation for Computational Linguistics.",
          "of modality.\nIn Proceedings of the 58th annual meet-": ""
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny,",
          "of modality.\nIn Proceedings of the 58th annual meet-": ""
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "and Le-Minh Nguyen. 2024. Bioserc:\nIntegrating",
          "of modality.\nIn Proceedings of the 58th annual meet-": ""
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "biography speakers supported by llms for erc tasks.",
          "of modality.\nIn Proceedings of the 58th annual meet-": ""
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "In International Conference on Artificial Neural Net-",
          "of modality.\nIn Proceedings of the 58th annual meet-": ""
        },
        {
          "Soujanya Poria, Navonil Majumder, Rada Mihalcea,": "works, pages 277–292.",
          "of modality.\nIn Proceedings of the 58th annual meet-": ""
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "JW300: A widecoverage parallel corpus for low-resource languages",
      "authors": [
        "Željko Agić",
        "Ivan Vulić"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P19-1310"
    },
    {
      "citation_id": "2",
      "title": "Tadesse Destaw Belay, Idris Abdulmumin, Seid Muhie Yimam, Shamsuddeen Hassan Muhammad, and Kenneth Church",
      "authors": [
        "Ibrahim Said",
        "Shiran Dudy"
      ],
      "year": "2025",
      "venue": "Tadesse Destaw Belay, Idris Abdulmumin, Seid Muhie Yimam, Shamsuddeen Hassan Muhammad, and Kenneth Church",
      "arxiv": "arXiv:2503.19642"
    },
    {
      "citation_id": "3",
      "title": "Massive vs. curated embeddings for low-resourced languages: the case of Yorùbá and Twi",
      "authors": [
        "O Jesujoba",
        "Kwabena Alabi",
        "David Amponsah-Kaakyire",
        "Cristina España-Bonet Adelani"
      ],
      "year": "2020",
      "venue": "Proceedings of the Twelfth Language Resources and Evaluation Conference"
    },
    {
      "citation_id": "4",
      "title": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation"
    },
    {
      "citation_id": "5",
      "title": "Msp-improv: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "Carlos Busso",
        "Srinivas Parthasarathy",
        "Alec Burmania",
        "Mohammed Abdelwahab",
        "Najmeh Sadoughi",
        "Emily Provost"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "Vggface2: A dataset for recognising faces across pose and age",
      "authors": [
        "Qiong Cao",
        "Li Shen",
        "Weidi Xie",
        "M Omkar",
        "Andrew Parkhi",
        "Zisserman"
      ],
      "year": "2018",
      "venue": "Vggface2: A dataset for recognising faces across pose and age",
      "arxiv": "arXiv:1710.08092"
    },
    {
      "citation_id": "7",
      "title": "Emotionlines: An emotion corpus of multi-party conversations",
      "authors": [
        "Sy Chen",
        "Hsu",
        "Kuo",
        "Ku"
      ],
      "year": "2018",
      "venue": "Emotionlines: An emotion corpus of multi-party conversations",
      "arxiv": "arXiv:1802.08379"
    },
    {
      "citation_id": "8",
      "title": "Pankaj Wasnik, and Naoyuki Onoe. 2022. M2fnet: Multi-modal fusion network for emotion recognition in conversation",
      "authors": [
        "Purbayan Vishal Chudasama",
        "Ashish Kar",
        "Nirmesh Gudmalwar",
        "Shah"
      ],
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "9",
      "title": "Crowd-sourcing prosodic annotation",
      "authors": [
        "Jennifer Cole",
        "Timothy Mahrt",
        "Joseph Roy"
      ],
      "year": "2017",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "10",
      "title": "Emotion unfolding and affective scenes: A case study in spoken conversations",
      "authors": [
        "Morena Danieli",
        "Giuseppe Riccardi",
        "Firoj Alam"
      ],
      "year": "2015",
      "venue": "Proceedings of the International Workshop on Emotion Representations and Modelling for Companion Technologies"
    },
    {
      "citation_id": "11",
      "title": "Collecting large, richly annotated facial-expression databases from movies",
      "authors": [
        "Abhinav Dhall",
        "Roland Goecke",
        "Simon Lucey",
        "Tom Gedoen"
      ],
      "year": "2012",
      "venue": "IEEE Multimedia"
    },
    {
      "citation_id": "12",
      "title": "Advancing africanaccented speech recognition: Epistemic uncertaintydriven data selection for generalizable asr models",
      "authors": [
        "F Bonaventure",
        "Dossou"
      ],
      "year": "2024",
      "venue": "Advancing africanaccented speech recognition: Epistemic uncertaintydriven data selection for generalizable asr models",
      "arxiv": "arXiv:2306.02105"
    },
    {
      "citation_id": "13",
      "title": "Are there basic emotions? Psychological review",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1992",
      "venue": "Are there basic emotions? Psychological review"
    },
    {
      "citation_id": "14",
      "title": "Measuring nominal scale agreement among many raters",
      "authors": [
        "Joseph Fleiss"
      ],
      "year": "1971",
      "venue": "Psychological Bulletin",
      "doi": "10.1037/h0031619"
    },
    {
      "citation_id": "15",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "Nickolaos Fragopanagos",
        "John Taylor"
      ],
      "year": "2005",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "16",
      "title": "Ckerc: Joint large language models with commonsense knowledge for emotion recognition in conversation",
      "authors": [
        "Yumeng Fu"
      ],
      "year": "2024",
      "venue": "Ckerc: Joint large language models with commonsense knowledge for emotion recognition in conversation",
      "arxiv": "arXiv:2403.07260"
    },
    {
      "citation_id": "17",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "arxiv": "arXiv:1908.11540"
    },
    {
      "citation_id": "18",
      "title": "Ast: Audio spectrogram transformer",
      "authors": [
        "Yuan Gong",
        "Yu-An Chung",
        "James Glass"
      ],
      "year": "2021",
      "venue": "Interspeech 2021",
      "doi": "10.21437/Interspeech.2021-698"
    },
    {
      "citation_id": "19",
      "title": "A mapping on current classifying categories of emotions used in multimodal models for emotion recognition",
      "authors": [
        "Ziwei Gong",
        "Muyin Yao",
        "Xinyi Hu",
        "Xiaoning Zhu",
        "Julia Hirschberg"
      ],
      "year": "2024",
      "venue": "Proceedings of The 18th Linguistic Annotation Workshop (LAW-XVIII)"
    },
    {
      "citation_id": "20",
      "title": "2018a. ICON: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Rada Mihalcea",
        "Erik Cambria",
        "Roger Zimmermann"
      ],
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/D18-1280"
    },
    {
      "citation_id": "21",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Amir Zadeh",
        "Erik Cambria",
        "Louis-Philippe Morency",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the conference"
    },
    {
      "citation_id": "22",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2015",
      "venue": "Deep residual learning for image recognition",
      "arxiv": "arXiv:1512.03385"
    },
    {
      "citation_id": "23",
      "title": "Mmgcn: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "Jingwen Hu",
        "Yuchen Liu",
        "Jinming Zhao",
        "Qin Jin"
      ],
      "year": "2021",
      "venue": "Mmgcn: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "arxiv": "arXiv:2107.06779"
    },
    {
      "citation_id": "24",
      "title": "Higru: Hierarchical gated recurrent units for utterance-level emotion recognition",
      "authors": [
        "Wenxiang Jiao",
        "Haiqin Yang",
        "Irwin King",
        "Michael R Lyu"
      ],
      "year": "2019",
      "venue": "Higru: Hierarchical gated recurrent units for utterance-level emotion recognition",
      "arxiv": "arXiv:1904.04446"
    },
    {
      "citation_id": "25",
      "title": "Emoberta: Speaker-aware emotion recognition in conversation with roberta",
      "authors": [
        "Taewoon Kim",
        "Piek Vossen"
      ],
      "year": "2021",
      "venue": "Emoberta: Speaker-aware emotion recognition in conversation with roberta",
      "arxiv": "arXiv:2108.12009"
    },
    {
      "citation_id": "26",
      "title": "On the prosodic expression of pragmatic prominence: The case of pitch register lowering in akan",
      "authors": [
        "Frank Kügler",
        "Susanne Genzel"
      ],
      "year": "2012",
      "venue": "Language and speech"
    },
    {
      "citation_id": "27",
      "title": "Languages of the world",
      "authors": [
        "William R Leben"
      ],
      "year": "2018",
      "venue": "Oxford Research Encyclopedia of Linguistics"
    },
    {
      "citation_id": "28",
      "title": "Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework",
      "authors": [
        "Shanglin Lei",
        "Guanting Dong",
        "Xiaoping Wang",
        "Keheng Wang",
        "Sirui Wang"
      ],
      "year": "2023",
      "venue": "Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework",
      "arxiv": "arXiv:2309.11911"
    },
    {
      "citation_id": "29",
      "title": "Mec 2017: Multimodal emotion recognition challenge",
      "authors": [
        "Ya Li",
        "Jianhua Tao",
        "Björn Schuller",
        "Shiguang Shan",
        "Dongmei Jiang",
        "Jia Jia"
      ],
      "year": "2018",
      "venue": "2018 First Asian Conference on Affective Computing and Intelligent Interaction (ACII Asia)"
    },
    {
      "citation_id": "30",
      "title": "Dailydialog: A manually labelled multi-turn dialogue dataset",
      "authors": [
        "Yanran Li",
        "Hui Su",
        "Xiaoyu Shen",
        "Wenjie Li",
        "Ziqiang Cao",
        "Shuzi Niu"
      ],
      "year": "2017",
      "venue": "Dailydialog: A manually labelled multi-turn dialogue dataset",
      "arxiv": "arXiv:1710.03957"
    },
    {
      "citation_id": "31",
      "title": "An attentive rnn for emotion detection in conversations. Association for the Advancement of Artificial Intelligence",
      "authors": [
        "Majumder",
        "D Poria",
        "R Hazarika",
        "Mihalcea",
        "E Cambria Gelbukh",
        "Dialoguernn"
      ],
      "year": "2019",
      "venue": "An attentive rnn for emotion detection in conversations. Association for the Advancement of Artificial Intelligence"
    },
    {
      "citation_id": "32",
      "title": "Africa in the World and the World in Africa: Facts & Figures",
      "year": "2023",
      "venue": "Africa in the World and the World in Africa: Facts & Figures"
    },
    {
      "citation_id": "33",
      "title": "Resources for Self-Instructional Learners of Less Commonly Taught Languages",
      "authors": [
        "Angeline Peterson",
        "Danya Al-Saleh",
        "Sam Allen",
        "Alex Fochios",
        "Olivia Mulford",
        "Kaden Paulson-Smith",
        "Lauren Marino"
      ],
      "year": "2025",
      "venue": "Resources for Self-Instructional Learners of Less Commonly Taught Languages"
    },
    {
      "citation_id": "34",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "35",
      "title": "Navonil Majumder, Gautam Naik, Erik Cambria, and Rada Mihalcea",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "36",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "Soujanya Poria",
        "Navonil Majumder",
        "Rada Mihalcea",
        "Eduard Hovy"
      ],
      "year": "2019",
      "venue": "IEEE access"
    },
    {
      "citation_id": "37",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "year": "2022",
      "venue": "Robust speech recognition via large-scale weak supervision",
      "arxiv": "arXiv:2212.04356"
    },
    {
      "citation_id": "38",
      "title": "Avec 2018 workshop and challenge: Bipolar disorder and cross-cultural affect recognition",
      "authors": [
        "Fabien Ringeval",
        "Björn Schuller",
        "Michel Valstar",
        "Roddy Cowie",
        "Heysem Kaya",
        "Maximilian Schmitt",
        "Shahin Amiriparian",
        "Nicholas Cummins",
        "Denis Lalanne",
        "Adrien Michaud"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 on audio/visual emotion challenge and workshop"
    },
    {
      "citation_id": "39",
      "title": "Memor: A dataset for multimodal emotion reasoning in videos",
      "authors": [
        "Guangyao Shen",
        "Xin Wang",
        "Xuguang Duan",
        "Hongzhi Li",
        "Wenwu Zhu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference Multimedia, MM '20",
      "doi": "10.1145/3394171.3413909"
    },
    {
      "citation_id": "40",
      "title": "Dialogxl: All-in-one xlnet for multiparty conversation emotion recognition",
      "authors": [
        "Weizhou Shen",
        "Junqing Chen",
        "Xiaojun Quan",
        "Zhixian Xie"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "41",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Liang",
        "J Zico Kolter",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the conference. Association for computational linguistics. Meeting"
    },
    {
      "citation_id": "42",
      "title": "2024a. Beyond silent letters: Amplifying llms in emotion recognition with vocal nuances",
      "authors": [
        "Zehui Wu",
        "Ziwei Gong",
        "Lin Ai",
        "Pengyuan Shi",
        "Kaan Donbekci",
        "Julia Hirschberg"
      ],
      "venue": "2024a. Beyond silent letters: Amplifying llms in emotion recognition with vocal nuances",
      "arxiv": "arXiv:2407.21315"
    },
    {
      "citation_id": "43",
      "title": "2024b. Multimodal multi-loss fusion network for sentiment analysis",
      "authors": [
        "Zehui Wu",
        "Ziwei Gong",
        "Jaywon Koo",
        "Julia Hirschberg"
      ],
      "venue": "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/2024.naacl-long.197"
    },
    {
      "citation_id": "44",
      "title": "Bioserc: Integrating biography speakers supported by llms for erc tasks",
      "authors": [
        "Jieying Xue",
        "Minh-Phuong Nguyen",
        "Blake Matheny",
        "Le-Minh Nguyen"
      ],
      "year": "2024",
      "venue": "International Conference on Artificial Neural Networks"
    },
    {
      "citation_id": "45",
      "title": "Ch-sims: A chinese multimodal sentiment analysis dataset with fine-grained annotation of modality",
      "authors": [
        "Wenmeng Yu",
        "Hua Xu",
        "Fanyang Meng",
        "Yilin Zhu",
        "Yixiao Ma",
        "Jiele Wu",
        "Jiyun Zou",
        "Kaicheng Yang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "46",
      "title": "Multimodal language analysis in the wild: Cmumosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "47",
      "title": "Emotion detection on tv show transcripts with sequence-based convolutional neural networks",
      "authors": [
        "M Sayyed",
        "Jinho D Zahiri",
        "Choi"
      ],
      "year": "2018",
      "venue": "Workshops at the thirty-second aaai conference on artificial intelligence"
    },
    {
      "citation_id": "48",
      "title": "Modeling both context-and speaker-sensitive dependence for emotion detection in multi-speaker conversations",
      "authors": [
        "Dong Zhang",
        "Liangqing Wu",
        "Changlong Sun",
        "Shoushan Li",
        "Qiaoming Zhu",
        "Guodong Zhou"
      ],
      "year": "2019",
      "venue": "IJCAI"
    },
    {
      "citation_id": "49",
      "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
      "authors": [
        "Kaipeng Zhang",
        "Zhanpeng Zhang",
        "Zhifeng Li",
        "Yu Qiao"
      ],
      "year": "2016",
      "venue": "IEEE Signal Processing Letters",
      "doi": "10.1109/lsp.2016.2603342"
    },
    {
      "citation_id": "50",
      "title": "Dialoguellm: Context and emotion knowledge-tuned llama models for emotion recognition in conversations",
      "authors": [
        "Yazhou Zhang",
        "Mengyao Wang",
        "Prayag Tiwari",
        "Qiuchi Li",
        "Benyou Wang",
        "Jing Qin"
      ],
      "year": "2023",
      "venue": "Dialoguellm: Context and emotion knowledge-tuned llama models for emotion recognition in conversations",
      "arxiv": "arXiv:2310.11374"
    },
    {
      "citation_id": "51",
      "title": "Multi-modal multi-scene multilabel emotional dialogue database",
      "authors": [
        "Jinming Zhao",
        "Tenggan Zhang",
        "Jingwen Hu",
        "Yuchen Liu",
        "Qin Jin",
        "Xinchao Wang",
        "Haizhou Li"
      ],
      "venue": "Multi-modal multi-scene multilabel emotional dialogue database",
      "arxiv": "arXiv:2205.10237"
    },
    {
      "citation_id": "52",
      "title": "2022b. M3ED: Multi-modal multi-scene multi-label emotional dialogue database",
      "authors": [
        "Jinming Zhao",
        "Tenggan Zhang",
        "Jingwen Hu",
        "Yuchen Liu",
        "Qin Jin",
        "Xinchao Wang",
        "Haizhou Li"
      ],
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2022.acl-long.391"
    },
    {
      "citation_id": "53",
      "title": "Knowledge-enriched transformer for emotion detection in textual conversations",
      "authors": [
        "Peixiang Zhong",
        "Di Wang",
        "Chunyan Miao"
      ],
      "year": "2019",
      "venue": "Knowledge-enriched transformer for emotion detection in textual conversations",
      "arxiv": "arXiv:1909.10681"
    }
  ]
}