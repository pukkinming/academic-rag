{
  "paper_id": "2405.06728v2",
  "title": "Theradia Woz: An Ecological Corpus For Appraisal-Based Affect Research In Healthcare",
  "published": "2024-05-10T14:50:30Z",
  "authors": [
    "Hippolyte Fournier",
    "Sina Alisamir",
    "Safaa Azzakhnini",
    "Hanna Chainay",
    "Olivier Koenig",
    "Isabella Zsoldos",
    "Eléeonore Trân",
    "Gérard Bailly",
    "Frédéeric Elisei",
    "Béatrice Bouchot",
    "Brice Varini",
    "Patrick Constant",
    "Joan Fruitet",
    "Franck Tarpin-Bernard",
    "Solange Rossato",
    "François Portet",
    "Fabien Ringeval"
  ],
  "keywords": [
    "Ecological corpus",
    "Computerised Cognitive Training",
    "Appraisal theories",
    "Dimensional / Categorical Affect Recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We present THERADIA WoZ, an ecological corpus designed for audiovisual research on affect in healthcare. Two groups of senior individuals, consisting of 52 healthy participants and 9 individuals with Mild Cognitive Impairment (MCI), performed Computerised Cognitive Training (CCT) exercises while receiving support from a virtual assistant, tele-operated by a human in the role of a Wizard-of-Oz (WoZ). The audiovisual expressions produced by the participants were fully transcribed, and partially annotated based on dimensions derived from recent models of the appraisal theories, including novelty, intrinsic pleasantness, goal conduciveness, and coping. Additionally, the annotations included 23 affective labels drawn from the literature of achievement affects. We present the protocols used for the data collection, transcription, and annotation, along with a detailed analysis of the annotated dimensions and labels. Baseline methods and results for their automatic prediction are also presented. The corpus aims to serve as a valuable resource for researchers in affective computing, and is made available to both industry and academia.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "O VER the past decade, special attention has been given to the use of Artificial Intelligence (AI) technologies to improve healthcare. One of the rationales behind this craze is that the automation of tasks that do not necessarily require human intervention saves therapists time and mental load, resulting in more efficient practices and less burden for both patients and clinicians. However, recent reviews have evidenced that, even though AI technologies' performance can Fig.  1 . Head movements, gaze, speech, and articulation of a remote operator were captured in real-time to drive a virtual assistant as a Wizard-of-Oz  [5] .\n\nrival that of humans in certain specific tasks, such as serum analysis, or assessment of cardiovascular MRI images, they still struggle to deal with social interaction skills  [1] .\n\nWhile the definition of social interaction skills remains a subject of debate  [2] , they can be summarised as the ability of autonomous agents to maintain equilibrium in a dynamic relationship with another agent, resulting in the development of autonomous relationships  [3] . In the context of interactions between AI technology and patients in healthcare, it corresponds to the capabilities of systems to detect and respond appropriately to signals from patients that may disrupt the balance of interaction; e.g., patients may express that they feel misunderstood or unsatisfied. While verbally expressed requests are fairly well understood by today's AI technologies, it is still difficult for them to accurately detect patients' affective states from non-verbal signals  [4] .\n\nYet, the use of AI technologies for social interactions in healthcare would have far-reaching consequences. A prime illustration of their relevance lies in Computerised Cognitive Training (CCT), which aims to enhance or preserve cognitive functions in patients suffering from cognitive impairment through the repetition of exercises that target specific functions such as attention or memory  [6] . Although the effectiveness of CCT has been evidenced by meta-analyses  [6] ,  [7] , it seems to be conditioned by social interaction during the process  [8] , which requires the presence of the therapist. Thus, entrusting social interaction to AI technologies would be highly beneficial in making the process autonomous. To move in this direction, it is important to use meaningful data and reliable methods to accurately recognise patients' affective state in the context of CCT.\n\nIn this study, we present the THERADIA WoZ corpus, an ecological corpus specifically tailored for the audiovisual detection of affective states in the healthcare domain. The corpus was built to help the development of a virtual assistant supporting CCT sessions at home, for older healthy people and patients  [5] . To the best of our knowledge, this is the first audiovisual corpus of affect in healthcare that fully relies on models of appraisal theories, while being accessible to both industrial and academic research communities 1 .\n\nThe corpus data come from natural interactions in French involving healthy and Mild Cognitive Impairment (MCI) senior participants. The interactions revolve around CCT exercises facilitated by a virtual assistant, and operated remotely by a human acting as a Wizard-of-Oz (WoZ), cf. Figure  1 . Expressions of the participants were then fully transcribed and partially annotated based on the dimensions of recent appraisal theories models, along with labels derived from the literature of achievement affects.\n\nA comprehensive analysis of the annotated labels was carried out, enabling the identification of a core set comprising ten affective states in the context of AI technology-human interactions in healthcare. This core set of affective labels was then used along with the appraisal dimensions to evaluate the performance of inference models based on audio, visual, and textual data.\n\nThe remainder of this paper is organised as follows. We first review the appraisal theory and annotation methods of affect, along with the existing corpora in healthcare in section II. The protocol used for the construction of our data set is then presented with the material and the participants in section III. The segmentation and the annotation of the recordings are explained in section IV. A comprehensive analysis of the affective annotations is then reported in section V, and results obtained in their automatic prediction are finally reported in section VI, before concluding in section VII.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. State Of The Art",
      "text": "In this section, we first review the definition of affective states and their annotation methods through the lens of appraisal theories, and then provide a concise overview of corpora of affect available in healthcare.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Affective States In The Lens Of The Appraisal Theory",
      "text": "According to appraisal theories, affective phenomena are defined as multi-component processes aiming to maintain the well-being and survival of individuals in their environment  [9] . Roughly speaking, affective phenomena arise when the meaning of environmental stimuli are cognitively appraised as potentially threatening to an individual's equilibrium in their environment.\n\nThis appraisal process would be divided into four criteria: the relevance of the events for the individual's well-being and/or survival, their implications, the possible coping with these implications, and their normative significance, i.e., the degree of matching with individual's norms and values  [10] . 1 The URL will be provided for camera-ready Following this appraisal process, the activation of the organism's subsystems, i.e., physiological responses, action tendencies and motor expressions, would be synchronised to help individuals carry out the behaviours that contribute to the restoration of their homeostasis. In this perspective, an affective state serves as a marker reflecting how individuals appraise themselves between the plausible deregulation of their homeostasis caused by the environment, and the search for the behaviour leading to a return to equilibrium.\n\nThe cognitive appraisal process is supposed to be causal in the elicitation of affective episodes  [11] . This position assumes a necessary and sufficient relationship would link appraisal to the synchronisation of the organism's subsystems  [12] . In other words, a specific pattern of criteria of the appraisal process would trigger a specific synchronised activation of the organism's subsystems. Conversely, the observation of a specific synchronised activation would allow to predict the pattern of criteria of the appraisal process performed by the experimenter of the affective episode.\n\nBased on these premises, the Tripartite Emotion Expression and Perception model postulates that the way humans perceive the affect of others is conditioned by this causal relationship  [13] . The idea is that if an individual A is able to determine the affective state of an individual B, this is because A inferred -in part unconsciously -the cognitive appraisal process performed by B on the basis of the apparent motor expressions, e.g., facial expressions, that B has produced. In brief, appraisal theories propose to solve the problem of affective state differentiation based on a n-dimensional hyperplane representing the criteria of the appraisal process.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Differentiable Annotations Of Affective States",
      "text": "1) Dimensional representations: Therefore, one way of describing individuals' affective states would be to consider them as points on a plane. Based on this approach, most of the literature have used the Circumplex model to represent them, a Cartesian plane defined by two axes, namely valence -from unpleasant to pleasant -and arousal -from deactivation to activation  [14] . However, this approach drew a number of criticisms for its ability to differentiate affective states; fear and anger are both defined by high arousal and negative valence, and the concept of valence itself should be rather seen as a multidimensional representation of several dimensions  [15] . It has therefore been stated that more than two dimensions should be taken into account to appropriately distinguish affective states  [16] . Thus, appraisal theory represents a sound framework for selecting relevant dimensions in the context of affective state annotation.\n\n2) Categorical representations: Labels are another common way of approximating individual's affective states. One of the crucial points of such an approach is to select affective labels likely to be relevant in the context of interest. While basic emotion labels, i.e., fear, disgust, happiness, sadness, and anger are commonly used to model affective states  [17] , they are not necessarily the most appropriate in the context of healthcare and AI-patients interactions, as they refer to affect from an evolutionary perspective.\n\n3) Relation between categorical and dimensional representations: Appraisal theories suggest that the use of labels represents a way for humans to characterise the conscious part of the activation states of the organism's subsystems, i.e., of the cognitive appraisal process, the physiological responses, the action tendencies, and the motor expressions  [18] , assuming that each affective label would represent a specific pattern of criteria of the appraisal process  [13] ,  [19] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "C. Affect Corpora For Healthcare",
      "text": "Automatic affect analysis can be applied to various aspects of healthcare  [20] : to support clinicians for the diagnosis of affective disorders, to facilitate the monitoring of patients' emotions during the therapy  [5] , which is the focus of this study, or between therapy sessions.\n\nWhereas numerous audiovisual corpora are available in the literature for affect recognition in human interactions  [21] , few of them were acquired in the context of healthcare. There are several reasons for this gap in the literature. One of them concerns the sensitive nature of the data that are protected by law and cannot be easily released. Moreover, populations related to healthcare are often fragile, and setting up experimental protocols to contrast affect corpora can be challenging because of their cognitive fatigue.\n\nNevertheless, there have been initiatives in the construction of affective corpora supporting clinicians during therapy. For instance, speech interactions between seniors and a virtual assistant playing the role of a health professional coach, were collected for the purpose of the EMPATHIC project  [22] , which aims to help the elderly maintain their independence as they age. Authors collected ∼13k speech segments (∼13h in total) from 136 participants over 64 years and from different countries (Spain, France, and Norway), that were further annotated by three annotators according to five labels: calm, happy, puzzled, tense, and sad, and three dimensions: arousal, valence, and dominance.\n\nAnother corpus of affect has recently been collected for children with developmental disorders using a mobile therapeutic game  [23] . The corpus contains ∼2k video sequences of 456 children from all over the world and was annotated by 11 individuals according to seven labels: happy, surprised, sad, fearful, angry, disgust, and neutral.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. Data Collection",
      "text": "The whole data collection protocol of this study has been documented in detail, reviewed, and approved by the Ethics Committee for Research in Grenoble Alpes with the reference: CERGA-Avis-2021-1. As the experiment lasted around three hours on average, and participants were asked to consent for the commercial exploitation of their data under the protection of the European General Data Protection Regulation laws, a 20 C gift card was awarded. Participants could ask for reimbursement of travel expenses, i.e., including public transport fares or petrol costs, for both the journey to and from the experiment site. The protocol used to collect data for the THERADIA WoZ corpus is detailed below. Fig.  2 . Office of the experimenter operating the virtual assistant. The dialog was fully scripted and overlaid onto the participant's video (A). The participant's screen (B) was only visible during the exercise completion, and the virtual assistant was visible to the participant only outside the exercise completion. The virtual assistant's rendering was consistently displayed (C) to monitor any tracking issue.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Participants",
      "text": "A total of 52 healthy senior participants (40 females; mean age = 67.9, SD age = 5.1) were recruited based on advertisements published in regional newspapers such as \"Le Progrès\" and \"Le Dauphiné Libéré\". Additionnaly, 9 participants diagnosed with MCI (two females; mean age = 75.8, SD age = 2.1) were recruited via clinicians collaborating on the project. Inclusion criteria included fluency in French, either normal or corrected-to-normal vision and hearing, and informed consent prior to recordings. Regarding the level of education, 40% of healthy senior participants had a master's degree or higher, 33% a bachelor's degree and 27% a baccalaureate. As for MCI participants, 22% had obtained a master's degree or higher, 22% a bachelor's degree, and 56% a National Vocational Qualification (French CAP), or lower.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Cct Supported By A Virtual Assistant",
      "text": "All participants carried out a CCT session, which was based on eight exercises selected from the HappyNeuronPro CCT platform. The exercises involved different cognitive functions such as memory, language, attention, and planning; details on these exercises are provided in section I.A of the supplementary material.\n\nThe CCT was supported by a virtual assistant, developed by the Dynamixyz company, and controlled by one coauthor of this study acting as a WoZ, cf. Figure  2 . More specifically, head movements, gaze, speech, and articulation of the experimenter were captured in real-time to drive the 3D virtual assistant whose rendering was cast to the participant's screen. The speech was based on a dialogue tree designed to cover a wide range of interactions, envisioned prior to the study. If none of the planned branches made sense within a given dialogue, free intervention was allowed. Eventually, the dialogue tree was updated if the unplanned interaction was deemed relevant. Technical problems that could not be solved by the operator of the virtual assistant were addressed by a second experimenter, who was sitting outside the room for the healthy participants, and inside the room for the MCI participants.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Material",
      "text": "Questionnaires were used to interrogate various psychological, affective and cognitive dimensions of the participants, as well as to evaluate the virtual assistant and the CCT  [24] . Experiments were carried out in two quiet rooms of the EMC laboratory of Université Lyon 2, one for the participant, and one for the experimenter.\n\nAs the original lighting conditions did not allow a robust tracking of the operator of the virtual assistant, we equipped each room with an additional continuous light (Amzdeal Softbox) that provided a color temperature closed to daylight (5500K). Each room was equipped with an high-end computer, comprising a 24 inches monitor, a webcam (Logitech streamcam), a headset with a built-in unidirectional microphone (Sound blasterX H6), that was plugged to an external sound card (Presonus Audiobox), for both audio recording and streaming to the CCT application.\n\nAs pre-tests revealed random freezes in the video recordings that were not apparent during the CCT session, we used an iPhone X to provide a stable video recording on the participant's side, along with an additional audio recording captured by a far-field omnidirectional microphone. Audio data were sampled at 44.1 kHz with 16 bits, whereas video data were sampled at a constant frame rate of 30 fps with a resolution of 1920x1080 and the YUV420p color format.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Procedure",
      "text": "Concerning the MCI participants, the session began with the completion of the consent form and the above-mentioned questionnaires. Then, after a few instructions, the experimenter left the room so that the CCT session could start. It comprised the realisation of four exercises, each performed twice. The difficulty level of the second exercise was adjusted based on the performance of the first. It was increased in the case of success and decreased in the case of difficulty. Throughout the session, participants were guided and engaged with the virtual assistant under the impression it was operated using AI technology, whereas it was actually operated by a human as a WoZ. The story line of the session started with a welcome dialogue designed to acquaint the participant with the virtual assistant, and to collect some information such as the participant's mood and motivation. This was followed by the exercise phase divided into five parts: the introductory dialogue detailing each exercise, the exercise completion, the feedback dialogue commenting on participant's performance, the repetition of the exercise, as well as the feedback dialogue, and the closing dialogue, which ended the session. Once the CCT session was terminated, the whole set-up was revealed.\n\nConcerning the healthy senior participants, the procedure was the same, except for the following: First, a session consisted of eight exercises instead of four. Second, healthy senior participants were divided into five groups. Twenty of them were assigned to a group without induction. The special feature of this group was that it carried out two CCT sessions, one week apart. This resulted in the set-up being revealed at the end of the second session. The remaining 32 healthy senior participants were divided into four induction groups, designed to increase the chances of observing affectivelycharged expressions. To this end, induction was used in one out of two of the eight exercises based on the combination of three parameters: (i) presenting the exercise as easy or hard beforehand; (ii) actually setting the difficulty as easy or hard; and (iii) giving feedback from the WoZ as critical or congratulatory. Details on the groups and the inductions are reported in section I.B of the supplementary material.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Data Segmentation And Annotation",
      "text": "We collected about 80 hours of data from the 52 healthy older partipants, and about 6 hours of data from the 9 MCI participants. We describe in this section the methods used for the segmentation, transcription, and annotation of this data.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Segmentation, Transcription And Selection Of Sequences",
      "text": "The segmentation of the recordings was based on the pragmatic completeness in breath groups, i.e., groups of words that are delimited by pauses taken for breathing, ensuring they formed a single proposition if they conveyed the same semantic information, or separate propositions if they related to different types of semantic information  [25] . Non-verbal expressions occurring within or proximate to a proposition were marked by a diacritic, and those occurring at distance were specifically segmented. Temporal markers for proposition start and stop were both positioned at a certain distance to allow a sufficient observation space for the annotation of affect.\n\nThe transcription relied mostly on the ESLO project principles  [26] , respecting spelling and spoken structures; details regarding our conventions are reported in section II.A of the supplementary material. Recordings were transcribed with the ELAN software  [27]  by five Master students in Language Engineering at the Université Grenoble Alpes with a two-steps process: an initial stage of segmentation and transcription, followed by a subsequent verification stage conducted by another student. The segmented sequences were selected to retain only those containing expressions of affect, and a fraction was randomly removed to reduce the overall duration. Obtained sequences were then all reviewed and some were further removed to reduce redundancy. Statistics of the sequences are given in table I.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Selection Of Affective Labels And Dimensions",
      "text": "The context of CCT aligns closely with the literature on achievement affects  [28] -  [30] , which considers the following types of affect:\n\n(i) activity affects generated by the task achievement itself;\n\n(ii) outcome affects generated by the success or failure results, whether retrospective or prospective; (iii) epistemic affects generated by a form of metacognition on the perception of the achievement; (iv) social affects generated by another individual.\n\nThese affects are particularly relevant in the context of AI-assisted healthcare, as they comprehensively cover those that can be experienced during CCT, i.e., those generated by the CCT itself (activity affects), those generated by past or future CCT failure or success (outcome affects), those generated by the perception of the cognitive abilities involved in CCT (epistemic affects), and those generated by AI-patients interactions (social affects). Our categorical representations of affect relied on the following 23 labels: angry, annoyed, anxious, ashamed, confident, contemptuous, curious, desperate, disappointed, embarrassed, excited, frustrated, interested, guilty, happy, hopeful, impatient, proud, relaxed, sad, satisfied, surprised, and upset.\n\nThe dimensions consisted of the four most important appraisal dimensions in the context of affect recognition  [13] ,  [19] ,  [31] ,  [32] , namely: novelty, intrinsic pleasantness, goal conduciveness, and coping, plus arousal. In the context of appraisal theory, arousal is a marker of the depth of the appraisal process, which is relevant to qualify the intensity of the affective state.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "C. Annotation Tool And Guidelines",
      "text": "We took benefit from previous collaborations with the company Viadialog 2 engaged in the development of a web-based annotation tool originally tailored for audio data: ANNOT. This tool has been adapted to the needs of our project and is available online 3 . The platform allowed annotators -here could annotate as many labels as desired, or none at all if the sequence lacked any particular affect. Further details on the annotations steps are reported in section II.C of the supplementary material.\n\nBefore starting, annotators participated in a training session with the project investigators, including an expert in appraisal theories of emotion. The purpose of this session was to familiarise the annotators with the concepts of appraisal theories, i.e., the meaning of dimensions and affective labels, and to explain in detail the guidelines for the annotation process. Annotators were instructed to put themselves in the participant's shoes to assess the emotional expression conveyed in the selected sequences, considering factors such as the chosen words, their oral delivery, and the expressive dynamics of the face and body. It was emphasised that there were no right or wrong answers, as the task was inherently subjective. Additionally, they were instructed to perform the task in fullscreen mode, in a quiet environment, with sessions lasting no longer than one hour, followed by ten-minute breaks. The annotation process was continuously monitored through an online tracking table, updated daily by the annotators, with additional communication via email or phone as needed. After the training session, each annotator received a document summarising all these points, including definitions of dimensions and labels. Each dimension was defined as follows:\n\n• Novelty: \"To what extent does the participant feel that what is happening is predictable or unexpected?\"; ranging from predictable to unexpected.\n\n• Intrinsic pleasantness: \"At what point does the participant express pleasure or displeasure?\"; ranging from unpleasantness to pleasantness.\n\n• Goal conduciveness: \"How much does what is happening seems to correspond to the participant's wishes?\"; ranging from not at all to absolutely.\n\n• Coping: \"How well does the participant seem able to cope with what is happening?\"; ranging from not at all to absolutely.\n\n• Arousal: \"How asleep or awake does the participant seem?\"; ranging from asleep to awake.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "V. Corpus Analysis",
      "text": "This section provides a comprehensive statistical analysis of the annotation data. We first assessed the annotation frequency of the affective labels as well as the inter-annotator agreement on their intensity. We then evaluated the cognitive delay involved in the annotation of each dimension, as well as the inter-annotator agreement. Relationships between dimensions and affective labels were also estimated, and a label core set was finally selected, based on compliance with consistency criteria relating to each of the analyses mentioned above.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Categorical Annotations",
      "text": "The frequency at which the affective labels were selected by at least one, two, or three annotators is reported in Figure  4 . Based on the sample of annotation frequency per label for at least one annotator, Cohen's d was calculated to identify the theoretical value at which the effect size of its deviation from the sample mean can be considered at least \"small\", i.e., Cohen's d > 0.2  [33] ; this theoretical value was equal to 747 sequences. The annotation frequency per label for at least one annotator is reported in Table  II , and shown in bold when above this theoretical value.\n\nWe evaluated the inter-annotator agreement on the presence of each affect label, i.e., when the intensity of a label was superior to zero, for each pair of annotators, and averaged over all pairs, by computing the Unweighted Average Recall (UAR) as follows:\n\nwhere k is the total number of classes, which corresponds here to the presence or absence of a label. N i is the total number of annotated sequences for a given affect label, and N i c is the number of commonly identified classes, whether present or absent. Results are reported in Table  II . According to a twotailored z-test, the inter-annotator agreement would be defined as statistically above chance if it would exceed 53%, which is the case for all the labels excepted embarrassed.\n\nWe further evaluated the inter-rater agreement on the intensity of the affective labels using the Pearson's Correlation Coefficient (PCC). The inter-rater agreement was defined as sufficiently high if its Cohen's d, i.e., its PCC value divided by the sample standard deviation, is superior to .2  [33] , which corresponds to a PCC greater than .065. This concerned most of the affective labels, which are reported in bold style.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Dimensional Annotations",
      "text": "The cognitive delay in the continuous annotation of each dimension was evaluated, and results showed that goal conduciveness (3.3 seconds) had a significantly longer delay than the other dimensions (less than 2 seconds), suggesting that annotators had more difficulty assessing it. This evaluation is detailed in section III.A of the supplementary material.\n\nWe assessed the inter-annotator agreement on the dimensions for both time-continuous and summary annotations, using the PCC, which was computed either on each sequence, or each CCT session, cf. Table  III . An high agreement was observed on the dimensions of coping, goal conduciveness, and intrinsic pleasantness, with a more pronounced consistency observed in summary annotations compared to time-continuous annotations, which is expected since time-continuous data imply a greater variation in the possible outcomes. A lower agreement was however found for arousal and novelty, which might be explained by several reasons as detailed below.\n\nThe novelty dimension was assumed to be more difficult to annotate, as it probably concerned very specific cases of our study, e.g., unplanned feedback from the virtual assistant on the participant's performance, occurring in fewer sequences than the other dimensions. This assumption was corroborated by an analysis evidencing that the frequency of high-value annotations was lower for novelty compared to other dimensions; for a high-value threshold set at .85, the proportion of high-value annotations for novelty accounted for only .13% of the time-continuous annotation data, whereas other dimensions were at least twice as frequent: .30% for arousal, .24% for intrinsic pleasantness, .62% for goal conduciveness, and .36% for coping. A similar trend was observed in summary annotations, with the occurrence of high-value annotations for novelty accounting to .26%, whereas all other dimensions were twice as prevalent; arousal: .92%, intrinsic pleasantness: .52%, goal conduciveness: 1.35%, and coping: 2.16%. The arousal dimension was thought to be more difficult to annotate, as the context of the study, i.e. exercising on a computer and conversing with a virtual assistant, is likely to provide little variation for this dimension. This assumption is supported by the fact that the variance of arousal dimension was systematically among the lowest, which was observed for both time-continuous annotations -arousal: .010, novelty: .022, intrinsic pleasantness: .009, goal conduciveness: .017, coping: .020 -, and summary annotations -arousal: .008, novelty: .026, intrinsic pleasantness: .012, goal conduciveness: .021, coping: .027.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "C. Relation Between Dimensions And Affective Labels",
      "text": "Further analysis of annotator agreement was conducted by exploring the relationship between summary values of appraisal dimensions and affective labels of each annotator separately. Specifically, the PCC was calculated between each pair of appraisal dimensions and affective labels for all sequences in which the value of the label concerned differed from zero; cf. section III.B of the supplementary material for a comprehensive view of the heatmap of each annotator. A correlation was considered consistent if it was statistically significant and in the same direction for at least two annotators. The average of consistent correlations between annotators are reported in Figure  5 . The labels that are consistently correlated with at least one appraisal dimension are marked with \"✓\" in the Table  II .\n\nBroadly speaking, our results are in line with the predictions of appraisal theories regarding the link between affective labels and dimensions: positive affective labels are correlated with the ability to cope, goal conduciveness, and pleasantness, while negative affective labels are correlated with disability to cope, goal obstructiveness, and unpleasantness. Some of our results are, however, unexpected. The anger affective label is correlated with an absence of coping, whereas we would expect the opposite  [34] , as it generally refers to a situation in which an event is appraised as goal obstructive, but individuals appraise themselves capable of coping with it, e.g., being mugged in the street but wanting to defend themselves. In the context of CCT, where participants sit in front of a computer, different forms of anger may likely occur, probably involving an inability to cope. As expected, the affective label of surprise is highly correlated with novelty. Even though the surprise affective label is generally considered to be positive or negative depending on the context, it seems to be rather negative in our results as it is correlated with goal obstructiveness, and no coping. This could be explained by the fact that the occurrence of unexpected situations were more related to negative events such as unexpected negative feedback from the virtual assistant.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "D. Label Core Set Selection",
      "text": "A core set of the likely most relevant affective labels in the context of AI-assisted CCT was defined based on our analyses. Four main criteria of selection were used: (i) the frequency is significantly higher than the average on all affective labels; (ii) the agreement on presence is significantly higher than chance; (iii) the agreement on intensity is sufficiently high; and (iv) there is a significant correlation, in the same direction, with at least one dimension for at least two annotators. Ten affective labels meet these four criteria, five positive and five negative, cf. Table  II .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Vi. Automatic Recognition Of Affective States",
      "text": "In this section, we report on the automatic prediction of affective labels and dimensions. An overview of the experiments performed in this section is given in Figure  6 . In brief, we extract different types of representations (expert or deep) from audio, textual, and visual data, and train specific models for the prediction of the affective labels and dimensions.\n\nAll experiments were conducted using a single partitioning of the dataset, divided into three subsets: training, validation and test, comprising 56 %, 23 % and 21 % of the participants respectively; see Table  IV . The partitions were carefully designed to minimise bias while maintaining key statistical properties, including: (i) the gender ratio (female vs. male); (ii) the proportion of healthy vs. MCI participants; (iii) the distribution of participants across affective induction conditions; (iv) the spectrum of education levels; and (v) the distribution of label presence, label intensity, and dimensional intensity. Heatmaps depicting these distributions are available in section IV of the supplementary material.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A. Representations Of Audio, Textual And Video Data",
      "text": "We included both expert and deep representations of audio, visual, and textual 4  data to evaluate their respective contribution in the analysis of affect. Expert based features rely on the statistical description of perceptually salient patterns in the data, and involve relatively light-weight computational resources, whereas deep representations exploit self-supervised learning (SSL) methods on large scale datasets  [35] .\n\n1) Expert based representations: We used the Mel-scale Filter Banks (MFBs)  [36] , Term Frequency -Inverse Document Frequency (TF-IDF)  [37] , and Facial Action Units (FAUs)  [38] , as representations of the audio, textual, and visual data, respectively.\n\nMFBs are popular features in acoustic processing. They are based on a simple signal processing pipeline which consists in taking the Fourier transform of the acoustic signal through a sliding window (25 ms shifted forward in time every 10 ms), and then mapping the power spectrum to a mel-scale (80 filter banks), which is a non-linear perceptual scale based on the human auditory system. MFBs were extracted using the SpeechBrain toolkit  [39] .\n\nThe TF-IDF is a classical feature in text processing, capturing the frequency of a word within a document (TF), relative to its rarity across a given set of documents (IDF). The method therefore tokenises the words of a sentence, which is represented by a vector of the count of the words it contains, and further normalised by the term frequency across the entire training partition of the dataset.\n\nFAUs describe the intensity of movements of facial muscles related to an apparent facial expression, and are commonly used as facial descriptors in affective computing. FAUs were extracted using OpenFace  [40] , which provides estimations of the intensity of a set of 17 FAUs with a framerate of 30 Hz.\n\n2) Self-supervised representations: Deep representations of audio, textual, and video data include the following models: Wav2Vec2  [41] , BERT  [42] , and CLIP  [43] , respectively.\n\nWe used a multilingual Wav2Vec2 model that exploits both convolutional and transformer layers to distinguish positive and negative samples of speech in a contrastive approach. The model  5  we used has a large architecture, and was trained on 56 different languages  [44] , including French, and then fine-tuned for ASR.\n\nTransformer-based representations such as BERT are popular in text processing, as they can encode the context of the words, which is crucial for affect related tasks  [45] ,  [46] . We used a BERT model 6  fine-tuned for sentiment analysis on six languages (English, Dutch, German, French, Spanish, and Italian), and with the base architecture.\n\nCLIP  7  is a model developed by OpenAI  [43]  and trained on a large dataset that includes both images and their corresponding textual descriptions allowing the model to learn the semantic relationship between visual and textual information. The architecture consists of image and text encoders that encode the input image and text into fixed-length embeddings using a contrastive learning mechanism.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "B. Models And Training Strategies",
      "text": "The aforementioned multimodal representations were used to model the affective labels and dimensions. We evaluated the three following predictive models for the labels intensity and dimensions summaries:\n\n1) Linear: One linear layer on top of the representations to map the features to the desired number of outputs. 2) Multi-Layer Perceptron (MLP): The linear layer as above, combined with a hidden linear layer whose size was the number of features divided by two. 3) Gated Recurrent Units (GRU): A GRU model with the hidden size defined to be the feature size divided by two. Outputs of the Linear and MLP models were averaged over the sequence, whereas the last output of the GRU model was used to represent the output of the whole sequence. Preliminary results obtained in the prediction of the affective labels showed that the performance of the MLP model was consistently better than the performance of Linear and GRU models, both for the classification and regression tasks.\n\nFor dimension summaries, the model was trained to predict the five dimensions altogether in a multi-task approach. Our first experiments revealed that the Linear model consistently outperformed both the MLP and GRU models.\n\nTo predict the time-continuous annotations of the dimensions, it was necessary to resample certain features to align with the annotation frame rate of 10 Hz. Audio representations, sampled at 100 Hz for MFBs, and 50 Hz for Wav2vec2, were averaged into 10 ms chunks to match the annotation frequency.\n\nThe same strategy was applied to the FAUs and CLIP visual representations. For textual representations, we utilised BERT exclusively, as TF-IDF provides a single feature vector for the entire sequence. Textual features were aligned to the sequence length using spline interpolation, ensuring a smooth and continuous representation of the features over time. Since the Linear and MLP models did not perform well in predicting time-continuous dimensions, we explored various GRU model architectures:\n\n1) One-Layer GRU: A single-layered GRU with a hidden size set to either half the input feature size or equal to the input feature size. 2) Two-Layers GRU: Similar to the One-Layer GRU but with two hidden layers. 3) Three-Layers GRU: Similar to before, but with three hidden layers. Additionally, we tested a fixed set of hidden sizes across all GRU models: 128, 256, and 512. The results indicated that a GRU model with three hidden layers, each matching the input size, performed optimally for deep representations. In contrast, the model with a hidden size of 256 produced the best results for hand-crafted representations.\n\nFor multimodal fusion, we explored both middle and late fusion techniques to address the issue of asynchronous inputs. The best performing approach for both labels and dimension summaries was decision-based fusion using an Ordinary Least Squares (OLS) regression model  8  . For time-continuous dimensions, fusion was achieved through a GRU functioning as a weighted averaging layer, effectively capturing the temporal dynamics of the features. The GRU weights were optimised on the training set.\n\nAll the models were trained with the Adam optimizer and an initial learning rate of 10 -3 for affective labels and 10 -4 for dimensions. The batch size and gradient accumulation were set to 10 for labels and 1 for dimensions. The maximum number of training epochs was set to 50 and an early stopping strategy was used with five epochs. The models were trained using the Mean Squared Error (MSE) as the loss function, and for evaluation, the Concordance Correlation Coefficient (CCC) was calculated on all sequences  [47] . All experiments were carried out using Pytorch 9    [48] , with random seeds set to zero. The computer's operating system was Debian GNU/Linux 10, and the GPU used to train the models was an NVIDIA Quadro RTX 6000 with 23 gigabytes of memory, CUDA version 11.3.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "C. Affect Prediction For The Core Set Labels",
      "text": "The results for label intensity predictions, presented in Table  V , reveal some strong dependencies between modalities and affective labels; see section IV.B of the supplementary material for additional details on prediction bias related to population demographics. For instance, the label \"happy\" was most accurately assessed via the video modality, \"interested\" through the textual modality, and \"frustrated\" and \"surprised\" using the audio modality with deep representations. The choice between expert-crafted and deep features also introduced notable differences in the results according to the modality. For example, the label \"frustrated\" achieved a significantly higher CCC (.327) with TF-IDF features compared to MFB (.151), whereas the reverse trend was observed when using SSL representations. Overall, deep representations across textual, audio, and video modalities consistently outperformed their hand-crafted counterparts on average. The fusion of the different modalities yielded the best results on average and for most affective labels. An analysis of the fusion weights revealed that the importance of each modality varied significantly based on its representation. For instance, when using expert features, TF-IDF had a substantially greater influence on the results, with an averaged fusion coefficient of 0.8, compared to 0.1 for audio (MFBs) and video (FAUs). However, with deep representations, the contributions of audio and video surpassed that of text, with average coefficients of .28, .35, and .35 for text, audio, and video respectively.\n\nAn affective label was considered present in a given sequence if at least one annotator reported an intensity value for that label. To predict these labels, we explored two modeling approaches: training a binary classification model for each affective label or deriving a threshold from the regression models. Since the binary classification approach yielded significantly poorer results compared to the thresholded-based decision from regression models, we opted for the latter in our experiments.\n\nThe results, presented in Table  VI , indicate some dependencies between the best-performing modality and the affective labels, although these differences are less pronounced than in previous experiments. Overall, no significant performance gap was observed between expert and deep representations across modalities, with the exception of the audio modality. Additionally, fusion consistently enhanced performance across most scenarios.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "D. Dimensions Summaries Prediction",
      "text": "Results are presented in table VII for two different gold standards: the average of annotations and the median; see section IV.B of the supplementary material for further details on prediction bias related to population demographics. Comparing these gold standards, the average of annotations consistently yielded the best performance across all five dimensions. In terms of individual dimensions, arousal was best predicted by the audio modality, with no significant improvement observed from fusion with the other modalities.\n\nIt also had the lowest scores among the dimensions, which might be explained by the factors discussed in Section V-C.\n\nCoping was the most accurately predicted dimension, with all modalities performing similarly well and demonstrating complementary effects during fusion. Goal conduciveness was best predicted by textual representations, with fusion incorporating audio and video modalities further improving performance. Intrinsic pleasantness was most effectively captured by the video modality, with textual representations proving relevant only when computed over the entire sequence using TF-IDF. Deep audio representations also performed comparably well. Finaly, novelty was particularly challenging for the video modality, which yielded extremely low scores, while textual representations proved to be more effective in capturing this dimension.\n\nBy analysing the relative importance of each modality based on the coefficients from the linear regression model, we found that audio features contributed very little to predicting arousal, especially when using expert features. This could explain the drop in performance observed with the multimodal approach, compared to using the audio modality alone. For the others dimensions, the contributions of each modality were influenced by the complexity of the representations. When using deep features, the audio modality emerged as the most influential. However, this contribution was diminished when expert features were used, with textual information playing a more significant role.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "E. Continuous Dimensions Prediction",
      "text": "Results are presented in Table  VIII  for three gold standards: the average and the median of the time-continous annotations, and a version based on the Evaluator Weighted Estimator (EWE), which calculates a weighted mean of the annotations using the average cross-CCC between each rater's scores as weights  [49] ; for further details on prediction bias based on population demographics, see section IV.B of the supplementary material. Overall, the EWE approach consistently outperformed the others two gold standards across the five dimensions.\n\nArousal was the best predicted dimension, but this was primarily due to the low variance observed within this dimension, as explained in the previous section. For coping, goal conduciveness and novelty, Wav2vec2 audio representations outperformed textual features, which contrasts with the findings on the summarised dimensions. This discrepancy may be attributed to the fact that textual features provide a static representation, which may not capture the continuous variations within the data. Regarding intrinsic pleasantness, the three modalities performed similarly and proved complementary during fusion, which is consistent with the previous section's findings. Additionally, the multimodal approach consistently improved performance. An analysis of the fusion weights revealed that when using the mean of annotations as the gold standard, the model evenly distributed importance across all modalities. However, when using the median or EWE gold standards, the audio modality emerged as the most significant, while text and video contributed similarly. An examination of the weights learned by the GRU during fusion showed that when using the mean of annotations as the gold standard, the model assigned equal importance to all modalities. In contrast, with the median and EWE gold standards, the audio modality became the most influential, while text and video modalities contributed similarly to the overall predictions.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Vii. Conclusion",
      "text": "This paper presents the THERADIA WoZ corpus, a valuable multimodal dataset for affective computing research in healthcare. Based on current affective science and appraisal theories, the corpus includes 39.5 hours of multimodal data from 52 healthy older participants and 9 with MCI performing WoZassisted cognitive training. To capture affective episodes, affect inductions were applied to 32 healthy older participants. The data is fully transcribed and partially annotated across four appraisal dimensions -novelty, intrinsic pleasantness, goal conduciveness, and coping -as well as arousal, and with 23 achievement-related affect labels. These dimensions, provided in both in continuous time and summarised forms, highlights ten core affective labels of significant relevance for AI-assisted CCT applications. In-depth baseline experiments were conducted to model affect using text, audio, and video modalities, including investigations of multimodal fusion through both computationally efficient feature and contextual deep learning representations. Results revealed key dependencies between modalities and affect representations. This work provides new insights into the field of affect recognition, offering novel insights into affect recognition in healthcare.\n\nThis work contributes to the ongoing development of affective computing in interaction -and specifically for healthcareproviding a valuable dataset for benchmark comparisons. We believe that combining the strengths of affective computing and psychology, as demonstrated in this multidisciplinary study, will significantly advance our understanding of affect and its modeling, benefiting both fields and their applications.\n\nTHERADIA WoZ: An Ecological Corpus for Appraisal based Affect Research in Healthcare Supplementary material\n\nThe supplementary material provides details for specific subsections of the main manuscript. The section and subsection names in this document correspond to those in the main manuscript to facilitate understanding, parts are being referred to, except for the last section, which specifically addresses bias and fairness in affect predictions.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "I. Data Collection",
      "text": "",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "A. Cct Supported By A Virtual Assistant",
      "text": "All participants completed a CCT session comprising eight exercises selected from the HappyNeuronPro CCT platform  1  . Figure  1  illustrates an on-screen trial of these exercises, which were designed to train various cognitive abilities, such as memory (e.g., visual, verbal), planning, reasoning, mental arithmetic, and grammatical logic.   end of virtual assistant interactions, as well as the presence of non-verbal information.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "B. Data Annonymisation",
      "text": "In the public version of the dataset, participants' privacy has been protected by removing personal identifiers, including participants' names, ages, cities, family backgrounds, and occupations. We used regular expressions to detect these identifiers from sequences transcriptions. Name and location patterns were detected based on capitalised words, age patterns look for numerical numbers followed by \"years\", and family and job patterns search for a list of keywords that might indicate someone's job, such as \"work\", \"occupation\", \"retired\", or words related to family relationships, such as \"son\", \"daughter\", or \"child/children\". Any audio segments that contained private information were carefully removed, and corresponding intervals in the complete audio files were replaced by silence. Similarly, for the complete video file, frames containing private information have been replaced with a black screen.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "C. Data Annotation",
      "text": "The order of annotation of the dimensions was fixed for all annotators according to the following order: novelty, intrinsic pleasantness, goal conduciveness, and coping. The choice of a fixed order was motivated by two reasons. First, annotating appraisal dimensions requires high-level cognitive processing, as annotators must clearly have the meaning of each dimension in mind during annotation. Since this process is demanding and sustained concentration is critical for high-quality annotations, we aimed to alleviate the burden on annotators by keeping the order consistent. Second, the Component Process Model, from which the appraisal dimensions were borrowed, assumes that emotion emerges based on a cumulative and temporally fixed sequence  [1] ,  [2] . This fixed temporal sequence suggests that the dimensions are assessed in a specific order, reflecting their level of cognitive processing from lowest to highest: novelty, intrinsic pleasantness, goal conduciveness, and coping. Higher-level appraisals are thought to take longer to process than lower-level ones, and may depend on prior assessments of lower-level dimensions-for example, coping with a situation's consequences may depend on whether the situation is goal conducive. Ultimately, it is the cumulative result of all dimensions that enables the verbalisation of the situation with an emotional label. Thus, presenting a fixed order of dimensions, followed by emotional labels, was both a theoretical and methodological choice. While this approach may introduce certain effects, it likely enhances the robustness of the data.\n\nTo ease the time-continuous annotation process, the position of the mouse was freeze once the video was played, and movements of the mouse were only applied to the annotation cursor which was always located at the middle of the scale at the start. At the end of the time-continuous annotation, a graph showing the dynamics of the dimension over time was presented to the annotators, who could then validate or perform the annotation again as shown in Figure  2 . After the validation, annotators were asked to provide a summary value for the whole sequence as depicted in the Figure  3 . Finally, once all dimensions were annotated, annotators were asked to indicate if one or more labels could describe the affect they had recognised in the sequence and provide an intensity value for them as illustrated in the Figure  4 . Labels were presented in different parts of the screen according to their valence. The order of the positive labels, and negative labels, were defined randomly for each annotator. In order to reduce the number of operations to be effected by the annotators to provide their rating, the annotation bar was displayed when a click was maintained on an annotation label, and vertical displacements of the mouse allowed to change the intensity value.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "Iii. Corpus Analysis",
      "text": "",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "A. Dimensional Annotations",
      "text": "To assess the difficulty of annotating each dimension, we evaluated the 'cognitive delay' associated with their annotation. We measured this delay by calculating the time interval between the affective event apex and the apex of the continuous time annotation curve for each dimension and participant, cf. Figure  5 . After performing it on several selected extracts, average of difference time between annotators within each selected extract for each dimension was calculated. Extracts were selected on the basis of whether they described a bell-shaped curve, facilitating the apex identifications. The cognitive delay between dimension annotations was close, with the exception of the goal conduciveness annotation, for which the cognitive delay was longer. Results are reported in Table  II .",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "B. Relation Between Dimensions And Affective Label",
      "text": "The Figure  6  presents a heatmap of the associations between affective labels and dimensions (summary) for each of the six annotators. Each cell of the heatmap corresponds to the value of the PCC calculated between a specific dimension and an affective label. Analysing the six heatmaps, we observe varying degrees of agreement in the correlations between appraisal dimensions and affective labels across the six annotators. For instance, no significant correlation with any dimension was obtained for the labels 'ashamed', 'sad', 'proud', and 'upset'. On the opposite, the affective label 'surprise' shows consistently a strong correlations with the dimension of novelty for all six annotators, which indicates a reliable association. Therefore, as explained in section V.C of the paper, affective  labels that do not demonstrate significant correlations with appraisal dimensions for at least two annotators is considered one of the key selection criteria for excluding labels from the core set.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Iv. Automatic Recognition Of Affective States",
      "text": "For data partitioning, we have ensured that the distribution of emotions across the training, validation, and test sets was balanced. Figure  8  shows the distribution of the presence percentage and mean intensity of emotions, while figure  7  shows the distribution of mean intensity for dimensions across the three partitions training, dev, and test sets.\n\nFrom the heatmaps, we observe that the mean values for the percentage of presence and intensity of emotions are consistent across the partitions. Similarly for dimensions. This indicates that the models were trained and evaluated on a balanced set of data, which reduces the chance of bias in the performance of the model.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "A. Representations Of Audio, Textual And Video Data",
      "text": "In real-life applications like THERADIA, human transcriptions are not always available. Therefore, automatic transcription is required for experiments involving the text modality. To this end, a benchmark was carried out to select the more robust ASR among those available. In accordance with an open science approach, the focus was placed on ASRs that are readily available and free of charge. The following candidates were selected: SpeechRecognition 2  toolkit, the Google, and the three Vosk 3  models that were trained on French speech (using the Kaldi toolkit 4  ). To compare the ASR models, Word Error Rate (WER) was calculated on all of the annotated sequences (see Table  IV ). The Google ASR achieved the best results (33.69 % average WER), followed by the Vosk-linto model (38.76 % average WER). Thus, automatic transcriptions of the current experiment were performed using the Google ASR.\n\nThe performance of predicting the intensity of affective labels of the core set was compared based on human or automatic transcriptions, using TF-IDF and BERT features. Results are reported in table III. They suggest a small drop in performance when using automatic transcriptions instead of human transcription. Specifically, similar performance was revealed between human and automated transcriptions with the use of TF-IDF (p > .05 based on two tailored z-test). However, a small effect of better performance was evidenced for human compared to automated transcription with the use of BERT (Cohen's d = 0.25).\n\nGiven the small drop in performance compared with human transcription, automatic transcription seems suitable for predicting affective labels. It should also be noted that these results are in line with the literature [3]-  [6] .",
      "page_start": 15,
      "page_end": 16
    },
    {
      "section_name": "B. Prediction Bias Based On Population Demographics",
      "text": "A prediction bias analysis of affective labels and dimensions was performed based on population demographics, i.e., male vs. female; healthy seniors vs. MCI. To this end, the test partition of the corpus was divided according to the two groups of participants, i.e., seniors and MCI, then the seniors were divided according to the gender. This analysis was performed on multimodal predictions of time-continuous dimensions using the EWE as gold standard, the average of dimension summaries, and the average of label intensities.\n\nThe results for the prediction of label intensity and dimensions are reported in table V, and table VI, respectively. Based on two-tailored z-test, both expert and deep representations show better performance for the female group than for the male group, p < .05. This could be explained by the female to male ratio in the participant sample  (40/12) , which translates into more robust training for the female group. Moreover, significantly better performances were evidenced for the MCI group compared to the healthy senior group, p < .05. This result was unexpected, given that the ratio of MCI to healthy seniors is low in the sample of participants (9/41), leaving less data for the models to train on. It is possible that MCI participants expressed more pronounced affects than healthy seniors facilitating the model training and prediction. The rationale of this assumption is underpinned by the fact that they were likely more involved in the CCT since they were directly concerned by the cognitive remediation aspect.",
      "page_start": 17,
      "page_end": 17
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Head movements, gaze, speech, and articulation of a remote operator",
      "page": 1
    },
    {
      "caption": "Figure 1: Expressions of the participants were then fully transcribed and",
      "page": 2
    },
    {
      "caption": "Figure 2: Office of the experimenter operating the virtual assistant. The",
      "page": 3
    },
    {
      "caption": "Figure 3: , followed by a summary value, for",
      "page": 5
    },
    {
      "caption": "Figure 3: Example of the ANNOT web-base platform for time-continuous",
      "page": 5
    },
    {
      "caption": "Figure 4: Frequency (logarithmic scale) of the annotated labels according to the agreement of at least one, two or three annotators among six.",
      "page": 6
    },
    {
      "caption": "Figure 4: Based on the sample of annotation frequency per label for",
      "page": 6
    },
    {
      "caption": "Figure 5: Heatmap of the PCC between affective labels and summary values",
      "page": 7
    },
    {
      "caption": "Figure 5: The labels that are consistently correlated",
      "page": 7
    },
    {
      "caption": "Figure 6: Overview of the predictive experiments performed on the corpus.",
      "page": 8
    },
    {
      "caption": "Figure 6: In brief,",
      "page": 8
    },
    {
      "caption": "Figure 1: illustrates an on-screen trial of these exercises, which",
      "page": 13
    },
    {
      "caption": "Figure 1: An on-screen trial display of the eight exercises selected for data",
      "page": 13
    },
    {
      "caption": "Figure 2: Example of a time-continuous annotation of the arousal dimension;",
      "page": 14
    },
    {
      "caption": "Figure 3: Example of a summarised annotation of the arousal dimension.",
      "page": 14
    },
    {
      "caption": "Figure 4: Example of summary annotation of the labels.",
      "page": 14
    },
    {
      "caption": "Figure 2: After the",
      "page": 14
    },
    {
      "caption": "Figure 3: . Finally,",
      "page": 14
    },
    {
      "caption": "Figure 4: Labels were presented",
      "page": 14
    },
    {
      "caption": "Figure 5: An example of continuously valued annotation results on intrinsic",
      "page": 15
    },
    {
      "caption": "Figure 5: After performing it on several selected extracts,",
      "page": 15
    },
    {
      "caption": "Figure 6: presents a heatmap of the associations between",
      "page": 15
    },
    {
      "caption": "Figure 8: shows the distribution of the presence",
      "page": 15
    },
    {
      "caption": "Figure 7: shows the distribution of mean intensity for dimensions across",
      "page": 15
    },
    {
      "caption": "Figure 6: Heatmap of the PCC between affective labels and summary values of appraisal dimensions for each annotator. The sign (+) or () after each label",
      "page": 16
    },
    {
      "caption": "Figure 7: Heatmap showing the mean intensity with its standard deviation for",
      "page": 17
    },
    {
      "caption": "Figure 8: Heatmap showing, for each emotion and partition, the presence percentage when at least one annotator reported a non-zero value (left) and the mean",
      "page": 18
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Label": "",
          "Expert Representations": "Text\nAudio\nVideo\nMulti.\nTF-IDF\nMFB\nFAU",
          "Deep Representations": "Text\nAudio\nVideo\nMulti.\nBERT\nW2V2\nCLIP"
        },
        {
          "Label": "Annoyed\nAnxious\nConfident\nDesperate\nFrustrated\nHappy\nInterested\nRelaxed\nSatisfied\nSurprised",
          "Expert Representations": ".075\n.047\n.274\n.255\n.115\n.081\n.129\n.171\n.233\n.176\n.168\n.422\n.172\n.148\n.193\n.259\n.327\n.151\n.175\n.387\n.117\n.273\n.446\n.490\n.147\n.135\n-.023\n.207\n.314\n.300\n.385\n.515\n.323\n.225\n.253\n.428\n.167\n.165\n.024\n.197",
          "Deep Representations": ".255\n.231\n.311\n.280\n.087\n.192\n.123\n.214\n.473\n.482\n.247\n.487\n.221\n.277\n.175\n.252\n.222\n.340\n.239\n.465\n.108\n.303\n.462\n.508\n.364\n.204\n.133\n.300\n.363\n.245\n.440\n.529\n.057\n.287\n.326\n.486\n.143\n.238\n.095\n.275"
        },
        {
          "Label": "Average",
          "Expert Representations": ".199\n.170\n.202\n.333",
          "Deep Representations": ".229\n.280\n.255\n.380"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Label": "",
          "Expert Representations": "Text\nAudio\nVideo\nMulti.\nTF-IDF\nMFB\nFAU",
          "Deep Representations": "Text\nAudio\nVideo\nMulti.\nBERT\nW2V2\nCLIP"
        },
        {
          "Label": "Annoyed\nAnxious\nConfident\nDesperate\nFrustrated\nHappy\nInterested\nRelaxed\nSatisfied\nSurprised",
          "Expert Representations": "63.2\n57.5\n62.4\n66.0\n66.3\n59.3\n57.0\n67.2\n70.4\n66.3\n59.8\n71.6\n71.3\n68.6\n62.4\n72.1\n72.2\n63.9\n63.3\n74.1\n65.4\n62.5\n71.1\n73.5\n61.7\n57.5\n50.1\n62.4\n68.7\n64.9\n68.7\n75.1\n70.4\n63.9\n64.0\n72.2\n62.3\n56.8\n52.2\n61.8",
          "Deep Representations": "63.1\n63.7\n65.1\n69.4\n65.5\n64.9\n58.2\n67.6\n73.4\n73.1\n59.5\n76.0\n69.3\n74.9\n65.6\n75.7\n70.4\n69.9\n63.4\n74.5\n65.8\n66.8\n73.9\n76.7\n64.9\n60.5\n59.1\n64.1\n69.0\n67.3\n64.1\n74.0\n73.9\n70.0\n63.8\n75.3\n62.1\n61.7\n54.1\n62.3"
        },
        {
          "Label": "Average",
          "Expert Representations": "67.2\n62.1\n61.1\n69.6",
          "Deep Representations": "67.7\n67.3\n62.7\n71.6"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Label": "Annoyed\nAnxious\nConfident\nDesperate\nFrustrated\nHappy\nInterested\nRelaxed\nSatisfied\nsurprised",
          "TF-IDF\nHuman\nAutomatic": ".031\n.075\n.140\n.115\n.263\n.233\n.211\n.172\n.381\n.327\n.127\n.117\n.156\n.147\n.341\n.314\n.344\n.323\n.209\n.167",
          "BERT\nHuman\nAutomatic": ".122\n.255\n.124\n.087\n.518\n.473\n.322\n.221\n.183\n.222\n.157\n.108\n.336\n.364\n.295\n.363\n.364\n.057\n.197\n.143"
        },
        {
          "Label": "Average",
          "TF-IDF\nHuman\nAutomatic": ".220\n.199",
          "BERT\nHuman\nAutomatic": ".262\n.229"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ASR model": "",
          "Senior": "Emotional\nNeutral\nAll",
          "MCI": "Emotional\nNeutral\nAll",
          "Senior + MCI": "Emotional\nNeutral\nAll"
        },
        {
          "ASR model": "Google",
          "Senior": "55.45%\n49.48%\n54.15%\n(43.40%)\n(44.96%)\n(43.82%)",
          "MCI": "49.56%\n46.72%\n47.80%\n(39.83%)\n(43.98%)\n(42.45%)",
          "Senior + MCI": "55.22%\n48.95%\n53.65%\n(43.28%)\n(44.78%)\n(43.74%)"
        },
        {
          "ASR model": "Vosk-small",
          "Senior": "57.79%\n52.60%\n56.65%\n(43.97%)\n(44.84%)\n(44.21%)",
          "MCI": "48.67%\n40.33%\n43.50%\n(39.45%)\n(41.51%)\n(40.92%)",
          "Senior + MCI": "57.43%\n50.23%\n55.63%\n(43.83%)\n(44.47%)\n(44.10%)"
        },
        {
          "ASR model": "Vosk-medium",
          "Senior": "48.00%\n38.49%\n45.92%\n(49.05%)\n(45.49%)\n(48.45%)",
          "MCI": "39.82%\n29.04%\n33.14%\n(42.72%)\n(39.60%)\n(41.12%)",
          "Senior + MCI": "47.68%\n36.67%\n44.92%\n(48.84%)\n(44.57%)\n(48.04%)"
        },
        {
          "ASR model": "Vosk-linto",
          "Senior": "56.51%\n50.90%\n55.28%\n(44.66%)\n(44.94%)\n(44.78%)",
          "MCI": "49.10%\n40.04%\n43.48%\n(39.42%)\n(41.12%)\n(40.70%)",
          "Senior + MCI": "56.21%\n48.81%\n54.36%\n(44.49%)\n(44.43%)\n(44.58%)"
        },
        {
          "ASR model": "Frequency",
          "Senior": "7859\n2201\n10060",
          "MCI": "322\n526\n848",
          "Senior + MCI": "8181\n2727\n10908"
        },
        {
          "ASR model": "Word / second",
          "Senior": ".791\n.719\n.775\n(.624)\n(.563)\n(.611)",
          "MCI": ".877\n.748\n.797\n(.641)\n(.601)\n(.620)",
          "Senior + MCI": ".794\n.725\n.777\n(.625)\n(.570)\n(.612)"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Label": "",
          "Traditional\nfeatures": "Senior\nMCI\nSenior+MCI\nFemale\nMale\nF+M",
          "Deep representations": "Senior\nMCI\nSenior+MCI\nFemale\nMale\nF+M"
        },
        {
          "Label": "Annoyed\nAnxious\nConfident\nDesperate\nFrustrated\nHappy\nInterested\nRelaxed\nSatisfied\nSurprised",
          "Traditional\nfeatures": ".210\n.131\n.247\n.367\n.255\n.202\n.099\n.174\n.128\n.171\n.434\n.361\n.424\n.337\n.422\n.285\n.178\n.260\n.218\n.259\n.472\n.143\n.377\n.508\n.387\n.456\n.402\n.471\n.597\n.490\n.208\n.179\n.210\n.212\n.207\n.492\n.529\n.510\n.548\n.515\n.439\n.351\n.435\n.365\n.428\n.204\n.072\n.179\n.339\n.197",
          "Deep representations": ".302\n.199\n.284\n.328\n.280\n.234\n.087\n.209\n.266\n.214\n.498\n.358\n.477\n.575\n.487\n.249\n.252\n.248\n.333\n.252\n.478\n.403\n.463\n.491\n.465\n.508\n.455\n.508\n.523\n.508\n.269\n.336\n.295\n.374\n.300\n.520\n.475\n.516\n.646\n.529\n.490\n.496\n.499\n.348\n.486\n.284\n.146\n.268\n.327\n.275"
        },
        {
          "Label": "Average",
          "Traditional\nfeatures": ".340\n.244\n.329\n.362\n.333",
          "Deep representations": ".383\n.321\n.377\n.421\n.380"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dimensions": "",
          "Time-continous": "Senior\nMCI\nSenior+MCI\nFemale\nMale\nF+M",
          "Summaries": "Senior\nMCI\nSenior+MCI\nFemale\nMale\nF+M"
        },
        {
          "Dimensions": "Arousal\nCoping\nGoal conduciveness\nIntrinsic pleasantness\nNovelty",
          "Time-continous": ".695\n.638\n.682\n.777\n.691\n.430\n.328\n.406\n.511\n.418\n.422\n.382\n.413\n.490\n.420\n.392\n.281\n.366\n.404\n.370\n.351\n.242\n.325\n.455\n.339",
          "Summaries": ".280\n.361\n.315\n.360\n.310\n.690\n.608\n.674\n.799\n.684\n.668\n.610\n.658\n.711\n.662\n.634\n.448\n.598\n.709\n.609\n.607\n.348\n.543\n.709\n.559"
        }
      ],
      "page": 18
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Artificial intelligence in healthcare",
      "authors": [
        "K.-H Yu",
        "A Beam",
        "I Kohane"
      ],
      "year": "2018",
      "venue": "Nature biomedical engineering"
    },
    {
      "citation_id": "2",
      "title": "in Handbook of Social Behavior and Skills in Children",
      "authors": [
        "S Little",
        "J Swangler",
        "A Akin-Little"
      ],
      "year": "2017",
      "venue": "in Handbook of Social Behavior and Skills in Children"
    },
    {
      "citation_id": "3",
      "title": "Can social interaction constitute social cognition?",
      "authors": [
        "H Jaegher",
        "E Paolo",
        "S Gallagher"
      ],
      "year": "2010",
      "venue": "Trends in Cognitive Sciences"
    },
    {
      "citation_id": "4",
      "title": "Automatic emotion recognition in clinical scenario: A systematic review of methods",
      "authors": [
        "L Pepa",
        "L Spalazzi",
        "M Capecci"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "5",
      "title": "THERADIA: Digital Therapies Augmented by Artificial Intelligence",
      "authors": [
        "F Tarpin-Bernard",
        "J Fruitet",
        "J.-P Vigne"
      ],
      "year": "2021",
      "venue": "Advances in Neuroergonomics and Cognitive Engineering"
    },
    {
      "citation_id": "6",
      "title": "Computerized cognitive training in older adults with mild cognitive impairment or dementia: a systematic review and meta-analysis",
      "authors": [
        "N Hill",
        "L Mowszowski",
        "S Naismith"
      ],
      "year": "2017",
      "venue": "American Journal of Psychiatry"
    },
    {
      "citation_id": "7",
      "title": "Computerized cognitive training in Parkinson's disease: A systematic review and meta-analysis",
      "authors": [
        "H Gavelin",
        "M Domellöf",
        "I Leung"
      ],
      "year": "2022",
      "venue": "Ageing Research Reviews"
    },
    {
      "citation_id": "8",
      "title": "Computerized cognitive training in cognitively healthy older adults: a systematic review and meta-analysis of effect modifiers",
      "authors": [
        "A Lampit",
        "H Hallock",
        "M Valenzuela"
      ],
      "year": "2014",
      "venue": "PLoS medicine"
    },
    {
      "citation_id": "9",
      "title": "Models of emotion: The affective neuroscience approach",
      "authors": [
        "D Sander"
      ],
      "year": "2013",
      "venue": "Models of emotion: The affective neuroscience approach"
    },
    {
      "citation_id": "10",
      "title": "A systems approach to appraisal mechanisms in emotion",
      "authors": [
        "D Sander",
        "D Grandjean",
        "K Scherer"
      ],
      "venue": "Neural networks"
    },
    {
      "citation_id": "11",
      "title": "On the causal role of appraisal in emotion",
      "authors": [
        "A Moors"
      ],
      "year": "2013",
      "venue": "Emotion Review"
    },
    {
      "citation_id": "12",
      "title": "Same situation-different emotions: how appraisals shape our emotions",
      "authors": [
        "M Siemer",
        "I Mauss",
        "J Gross"
      ],
      "year": "2007",
      "venue": "Emotion"
    },
    {
      "citation_id": "13",
      "title": "Dynamic facial expression of emotion and observer inference",
      "authors": [
        "K Scherer",
        "H Ellgring",
        "A Dieckmann"
      ],
      "year": "2019",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "14",
      "title": "A circumplex model of emotions",
      "authors": [
        "J Russel"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "15",
      "title": "Combined effects of intrinsic and goal relevances on attention and action tendency during the emotional episode",
      "authors": [
        "H Fournier",
        "O Koenig"
      ],
      "year": "2023",
      "venue": "Emotion"
    },
    {
      "citation_id": "16",
      "title": "The world of emotions is not two-dimensional",
      "authors": [
        "J Fontaine",
        "K Scherer",
        "E Roesch"
      ],
      "year": "2007",
      "venue": "Psychological science"
    },
    {
      "citation_id": "17",
      "title": "Four models of basic emotions: A review of Ekman and Cordaro, Izard, Levenson, and Panksepp and Watt",
      "authors": [
        "J Tracy",
        "D Randles"
      ],
      "year": "2011",
      "venue": "Emotion review"
    },
    {
      "citation_id": "18",
      "title": "Conscious emotional experience emerges as a function of multilevel, appraisal-driven response synchronization",
      "authors": [
        "D Grandjean",
        "D Sander",
        "K Scherer"
      ],
      "year": "2008",
      "venue": "Consciousness and cognition"
    },
    {
      "citation_id": "19",
      "title": "Appraisal-driven facial actions as building blocks for emotion inference",
      "authors": [
        "K Scherer",
        "M Mortillaro",
        "I Rotondi"
      ],
      "year": "2018",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "20",
      "title": "Automated assessment of psychiatric disorders using speech: A systematic review",
      "authors": [
        "D Low",
        "K Bentley",
        "S Ghosh"
      ],
      "year": "2020",
      "venue": "Laryngoscope investigative otolaryngology"
    },
    {
      "citation_id": "21",
      "title": "SEWA DB: A Rich Database for Audio-Visual Emotion and Sentiment Research in the Wild",
      "authors": [
        "J Kossaifi",
        "R Walecki",
        "Y Panagakis"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "22",
      "title": "Exploring emotion expression recognition in older adults interacting with a virtual coach",
      "authors": [
        "C Palmero",
        "M Develasco",
        "M Hmani"
      ],
      "year": "2023",
      "venue": "Exploring emotion expression recognition in older adults interacting with a virtual coach",
      "arxiv": "arXiv:2311.05567"
    },
    {
      "citation_id": "23",
      "title": "Training an emotion detection classifier using frames from a mobile therapeutic game for children with developmental disorders",
      "authors": [
        "P Washington",
        "H Kalantarian",
        "J Kent"
      ],
      "year": "2022",
      "venue": "JMIR pediatrics and parenting"
    },
    {
      "citation_id": "24",
      "title": "The value of a virtual assistant to improve engagement in computerized cognitive training at home: An exploratory study",
      "authors": [
        "I Zsoldos",
        "E Trân",
        "H Fournier"
      ],
      "year": "2024",
      "venue": "JMIR Rehabilitation and Assistive Technologies"
    },
    {
      "citation_id": "25",
      "title": "Interactional units in conversation: Syntactic, intonational, and pragmatic resources for the management of turns",
      "authors": [
        "C Ford",
        "S Thompson"
      ],
      "year": "1996",
      "venue": "Studies in interactional sociolinguistics"
    },
    {
      "citation_id": "26",
      "title": "Un grand corpus oral «disponible»: le corpus d'Orléans 1 1968-2012",
      "authors": [
        "I Eshkol-Taravella",
        "O Baude",
        "D Maurel"
      ],
      "year": "2011",
      "venue": "Revue TAL"
    },
    {
      "citation_id": "27",
      "title": "Annotating multimedia/multi-modal resources with elan",
      "authors": [
        "H Brugman",
        "A Russel",
        "X Nijmegen"
      ],
      "year": "2004",
      "venue": "LREC"
    },
    {
      "citation_id": "28",
      "title": "The control-value theory of achievement emotions: Assumptions, corollaries, and implications for educational research and practice",
      "authors": [
        "R Pekrun"
      ],
      "year": "2006",
      "venue": "Educational psychology review"
    },
    {
      "citation_id": "29",
      "title": "Achievement emotions",
      "year": "2022",
      "venue": "Emotions in late modernity"
    },
    {
      "citation_id": "30",
      "title": "Confusion and interest: The role of knowledge emotions in aesthetic experience",
      "authors": [
        "P Silvia"
      ],
      "year": "2010",
      "venue": "Psychology of Aesthetics, Creativity, and the Arts"
    },
    {
      "citation_id": "31",
      "title": "Investigating appraisaldriven facial expression and inference in emotion communication",
      "authors": [
        "K Scherer",
        "A Dieckmann",
        "M Unfried"
      ],
      "year": "2021",
      "venue": "Emotion"
    },
    {
      "citation_id": "32",
      "title": "Understanding the mechanisms underlying the production of facial expression of emotion: A componential perspective",
      "authors": [
        "K Scherer",
        "M Mortillaro",
        "M Mehu"
      ],
      "year": "2013",
      "venue": "Emotion Review"
    },
    {
      "citation_id": "33",
      "title": "A power primer",
      "authors": [
        "J Cohen"
      ],
      "year": "1992",
      "venue": "Psychological Bulletin"
    },
    {
      "citation_id": "34",
      "title": "Dynamic facial expression of emotion and observer inference",
      "authors": [
        "K Scherer",
        "H Ellgring",
        "A Dieckmann",
        "M Unfried",
        "M Mortillaro"
      ],
      "year": "2019",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "35",
      "title": "On the Evolution of Speech Representations for Affective Computing: A brief history and critical overview",
      "authors": [
        "S Alisamir"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "36",
      "title": "Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences",
      "authors": [
        "S Davis",
        "P Mermelstein"
      ],
      "year": "1980",
      "venue": "IEEE transactions on acoustics, speech, and signal processing"
    },
    {
      "citation_id": "37",
      "title": "Term-weighting approaches in automatic text retrieval",
      "authors": [
        "G Salton",
        "C Buckley"
      ],
      "year": "1988",
      "venue": "Information processing & management"
    },
    {
      "citation_id": "38",
      "title": "Manual of the facial action coding system (facs)",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1978",
      "venue": "Trans. ed"
    },
    {
      "citation_id": "39",
      "title": "SpeechBrain: A generalpurpose speech toolkit",
      "authors": [
        "M Ravanelli",
        "T Parcollet",
        "P Plantinga"
      ],
      "year": "2021",
      "venue": "SpeechBrain: A generalpurpose speech toolkit",
      "arxiv": "arXiv:2106.04624"
    },
    {
      "citation_id": "40",
      "title": "Openface: an open source facial behavior analysis toolkit",
      "authors": [
        "T Baltrušaitis",
        "P Robinson",
        "L.-P Morency"
      ],
      "year": "2016",
      "venue": "IEEE Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "41",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "42",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "43",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "C Hallacy",
        "A Ramesh",
        "G Goh",
        "S Agarwal",
        "G Sastry",
        "A Askell",
        "P Mishkin",
        "J Clark"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "44",
      "title": "Unsupervised cross-lingual representation learning for speech recognition",
      "authors": [
        "A Conneau",
        "A Baevski",
        "R Collobert",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech 2021"
    },
    {
      "citation_id": "45",
      "title": "Utilizing BERT for aspectbased sentiment analysis via constructing auxiliary sentence",
      "authors": [
        "C Sun",
        "L Huang",
        "X Qiu"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "46",
      "title": "Emotionx-ku: Bert-max based contextual emotion classifier",
      "authors": [
        "K Yang",
        "D Lee",
        "T Whang"
      ],
      "year": "2019",
      "venue": "Emotionx-ku: Bert-max based contextual emotion classifier",
      "arxiv": "arXiv:1906.11565"
    },
    {
      "citation_id": "47",
      "title": "Discriminatively Trained Recurrent Neural Networks for Continuous Dimensional Emotion Recognition from Audio",
      "authors": [
        "F Weninger",
        "F Ringeval",
        "E Marchi"
      ],
      "year": "2016",
      "venue": "Proceedings of the 25th International Joint Conference on Artificial Intelligence (IJCAI 2016)"
    },
    {
      "citation_id": "48",
      "title": "Pytorch: An imperative style, highperformance deep learning library",
      "authors": [
        "A Paszke",
        "S Gross",
        "F Massa"
      ],
      "year": "2019",
      "venue": "Proceedings of the thirty-third Conference on Neural Information Processing Systems (NIPS). Vancouver"
    },
    {
      "citation_id": "49",
      "title": "Muse-toolbox: The multimodal sentiment analysis continuous annotation fusion and discrete class transformation toolbox",
      "authors": [
        "L Stappen",
        "L Schumann",
        "B Sertolli"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2nd Workshop on multimodal sentiment analysis challenge"
    },
    {
      "citation_id": "50",
      "title": "Unpacking the cognitive architecture of emotion processes",
      "authors": [
        "D Grandjean",
        "K Scherer"
      ],
      "year": "2008",
      "venue": "Emotion"
    },
    {
      "citation_id": "51",
      "title": "A systems approach to appraisal mechanisms in emotion",
      "authors": [
        "D Sander",
        "D Grandjean",
        "K Scherer"
      ],
      "year": "2005",
      "venue": "Neural networks"
    },
    {
      "citation_id": "52",
      "title": "Bimodal speech emotion recognition using pre-trained language models",
      "authors": [
        "V Heusser",
        "N Freymuth",
        "S Constantin",
        "A Waibel"
      ],
      "year": "2019",
      "venue": "Bimodal speech emotion recognition using pre-trained language models",
      "arxiv": "arXiv:1912.02610"
    },
    {
      "citation_id": "53",
      "title": "Speech emotion recognition using multi-hop attention mechanism",
      "authors": [
        "S Yoon",
        "S Byun",
        "S Dey",
        "K Jung"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "54",
      "title": "Emotion recognition by fusing time synchronous and time asynchronous representations",
      "authors": [
        "W Wu",
        "C Zhang",
        "P Woodland"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "55",
      "title": "Efficient speech emotion recognition using multi-scale cnn and attention",
      "authors": [
        "Z Peng",
        "Y Lu",
        "S Pan",
        "Y Liu"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    }
  ]
}