{
  "paper_id": "2404.18327v2",
  "title": "Multimae-Der: Multimodal Masked Autoencoder For Dynamic Emotion Recognition",
  "published": "2024-04-28T21:53:42Z",
  "authors": [
    "Peihao Xiang",
    "Chaohao Lin",
    "Kaida Wu",
    "Ou Bai"
  ],
  "keywords": [
    "Dynamic Emotion Recognition",
    "Multimodal Model",
    "Self-Supervised Learning",
    "Video Masked Autoencoder",
    "Vision Transformer"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper presents a novel approach to processing multimodal data for dynamic emotion recognition, named as the Multimodal Masked Autoencoder for Dynamic Emotion Recognition (MultiMAE-DER). The MultiMAE-DER leverages the closely correlated representation information within spatiotemporal sequences across visual and audio modalities. By utilizing a pre-trained masked autoencoder model, the MultiMAE-DER is accomplished through simple, straightforward finetuning. The performance of the MultiMAE-DER is enhanced by optimizing six fusion strategies for multimodal input sequences. These strategies address dynamic feature correlations within cross-domain data across spatial, temporal, and spatiotemporal sequences. In comparison to state-of-the-art multimodal supervised learning models for dynamic emotion recognition, MultiMAE-DER enhances the weighted average recall (WAR) by 4.41% on the RAVDESS dataset and by 2.06% on the CREMA-D. Furthermore, when compared with the state-of-the-art model of multimodal self-supervised learning, MultiMAE-DER achieves a 1.86% higher WAR on the IEMOCAP dataset.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "With the development of deep learning technology, convolutional neural networks (CNN) have shown excellent results in static emotion recognition, especially in facial expression recognition (FER) tasks, as seen in models like RTCNN  [1] , DeepEmotion  [2] , and PAtt-Lite  [3] . Dynamic emotion recognition (DER) is one of the important fields of affective computing. Psychologists and neuroscientists have been exploring this field for a long time, while constructing a series of quantitative rules for recognizing emotional characteristics.\n\nIn DER, a new architecture has emerged by combining CNN with recurrent neural networks (RNN). For example, ResNet-18+LSTM  [4]  and C3D+LSTM  [5]  have been developed to construct dynamic emotion recognition models. More importantly, with the advent of Transformers  [6] , there has been further exploration into the global context correlation. This Fig.  1 . General Multimodal Model vs. MultiMAE-DER. The uniqueness of our approach lies in the capability to extract features from cross-domain data using only a single encoder, eliminating the need for targeted feature extraction for different modalities.\n\nstructure is well-suited for capturing the spatial and temporal context features in dynamic emotion. Examples of such models include Former-DFER  [7]  and STT-DFER  [8] .\n\nHowever, CNN, RNN, or Transformers  [6] , requires a large amount of labeled data for supervised learning to build highperformance models. This issue was only alleviated with the advent of self-supervised learning, especially when applied to natural language processing (NLP), where the BERT  [9]  model demonstrated performance beyond supervised learning models.\n\nIn the field of computer vision, the introduction of Masked Autoencoder (MAE) strongly indicated that self-supervised learning would shine in the field of vision such as ImageMAE  [10] , VideoMAE  [11] , and AudioMAE  [12] . On the other hand, while single-modal (visual or audio) inputs have shown good results in dynamic emotion recognition, as shown in 979-8-3503-7565-7/24/$31.00 ©2024 IEEE models like HuBERT  [13]  and MAE-DFER  [14] , their performance has not yet reached the level of real human emotion recognition. Therefore, multimodal (Visual-Audio) input has become a desirable development trend, as demonstrated by models like AV-LSTM  [15]  and MSAF  [16] .\n\nThe aim of this study is to explore a new framework for dynamic emotion recognition model. The proposed approach is a straightforward inheritance and extension of VideoMAE  [11] , extending the originally single-modal visual input into multimodal input encompassing both visual and audio elements. Simultaneously, the self-supervised learning pre-trained Video Masked Autoencoder model is used to process visualaudio sequence strategies. This extended model is named MultiMAE-DER, standing for Multimodal Masked Autoencoder for Dynamic Emotion Recognition. The MultiMAE-DER is optimized by investigating the performance of six visual-audio sequence strategies.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Dynamic Emotion Recognition",
      "text": "Dynamic emotion recognition is a significant challenge in the field of affective computing that requires the utilization of multimodal feature extraction, context semantic analysis, and causal reasoning techniques. In recent years, researchers have addressed the feature extraction task from multimodal using transfer learning by combining pre-trained models for video facial expression feature extraction and audio tone emotion feature extraction, such as MSAF  [16]  and CFN-SR  [17] . However, the above-mentioned model architecture fails to establish cross-domain associative features between visual and audio data, leading to the loss of highly correlated spatiotemporal feature dimensions in the emotion representation information. Therefore, we propose a MultiMAE-DER framework, a unified feature extraction pre-trained model that will be employed for visual and audio multimodal data to find the associated features in visual-audio spatio-temporal emotion representation information.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Masked Autoencoder",
      "text": "Masked Autoencoder is a variant of denoising autoencoder that consists of an encoder and a decoder. In ImageMAE  [10] , that is designed as an asymmetric structure to reconstruct normalized pixels. VideoMAE  [11]  follows the design approach of ImageMAE  [10]  but extends the 2D image input to 3D video input and incorporates a joint spatio-temporal selfattention mechanism to replace the vanilla Vision Transformer  [18]  spatial self-attention mechanism.\n\nFor downstream tasks, the decoder of the masked autoencoder is discarded, and only the pre-trained encoder is used for supervised learning and fine-tuning on the downstream task dataset. As a leader in self-supervised learning, our proposal will continue to inherit this characteristic and utilize the pre-trained encoder model to fine-tune multimodal dynamic emotion data, aiming for improved emotion recognition performance.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Vision Transformer",
      "text": "Vision Transformer (ViT)  [18]  is a variant of the Transformer  [6]  applied in the field of computer vision. The principle is to treat a pixel block (patch) as an individual \"word\" to emulate the token sequence input of Transformer  [6] , allowing for the serialization of image processing. Furthermore, ViT  [18]  serves as the backbone network for VideoMAE  [11] , which is used to associate the correlation of spatio-temporal context in video data.\n\nFor MultiMAE-DER, ViT  [18]  will continue to be used as the backbone network for the encoder. Simultaneously, MultiMAE-DER will inherit the joint spatio-temporal selfattention mechanism from VideoMAE  [11]  to replace the spatial self-attention mechanism in vanilla Vision Transformer  [18] . This adaptation aims to analyze the contextual emotion representation information in visual-audio data.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Method",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Revisiting Videomae",
      "text": "To illustrate that this work is an inheritance and extension of VideoMAE  [11] , as depicted in Fig.  1 , the encoder will directly call the pre-trained model of VideoMAE  [11]  encoder as the backbone network. This approach serves to demonstrate effectiveness as a video representation learner while also reducing significant training time. The pre-trained encoder model from VideoMAE  [11]  is used, which requires consistency with the input size of 16×224×224, where 16 is the number of frames, and the image size is 224 × 224. Additionally, the patch size remains 2 × 16 × 16, resulting in 1568 patches as the input size for the token sequence. The VideoMAE  [11]  encoder uses ViT-L  [18]  as the backbone network with a dimension of 1024. However, this work primarily analyzes multimodal inputs. Therefore, several different sequence fusion methods will be optimized while maintaining the input size and patch size.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Multimodal Sequence Strategy",
      "text": "Firstly, 16 frames of video V ∈ R 16×224×224 are uniformly downsampled from the single clip. At the same time, to align with each video frame, the audio signal is uniformly downsampled to generate 16 spectrograms A ∈ R 16×224×224 . As shown in Fig.  2 , in the same time sequence, each sampled time point of the video frame corresponds to the audio signal contained before and after each time point. This ensures the maximum alignment of partial emotion features between the audio signal and facial expression in the video.\n\nOn the other hand, in order to verify that extracting spatiotemporally highly correlated dynamic representation information from unified video and audio signals can help improve the accuracy of emotion recognition. We explore the following hypotheses from the sequence fusion strategy of spatiotemporal aspects of audio and video signals:\n\n• From the spatial perspective, video signals and audio signals are spliced or superimposed to explore the dynamic representation information of the video and audio signals in spatial integrity under the certain time sequence.\n\n• From the temporal perspective, the video signal and audio signal are sequence transform to explore the dynamic representation information of the video and audio signal in time sequence under the condition of spatial integrity.\n\nTherefore, as shown in Fig.  3 , this work will propose six different multimodal sequence fusion strategies to optimize MultiMAE-DER performance based on the above hypotheses. For strategy 1 (CFAS), 16 frames of video V ∈ R 16×112×224 and corresponding 16 audio spectrograms A ∈ R 16×112×224 will be directly spliced to explore whether there is a certain correlation in the spatiotemporal sequence for this multimodal input X ∈ R 16×224×224 . For strategy 2 (SFAS), 16 frames of video V ∈ R 16×224×224 and corresponding 16 audio spectrograms A ∈ R 16×224×224 will be normalized, and then directly added to verify the feasibility of spatially fusing audiovisual signals. For strategy 3 (FFLS), considering input size constraints, the single clip are uniformly downsampled to 8 frames of video V ∈ R 8×224×224 and corresponding 8 audio spectrograms A ∈ R 8×224×224 . They are sorted based on facial frames first and then audio spectrograms to consider the impact of solely the temporal sequence in the visual-audio sorting of multimodal input X ∈ R 16×224×224 on emotion recognition results. The mathematical expressions for these three multimodal sequence fusion strategies are as follows:\n\nFor strategy 4 (FSLF), following the method of Strategy 3, the order of video frames V ∈ R 8×224×224 and audio spectrograms A ∈ R 8×224×224 are swapped to consider the impact of different orders in the temporal sequence of visualaudio input X ∈ R 16×224×224 on emotion recognition results. For strategy 5 (OFOS), following the order of one video frame V ∈ R 8×224×224 followed by one audio spectrogram A ∈ R 8×224×224 , to explore the impact of different permutations of the time sequence on emotion recognition results.\n\nFinally, for strategy 6 (RFAS), randomly sorting 8 frames of video V ∈ R 8×224×224 and corresponding 8 spectrograms A ∈ R 8×224×224 to verify the influence of unordered temporal sequence on visual-audio multimodal data input X ∈ R 16×224×224 on emotion recognition results. The mathematical expressions for these three multimodal sequence fusion strategies are as follows:",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Multimae-Der: Model Structure",
      "text": "As shown in Fig.  1 , uniform downsampling is applied to both video and audio in the single clip to align them and meet input size requirements. Subsequently, different sequence fusion strategies from Section III.B are employed as multimodal input X ∈ R 16×224×224 to verify the assumptions of this work. Like VideoMAE  [11] , before entering the encoder, the input data is patched to construct token sequence S ∈ R 1568×512 . In this model, 3D convolutional kernels are used as embedding layers to perform non-overlapping patching, where the kernel size is 2 × 16 × 16. Linear projection (LP) is applied to each cube patch to flatten the input X ∈ R 1568×1024 into the encoder. After processing through the encoder, the output feature matrix F ∈ R 1568×1024 with the same input size is obtained. For a given downstream task (dynamic emotion recognition in this case), a single linear layer (MLP) serves as a specifictask head to process the feature matrix F ∈ R 1568×1024 , yielding the final emotion result Y ∈ R 1×7 for this clip. The mathematical expressions for the above process are as follows:\n\nIV. EXPERIMENTS\n\nA. Datasets 1) RAVDESS  [19] : The Ryerson Audio-Visual Database of Emotional Speech and Song is a dataset consisting of emotional performances conducted by 24 North American professional actors in a laboratory environment with North American accent. The emotions depicted in this dataset include anger, disgust, fear, happiness, neutral, sadness, and surprise. Each emotion has two intensity variations: normal and strong. In this work, we utilize only the visual-audio speech dataset, comprising 1440 video files. Among them, there are 288 videos for the neutral emotion, and the remaining emotions each have 192 videos. Our model evaluation is conducted using a 6-fold cross-validation on subjects-independent of the emotion.\n\n2) CREMA-D  [20] : Crowd-sourced Emotional Multimodal Actor Dataset comprises emotional performances by 91 professional actors from around the world, representing different countries, and ethnicities. These actors conducted emotional performances in a laboratory environment with diverse accents. Each actor presented six different emotions: anger, disgust, fear, happiness, neutral, and sadness. Additionally, they expressed each emotion at low, medium, high, and unspecified intensity levels to dialogue. The dataset includes a total of 7,442 video clips, with 1,087 clips for the neutral emotion and 1,271 clips for each of the remaining emotions. Model evaluation will be conducted using 5-fold cross-validation on subjects-independent of the emotion.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "3) Iemocap [21]:",
      "text": "The Interactive Emotional Dyadic Motion Capture Database is a multimodal and multi-speaker visual-audio dataset featuring improvised emotional binary scene creations by ten non-professional actors. The dataset includes approximately 12 hours of video data, capturing nine different emotional expressions: anger, happiness, excitement, sadness, discouragement, fear, surprise, other, and neutral states. Following the emotion categorization approach of AV-Superb  [22] , we conduct a four-emotion classification task (anger, happiness, sadness, and neutral). To increase the sample size, the data for the excited state are added to the data for the happy state. The fine-tuned model will be evaluated on this dataset using 5-fold cross-validation.\n\nB. Implementation Details 1) Downsampling: As discussed in Section III.B regarding the strategies for multimodal sequence fusion, downsampling is a crucial step in this proposal. Depending on the different sequence fusion strategies, we have varying uniform downsampling steps. Taking the RAVDESS  [19]  dataset as an example, for strategies 1 and 2, where facial videos and audio spectrograms have 16 frames per clip, the downsampling step size is set to 6. For strategies 3, 4, 5, and 6, where facial videos and audio spectrograms have 8 frames per clip, the downsampling step size is set to 12. The data preprocessing for the above process is illustrated in Fig.  4 , depicting video preprocessing and audio preprocessing. For video preprocessing, the cutoff time of a single clip is computed based on the required frame number and downsampling step size, followed by the downsampling of the clip. Simultaneously, for audio preprocessing, the audio signal of the clip is extracted. The time point T corresponding to the facial frame is determined based on the frame number. Then, the audio signal is cut at the time point T -1 and T + 1 to perform Short-Time Fourier Transform (STFT) and Mel-Frequency Cepstral Coefficients (MFCC) to obtain the audio spectrogram corresponding to the facial frame at time point T .\n\n2) Pre-training: After preprocessing, the obtained facial frames and audio spectrograms implement sequence fusion, which includes methods such as splice, addition, and combine. For the pre-trained encoder model, to eliminate domain differences, the pre-trained encoder model used in our proposal is derived from MAE-DFER  [14] . This encoder is built exclusively upon VideoMAE  [11]  and has undergone selfsupervised training on the extensive dataset of facial videos from YouTube, aiming to mitigate domain differences.\n\n3) Fine-tuning: As described in Section III.C, to minimize the interference of inductive bias, our approach utilizes only single-layer neural network as a specific-task head for finetuning on the downstream task. The fine-tuning employs the AdamW  [23]  optimizer with the base learning rate of 1e-2, weight decay of 0.05, sparse categorical cross-entropy as the loss function, and sparse categorical accuracy as the metric function. The evaluation metrics used by the model include unweighted average recall (UAR) and weighted average recall (WAR).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Results",
      "text": "In this section, we first compared MultiMAE-DER with other previous state-of-the-art multimodal models on the RAVDESS  [19] . As shown in Table  1 , in the multimodal model domain, the results exhibit significant variations based on different learning methods, whether supervised learning or self-supervised learning. The self-supervised multimodal model VQ-MAE-AV  [30]  outperforms the supervised multimodal model AVT  [29]  by 4% (83.20% vs. 79.20%) in terms of WAR. Furthermore, our MultiMAE-DER model, employing different multimodal sequence fusion strategies, generally surpasses the supervised multimodal model. Particularly, when using strategy 4, MultiMAE-DER-FSLF outperforms the Method SSL Modality UAR WAR AV-LSTM  [15]  χ V+A -65.80 AV-Gating  [15]  χ V+A -67.70 MCBP  [24]  χ V+A -71.32 MMTM  [25]  χ V+A -73.12 ERANNs  [26]  χ V+A -74.80 MSAF  [16]  χ V+A -74.86 SFN-SR  [17]  χ V+A -75.76 MATER  [27]  χ V+A -76.30 MulT  [28]  χ V+A -76.60 AVT  [29]  χ V+A -79.20 VQ-MAE-AV  [  The visual-audio results on the CREMA-D  [20]  are presented in Table  2 . We observe that the optimal strategy of MultiMAE-DER achieves the WAR result surpassing the VQ-MAE-AV  [30]  multimodal self-supervised learning model by 0.96% (79.36% vs. 78.40%). Compared to supervised learning approaches, the optimal strategy of MultiMAE-DER outperforms the multimodal model RAVER  [34]  by 2.06% (79.36% vs. 77.30%) in WAR. In addition, most of the results of other strategies in MultiMAE-DER outperform state-of-theart multimodal supervised learning models. This indicates the effectiveness of multimodal sequence fusion strategies, which can aid in identifying spatio-temporal correlations across cross-domain data to some extent.\n\nIn Table  3 , we compare state-of-the-art self-supervised multimodal models on the IEMOCAP  [21] . The results are derived from the evaluation test conducted by AV-Superb  [22] . The results showed that in the field of self-supervised learning, our MultiMAE-DER optimal strategy model (FSLF) outperforms the multimodal model AVBERT  [37]  by 1.86% (63.73% vs. 61.87%) in WAR. Upon observing the results of multiple strategies in our model and comparing them with the results of these three self-supervised multimodal models, we notice that MultiMAE-DER model performs slightly better in handling cross-domain data. This indirectly reflects the model's ability to extract representational information regarding spatio-temporal correlations across cross-domain data. Moreover, these dynamic features play a crucial role in inferring emotional context. Method SSL Modality UAR WAR EF-GRU  [31]  χ V+A -57.06 LF-GRU  [31]  χ V+A -58.53 TFN  [32]  χ V+A -63.09 MATER  [27]  χ V+A -67.20 AuxFormer  [33]  χ V+A -71.70 AV-LSTM  [15]  χ V+A -72.90 AV-Gating  [15]  χ V+A -74.00 RAVER  [34]  χ V+A -77.30 VQ-MAE-AV  [   In addition, compared with the MultiMAE-DER singlemodal model, the MultiMAE-DER multi-modal optimal strategy model (FSLF) is far superior in performance to the visualonly model and the audio-only model. The results show that the WAR on RAVDESS  [19]  is enhanced by 9.48% (83.61% vs. 74.13%) and 3.06% (83.61% vs. 80.55%) respectively, the WAR on CREMA-D  [20]  is enhanced by 1.53% (79.36% vs. 77.83%) and 0.91% (79.36% vs. 78.45%) respectively, and the WAR on IEMOCAP  [21]  is enhanced by 7.60% (63.73% vs. 56.13%) and 5.04% (63.73% vs. 58.69%) respectively.\n\nUpon analyzing the outcomes across the three datasets, we observed that MultiMAE-DER exhibits negligible discrepancies in emotion recognition results between strategy 3 (FFLS) and 4 (FSLF). This aligns with expectations, given that their distinctions lie solely in the sorting of multimodal sequences. Furthermore, the effects of these two strategies surpass those of other strategies, demonstrating enhancements ranging from 2% to 8% in WAR on the RAVDESS  [19] , 1% to 5% on the CREMA-D  [20] , and 1% to 3% on IEMOCAP  [21] .\n\nAccording to our analysis, different multimodal sequence fusion strategies determine the state of the token sequence. As we know, in the ViT  [18] , the sorting of the token sequence is constructed by the linear projection of patches. In our strategy, for strategy 1, the top half of all frames consists of visual data, while the bottom half consists of audio data. After patching, every 98 sequences among the 1568 token sequences will undergo cross-domain sequence exchange. Strategy 2 is the sum of cross-domain data normalization, which will directly disrupt the overall spatial structure. Strategy 3, which involves sorting visual data and the corresponding audio data, results in 1568 token sequences where the first 784 sequences are visual, and the latter 784 sequences are audio. Strategy 4 involves swapping the order of visual data and audio data from strategy 3. In the resulting 1568 token sequences, the first 784 sequences are audio, and the latter 784 sequences are visual. Strategy 5 involves interleaving the order of visual data and corresponding audio data. In the resulting 1568 token sequences, half of each sequence's dimensions are visual features, while the other half consists of audio features. Strategy 6 involves randomly sorting visual data and audio data. The resulting 1568 token sequences lack coherence, whether visual-audio sequence or spatio-temporal sequence. Finally, we can draw the following observations:\n\n• Strategy 1 exhibits temporal continuity but lacks spatial continuity.\n\n• Strategy 2 has temporal continuity but disrupts the overall spatial sequence structure. • Strategies 3 and 4 demonstrate both spatial and temporal continuity, with a high concentration of spatio-temporal correlation. • Strategy 5 shows spatial continuity but disrupts the overall temporal sequence structure. • Strategy 6 lacks both spatial and temporal continuity, simultaneously disrupting the overall spatio-temporal correlation.",
      "page_start": 5,
      "page_end": 7
    },
    {
      "section_name": "V. Conclusions And Future Work",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Conclusions",
      "text": "This work represents a new exploration of the approach to handling multimodal inputs, establishing a novel framework for the integrated processing of multimodal data under the conditions of self-supervised learning architecture. This paper investigates six different multimodal sequence fusion strategies to explore the diverse interpretations of multimodal representation information extracted by the same model when exposed to different spatio-temporal sequence inputs. Results indicate that fusing multimodal data on spatio-temporal sequences significantly improves the model performance by capturing correlations between cross-domain data. Furthermore, MultiMAE-DER validates the performance effectiveness of self-supervised learning as an efficient learner, contributing to the potential of the masked autoencoder as a unified framework for solving contextual semantic inference problems.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Future Work",
      "text": "In future work, the framework can be further developed from two aspects. In terms of depth, the self-supervised learning approach of the multimodal masked autoencoder involves reconstructing the input of multimodal visual-audio data to enhance the pre-training encoder. This enhanced model can subsequently function as a multimodal pre-trained model, substituting the video autoencoder pre-trained model utilized in this study. In terms of breadth, this work has covered the aspects of \"see\" (video) and \"listen\" (audio) in the model feature extraction process for input data, which is essential for further incorporating \"read\" (text) into the model inference process.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: General Multimodal Model vs. MultiMAE-DER. The uniqueness of",
      "page": 1
    },
    {
      "caption": "Figure 2: Time sequence of video frames and corresponding audio spectrograms.",
      "page": 2
    },
    {
      "caption": "Figure 3: Six Multimodal Sequence Fusion Strategies.",
      "page": 3
    },
    {
      "caption": "Figure 1: , the encoder will directly",
      "page": 3
    },
    {
      "caption": "Figure 2: , in the same time sequence, each sampled",
      "page": 3
    },
    {
      "caption": "Figure 3: , this work will propose six",
      "page": 3
    },
    {
      "caption": "Figure 1: , uniform downsampling is applied to",
      "page": 4
    },
    {
      "caption": "Figure 4: MultiMAE-DER Program Flow-Chart.",
      "page": 4
    },
    {
      "caption": "Figure 4: , depicting video",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "kwu020@fiu.edu": "Abstract—This paper presents a novel approach to processing",
          "obai@fiu.edu": ""
        },
        {
          "kwu020@fiu.edu": "multimodal\ndata\nfor\ndynamic\nemotion\nrecognition,\nnamed\nas",
          "obai@fiu.edu": ""
        },
        {
          "kwu020@fiu.edu": "the Multimodal Masked Autoencoder\nfor Dynamic\nEmotion",
          "obai@fiu.edu": ""
        },
        {
          "kwu020@fiu.edu": "Recognition (MultiMAE-DER). The MultiMAE-DER leverages",
          "obai@fiu.edu": ""
        },
        {
          "kwu020@fiu.edu": "the closely correlated representation information within spatio-",
          "obai@fiu.edu": ""
        },
        {
          "kwu020@fiu.edu": "temporal sequences across visual and audio modalities. By utiliz-",
          "obai@fiu.edu": ""
        },
        {
          "kwu020@fiu.edu": "ing\na pre-trained masked autoencoder model,\nthe MultiMAE-",
          "obai@fiu.edu": ""
        },
        {
          "kwu020@fiu.edu": "DER is\naccomplished\nthrough\nsimple,\nstraightforward\nfine-",
          "obai@fiu.edu": ""
        },
        {
          "kwu020@fiu.edu": "tuning. The performance\nof\nthe MultiMAE-DER is\nenhanced",
          "obai@fiu.edu": ""
        },
        {
          "kwu020@fiu.edu": "by\noptimizing\nsix\nfusion\nstrategies\nfor multimodal\ninput\nse-",
          "obai@fiu.edu": ""
        },
        {
          "kwu020@fiu.edu": "quences. These\nstrategies address dynamic\nfeature\ncorrelations",
          "obai@fiu.edu": ""
        },
        {
          "kwu020@fiu.edu": "within cross-domain data across\nspatial,\ntemporal, and spatio-",
          "obai@fiu.edu": ""
        },
        {
          "kwu020@fiu.edu": "temporal sequences. In comparison to state-of-the-art multimodal",
          "obai@fiu.edu": ""
        },
        {
          "kwu020@fiu.edu": "supervised\nlearning models\nfor\ndynamic\nemotion\nrecognition,",
          "obai@fiu.edu": ""
        },
        {
          "kwu020@fiu.edu": "MultiMAE-DER enhances the weighted average recall (WAR) by",
          "obai@fiu.edu": ""
        },
        {
          "kwu020@fiu.edu": "4.41% on the RAVDESS dataset and by 2.06% on the CREMA-",
          "obai@fiu.edu": ""
        },
        {
          "kwu020@fiu.edu": "D. Furthermore, when compared with the state-of-the-art model",
          "obai@fiu.edu": ""
        },
        {
          "kwu020@fiu.edu": "of multimodal self-supervised learning, MultiMAE-DER achieves",
          "obai@fiu.edu": ""
        },
        {
          "kwu020@fiu.edu": "",
          "obai@fiu.edu": "Fig. 1.\nGeneral Multimodal Model vs. MultiMAE-DER. The uniqueness of"
        },
        {
          "kwu020@fiu.edu": "a 1.86% higher WAR on the IEMOCAP dataset.",
          "obai@fiu.edu": ""
        },
        {
          "kwu020@fiu.edu": "",
          "obai@fiu.edu": "our approach lies in the capability to extract features from cross-domain data"
        },
        {
          "kwu020@fiu.edu": "Index\nTerms—Dynamic\nEmotion\nRecognition, Multimodal",
          "obai@fiu.edu": ""
        },
        {
          "kwu020@fiu.edu": "",
          "obai@fiu.edu": "using only a single encoder, eliminating the need for targeted feature extraction"
        },
        {
          "kwu020@fiu.edu": "Model, Self-Supervised Learning, Video Masked Autoencoder,",
          "obai@fiu.edu": ""
        },
        {
          "kwu020@fiu.edu": "",
          "obai@fiu.edu": "for different modalities."
        },
        {
          "kwu020@fiu.edu": "Vision Transformer",
          "obai@fiu.edu": ""
        },
        {
          "kwu020@fiu.edu": "I.\nINTRODUCTION",
          "obai@fiu.edu": ""
        },
        {
          "kwu020@fiu.edu": "",
          "obai@fiu.edu": "structure is well-suited for capturing the spatial and temporal"
        },
        {
          "kwu020@fiu.edu": "With the development of deep learning technology, convo-",
          "obai@fiu.edu": "context features in dynamic emotion. Examples of such models"
        },
        {
          "kwu020@fiu.edu": "lutional neural networks (CNN) have shown excellent\nresults",
          "obai@fiu.edu": "include Former-DFER [7] and STT-DFER [8]."
        },
        {
          "kwu020@fiu.edu": "in static emotion recognition, especially in facial expression",
          "obai@fiu.edu": "However, CNN, RNN, or Transformers [6], requires a large"
        },
        {
          "kwu020@fiu.edu": "recognition\n(FER)\ntasks,\nas\nseen\nin models\nlike RTCNN",
          "obai@fiu.edu": "amount of\nlabeled data for supervised learning to build high-"
        },
        {
          "kwu020@fiu.edu": "[1], DeepEmotion [2],\nand PAtt-Lite\n[3]. Dynamic\nemotion",
          "obai@fiu.edu": "performance models. This issue was only alleviated with the"
        },
        {
          "kwu020@fiu.edu": "recognition (DER)\nis one of\nthe important fields of affective",
          "obai@fiu.edu": "advent\nof\nself-supervised\nlearning,\nespecially when\napplied"
        },
        {
          "kwu020@fiu.edu": "computing. Psychologists\nand neuroscientists have been ex-",
          "obai@fiu.edu": "to natural\nlanguage processing (NLP), where\nthe BERT [9]"
        },
        {
          "kwu020@fiu.edu": "ploring this field for a long time, while constructing a series",
          "obai@fiu.edu": "model demonstrated performance beyond supervised learning"
        },
        {
          "kwu020@fiu.edu": "of quantitative rules for recognizing emotional characteristics.",
          "obai@fiu.edu": "models."
        },
        {
          "kwu020@fiu.edu": "In DER, a new architecture has emerged by combining CNN",
          "obai@fiu.edu": "In the field of computer vision,\nthe introduction of Masked"
        },
        {
          "kwu020@fiu.edu": "with recurrent neural networks (RNN). For example, ResNet-",
          "obai@fiu.edu": "Autoencoder\n(MAE)\nstrongly\nindicated\nthat\nself-supervised"
        },
        {
          "kwu020@fiu.edu": "18+LSTM [4] and C3D+LSTM [5] have been developed to",
          "obai@fiu.edu": "learning would shine in the field of vision such as ImageMAE"
        },
        {
          "kwu020@fiu.edu": "construct dynamic emotion recognition models. More impor-",
          "obai@fiu.edu": "[10], VideoMAE [11],\nand AudioMAE [12]. On\nthe\nother"
        },
        {
          "kwu020@fiu.edu": "tantly, with the\nadvent of Transformers\n[6],\nthere has been",
          "obai@fiu.edu": "hand, while single-modal (visual or audio) inputs have shown"
        },
        {
          "kwu020@fiu.edu": "further\nexploration into the global\ncontext\ncorrelation. This",
          "obai@fiu.edu": "good\nresults\nin\ndynamic\nemotion\nrecognition,\nas\nshown\nin"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "formance has not yet reached the level of real human emotion",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "",
          "C. Vision Transformer": "Vision Transformer\n(ViT)\n[18]\nis"
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "recognition. Therefore, multimodal\n(Visual-Audio)\ninput has",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "become\na desirable development\ntrend,\nas demonstrated by",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "models like AV-LSTM [15] and MSAF [16].",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "The aim of\nthis\nstudy is\nto explore a new framework for",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "",
          "C. Vision Transformer": "for\nthe\nserialization of"
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "dynamic emotion recognition model. The proposed approach",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "",
          "C. Vision Transformer": "[18]\nserves\nas\nthe\nbackbone\nnetwork"
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "is a straightforward inheritance and extension of VideoMAE",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "",
          "C. Vision Transformer": "which is used to associate the correlation of"
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "[11], extending the originally single-modal visual\ninput\ninto",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "",
          "C. Vision Transformer": "context\nin video data."
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "multimodal\ninput\nencompassing\nboth\nvisual\nand\naudio\nele-",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "",
          "C. Vision Transformer": "For MultiMAE-DER, ViT [18] will"
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "ments. Simultaneously, the self-supervised learning pre-trained",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "",
          "C. Vision Transformer": "as\nthe\nbackbone\nnetwork\nfor\nthe\nencoder."
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "Video Masked Autoencoder model\nis used to process visual-",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "",
          "C. Vision Transformer": "MultiMAE-DER will\ninherit\nthe\njoint"
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "audio\nsequence\nstrategies. This\nextended model\nis\nnamed",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "",
          "C. Vision Transformer": "attention mechanism from VideoMAE [11]"
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "MultiMAE-DER,\nstanding for Multimodal Masked Autoen-",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "coder\nfor Dynamic Emotion Recognition. The MultiMAE-",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "DER is\noptimized\nby\ninvestigating\nthe\nperformance\nof\nsix",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "",
          "C. Vision Transformer": "representation information in visual-audio data."
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "visual-audio sequence strategies.",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "II. RELATED WORK",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "A. Dynamic Emotion Recognition",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "Dynamic emotion recognition is a significant challenge in",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "the field of affective computing that requires the utilization of",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "multimodal\nfeature extraction, context semantic analysis, and",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "causal reasoning techniques. In recent years, researchers have",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "addressed the feature extraction task from multimodal using",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "transfer\nlearning by combining pre-trained models\nfor video",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "facial\nexpression feature\nextraction and audio tone\nemotion",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "feature\nextraction,\nsuch\nas MSAF [16]\nand CFN-SR [17].",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "However,\nthe\nabove-mentioned model\narchitecture\nfails\nto",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "establish cross-domain associative features between visual and",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "audio data,\nleading to the\nloss of highly correlated spatio-",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "temporal\nfeature\ndimensions\nin\nthe\nemotion\nrepresentation",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "information. Therefore, we propose a MultiMAE-DER frame-",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "work, a unified feature extraction pre-trained model\nthat will",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "be\nemployed for visual\nand audio multimodal data\nto find",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "the associated features in visual-audio spatio-temporal emotion",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "representation information.",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "B. Masked Autoencoder",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "Masked Autoencoder\nis a variant of denoising autoencoder",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "that\nconsists\nof\nan\nencoder\nand\na\ndecoder.\nIn\nImageMAE",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "[10],\nthat\nis designed as\nan asymmetric\nstructure\nto recon-",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "struct normalized pixels. VideoMAE [11]\nfollows\nthe design",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "approach of\nImageMAE [10] but extends the 2D image input",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "to 3D video input and incorporates a joint spatio-temporal self-",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "attention mechanism to replace the vanilla Vision Transformer",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "[18] spatial self-attention mechanism.",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "For downstream tasks,\nthe decoder of\nthe masked autoen-",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "coder\nis discarded, and only the pre-trained encoder\nis used",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "for supervised learning and fine-tuning on the downstream task",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "dataset. As a leader\nin self-supervised learning, our proposal",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "will\ncontinue\nto\ninherit\nthis\ncharacteristic\nand\nutilize\nthe",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "pre-trained encoder model\nto fine-tune multimodal dynamic",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "emotion data, aiming for\nimproved emotion recognition per-",
          "C. Vision Transformer": ""
        },
        {
          "models\nlike HuBERT [13] and MAE-DFER [14],\ntheir per-": "formance.",
          "C. Vision Transformer": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "•\nFrom the temporal perspective, the video signal and audio": "signal\nare\nsequence\ntransform to\nexplore\nthe\ndynamic"
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "representation information of\nthe video and audio signal"
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "in time sequence under\nthe condition of spatial\nintegrity."
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "Therefore, as\nshown in Fig. 3,\nthis work will propose six"
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "different multimodal\nsequence\nfusion strategies\nto optimize"
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "MultiMAE-DER performance based on the above hypotheses."
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "For strategy 1 (CFAS), 16 frames of video V ∈ R16×112×224"
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "and corresponding 16 audio spectrograms A ∈ R16×112×224"
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "will be directly spliced to explore whether\nthere is a certain"
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "correlation in the spatiotemporal sequence for this multimodal"
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "input X ∈ R16×224×224. For\nstrategy 2 (SFAS), 16 frames"
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "∈ R16×224×224\nof\nvideo V\nand\ncorresponding\n16\naudio"
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": ""
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "spectrograms A ∈ R16×224×224 will be normalized, and then"
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "directly added to verify the feasibility of spatially fusing audio-"
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "visual signals. For strategy 3 (FFLS), considering input size"
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": ""
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "constraints,\nthe single clip are uniformly downsampled to 8"
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": ""
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "frames of video V ∈ R8×224×224 and corresponding 8 audio"
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "spectrograms A ∈ R8×224×224. They\nare\nsorted\nbased\non"
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "facial\nframes first\nand then audio spectrograms\nto consider"
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "the impact of solely the temporal sequence in the visual-audio"
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "sorting of multimodal\ninput X ∈ R16×224×224\non emotion"
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "recognition results. The mathematical\nexpressions\nfor\nthese"
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "three multimodal sequence fusion strategies are as follows:"
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": ""
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": ""
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "(1)\nXi = Concat(Vi, Ai)"
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": ""
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": ""
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "(2)\nXi = Add(Norm(Vi), Norm(Ai))"
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": ""
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": ""
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "(3)\nX = Seq(V1, V2, . . . , V8, A1, A2, . . . , A8)"
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": ""
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "For strategy 4 (FSLF),\nfollowing the method of Strategy"
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "3,\nthe\norder\nof\nvideo\nframes V ∈ R8×224×224\nand\naudio"
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "spectrograms A ∈ R8×224×224\nare swapped to consider\nthe"
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": ""
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "impact of different orders in the temporal sequence of visual-"
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "audio input X ∈ R16×224×224 on emotion recognition results."
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "For\nstrategy\n5\n(OFOS),\nfollowing the order of one video"
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "frame V ∈ R8×224×224\nfollowed by one audio spectrogram"
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "A ∈ R8×224×224,\nto explore the impact of different permuta-"
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "tions of\nthe time sequence on emotion recognition results."
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "Finally, for strategy 6 (RFAS), randomly sorting 8 frames"
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "of video V ∈ R8×224×224 and corresponding 8 spectrograms"
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "A ∈ R8×224×224\nto verify the\ninfluence of unordered tem-"
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "poral\nsequence on visual-audio multimodal data\ninput X ∈"
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "R16×224×224 on emotion recognition results. The mathemat-"
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "ical\nexpressions\nfor\nthese\nthree multimodal\nsequence\nfusion"
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "strategies are as follows:"
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": ""
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": ""
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "(4)\nX = Seq(A1, A2, . . . , A8, V1, V2, . . . , V8)"
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": ""
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": ""
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "(5)\nX = Seq(V1, A1, V2, A2, . . . , V8, A8)"
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": ""
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": ""
        },
        {
          "•\nFrom the temporal perspective, the video signal and audio": "(6)\nX = Rand(V1, V2, . . . , V8, A1, A2, . . . , A8)"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "C. MultiMAE-DER: Model Structure": "As\nshown in Fig. 1, uniform downsampling is applied to"
        },
        {
          "C. MultiMAE-DER: Model Structure": "both video and audio in the single clip to align them and meet"
        },
        {
          "C. MultiMAE-DER: Model Structure": "input size requirements. Subsequently, different sequence fu-"
        },
        {
          "C. MultiMAE-DER: Model Structure": "sion strategies from Section III.B are employed as multimodal"
        },
        {
          "C. MultiMAE-DER: Model Structure": "input X ∈ R16×224×224 to verify the assumptions of this work."
        },
        {
          "C. MultiMAE-DER: Model Structure": "Like VideoMAE [11], before entering the encoder,\nthe input"
        },
        {
          "C. MultiMAE-DER: Model Structure": "data is patched to construct\ntoken sequence S ∈ R1568×512. In"
        },
        {
          "C. MultiMAE-DER: Model Structure": "this model, 3D convolutional kernels are used as embedding"
        },
        {
          "C. MultiMAE-DER: Model Structure": "layers to perform non-overlapping patching, where the kernel"
        },
        {
          "C. MultiMAE-DER: Model Structure": "size is 2 × 16 × 16. Linear projection (LP)\nis applied to each"
        },
        {
          "C. MultiMAE-DER: Model Structure": "X ∈ R1568×1024\ncube patch to flatten the input\ninto the en-"
        },
        {
          "C. MultiMAE-DER: Model Structure": "coder. After processing through the encoder, the output feature"
        },
        {
          "C. MultiMAE-DER: Model Structure": "matrix F ∈ R1568×1024 with the same input size is obtained."
        },
        {
          "C. MultiMAE-DER: Model Structure": "For a given downstream task (dynamic emotion recognition in"
        },
        {
          "C. MultiMAE-DER: Model Structure": "this\ncase),\na\nsingle\nlinear\nlayer\n(MLP)\nserves\nas\na\nspecific-"
        },
        {
          "C. MultiMAE-DER: Model Structure": "task\nhead\nto\nprocess\nthe\nfeature matrix F ∈ R1568×1024,"
        },
        {
          "C. MultiMAE-DER: Model Structure": "yielding the final emotion result Y ∈ R1×7\nfor\nthis clip. The"
        },
        {
          "C. MultiMAE-DER: Model Structure": "mathematical expressions for the above process are as follows:"
        },
        {
          "C. MultiMAE-DER: Model Structure": "S = Patching(X)\n(7)"
        },
        {
          "C. MultiMAE-DER: Model Structure": "ˆ"
        },
        {
          "C. MultiMAE-DER: Model Structure": "X = LP(S)\n(8)"
        },
        {
          "C. MultiMAE-DER: Model Structure": "F = Encoder( ˆX)\n(9)"
        },
        {
          "C. MultiMAE-DER: Model Structure": "Y = MLP(F)\n(10)"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "videos\nfor\nthe neutral emotion, and the remaining emotions": ""
        },
        {
          "videos\nfor\nthe neutral emotion, and the remaining emotions": "each\nhave\n192\nvideos. Our model\nevaluation\nis\nconducted"
        },
        {
          "videos\nfor\nthe neutral emotion, and the remaining emotions": "using a 6-fold cross-validation on subjects-independent of the"
        },
        {
          "videos\nfor\nthe neutral emotion, and the remaining emotions": "emotion."
        },
        {
          "videos\nfor\nthe neutral emotion, and the remaining emotions": "2) CREMA-D [20]: Crowd-sourced Emotional Multimodal"
        },
        {
          "videos\nfor\nthe neutral emotion, and the remaining emotions": "Actor Dataset comprises emotional performances by 91 pro-"
        },
        {
          "videos\nfor\nthe neutral emotion, and the remaining emotions": "fessional actors from around the world,\nrepresenting different"
        },
        {
          "videos\nfor\nthe neutral emotion, and the remaining emotions": "countries, and ethnicities. These actors conducted emotional"
        },
        {
          "videos\nfor\nthe neutral emotion, and the remaining emotions": "performances\nin\na\nlaboratory\nenvironment with\ndiverse\nac-"
        },
        {
          "videos\nfor\nthe neutral emotion, and the remaining emotions": "cents. Each actor presented six different emotions: anger, dis-"
        },
        {
          "videos\nfor\nthe neutral emotion, and the remaining emotions": "gust,\nfear, happiness, neutral, and sadness. Additionally,\nthey"
        },
        {
          "videos\nfor\nthe neutral emotion, and the remaining emotions": "expressed each emotion at\nlow, medium, high, and unspecified"
        },
        {
          "videos\nfor\nthe neutral emotion, and the remaining emotions": "intensity levels\nto dialogue. The dataset\nincludes\na\ntotal of"
        },
        {
          "videos\nfor\nthe neutral emotion, and the remaining emotions": "7,442 video clips, with 1,087 clips\nfor\nthe neutral\nemotion"
        },
        {
          "videos\nfor\nthe neutral emotion, and the remaining emotions": "and 1,271 clips\nfor each of\nthe remaining emotions. Model"
        },
        {
          "videos\nfor\nthe neutral emotion, and the remaining emotions": "evaluation will be conducted using 5-fold cross-validation on"
        },
        {
          "videos\nfor\nthe neutral emotion, and the remaining emotions": "subjects-independent of\nthe emotion."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: We observe that the optimal strategy",
      "data": [
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": "✓"
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": "83.23\n83.61\nMultiMAE-DER-FSLF\nV+A"
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": ""
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": ""
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": ""
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": "supervised model by 4.41% (83.61% vs. 79.20%)\nin terms"
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": ""
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": "of WAR. Among the self-supervised multimodal models, our"
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": ""
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": "optimal\nstrategy (i.e.,\nstrategy 4 FSLF) of MultiMAE-DER"
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": ""
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": "also outperforms the multimodal model VQ-MAE-AV [30] by"
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": ""
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": "0.41% (83.61% vs. 83.20%)\nin terms of WAR."
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": ""
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": "The visual-audio results on the CREMA-D [20]\nare pre-"
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": ""
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": "sented\nin\nTable\n2. We\nobserve\nthat\nthe\noptimal\nstrategy"
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": ""
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": "of MultiMAE-DER achieves\nthe WAR result\nsurpassing the"
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": ""
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": "VQ-MAE-AV [30] multimodal self-supervised learning model"
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": ""
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": "by\n0.96% (79.36% vs.\n78.40%). Compared\nto\nsupervised"
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": ""
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": "learning approaches,\nthe optimal strategy of MultiMAE-DER"
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": ""
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": "outperforms\nthe multimodal model RAVER [34] by 2.06%"
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": ""
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": "(79.36% vs. 77.30%) in WAR. In addition, most of the results"
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": ""
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": "of other strategies in MultiMAE-DER outperform state-of-the-"
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": ""
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": "art multimodal supervised learning models. This indicates the"
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": ""
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": "effectiveness of multimodal sequence fusion strategies, which"
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": ""
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": "can\naid\nin\nidentifying\nspatio-temporal\ncorrelations\nacross"
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": ""
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": "cross-domain data to some extent."
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": ""
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": "In\nTable\n3, we\ncompare\nstate-of-the-art\nself-supervised"
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": ""
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": "multimodal models on the\nIEMOCAP [21]. The\nresults\nare"
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": ""
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": "derived\nfrom the\nevaluation\ntest\nconducted\nby AV-Superb"
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": "[22]. The results\nshowed that\nin the field of\nself-supervised"
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": "learning, our MultiMAE-DER optimal strategy model (FSLF)"
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": "outperforms\nthe multimodal model AVBERT [37] by 1.86%"
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": "(63.73% vs. 61.87%)\nin WAR. Upon observing the results of"
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": "multiple\nstrategies\nin\nour model\nand\ncomparing them with"
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": "the\nresults\nof\nthese\nthree\nself-supervised multimodal mod-"
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": "els, we notice that MultiMAE-DER model performs\nslightly"
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": "better\nin handling cross-domain data. This\nindirectly reflects"
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": "the model’s\nability\nto\nextract\nrepresentational\ninformation"
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": "regarding\nspatio-temporal\ncorrelations\nacross\ncross-domain"
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": "data. Moreover,\nthese dynamic features play a crucial\nrole in"
        },
        {
          "✓\n82.27\nMultiMAE-DER-FFLS\nV+A\n83.56": "inferring emotional context."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 2: We observe that the optimal strategy",
      "data": [
        {
          "B.\nImplementation Details": "",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "1) Downsampling: As discussed in Section III.B regarding",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "",
          "TABLE I": "METHOD OR NOT. UAR: UNWEIGHTED AVERAGE RECALL. WAR:"
        },
        {
          "B.\nImplementation Details": "the strategies for multimodal sequence fusion, downsampling",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "",
          "TABLE I": "RESULT. BOLD: THE BEST RESULT."
        },
        {
          "B.\nImplementation Details": "is\na\ncrucial\nstep\nin\nthis\nproposal. Depending\non\nthe\ndif-",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "ferent\nsequence\nfusion strategies, we have varying uniform",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "",
          "TABLE I": "SSL\nModality\nUAR"
        },
        {
          "B.\nImplementation Details": "downsampling steps. Taking the RAVDESS [19] dataset as an",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "",
          "TABLE I": "χ\n−\nV+A"
        },
        {
          "B.\nImplementation Details": "example, for strategies 1 and 2, where facial videos and audio",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "",
          "TABLE I": "χ\n−\nV+A"
        },
        {
          "B.\nImplementation Details": "spectrograms have 16 frames per clip,\nthe downsampling step",
          "TABLE I": "χ\n−\nV+A"
        },
        {
          "B.\nImplementation Details": "",
          "TABLE I": "χ\n−\nV+A"
        },
        {
          "B.\nImplementation Details": "size\nis\nset\nto 6. For\nstrategies 3, 4, 5,\nand 6, where\nfacial",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "",
          "TABLE I": "χ\n−\nV+A"
        },
        {
          "B.\nImplementation Details": "videos\nand audio spectrograms have 8 frames per\nclip,\nthe",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "",
          "TABLE I": "χ\n−\nV+A"
        },
        {
          "B.\nImplementation Details": "downsampling step size is\nset\nto 12. The data preprocessing",
          "TABLE I": "χ\n−\nV+A"
        },
        {
          "B.\nImplementation Details": "",
          "TABLE I": "χ\n−\nV+A"
        },
        {
          "B.\nImplementation Details": "for\nthe above process is illustrated in Fig. 4, depicting video",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "",
          "TABLE I": "χ\n−\nV+A"
        },
        {
          "B.\nImplementation Details": "preprocessing and audio preprocessing. For video preprocess-",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "",
          "TABLE I": "χ\n−\nV+A"
        },
        {
          "B.\nImplementation Details": "",
          "TABLE I": "✓"
        },
        {
          "B.\nImplementation Details": "ing,\nthe cutoff\ntime of a single clip is computed based on the",
          "TABLE I": "−\nV+A"
        },
        {
          "B.\nImplementation Details": "",
          "TABLE I": "✓\n−\nV"
        },
        {
          "B.\nImplementation Details": "required frame number and downsampling step size, followed",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "",
          "TABLE I": "✓\n−\nA"
        },
        {
          "B.\nImplementation Details": "by the downsampling of\nthe clip. Simultaneously,\nfor audio",
          "TABLE I": "✓"
        },
        {
          "B.\nImplementation Details": "",
          "TABLE I": "75.97\nV+A"
        },
        {
          "B.\nImplementation Details": "",
          "TABLE I": "✓"
        },
        {
          "B.\nImplementation Details": "preprocessing,\nthe audio signal of\nthe clip is extracted. The",
          "TABLE I": "75.79\nV+A"
        },
        {
          "B.\nImplementation Details": "",
          "TABLE I": "✓\n77.78\nV+A"
        },
        {
          "B.\nImplementation Details": "time point T corresponding to the facial\nframe is determined",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "",
          "TABLE I": "✓\n80.65\nV+A"
        },
        {
          "B.\nImplementation Details": "based on the frame number. Then,\nthe audio signal\nis cut at",
          "TABLE I": "✓"
        },
        {
          "B.\nImplementation Details": "",
          "TABLE I": "82.27\nV+A"
        },
        {
          "B.\nImplementation Details": "",
          "TABLE I": "✓"
        },
        {
          "B.\nImplementation Details": "the time point T − 1 and T + 1 to perform Short-Time Fourier",
          "TABLE I": "83.23\nV+A"
        },
        {
          "B.\nImplementation Details": "Transform (STFT)\nand Mel-Frequency Cepstral Coefficients",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "(MFCC) to obtain the audio spectrogram corresponding to the",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "facial\nframe at\ntime point T .",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "2) Pre-training: After\npreprocessing,\nthe\nobtained\nfacial",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "frames\nand audio spectrograms\nimplement\nsequence\nfusion,",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "",
          "TABLE I": "strategy 4 FSLF) of MultiMAE-DER"
        },
        {
          "B.\nImplementation Details": "which includes methods\nsuch as\nsplice,\naddition,\nand com-",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "bine. For\nthe pre-trained encoder model,\nto eliminate domain",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "",
          "TABLE I": "in terms of WAR."
        },
        {
          "B.\nImplementation Details": "differences,\nthe pre-trained encoder model used in our pro-",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "posal\nis derived from MAE-DFER [14]. This encoder\nis built",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "",
          "TABLE I": "observe\nthat\nthe\noptimal"
        },
        {
          "B.\nImplementation Details": "exclusively\nupon VideoMAE [11]\nand\nhas\nundergone\nself-",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "",
          "TABLE I": "the WAR result"
        },
        {
          "B.\nImplementation Details": "supervised training on the extensive dataset of\nfacial videos",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "from YouTube, aiming to mitigate domain differences.",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "",
          "TABLE I": "78.40%). Compared\nto"
        },
        {
          "B.\nImplementation Details": "3) Fine-tuning: As described in Section III.C,\nto minimize",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "",
          "TABLE I": "the optimal strategy of MultiMAE-DER"
        },
        {
          "B.\nImplementation Details": "the interference of\ninductive bias, our approach utilizes only",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "",
          "TABLE I": "the multimodal model RAVER [34] by 2.06%"
        },
        {
          "B.\nImplementation Details": "single-layer neural network as\na\nspecific-task head for fine-",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "tuning on the downstream task. The fine-tuning employs\nthe",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "AdamW [23] optimizer with the base learning rate of 1e-2,",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "weight decay of 0.05, sparse categorical cross-entropy as the",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "loss\nfunction,\nand sparse\ncategorical\naccuracy as\nthe metric",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "",
          "TABLE I": "spatio-temporal\ncorrelations"
        },
        {
          "B.\nImplementation Details": "function. The evaluation metrics used by the model\ninclude",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "unweighted average recall (UAR) and weighted average recall",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "",
          "TABLE I": "state-of-the-art"
        },
        {
          "B.\nImplementation Details": "(WAR).",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "",
          "TABLE I": "IEMOCAP [21]. The"
        },
        {
          "B.\nImplementation Details": "C. Results",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "",
          "TABLE I": "test\nconducted"
        },
        {
          "B.\nImplementation Details": "In\nthis\nsection, we first\ncompared MultiMAE-DER with",
          "TABLE I": "in the field of"
        },
        {
          "B.\nImplementation Details": "other\nprevious\nstate-of-the-art multimodal models\non\nthe",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "RAVDESS [19]. As\nshown\nin Table\n1,\nin\nthe multimodal",
          "TABLE I": "the multimodal model AVBERT [37] by 1.86%"
        },
        {
          "B.\nImplementation Details": "model domain,\nthe results exhibit significant variations based",
          "TABLE I": "in WAR. Upon observing the results of"
        },
        {
          "B.\nImplementation Details": "on\ndifferent\nlearning methods, whether\nsupervised\nlearning",
          "TABLE I": "our model\nand"
        },
        {
          "B.\nImplementation Details": "or\nself-supervised\nlearning. The\nself-supervised multimodal",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "model VQ-MAE-AV [30] outperforms\nthe supervised multi-",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "modal model AVT [29] by 4% (83.20% vs. 79.20%) in terms",
          "TABLE I": ""
        },
        {
          "B.\nImplementation Details": "of WAR. Furthermore, our MultiMAE-DER model, employ-",
          "TABLE I": "extract\nrepresentational"
        },
        {
          "B.\nImplementation Details": "ing\ndifferent multimodal\nsequence\nfusion\nstrategies,\ngener-",
          "TABLE I": "correlations\nacross"
        },
        {
          "B.\nImplementation Details": "ally surpasses the supervised multimodal model. Particularly,",
          "TABLE I": "these dynamic features play a crucial"
        },
        {
          "B.\nImplementation Details": "when using strategy 4, MultiMAE-DER-FSLF outperforms the",
          "TABLE I": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE II": "",
          "According to our": "",
          "analysis, different multimodal": "",
          "sequence": ""
        },
        {
          "TABLE II": "",
          "According to our": "",
          "analysis, different multimodal": "",
          "sequence": ""
        },
        {
          "TABLE II": "METHOD OR NOT. UAR: UNWEIGHTED AVERAGE RECALL. WAR:",
          "According to our": "",
          "analysis, different multimodal": "",
          "sequence": ""
        },
        {
          "TABLE II": "",
          "According to our": "we know,",
          "analysis, different multimodal": "",
          "sequence": ""
        },
        {
          "TABLE II": "WEIGHTED AVERAGE RECALL. UNDERLINED: THE BEST SUPERVISED",
          "According to our": "",
          "analysis, different multimodal": "",
          "sequence": ""
        },
        {
          "TABLE II": "RESULT. BOLD: THE BEST RESULT.",
          "According to our": "",
          "analysis, different multimodal": "",
          "sequence": ""
        },
        {
          "TABLE II": "",
          "According to our": "",
          "analysis, different multimodal": "",
          "sequence": ""
        },
        {
          "TABLE II": "SSL",
          "According to our": "",
          "analysis, different multimodal": "",
          "sequence": ""
        },
        {
          "TABLE II": "χ",
          "According to our": "",
          "analysis, different multimodal": "",
          "sequence": ""
        },
        {
          "TABLE II": "",
          "According to our": "every\n98\nsequences",
          "analysis, different multimodal": "the",
          "sequence": "token\nsequences will"
        },
        {
          "TABLE II": "χ",
          "According to our": "",
          "analysis, different multimodal": "",
          "sequence": ""
        },
        {
          "TABLE II": "",
          "According to our": "",
          "analysis, different multimodal": "",
          "sequence": "exchange. Strategy 2 is\nthe"
        },
        {
          "TABLE II": "χ",
          "According to our": "",
          "analysis, different multimodal": "",
          "sequence": ""
        },
        {
          "TABLE II": "χ",
          "According to our": "",
          "analysis, different multimodal": "",
          "sequence": ""
        },
        {
          "TABLE II": "χ",
          "According to our": "",
          "analysis, different multimodal": "",
          "sequence": ""
        },
        {
          "TABLE II": "",
          "According to our": "disrupt",
          "analysis, different multimodal": "",
          "sequence": ""
        },
        {
          "TABLE II": "χ",
          "According to our": "",
          "analysis, different multimodal": "",
          "sequence": ""
        },
        {
          "TABLE II": "",
          "According to our": "",
          "analysis, different multimodal": "",
          "sequence": "results"
        },
        {
          "TABLE II": "χ",
          "According to our": "",
          "analysis, different multimodal": "",
          "sequence": ""
        },
        {
          "TABLE II": "χ",
          "According to our": "",
          "analysis, different multimodal": "",
          "sequence": "the first 784 sequences\nare"
        },
        {
          "TABLE II": "✓",
          "According to our": "",
          "analysis, different multimodal": "",
          "sequence": ""
        },
        {
          "TABLE II": "",
          "According to our": "visual,\nand\nthe",
          "analysis, different multimodal": "784",
          "sequence": "are\naudio. Strategy\n4"
        },
        {
          "TABLE II": "✓",
          "According to our": "",
          "analysis, different multimodal": "",
          "sequence": ""
        },
        {
          "TABLE II": "✓",
          "According to our": "involves\nswapping",
          "analysis, different multimodal": "order\nof",
          "sequence": "data\nand\naudio\ndata"
        },
        {
          "TABLE II": "",
          "According to our": "",
          "analysis, different multimodal": "",
          "sequence": ""
        },
        {
          "TABLE II": "✓",
          "According to our": "",
          "analysis, different multimodal": "",
          "sequence": ""
        },
        {
          "TABLE II": "",
          "According to our": "from strategy 3.",
          "analysis, different multimodal": "",
          "sequence": "the"
        },
        {
          "TABLE II": "✓",
          "According to our": "",
          "analysis, different multimodal": "",
          "sequence": ""
        },
        {
          "TABLE II": "",
          "According to our": "first 784 sequences",
          "analysis, different multimodal": "audio,",
          "sequence": "latter 784 sequences"
        },
        {
          "TABLE II": "✓",
          "According to our": "",
          "analysis, different multimodal": "",
          "sequence": ""
        },
        {
          "TABLE II": "✓",
          "According to our": "",
          "analysis, different multimodal": "",
          "sequence": ""
        },
        {
          "TABLE II": "",
          "According to our": "",
          "analysis, different multimodal": "",
          "sequence": ""
        },
        {
          "TABLE II": "✓",
          "According to our": "",
          "analysis, different multimodal": "",
          "sequence": ""
        },
        {
          "TABLE II": "",
          "According to our": "data\nand",
          "analysis, different multimodal": "audio",
          "sequence": "the\nresulting\n1568"
        },
        {
          "TABLE II": "✓",
          "According to our": "",
          "analysis, different multimodal": "",
          "sequence": ""
        },
        {
          "TABLE II": "",
          "According to our": "token\nsequences,",
          "analysis, different multimodal": "of\neach",
          "sequence": "dimensions\nare"
        },
        {
          "TABLE II": "",
          "According to our": "",
          "analysis, different multimodal": "",
          "sequence": ""
        },
        {
          "TABLE II": "",
          "According to our": "Strategy 6 involves",
          "analysis, different multimodal": "randomly sorting visual data",
          "sequence": "and audio"
        },
        {
          "TABLE II": "TABLE III",
          "According to our": "",
          "analysis, different multimodal": "",
          "sequence": ""
        },
        {
          "TABLE II": "",
          "According to our": "data. The\nresulting",
          "analysis, different multimodal": "token",
          "sequence": "lack\ncoherence,"
        },
        {
          "TABLE II": "RESULTS ON IEMOCAP (4-CLASS). SSL: SELF-SUPERVISED LEARNING",
          "According to our": "",
          "analysis, different multimodal": "",
          "sequence": ""
        },
        {
          "TABLE II": "",
          "According to our": "",
          "analysis, different multimodal": "",
          "sequence": "sequence."
        },
        {
          "TABLE II": "METHOD OR NOT. UAR: UNWEIGHTED AVERAGE RECALL. WAR:",
          "According to our": "",
          "analysis, different multimodal": "",
          "sequence": ""
        },
        {
          "TABLE II": "WEIGHTED AVERAGE RECALL. BOLD: THE BEST RESULT.",
          "According to our": "",
          "analysis, different multimodal": "",
          "sequence": ""
        },
        {
          "TABLE II": "",
          "According to our": "•",
          "analysis, different multimodal": "",
          "sequence": "lacks spatial"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Strategy 6 involves": "",
          "randomly sorting visual data": "",
          "and audio": ""
        },
        {
          "Strategy 6 involves": "data. The",
          "randomly sorting visual data": "1568",
          "and audio": "coherence,"
        },
        {
          "Strategy 6 involves": "",
          "randomly sorting visual data": "",
          "and audio": ""
        },
        {
          "Strategy 6 involves": "whether visual-audio sequence or",
          "randomly sorting visual data": "",
          "and audio": "sequence."
        },
        {
          "Strategy 6 involves": "",
          "randomly sorting visual data": "",
          "and audio": ""
        },
        {
          "Strategy 6 involves": "Finally, we can draw the following observations:",
          "randomly sorting visual data": "",
          "and audio": ""
        },
        {
          "Strategy 6 involves": "",
          "randomly sorting visual data": "",
          "and audio": "lacks spatial"
        },
        {
          "Strategy 6 involves": "",
          "randomly sorting visual data": "",
          "and audio": ""
        },
        {
          "Strategy 6 involves": "continuity.",
          "randomly sorting visual data": "",
          "and audio": ""
        },
        {
          "Strategy 6 involves": "",
          "randomly sorting visual data": "",
          "and audio": ""
        },
        {
          "Strategy 6 involves": "",
          "randomly sorting visual data": "",
          "and audio": ""
        },
        {
          "Strategy 6 involves": "",
          "randomly sorting visual data": "",
          "and audio": ""
        },
        {
          "Strategy 6 involves": "",
          "randomly sorting visual data": "",
          "and audio": ""
        },
        {
          "Strategy 6 involves": "",
          "randomly sorting visual data": "",
          "and audio": ""
        },
        {
          "Strategy 6 involves": "",
          "randomly sorting visual data": "",
          "and audio": ""
        },
        {
          "Strategy 6 involves": "",
          "randomly sorting visual data": "",
          "and audio": ""
        },
        {
          "Strategy 6 involves": "",
          "randomly sorting visual data": "",
          "and audio": "spatio-temporal"
        },
        {
          "Strategy 6 involves": "",
          "randomly sorting visual data": "",
          "and audio": ""
        },
        {
          "Strategy 6 involves": "",
          "randomly sorting visual data": "",
          "and audio": ""
        },
        {
          "Strategy 6 involves": "",
          "randomly sorting visual data": "",
          "and audio": ""
        },
        {
          "Strategy 6 involves": "",
          "randomly sorting visual data": "",
          "and audio": ""
        },
        {
          "Strategy 6 involves": "",
          "randomly sorting visual data": "",
          "and audio": ""
        },
        {
          "Strategy 6 involves": "all",
          "randomly sorting visual data": "",
          "and audio": ""
        },
        {
          "Strategy 6 involves": "",
          "randomly sorting visual data": "",
          "and audio": ""
        },
        {
          "Strategy 6 involves": "Strategy",
          "randomly sorting visual data": "both",
          "and audio": "continuity,"
        },
        {
          "Strategy 6 involves": "",
          "randomly sorting visual data": "",
          "and audio": ""
        },
        {
          "Strategy 6 involves": "relation.",
          "randomly sorting visual data": "",
          "and audio": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "simultaneously disrupting the overall spatio-temporal cor-": "relation."
        },
        {
          "simultaneously disrupting the overall spatio-temporal cor-": ""
        },
        {
          "simultaneously disrupting the overall spatio-temporal cor-": "V. CONCLUSIONS AND FUTURE WORK"
        },
        {
          "simultaneously disrupting the overall spatio-temporal cor-": ""
        },
        {
          "simultaneously disrupting the overall spatio-temporal cor-": ""
        },
        {
          "simultaneously disrupting the overall spatio-temporal cor-": "A. CONCLUSIONS"
        },
        {
          "simultaneously disrupting the overall spatio-temporal cor-": ""
        },
        {
          "simultaneously disrupting the overall spatio-temporal cor-": "This work represents\na new exploration of\nthe\napproach"
        },
        {
          "simultaneously disrupting the overall spatio-temporal cor-": "to\nhandling multimodal\ninputs,\nestablishing\na\nnovel\nframe-"
        },
        {
          "simultaneously disrupting the overall spatio-temporal cor-": "work for\nthe integrated processing of multimodal data under"
        },
        {
          "simultaneously disrupting the overall spatio-temporal cor-": "the\nconditions of\nself-supervised learning architecture. This"
        },
        {
          "simultaneously disrupting the overall spatio-temporal cor-": "paper\ninvestigates\nsix different multimodal\nsequence\nfusion"
        },
        {
          "simultaneously disrupting the overall spatio-temporal cor-": "strategies to explore the diverse interpretations of multimodal"
        },
        {
          "simultaneously disrupting the overall spatio-temporal cor-": "representation information extracted by the same model when"
        },
        {
          "simultaneously disrupting the overall spatio-temporal cor-": "exposed to different spatio-temporal sequence inputs. Results"
        },
        {
          "simultaneously disrupting the overall spatio-temporal cor-": "indicate\nthat\nfusing multimodal data on spatio-temporal\nse-"
        },
        {
          "simultaneously disrupting the overall spatio-temporal cor-": "quences significantly improves the model performance by cap-"
        },
        {
          "simultaneously disrupting the overall spatio-temporal cor-": "turing correlations between cross-domain data. Furthermore,"
        },
        {
          "simultaneously disrupting the overall spatio-temporal cor-": "MultiMAE-DER validates\nthe\nperformance\neffectiveness\nof"
        },
        {
          "simultaneously disrupting the overall spatio-temporal cor-": "self-supervised learning as an efficient\nlearner, contributing to"
        },
        {
          "simultaneously disrupting the overall spatio-temporal cor-": "the potential of the masked autoencoder as a unified framework"
        },
        {
          "simultaneously disrupting the overall spatio-temporal cor-": "for solving contextual semantic inference problems."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "Database\nof Emotional Speech\nand Song\n(RAVDESS): A dynamic,"
        },
        {
          "B. FUTURE WORK": "In future work,\nthe\nframework can be\nfurther developed",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "multimodal\nset\nof\nfacial\nand\nvocal\nexpressions\nin North American"
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "English.” PloS one 13.5 (2018): e0196391."
        },
        {
          "B. FUTURE WORK": "from two\naspects.\nIn\nterms\nof\ndepth,\nthe\nself-supervised",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "[20] Cao, Houwei,\net\nal.\n”Crema-d: Crowd-sourced emotional multimodal"
        },
        {
          "B. FUTURE WORK": "learning\napproach\nof\nthe multimodal masked\nautoencoder",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "actors dataset.”\nIEEE transactions on affective\ncomputing 5.4 (2014):"
        },
        {
          "B. FUTURE WORK": "involves\nreconstructing the input of multimodal visual-audio",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "377-390."
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "[21] Busso, Carlos, et al. ”IEMOCAP:\nInteractive emotional dyadic motion"
        },
        {
          "B. FUTURE WORK": "data to enhance the pre-training encoder. This enhanced model",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "capture database.” Language resources and evaluation 42 (2008): 335-"
        },
        {
          "B. FUTURE WORK": "can subsequently function as a multimodal pre-trained model,",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "359."
        },
        {
          "B. FUTURE WORK": "substituting the video autoencoder pre-trained model utilized",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "[22]\nTseng, Yuan, et al. ”Av-superb: A multi-task evaluation benchmark for"
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "audio-visual\nrepresentation models.”\narXiv preprint\narXiv:2309.10787"
        },
        {
          "B. FUTURE WORK": "in this\nstudy.\nIn terms of breadth,\nthis work has covered the",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "(2023)."
        },
        {
          "B. FUTURE WORK": "aspects\nof\n“see”\n(video)\nand\n“listen”\n(audio)\nin\nthe model",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "[23]\nLoshchilov,\nIlya, and Frank Hutter. ”Decoupled weight decay regular-"
        },
        {
          "B. FUTURE WORK": "feature\nextraction process\nfor\ninput data, which is\nessential",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "ization.” arXiv preprint arXiv:1711.05101 (2017)."
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "[24]\nFukui,\nAkira,\net\nal.\n”Multimodal\ncompact\nbilinear\npooling\nfor"
        },
        {
          "B. FUTURE WORK": "for further incorporating “read” (text) into the model inference",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "visual\nquestion\nanswering\nand\nvisual\ngrounding.”\narXiv\npreprint"
        },
        {
          "B. FUTURE WORK": "process.",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "arXiv:1606.01847 (2016)."
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "[25]\nJoze, Hamid Reza Vaezi, et al. ”MMTM: Multimodal\ntransfer module"
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "for CNN fusion.” Proceedings of the IEEE/CVF conference on computer"
        },
        {
          "B. FUTURE WORK": "REFERENCES",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "vision and pattern recognition. 2020."
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "[26] Verbitskiy, Sergey, Vladimir Berikov, and Viacheslav Vyshegorodtsev."
        },
        {
          "B. FUTURE WORK": "[1] Arriaga, Octavio, Matias Valdenegro-Toro, and Paul Pl¨oger. ”Real-time",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "”Eranns: Efficient\nresidual\naudio\nneural\nnetworks\nfor\naudio\npattern"
        },
        {
          "B. FUTURE WORK": "convolutional neural networks\nfor\nemotion and gender\nclassification.”",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "recognition.” Pattern Recognition Letters 161 (2022): 38-44."
        },
        {
          "B. FUTURE WORK": "arXiv preprint arXiv:1710.07557 (2017).",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "[27] Ghaleb, Esam,\nJan Niehues,\nand\nStylianos Asteriadis.\n”Multimodal"
        },
        {
          "B. FUTURE WORK": "[2] Minaee,\nShervin, Mehdi Minaei,\nand Amirali Abdolrashidi.\n”Deep-",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "attention-mechanism for\ntemporal\nemotion\nrecognition.”\n2020\nIEEE"
        },
        {
          "B. FUTURE WORK": "emotion: Facial expression recognition using attentional convolutional",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "International Conference on Image Processing (ICIP).\nIEEE, 2020."
        },
        {
          "B. FUTURE WORK": "network.” Sensors 21.9 (2021): 3046.",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "[28]\nTsai, Yao-Hung Hubert, et al. ”Multimodal\ntransformer\nfor unaligned"
        },
        {
          "B. FUTURE WORK": "[3] Ngwe,\nJia Le, et al. ”PAtt-Lite: Lightweight Patch and Attention Mo-",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "multimodal\nlanguage sequences.” Proceedings of\nthe conference. Asso-"
        },
        {
          "B. FUTURE WORK": "bileNet\nfor Challenging Facial Expression Recognition.” arXiv preprint",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "ciation for computational\nlinguistics. Meeting. Vol. 2019. NIH Public"
        },
        {
          "B. FUTURE WORK": "arXiv:2306.09626 (2023).",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "Access, 2019."
        },
        {
          "B. FUTURE WORK": "[4]\nZhang, Yongqiang,\net\nal.\n”Dynamic gesture\nrecognition model based",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "[29] Chumachenko, Kateryna, Alexandros\nIosifidis,\nand Moncef Gabbouj."
        },
        {
          "B. FUTURE WORK": "on millimeter-wave\nradar with ResNet-18\nand LSTM.” Frontiers\nin",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "”Self-attention fusion for audiovisual emotion recognition with incom-"
        },
        {
          "B. FUTURE WORK": "Neurorobotics 16 (2022): 903197.",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "plete data.” 2022 26th International Conference on Pattern Recognition"
        },
        {
          "B. FUTURE WORK": "[5] Ouyang, Xi, et al. ”A 3D-CNN and LSTM based multi-task learning",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "(ICPR).\nIEEE, 2022."
        },
        {
          "B. FUTURE WORK": "architecture\nfor\naction\nrecognition.”\nIEEE Access\n7\n(2019):\n40757-",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "[30]\nSadok, Samir, Simon Leglaive, and Renaud S´eguier. ”A vector quan-"
        },
        {
          "B. FUTURE WORK": "40770.",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "tized masked autoencoder\nfor speech emotion recognition.” 2023 IEEE"
        },
        {
          "B. FUTURE WORK": "[6] Vaswani, Ashish, et al. ”Attention is all you need.” Advances in neural",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "International\nconference\non\nacoustics,\nspeech,\nand\nsignal\nprocessing"
        },
        {
          "B. FUTURE WORK": "information processing systems 30 (2017).",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "workshops (ICASSPW).\nIEEE, 2023."
        },
        {
          "B. FUTURE WORK": "[7]\nZhao, Zengqun, and Qingshan Liu. ”Former-dfer: Dynamic facial ex-",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "[31]\nTran, Minh,\nand Mohammad Soleymani.\n”A pre-trained audio-visual"
        },
        {
          "B. FUTURE WORK": "pression recognition transformer.” Proceedings of\nthe 29th ACM Inter-",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "transformer\nfor\nemotion\nrecognition.”\nICASSP 2022-2022\nIEEE In-"
        },
        {
          "B. FUTURE WORK": "national Conference on Multimedia. 2021.",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "ternational Conference\non Acoustics,\nSpeech\nand\nSignal\nProcessing"
        },
        {
          "B. FUTURE WORK": "[8] Ma,\nFuyan, Bin\nSun,\nand\nShutao Li.\n”Spatio-temporal\ntransformer",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "(ICASSP).\nIEEE, 2022."
        },
        {
          "B. FUTURE WORK": "for dynamic facial expression recognition in the wild.” arXiv preprint",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "[32]\nZadeh, Amir, et al. ”Tensor\nfusion network for multimodal\nsentiment"
        },
        {
          "B. FUTURE WORK": "arXiv:2205.04749 (2022).",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "analysis.” arXiv preprint arXiv:1707.07250 (2017)."
        },
        {
          "B. FUTURE WORK": "[9] Devlin,\nJacob,\net\nal.\n”Bert: Pre-training\nof\ndeep\nbidirectional\ntrans-",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "[33] Goncalves, Lucas,\nand Carlos Busso.\n”AuxFormer: Robust\napproach"
        },
        {
          "B. FUTURE WORK": "formers for\nlanguage understanding.” arXiv preprint arXiv:1810.04805",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "to\naudiovisual\nemotion\nrecognition.”\nICASSP\n2022-2022\nIEEE\nIn-"
        },
        {
          "B. FUTURE WORK": "(2018).",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "ternational Conference\non Acoustics,\nSpeech\nand\nSignal\nProcessing"
        },
        {
          "B. FUTURE WORK": "[10] He, Kaiming, et al. ”Masked autoencoders are scalable vision learners.”",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "(ICASSP).\nIEEE, 2022."
        },
        {
          "B. FUTURE WORK": "Proceedings\nof\nthe\nIEEE/CVF\nconference\non\ncomputer\nvision\nand",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "[34] Goncalves, Lucas,\nand Carlos Busso.\n”Robust\naudiovisual\nemotion"
        },
        {
          "B. FUTURE WORK": "pattern recognition. 2022.",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "recognition: Aligning modalities, capturing temporal\ninformation, and"
        },
        {
          "B. FUTURE WORK": "[11]\nTong, Zhan, et al. ”Videomae: Masked autoencoders are data-efficient",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "handling missing features.” IEEE Transactions on Affective Computing"
        },
        {
          "B. FUTURE WORK": "learners\nfor\nself-supervised\nvideo\npre-training.” Advances\nin\nneural",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "13.4 (2022): 2156-2170."
        },
        {
          "B. FUTURE WORK": "information processing systems 35 (2022): 10078-10093.",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "[35]\nShi, Bowen,\net\nal.\n”Learning\naudio-visual\nspeech\nrepresentation\nby"
        },
        {
          "B. FUTURE WORK": "[12] Huang, Po-Yao, et al. ”Masked autoencoders\nthat\nlisten.” Advances\nin",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "masked multimodal cluster prediction.” arXiv preprint arXiv:2201.02184"
        },
        {
          "B. FUTURE WORK": "Neural\nInformation Processing Systems 35 (2022): 28708-28720.",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "(2022)."
        },
        {
          "B. FUTURE WORK": "[13] Hsu, Wei-Ning,\net\nal.\n”Hubert: Self-supervised speech representation",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "[36] Huang, Po-Yao, et al. ”Mavil: Masked audio-video learners.” Advances"
        },
        {
          "B. FUTURE WORK": "learning by masked prediction of hidden units.” IEEE/ACM Transactions",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "in Neural\nInformation Processing Systems 36 (2024)."
        },
        {
          "B. FUTURE WORK": "on Audio, Speech, and Language Processing 29 (2021): 3451-3460.",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "[37]\nLee, Sangho,\net\nal.\n”Parameter\nefficient multimodal\ntransformers\nfor"
        },
        {
          "B. FUTURE WORK": "[14]\nSun, Licai,\net\nal.\n”Mae-dfer: Efficient masked\nautoencoder\nfor\nself-",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "video representation learning.” arXiv preprint arXiv:2012.04124 (2020)."
        },
        {
          "B. FUTURE WORK": "supervised dynamic facial expression recognition.” Proceedings of\nthe",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "Database\nof Emotional Speech\nand Song\n(RAVDESS): A dynamic,"
        },
        {
          "B. FUTURE WORK": "In future work,\nthe\nframework can be\nfurther developed",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "multimodal\nset\nof\nfacial\nand\nvocal\nexpressions\nin North American"
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "English.” PloS one 13.5 (2018): e0196391."
        },
        {
          "B. FUTURE WORK": "from two\naspects.\nIn\nterms\nof\ndepth,\nthe\nself-supervised",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "[20] Cao, Houwei,\net\nal.\n”Crema-d: Crowd-sourced emotional multimodal"
        },
        {
          "B. FUTURE WORK": "learning\napproach\nof\nthe multimodal masked\nautoencoder",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "actors dataset.”\nIEEE transactions on affective\ncomputing 5.4 (2014):"
        },
        {
          "B. FUTURE WORK": "involves\nreconstructing the input of multimodal visual-audio",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "377-390."
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "[21] Busso, Carlos, et al. ”IEMOCAP:\nInteractive emotional dyadic motion"
        },
        {
          "B. FUTURE WORK": "data to enhance the pre-training encoder. This enhanced model",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "capture database.” Language resources and evaluation 42 (2008): 335-"
        },
        {
          "B. FUTURE WORK": "can subsequently function as a multimodal pre-trained model,",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "359."
        },
        {
          "B. FUTURE WORK": "substituting the video autoencoder pre-trained model utilized",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "[22]\nTseng, Yuan, et al. ”Av-superb: A multi-task evaluation benchmark for"
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "audio-visual\nrepresentation models.”\narXiv preprint\narXiv:2309.10787"
        },
        {
          "B. FUTURE WORK": "in this\nstudy.\nIn terms of breadth,\nthis work has covered the",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "(2023)."
        },
        {
          "B. FUTURE WORK": "aspects\nof\n“see”\n(video)\nand\n“listen”\n(audio)\nin\nthe model",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "[23]\nLoshchilov,\nIlya, and Frank Hutter. ”Decoupled weight decay regular-"
        },
        {
          "B. FUTURE WORK": "feature\nextraction process\nfor\ninput data, which is\nessential",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "ization.” arXiv preprint arXiv:1711.05101 (2017)."
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "[24]\nFukui,\nAkira,\net\nal.\n”Multimodal\ncompact\nbilinear\npooling\nfor"
        },
        {
          "B. FUTURE WORK": "for further incorporating “read” (text) into the model inference",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "visual\nquestion\nanswering\nand\nvisual\ngrounding.”\narXiv\npreprint"
        },
        {
          "B. FUTURE WORK": "process.",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "arXiv:1606.01847 (2016)."
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "[25]\nJoze, Hamid Reza Vaezi, et al. ”MMTM: Multimodal\ntransfer module"
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "for CNN fusion.” Proceedings of the IEEE/CVF conference on computer"
        },
        {
          "B. FUTURE WORK": "REFERENCES",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "vision and pattern recognition. 2020."
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "[26] Verbitskiy, Sergey, Vladimir Berikov, and Viacheslav Vyshegorodtsev."
        },
        {
          "B. FUTURE WORK": "[1] Arriaga, Octavio, Matias Valdenegro-Toro, and Paul Pl¨oger. ”Real-time",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "”Eranns: Efficient\nresidual\naudio\nneural\nnetworks\nfor\naudio\npattern"
        },
        {
          "B. FUTURE WORK": "convolutional neural networks\nfor\nemotion and gender\nclassification.”",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "recognition.” Pattern Recognition Letters 161 (2022): 38-44."
        },
        {
          "B. FUTURE WORK": "arXiv preprint arXiv:1710.07557 (2017).",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "[27] Ghaleb, Esam,\nJan Niehues,\nand\nStylianos Asteriadis.\n”Multimodal"
        },
        {
          "B. FUTURE WORK": "[2] Minaee,\nShervin, Mehdi Minaei,\nand Amirali Abdolrashidi.\n”Deep-",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "attention-mechanism for\ntemporal\nemotion\nrecognition.”\n2020\nIEEE"
        },
        {
          "B. FUTURE WORK": "emotion: Facial expression recognition using attentional convolutional",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "International Conference on Image Processing (ICIP).\nIEEE, 2020."
        },
        {
          "B. FUTURE WORK": "network.” Sensors 21.9 (2021): 3046.",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "[28]\nTsai, Yao-Hung Hubert, et al. ”Multimodal\ntransformer\nfor unaligned"
        },
        {
          "B. FUTURE WORK": "[3] Ngwe,\nJia Le, et al. ”PAtt-Lite: Lightweight Patch and Attention Mo-",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "multimodal\nlanguage sequences.” Proceedings of\nthe conference. Asso-"
        },
        {
          "B. FUTURE WORK": "bileNet\nfor Challenging Facial Expression Recognition.” arXiv preprint",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "ciation for computational\nlinguistics. Meeting. Vol. 2019. NIH Public"
        },
        {
          "B. FUTURE WORK": "arXiv:2306.09626 (2023).",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "Access, 2019."
        },
        {
          "B. FUTURE WORK": "[4]\nZhang, Yongqiang,\net\nal.\n”Dynamic gesture\nrecognition model based",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "[29] Chumachenko, Kateryna, Alexandros\nIosifidis,\nand Moncef Gabbouj."
        },
        {
          "B. FUTURE WORK": "on millimeter-wave\nradar with ResNet-18\nand LSTM.” Frontiers\nin",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "”Self-attention fusion for audiovisual emotion recognition with incom-"
        },
        {
          "B. FUTURE WORK": "Neurorobotics 16 (2022): 903197.",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "plete data.” 2022 26th International Conference on Pattern Recognition"
        },
        {
          "B. FUTURE WORK": "[5] Ouyang, Xi, et al. ”A 3D-CNN and LSTM based multi-task learning",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "(ICPR).\nIEEE, 2022."
        },
        {
          "B. FUTURE WORK": "architecture\nfor\naction\nrecognition.”\nIEEE Access\n7\n(2019):\n40757-",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "[30]\nSadok, Samir, Simon Leglaive, and Renaud S´eguier. ”A vector quan-"
        },
        {
          "B. FUTURE WORK": "40770.",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "tized masked autoencoder\nfor speech emotion recognition.” 2023 IEEE"
        },
        {
          "B. FUTURE WORK": "[6] Vaswani, Ashish, et al. ”Attention is all you need.” Advances in neural",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "International\nconference\non\nacoustics,\nspeech,\nand\nsignal\nprocessing"
        },
        {
          "B. FUTURE WORK": "information processing systems 30 (2017).",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "workshops (ICASSPW).\nIEEE, 2023."
        },
        {
          "B. FUTURE WORK": "[7]\nZhao, Zengqun, and Qingshan Liu. ”Former-dfer: Dynamic facial ex-",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "[31]\nTran, Minh,\nand Mohammad Soleymani.\n”A pre-trained audio-visual"
        },
        {
          "B. FUTURE WORK": "pression recognition transformer.” Proceedings of\nthe 29th ACM Inter-",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "transformer\nfor\nemotion\nrecognition.”\nICASSP 2022-2022\nIEEE In-"
        },
        {
          "B. FUTURE WORK": "national Conference on Multimedia. 2021.",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "ternational Conference\non Acoustics,\nSpeech\nand\nSignal\nProcessing"
        },
        {
          "B. FUTURE WORK": "[8] Ma,\nFuyan, Bin\nSun,\nand\nShutao Li.\n”Spatio-temporal\ntransformer",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "(ICASSP).\nIEEE, 2022."
        },
        {
          "B. FUTURE WORK": "for dynamic facial expression recognition in the wild.” arXiv preprint",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "[32]\nZadeh, Amir, et al. ”Tensor\nfusion network for multimodal\nsentiment"
        },
        {
          "B. FUTURE WORK": "arXiv:2205.04749 (2022).",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "analysis.” arXiv preprint arXiv:1707.07250 (2017)."
        },
        {
          "B. FUTURE WORK": "[9] Devlin,\nJacob,\net\nal.\n”Bert: Pre-training\nof\ndeep\nbidirectional\ntrans-",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "[33] Goncalves, Lucas,\nand Carlos Busso.\n”AuxFormer: Robust\napproach"
        },
        {
          "B. FUTURE WORK": "formers for\nlanguage understanding.” arXiv preprint arXiv:1810.04805",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "to\naudiovisual\nemotion\nrecognition.”\nICASSP\n2022-2022\nIEEE\nIn-"
        },
        {
          "B. FUTURE WORK": "(2018).",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "ternational Conference\non Acoustics,\nSpeech\nand\nSignal\nProcessing"
        },
        {
          "B. FUTURE WORK": "[10] He, Kaiming, et al. ”Masked autoencoders are scalable vision learners.”",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "(ICASSP).\nIEEE, 2022."
        },
        {
          "B. FUTURE WORK": "Proceedings\nof\nthe\nIEEE/CVF\nconference\non\ncomputer\nvision\nand",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "[34] Goncalves, Lucas,\nand Carlos Busso.\n”Robust\naudiovisual\nemotion"
        },
        {
          "B. FUTURE WORK": "pattern recognition. 2022.",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "recognition: Aligning modalities, capturing temporal\ninformation, and"
        },
        {
          "B. FUTURE WORK": "[11]\nTong, Zhan, et al. ”Videomae: Masked autoencoders are data-efficient",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "handling missing features.” IEEE Transactions on Affective Computing"
        },
        {
          "B. FUTURE WORK": "learners\nfor\nself-supervised\nvideo\npre-training.” Advances\nin\nneural",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "13.4 (2022): 2156-2170."
        },
        {
          "B. FUTURE WORK": "information processing systems 35 (2022): 10078-10093.",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "[35]\nShi, Bowen,\net\nal.\n”Learning\naudio-visual\nspeech\nrepresentation\nby"
        },
        {
          "B. FUTURE WORK": "[12] Huang, Po-Yao, et al. ”Masked autoencoders\nthat\nlisten.” Advances\nin",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "masked multimodal cluster prediction.” arXiv preprint arXiv:2201.02184"
        },
        {
          "B. FUTURE WORK": "Neural\nInformation Processing Systems 35 (2022): 28708-28720.",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "(2022)."
        },
        {
          "B. FUTURE WORK": "[13] Hsu, Wei-Ning,\net\nal.\n”Hubert: Self-supervised speech representation",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "[36] Huang, Po-Yao, et al. ”Mavil: Masked audio-video learners.” Advances"
        },
        {
          "B. FUTURE WORK": "learning by masked prediction of hidden units.” IEEE/ACM Transactions",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "in Neural\nInformation Processing Systems 36 (2024)."
        },
        {
          "B. FUTURE WORK": "on Audio, Speech, and Language Processing 29 (2021): 3451-3460.",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "[37]\nLee, Sangho,\net\nal.\n”Parameter\nefficient multimodal\ntransformers\nfor"
        },
        {
          "B. FUTURE WORK": "[14]\nSun, Licai,\net\nal.\n”Mae-dfer: Efficient masked\nautoencoder\nfor\nself-",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": "video representation learning.” arXiv preprint arXiv:2012.04124 (2020)."
        },
        {
          "B. FUTURE WORK": "supervised dynamic facial expression recognition.” Proceedings of\nthe",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "31st ACM International Conference on Multimedia. 2023.",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "[15] Ghaleb, Esam, Mirela Popa, and Stylianos Asteriadis. ”Multimodal and",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "temporal perception of audio-visual cues for emotion recognition.” 2019",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "8th International Conference on Affective Computing and Intelligent",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "Interaction (ACII).\nIEEE, 2019.",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "[16]\nSu, Lang, et al. ”Msaf: Multimodal split attention fusion.” arXiv preprint",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "arXiv:2012.07175 (2020).",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "[17]\nFu, Ziwang,\net\nal.\n”A cross-modal\nfusion\nnetwork\nbased\non\nself-",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "attention and residual\nstructure\nfor multimodal\nemotion recognition.”",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "arXiv preprint arXiv:2111.02172 (2021).",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "[18] Dosovitskiy, Alexey, et al. ”An image is worth 16x16 words: Transform-",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "ers\nfor\nimage\nrecognition at\nscale.”\narXiv preprint\narXiv:2010.11929",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        },
        {
          "B. FUTURE WORK": "(2020).",
          "[19]\nLivingstone, Steven R., and Frank A. Russo. ”The Ryerson Audio-Visual": ""
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Real-time convolutional neural networks for emotion and gender classification",
      "authors": [
        "Octavio Arriaga",
        "Matias Valdenegro-Toro",
        "Paul Plöger"
      ],
      "year": "2017",
      "venue": "Real-time convolutional neural networks for emotion and gender classification",
      "arxiv": "arXiv:1710.07557"
    },
    {
      "citation_id": "2",
      "title": "Deepemotion: Facial expression recognition using attentional convolutional network",
      "authors": [
        "Shervin Minaee",
        "Mehdi Minaei",
        "Amirali Abdolrashidi"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "3",
      "title": "PAtt-Lite: Lightweight Patch and Attention Mo-bileNet for Challenging Facial Expression Recognition",
      "authors": [
        "Jia Ngwe",
        "Le"
      ],
      "year": "2023",
      "venue": "PAtt-Lite: Lightweight Patch and Attention Mo-bileNet for Challenging Facial Expression Recognition",
      "arxiv": "arXiv:2306.09626"
    },
    {
      "citation_id": "4",
      "title": "Dynamic gesture recognition model based on millimeter-wave radar with ResNet-18 and LSTM",
      "authors": [
        "Yongqiang Zhang"
      ],
      "year": "2022",
      "venue": "Frontiers in Neurorobotics"
    },
    {
      "citation_id": "5",
      "title": "A 3D-CNN and LSTM based multi-task learning architecture for action recognition",
      "authors": [
        "Xi Ouyang"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "6",
      "title": "Advances in neural information processing systems",
      "authors": [
        "Ashish Vaswani"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "7",
      "title": "Former-dfer: Dynamic facial expression recognition transformer",
      "authors": [
        "Zengqun Zhao",
        "Qingshan Liu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "8",
      "title": "Spatio-temporal transformer for dynamic facial expression recognition in the wild",
      "authors": [
        "Fuyan Ma",
        "Bin Sun",
        "Shutao Li"
      ],
      "year": "2022",
      "venue": "Spatio-temporal transformer for dynamic facial expression recognition in the wild",
      "arxiv": "arXiv:2205.04749"
    },
    {
      "citation_id": "9",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "10",
      "title": "Masked autoencoders are scalable vision learners",
      "authors": [
        "Kaiming He"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "11",
      "title": "Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training",
      "authors": [
        "Zhan Tong"
      ],
      "year": "2022",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "12",
      "title": "Masked autoencoders that listen",
      "authors": [
        "Po- Huang",
        "Yao"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "13",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei Hsu",
        "Ning"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "14",
      "title": "Mae-dfer: Efficient masked autoencoder for selfsupervised dynamic facial expression recognition",
      "authors": [
        "Licai Sun"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "15",
      "title": "Multimodal and temporal perception of audio-visual cues for emotion recognition",
      "authors": [
        "Esam Ghaleb",
        "Mirela Popa",
        "Stylianos Asteriadis"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "16",
      "title": "Msaf: Multimodal split attention fusion",
      "authors": [
        "Lang Su"
      ],
      "year": "2020",
      "venue": "Msaf: Multimodal split attention fusion",
      "arxiv": "arXiv:2012.07175"
    },
    {
      "citation_id": "17",
      "title": "A cross-modal fusion network based on selfattention and residual structure for multimodal emotion recognition",
      "authors": [
        "Ziwang Fu"
      ],
      "year": "2021",
      "venue": "A cross-modal fusion network based on selfattention and residual structure for multimodal emotion recognition",
      "arxiv": "arXiv:2111.02172"
    },
    {
      "citation_id": "18",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "Alexey Dosovitskiy"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "19",
      "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
      "authors": [
        "Steven Livingstone",
        "Frank Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "20",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "Cao",
        "Houwei"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "21",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "22",
      "title": "Av-superb: A multi-task evaluation benchmark for audio-visual representation models",
      "authors": [
        "Yuan Tseng"
      ],
      "year": "2023",
      "venue": "Av-superb: A multi-task evaluation benchmark for audio-visual representation models",
      "arxiv": "arXiv:2309.10787"
    },
    {
      "citation_id": "23",
      "title": "Decoupled weight decay regularization",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2017",
      "venue": "Decoupled weight decay regularization",
      "arxiv": "arXiv:1711.05101"
    },
    {
      "citation_id": "24",
      "title": "Multimodal compact bilinear pooling for visual question answering and visual grounding",
      "authors": [
        "Akira Fukui"
      ],
      "year": "2016",
      "venue": "Multimodal compact bilinear pooling for visual question answering and visual grounding",
      "arxiv": "arXiv:1606.01847"
    },
    {
      "citation_id": "25",
      "title": "MMTM: Multimodal transfer module for CNN fusion",
      "authors": [
        "Hamid Joze",
        "Vaezi"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "26",
      "title": "Eranns: Efficient residual audio neural networks for audio pattern recognition",
      "authors": [
        "Sergey Verbitskiy",
        "Vladimir Berikov",
        "Viacheslav Vyshegorodtsev"
      ],
      "year": "2022",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "27",
      "title": "Multimodal attention-mechanism for temporal emotion recognition",
      "authors": [
        "Esam Ghaleb",
        "Jan Niehues",
        "Stylianos Asteriadis"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "28",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Yao-Hung Tsai",
        "Hubert"
      ],
      "year": "2019",
      "venue": "Proceedings of the conference. Association for computational linguistics. Meeting"
    },
    {
      "citation_id": "29",
      "title": "Self-attention fusion for audiovisual emotion recognition with incomplete data",
      "authors": [
        "Kateryna Chumachenko",
        "Alexandros Iosifidis",
        "Moncef Gabbouj"
      ],
      "year": "2022",
      "venue": "2022 26th International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "30",
      "title": "A vector quantized masked autoencoder for speech emotion recognition",
      "authors": [
        "Samir Sadok",
        "Simon Leglaive",
        "Renaud Séguier"
      ],
      "year": "2023",
      "venue": "2023 IEEE International conference on acoustics, speech, and signal processing workshops (ICASSPW)"
    },
    {
      "citation_id": "31",
      "title": "A pre-trained audio-visual transformer for emotion recognition",
      "authors": [
        "Minh Tran",
        "Mohammad Soleymani"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "32",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "Amir Zadeh"
      ],
      "year": "2017",
      "venue": "Tensor fusion network for multimodal sentiment analysis",
      "arxiv": "arXiv:1707.07250"
    },
    {
      "citation_id": "33",
      "title": "AuxFormer: Robust approach to audiovisual emotion recognition",
      "authors": [
        "Lucas Goncalves",
        "Carlos Busso"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "34",
      "title": "Robust audiovisual emotion recognition: Aligning modalities, capturing temporal information, and handling missing features",
      "authors": [
        "Lucas Goncalves",
        "Carlos Busso"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "35",
      "title": "Learning audio-visual speech representation by masked multimodal cluster prediction",
      "authors": [
        "Shi",
        "Bowen"
      ],
      "year": "2022",
      "venue": "Learning audio-visual speech representation by masked multimodal cluster prediction",
      "arxiv": "arXiv:2201.02184"
    },
    {
      "citation_id": "36",
      "title": "Mavil: Masked audio-video learners",
      "authors": [
        "Po- Huang",
        "Yao"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "37",
      "title": "Parameter efficient multimodal transformers for video representation learning",
      "authors": [
        "Sangho Lee"
      ],
      "year": "2020",
      "venue": "Parameter efficient multimodal transformers for video representation learning",
      "arxiv": "arXiv:2012.04124"
    }
  ]
}