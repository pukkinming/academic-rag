{
  "paper_id": "2111.05222v2",
  "title": "Cross Attentional Audio-Visual Fusion For Dimensional Emotion Recognition",
  "published": "2021-11-09T16:01:56Z",
  "authors": [
    "R. Gnana Praveen",
    "Eric Granger",
    "Patrick Cardinal"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal analysis has recently drawn much interest in affective computing, since it can improve the overall accuracy of emotion recognition over isolated uni-modal approaches. The most effective techniques for multimodal emotion recognition efficiently leverage diverse and complimentary sources of information, such as facial, vocal, and physiological modalities, to provide comprehensive feature representations. In this paper, we focus on dimensional emotion recognition based on the fusion of facial and vocal modalities extracted from videos, where complex spatiotemporal relationships may be captured. Most of the existing fusion techniques rely on recurrent networks or conventional attention mechanisms that do not effectively leverage the complimentary nature of audiovisual (A-V) modalities. We introduce a cross-attentional fusion approach to extract the salient features across A-V modalities, allowing for accurate prediction of continuous values of valence and arousal. Our new cross-attentional A-V fusion model efficiently leverages the inter-modal relationships. In particular, it computes cross-attention weights to focus on the more contributive features across individual modalities, and thereby combine contributive feature representations, which are then fed to fully connected layers for the prediction of valence and arousal. The effectiveness of the proposed approach is validated experimentally on videos from the RECOLA and Fatigue (private) data-sets. Results indicate that our cross-attentional A-V fusion model is a cost-effective approach that outperforms state-of-the-art fusion approaches. Code is available: https: //github.com/praveena2j/Cross-Attentional-AV-Fusion",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Automatic recognition of emotions is an important task that facilitates natural interaction between human and machine. Emotion recognition is found in many applications, such as assessment of anger, fatigue, depression, pain, motivation, and stress in health care, e-learning, security, etc. Recognizing emotions is a challenging problem in real world scenarios as expressions linked to an emotional state are often diverse in nature across individuals and cultures  [1] . Human emotions can be conveyed through various modalities such as face, voice, text and physiology (electroencephalogram, electrocardiogram, etc.), which typically carry complementary information among them. Although human emotions can be expressed through various modalities, vocal and facial modalities are the predominant contact-free channels through which they can be efficiently expressed. Therefore, audiovisual (A-V) fusion for emotion recognition has been widely explored for several decades  [2] . Depending on the type of labels, emotion recognition can be formulated as a discrete classification problem (e.g., a person eliciting happy or sad emotions), or as a continuous regression problem (e.g. con-tinuous values of valence and arousal). Though classification conveys the type of emotion being expressed, it fails to capture the wide range of emotions on a finer granularity. In this paper, we focus on dimensional A-V emotion recognition based on videos in the context of regression. Valence and arousal are widely used for estimating emotion intensities in continuous domain, where valence spans the wide range of emotions from sad to happy, and arousal reflects the energy or intensity of the emotions.\n\nTypically, A-V fusion for emotion recognition can be achieved by three major strategies: decision-, feature-, and model-level fusion  [2] . In decision-level fusion (late fusion), multiple modalities are trained end-to-end independently, and then the predictions obtained from the individual modalities are fused to obtain the final predictions. Although decision-level fusion is easy to implement, and requires less training, it neglects the interactions across the individual modalities, thereby resulting in limited improvement over uni-modal approaches. Conventionally, feature-level fusion (early fusion) is achieved by concatenating the features of A-V modalities immediately after they are extracted, which is further used for predicting the final outputs. Though featurelevel fusion allows interaction between the modalities at the low level features, it fails to leverage the interactions between the A-V features (inter-modal relationships) across the individual modalities, thereby resulting in limited improvement in performance  [2] . Model-level fusion is the most effective way to leverage the complimentary nature of the modalities to obtain comprehensive feature representation. The interactions between A-V features such as intra-or intermodal relationships are explicitly captured, usually based on models, e.g., deep networks  [3] , hidden Markov models  [4] , and kernel methods  [5] . Inspired by the performance of model-based fusion and deep networks, we focus on deep multimodal representation learning for A-V dimensional emotion recognition to efficiently leverage the correlation across A and V modalities.\n\nDeep learning (DL) models provide state-of-the-art performance in many V recognition applications, such as object detection, action recognition, etc. Inspired by their performance, several approaches have been proposed for videobased dimensional emotion recognition using CNNs to obtain the deep features, and a recurrent neural network to capture the temporal dynamics  [6] ,  [7] . In most of these approaches  [7] ,  [8] , A-V fusion is performed by concatenating the deep features extracted from individual facial and vocal modalities, and fed to LSTM for predicting valence and arousal. Although LSTM based fusion models the intramodal relationship (temporal) and improves the performance of the system, it does not effectively capture the inter-modal relationships across the individual modalities. We therefore investigate the prospect of extracting more contributive features across A and V modalities in order to leverage their complimentary temporal relationships.\n\nAttention mechanisms have recently gained much interest in the areas of computer vision and machine learning as they allow extracting task relevant features, thereby improving system performance. This has been extensively explored for various applications, such as event/action recognition  [9] , emotion recognition  [10] , etc. Most of the existing attention based approaches for dimensional emotion recognition explore the intra-modal relationships  [10] . Although a few approaches  [11] ,  [8]  attempt to capture the cross-modal relationships using cross-attention based on transformers, they fail to effectively leverage the complimentary relationship of A-V modalities. Indeed, their computation of attention weights does not consider the correlation among the A and V features. In this paper, we introduce a cross-attentional A-V fusion model for predicting valence and arousal by exploiting the interactions between A-V features. By estimating the cross-correlation across deep A and V features, the proposed model employs the cross-correlation matrix to capture the relevant complimentary information for accurate dimensional emotion recognition. The cross-correlation based attention helps to capture the semantic relevance between A and V features, thereby effectively leveraging the complimentary A-V relationships. Though cross-attention based on crosscorrelation have been explored for other applications such as few shot classification  [12]  and action classification  [13] , we investigate it in the context of regression for dimensional emotion recognition. Main contributions:  (1)  We propose a cross-attentional A-V fusion model based on cross-correlation to effectively exploit the complimentary relationship across modalities for dimensional emotion recognition. (2) Unlike prior approaches, we leverage the interactions between A-V features (inter-modal relationships) to obtain complimentary representations for dimensional emotion recognition. (3) For proof-of-concept, we consider Inflated 3D CNN model  [14]  to efficiently extract the spatiotemporal features for the facial modality, coupled with a 2D-CNN model to extract A features from a spectrogram representation for the vocal modality. Experimental results on RECOLA and Fatigue (private) data-sets shows that our proposed cross-attentional A-V fusion can outperform state-of-the-art fusion models for dimensional emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "A) A-V Fusion for Dimensional Emotion Recognition: One of the early approaches using DL models for A-V fusion based dimensional emotion recognition was proposed by Tzirakis et al.  [7] , where A and V features are obtained using ResNet50 and 1D CNN respectively. The obtained features are then concatenated and fed to Long short-term memory model (LSTM) for the prediction of valence and arousal. Juan et al.  [15]  investigated an empirical study of fine-tuning pretrained CNN models by freezing various convolutional layers. Schonevald et al.  [6]  explored knowledge distillation using student-teacher model for V modality and CNN model for A modality using spectrograms. The deep feature representations are combined using a model-based fusion strategy, where RNNs are used to capture the temporal dynamics. Inspired by the deep auto-encoders, Nguyen et al.  [16]  investigated the prospect of how to simultaneously learn compact representative features from A and V modalities using deep auto-encoders. They have proposed a deep model of twostream auto-encoders and LSTM for efficiently integrating V and A streams for dimensional emotion recognition. Though the above mentioned approaches have shown significant improvement for dimensional emotion recognition, they fail to capture the inter-modal relationships and relevant salient features specific to the task. Therefore, we have focused on capturing the comprehensive features in a complimentary fashion using attention mechanisms. B) Attention for A-V Fusion: Attention mechanisms are widely used in the context of multimodal fusion with various modalities such as A and text  [17] ,  [18] , V and text  [19] ,  [20] , etc. In this work, we focus on attention mechanisms for model-based A-V fusion in videos. Zhao et al.  [21]  proposed an end-to-end architecture for emotion classification by integrating spatial, channel-wise and temporal attentions into V network and temporal attention into A network. Esam et al.  [22]  explored attention to weigh the time windows of a video sequence to efficiently exploit the temporal interactions between the A-V modalities. They used transformer  [23]  based encoders to obtain the attention weights through self attention for emotion classification. Lee et al.  [10]  proposed spatiotemporal attention for the V modality to focus on emotional salient parts using Convolutional LSTM (Con-vLSTM) modules and a temporal attention network using deep networks for A modality. Then the attended features are concatenated and fed to the regression network for the prediction of valence and arousal. However, these approaches focus on modeling the intra-modal relationships and fail to exploit the complimentary nature of the A-V modalities.\n\nWang et al.  [24]  investigated the prospect of exploiting the implicit contextual information along with the A and V modalities. They have proposed an end-to-end architecture using cross-attention based on transformers for A-V group emotion recognition. Srinivas et al.  [11]  also explored transformers with cross modal attention for dimensional emotion recognition, where cross-attention is integrated along with self attention. Tzirakis et al.  [8]  investigated various fusion strategies along with attention mechanisms for A-V fusion based dimensional emotion recognition. They have further explored self attention as well as cross-attention fusion based on transformers in order to enable the extracted features of different modalities to attend to each other. Although these approaches have explored cross-attention with transformers, they fail to leverage the cross-correlation based semantic relevance among the A-V features.\n\nWang et al.  [25]  addressed the problem of multimodal feature fusion along with frame alignment issues between A and V modalities using cross-attention for speech recognition. Unlike prior approaches based on transformers, we use a simple yet efficient attention mechanism based on crosscorrelation across A and V modalities. The cross-correlation across A and V features helps to effectively retain the semantic relevance and capture the complimentary relationships among the A-V modalities. Jun et al.  [13]  is the most similar to our approach, where multi-stage cross-attentional A-V fusion is used to collaboratively fuse A and V features for localizing and classifying actions in videos. The proposed approach primarily differs from  [13]  in two respects: (1) cross-attentional fusion is explored in an iterative mechanism in  [13] , whereas we apply cross-attention in a single iteration, and still improve system performance. (2) In  [13] , crossattentional fusion is used for action recognition in the context of classification, whereas we adapt it to dimensional emotion recognition in the context of regression.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Proposed Approach",
      "text": "In this section, we introduce the cross-attentional A-V fusion model that extracts complimentary features across facial and vocal modalities, thereby providing a comprehensive representation to improve the overall performance.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Visual Network",
      "text": "Facial expressions from videos involve both appearance and temporal dynamics of the video sequences. Efficient modeling of the spatial and temporal dynamics of the video sequences plays a crucial role in extracting robust features, which in-turn improves the overall system performance. State-of-the-art performance is typically achieved using CNN in combination with Recurrent Neural Networks (RNN) to capture the effective latent appearance representation along with temporal dynamics  [26] . Several approaches have been explored for dimensional emotion recognition based on LSTMs  [27] ,  [28] . However, 3D-CNNs are found to be efficient in capturing the spatiotemporal dynamics in videos. Specifically, we consider Inflated 3D-CNN  [14] , to extract spatiotemporal features of the facial clips from a video sequence. Compared to conventional 3D CNNs, I3D can efficiently captures the spatiotemporal dynamics of the V modality while training with less parameters than that of 3D CNNs. Also, it helps to explore the existing pretrained 2D-CNNs, which are trained on many images with facial expressions, thereby improving the spatial discrimination for videos. In the proposed approach, we have individually trained the I3D model for the facial modality (see implementation details in Section IV-B).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Audio Network",
      "text": "The para-lingual information of the speech signal was found to have significant information conveying the emotional state of a person. Even though emotion recognition using voice has been widely explored using the conventional handcrafted features such as MFCC, global features  [29] , there has been a significant improvement over the recent years with the introduction of DL models. Spectrograms are found to carry significant para-lingual information pertaining to the affective state of a person  [30] ,  [31] . Therefore, spectrograms are used in the framework of DL models for speech based emotion recognition. Spectrograms have been explored with various 2D CNNs in the literature for emotion recognition  [32] ,  [33] . We use the A network as shown in Table  I  (see implementation details in Section IV-B).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Cross-Attentional Fusion",
      "text": "The A and V models have been trained separately and the deep features are extracted for the A and V modalities. The performance of valence and arousal varies significantly for A and V modalities. Due to the rich appearance based information in the V modality, it conveys significant information pertinent to the valence as it depicts the expressions of the sequence. Audio signals carry significant information relevant to the intensity of the expressions, which is efficiently manifested in the energy of A signals. For a given video sequence, the V modality carries relevant information in some video clips, whereas A modality might be more relevant for other clips. Since, multiple modalities convey diverse information for valence and arousal than a single modality, multiple modalities can be effectively leveraged by fusing the A and V modalities in a complimentary fashion. In order to reliably fuse these modalities for the prediction of valence and arousal, we use cross-attention based fusion mechanism to efficiently encode the inter-modal information while preserving the intra-modal characteristics. A block diagram of the proposed model is shown in Figure  1 .\n\nLet X a and X v represents the deep features of A and V modalities of a given video sequence X, where X a = (x l a )\n\nL l=1 and\n\nL denotes the number of subsequences of X, x l a and x l v denotes the A and V feature vectors respectively of l th subsequence of the video sequence X. Next, the cross-correlation of A and V features for the subsequences is computed from the given video sequence X to capture the relevance across the modalities. In order to minimize the heterogeneity between the modalities, a learnable weight matrix W ∈ R K×K is learned, and the cross-correlation is computed as\n\nwhere Z ∈ R L×L , W represents cross-correlation weights among the A and V features and K denotes the feature dimension of the A and V features.\n\nThe cross-correlation matrix Z gives the measure of correlation among the A and V features. Higher correlation coefficient in matrix Z shows that the corresponding A and V features of the sub-sequence are strongly related to each other. Therefore, l th column of the cross-correlation matrix Z shows the correlation measure of l th V feature with L A features. Based on this idea, we compute the cross-attention weights of A and V features, A a and A v by applying column-wise softmax of Z and Z T , respectively:\n\nwhere i and j represents the i th row and j th column of the cross-correlation matrix Z, and T the softmax temperature.\n\nSince the weights W are being learned based on the crosscorrelation of the A and V features, the attention weights of each modality is guided by the other modality, thereby efficiently leveraging complimentary nature of the A and V modalities. After obtaining the cross-attention weights, they are used to obtain the attention maps of the A and V features to make it more comprehensive and discriminative: X\n\nwhere A a and A v denotes the cross-attention weights of A and V features, respectively. The re-weighted attention maps are added to the corresponding features to obtain the attended features:\n\nThe attended V and A features, X att,v and X att,a are concatenated to obtain the A-V feature representation, which is given by X = [X att,v ; X att,a ]. Finally, the A-V features are fed to the fully connected layers for the prediction of valence and arousal.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iv. Experimental Methodology",
      "text": "A) Dataset: The proposed architecture is validated with the REmote COLlaborative and Affective (RECOLA) dataset  [34] . In total, the data-set consists of 9.5 hours of multimodal recordings, which is recorded by 46 French -speaking participants, performing a collaborative task during a video conference. Among the participants, 17 are French, 3 are German and 3 are Italian. The video sequences are divided into sequences of 5 minutes each, which is annotated with a regressed intensity value for every 40 msec by 6 French speaking annotators (three male and three female). The dataset is split into three partitions: train (16 subjects), validation (15 subjects) and test (15 subjects) by balancing the age and gender of the speakers. Due to the uncontrolled spontaneous nature of expressions of the subjects, the dataset has been widely used by the research community in affective computing for various challenges such as AVEC 2015  [35] , AVEC 2016  [36] , etc. Most of the existing approaches  [6] ,  [7]  in the literature have validated on the dataset used for AVEC 2016  [36]  challenge, which consists of 9 subjects for training and 9 subjects for validation. Therefore, we have also validated the proposed approach on the data-set used in AVEC 2016 challenge.\n\nB) Implementation Details: For the V modality, the faces are extracted and pre-processed from the video sequences of the dataset using MTCNN model  [37] , which is deep cascaded multi-task framework of face detection and alignment. Faces are resized to 224x224 to be fed to the I3D  [14]  network. In order to generate more samples, the videos of the dataset are converted to sequences of 128 frames with a subsequence length of 16, resulting in 21,284 training samples and 16,177 validation samples. In the proposed approach, inception v-1 architecture is used as the base model, which is inflated from 2D pre-trained model on ImageNet to 3D CNN for videos of facial expressions. For regularizing the network, dropout is used with p = 0.8 on the linear layers. The initial learning rate of the network was set to be 1e -4 and the momentum of 0.9 is used for Stochastic Gradient Descent (SGD). Also weight decay of 5e -4 is used. Due to the hardware limitations and memory constraints, the batch size of the network is set to be 8. Data augmentation is performed on the training data by random cropping, which produces scale invariant model. The number of epochs is set to be 50 and early stopping is used to obtain the best weights of the network. The A network is composed of 3 blocks of convolutional layers, where the first block has convolutional layer followed by max pooling layer. In the second block, there are two convolutional layers followed by max pooling layer. In third block also, there are two convolutional layers followed by average pooling layer, which gives the feature vectors. Finally, the feature vectors are fed to the linear layers to obtain the final prediction of valence or arousal. All the convolutional and linear layers layers are followed by ReLu activation functions. The speech signal is extracted from the corresponding video sequence and re-sampled to 16KHz, which is further segmented to short speech segments. First, we split the extracted speech signal to 5.12 sec, which corresponds to the sequence of 128 frames of the V network. Next spectrogram is obtained using Discrete Fourier Transform (DFT) of length 1024 for each short speech segment of 5.12 sec, where the window length is considered to be 40 msec and the shift length to be 40 msec in order to match with the granularity of annotation frequency. Following aggregation of short-time spectra, we obtain the spectrogram of 128 x 129. Now a normalization step is performed on the obtained spectrograms. The spectrogram is converted to logpower-spectrum, expressed in dB. Finally, mean and variance normalization is performed on the spectrogram. Apart from mean and variance normalization, no other speech specific processing such as silence removal, noise filtering, etc are performed. These spectrograms are then fed to the deep network depicted in Table  I . The A network is trained from scratch, where the initial weights of the network are initialized with values from normal distribution. The number of epochs are set to be 100, and early stopping is used. The network is optimized using SGD with momentum of 0.9. The initial learning rate is set to be 0.001 and batch size is fixed to be 16. Due to the limited data, the network might be prone to over-fitting . Therefore, in order to prevent the network from over-fitting, dropout is used with p = 0.5 after the last linear layer. Also weight decay of 5e -4 is used for all the experiments.\n\nFor the A-V fusion network, we used hyperbolic tangent functions for the activation of cross-attention modules. The dimension of the extracted features of A-V modalities are set to be 1024. In the cross-attention module, the initial weights of the cross-attention matrix is initialized with Xavier method  [38]  and the weights are updated using SGD with momentum of 0.9. The initial learning rate is set to be 0.001 and batch size is fixed to be 16. Also dropout of 0.5 is applied on the attended A-V features and weight decay of 5e -4 is used for all the experiments.\n\nDue to the spontaneity of the expressions of the subjects, the annotations was also found to be highly stochastic in nature. Therefore, a chain of post processing steps are applied to the predictions and labels. A rigorous analysis on some of the post processing steps performed on the annotations was investigated by Huang et al.  [39] . Tzirakis et al.  [7]  explored a series of post processing steps for validating their architecture on the data-set. Inspired by their approach, we have followed similar series of post processing steps to validate our architecture. (i) median filtering with the window size ranging from 0.4sec to 20sec (ii) centering the predicted values by computing the bias between annotated (ground truth) values and predicted values (iii) to match the scaling of predicted values and annotations using the ratio of standard deviation of annotated values and predicted values. (iv) shift in time by shifting the annotations forward in time with values ranging from 0.04 to 10sec to compensate for delay in annotations as there can be a delay in correspondence between the annotated values and the video frames. We use a loss function based on Concordance Correlation Coefficient (ρ c ) as it has been widely explored for dimensional emotion recognition  [7] , which is given by:\n\nC) Evaluation Metric: CCC (ρ c ) has been widely used in the literature to measure the agreement level between the predictions (x) and ground truth (y) annotations for dimensional emotion recognition  [7] . Let µ x and µ y represents the mean of predictions and ground truth, respectively. Similarly, if σ 2\n\nx and σ 2 y denotes the variance of predictions and ground truth, respectively, then ρ c between the predictions and ground truth is:\n\nwhere σ 2 xy denotes the predictions -ground truth covariance.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "V. Results And Discussion",
      "text": "A) Ablation Study: In order to analyze the performance of cross-attention mechanism of the proposed approach, The VGG architecture is pretrained on FER data-set similar to  [15]  and further fine-tuned on the RECOLA data-set. Initially, we compare the proposed approach without LSTM, where the A and V features are concatenated and directly fed to linear layers. Then we have used LSTM based fusion by feeding the concatenated features to LSTM layer followed by fully connected layers. Due to the temporal modeling of the concatenated features, the fusion performance improves over the non LSTM based fusion strategy. We also compare the performance of I3D with baseline concatenation, where the A-V features concatenated without attention and fed to linear layers for valence/arousal prediction similar to that of the fusion mechanism of Juan et al.  [15] . We further compare the proposed approach to that of self-attention  [8] . Unlike simple feature concatenation, self attention emphasizes on the relevant feature components. Therefore, it improves the system performance compared to simple feature concatenation. However, self-attention does not capture the relevance of correlation across the features of A-V modalities, thereby fails to capture the complimentary nature of the modalities. Since cross-attention based mechanism efficiently captures the complementary nature of the modalities by leveraging the cross-correlation across the A-V features, it was shown that the cross-attention based fusion outperforms most of the widely used fusion strategies.Table  II  presents the results of our ablation study. B) Visual Results: We have further validated the proposed approach by visualizing the predictions of valence and arousal for some of the subjects as shown in Figure  2 . Since the cross-attention mechanism efficiently capture the complimentary nature of the A-V modalities, the proposed approach effectively tracks the ground truth for both valence and arousal. For example, though the full frontal pose of the V modality is not available, the proposed approach still tracks the ground truth of valence and arousal by leveraging the A modality, thereby effectively utilizing the complimentary nature of the A and V modalities. (See Figure  2  lower part) C) Comparison with State-of-the-Art: Conventional ML approaches have been explored based on hand-crafted features for dimensional emotion recognition. He et al.  [40]  explored handcrafted features (LPQ-TOP) for V and Low level descriptors (LLD) such as MFCC, energy, etc. for A, along with physiological modalities including electrocardiogram (ECG) and electro-dermal activity (EDA). Due to the use of additional physiological modalities and more LLD descriptors in A, as well as additional geometric features of V, the fusion performance has significant improvement. Han et al.  [41]  explored LLD features for A and facial landmark features (only geometric) for V, and fused in a hierarchical fashion by leveraging the individual advantages of SVM and neural networks and showed further improvement in valence. Inspired by performance of DL models, Tzirakis et al.  [7]  explored deep models in an end-to-end fashion by using Resnet50 for V and 1DCNN for A on raw data. However, the features are directly concatenated and fed to 2 LSTMs, thereby resulting in a decline of fusion performance over individual modalities. The performance has been further improved by Juan et al  [15] , where they have used pretrained model on FER for V and LLD for A. In most of the above mentioned approaches, A-V features are directly concatenated and fed to the prediction model. However, it was found that direct concatenation of features fails to capture the inter-modal relationships  [42] . Recently, Schoneval et al.  [6]  used knowledge distillation for V and VGG network using spectrograms for A. Instead of direct concatenation, they have used two independent CNNs before concatenating them and showed that their fusion performance has improved over the performance of individual modalities. Though deep models have improved the performance over handcrafted features, they fail to effectively leverage the complimentary nature of the A-V modalities. By leveraging the cross-correlation of A and V features, we have improved the system performance using cross-attention based fusion. Therefore, the proposed approach was found to outperform state-of-the-art approaches. For the sake of completeness, we have also compared the proposed approach with 2stage cross-attention as in  [13] . Though it shows slight improvement in performance, the proposed approach with single iteration outperforms state-of-the-art for dimensional emotion recognition with less computational complexity compared to 2-stage cross-attention. D) Results with Fatigue Dataset: The proposed approach was also validated on Fatigue (private) dataset. The Fatigue dataset is obtained from 18 participants in Rehabilitation center, who are suffering from degenerative diseases inducing fatigue which affect their life quality. A total of 27 video sessions are captured from 18 participants with a duration of 40 -45 minutes and the videos are labeled at sequence level on a scale of 0 to 10 for every 10 to 15 minutes. We have considered 80% of data as training data (50,845 samples) and 20% as validation data  (21,792 samples) . The results are presented on the validation data. Due to the rich information of arousal in A, the performance of arousal performs better than valence in A. Similarly, the performance of valence is better than arousal for V modality. We have  also compared our proposed approach with the baseline feature concatenation without cross-attention. Finally, the performance of the proposed approach with cross-attention module is presented. It was found that the proposed approach significantly improves the system performance over baseline of feature concatenation as shown in Table  IV .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "In this paper, we introduce cross-attentional A-V fusion for dimensional emotion recognition from videos. Unlike prior approaches, we focus on improving the fusion strategy based on cross-correlation across the A-V features. First, DL models are trained individually for V and A modalities, where features are extracted using and I3D and 2D-CNN, respectively. Then, an attention mechanism based on crosscorrelation between A and V features are applied on the individual modalities. Finally, the attention weighted features are concatenated and fed to two linear connected layers for predicting valence and arousal. The proposed model efficiently combines the modalities in a complimentary fashion, and significantly improves the performance of the system. Experiments show that our cross-attentional A-V fusion provides competitive results w.r.t. to state-of-the-art approaches.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Let Xa and Xv represents the deep features of A and",
      "page": 3
    },
    {
      "caption": "Figure 1: Block diagram of the proposed cross-attention based A-V fusion model. ”Inc” in V feature extractor denotes Inception Module.",
      "page": 4
    },
    {
      "caption": "Figure 2: Since the cross-attention mechanism efficiently capture the",
      "page": 6
    },
    {
      "caption": "Figure 2: lower part)",
      "page": 6
    },
    {
      "caption": "Figure 2: Visualization of predictions of valence and arousal for subjects ”dev 1” and ”dev 3” respectively",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Input": "Block 1",
          "-": "Conv : 64, 5x5, 1x2\nMax pool\n: 4x4, 4x4",
          "1 x 128 x 129": "64 x 31 x 15"
        },
        {
          "Input": "Block 2",
          "-": "Conv : 128, 5x5, 1x2\nConv : 256, 3x3, 1x1\nMax pool\n: 2x2, 2x2",
          "1 x 128 x 129": "256 x 15 x 4"
        },
        {
          "Input": "Block 3",
          "-": "Conv : 512, 5x5, 1x1\nConv : 1024, 3x3, 1x1\nAvg pool\n: 13x2, 1x1",
          "1 x 128 x 129": "1024 x 1 x 1"
        },
        {
          "Input": "Block 4",
          "-": "Linear\n:\nin = 1024, out = 256",
          "1 x 128 x 129": "256x1"
        },
        {
          "Input": "Block 5",
          "-": "Linear\n:\nin = 256, out = 1",
          "1 x 128 x 129": "1x1"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "He et al.\n[40]": "Han et al.\n[41]",
          "AVEC (2015)": "IVU (2017)",
          "0.400": "0.480",
          "0.441": "0.592",
          "0.609": "0.554",
          "0.800": "0.760",
          "0.587": "0.350",
          "0.747": "0.685"
        },
        {
          "He et al.\n[40]": "Tzirakis et al.\n[7]",
          "AVEC (2015)": "IEEE JSTSP (2017)",
          "0.400": "0.428",
          "0.441": "0.637",
          "0.609": "0.502",
          "0.800": "0.786",
          "0.587": "0.371",
          "0.747": "0.731"
        },
        {
          "He et al.\n[40]": "Juan et al.\n[15]",
          "AVEC (2015)": "IEEE SMC (2019)",
          "0.400": "-",
          "0.441": "-",
          "0.609": "0.565",
          "0.800": "-",
          "0.587": "-",
          "0.747": "0.749"
        },
        {
          "He et al.\n[40]": "Schoneval et al.\n[6]",
          "AVEC (2015)": "PR Letters (2021)",
          "0.400": "0.460",
          "0.441": "0.550",
          "0.609": "0.630",
          "0.800": "0.800",
          "0.587": "0.570",
          "0.747": "0.810"
        },
        {
          "He et al.\n[40]": "Proposed Approach",
          "AVEC (2015)": "Cross-Attention",
          "0.400": "0.463",
          "0.441": "0.642",
          "0.609": "0.685",
          "0.800": "0.822",
          "0.587": "0.582",
          "0.747": "0.835"
        },
        {
          "He et al.\n[40]": "Proposed Approach",
          "AVEC (2015)": "2-stage Cross-Attention",
          "0.400": "0.463",
          "0.441": "0.642",
          "0.609": "0.690",
          "0.800": "0.822",
          "0.587": "0.582",
          "0.747": "0.838"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "Audio only (2D-CNN)",
          "Fatigue Level": "0.312"
        },
        {
          "Method": "Visual only (I3D)",
          "Fatigue Level": "0.415"
        },
        {
          "Method": "Feature Concatenation",
          "Fatigue Level": "0.378"
        },
        {
          "Method": "Proposed Approach (Cross-Attention)",
          "Fatigue Level": "0.421"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Features and classifiers for emotion recognition from speech: a survey from 2000 to 2011",
      "authors": [
        "C Anagnostopoulos",
        "T Iliou",
        "I Giannoukos"
      ],
      "year": "2015",
      "venue": "Artif Intell Rev"
    },
    {
      "citation_id": "2",
      "title": "Survey on audiovisual emotion recognition: databases, features, and data fusion strategies",
      "authors": [
        "C.-H Wu",
        "J.-C Lin",
        "W.-L Wei"
      ],
      "year": "2014",
      "venue": "APSIPA Trans. on Signal and Information Processing"
    },
    {
      "citation_id": "3",
      "title": "Deep multimodal representation learning from temporal data",
      "authors": [
        "X Yang",
        "P Ramesh",
        "R Chitta",
        "S Madhvanath",
        "E Bernal",
        "J Luo"
      ],
      "year": "2017",
      "venue": "CVPR"
    },
    {
      "citation_id": "4",
      "title": "Audio-visual affect recognition through multi-stream fused hmm for hci",
      "authors": [
        "Z Zeng",
        "J Tu",
        "B Pianfetti",
        "M Liu",
        "T Zhang",
        "Z Zhang",
        "T Huang",
        "S Levinson"
      ],
      "year": "2005",
      "venue": "CVPR"
    },
    {
      "citation_id": "5",
      "title": "Emotion recognition in the wild with feature fusion and multiple kernel learning",
      "authors": [
        "J Chen",
        "Z Chen",
        "Z Chi",
        "H Fu"
      ],
      "year": "2014",
      "venue": "ICMI"
    },
    {
      "citation_id": "6",
      "title": "Leveraging recent advances in deep learning for audio-visual emotion recognition",
      "authors": [
        "L Schoneveld",
        "A Othmani",
        "H Abdelkawy"
      ],
      "year": "2021",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "7",
      "title": "End-to-end multimodal emotion recognition using deep neural networks",
      "authors": [
        "P Tzirakis",
        "G Trigeorgis",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2017",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "8",
      "title": "End-to-end multimodal affect recognition in real-world environments",
      "authors": [
        "P Tzirakis",
        "J Chen",
        "S Zafeiriou",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "9",
      "title": "Weakly-supervised action localization by generative attention modeling",
      "authors": [
        "B Shi",
        "Q Dai",
        "Y Mu",
        "J Wang"
      ],
      "year": "2020",
      "venue": "CVPR"
    },
    {
      "citation_id": "10",
      "title": "Audio-visual attention networks for emotion recognition",
      "authors": [
        "J Lee",
        "S Kim",
        "S Kim",
        "K Sohn"
      ],
      "year": "2018",
      "venue": "Workshop AVSUIM"
    },
    {
      "citation_id": "11",
      "title": "Detecting expressions with multimodal transformers",
      "authors": [
        "S Parthasarathy",
        "S Sundaram"
      ],
      "year": "2021",
      "venue": "2021 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "12",
      "title": "Cross attention network for few-shot classification",
      "authors": [
        "R Hou",
        "H Chang",
        "B Ma",
        "S Shan",
        "X Chen"
      ],
      "year": "2019",
      "venue": "NIPS"
    },
    {
      "citation_id": "13",
      "title": "Cross-attentional audio-visual fusion for weakly-supervised action localization",
      "authors": [
        "J.-T Lee",
        "M Jain",
        "H Park",
        "S Yun"
      ],
      "year": "2021",
      "venue": "ICLR"
    },
    {
      "citation_id": "14",
      "title": "Quo vadis, action recognition? a new model and the kinetics dataset",
      "authors": [
        "J Carreira",
        "A Zisserman"
      ],
      "year": "2017",
      "venue": "CVPR"
    },
    {
      "citation_id": "15",
      "title": "Emotion recognition using fusion of audio and video features",
      "authors": [
        "J Ortega",
        "P Cardinal",
        "A Koerich"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Systems, Man and Cybernetics (SMC)"
    },
    {
      "citation_id": "16",
      "title": "Deep auto-encoders with sequential learning for multimodal dimensional emotion recognition",
      "authors": [
        "D Nguyen",
        "D Nguyen",
        "R Zeng",
        "T Nguyen",
        "S Tran",
        "T Nguyen",
        "S Sridharan",
        "C Fookes"
      ],
      "year": "2021",
      "venue": "IEEE Trans. on Multimedia"
    },
    {
      "citation_id": "17",
      "title": "Multimodal Speech Emotion Recognition Using Cross Attention with Aligned Audio and Text",
      "authors": [
        "Y Lee",
        "S Yoon",
        "K Jung"
      ],
      "year": "2020",
      "venue": "INTERSPEECH"
    },
    {
      "citation_id": "18",
      "title": "Multimodal Emotion Recognition Using Cross-Modal Attention and 1D Convolutional Neural Networks",
      "authors": [
        "A Patil"
      ],
      "year": "2020",
      "venue": "INTERSPEECH"
    },
    {
      "citation_id": "19",
      "title": "Visual question answering with memory-augmented networks",
      "authors": [
        "C Ma",
        "C Shen",
        "A Dick",
        "Q Wu",
        "P Wang",
        "A Hengel",
        "I Reid"
      ],
      "year": "2018",
      "venue": "CVPR"
    },
    {
      "citation_id": "20",
      "title": "Multi-modality cross attention network for image and sentence matching",
      "authors": [
        "X Wei",
        "T Zhang",
        "Y Li",
        "Y Zhang",
        "F Wu"
      ],
      "year": "2020",
      "venue": "CVPR"
    },
    {
      "citation_id": "21",
      "title": "An end-to-end visual-audio attention network for emotion recognition in user-generated videos",
      "authors": [
        "S Zhao",
        "Y Ma",
        "Y Gu",
        "J Yang",
        "T Xing",
        "P Xu",
        "R Hu",
        "H Chai",
        "K Keutzer"
      ],
      "year": "2020",
      "venue": "AAAI"
    },
    {
      "citation_id": "22",
      "title": "Multimodal attentionmechanism for temporal emotion recognition",
      "authors": [
        "E Ghaleb",
        "J Niehues",
        "S Asteriadis"
      ],
      "year": "2020",
      "venue": "ICIP"
    },
    {
      "citation_id": "23",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "NIPS"
    },
    {
      "citation_id": "24",
      "title": "Implicit knowledge injectable cross attention audiovisual model for group emotion recognition",
      "authors": [
        "Y Wang",
        "J Wu",
        "P Heracleous",
        "S Wada",
        "R Kimura",
        "S Kurihara"
      ],
      "year": "2020",
      "venue": "ICMI"
    },
    {
      "citation_id": "25",
      "title": "Wavenet with cross-attention for audiovisual speech recognition",
      "authors": [
        "H Wang",
        "F Gao",
        "Y Zhao",
        "L Wu"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "26",
      "title": "Multi-objective based spatio-temporal feature representation learning robust to expression intensity variations for facial expression recognition",
      "authors": [
        "D Kim",
        "W Baddar",
        "J Jang",
        "Y Ro"
      ],
      "year": "2019",
      "venue": "IEEE TAC"
    },
    {
      "citation_id": "27",
      "title": "Continuous prediction of spontaneous affect from multiple cues and modalities in valencearousal space",
      "authors": [
        "M Nicolaou",
        "H Gunes",
        "M Pantic"
      ],
      "year": "2011",
      "venue": "IEEE TAC"
    },
    {
      "citation_id": "28",
      "title": "Lstmmodeling of continuous emotions in an audiovisual affect recognition framework",
      "authors": [
        "M Wöllmer",
        "M Kaiser",
        "F Eyben",
        "B Schuller",
        "G Rigoll"
      ],
      "year": "2013",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "29",
      "title": "Speech based emotion recognition",
      "authors": [
        "V Sethu",
        "J Epps",
        "E Ambikairajah"
      ],
      "year": "2015",
      "venue": "Speech and Audio Processing for Coding, Enhancement and Recognition"
    },
    {
      "citation_id": "30",
      "title": "Emotion recognition from variable-length speech segments using deep learning on spectrograms",
      "authors": [
        "X Ma",
        "Z Wu",
        "J Jia",
        "M Xu",
        "H Meng",
        "L Cai"
      ],
      "year": "2018",
      "venue": "INTERSPEECH"
    },
    {
      "citation_id": "31",
      "title": "Efficient emotion recognition from speech using deep learning on spectrograms",
      "authors": [
        "A Satt",
        "S Rozenberg",
        "R Hoory"
      ],
      "year": "2017",
      "venue": "Efficient emotion recognition from speech using deep learning on spectrograms"
    },
    {
      "citation_id": "32",
      "title": "Emotion recognition from speech using spectrograms and shallow neural networks",
      "authors": [
        "A Slimi",
        "M Hamroun",
        "M Zrigui",
        "H Nicolas"
      ],
      "year": "2020",
      "venue": "In ICAMCM"
    },
    {
      "citation_id": "33",
      "title": "Emotion recognition in speech using cross-modal transfer in the wild",
      "authors": [
        "S Albanie",
        "A Nagrani",
        "A Vedaldi",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "26th ACM'M"
    },
    {
      "citation_id": "34",
      "title": "Introducing the recola multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "F Ringeval",
        "A Sonderegger",
        "J Sauer",
        "D Lalanne"
      ],
      "year": "2013",
      "venue": "FG"
    },
    {
      "citation_id": "35",
      "title": "Av+ec 2015: The first affect recognition challenge bridging across audio, video, and physiological data",
      "authors": [
        "F Ringeval",
        "B Schuller",
        "M Valstar",
        "S Jaiswal",
        "E Marchi",
        "D Lalanne",
        "R Cowie",
        "M Pantic"
      ],
      "year": "2015",
      "venue": "AVEC"
    },
    {
      "citation_id": "36",
      "title": "Avec 2016: Depression, mood, and emotion recognition workshop and challenge",
      "authors": [
        "M Valstar",
        "J Gratch",
        "B Schuller",
        "F Ringeval",
        "D Lalanne",
        "M Torres",
        "S Scherer",
        "G Stratou",
        "R Cowie",
        "M Pantic"
      ],
      "year": "2016",
      "venue": "AVEC"
    },
    {
      "citation_id": "37",
      "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
      "authors": [
        "K Zhang",
        "Z Zhang",
        "Z Li",
        "Y Qiao"
      ],
      "year": "2016",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "38",
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "authors": [
        "X Glorot",
        "Y Bengio"
      ],
      "year": "2010",
      "venue": "In ACAIS"
    },
    {
      "citation_id": "39",
      "title": "An investigation of annotation delay compensation and output-associative fusion for multimodal continuous emotion prediction",
      "authors": [
        "Z Huang",
        "T Dang",
        "N Cummins",
        "B Stasak",
        "P Le",
        "V Sethu",
        "J Epps"
      ],
      "year": "2015",
      "venue": "AVEC"
    },
    {
      "citation_id": "40",
      "title": "Multimodal affective dimension prediction using deep bidirectional long short-term memory recurrent neural networks",
      "authors": [
        "L He",
        "D Jiang",
        "L Yang",
        "E Pei",
        "P Wu",
        "H Sahli"
      ],
      "year": "2015",
      "venue": "Multimodal affective dimension prediction using deep bidirectional long short-term memory recurrent neural networks"
    },
    {
      "citation_id": "41",
      "title": "Strength modelling for real-world automatic continuous affect recognition from audiovisual signals",
      "authors": [
        "J Han",
        "Z Zhang",
        "N Cummins",
        "F Ringeval",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "Image Vision Comput"
    },
    {
      "citation_id": "42",
      "title": "Multimodal machine learning: A survey and taxonomy",
      "authors": [
        "T Baltrušaitis",
        "C Ahuja",
        "L.-P Morency"
      ],
      "year": "2019",
      "venue": "IEEE PAMI"
    }
  ]
}