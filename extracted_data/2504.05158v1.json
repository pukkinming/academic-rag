{
  "paper_id": "2504.05158v1",
  "title": "Leveraging Label Potential For Enhanced Multimodal Emotion Recognition",
  "published": "2025-04-07T15:00:34Z",
  "authors": [
    "Xuechun Shao",
    "Yinfeng Yu",
    "Liejun Wang"
  ],
  "keywords": [
    "multimodal emotion recognition",
    "label signal enhancement module",
    "joint objective optimization",
    "attributionprediction consistency constraint"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal emotion recognition (MER) seeks to integrate various modalities to predict emotional states accurately. However, most current research focuses solely on the fusion of audio and text features, overlooking the valuable information in emotion labels. This oversight could potentially hinder the performance of existing methods, as emotion labels harbor rich, insightful information that could significantly aid MER. We introduce a novel model called Label Signal-Guided Multimodal Emotion Recognition (LSGMER) to overcome this limitation. This model aims to fully harness the power of emotion label information to boost the classification accuracy and stability of MER. Specifically, LSGMER employs a Label Signal Enhancement module that optimizes the representation of modality features by interacting with audio and text features through label embeddings, enabling it to capture the nuances of emotions precisely. Furthermore, we propose a Joint Objective Optimization(JOO) approach to enhance classification accuracy by introducing the Attribution-Prediction Consistency Constraint (APC), which strengthens the alignment between fused features and emotion categories. Extensive experiments conducted on the IEMOCAP and MELD datasets have demonstrated the effectiveness of our proposed LSGMER model.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotion is an essential aspect of human life that profoundly influences our thoughts, behaviors, and decision-making. Emotion recognition technology is widely used in chatbots  [1] , social media analytics  [2] , intelligent customer service  [3] , and mental health monitoring  [4] , etc. It has become a significant research topic in the field of human-computer interaction and plays an important role in enhancing user experience, optimizing the interaction process, and assisting in emotion analysis.\n\nEmotion is inherently multimodal, and a single modality is insufficient to fully capture its complexity. This challenge exists in other domains as well: for example, navigation requires the fusion of visual and auditory modalities to enhance environmental perception  [5]    [6] . In emotion recognition, effective integration of multimodal information such as speech and text is essential to improve recognition performance  [7] . † Both Yinfeng Yu and Liejun Wang are corresponding authors. Currently, most emotion recognition methods use interactions based on attention mechanisms  [8]  to fuse multimodal information. For instance, MFCN  [9]  employs a cross-modal attention mechanism to directly integrate audio and text features, while MSMSER  [10]  first computes self-attention for audio and text features separately, then applies an additional attention mechanism again to extract emotional information using learnable query vectors from the concatenated modal features. Although these methods effectively integrate information from different modalities, they still have some limitations. Existing methods mainly focus on the fusion of audio and text features, as illustrated in Fig.  1(a) . They overlook the fact that emotion labels not only indicate emotion categories but also contain multidimensional features that can assist models in better capturing complex emotional characteristics. Therefore, how to fully utilize emotion labels is the core issue of this research.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Label Embeddings",
      "text": "We propose a Label Signal-Guided Multimodal Emotion Recognition (LSGMER) to address the limitations of existing methods in utilizing emotion labels and aligning multimodal features. The model introduces label signals through two key components: First, we design a Label Signal Enhancement Module that explicitly combines label signals with audio and text features, enabling the model to capture emotion features shared across different modalities. At the same time, we apply the moving average(MA) method to further smooth the label embedding updates, which helps the model to better capture fine-grained emotion knowledge. Second, we propose an innovative joint objective optimization(JOO) method that combines the Attribution-Prediction Consistency Constraint (APC) with cross-entropy loss to guide the training process. This method forces the fused features to align with the label embeddings at the entity level, enhancing both the accuracy and the consistency of feature expression. As shown in Fig.  1 (b), LS-GMER models the multimodal interaction between audio, text, and emotion labels while using label embeddings as anchors to effectively guide the fusion and optimization of emotion features. This design not only strengthens the guiding role of label signals but also significantly improves the performance of multimodal emotion recognition.\n\nThe main contributions of this paper are as follows:",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "The attention mechanism has become one of the most widely used approaches in multimodal emotion recognition due to its advantages in information capture and cross-modal association modeling. Many studies propose innovations based on the attention mechanism, achieving significant progress. For example,  [11]  proposes a Bayesian attention mechanism that improves emotion recognition accuracy by incorporating emotion-related external knowledge, helping the model focus more effectively on emotion-related features.  [7]  introduces a sliding-window attention mechanism that limits the scope of attention to reduce redundant computation and interference from extraneous information, enabling the model to dynamically fuse text and audio modalities within the maximum effective feature perception.  [12]  introduces a novel multimodal neural network architecture that captures fine-grained emotion features by combining audio and text information while enhancing the model's expressive capability through multitask learning.  [13]  proposes a transformer-based model combined with self-distillation, which effectively captures intra-modal and inter-modal interactions and dynamically fuses multimodal information to enhance emotion recognition performance.  [14]  introduces a low-rank matching attention method, which aims to solve the issue of intra-modal and inter-modal affective interactions, as well as the problem of excessive computational complexity in existing methods.\n\nDespite the progress made by existing approaches in crossmodal interaction and emotion recognition, most of the studies ignore the guiding role of emotion labels in the process of feature alignment and feature fusion, resulting in models that fail to make full use of the emotion information during feature extraction and information integration, thus limiting the overall performance improvement.\n\nRecently, some studies have begun exploring the application of emotion label information in multimodal emotion recognition. For example,  [15]  uses a pre-trained text model to extract features from label text, and optimizes the distribution of audio features and label text features through contrastive learning. This approach brings audio features with the same emotion label closer together, while further distancing audio features with different emotion labels. However, directly using label text as entity-level anchor information provides a highly simplified description of emotion, which fails to capture more complex emotional details or subtle emotional changes.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Task Definition",
      "text": "We formulate MER as a classification task. Given a text and its corresponding audio, we extract the corresponding text feature sequence T = {(t i )} n i=1 from the text and the corresponding audio feature sequence A = {(a i )} m i=1 from the audio, where n is the length of the text sequence and m is the length of the audio sequence. Our goal is to predict emotion categories using both modalities simultaneously.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iv. Methodology",
      "text": "The overall architecture of the LSGMER model we proposed is shown in Fig.  2 . It consists of five main components: Feature Extraction, Cross-modal Interaction, Label Signal Enhancement Module with MA Method, Feature Fusion and Classification, and Joint Objective Optimization.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Feature Extraction",
      "text": "Building on previous work  [16] , we adopt RoBERTa  [17]  as the text feature extractor and use the output of its final layer as the text features. These extracted features are then passed through a Bi-LSTM layer to further capture the contextual information in the sequence.\n\nConsidering that audio features contain more information compared to text features  [18] , relying solely on pre-trained models for audio feature extraction may not fully capture the subtle variations in the audio signal. Inspired by the multilevel audio feature extraction method proposed in  [19] , we combine spectral features with a pre-trained audio model to capture the rich information in the audio signal from multiple levels. To maintain consistency, we use AlexNet to process the spectrogram. At the same time, we use WavLM That is amazing, congratulations.\n\nElement-wise Addition Element-wise Product\n\nFig.  2 : The overall architecture of the LSGMER. MHA refers to Multi-Head Attention, where g 1 and g 2 represent the learning weights for the audio and text modalities, respectively.\n\n[20] to extract high-level features of audio. WavLM focuses more on contextual modeling, shows better performance than other pre-trained models in several audio tasks, and has been widely used in emotion recognition tasks  [7] [21] . Finally, the extracted audio features are summed to create a richer audio feature representation.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Cross-Modal Interaction",
      "text": "The simple sharing of the hidden state is insufficient to achieve effective information transfer between the two modalities. Therefore, we propose a cross-modal attention module that enables the model to capture the relationships between text and audio from multiple perspectives.\n\nSpecifically, we use the text features H t0 as the query, and the audio features H a0 as both the key and value to perform cross-modal attention. This process produces text features enriched with audio information:\n\nSimilarly, we obtain audio features enriched with text information:",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Label Signal Enhancement Module With Ma Method",
      "text": "Although we perform preliminary fusion and alignment of audio and text features in the cross-modal interaction module, the mechanism still has limitations in coping with feature inconsistency and precise alignment of emotion information, which is insufficient to effectively guide the model to focus on the core features of the emotion. Inspired by  [22] , we construct the LSMA module. In this process, label embeddings are used as additional emotion category information to establish associations with audio and text features, respectively, allowing the model to concentrate on key features related to emotion.\n\nTo begin, we define a series of label embeddings L s , each corresponding to a specific emotion category. These label embeddings are then used to compute attention with audio features H a1 and text features H t1 respectively. The label embeddings serve as the query, while the audio and text features are used as the key and value. The resulting enhanced audio and text features can be represented as:\n\nNext, we perform a weighted fusion of the obtained features with the original features to ensure that the crucial original signals are preserved during the label signal enhancement process. Taking audio features as an example, we apply the sigmoid function to H a2 to obtain the weight vector w a = σ(H a2 ). This weight vector is then element-wise multiplied with H a0 , and the weighted original audio features are added to H a2 to obtain the final audio features H a :\n\nSimilarly, the final text features H t can be expressed as:\n\nSince the label embeddings are learnable, they are adapted to the data of the current batch and serve as inputs for the next batch. Therefore the difference in label embeddings at the beginning of each epoch can be substantial, requiring the model to constantly adapt to the new label embeddings, leading to instability in the training process. To alleviate this concern, we employ the MA method to compute the label embeddings.\n\nSpecifically, we update the label embeddings using the following formula:\n\nwhere α is a hyperparameter that controls the update rate, L t-1 are the label embeddings at the beginning of epoch t -1, and L ′ t-1 is the updated label embeddings after epoch t -1. When t = 1, the label embeddings are randomly initialized.\n\nThis update method ensures that historical information effectively guides the current learning process while preventing excessive fluctuations in label embeddings during training.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "D. Dynamics Fusion And Classification",
      "text": "We perform the fusion of audio and text features using an expert gate  [23]  to dynamically adjust their contributions to emotion recognition.\n\nThe features H a and H t are concatenated into a combined feature vector H concat , and the dynamic weights are computed through a fully connected layer, with the output values normalized by the softmax function. The calculation is as follows:\n\nwhere g a and g t are the dynamic weights of audio features and text features, respectively. The audio and text features are then weighted according to these weights to obtain the final fused feature representation:\n\nwhere ⊙ denotes element-wise multiplication.\n\nThe fused feature H fused is subsequently passed through a Multi-Layer Perceptron to compute the emotion category scores for the samples. Next, the scores of each sample are normalized using the softmax function to get the predicted probability for each emotion category:\n\np = softmax(logits).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "E. Joint Objective Optimization",
      "text": "Feature fusion can effectively compensate for the limitations of a single modality, but ensuring that the fused features accurately represent emotion category information remains a challenge due to the lack of effective supervisory signals.\n\nTo address this issue, we propose a JOO that combines the Attribution-Predictive Consistency Constraint (APC)  [24]  and the cross-entropy loss to guide the model training. The schematic of APC loss is shown in Fig.  3 . The APC loss minimizes the difference between the similarity distribution of the fused features and label embeddings and the model predictive distribution. It ensures that when the model performs multimodal feature fusion, the learned features can better express emotions, thus improving the classification accuracy.\n\nSpecifically, we compute the cosine similarity between fused features and label embeddings:\n\nNext, the similarity distribution q i = {q(i, j)} N j=1 is produced by applying the softmax function on the cosine similarity score distribution:\n\nwhere q(i, j) represents the probability of similarity between the i-th sample and the j-th emotion label, reflecting the likelihood that the sample belongs to the emotion category. Finally, the APC improves the consistency between the similarity distribution q i and the predictive probability distribution p i by minimizing the Jensen-Shannon divergence. The objective function can be expressed as:\n\nJsDiv(q i,j ∥ p i,j ), (  16 )\n\nwhere M is the training data size and N is the number of emotion categories. Then, we utilize cross-entropy loss to estimate the quality of emotion prediction:\n\nwhere y i,j is the one-hot encoding of the ground truth. Finally, the APC loss and the cross-entropy loss together constitute the total loss of the LSGMER framework. The total loss is calculated as follows:\n\nwhere α and β are the balancing weights.\n\nV. EXPERIMENTS",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "A. Dataset",
      "text": "We use the IEMOCAP  [25]  and MELD  [26]  datasets to evaluate the proposed model.\n\nThe IEMOCAP dataset contains approximately 12 hours of data, divided into five sessions, each consisting of two speakers. All the discourses are labeled with one of ten emotions: angry, happy, sad, neutral, frustrated, excited, fearful, surprised, disgusted, and other. Following previous research, we perform a categorization task on 5531 discourses, focusing on four emotional categories: happy (combined with excited), angry, sad, and neutral. We conduct our experiments in the 5-fold leave-one-session-out strategy. One session at a time is selected as the test set and the remaining four serve as the training set.\n\nThe MELD dataset is a multi-party dataset created from the Friends TV series. The dataset contains about 13,000 discourses. Each is labeled with one of the following seven emotions: anger, disgust, sad, joy, neutral, surprise and fear. The dataset is divided into train, valid, and test sets. In this experiment, we use only the train and test sets.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Implementation Details",
      "text": "We extract 768-dimensional text embeddings and 1024dimensional audio embeddings using the pre-trained models RoBERTa and WavLM, and employ AdamW as the optimizer. On the IEMOCAP dataset, the learning rate is set to 5e-4, and α and β are set to 1 and 0.05, respectively. On the MELD dataset, the learning rate is set to 5e-4, and α and β are set to 1 and 0.1, respectively. In the MA method, the hyperparameter controlling the update rate is set to 0.99. We train the model on these two datasets for 50 epochs respectively.\n\nWe use Weighted Accuracy (WA), Unweighted Accuracy (UA) and Weighted F1-Score (WF1) as evaluation metrics.  [27]  combines three attention mechanisms, local intra-modal attention, cross-modal attention, and global intermodal attention to effectively learn emotional features.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Baselines",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Mer-Han",
      "text": "DWFORMER  [28]  proposes a dynamic local window transformer module and a dynamic global window transformer module to fully utilize local details and global context information.\n\nLSTM-Attn  [29]  introduces a novel strategy for feature pooling over time which uses local attention in order to focus on specific regions of a speech signal that are more emotionally salient. KS-Transformer  [30]  proposes a sparse transformer model that can focus on the key emotional information from different modalities during cross-modal computation, while also reducing redundant calculations.\n\nMSMSER  [10]  designs a set of trainable emotion tokens to retrieve emotion information from concatenated audio and text features. Randomly masking a modality during training forces the model to fully perceive the emotional features in each modality.\n\nMMRBN  [18]  proposes that audio is more important than text in emotion recognition. It utilizes cross-modal attention to fuse features between modalities, and then dynamically combines emotional information from each modality.\n\nMFDR  [21]  proposes sliding adaptive window attention for modeling the acoustic-text fusion stage, using dynamic frame convolution to identify and weaken fine-grained information that is irrelevant to emotional expression.\n\nDST  [31]  introduces a deformable speech transformer that adaptively determines the size and position of the attention window to reduce redundant computation.\n\nMSTR  [32]  utilizes a multi-scale transformer to capture emotional information in speech, effectively capturing the variations of emotion across different temporal scales, thereby enhancing the accuracy of emotion recognition.\n\nMM-DFN  [33]  designs a new graph-based dynamic fusion module that helps to fuse multimodal information and enhance the complementarity between modalities.\n\nMCFN  [9]  proposes a two-stage fusion network that first learns audio and text features separately and then fuses them through a modality collaborative learning module.\n\nGateM 2 Former  [34]  dynamically integrates representations from different layers of a pre-trained model through a gating mechanism, combining different modality experts and effectively enhancing model flexibility.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "D. Results And Discussion",
      "text": "Tables I and II present the performance of the baseline models and LSGMER on the IEMOCAP and MELD datasets, respectively. On the IEMOCAP dataset, LSGMER outperforms all baseline models, achieving improvements of 2.11% in WA and 0.65% in UA compared to GateM 2 Former. On the MELD dataset, the LSGMER model also demonstrates leading performance, achieving a WF1 score of 62.9%, which is significantly higher than the other comparative models, with a 1.13% improvement over MCFN. These results further validate the superiority and wide applicability of LSGMER in multimodal emotion recognition tasks.\n\nWe attribute the performance improvement to the guiding role of emotion labels in the model. We introduce emotion label information into the model, and emotion label embeddings provide explicit emotion category anchors during the training process. This approach is different from traditional methods, which rely only on audio and text features for training and ignore the rich information embedded in emotion labels. Guided by the emotion label embeddings, the model is able to capture the emotion features more accurately and make more appropriate decisions when fusing multimodal information. At the same time, the APC loss can ensure that the features learned by the model match the emotion label embeddings. This process can effectively improve the accuracy of emotion recognition and reduce redundant information in multimodal information fusion, ultimately realizing more efficient and accurate emotion recognition.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "E. Ablation Study",
      "text": "We carry out ablation experiments on the IEMOCAP and MELD datasets. Table  III      Removing the LSMA module and JOO shows a significant decrease in model performance, indicating that these two modules play a crucial role in emotion recognition tasks. The LSMA module utilizes label embeddings to align audio and text features, allowing the model to focus on key features related to emotion. Meanwhile, JOO further strengthens the guidance of label signals. Without these two modules, the model can only rely on audio and text features for training, failing to accurately capture emotional information, which negatively impacts the accuracy of emotion classification. This experiment further emphasizes the importance of label signals, as they provide effective guidance for emotion categories and enhance the overall recognition performance of the model.\n\nThe results of these ablation experiments demonstrate that each module is essential and works synergistically to enhance the model's ability to recognize emotion.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "F. Visualization",
      "text": "Fig.  4  illustrates the normalized confusion matrix comparison of LSGMER on the IEMOCAP dataset with and without label guidance. The results clearly show that the recognition precisions of emotion categories are significantly improved with the label guidance, especially for the categories of sadness   and happiness, which are increased by 4% and 8%, respectively. These enhancements indicate that label embeddings help the model to distinguish and recognize emotions more clearly by providing additional emotion category information to the model, which reduces the confusion between emotion categories, especially the crossover between neutral, sad and happy. By integrating label embeddings, the model is able to optimize feature representations more effectively across categories during the learning process, thereby boosting the overall recognition accuracy. We extract the fused feature of each discourse on IEMOCAP from LSGMER with or without label guidance. As can be seen in Fig.  5 , when there is no guidance from the label embeddings, the distribution between emotion categories is more chaotic, and there is a large overlap between different emotions, which leads to the model's difficulty in distinguishing these emotion categories. After adding label signals, the distribution of emotion categories in the feature space is more dispersed, the boundaries between emotion categories are more obvious, and the overlapping region is significantly reduced. This demonstrates that the guiding effect of label signals enables the model to better optimize the boundaries between emotion categories and improves the separability of the features.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "In this paper, we introduce a novel label signal-guided MER architecture that innovatively incorporates LSMA and JOO modules. These modules effectively introduce label signals into the model, providing additional emotion category information and utilizing label embeddings to align features across different modalities. This enables the model to process multimodal data, such as audio and text, more efficiently and to achieve more accurate emotion classification. Experimental results on two datasets demonstrate that our proposed model achieves new state-of-the-art performance.\n\nFor future work, we will improve the generalization ability of the model and extend its application to real-world scenarios. Drawing on research in the field of audio-visual navigation  [35] , which focuses on improving robustness in dynamic and noisy environments, we would like to employ a similar strategy to enhance the adaptability of emotion recognition systems in complex environments. This will enable us to maintain efficient emotion categorization and decision-making capabilities in the face of various disturbances and environmental noise.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: A sketched comparison between the previous main-",
      "page": 1
    },
    {
      "caption": "Figure 1: (a). They overlook the fact that emotion",
      "page": 1
    },
    {
      "caption": "Figure 2: It consists of five main components:",
      "page": 2
    },
    {
      "caption": "Figure 2: The overall architecture of the LSGMER. MHA refers to Multi-Head Attention, where g1 and g2 represent the learning",
      "page": 3
    },
    {
      "caption": "Figure 3: The Attribution-Prediction Consistency Constraint.",
      "page": 4
    },
    {
      "caption": "Figure 3: The APC loss",
      "page": 4
    },
    {
      "caption": "Figure 4: illustrates the normalized confusion matrix compari-",
      "page": 6
    },
    {
      "caption": "Figure 4: Comparison of normalized confusion matrices on the",
      "page": 7
    },
    {
      "caption": "Figure 5: The t-SNE visualization of feature distribution on the",
      "page": 7
    },
    {
      "caption": "Figure 5: , when there is no guidance from the label",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "S11": "...",
          "...": "...",
          "S1i": "...",
          "S1m": "..."
        },
        {
          "S11": "Sj1",
          "...": "...",
          "S1i": "Sji",
          "S1m": "Sjm"
        },
        {
          "S11": "...",
          "...": "...",
          "S1i": "...",
          "S1m": "..."
        },
        {
          "S11": "Sn1",
          "...": "...",
          "S1i": "Sni",
          "S1m": "Snm"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.1": "...",
          "...": "...",
          "0.5": "...",
          "0.8": "..."
        },
        {
          "0.1": "0.5",
          "...": "",
          "0.5": "0.1",
          "0.8": "0.2"
        },
        {
          "0.1": "...",
          "...": "...",
          "0.5": "...",
          "0.8": "..."
        },
        {
          "0.1": "0.1",
          "...": "...",
          "0.5": "0.2",
          "0.8": "0"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The design and implementation of xiaoice, an empathetic social chatbot",
      "authors": [
        "L Zhou",
        "J Gao",
        "D Li",
        "H.-Y Shum"
      ],
      "year": "2020",
      "venue": "Computational Linguistics"
    },
    {
      "citation_id": "2",
      "title": "Emotion detection and analysis on social media",
      "authors": [
        "B Gaind",
        "V Syal",
        "S Padgalwar"
      ],
      "year": "2019",
      "venue": "Emotion detection and analysis on social media",
      "arxiv": "arXiv:1901.08458"
    },
    {
      "citation_id": "3",
      "title": "Acoustic and lexical sentiment analysis for customer service calls",
      "authors": [
        "B Li",
        "D Dimitriadis",
        "A Stolcke"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Emokey: An emotion-aware smartphone keyboard for mental health monitoring",
      "authors": [
        "S Ghosh",
        "S Sahu",
        "N Ganguly",
        "B Mitra",
        "P De"
      ],
      "year": "2019",
      "venue": "2019 11th international conference on communication systems & networks (COMSNETS)"
    },
    {
      "citation_id": "5",
      "title": "Pay self-attention to audio-visual navigation",
      "authors": [
        "Y Yu",
        "L Cao",
        "F Sun",
        "X Liu",
        "L Wang"
      ],
      "year": "2022",
      "venue": "Pay self-attention to audio-visual navigation",
      "arxiv": "arXiv:2210.01353"
    },
    {
      "citation_id": "6",
      "title": "Echo-enhanced embodied visual navigation",
      "authors": [
        "Y Yu",
        "L Cao",
        "F Sun",
        "C Yang",
        "H Lai",
        "W Huang"
      ],
      "year": "2023",
      "venue": "Neural Computation"
    },
    {
      "citation_id": "7",
      "title": "Swrr: Feature map classifier based on sliding window attention and high-response feature reuse for multimodal emotion recognition",
      "authors": [
        "Z Zhao",
        "T Gao",
        "H Wang",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "Proc. INTERSPEECH 2023"
    },
    {
      "citation_id": "8",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "9",
      "title": "A dual attention-based modality-collaborative fusion network for emotion recognition",
      "authors": [
        "X Zhang",
        "Y Li"
      ],
      "year": "2023",
      "venue": "A dual attention-based modality-collaborative fusion network for emotion recognition"
    },
    {
      "citation_id": "10",
      "title": "Exploring complementary features in multi-modal speech emotion recognition",
      "authors": [
        "S Wang",
        "Y Ma",
        "Y Ding"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "Knowledge-aware bayesian coattention for multimodal emotion recognition",
      "authors": [
        "Z Zhao",
        "Y Wang",
        "Y Wang"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "12",
      "title": "Mmer: Multimodal multi-task learning for speech emotion recognition",
      "authors": [
        "S Ghosh",
        "U Tyagi",
        "S Ramaneswaran",
        "H Srivastava",
        "D Manocha"
      ],
      "year": "2022",
      "venue": "Mmer: Multimodal multi-task learning for speech emotion recognition",
      "arxiv": "arXiv:2203.16794"
    },
    {
      "citation_id": "13",
      "title": "A transformerbased model with self-distillation for multimodal emotion recognition in conversations",
      "authors": [
        "H Ma",
        "J Wang",
        "H Lin",
        "B Zhang",
        "Y Zhang",
        "B Xu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "14",
      "title": "A low-rank matching attention based cross-modal feature fusion method for conversational emotion recognition",
      "authors": [
        "Y Shou",
        "H Liu",
        "X Cao",
        "D Meng",
        "B Dong"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "15",
      "title": "Gemo-clap: Gender-attribute-enhanced contrastive language-audio pretraining for accurate speech emotion recognition",
      "authors": [
        "Y Pan",
        "Y Hu",
        "Y Yang",
        "W Fei",
        "J Yao",
        "H Lu",
        "L Ma",
        "J Zhao"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "16",
      "title": "Focus-attention-enhanced crossmodal transformer with metric learning for multimodal speech emotion recognition",
      "authors": [
        "K Kim",
        "N Cho"
      ],
      "year": "2023",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "17",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Y Liu"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "18",
      "title": "Mmrbn: Rule-based network for multimodal emotion recognition",
      "authors": [
        "X Chen"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "Speech emotion recognition with co-attention based multi-level acoustic information",
      "authors": [
        "H Zou",
        "Y Si",
        "C Chen",
        "D Rajan",
        "E Chng"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "20",
      "title": "Wavlm: Large-scale self-supervised pretraining for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "21",
      "title": "Mfdr: Multiple-stage fusion and dynamically refined network for multimodal emotion recognition",
      "authors": [
        "Z Zhao",
        "T Gao",
        "H Wang",
        "B Schuller"
      ],
      "year": "2024",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "22",
      "title": "Empathy intent drives empathy detection",
      "authors": [
        "L Jiang",
        "D Wu",
        "B Mao",
        "Y Li",
        "W Slamu"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "23",
      "title": "Expert gate: Lifelong learning with a network of experts",
      "authors": [
        "R Aljundi",
        "P Chakravarty",
        "T Tuytelaars"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "24",
      "title": "Pram: An end-to-end prototype-based representation alignment model for zeroresource cross-lingual named entity recognition",
      "authors": [
        "Y Huang",
        "W Liu",
        "X Zhang",
        "J Lang",
        "T Gong",
        "C Li"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2023"
    },
    {
      "citation_id": "25",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "26",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "27",
      "title": "Multimodal emotion recognition based on audio and text by using hybrid attention networks",
      "authors": [
        "S Zhang",
        "Y Yang",
        "C Chen",
        "R Liu",
        "X Tao",
        "W Guo",
        "Y Xu",
        "X Zhao"
      ],
      "year": "2023",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "28",
      "title": "Dwformer: Dynamic window transformer for speech emotion recognition",
      "authors": [
        "S Chen",
        "X Xing",
        "W Zhang",
        "W Chen",
        "X Xu"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "29",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "S Mirsamadi",
        "E Barsoum",
        "C Zhang"
      ],
      "year": "2017",
      "venue": "2017 IEEE International conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "30",
      "title": "Key-sparse transformer for multimodal speech emotion recognition",
      "authors": [
        "W Chen",
        "X Xing",
        "X Xu",
        "J Yang",
        "J Pang"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "31",
      "title": "Dst: Deformable speech transformer for emotion recognition",
      "authors": [
        "W Chen",
        "X Xing",
        "X Xu",
        "J Pang",
        "L Du"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "32",
      "title": "Multi-scale temporal transformer for speech emotion recognition",
      "authors": [
        "Z Li",
        "X Xing",
        "Y Fang",
        "W Zhang",
        "H Fan",
        "X Xu"
      ],
      "year": "2024",
      "venue": "Multi-scale temporal transformer for speech emotion recognition",
      "arxiv": "arXiv:2410.00390"
    },
    {
      "citation_id": "33",
      "title": "Mm-dfn: Multimodal dynamic fusion network for emotion recognition in conversations",
      "authors": [
        "D Hu",
        "X Hou",
        "L Wei",
        "L Jiang",
        "Y Mo"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "34",
      "title": "Gatem 2 former: Gated feature selection and expert modeling in multimodal emotion recognition",
      "authors": [
        "W Xu",
        "Z Dong",
        "R Wang",
        "X Xu",
        "Z Zhang"
      ],
      "year": "2025",
      "venue": "ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "35",
      "title": "Sound adversarial audio-visual navigation",
      "authors": [
        "Y Yu",
        "W Huang",
        "F Sun",
        "C Chen",
        "Y Wang",
        "X Liu"
      ],
      "year": "2022",
      "venue": "The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event"
    }
  ]
}