{
  "paper_id": "2407.04963v1",
  "title": "Towards Context-Aware Emotion Recognition Debiasing From A Causal Demystification Perspective Via De-Confounded Training",
  "published": "2024-07-06T05:29:02Z",
  "authors": [
    "Dingkang Yang",
    "Kun Yang",
    "Haopeng Kuang",
    "Zhaoyu Chen",
    "Yuzheng Wang",
    "Lihua Zhang"
  ],
  "keywords": [
    "Human emotion recognition",
    "Context awareness",
    "Bias elimination",
    "Causal intervention",
    "De-confounded training Kosti et al. + CCIM(ours) Similar Context Disapproval Disconnection Disquietment Doubt/Confusion Engagement Sadness Grass",
    "trees",
    "outdoors",
    "etc JC score: 0 JC score: 1 JC score: 0 JC score: 1 JC score: 0.38 JC score: 0.50 JC score: 0.30 JC score: 0.63 JC score: 0.27 JC score: 0.55 JC score: 0.17 JC score: 0.80 JC score: 0.50 JC score: 1 JC score: 0.33 JC score: 1 JC score: 0.67 JC score: 1 JC score: 0 JC score: 0.5"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Understanding emotions from diverse contexts has received widespread attention in computer vision communities. The core philosophy of Context-Aware Emotion Recognition (CAER) is to provide valuable semantic cues for recognizing the emotions of target persons by leveraging rich contextual information. Current approaches invariably focus on designing sophisticated structures to extract perceptually critical representations from contexts. Nevertheless, a long-neglected dilemma is that a severe context bias in existing datasets results in an unbalanced distribution of emotional states among different contexts, causing biased visual representation learning. From a causal demystification perspective, the harmful bias is identified as a confounder that misleads existing models to learn spurious correlations based on likelihood estimation, limiting the models' performance. To address the issue, we embrace causal inference to disentangle the models from the impact of such bias, and formulate the causalities among variables in the CAER task via a customized causal graph. Subsequently, we present a Contextual Causal Intervention Module (CCIM) to de-confound the confounder, which is built upon backdoor adjustment theory to facilitate seeking approximate causal effects during model training. As a plug-and-play component, CCIM can easily integrate with existing approaches and bring significant improvements. Systematic experiments on three datasets demonstrate the effectiveness of our CCIM.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "\"Context is the key to understanding, but it can also be the key to misunderstanding.\"",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "-Jonathan Lockwood Huie",
      "text": "A S an essential element of human experience, emotion significantly influences social interactions and communication  [1] ,  [2] . Accurately identifying human emotions from visually accessible content has become integral to pattern recognition methods  [3] . Due to the promising application prospects in understanding human intentions and expressions, emotion recognition has received widespread attention in various fields, such as assisted driver monitoring  [4] , online education  [5] , and human-machine interaction  [6] . For instance, intelligent vehicle systems can automatically detect drivers' emotional states and provide the necessary alerts to reduce road safety hazards when subjects are distracted  [4] .\n\nConventional emotion recognition in computer vision mainly focuses on subject-centered representation channels, including but not limited to facial expressions  [7] ,  [8] , bodily postures  [9] ,  [10] , skeletal gestures  [11] ,  [12] , and multimodal combinations  [13] ,  [14] ,  [15] ,  [16] . Figure  1 (a) presents an intuitive example of these endogenous factors from facial landmarks and body keypoints, providing valuable multisource cues to recognize the subject's happiness. Despite\n\n• Dingkang Yang, Kun Yang, Haopeng Kuang, Zhaoyu Chen, Yuzheng Wang, and Lihua Zhang are with the Academy for Engineering and Technology, Fudan University, Shanghai 200433, China. (Email: {dkyang20, kunyang20, hpkuang19, zhaoyuchen20, yzwang20, lihuazhang}@fudan.edu.cn). Corresponding author: Lihua Zhang.  the remarkable advancements, their performance suffers the inevitable dilemma in uncontrolled wild scenarios. As shown in Figure  1 (b), the perceptually critical regions of the subject in field-collected data are generally indistinguishable (e.g., ambiguous gestures) due to natural occlusions or recorded viewpoints. In this case, the representation channels from the Most training samples containing vegetated surround contexts have similar positive emotion categories. In this case, the model  [22]  relies on spurious correlations between specific contexts and emotion categories to learn misleading visual representations, causing entirely incorrect predictions. Thanks to the proposed CCIM, the model automatically corrects the prediction errors and gives more accurate results.\n\nsubject would fail to provide meaningful emotional signals.\n\nRecently, several emerging approaches  [17] ,  [18] ,  [19] ,  [20] ,  [21]  have suggested capturing additional emotion semantics from out-of-subject contexts to overcome performance bottlenecks in real-world applications. According to the pioneering work  [22] , contexts are considered to include diverse surrounding factors, such as place attributes, scene concepts, background objects, and the actions of others nearby the subject. Cognitive psychology research  [23]  has demonstrated that different contexts spontaneously affect human emotional states in society and offer complementary affective cues. An interesting illustration is given in Figure  1(c) & (d ). When the situational context is ignored in Figure  1 (c), we can just observe a blurred outline of the subject, and hard to recognize the probable emotion polarity. In contrast, we find the subject interacting with his family or friends in a warm room with a pleasurable atmosphere when the context is visible in Figure  1(d) . Although physical signals are ambiguous, exogenous stimuli from the surrounding context can help us better infer the positive state of the subject that he may feel anticipated, esteemed, or pleased. This promising technology for combining contextual information is called Context-Aware Emotion Recognition (CAER).\n\nCurrent mainstream CAER studies usually follow a common procedure pipeline: (1) extracting endogenous characteristics from the recognized subject's region;  (2)  exploring different context branches and learning emotion-related representations; (3) constructing well-designed fusion mechanisms to integrate these features for downstream emotion label predictions. Despite the considerable improvements achieved by existing methods relying on sophisticated model structures  [19] ,  [20] ,  [24] ,  [25] ,  [26] ,  [27]  or different fusion strategies  [28] ,  [29] ,  [30] ,  [31] , they invariably suffer from a context bias of the datasets. To our best knowledge, it is the first time that this long-neglected problem has been investigated. Reflecting on the process of creating CAER datasets, varied annotators were tasked with labeling each image based on their subjective thoughts of the emotions being experienced by individuals in the images across a range of contexts  [22] . This procedure unavoidably influences the distribution of emotion categories across various contexts Fig.  3 . We present a preliminary toy experiment using the EMOTIC  [22]  and CAER-S  [18]  datasets, focusing on scene categories associated with fear, anger, and happy emotions. The inclusion of more scene categories exhibiting normalized zero-conditional entropy reveals a pronounced presence of the harmful context bias.\n\ndue to annotators' preferences, consequently resulting in the context bias. Figure  2  provides an intuitive demonstration of how understanding such bias confounds emotion predictions. The training data primarily encompasses images showcasing scenes abundant in vegetation, which are associated with positive emotion categories. Conversely, instances of negative emotions within analogous contexts are notably rare. As a consequence, the baseline model  [22]  has the potential to be misguided, acquiring spurious correlations between contextspecific features and label semantics. When confronted with the testing image featuring similar contexts yet containing negative emotion categories, the model inevitably arrives at inaccurate deductions about emotional states.\n\nMore intrigued, we perform a toy experiment on two CAER datasets to further verify the severe context bias through the scene context attributes. This preliminary test focuses on quantitatively investigating how emotions are related to contexts (e.g., scene categories). Concretely speaking, we use the ResNet-152  [32]  pre-trained on the Places365 dataset  [33]  to predict scene categories from images with three common emotion categories (i.e., \"fear\", \"anger\", and \"happy\") across two datasets. The selection process involves identifying the 200 scenes with the highest occurrence rates in each emotion category. Subsequently, the normalized conditional entropy is calculated across positive and negative subsets of a specific emotion  [34] . Given a scene category c, the conditional entropy is computed as\n\nwhere e p and e n denote the positive and negative subsets of emotion e respectively (e.g., \"happy\" and \"non-happy\"). While examining associations between scene contexts and emotion categories in Figure  3 , it becomes evident that more occurrence of scene categories featuring zero conditional entropy likely implies the existence of the notable context bias within the datasets. This is characterized by scenes exclusively appearing either in the positive or negative subsets of emotions. Specifically, 33% and 27% of the scene categories targeting fear on the EMOTIC and CAER-S datasets, respectively, are in the entropy range of [0, 0.2]. Within the EMOTIC dataset  [22] , approximately 40% of angerrelated scene categories exhibit zero conditional entropy, while around 45% of the categories for happy (i.e., happiness) have zero conditional entropy. As a tangible example, scene contexts closely related to celebrations are predominantly present in instances with the happy category, while their presence is virtually absent in the negative emotion categories. These findings substantiate the pronounced context bias within the CAER datasets, resulting in discernible disparities in the distribution of emotion categories across various contexts and imbalanced visual representations.\n\nMotivated by the above analyses, we attempt to embrace causal inference  [35]  to reveal the culprit that poisons the CAER models, rather than focusing on beating the previous approaches. As a groundbreaking scientific paradigm that propels models towards unbiased predictions, the primary hurdle in applying traditional causal inference to the contemporary CAER task lies in effectively depicting genuine causal effects and recognizing task-specific dataset bias. To this purpose, we seek causalities rather than shallow associations to improve bias-plagued models from a causal demystification perspective  [35] . Concretely, we propose a causality-based bias mitigation component that is simple in implementation but powerful in functionality. A tailored structured causal model is first presented to explain the causal procedure of the CAER task. In this case, the harmful context bias in datasets is essentially an unintended confounder that misleads the models to learn the spurious correlation between similar contexts and specific emotion semantics. We decouple the causal dependencies among the input images X, subject features S, context features C, confounder Z, and predictions Y . Essentially differentiating from conventional likelihood estimation P (Y |X), we propose a Contextual Causal Intervention Module (CCIM) to accomplish context de-confounding during model training with a novel dooperation P (Y |do(X)). As a causal intervention philosophy, do(•) operator can effectively prevent the establishment of spurious correlations among variables in the non-causal direction. CCIM is based on the backdoor adjustment theory  [36]  to approximate true causal effects and remove the unfavorable impact of the confounder caused by the context bias. As a plug-and-play, model-agnostic, and lightweight component, CCIM can be easily integrated into existing baselines and bring significant and consistent improvements. Quantitative and qualitative experiments demonstrate the necessity and effectiveness of the proposed CCIM. Our main contributions follow below:\n\n• To our best knowledge, we are the first to disentangle variables in the CAER task from the causal demystification perspective and to deeply investigate deleterious context bias in the datasets. Such bias is essentially an unplanned confounder to mislead CAER models to unconsciously capture spurious correlations and misinterpret context semantics.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "•",
      "text": "We present a Contextual Causal Intervention Module (CCIM) through theoretical derivations based on the backdoor adjustment and the practical implementation based on network parameterization. CCIM can be incorporated into most CAER models to achieve a fair contribution of different contexts to emotion understanding by approximating true causal effects.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "•",
      "text": "Extensive experiments are implemented on three standard CAER datasets. Systematic analyses show the potential of the proposed CCIM to improve existing models and thus enable bias-free predictions. This work significantly extends our preliminary paper  [37]  at the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR2023). We provide improvements in multiple aspects to enhance further the applicability and scalability of our work. Specifically, (i) we introduce the emotional state model based on continuous dimensions to assess and show the context bias dilemma. The continuous emotion representations contain three subspaces, Valence, Arousal, and Dominance, contributing to a complementary evaluation of CCIM's gain in complex emotion-binding situations; (ii) we present the Jaccard coefficient scores to more abundantly explain the different performances and roles of our causal intervention in diverse context instances. These criteria give intuitive explanations for measuring the differences between the causal intervention process and the traditional likelihood estimation procedure; (iii) considering that a subject may have multiple emotion intentions in multilabel emotion recognition, label-based and sample-based evaluation rules are proposed to measure existing methods and provide additional prediction results more rationally; (iv) we combine CCIM with more state-of-the-art (SOTA) approaches. The noteworthy performance improvements of CCIM on model structures with distinct design philosophies and fusion mechanisms enhance the persuasiveness and usefulness of our work; (v) more details and discussions on the motivation, algorithms, and implementation are provided to reinforce our insights. Furthermore, more experiments are conducted to highlight the effectiveness of CCIM, including quantitative, qualitative, ablative, and customized analyses.\n\nThe rest of this paper is organized as follows. In Section 2, we discuss the background among related techniques in prior works, including uni/multimodal emotion recognition, context-aware emotion recognition, and causal demystification. The detailed methodology is provided in Section 3, which consists of the causal graph construction, the causal intervention interpretation, and the parameterized implementation. Section 4 introduces used standard datasets, representative models, and evaluation metrics. The systematic experiments and analyses are described in Section 5. Finally, our conclusions are drawn in Section 6.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Uni/Multimodal Emotion Recognition",
      "text": "Emotion is an essential medium for humans to communicate their intentions and maintain social relationships with the external world  [23] . As an important component in the affective computing fields, emotion recognition technology has received widespread attention and exploration over the past decade  [1] ,  [2] ,  [3] ,  [4] ,  [5] . As a complex psychological activity, emotion descriptions are generally summarized in two directions: discrete categories and continuous dimensions. The basic discrete emotions  [38]  are categorized as Happiness, Fear, Surprise, Sadness, Disgust, and Anger. Several subsequent taxonomies  [4] ,  [39]  are combinations or refinements of these six typical emotions. Continuous dimensions usually utilize numerical representations of sequential descriptors to granularly depict different emotion subspaces. The most influential way is the VAD emotion model  [40] , which decouples emotional states into three dimensions regarding Valence, Arousal, and Dominance. Early works focused on unimodal recognition patterns, which were dominated by facial expression analysis. The face-oriented approaches  [7] ,  [8] ,  [41]  usually attend to geometric or appearance characteristics for extracting informative representations that reflect emotions. Another research direction  [9] ,  [10] ,  [11] ,  [12] ,  [42]  is to explore the emotional semantics embedded in body language through gestural or postural information. More recently, multimodal emotion recognition aims to aggregate heterogeneous modalities from different channels to jointly learn emotion-related representations  [13] ,  [14] ,  [15] ,  [16] ,  [43] ,  [44] ,  [45] . Despite impressive advances, previous efforts have generally been restricted to laboratory-controlled or well-designed settings. The performance of subject-centered approaches may deteriorate when modalities are missing or signals are ambiguous in uncontrolled real-world scenarios.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Context-Aware Emotion Recognition",
      "text": "Context-Aware Emotion Recognition (CAER)  [17]  provides new possibilities for robust affective interactions in uncontrolled wilderness environments. As an emerging task, CAER not only incorporates the subject-centered learning paradigm, but also considers substantial emotion cues from out-ofsubject contexts. Existing approaches  [18] ,  [19] ,  [20] ,  [22] ,  [24] ,  [25] ,  [26] ,  [27] ,  [29] ,  [30] ,  [31] ,  [46]  typically extract multiple representations from subject and context sources to perform feature fusion and subsequent label prediction. Specifically, Kosti et al.  [17]  first treat the complete image as a global context support and implement a two-stream Convolutional Neural Network (CNN) model with low-rank filtering properties. Zhang et al.  [19]  utilize the region proposal network  [47]  to select different elements in the background context to construct an affective graph and infer emotional states. Then, CAGBN  [24]  is proposed to fuse global and local information in the images with the view of a sequence generation task. Besides using multiple modalities from the subject, EmotiCon  [20]  introduces the scene and sociodynamic contexts following Frege's Principle. After that, SIB-Net  [25]  is presented to capture the relation of sequence and interaction among face, body, and scene context. In the recent methodology, Yang et al.  [30]  discover the fine-grained relationship between agents and objects to mitigate the uncertainty of contextual semantics in expressing emotions from a sociological perspective. While previous approaches have achieved promising improvements by seeking rich complementary factors from diverse contexts, they have all ignored the intrinsic dilemma of performance bottlenecks caused by the context bias of the datasets. Instead of focusing on beating the latest SOTA, we step back to disentangle the harmful bias from a novel causal perspective and bring consistent gains for existing models via the proposed CCIM.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Causal Demystification",
      "text": "Causal demystification is an essential application of causal inference, which aims to analyze the intrinsic dynamics and possible outcomes of events when the corresponding conditional variables are changed  [35] . By adjusting different treatments and interventions, this theory has been widely applied and achieved considerable exploration in various fields, such as economics  [48]  and developmental psychology  [49] . Maintaining the principle of universal applicability, the pursuit of causal demystification bifurcates into two fundamental avenues: the structured causal model  [50]  and the potential outcome framework  [51] . These dual methodologies serve as illuminative instruments, delving into the underpinnings of causalities rather than remaining confined to the realm of superficial variable associations. Some early works attempt to provide reliable explanations for the models by relying on causal theories  [52] ,  [53] . Benefiting from learning-based technologies  [54] ,  [55] ,  [56] ,  [57] ,  [58] ,  [59] ,  [60] ,  [61] ,  [62] , modern deep learning tasks  [63] ,  [64] ,  [65]  have begun to embrace causal tools for unbiased estimation solutions, including computer vision  [66] ,  [67] ,  [68] ,  [69] ,  [70]  and natural language processing  [71] ,  [72] ,  [73] . Unlike the task-specific causal paradigm described above, this is the first investigation of the confounding effect through causal inference in the CAER task while exploiting causal intervention to interpret and address the confounding bias from contexts.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Causal Perspective At Caer Task",
      "text": "Before starting, we present a customized causal graph to disentangle the general CAER process. In particular, we adhere to the same graphical notation within the framework of the structured causal model  [50] , attributing this choice to its inherent quality of intuitive lucidity and facilitative interpretability. The causal graph is formally a directed acyclic graph G = {N , E} that can be utilized to achieve causal estimates across data. The nodes N denote variables and the links E denote direct causal effects. As illustrated in Figure  4 , the CAER procedure contains five different variables, which are input images X, subject features S, context features C, confounder Z, and predictions Y . Note that the proposed causal graph is well adapted to a wide range of CAER models because it is highly summarized and not restricted by implementation details. The comprehensive exposition regarding the underlying architecture of these causal interconnections is furnished hereinafter. Link Z → X. In uncontrolled environments, distinct subjects are recorded by the publishers of the datasets in diverse context scenarios to produce image samples X. When assessing emotional states the subjects evinced, the annotators generally provide possible emotion annotations with biased and subjective awareness  [18] ,  [22] . Despite adopting several qualifications and control measures for annotators, bounded human observations of the natural world inevitably lead to biased annotation performance  [74] . From the intuitive example in Figure  2 , subjects are habitually ascribed positive emotion categories within contexts rich in vegetative cover, with such assignments often transpiring without overt cognitive deliberation. Another nonnegligible reason is that the data nature results in the unbalanced distribution of the emotional states in the real world  [75] . Fundamentally, the collection of positive emotions in contexts characterized by comfort is markedly less challenging than in those marked by negativity. The context bias induced by the above situations is identified as a harmful confounder Z, which establishes spurious associations between similar context representations and specific emotion semantics. To be precise, the confounder Z directly determines the recorded biased content in the input images X, i.e., Z → X.\n\nLink Z → C → Y . C implies that the total context features come from the context representation encoders. Since the node variable C is a generalized descriptor, its specific implementation depends on the definition and modeling of contexts by different methods. For instance, context features may derive from the background region after hiding the subject's face  [18]  or from the aggregation of scene and socio-dynamic context information  [20] . The causal path Z → C represents the deleterious confounder Z misleading the models to capture the contextual semantics from C, which has unreliable emotion correlations. In this situation, the unpure C would further impact the predictions of the emotion labels, and its effect would propagate along the link C → Y . A noteworthy point is that Z potentially contains prior knowledge from the training data to assist the models in estimating appropriately when the subject's characteristics are indistinguishable. Nevertheless, the confounding attributes in C largely mislead the models to learn spurious \"context-emotion\" mapping during training, causing biased predictions with performance bottlenecks.\n\nS represents the total subject features obtained by subject representation encoders. Similarly to C, the detailed implementation of S is also not limited to a specific method. That is, subject features could be extracted from appearance characteristics of the body region  [46]  or the cropped face position  [18] . In the CAER causal graph, the desperately desired effects of input images X on predictions Y follow two causal links: X → C → Y and X → S → Y . These two causal links represent the CAER models' pure estimation of Y based on the total context representations C and subject representations S learned from X. In practical implementations, C and S are generally integrated to serve the final emotion predictions in a joint manner, e.g., feature concatenation integration  [20] .\n\nAccording to the causal theory  [35] , the confounder Z is the common cause of the input images X and corresponding predictions Y . The positive effects from context and subject features are reflected upon causal paths X → C → Y and X → S → Y , respectively, which provide beneficial semantic information for recognition purposes. Unfortunately, the confounder Z causes the negative effect of misleading the models to focus on spurious correlations instead of pure causal relationships. This deleterious effect is propagated through a backdoor path X ← Z → C → Y built with Z as the mediator, which we aim to prevent.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Causal Intervention Via Backdoor Adjustment",
      "text": "We have now clarified the causal relationships among the CAER variables based on the aforementioned explanations. As shown in Figure  4 (a), the predictions of emotion probabilities from existing methods follow the likelihood estimation P (Y |X). This process is formulated by the Bayes rule:\n\nwhere f s (•) and f c (•) are two generalized encoding functions that obtain the total S and C, respectively. The backdoor confounder z ∈ Z introduces the observational context bias through the conditional probability P (z|X). Theoretically, our goal is to remove confounding interference from Z and force the models to achieve unbiased predictions by relying only on valuable effects from X to Y , i.e., X → C/S → Y . The intuitive insight is implementing an intervention on X to enable the models to treat all context semantics fairly during training without favoring any of them. This philosophy can be viewed as conducting a randomized controlled experiment by collecting images of subjects with any emotion in any context. Nevertheless, this intervention is impractical since different subjects are situated in countless context scenarios in the real world, and enumerating them is difficult. To address this, we perform the causal intervention P (Y |do(X)) via the backdoor adjustment principle  [35]  to interrupt the unfavorable effect propagated along the backdoor path between X and Y , where do(•) operator is an effective approximation for the imaginative intervention  [36] . According to the backdoor adjustment, we stratify Z to measure the causal effect. This operation means partitioning the contexts into homogeneous groups with respect to Z and then estimating the average causal effect by computing a weighted average based on the proportion of samples containing different context prototypes in the training data. Thus, the models would approximate the causal effects through the intervention P (Y |do(X)) rather than capture spurious correlations through the likelihood P (Y |X). As shown in Figure  4 (b), the backdoor path would be invalid because the link from Z to X is cut off. After implementing the intervention in the new graph, Equation (  1 ) is represented as follows by the Bayes rule:\n\n(\n\nSince z is no longer influenced by X, the intervention deliberately encourages X to fairly account for the effects of each z when predicting Y . P (z) is the priori probability that depicts the proportion of each z in the whole.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Context-Deconfounded Training With Ccim",
      "text": "To achieve the theoretical intervention in Equation (  2 ) at the implementation level, we present a Contextual Causal Intervention Module (CCIM) for context-deconfounded training of CAER models. From Figure  5 , CCIM is incorporated into the general pipeline of existing methods in a plug-and-play and model-agnostic manner. The output of CCIM is used in the task-specific classifier (i.e., neurons with the number of emotion categories) to perform the final predictions. The implementation of CCIM is described as follows.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Confounder Dictionary",
      "text": "Since collecting all contexts in the real world is impossible and there is a lack of supervised contextual information in the training data, we approximate a stratified confounder dictionary Z = [z 1 , z 2 , . . . , z N ] over the whole training samples using an unsupervised approach. N is a size hyperparameter that represents the possible confounder number. Each z i ∈ R d stands for a kind of context prototype in a stratified homogeneous group. As Figure  6  shows, we first mask the recognized subjects based on their priori bounding boxes to produce a context image set I. The masking operation aims to preserve only context-dependent regions to prevent subject-based attributes from impacting the construction of the confounder dictionary. We discuss its necessity in the experimental part. Concretely, for a given input image x, its corresponding context image I x is expressed as follows:\n\nwhere bbox subject means the bounding box of the recognized subject. Then, we employ a candidate pre-trained network φ(•) to generate the context feature set\n\nfrom the context image set I, where N m is the number of training samples. For flexibly obtaining context prototypes, we utilize unsupervised K-Means++ to learn Z so that each z i represents a form of context cluster. Each z i is set to the average feature from each cluster that aggregates the homogeneous confounding characteristics, which is expressed as follows:\n\nwhere N i is the number of context features in the i-th cluster.\n\nNote that there is no specific requirement for the choice of the clustering algorithm here, which we give justification in the subsequent ablation study.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Parameterization Of The Proposed Ccim",
      "text": "According to Equation (2), the computation for P (Y |do(X)) is very expensive since we need to forward process each pair of X and z multiple times. To efficiently implement the causal intervention, we apply the Normalized Weighted Geometric Mean (NWGM)  [76]  to allow approximating the above expectation at the feature level:\n\nHere, we instantiate a parameterized network to efficiently approximate the conditional probability in Equation (  5 ):\n\nwhere\n\n, and ϕ(•) is a fusion strategy (e.g., concatenation) that integrates s and c into the joint representation h. The above approximation implies that the output expectation for all possible confounders z can be calculated simply by feed-forward propagation with the expectation vector E z [g(z)] as the input. Specifically,\n\n] is approximated as a weighted integration of all context prototypes referencing the corresponding proportion:\n\nwhere P (z i ) = Ni Nm and λ i is a weighted attention score to measure the importance of the corresponding z i . In practice, the integrated feature h from one sample queries each z i in the confounder dictionary Z ∈ R N ×d to obtain the samplespecific attention set {λ i } N i=1 . The intuitive insight is that each sample is impacted to varying degrees of distinct z i . We provide two implementation patterns for λ i . The first one is the dot product attention:\n\nand the second one is the additive attention:\n\nAdditive :\n\nwhere W t ∈ R dn×1 , W q ∈ R dn×d h , and W k ∈ R dn×d are learnable mapping weights.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Datasets And Evaluation Metrics",
      "text": "Our experiments are conducted on three standard CAER datasets, including EMOTIC  [22] , CAER-S  [18] , and Group-Walk  [20]  datasets. EMOTIC is the first large-scale CAER benchmark that contains 23,571 images of 34,320 annotated subjects. The majority of the images come from unconstrained environments to provide rich data resources on different subjects in diverse context scenarios. The bounding box coordinates of each recognized subject are provided in the annotation file to give the location information. EMOTIC supports two types of emotion descriptors: 26 discrete emotion categories for multilabel classification and 3 continuous emotion dimensions for regression. The discrete categories consist of \"Affection, Anger, Anticipation, Aversion, Confidence, Disapproval, Disconnection, Disquietment, Doubt/Confusion, Embarrassment, Engagement, Esteem, Excitement, Fatigue, Fear, Happiness, Pain, Peace, Pleasure, Sadness, Sensitivity, Suffering, Surprise, Sympathy, and Yearning\". The continuous dimensions are annotated following the mainstream VAD emotional state model  [40] , which consists of \"Valence, Arousal, and Dominance\". The values of each dimension are constrained to range from 1 to 10 to express different emotion intensities. We adopt the standard dataset partitioning for a fair comparison, i.e., 70% data in the training set, 10% data in the validation set, and 20% data in the testing set. CAER-S contains 70k static images captured from video clips. These images record different subjects in indoor and outdoor scenarios from 79 TV shows to include diverse contextual elements. CAER-S supports multi-class classification of emotion labels, and its annotated emotion categories include \"Anger, Disgust, Fear, Happy, Sad, Surprise, and Neutral\". The training, validation, and testing samples are randomly partitioned in a ratio of 7:1:2 during utilization. GroupWalk consists of 45 manually collected videos from real-world environments. The annotated subjects have visible faces and gaits in all videos. A characteristic of GroupWalk is containing extensive agent flows and interactions for understanding the subjects' affective effluence in the group effect. The annotations provide discrete emotion categories to support implementing multi-label classification over \"Angry, Happy, Neutral, and Sad\". The dataset partitioning is categorized as 85% training set and 15% testing set. Evaluation Metrics. We utilize the Average Precision (AP) to evaluate the discrete results on the EMOTIC and GroupWalk datasets. The Average Absolute Error (AAE) is employed to evaluate the testing results of the continuous dimensions on the EMOTIC. In addition, we follow  [46]  to deeply evaluate the performance of multi-label classification on the EMOTIC using the label-based metrics (C-F1) and sample-based metrics (O-F1). For the CAER-S, the standard classification accuracy is used for evaluation.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Implementation Details",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Model Zoo",
      "text": "While current studies offer promising advancements, most efforts are not open-source, and the design philosophies in several approaches are similar. In this situation, we choose five representative approaches that include classical and stateof-the-art (SOTA) works. The selected approaches have entirely different network structures and modeling paradigms to support an exhaustive evaluation of the effectiveness and applicability of the proposed CCIM. A brief introduction to these approaches is given below. EMOT-Net  [22]  is a classical Convolutional Neural Network (CNN) model. The model has two different branches, one for extracting physical features from the recognized subject region and the other for extracting contextual semantics from the global background region.   [19]  extracts subject body information using standard CNN network. Moreover, the context-related elements extracted via the region proposal network  [47]  are considered as nodes and infer the emotional states via the Graph Neural Network (GNN). CAER-Net  [18]  consists of two CNN encoding networks and an adaptive fusion module. The two encoders extract information from the subject's face and emotional cues from the background context after masking the face. CD-Net  [31]  first obtains intermediate features for face, body, and context regions via ResNet  [32] . Then, a tubal transformer is designed to facilitate fine-grained interactions and hierarchical fusion across multi-scale features.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Gnn-Cnn",
      "text": "EmotiCon  [20]  is a multi-stream model with three contextdependent branches. The subject-centered branch uses facial and gait keypoints to learn human dynamics. The out-ofsubject context branches utilize visual attention and depth maps to capture specific emotion semantics. We re-train EMOT-Net according to the official codebase. Meanwhile, we reproduce the results of other SOTA models (i.e., GNN-CNN, CAER-Net, CD-Net, and EmotiCon) based on the details provided in the original reports.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Confounder Construction",
      "text": "One of the vital steps in the confounder construction is to locate the recognized subject and remove the influence from the subject's information. To this end, we use the pre-trained Faster R-CNN  [47]  to detect the bounding boxes of the recognized subject for each training sample on both CAER-S and GroupWalk. EMOTIC provides annotated information about the bounding box for utilization. We then utilize the bounding boxes to mask the target subjects according to Equation (3) for producing the context image set I. After that, we employ the ResNet-152  [32]  pre-trained on the Places365  [33]  dataset to extract the context feature set M . The rich scene context resources in the Places365 dataset facilitate distilling informative context semantics from the pre-training backbone and learning better context prototypes in the subsequent clustering process. The final pooling layer is applied to obtain each context feature m for retaining the refined feature semantics. The hidden feature dimension d is set to 2048. The default size N (i.e., the number of clusters) is set to 256, 128, and 256 in the EMOTIC, CAER-S, and GroupWalk datasets, respectively.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Training Details",
      "text": "All reproduced models and our CCIM are implemented via the PyTorch toolbox  [77] . The computational resources utilize Nvidia Tesla V100 GPUs. Note that CCIM as a plug-andplay component does not affect the training protocols of the original methods. For this reason, we adopt precisely the identical training details provided by the original models to ensure a fair comparison. To implement our CCIM, the hidden dimensions d m and d n are set to 128 and 256, respectively. The output dimension d h of the joint feature h in the different approaches is 256 (EMOT-Net), 1024 (GNN-CNN), 128 (CAER-Net), 512 (CD-Net), and 78 (EmotiCon).",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Comparison With State-Of-The-Art Methods",
      "text": "To comprehensively evaluate the performance of the proposed CCIM, we compare the CCIM-based models with existing SOTA methods, including HLCR  [21] , TEKG  [27] , RRLA  [46] , VRD  [29] , SIB-Net  [25] , MCA  [30] , CAGBN  [24] , and GRERN  [26] . The dot product attention of Equation (  8 ) is used as the default implementation. on of 8.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Quantitative Results On The Emotic",
      "text": "As a holistic benchmark in the CAER field, the EMOTIC dataset features assessment patterns on 26 discrete categories and 3 continuous dimensions (VAD) of emotions. To this end, we provide systematic experiments in both directions. For discrete categories, we have the following core observations. (i) Table  1  first presents the mean average precision (mAP) results across all emotion categories to support the macroscopic evaluation. CCIM significantly brings consistent performance gains to existing models and achieves new SOTAs. Concretely, the CCIM-based EMOT-Net, CAER-Net, GNN-CNN, CD-Net, and EmotiCon improve the mAP scores by 2.95%, 2.66%, 3.56%, 3.42%, and 3.85%, respectively, outperforming the vanilla methods by large margins. (ii) HLCR and TEKG attempt to incorporate external knowledge and enhance emotion perception through linguistic semantic descriptors extracted from images. Despite promising solutions, they suffer from severe performance bottlenecks since linguistic information produced from confounded contexts is essentially an implicit amplification of the adverse effects of the confounder. In comparison, the baselines (e.g., EMOT-Net, GNN-CNN, and CD-Net) improved by CCIM exhibit competitive or better results. Based on this finding, the key to solving the vision-driven CAER task is disentangling the underlying causal dependencies rather than fancy resorting to language community development. (iii) Compared to current SOTA works (e.g., SIB-Net and MCA) with complex module stacking and massive parameters, EmotiCon achieves the best performance with the mAP score of 39.13% only through the lightweight CCIM. This observation further demonstrates the effectiveness of our component.\n\nMicroscopically, we show the average precision (AP) scores for the CCIM-based models and their vanilla counterparts on each emotion category in Table  2  to provide more in-depth analyses. (i) CCIM consistently improves performance in all methods for most emotion categories. For instance, CCIM yields an average gain of 8.25% on the AP scores across the five models for the \"Happiness\" category reflecting positivity. Meanwhile, CCIM provides an average gain of 4.04% on the AP scores across the five models for the \"Pain\" category reflecting negativity. These results imply that our component can effectively mitigate the performance bottleneck caused by the uneven distribution of emotion semantics of different polarities in context-based visual scenarios. (ii) Moreover, CCIM remarkably improves the AP scores of some categories heavily persecuted by the confounder. For example, These CCIM-based methods boosted the AP scores by 29%∼37% and 14%∼29% on the \"Anticipation\" and \"Sympathy\" categories, respectively, significantly superior to their original models. (iii) Due to adverse effects from the context bias, the performance of most models is usually poor on infrequent categories, such as \"Aversion\" (AP scores of about 3%∼11%) and \"Embarrassment\" (AP scores of about 1%∼10%). Thanks to the proposed CCIM, the AP scores in these two categories are achieved at about 12%∼19% and 5%∼16%.\n\nEvaluating the EMOTIC dataset from the multi-label learning (MLL) perspective is an emerging paradigm due to the intrinsic connections among multiple emotion labels. Following the validation metrics  [24]  of MLL, we adopt the Label-based F1 (C-F1) and example-based F1 (O-F1) scores in Table  3  to measure the performance of the accessible methods and the CCIM-based models, where the average is taken over all classes and all testing examples, respectively. Some key findings are as follows. (i) Deep learning-driven works usually obtain better results than machine learningbased efforts (i.e., ML-KNN and Label Powerset), suggesting that traditional efforts fail to capture profound dependencies across emotion categories. (ii) CCIM yields considerable gains for most reproduced models. For instance, the O-F1 scores increased by an average of 2.92% across the five models. (iii) Despite the competitive results achieved by CAGBN and RRLA under the MLL evaluation scheme through the multi-label dependency modeling, they either rely on an incremental sequence generation  [24]  or require additional topological guidance  [46] , causing sub-optimal solutions. In comparison, the CCIM-based EmotiCon achieves comparable or better performance with C-F1 and O-F1 scores of 15.01% and 48.18%, demonstrating the superiority of our component. Table  4  reports quantitative results on the continuous dimensions of VAD for emotional states using the AAE scores (the lower, the better). For the VAD model, Valence measures the degree of negativity or positivity of a subject's emotion. Arousal measures a subject's agitation level, usually from inactive to ready for action. Dominance measures a subject's control level over the situation, usually from uncontrolled to totally dominant. We only evaluate EMOT-Net and GNN-CNN due to other implemented models that do not support the regression task. (i) Overall, CCIM consistently reduces the prediction errors on the three emotion dimensions due to producing lower AAE results, implying the applicability and effectiveness of our component over different emotional state spaces. (ii) An interesting phenomenon is that improvements on Arousal and Dominance are significantly better than those on Valence. A plausible explanation is that subjects generally show pronounced differences in agitation and control levels across distinct contexts, which are more vulnerable to the poison of context bias. For instance, inactive subjects with lower numerical values of Arousal are usually located in similar indoor scenarios. Conversely, subjects with higher Arousal values are usually located outdoors in diverse venues. In this case, spurious correlations between similar contexts and specific emotion polarity are more likely to be estab-",
      "page_start": 8,
      "page_end": 10
    },
    {
      "section_name": "Methods Accuracy (%)",
      "text": "Fine-tuned AlexNet  [80]  61.73 * Fine-tuned VGGNet  [81]  64.85 * Fine-tuned ResNet  [32]  68.46 * SIB-Net  [25]  74.56 * MCA  [30]  79.57 * GRERN  [26]  81.31 * RRLA  [46]  84.82 * VRD  [29]  90. lished, causing harmful performance bottlenecks. Fortunately, CCIM reasonably mitigates the detrimental effects and helps existing models achieve better overall performance (mean scores of 0.0559 on EMOT-Net and 0.0538 on GNN-CNN).",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Quantitative Results On The Caer-S",
      "text": "Table  5  provides the overall accuracy of different methods and CCIM-based models on the CAER-S dataset. Some key observations and analyses are as follows. (i) Fine-tuned conventional models (i.e., AlexNet, VGGNet, and ResNet) typically have restricted performance upper bounds since the results usually do not exceed 70%, implying a failure to capture adequate emotion semantics. (ii) The proposed CCIM comprehensively enhances the performance of EMOT-Net, CAER-Net, GNN-CNN, and CD-Net. Compared to the vanilla models, their CCIM-based versions are improved by 1.31%, 1.34%, 1.45%, and 1.28%, respectively. The potential deduction is that CCIM forces each context prototype extracted from TV show scenarios to reasonably incorporate into the emotion predictions and improve the overall accuracy. (iii) More importantly, the CCIM-based EmotiCon obtains the most significant gain of 2.52% while beating all existing methods with an accuracy of 91.17%. (iv) Furthermore, we show the classification accuracy of each emotion category from different CCIM-based models on the CAER-S dataset in Figure  7 . Overall, all models obtain considerable performance gains in most categories.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Quantitative Results On The Groupwalk",
      "text": "As shown in Table  6 , our CCIM effectively improves the performance of EMOT-Net, CAER-Net, GNN-CNN, CD-Net, and EmotiCon on the GroupWalk dataset for most categories. The mAP scores for these models are increased by 2.41%, 2.25%, 2.99%, 2.72%, and 3.73%, respectively. A noteworthy observation is that the \"Neutral\" category exhibits slight deterioration across different models. The potential reason  may be that samples with neutral emotions are more dispersed across contexts than samples with other emotions, leading to insufficient confounding effects. Consequently, our component may experience slight over-intervention when decoupling the spurious \"context-emotion\" mapping. However, the minor sacrifice is tolerable compared to the overall superiority of the proposed CCIM.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Discussion From The Causal Perspective",
      "text": "Besides the above observations and analysis, we have two critical insights from the causal perspective across all datasets.\n\n(i) The performance improvements of all methods on the EMOTIC and GroupWalk datasets are more significant than on the CAER-S dataset. Taking the mAP and accuracy metrics as examples, the average gains across models on EMOTIC, CAER-S, and GroupWalk are 3.29%, 1.59%, and 2.82%, respectively. Combined with Figure  3 , we realize that EMOTIC suffers a more severe context bias than the CAER-S dataset. These findings exhibit an encouraging conclusion: the more the data bias, the more the proposed plug-in component improves the performance of the vanilla models. There are two rational explanations for this phenomenon:\n\n(1) Bias-heavy datasets typically have large numbers of contextual representations that potentially induce bias effects. Specifically, the samples on the EMOTIC and GroupWalk datasets derive from uncontrolled real-world scenarios that contain informative context semantics, such as diverse scene information and agent interaction dynamics. As a result, our component could learn more discriminative context prototypes to serve context-deconfounded training better. Further, causal intervention can more effectively eliminate spurious correlations caused by the adequately extracted confounder and provide sufficient gains.  (2)  The implementation of backdoor adjustment  [35]  for the causal intervention relies on stratifying the contexts belonging to homogeneous groups by the clustering algorithm. The more severe context bias in the datasets has the bias distribution across more data samples with heterogeneous contexts. In this case, our design can better approximate the theoretical intervention by estimating the average causal effect in stratified contexts that leads to better debiased results. (ii) Another finding is that CCIM provides richer performance gains for fine-grained methods that capture context semantics. For instance, EmotiCon (average gain of 3.37% across datasets) with two out-ofsubject context modeling branches significantly outperforms EMOT-Net (average gain of 2.22% across datasets) with only one background context stream on all three datasets. We argue that the essence of fine-grained modeling is the potential context stratification within the sample from the perspective of backdoor adjustment. Fortunately, CCIM can better refine this stratification effect.  patterns of modeling context semantics; (iii) there are similar observations and results from other methods in practice.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Ablation Studies",
      "text": "",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Rationality Of Confounder Dictionary",
      "text": "Evaluating the confounder dictionary plays an important role in the causal intervention. (i) We first design a random dictionary with identical dimensions to replace the customized dictionary Z. The random dictionary represents that the confounder dictionary is initialized by random parameterization instead of carefully extracted average context features. We observe that the random dictionary significantly compromises the performance gain of our CCIM. Specifically, the randomized versions of CCIM-based EMOT-Net and EmotiCon decrease their performance by an average of 3.56% and 3.84% across the three datasets, respectively. This observation confirms the effectiveness of our context prototypes and the necessity of the causal implementation.\n\n(ii) Furthermore, we answer what context prototypes are reasonable. To this end, the ResNet-152 pre-trained on the ImageNet dataset  [84]  is employed to extract context features for replacing the default settings regarding the pretraining on the Places-365 dataset. The results are interesting: although the ImageNet-based versions also improve on the vanilla models, they fall short compared to the Places-365based results. The decreased across models suggest that context prototypes based on scene semantics are more conducive to approximating the confounder than those based on object semantics. It is common sense as scene contexts usually include object contexts, e.g., in Figure  2 , \"grass\" is the child of the confounder \"vegetated scenes\".",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Robustness Of Pre-Trained Backbones",
      "text": "Here, we provide alternative investigations on the pretrained backbone of extracting the context feature set M . The alternatives to the default ResNet-152 are the ResNet-50 and VGG-16 to evaluate the impact on the performance of the same and different families of backbones, respectively. The ablation results from both methods imply that the gains brought by CCIM gradually increase as more advanced pretrained backbones are introduced. This phenomenon shows that improvements indeed come from CCIM itself rather than depending on a well-chosen pre-trained backbone φ(•).",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Effectiveness Of Approximate Expectation",
      "text": "The expectation E z [g(z)] is the centerpiece for achieving effective causal approximation since it incorporates the extent to which potential confounders z i representing distinct context prototypes impact each sample. We perform systematic explorations of different compositions in E z [g(z)]. (i) First, our proposed additive attention in Equation (  9 ) is utilized to substitute the default dot product attention for producing the dynamic weight λ i . The competitive or comparable gains",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Effect Of Confounder Size",
      "text": "The size N of the confounder dictionary reflects the overall confounding degree on a dataset. We set N to 64, 128, 256, 512, and 1024 on all datasets to measure the impact of N on the performance. As shown in Figure  8 , when the sizes on the EMOTIC, CAER-S, and GroupWalk datasets are set to 256, 128, and 256, CCIM-based EMOT-Net and EmotiCon achieve the best gains, justifying the default implementation. We conjecture that the smaller confounder size required on the CAER-S dataset is because the samples have limited context scenes and elements as the data are collected in fixed TV shows. As a result, the vanilla models suffer from the context bias less severely on the CAER-S dataset than the other two datasets. The above observation suggests that selecting the suitable size N for a dataset containing varying degrees of the harmful bias can well help the models perform de-confounded training.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Necessity Of Masking Strategy",
      "text": "The masking strategy aims to mask the recognized subject to learn prototype representations using pure background contexts. The design intuition expects the prototype learning to pay more attention to the background regions that contain more contextual interpretations in a small portion of samples where the recognized subjects occupy a large region. The gain degradation across all datasets is observed when the target subject regions are not masked. The above observation suggests that the masking strategy strengthens our debiasing component, consistently providing valuable improvements for different baselines across all real-world datasets.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Effect Of Clustering Algorithm",
      "text": "To compute context prototypes, we use the K-Means++ to learn the confounder dictionary Z. Here, we provide two alternatives (i.e., Dichotomous K-Means and K-Medoids ) to replace the K-Means++ to evaluate the effect on performance. We observe that the performance difference of the models across all three clustering algorithms is less than 0.13%, i.e., the choice of clustering algorithm barely affects the performance, demonstrating that the proposed CCIM is robust to the clustering process.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Effect Of Object Detectors",
      "text": "We also investigate the effect of different object detectors on the confounder dictionary on CAER-S and GroupWalk datasets, which require tagging out the recognized subjects. Concretely, the default Faster R-CNN in our pipeline is replaced with R-FCN  [82]  and SSD  [83]  detectors to perform experiments. From the results, Faster R-CNN performs better in most cases, while SSD benefits from the multi-scale feature prediction pattern slightly better than R-FCN. Overall, the effect of different object detectors on the confounder dictionary construction is slight since the gain variation errors for all metrics on both datasets are less than 0.2%.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Qualitative Evaluation",
      "text": "",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Difference Between Likelihood And Intervention",
      "text": "Figure  9  visualizes the distributions of context features learned by EMO-Net and EmotiCon on the testing samples to understand the differences between the models approximate traditional likelihood P (Y |X) and causal intervention P (Y |do(X)). We utilize the GroupWalk dataset due to the modest emotion categories that provide intuitive distinctions visually. Specifically, these sample images contain four types of realistic contexts, i.e., park, market, hospital, and station. In vanilla models, the context features with the same emotion categories are generally compactly distributed within similar context clusters, e.g., the context features of the hospital with the sad category are closer. This phenomenon implies that context bias causes the models to rely on context-specific spurious correlations for predicting emotions lopsidedly. Conversely, in the CCIM-based models, context-specific features form clusters containing diverse emotion categories.\n\nThe distributional change confirms that the causal intervention facilitates the models to fairly integrate each context prototype semantics when predicting emotions, eliminating the detrimental effect of the confounder.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Qualitative Analysis On The Emotic&Groupwalk",
      "text": "As the challenging multi-label classification on the EMOTIC and GroupWalk, we introduce Jaccard Coefficient (JC) scores to more abundantly explain the different performances and roles of our CCIM in different context instances. Despite  (iii) We observe similar phenomena on the GroupWalk dataset, i.e., the proposed CCIM consistently improves the performance of the vanilla model in testing samples with diverse contexts. For instance, in Figure  10 (g), CCIM disentangles the spurious correlation between the context (\"hospital entrance\") and the emotion semantics (\"sad\"), yielding correct results aligned with the ground truth.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Qualitative Analysis On The Caer-S",
      "text": "For the typical multi-class classification in the CAER-S dataset, we randomly show five testing samples with different ground truth emotion categories in Figure  11 . Inspired by attention-based efforts  [25] ,  [30] ,  [46] , we use heat maps to observe differences in network models for regions of interest before and after the causal intervention. Here, EMOT-Net is employed instead of EmotiCon to reflect the diversity of evaluation. In heat maps, the red regions imply that the model focuses on during the semantic learning process. The important findings are summarized below.\n\n(i) The vanilla method usually yields undesirable results because of overly gullible beliefs about misleading context cues in the scene. In the first row, for example, the context elements around the recognized subject in the baseline's heat map are interpreted to suggest the wrong \"surprise\" category. In contrast, CCIM weakens the detrimental effect of contextual stimuli and facilitates the model to pay more attention to anger-related body semantics from the subject region. Similar observations can be found in the fourth and fifth rows of the samples.\n\n(ii) Our component can also correct biased semantics in the subject-focused part in some cases. A typical example is shown in the second row of the sample. By comparing the heat maps before and after applying the CCIM, the causal intervention helps the model to capture the facial prompts from the subject reflecting the \"disgust\" emotion, resulting in the correct prediction. Interestingly, our component simultaneously removes context cues for agent interactions in the right background that may cause the \"happy-context\" mapping prejudice. Similar capabilities are recognized in the third row of the sample.",
      "page_start": 15,
      "page_end": 16
    },
    {
      "section_name": "Confounder Visualization",
      "text": "To intuitively understand the confounder impact in causal intervention, we visualize 64 clustering centers representing different context confounder prototypes. From Figure  12 , distinct prototypes are well separated distributionally, verifying that our strategy can correctly model stratified confounder features. Then, we perform testing experiments on the EMOTIC dataset using the CCIM-based EmotiCon as the baseline. While inferring two testing samples, we visualize weight maps of confounders of the corresponding coefficient set λ in intervention P (Y |do(X)), where brighter colors represent higher values. In Sample 1, the corresponding weight of the vegetation-related confounder is activated higher to facilitate our component to disentangle the spurious correlation between vegetation context and positive sentiment, leading to reasonable predictions. In Sample 2, our causal intervention gives greater attention to the darknessrelated confounder and forces the model to correct the erroneously negative emotions induced by the dark scene in the vanilla P (Y |X). In summary, CCIM can dynamically decouple the effects of the context bias to different degrees for samples, enabling the model to extract meaningful cues related to the correct emotions in the de-confounded training.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Conclusion And Discussion",
      "text": "This paper proposes a causal debiasing component to reduce the harmful bias of uneven distribution of emotional states across diverse contexts in the CAER task. As a first in-depth investigation of the context, we disentangle the causalities among variables via a tailored causal graph and present a Contextual Causal Intervention Module (CCIM) to remove the adverse effect caused by the context bias as a confounder. Systematic experiments demonstrate the reasonableness of the causality-driven learning paradigm. It is worth noting that our causal weapon can be applied to other context-aware tasks to facilitate the progress of the community.\n\nThis work has potential applications and broad impacts in other fields. (i) CCIM can be readily extended to other context-driven tasks to promote unbiased estimation in the corresponding domains, such as egocentric action anticipation and salient object detection. Through causal debiasing, our component can help researchers build task-specific context confounders and breakthrough performance bottlenecks in vanilla models. (ii) The proposed intervention paradigm can be extended to temporal context scenarios to facilitate biased interference due to temporal asynchrony in sequential modeling applications. Specifically, the context stratification strategy can capture the average causal effect in long-range contextual dependencies from temporal representations to boost de-confounded training. (iii) This work contributes to improving the fairness of baseline methods in context-aware tasks and preventing potential discrimination due to bias in deep models. Related techniques offer promising solutions for developing trustworthy intelligent systems.",
      "page_start": 16,
      "page_end": 17
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (a) presents an",
      "page": 1
    },
    {
      "caption": "Figure 1: We provide several examples of emotion recognition in non-",
      "page": 1
    },
    {
      "caption": "Figure 1: (b), the perceptually critical regions of the subject",
      "page": 1
    },
    {
      "caption": "Figure 2: The harmful context bias in the CAER task is intuitively demon-",
      "page": 2
    },
    {
      "caption": "Figure 1: (c)&(d). When the situational context is ignored in Figure",
      "page": 2
    },
    {
      "caption": "Figure 1: (d). Although physical signals are",
      "page": 2
    },
    {
      "caption": "Figure 3: We present a preliminary toy experiment using the EMOTIC [22]",
      "page": 2
    },
    {
      "caption": "Figure 2: provides an intuitive demonstration of",
      "page": 2
    },
    {
      "caption": "Figure 3: , it becomes evident that more",
      "page": 3
    },
    {
      "caption": "Figure 4: Illustration of our CAER causal graph. (a) The conventional",
      "page": 4
    },
    {
      "caption": "Figure 4: , the CAER procedure contains five different",
      "page": 4
    },
    {
      "caption": "Figure 2: , subjects are habitually",
      "page": 5
    },
    {
      "caption": "Figure 4: (a), the predictions of emotion probabil-",
      "page": 5
    },
    {
      "caption": "Figure 4: (b), the backdoor path would be invalid",
      "page": 5
    },
    {
      "caption": "Figure 5: We present a general pipeline for the context-deconfounded training in the CAER task. The pipeline can be adapted to most CAER models.",
      "page": 6
    },
    {
      "caption": "Figure 6: We show the generation procedure framework of the confounder dictionary Z. The context image set I is first generated by masking the",
      "page": 6
    },
    {
      "caption": "Figure 5: , CCIM is incorporated into",
      "page": 6
    },
    {
      "caption": "Figure 6: shows, we",
      "page": 6
    },
    {
      "caption": "Figure 7: Overall, all models obtain considerable performance",
      "page": 10
    },
    {
      "caption": "Figure 7: Emotion classification accuracy (%) for each category of different CCIM-based models on the CAER-S dataset.",
      "page": 11
    },
    {
      "caption": "Figure 3: , we realize that",
      "page": 11
    },
    {
      "caption": "Figure 2: , “grass” is the",
      "page": 12
    },
    {
      "caption": "Figure 8: Ablation study results for the size N of the confounder dictionary Z on three datasets. (a), (b), and (c) from the EMOTIC, CAER-S, and",
      "page": 13
    },
    {
      "caption": "Figure 9: We employ the GroupWalk dataset with four emotion categories to perform the distribution visualization of features for visual clarity. The results",
      "page": 13
    },
    {
      "caption": "Figure 8: , when the sizes on",
      "page": 13
    },
    {
      "caption": "Figure 10: Qualitative results of the vanilla and CCIM-based EmotiCon on the EMOTIC and GroupWalk datasets with different Jaccard coefficient (JC)",
      "page": 14
    },
    {
      "caption": "Figure 9: visualizes the distributions of context features",
      "page": 14
    },
    {
      "caption": "Figure 11: Qualitative results of the vanilla and CCIM-based EMOT-Net on",
      "page": 15
    },
    {
      "caption": "Figure 10: (a) as an example, since most",
      "page": 15
    },
    {
      "caption": "Figure 10: (b) decouples the misleadingly",
      "page": 15
    },
    {
      "caption": "Figure 12: Visualization results of different confounder distributions and",
      "page": 15
    },
    {
      "caption": "Figure 10: (g), CCIM disen-",
      "page": 15
    },
    {
      "caption": "Figure 11: Inspired by",
      "page": 15
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Vanilla Model\nw/ CCIM": "w/ Random Dictionary Z\nw/ ImageNet Pre-training",
          "27.93\n74.51\n58.33\n30.88\n75.82\n60.74": "26.56\n73.36\n57.45\n28.72\n74.75\n58.96"
        },
        {
          "Vanilla Model\nw/ CCIM": "w/ ResNet-50 [32]\nw/ VGG-16 [81]",
          "27.93\n74.51\n58.33\n30.88\n75.82\n60.74": "29.53\n75.34\n59.92\n28.78\n74.95\n59.47"
        },
        {
          "Vanilla Model\nw/ CCIM": "w/ Additive Attention\nw/o λi\nw/o P (zi)",
          "27.93\n74.51\n58.33\n30.88\n75.82\n60.74": "60.85\n30.79\n75.64\n30.05\n75.21\n59.83\n30.63\n75.59\n59.94"
        },
        {
          "Vanilla Model\nw/ CCIM": "w/o Masking Strategy\nw/ Dichotomous K-Means\nw/ K-Medoids",
          "27.93\n74.51\n58.33\n30.88\n75.82\n60.74": "29.86\n74.84\n59.22\n30.76\n75.77\n60.68\n60.77\n30.85\n75.80"
        },
        {
          "Vanilla Model\nw/ CCIM": "w/ R-FCN [82]\nw/ SSD [83]",
          "27.93\n74.51\n58.33\n30.88\n75.82\n60.74": "30.88\n75.67\n60.55\n30.88\n75.74\n60.69"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Vanilla Model\nw/ CCIM": "w/ Random Dictionary Z\nw/ ImageNet Pre-training",
          "35.28\n88.65\n65.58\n39.13\n91.17\n69.31": "35.12\n87.34\n65.62\n37.48\n90.46\n68.28"
        },
        {
          "Vanilla Model\nw/ CCIM": "w/ ResNet-50 [32]\nw/ VGG-16 [81]",
          "35.28\n88.65\n65.58\n39.13\n91.17\n69.31": "38.86\n90.41\n68.85\n37.93\n89.82\n68.11"
        },
        {
          "Vanilla Model\nw/ CCIM": "w/ Additive Attention\nw/o λi\nw/o P (zi)",
          "35.28\n88.65\n65.58\n39.13\n91.17\n69.31": "39.16\n91.08\n69.26\n38.53\n89.67\n68.75\n39.05\n90.06\n69.15"
        },
        {
          "Vanilla Model\nw/ CCIM": "w/o Masking Strategy\nw/ Dichotomous K-Means\nw/ K-Medoids",
          "35.28\n88.65\n65.58\n39.13\n91.17\n69.31": "38.06\n90.57\n67.79\n39.11\n91.08\n69.25\n39.16\n69.35\n91.15"
        },
        {
          "Vanilla Model\nw/ CCIM": "w/ R-FCN [82]\nw/ SSD [83]",
          "35.28\n88.65\n65.58\n39.13\n91.17\n69.31": "39.13\n91.04\n69.16\n39.13\n69.33\n91.12"
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Sewa db: A rich database for audio-visual emotion and sentiment research in the wild",
      "authors": [
        "J Kossaifi",
        "R Walecki",
        "Y Panagakis",
        "J Shen",
        "M Schmitt",
        "F Ringeval",
        "J Han",
        "V Pandit",
        "A Toisoul",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "2",
      "title": "Dawn of the transformer era in speech emotion recognition: closing the valence gap",
      "authors": [
        "J Wagner",
        "A Triantafyllopoulos",
        "H Wierstorf",
        "M Schmitt",
        "F Burkhardt",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "3",
      "title": "Brainmachine coupled learning method for facial emotion recognition",
      "authors": [
        "D Liu",
        "W Dai",
        "H Zhang",
        "X Jin",
        "J Cao",
        "W Kong"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "4",
      "title": "Aide: A vision-driven multi-view, multi-modal, multi-tasking dataset for assistive driving perception",
      "authors": [
        "D Yang",
        "S Huang",
        "Z Xu",
        "Z Li",
        "S Wang",
        "M Li",
        "Y Wang",
        "Y Liu",
        "K Yang",
        "Z Chen",
        "Y Wang",
        "J Liu",
        "P Zhang",
        "P Zhai",
        "L Zhang"
      ],
      "year": "2023",
      "venue": "Proc. IEEE Int. Conf. Comput. Vis"
    },
    {
      "citation_id": "5",
      "title": "Shoelace pattern-based speech emotion recognition of the lecturers in distance education: Shoepat23",
      "authors": [
        "D Tanko",
        "S Dogan",
        "F Demir",
        "M Baygin",
        "S Sahin",
        "T Tuncer"
      ],
      "year": "2022",
      "venue": "Appl. Acoust"
    },
    {
      "citation_id": "6",
      "title": "Human-computer interaction with detection of speaker emotions using convolution neural networks",
      "authors": [
        "A Alnuaim",
        "M Zakariah",
        "A Alhadlaq",
        "C Shashidhar",
        "W Hatamleh",
        "H Tarazi",
        "P Shukla",
        "R Ratna"
      ],
      "year": "2022",
      "venue": "Comput. Intell. Neurosci"
    },
    {
      "citation_id": "7",
      "title": "Learning associative representation for facial expression recognition",
      "authors": [
        "Y Du",
        "D Yang",
        "P Zhai",
        "M Li",
        "L Zhang"
      ],
      "year": "2021",
      "venue": "Proc. Int. Conf. Image Process"
    },
    {
      "citation_id": "8",
      "title": "Facial expression recognition in the wild via deep attentive center loss",
      "authors": [
        "A Farzaneh",
        "X Qi"
      ],
      "year": "2021",
      "venue": "Proc. IEEE Winter Conf. Appl. Comput. Vis"
    },
    {
      "citation_id": "9",
      "title": "Bodily expression of emotion",
      "authors": [
        "H Wallbott"
      ],
      "year": "1998",
      "venue": "Eur. J. Soc. Psychol"
    },
    {
      "citation_id": "10",
      "title": "Emotion expression in human body posture and movement: a survey on intelligible motion factors, quantification and validation",
      "authors": [
        "M.-A Mahfoudi",
        "A Meyer",
        "T Gaudin",
        "A Buendia",
        "S Bouakaz"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Affective Comput"
    },
    {
      "citation_id": "11",
      "title": "imigue: An identity-free video dataset for micro-gesture understanding and emotion analysis",
      "authors": [
        "X Liu",
        "H Shi",
        "H Chen",
        "Z Yu",
        "X Li",
        "G Zhao"
      ],
      "year": "2021",
      "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit"
    },
    {
      "citation_id": "12",
      "title": "Emotion recognition based on multi-view body gestures",
      "authors": [
        "Z Shen",
        "J Cheng",
        "X Hu",
        "Q Dong"
      ],
      "year": "2019",
      "venue": "Proc. Int. Conf. Image Process"
    },
    {
      "citation_id": "13",
      "title": "Disentangled representation learning for multimodal emotion recognition",
      "authors": [
        "D Yang",
        "S Huang",
        "H Kuang",
        "Y Du",
        "L Zhang"
      ],
      "venue": "Proc. ACM Int. Conf. Multimedia, 2022"
    },
    {
      "citation_id": "14",
      "title": "Target and source modality co-reinforcement for emotion understanding from asynchronous multimodal sequences",
      "authors": [
        "D Yang",
        "Y Liu",
        "C Huang",
        "M Li",
        "X Zhao",
        "Y Wang",
        "K Yang",
        "Y Wang",
        "P Zhai",
        "L Zhang"
      ],
      "year": "2023",
      "venue": "Knowl.-Based Syst"
    },
    {
      "citation_id": "15",
      "title": "Learning modalityspecific and -agnostic representations for asynchronous multimodal language sequences",
      "authors": [
        "D Yang",
        "H Kuang",
        "S Huang",
        "L Zhang"
      ],
      "venue": "Proc. ACM Int. Conf. Multimedia, 2022"
    },
    {
      "citation_id": "16",
      "title": "Contextual and crossmodal interaction for multi-modal speech emotion recognition",
      "authors": [
        "D Yang",
        "S Huang",
        "Y Liu",
        "L Zhang"
      ],
      "year": "2022",
      "venue": "IEEE Signal Process. Lett"
    },
    {
      "citation_id": "17",
      "title": "Emotion recognition in context",
      "authors": [
        "R Kosti",
        "J Alvarez",
        "A Recasens",
        "A Lapedriza"
      ],
      "year": "2017",
      "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit"
    },
    {
      "citation_id": "18",
      "title": "Context-aware emotion recognition networks",
      "authors": [
        "J Lee",
        "S Kim",
        "S Kim",
        "J Park",
        "K Sohn"
      ],
      "year": "2019",
      "venue": "Proc. IEEE/CVF Int. Conf. Comput. Vis"
    },
    {
      "citation_id": "19",
      "title": "Context-aware affective graph reasoning for emotion recognition",
      "authors": [
        "M Zhang",
        "Y Liang",
        "H Ma"
      ],
      "year": "2019",
      "venue": "Proc. IEEE Int. Conf. Multimedia Expo"
    },
    {
      "citation_id": "20",
      "title": "Emoticon: Context-aware multimodal emotion recognition using frege's principle",
      "authors": [
        "T Mittal",
        "P Guhan",
        "U Bhattacharya",
        "R Chandra",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit"
    },
    {
      "citation_id": "21",
      "title": "High-level context representation for emotion recognition in images",
      "authors": [
        "W De Lima Costa",
        "E Talavera",
        "L Figueiredo",
        "V Teichrieb"
      ],
      "year": "2023",
      "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. Workshop"
    },
    {
      "citation_id": "22",
      "title": "Context based emotion recognition using emotic dataset",
      "authors": [
        "R Kosti",
        "J Alvarez",
        "A Recasens",
        "A Lapedriza"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "23",
      "title": "Context in emotion perception",
      "authors": [
        "L Barrett",
        "B Mesquita",
        "M Gendron"
      ],
      "year": "2011",
      "venue": "Cur. Direct. Psychol. Sci"
    },
    {
      "citation_id": "24",
      "title": "Context-aware generation-based net for multi-label visual emotion recognition",
      "authors": [
        "S Ruan",
        "K Zhang",
        "Y Wang",
        "H Tao",
        "W He",
        "G Lv",
        "E Chen"
      ],
      "year": "2020",
      "venue": "Proc. IEEE Int. Conf. Multimedia Expo"
    },
    {
      "citation_id": "25",
      "title": "Sequential interactive biased network for context-aware emotion recognition",
      "authors": [
        "X Li",
        "X Peng",
        "C Ding"
      ],
      "year": "2021",
      "venue": "Proc. IEEE Int. Joint Conf. Joint Biomet"
    },
    {
      "citation_id": "26",
      "title": "Graph reasoning-based emotion recognition network",
      "authors": [
        "Q Gao",
        "H Zeng",
        "G Li",
        "T Tong"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "27",
      "title": "Incorporating structured emotion commonsense knowledge and interpersonal relation into context-aware emotion recognition",
      "authors": [
        "J Chen",
        "T Yang",
        "Z Huang",
        "K Wang",
        "M Liu",
        "C Lyu"
      ],
      "year": "2023",
      "venue": "Appl. Intell"
    },
    {
      "citation_id": "28",
      "title": "Multimodal and contextaware emotion perception model with multiplicative fusion",
      "authors": [
        "T Mittal",
        "A Bera",
        "D Manocha"
      ],
      "year": "2021",
      "venue": "IEEE MultiMedia"
    },
    {
      "citation_id": "29",
      "title": "Context-aware emotion recognition based on visual relationship detection",
      "authors": [
        "M.-H Hoang",
        "S.-H Kim",
        "H.-J Yang",
        "G.-S Lee"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "30",
      "title": "Emotion recognition for multiple context awareness",
      "authors": [
        "D Yang",
        "S Huang",
        "S Wang",
        "Y Liu",
        "P Zhai",
        "L Su",
        "M Li",
        "L Zhang"
      ],
      "year": "2022",
      "venue": "Proc. Eur. Conf. Comput. Vis"
    },
    {
      "citation_id": "31",
      "title": "Contextdependent emotion recognition",
      "authors": [
        "Z Wang",
        "L Lao",
        "X Zhang",
        "Y Li",
        "T Zhang",
        "Z Cui"
      ],
      "year": "2022",
      "venue": "J. Visual Commun. Image Represent"
    },
    {
      "citation_id": "32",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit"
    },
    {
      "citation_id": "33",
      "title": "Places: A 10 million image database for scene recognition",
      "authors": [
        "B Zhou",
        "A Lapedriza",
        "A Khosla",
        "A Oliva",
        "A Torralba"
      ],
      "year": "2017",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "34",
      "title": "Contemplating visual emotions: Understanding and overcoming dataset bias",
      "authors": [
        "R Panda",
        "J Zhang",
        "H Li",
        "J.-Y Lee",
        "X Lu",
        "A Roy-Chowdhury"
      ],
      "year": "2018",
      "venue": "Proc. Eur. Conf. Comput. Vis"
    },
    {
      "citation_id": "35",
      "title": "Causal inference in statistics: An overview",
      "authors": [
        "J Pearl"
      ],
      "year": "2009",
      "venue": "Statist. Surv"
    },
    {
      "citation_id": "36",
      "title": "Causal inference in statistics: A primer",
      "authors": [
        "M Glymour",
        "J Pearl",
        "N Jewell"
      ],
      "year": "2016",
      "venue": "Causal inference in statistics: A primer"
    },
    {
      "citation_id": "37",
      "title": "Context de-confounded emotion recognition",
      "authors": [
        "D Yang",
        "Z Chen",
        "Y Wang",
        "S Wang",
        "M Li",
        "S Liu",
        "X Zhao",
        "S Huang",
        "Z Dong",
        "P Zhai",
        "L Zhang"
      ],
      "year": "2023",
      "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit"
    },
    {
      "citation_id": "38",
      "title": "Constants across cultures in the face and emotion",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1971",
      "venue": "J. Pers. Soc. Psychol"
    },
    {
      "citation_id": "39",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Affective Comput"
    },
    {
      "citation_id": "40",
      "title": "Framework for a comprehensive description and measurement of emotional states",
      "authors": [
        "A Mehrabian"
      ],
      "year": "1995",
      "venue": "Genetic Soc. Gener. Psychol. Monographs"
    },
    {
      "citation_id": "41",
      "title": "Expert system for automatic analysis of facial expressions",
      "authors": [
        "M Pantic",
        "L Rothkrantz"
      ],
      "year": "2000",
      "venue": "Image Vision Comput"
    },
    {
      "citation_id": "42",
      "title": "Recognizing emotions expressed by body pose: A biologically inspired neural model",
      "authors": [
        "K Schindler",
        "L Van Gool",
        "B Gelder"
      ],
      "year": "2008",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "43",
      "title": "Textoriented modality reinforcement network for multimodal sentiment analysis from unaligned multimodal sequences",
      "authors": [
        "Y Lei",
        "D Yang",
        "M Li",
        "S Wang",
        "J Chen",
        "L Zhang"
      ],
      "year": "2023",
      "venue": "Textoriented modality reinforcement network for multimodal sentiment analysis from unaligned multimodal sequences",
      "arxiv": "arXiv:2307.13205"
    },
    {
      "citation_id": "44",
      "title": "A unified self-distillation framework for multimodal sentiment analysis with uncertain missing modalities",
      "authors": [
        "M Li",
        "D Yang",
        "Y Lei",
        "S Wang",
        "S Wang",
        "L Su",
        "K Yang",
        "Y Wang",
        "M Sun",
        "L Zhang"
      ],
      "year": "2024",
      "venue": "Proc. AAAI Conf"
    },
    {
      "citation_id": "45",
      "title": "Towards robust multimodal sentiment analysis under uncertain signal missing",
      "authors": [
        "M Li",
        "D Yang",
        "L Zhang"
      ],
      "year": "2023",
      "venue": "IEEE Signal Process. Lett"
    },
    {
      "citation_id": "46",
      "title": "Human emotion recognition with relational region-level analysis",
      "authors": [
        "W Li",
        "X Dong",
        "Y Wang"
      ],
      "year": "2021",
      "venue": "IEEE Trans. Affective Comput"
    },
    {
      "citation_id": "47",
      "title": "Faster r-cnn: Towards realtime object detection with region proposal networks",
      "authors": [
        "S Ren",
        "K He",
        "R Girshick",
        "J Sun"
      ],
      "year": "2015",
      "venue": "Adv. Neural Inf. Process. Syst"
    },
    {
      "citation_id": "48",
      "title": "Causal inference in economics and marketing",
      "authors": [
        "H Varian"
      ],
      "year": "2016",
      "venue": "Proc. Nat. Acad. Sci"
    },
    {
      "citation_id": "49",
      "title": "Causal inference and developmental psychology",
      "authors": [
        "E Foster"
      ],
      "year": "2010",
      "venue": "Develop. Psychol"
    },
    {
      "citation_id": "50",
      "title": "Models, reasoning and inference",
      "authors": [
        "J Pearl"
      ],
      "year": "2000",
      "venue": "Models, reasoning and inference"
    },
    {
      "citation_id": "51",
      "title": "Causal inference using potential outcomes: Design, modeling, decisions",
      "authors": [
        "D Rubin"
      ],
      "year": "2005",
      "venue": "J. Am. Stat. Assoc"
    },
    {
      "citation_id": "52",
      "title": "Interpretation and identification of causal mediation",
      "authors": [
        "J Pearl"
      ],
      "year": "2014",
      "venue": "Psychol. Methods"
    },
    {
      "citation_id": "53",
      "title": "Causalgan: Learning causal implicit generative models with adversarial training",
      "authors": [
        "M Kocaoglu",
        "C Snyder",
        "A Dimakis",
        "S Vishwanath"
      ],
      "year": "2017",
      "venue": "Causalgan: Learning causal implicit generative models with adversarial training",
      "arxiv": "arXiv:1709.02023"
    },
    {
      "citation_id": "54",
      "title": "How2comm: Communication-efficient and collaboration-pragmatic multi-agent perception",
      "authors": [
        "D Yang",
        "K Yang",
        "Y Wang",
        "J Liu",
        "Z Xu",
        "R Yin",
        "P Zhai",
        "L Zhang"
      ],
      "year": "2023",
      "venue": "Adv. Neural Inf. Proces. Syst"
    },
    {
      "citation_id": "55",
      "title": "Spatio-temporal domain awareness for multi-agent collaborative perception",
      "authors": [
        "K Yang",
        "D Yang",
        "J Zhang",
        "M Li",
        "Y Liu",
        "J Liu",
        "H Wang",
        "P Sun",
        "L Song"
      ],
      "year": "2023",
      "venue": "Proc. IEEE Int. Conf. Comput. Vis"
    },
    {
      "citation_id": "56",
      "title": "What2comm: Towards communication-efficient collaborative perception via feature decoupling",
      "authors": [
        "K Yang",
        "D Yang",
        "J Zhang",
        "H Wang",
        "P Sun",
        "L Song"
      ],
      "venue": "Proc. ACM Int. Conf. Multimedia, 2023"
    },
    {
      "citation_id": "57",
      "title": "Towards asynchronous multimodal signal interaction and fusion via tailored transformers",
      "authors": [
        "D Yang",
        "H Kuang",
        "K Yang",
        "M Li",
        "L Zhang"
      ],
      "year": "2024",
      "venue": "IEEE Signal Process. Lett"
    },
    {
      "citation_id": "58",
      "title": "Mgr3net: Multigranularity region relation representation network for facial expression recognition in affective robots",
      "authors": [
        "Y Wang",
        "S Yan",
        "W Song",
        "A Liotta",
        "J Liu",
        "D Yang",
        "S Gao",
        "W Zhang"
      ],
      "year": "2024",
      "venue": "IEEE Trans. Ind. Inf"
    },
    {
      "citation_id": "59",
      "title": "Can llms' tuning methods work in medical multimodal domain?",
      "authors": [
        "J Chen",
        "Y Jiang",
        "D Yang",
        "M Li",
        "J Wei",
        "Z Qian",
        "L Zhang"
      ],
      "year": "2024",
      "venue": "Can llms' tuning methods work in medical multimodal domain?",
      "arxiv": "arXiv:2403.06407"
    },
    {
      "citation_id": "60",
      "title": "Detecting and evaluating medical hallucinations in large vision language models",
      "authors": [
        "J Chen",
        "D Yang",
        "T Wu",
        "Y Jiang",
        "X Hou",
        "M Li",
        "S Wang",
        "D Xiao",
        "K Li",
        "L Zhang"
      ],
      "year": "2024",
      "venue": "Detecting and evaluating medical hallucinations in large vision language models",
      "arxiv": "arXiv:2406.10185"
    },
    {
      "citation_id": "61",
      "title": "Medthink: Inducing medical large-scale visual language models to hallucinate less by thinking more",
      "authors": [
        "Y Jiang",
        "J Chen",
        "D Yang",
        "M Li",
        "S Wang",
        "T Wu",
        "K Li",
        "L Zhang"
      ],
      "year": "2024",
      "venue": "Medthink: Inducing medical large-scale visual language models to hallucinate less by thinking more",
      "arxiv": "arXiv:2406.11451"
    },
    {
      "citation_id": "62",
      "title": "Efficiency in focus: Layernorm as a catalyst for fine-tuning medical visual language pre-trained models",
      "authors": [
        "J Chen",
        "D Yang",
        "Y Jiang",
        "M Li",
        "J Wei",
        "X Hou",
        "L Zhang"
      ],
      "year": "2024",
      "venue": "Efficiency in focus: Layernorm as a catalyst for fine-tuning medical visual language pre-trained models",
      "arxiv": "arXiv:2404.16385"
    },
    {
      "citation_id": "63",
      "title": "Towards multimodal human intention understanding debiasing via subject-deconfounding",
      "authors": [
        "D Yang",
        "D Xiao",
        "K Li",
        "Y Wang",
        "Z Chen",
        "J Wei",
        "L Zhang"
      ],
      "year": "2024",
      "venue": "Towards multimodal human intention understanding debiasing via subject-deconfounding",
      "arxiv": "arXiv:2403.05025"
    },
    {
      "citation_id": "64",
      "title": "Towards multimodal sentiment analysis debiasing via bias purification",
      "authors": [
        "D Yang",
        "M Li",
        "D Xiao",
        "Y Liu",
        "K Yang",
        "Z Chen",
        "Y Wang",
        "P Zhai",
        "K Li",
        "L Zhang"
      ],
      "year": "2024",
      "venue": "Proc. Eur. Conf. Comput. Vis"
    },
    {
      "citation_id": "65",
      "title": "Robust emotion recognition in context debiasing",
      "authors": [
        "D Yang",
        "K Yang",
        "M Li",
        "S Wang",
        "S Wang",
        "L Zhang"
      ],
      "year": "2024",
      "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit"
    },
    {
      "citation_id": "66",
      "title": "Ca-spacenet: Counterfactual analysis for 6d pose estimation in space",
      "authors": [
        "S Wang",
        "S Wang",
        "B Jiao",
        "D Yang",
        "L Su",
        "P Zhai",
        "C Chen",
        "L Zhang"
      ],
      "year": "2022",
      "venue": "in Int. Conf. Intell. Robots Syst"
    },
    {
      "citation_id": "67",
      "title": "Visual commonsense r-cnn",
      "authors": [
        "T Wang",
        "J Huang",
        "H Zhang",
        "Q Sun"
      ],
      "year": "2020",
      "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit"
    },
    {
      "citation_id": "68",
      "title": "Two causal principles for improving visual dialog",
      "authors": [
        "J Qi",
        "Y Niu",
        "J Huang",
        "H Zhang"
      ],
      "year": "2020",
      "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit"
    },
    {
      "citation_id": "69",
      "title": "Unbiased scene graph generation from biased training",
      "authors": [
        "K Tang",
        "Y Niu",
        "J Huang",
        "J Shi",
        "H Zhang"
      ],
      "year": "2020",
      "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit"
    },
    {
      "citation_id": "70",
      "title": "Causal intervention for subject-deconfounded facial action unit recognition",
      "authors": [
        "Y Chen",
        "D Chen",
        "T Wang",
        "Y Wang",
        "Y Liang"
      ],
      "year": "2022",
      "venue": "Proc. AAAI Conf"
    },
    {
      "citation_id": "71",
      "title": "De-biasing distantly supervised named entity recognition via causal intervention",
      "authors": [
        "W Zhang",
        "H Lin",
        "X Han",
        "L Sun"
      ],
      "year": "2021",
      "venue": "De-biasing distantly supervised named entity recognition via causal intervention",
      "arxiv": "arXiv:2106.09233"
    },
    {
      "citation_id": "72",
      "title": "Counterfactual inference for text classification debiasing",
      "authors": [
        "C Qian",
        "F Feng",
        "L Wen",
        "C Ma",
        "P Xie"
      ],
      "year": "2021",
      "venue": "Proc. Conf"
    },
    {
      "citation_id": "73",
      "title": "Counterfactuallyaugmented snli training data does not yield better generalization than unaugmented data",
      "authors": [
        "W Huang",
        "H Liu",
        "S Bowman"
      ],
      "year": "2020",
      "venue": "Counterfactuallyaugmented snli training data does not yield better generalization than unaugmented data",
      "arxiv": "arXiv:2010.04762"
    },
    {
      "citation_id": "74",
      "title": "Bounded rationality",
      "authors": [
        "B Jones"
      ],
      "year": "1999",
      "venue": "Annu. Rev. Political Sci"
    },
    {
      "citation_id": "75",
      "title": "Shortcut learning in deep neural networks",
      "authors": [
        "R Geirhos",
        "J.-H Jacobsen",
        "C Michaelis",
        "R Zemel",
        "W Brendel",
        "M Bethge",
        "F Wichmann"
      ],
      "year": "2020",
      "venue": "Nat. Mach. Intell"
    },
    {
      "citation_id": "76",
      "title": "Show, attend and tell: Neural image caption generation with visual attention",
      "authors": [
        "K Xu",
        "J Kiros",
        "K Cho",
        "A Courville",
        "R Salakhudinov",
        "R Zemel",
        "Y Bengio"
      ],
      "year": "2015",
      "venue": "Int. Conf. Mach. Learn"
    },
    {
      "citation_id": "77",
      "title": "Automatic differentiation in pytorch",
      "authors": [
        "A Paszke",
        "S Gross",
        "S Chintala",
        "G Chanan",
        "E Yang",
        "Z Devito",
        "Z Lin",
        "A Desmaison",
        "L Antiga",
        "A Lerer"
      ],
      "year": "2017",
      "venue": "Automatic differentiation in pytorch"
    },
    {
      "citation_id": "78",
      "title": "Ml-knn: A lazy learning approach to multi-label learning",
      "authors": [
        "M.-L Zhang",
        "Z.-H Zhou"
      ],
      "year": "2007",
      "venue": "Pattern Recognit"
    },
    {
      "citation_id": "79",
      "title": "Random k-labelsets for multilabel classification",
      "authors": [
        "G Tsoumakas",
        "I Katakis",
        "I Vlahavas"
      ],
      "year": "2010",
      "venue": "IEEE Trans. Knowl. Data Eng"
    },
    {
      "citation_id": "80",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2012",
      "venue": "Adv. Neural Inf. Process. Syst"
    },
    {
      "citation_id": "81",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "82",
      "title": "R-fcn: Object detection via regionbased fully convolutional networks",
      "authors": [
        "J Dai",
        "Y Li",
        "K He",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Adv. Neural Inf. Proces. Syst"
    },
    {
      "citation_id": "83",
      "title": "Ssd: Single shot multibox detector",
      "authors": [
        "W Liu",
        "D Anguelov",
        "D Erhan",
        "C Szegedy",
        "S Reed",
        "C.-Y Fu",
        "A Berg"
      ],
      "year": "2016",
      "venue": "Proc. Eur. Conf. Comput. Vis"
    },
    {
      "citation_id": "84",
      "title": "Imagenet: A large-scale hierarchical image database",
      "authors": [
        "J Deng",
        "W Dong",
        "R Socher",
        "L.-J Li",
        "K Li",
        "L Fei-Fei"
      ],
      "year": "2009",
      "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit"
    }
  ]
}