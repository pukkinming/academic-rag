{
  "paper_id": "2409.10985v2",
  "title": "Improving Speech Emotion Recognition In Under-Resourced Languages Via Speech-To-Speech Translation With Bootstrapping Data Selection",
  "published": "2024-09-17T08:36:45Z",
  "authors": [
    "Hsi-Che Lin",
    "Yi-Cheng Lin",
    "Huang-Cheng Chou",
    "Hung-yi Lee"
  ],
  "keywords": [
    "multilingual",
    "emotion",
    "speech translation"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech Emotion Recognition (SER) is a crucial component in developing general-purpose AI agents capable of natural humancomputer interaction. However, building robust multilingual SER systems remains challenging due to the scarcity of labeled data in languages other than English and Chinese. In this paper, we propose an approach to enhance SER performance in low SER resource languages by leveraging data from high-resource languages. Specifically, we employ expressive Speech-to-Speech translation (S2ST) combined with a novel bootstrapping data selection pipeline to generate labeled data in the target language. Extensive experiments demonstrate that our method is both effective and generalizable across different upstream models and languages. Our results suggest that this approach can facilitate the development of more scalable and robust multilingual SER systems. Our code is available at: https://github.com/hsi-che-lin/Improve-SER-via-S2ST",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Speech Emotion Recognition (SER) is key to improving humancomputer interaction (HCI)  [4] ,  [34]  by enabling machines to understand and respond to users' emotions, enhancing experiences in areas like customer service, mental health, and virtual assistants. While large SER datasets exist for high-resource languages like English  [25] , Chinese  [39] , and Russian  [20] , many languages have far smaller datasets, often under 10 hours of speech, hindering SER performance. Addressing this gap is critical for developing inclusive emotion recognition systems across diverse linguistic communities. As a result, our research question is whether it is possible to boost the performance of SER on under-resourced languages, which, in this paper, refer to languages lacking large labeled SER datasets.\n\nSeveral approaches have been investigated to address this problem, which can be broadly categorized into three groups: transfer learning, domain adaptation, and data augmentation. Transfer learning methods  [3] ,  [29]  aim to enhance performance in low-resource languages by leveraging models trained on high-resource languages. However, when the amount of data available in different languages is imbalanced, a model predominantly trained on a high-resource language might become biased towards the features of that language, neglecting the characteristics of the lower-resource languages. Domain adaptation methods focus on aligning features across languages through techniques such as feature normalization  [19] ,  [35] , direct alignment  [18] ,  [36] ,  [44] , and domain adversarial learning  [44] . Despite these efforts, feature space alignment does not always ensure accurate prediction alignment, as certain undesired information may still be encoded in the features. For augmentation based methods, people usually adopt GAN  [9] ,  [16] ,  [22] ,  [23] ,  [42]  and CycleGAN  [6] ,  [37]  types of methods. However, instability training of GAN makes generating high-quality data difficult and limits their usability.\n\nA recent study  [28]  leverages GPT-4  [30]  and expressive Text-to-Speech (TTS) for data augmentation, enabling the generation of large datasets. However, the synthesized data is conditioned on predefined text and discrete emotion classes, which lack naturalness, fail to capture nuanced paralinguistic cues such as hesitations  [24]  and vocal burst  [7] , and overlook the possibility of multiple, complex emotions co-occurring within a single speech instance  [11] ,  [14] . Moreover, it depends on Azure TTS, which supports only English, restricting its applicability to other languages.\n\nIn this paper, we generate synthetic target language data directly from data of a high-resource language using expressive S2ST. This approach offers three benefits: it avoids feature-prediction mismatches in domain adaptation method by working directly in the speech sample space, addresses language imbalance in transfer learning through target data generation, and captures more paralinguistic cues and emotional nuances by conditioning on real-world samples.\n\nAs shown in Fig.  1 , our method uses a two-phase pipeline: data synthesis and bootstrapping data selection. First, we generate target language data using an expressive S2ST model. Then, we apply a novel bootstrapping method to iteratively select the most beneficial data for training according to the prediction of the model from previous iterations. Despite its simplicity, our method consistently improves performance across various models, languages, and datasets.\n\nOur work yields the following contributions:\n\n• We show that it is possible to improve SER in under-resourced languages by expressive S2ST and large-scale datasets of highresource language. • We propose a simple, effective method to achieve consistent improvement in different upstream models, languages, and datasets. • Based on our experiment, we give some principles for choosing high-resource datasets that can achieve consistent improvement. This paper aims to enhance the performance of SER on a small target dataset, Dtgt, in a target language Lt. We assume access to a large-scale SER dataset, Dsrc, in a source language Ls, which differs from Lt. All samples in both datasets are assumed to have four-class emotion labels, that is, angry, happy, neutral, and sad. Note that, in this paper, we do not consider unlabeled data that may be used in other settings like semi-supervised learning  [2] ,  [32] ,  [43] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "B. Phase 1 -Data Synthesis",
      "text": "Given a SER dataset in a high-resource source language (Dsrc), we propose translating each sample into the target language Lt using an expressive S2ST model, thereby creating a translated version of the source dataset. We observe the S2ST system sometimes may fail at translating. One common failure mode is that the output language is not the target language. As a result, we apply a language identification model, Whisper Large v3 1  [33] , to filter out samples detected as nontarget language. The resulting filtered dataset, Dsyn = {(xi, yi)} N i=0 , where xi and yi represent a speech sample and its label, respectively, and N is the number of samples in Dsrc, is used for phase 2.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Phase 2 -Bootstrapping",
      "text": "After some pilot studies, we find that naively using all Dsyn for training often leads to great performance degradation. To address this, we propose a bootstrapping method that iteratively selects data from Dsyn more likely to be from a distribution similar to Dtgt.\n\nLet Mi represent the model trained on D (i) syn ∪ Dtgt in the i-th iteration. We define a criterion, χ, which takes a data sample and its corresponding model prediction as input, and outputs either 0 or 1. The selected synthesized data for iteration i + 1 is given by\n\nWe let D (0) syn = ϕ, that is, M0 is trained solely on Dtgt. The total number of iterations I is a hyperparameter, defaulting to 2, and we use MI for final evaluation.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "D. Selection Criterion For Bootstrapping",
      "text": "In this paper, we propose two selection criteria, denoted as χ. We hypothesize that the performance drop observed when naively using all Dsyn is due to a domain discrepancy between Dtgt and Dsyn. Our intuition is that if the baseline model can predict a synthesized data sample with reasonable accuracy, that sample likely originates from a distribution similar to that of the target data  [45] . Therefore, our first selection criterion retains samples that are correctly predicted by the model from the previous iteration. That is\n\n1 https://github.com/openai/whisper Language Emotion #Param. Hidden dim. emotion2vec  [27]  Eng. ✓ 93.8M 768 exp. encoder a  [12]  Multi. ✓ 7.2M 512 WavLM  [10]  Eng. 316.6M 1024 XLS-R 300M  [5]  Multi. 315.4M 1024 a Expressivity encoder in Seamless Expressive Additionally, given the availability of soft labels-a probability distribution derived from the voting results of multiple annotators and adjusted using a smoothing technique  [38] -we leverage these labels to account for the multidimensional nature of human emotions  [14] . This forms the basis of our second selection criterion, which we adopt as the default approach. The criterion is given by\n\nwhere y here is a distribution rather a label ID as it is in equation (  2 ), KL stands for Kullback-Leibler (KL) divergence between two distribution, and medM,D syn means the median of KL divergence of all prediction and soft labels in Dsyn.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Experiment Setup A. Datasets",
      "text": "The overview of the datasets utilized in this paper is shown in Tab. I. For high-resource SER datasets Dsrc, we choose to use MSP-Podcast  [25] , a large-scale English dataset, by default, and we also experiment with a large-scale Chinese dataset, BIIC-Podcast  [39] , for comparison. For target datasets, we experiment with four target datasets Dtgt of different languages Lt, including EmoDB  [8] , CaFE  [17] , EMOVO  [13] , and MESD  [15] , which correspond to German, French, Italian, and Spanish, respectively.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Speech Translation",
      "text": "We use Seamless Expressive 2    [12]  as the expressive speech translation model. We also use S2ST models trained without expressivity, SeamlessM4T-Large v2, for comparison. Seamless Expressive is built upon SeamlessM4T-Large v2 as its foundational model and integrates an expressivity encoder to guide the generation of speech units, ensuring appropriate rhythm, speaking rate, and pauses. Additionally, it replaces the HiFi-GAN  [21]  vocoder with an expressive unit-tospeech generator to convey vocal styles more effectively. We use a speech unit generation beam size of 5.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Upstream Model",
      "text": "To demonstrate the generalizability of our method, we experiment with four different upstream models: emotion2vec  [27] , WavLM  [10] , Wav2Vec2 XLS-R 300M  [5] , and the expressivity encoder from Seamless Expressive  [12] . The overview of these models is shown in Tab. II. These models were selected for their strong SER performance  [26] ,  [27] ,  [41]  and diverse pretraining conditions. Some models are trained on multilingual data, others on English, and some use emotional data or paralinguistic-aware objectives during pretraining, while others do not. We selected the expressivity encoder from Seamless Expressive, which is not a common choice for SER upstream models, not only because it is a multilingual, paralinguisticaware pre-trained model, but also because it plays a key role in our data synthesis pipeline. This connection may help explain the effectiveness of our method.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "D. Evaluation Protocol",
      "text": "Our evaluation protocol primarily follows the EmoBox framework  [26] . We perform speaker-aware cross-validation, ensuring that speakers in the training and test sets are different. The partitioning of folds in each dataset also follows EmoBox. To reduce performance variance, we repeat each experiment with three different random seeds and average the results, as we observed sensitivity to initialization, particularly on smaller target datasets.\n\nThe downstream model is a simple two-layer feed-forward network with ReLU  [1]  activation, incorporating a pooling layer between the layers. The inputs to this network are the final layer features extracted by the upstream model. In all experiments, the upstream model is frozen, and the downstream model is trained using the standard cross-entropy loss function. For each experiment, we report three commonly used performance metrics: Unweighted Average Accuracy (UA), Weighted Average Accuracy (WA), and Macro F1 Score (F1).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Result And Discussion",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Main Result",
      "text": "As shown in Table  III , our proposed method achieves performance improvements across all upstream models and target datasets, regardless of language. Notably, performance gains are observed even on well-performing datasets like EMO-DB, while on underperforming datasets, such as WavLM on EMOVO or XLS-R on CaFE, our method boosts the F1 score by as much as 8 to 11 points.\n\nOur experiments include models pretrained in English or multilingual settings and models pretrained for prosody or emotion awareness. The consistent improvements demonstrate the generalizability of our method across various types of upstream models. Importantly, even on a multilingual and prosody-aware pretrained model (the expressivity encoder in Seamless Expressive), we observe performance gains, underscoring the effectiveness of scaling up the downstream training dataset.\n\nInterestingly, we find that the expressivity encoder in Seamless Expressive, which, to our knowledge, has not been previously used as an upstream model for SER tasks, performs remarkably well despite its smaller model size and reduced hidden dimensions. We attribute this to its strong ability to capture subtle paralinguistic cues that are closely correlated with emotional information-an essential factor in Seamless Expressive's ability to generate high-quality expressive speech. Since our method relies on high-quality data synthesized by Seamless Expressive, the strong emotion recognition performance of Seamless Expressive further explains the effectiveness of our approach. We validate this further through an ablation study, where we swap Seamless Expressive with its non-expressive counterpart, as detailed in the next section.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Comparison To Other Methods",
      "text": "Since our method can be considered a type of data augmentation, we compare it with two other data augmentation methods-noise augmentation and CopyPaste  [31] -to demonstrate its effectiveness. As shown in Table  IV , while CopyPaste performs slightly better on one dataset, only our method consistently improves performance across all languages. Notably, the upstream models used already exhibit strong performance in multilingual ER  [26] , making consistent improvement challenging even for SOTA augmentation methods like CopyPaste. This further highlights the effectiveness of our proposed method.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Ablation Study",
      "text": "In this section, we conduct an ablation study using emotion2vec as the upstream model and our default selection criterion (3) to validate each component of our method. We break down our approach into four key components: training on external data from a high-resource language, translation, translation with expressivity, and bootstrapping data selection process. As shown in Table  V , all components are necessary for achieving optimal and consistent performance. Specifically, comparing row 4 and 5, we observe that bootstrapping data selection plays a crucial role in achieving significant performance improvement across all languages. We also find that bootstrapping consistently improves performance across all ablation settings. Therefore, results with bootstrapping are presented in row 2 and 3 of the ablation table.\n\nBy comparing row 3 and 5, it is clear that our method achieves much better overall performance, indicating that the expressivity of the synthesized training data is important. This finding is unsurprising, as most emotional information is conveyed through paralinguistic details in speech. The significant difference between these settings aligns with the strong performance of the expressivity encoder and supports our hypothesis regarding the effectiveness of our method. The Dsyn in row 3 was generated by SeamlessM4T-Large v2, the non-expressive version of Seamless Expressive. Without the expressivity encoder, it fails to synthesize data that benefits emotion recognition.   Interestingly, when using the source dataset without translation (row 2), the audio retains its paralinguistic information. Based on this and the results in row 2 and 3, we conclude that expressivity may be more important than training on the target language itself. However, both of these settings are still inferior to the baseline, and only when the external data is both expressive and matched to the target language is a notable performance boost observed.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "D. Selecting Proper Source Dataset",
      "text": "We conducted two additional experiments about the source dataset Dsrc to evaluate the impact of using different source datasets and to develop guidelines for selecting Dsrc. The first experiment investigates the effectiveness of our method when the assumption of access to soft labels for the source data does not hold-in other words, when only one-hot label is available for the source dataset. In this case, our default selection criterion (3) is no longer applicable, so we use (2) as the selection criterion.\n\nAs shown in Table  VI , we still observe performance gains in 3 out of 4 languages; however, the improvements are less substantial compared to using our default selection criterion. The advantage of soft labels likely stems from the complex, multidimensional nature of human emotion. When using criterion  (2) , false positive data (where the prediction matches its highest-voted class but actually comes from a significantly different distribution) may be selected, leading to diminished performance. In contrast, selecting based on KL divergence, as in (3), allows us to capture more nuanced differences, such as distinguishing between sadness with happiness versus sadness with anger. Consequently, we are more likely to select data that aligns better with the distribution of the target dataset Dtgt.\n\nIn the second experiment, we switch the source dataset from MSP-Podcast to BIIC-Podcast to assess the effect of changing the source language Ls. As shown in Table  VI , using BIIC-Podcast as the source dataset generally leads to performance degradation. We suspect this may be related to how emotions are expressed and the level of expressivity in different languages. Previous studies have found that Chinese, for example, tends to be less expressive than English, or that emotions are conveyed through more subtle paralinguistic cues, which are harder to capture  [40] . Subtle paralinguistic cues may be difficult for the S2ST to detect and capture. As we observed in Tab V, without expressivity, even with bootstrapping data selection, Dsyn may still lead to performance degradation. This could negatively affect the quality of synthesized data.\n\nBased on these results, we derive two guidelines for selecting an appropriate source dataset Dsrc:\n\n• Although Dsrc with simple one-hot annotations can provide some improvement, it is preferable to select a dataset with soft labels, as these are beneficial in the bootstrapping data selection process. • Choosing datasets in more expressive languages may help the data synthesis process preserve more emotional information, thus improving the quality of Dsyn and the performance.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "E. Number Of Iterations For Bootstrapping",
      "text": "In all experiments, we fix the number of iterations to 2, based on empirical findings that this configuration achieves a good balance between performance and training time. However, the optimal number of iterations may vary depending on the upstream models and target datasets. Fig.  2  shows the performance for different numbers of iterations alongside the oracle result (where the best number of iterations is selected for each fold and seed) for emotion2vec on the EmoDB and EMOVO datasets.\n\nAs shown in Fig.  2 , the optimal number of iterations is 2 for EmoDB and 3 for EMOVO. While 2 iterations already provide a substantial performance boost, we observe an additional gain of 2.3 F1 points with one more iteration. Furthermore, a significant gap remains between the fixed number of iterations and the oracle result, indicating room for improvement. These findings suggest that an adaptive method for dynamically determining whether to continue or stop the bootstrapping process is needed. We leave this exploration for future work.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "V. Conclusion",
      "text": "In this paper, we demonstrate that multilingual SER can be significantly improved by leveraging large-scale datasets in the target language. We employ an expressive S2ST model and propose a simple yet effective data selection process to construct a synthesized dataset, leading to substantial performance gains. Our experiments show the generalizability of the proposed method across various upstream models and datasets in different languages. Additionally, we provide guidelines for selecting an appropriate source dataset to further enhance performance. We hope that our work paves the way for more robust and scalable approaches in multilingual SER.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Vi. Limitation And Future Work",
      "text": "Although our proposed method is effective and generalizable across different upstream models and languages, there are still limitations that can be addressed. Currently, we treat the number of iterations for bootstrapping as a fixed hyperparameter, and an adaptive strategy for determining the optimal number of iterations is needed for further improvement. Additionally, achieving the highest performance gains requires access to soft labels from the source SER dataset. These factors suggest that there is still room to develop a pipeline that can be applied across all scenarios.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Vii. Acknowledgement",
      "text": "",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of our method.",
      "page": 1
    },
    {
      "caption": "Figure 2: Effect of different numbers of iterations for bootstrapping on EmoDB",
      "page": 4
    },
    {
      "caption": "Figure 2: shows the performance for different numbers of",
      "page": 4
    },
    {
      "caption": "Figure 2: , the optimal number of iterations is 2 for",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "EmoDB (de)": "UA(%)↑\nWA(%)↑\nF1(%)↑",
          "CaFE (fr)": "UA(%)↑\nWA(%)↑\nF1(%)↑",
          "EMOVO (it)": "UA(%)↑\nWA(%)↑\nF1(%)↑",
          "MESD (es)": "UA(%)↑\nWA(%)↑\nF1(%)↑"
        },
        {
          "EmoDB (de)": "84.97\n83.79\n84.04\n85.94\n85.17\n85.27",
          "CaFE (fr)": "67.33\n66.72\n66.02\n68.32\n68.29\n67.74",
          "EMOVO (it)": "49.90\n49.90\n44.22\n55.56\n55.56\n51.04",
          "MESD (es)": "64.75\n65.18\n64.33\n64.87\n65.21\n64.65"
        },
        {
          "EmoDB (de)": "92.73\n92.41\n92.56\n93.43\n93.01\n93.30",
          "CaFE (fr)": "64.55\n62.62\n63.11\n65.01\n63.31\n64.34",
          "EMOVO (it)": "53.37\n53.37\n49.20\n54.27\n54.27\n49.44",
          "MESD (es)": "84.09\n84.08\n84.03\n84.49\n84.23\n84.42"
        },
        {
          "EmoDB (de)": "93.65\n94.54\n93.74\n94.11\n94.94\n94.17",
          "CaFE (fr)": "74.80\n74.19\n74.45\n76.12\n75.64\n76.18",
          "EMOVO (it)": "58.33\n58.33\n53.36\n64.98\n64.98\n62.21",
          "MESD (es)": "68.00\n68.69\n67.34\n68.12\n68.71\n67.96"
        },
        {
          "EmoDB (de)": "80.68\n79.17\n79.67\n81.83\n80.60\n80.74",
          "CaFE (fr)": "44.18\n40.80\n37.97\n51.72\n48.61\n49.67",
          "EMOVO (it)": "46.43\n46.43\n40.25\n47.42\n47.42\n41.91",
          "MESD (es)": "73.93\n74.18\n73.70\n77.00\n77.71\n76.73"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "EmoDB (de)": "UA(%)↑\nWA(%)↑\nF1(%)↑",
          "CaFE (fr)": "UA(%)↑\nWA(%)↑\nF1(%)↑",
          "EMOVO (it)": "UA(%)↑\nWA(%)↑\nF1(%)↑",
          "MESD (es)": "UA(%)↑\nWA(%)↑\nF1(%)↑"
        },
        {
          "EmoDB (de)": "84.97\n83.79\n84.04\n84.57\n84.07\n84.10\n84.66\n83.93\n84.34\n85.94\n85.17\n85.27",
          "CaFE (fr)": "67.33\n66.72\n66.02\n63.29\n63.14\n61.46\n66.80\n65.91\n65.43\n68.32\n68.29\n67.74",
          "EMOVO (it)": "49.90\n49.90\n44.22\n49.80\n49.80\n45.01\n55.12\n55.12\n50.48\n55.56\n55.56\n51.04",
          "MESD (es)": "64.75\n65.18\n64.33\n64.69\n65.13\n64.55\n64.87\n65.37\n64.69\n64.87\n65.21\n64.65"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Idx": "",
          "HRa TRb EXc BOd": "",
          "EmoDB (de)": "UA(%)↑\nWA(%)↑\nF1(%)↑",
          "CaFE (fr)": "UA(%)↑\nWA(%)↑\nF1(%)↑",
          "EMOVO (it)": "UA(%)↑\nWA(%)↑\nF1(%)↑",
          "MESD (es)": "UA(%)↑\nWA(%)↑\nF1(%)↑"
        },
        {
          "Idx": "1\n2\n3\n4\n5",
          "HRa TRb EXc BOd": "✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓",
          "EmoDB (de)": "84.97\n83.79\n84.04\n83.49\n83.26\n83.02\n79.53\n79.50\n78.90\n72.61\n71.56\n71.30\n85.94\n85.17\n85.27",
          "CaFE (fr)": "67.33\n66.72\n66.02\n67.26\n67.30\n66.87\n63.76\n63.37\n62.82\n52.58\n53.30\n51.73\n68.32\n68.29\n67.74",
          "EMOVO (it)": "49.90\n49.90\n44.22\n52.28\n52.28\n48.31\n57.54\n57.54\n53.68\n53.17\n53.17\n50.03\n55.56\n55.56\n51.04",
          "MESD (es)": "64.75\n65.18\n64.33\n62.31\n62.95\n62.03\n58.83\n58.85\n58.40\n55.57\n56.15\n55.35\n64.87\n65.21\n64.65"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "EmoDB (de)": "UA(%)↑\nWA(%)↑\nF1(%)↑",
          "CaFE (fr)": "UA(%)↑\nWA(%)↑\nF1(%)↑",
          "EMOVO (it)": "UA(%)↑\nWA(%)↑\nF1(%)↑",
          "MESD (es)": "UA(%)↑\nWA(%)↑\nF1(%)↑"
        },
        {
          "EmoDB (de)": "84.97\n83.79\n84.04\n85.07\n84.44\n84.55\n81.95\n81.83\n81.72\n85.94\n85.17\n85.27",
          "CaFE (fr)": "67.33\n66.72\n66.02\n67.79\n68.00\n66.78\n64.68\n64.18\n62.80\n68.32\n68.29\n67.74",
          "EMOVO (it)": "49.90\n49.90\n44.22\n54.56\n54.56\n50.14\n55.66\n55.66\n51.99\n55.56\n55.56\n51.04",
          "MESD (es)": "64.75\n65.18\n64.33\n62.66\n63.02\n62.37\n62.31\n62.64\n62.17\n64.87\n65.21\n64.65"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Deep Learning using Rectified Linear Units (ReLU)",
      "authors": [
        "A Agarap"
      ],
      "year": "2019",
      "venue": "Deep Learning using Rectified Linear Units (ReLU)"
    },
    {
      "citation_id": "2",
      "title": "Semi-supervised cross-lingual speech emotion recognition",
      "authors": [
        "M Agarla",
        "S Bianco",
        "L Celona",
        "P Napoletano",
        "A Petrovsky",
        "F Piccoli",
        "R Schettini",
        "I Shanin"
      ],
      "year": "2024",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "3",
      "title": "Borrow from rich cousin: transfer learning for emotion detection using cross lingual embedding",
      "authors": [
        "Z Ahmad"
      ],
      "year": "2020",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "4",
      "title": "Human-computer interaction with a real-time speech emotion recognition with ensembling techniques 1D convolution neural network and attention",
      "authors": [
        "W Alsabhan"
      ],
      "year": "2023",
      "venue": "Sensors"
    },
    {
      "citation_id": "5",
      "title": "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale",
      "authors": [
        "A Babu",
        "C Wang",
        "A Tjandra",
        "K Lakhotia",
        "Q Xu",
        "N Goyal",
        "K Singh",
        "P Von Platen",
        "Y Saraf",
        "J Pino",
        "A Baevski",
        "A Conneau",
        "M Auli"
      ],
      "venue": "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale"
    },
    {
      "citation_id": "6",
      "title": "CycleGAN-Based Emotion Style Transfer as Data Augmentation for Speech Emotion Recognition",
      "authors": [
        "F Bao",
        "M Neumann",
        "N Vu"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "7",
      "title": "Deep learning reveals what vocal bursts express in different cultures",
      "authors": [
        "J Brooks",
        "P Tzirakis",
        "A Baird",
        "L Kim",
        "M Opara",
        "X Fang",
        "D Keltner",
        "M Monroy",
        "R Corona",
        "J Metrick"
      ],
      "year": "2023",
      "venue": "Nature Human Behaviour"
    },
    {
      "citation_id": "8",
      "title": "A database of German emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "A database of German emotional speech"
    },
    {
      "citation_id": "9",
      "title": "Data Augmentation Using GANs for Speech Emotion Recognition",
      "authors": [
        "A Chatziagapi"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "10",
      "title": "WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing",
      "authors": [
        "S Chen"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "Minority Views Matter: Evaluating Speech Emotion Classifiers with Human Subjective Annotations by an All-Inclusive Aggregation Rule",
      "authors": [
        "H.-C Chou",
        "L Goncalves",
        "S.-G Leem",
        "A Salman",
        "C.-C Lee",
        "C Busso"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "12",
      "title": "Seamless: Multilingual Expressive and Streaming Speech Translation",
      "authors": [
        "S Communication"
      ],
      "year": "2023",
      "venue": "Seamless: Multilingual Expressive and Streaming Speech Translation"
    },
    {
      "citation_id": "13",
      "title": "EMOVO Corpus: an Italian Emotional Speech Database",
      "authors": [
        "G Costantini"
      ],
      "year": "2014",
      "venue": "Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14)"
    },
    {
      "citation_id": "14",
      "title": "Semantic Space Theory: A Computational Approach to Emotion",
      "authors": [
        "A Cowen",
        "D Keltner"
      ],
      "year": "2021",
      "venue": "Trends in Cognitive Sciences"
    },
    {
      "citation_id": "15",
      "title": "Mexican Emotional Speech Database Based on Semantic, Frequency, Familiarity, Concreteness, and Cultural Shaping of Affective Prosody",
      "authors": [
        "M Duville",
        "L Alonso-Valerdi",
        "D Ibarra-Zarate"
      ],
      "year": "2021",
      "venue": "Data"
    },
    {
      "citation_id": "16",
      "title": "GAN-Based Data Generation for Speech Emotion Recognition",
      "authors": [
        "S Eskimez",
        "D Dimitriadis",
        "R Gmyr",
        "K Kumanati"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "17",
      "title": "A canadian french emotional speech dataset",
      "authors": [
        "P Gournay",
        "O Lahaie",
        "R Lefebvre"
      ],
      "year": "2018",
      "venue": "Proceedings of the 9th ACM Multimedia Systems Conference"
    },
    {
      "citation_id": "18",
      "title": "On acoustic emotion recognition: compensating for covariate shift",
      "authors": [
        "A Hassan"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "19",
      "title": "Context-independent multilingual emotion recognition from speech signals",
      "authors": [
        "V Hozjan",
        "Z Kačič"
      ],
      "year": "2003",
      "venue": "International journal of speech technology"
    },
    {
      "citation_id": "20",
      "title": "Large raw emotional dataset with aggregation mechanism",
      "authors": [
        "V Kondratenko",
        "A Sokolov",
        "N Karpov",
        "O Kutuzov",
        "N Savushkin",
        "F Minkin"
      ],
      "year": "2022",
      "venue": "Large raw emotional dataset with aggregation mechanism",
      "arxiv": "arXiv:2212.12266"
    },
    {
      "citation_id": "21",
      "title": "Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis",
      "authors": [
        "J Kong",
        "J Kim",
        "J Bae"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "22",
      "title": "Augmenting Generative Adversarial Networks for Speech Emotion Recognition",
      "authors": [
        "S Latif",
        "M Asim",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "23",
      "title": "Generative emotional AI for speech emotion recognition: The case for synthetic emotional speech augmentation",
      "authors": [
        "S Latif"
      ],
      "year": "2023",
      "venue": "Applied Acoustics"
    },
    {
      "citation_id": "24",
      "title": "SponTTS: modeling and transferring spontaneous style for TTS",
      "authors": [
        "H Li",
        "X Zhu",
        "L Xue",
        "Y Song",
        "Y Chen",
        "L Xie"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "25",
      "title": "Building Naturalistic Emotionally Balanced Speech Corpus by Retrieving Emotional Speech from Existing Podcast Recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "26",
      "title": "EmoBox: Multilingual Multi-corpus Speech Emotion Recognition Toolkit and Benchmark",
      "authors": [
        "Z Ma",
        "M Chen",
        "H Zhang",
        "Z Zheng",
        "W Chen",
        "X Li",
        "J Ye",
        "X Chen",
        "T Hain"
      ],
      "venue": "EmoBox: Multilingual Multi-corpus Speech Emotion Recognition Toolkit and Benchmark"
    },
    {
      "citation_id": "27",
      "title": "emotion2vec: Self-Supervised Pre-Training for Speech Emotion Representation",
      "authors": [
        "Z Ma"
      ],
      "year": "2024",
      "venue": "Findings of the Association for Computational Linguistics ACL 2024"
    },
    {
      "citation_id": "28",
      "title": "Leveraging speech ptm, text llm, and emotional tts for speech emotion recognition",
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "29",
      "title": "Cross-lingual and multilingual speech emotion recognition on english and french",
      "authors": [
        "Neumann"
      ],
      "year": "2018",
      "venue": "2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "30",
      "title": "GPT-4 Technical Report",
      "authors": [
        "Openai"
      ],
      "year": "2024",
      "venue": "GPT-4 Technical Report"
    },
    {
      "citation_id": "31",
      "title": "Copypaste: An augmentation method for speech emotion recognition",
      "authors": [
        "R Pappagari",
        "J Villalba",
        "P Żelasko",
        "L Moro-Velazquez",
        "N Dehak"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "32",
      "title": "Semi-supervised speech emotion recognition with ladder networks",
      "authors": [
        "S Parthasarathy",
        "C Busso"
      ],
      "year": "2020",
      "venue": "IEEE/ACM transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "33",
      "title": "Robust Speech Recognition via Large-Scale Weak Supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2022",
      "venue": "Robust Speech Recognition via Large-Scale Weak Supervision"
    },
    {
      "citation_id": "34",
      "title": "Speech emotion recognition approaches in human computer interaction",
      "authors": [
        "S Ramakrishnan",
        "I Emary"
      ],
      "year": "2013",
      "venue": "Telecommunication Systems"
    },
    {
      "citation_id": "35",
      "title": "Enhancing Multilingual Recognition of Emotion in Speech by Language Identification",
      "authors": [
        "H Sagha"
      ],
      "year": "2016",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "36",
      "title": "Cross-corpus speech emotion recognition based on transfer non-negative matrix factorization",
      "authors": [
        "P Song"
      ],
      "year": "2016",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "37",
      "title": "A Conditional Cycle Emotion Gan for Cross Corpus Speech Emotion Recognition",
      "authors": [
        "B.-H Su",
        "C.-C Lee"
      ],
      "year": "2021",
      "venue": "2021 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "38",
      "title": "Rethinking the inception architecture for computer vision",
      "authors": [
        "C Szegedy",
        "V Vanhoucke",
        "S Ioffe",
        "J Shlens",
        "Z Wojna"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "39",
      "title": "An Intelligent Infrastructure Toward Large Scale Naturalistic Affective Speech Corpora Collection",
      "authors": [
        "S Upadhyay",
        "W.-S Chien",
        "B.-H Su",
        "L Goncalves",
        "Y.-T Wu",
        "A Salman",
        "C Busso",
        "C.-C Lee"
      ],
      "venue": "2023 11th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "40",
      "title": "Within and Across-Language Comparison of Vocal Emotions in Mandarin and English",
      "authors": [
        "T Wang",
        "Other"
      ],
      "year": "2018",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "41",
      "title": "Open-Emotion: A Reproducible EMO-SUPERB for Speech Emotion Recognition Systems",
      "authors": [
        "H Wu",
        "H.-C Chou",
        "K.-W Chang",
        "L Goncalves",
        "J Du",
        "J.-S Jang",
        "C.-C Lee",
        "H.-Y Lee"
      ],
      "year": "2024",
      "venue": "2024 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "42",
      "title": "Adversarial Data Augmentation Network for Speech Emotion Recognition",
      "authors": [
        "L Yi",
        "M.-W Mak"
      ],
      "year": "2019",
      "venue": "2019 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "43",
      "title": "Enhanced semi-supervised learning for multimodal emotion recognition",
      "authors": [
        "Z Zhang",
        "F Ringeval",
        "B Dong",
        "E Coutinho",
        "E Marchi",
        "B Schüller"
      ],
      "year": "2016",
      "venue": "2016 IEEE international conference on acoustics, speech and signal processing (ICASSP"
    },
    {
      "citation_id": "44",
      "title": "Deep Implicit Distribution Alignment Networks for cross-Corpus Speech Emotion Recognition",
      "authors": [
        "Y Zhao"
      ],
      "venue": "ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "45",
      "title": "Out-of-domain detection for natural language understanding in dialog systems",
      "authors": [
        "Y Zheng",
        "G Chen",
        "M Huang"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    }
  ]
}