{
  "paper_id": "2003.06692v1",
  "title": "Emoticon: Context-Aware Multimodal Emotion Recognition Using Frege'S Principle",
  "published": "2020-03-14T19:55:21Z",
  "authors": [
    "Trisha Mittal",
    "Pooja Guhan",
    "Uttaran Bhattacharya",
    "Rohan Chandra",
    "Aniket Bera",
    "Dinesh Manocha"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We present EmotiCon, a learning-based algorithm for context-aware perceived human emotion recognition from videos and images. Motivated by Frege's Context Principle from psychology, our approach combines three interpretations of context for emotion recognition. Our first interpretation is based on using multiple modalities (e.g. faces and gaits) for emotion recognition. For the second interpretation, we gather semantic context from the input image and use a self-attention-based CNN to encode this information. Finally, we use depth maps to model the third interpretation related to socio-dynamic interactions and proximity among agents. We demonstrate the efficiency of our network through experiments on EMOTIC, a benchmark dataset. We report an Average Precision (AP) score of 35.48 across 26 classes, which is an improvement of 7-8 over prior methods. We also introduce a new dataset, GroupWalk, which is a collection of videos captured in multiple real-world settings of people walking. We report an AP of 65.83 across 4 categories on GroupWalk, which is also an improvement over prior methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Perceiving the emotions of people around us is vital in everyday life. Humans often alter their behavior while interacting with others based on their perceived emotions. In particular, automatic emotion recognition has been used for different applications, including human-computer interaction  [13] , surveillance  [12] , robotics, games, entertainment, and more. Emotions are modeled as either discrete categories or as points in a continuous space of affective dimensions  [16] . In the continuous space, emotions are treated as points in a 3D space of valence, arousal, and dominance. In this work, our focus is on recognizing perceived human emotion rather than the actual emotional state of a person in the discrete emotion space.\n\nInitial works in emotion recognition have been mostly unimodal  [46, 1, 47, 44]  approaches. The unique modality may correspond to facial expressions, voice, text, body posture, gaits, or physiological signals. This was followed by multimodal emotion recognition  [49, 21, 50] , where various combinations of modalities were used and combined in various manners to infer emotions.\n\nAlthough such modalities or cues extracted from a person can provide us with information regarding the perceived emotion, context also plays a very crucial role in the understanding of the perceived emotion. Frege's context principle  [45]  urges not asking for the meaning of a word in isolation and instead of finding the meaning in the context of a sentence. We use this notion behind the context principle in psychology for emotion recognition. 'Context' has been interpreted in multiple ways by researchers in psychology, including:\n\n(a) Context 1 (Multiple Modalities): Incorporating cues from different modalities was one of the initial definitions of context. This domain is also known as Multimodal Emotion Recognition. Combining modalities provides complementary information, which leads to better inference and also performs better on in-the-wild datasets. (b) Context 2 (Background Context): Semantic understanding of the scene from visual cues in the image helps in getting insights about the agent's surroundings and activity, both of which can affect the perceived emotional state of the agent. (c) Context 3 (Socio-Dynamic Inter-Agent Interactions):\n\nResearchers in psychology suggest that the presence or absence of other agents affects the perceived emotional state of an agent. When other agents share an identity or are known to the agent, they often coordinate their behaviors. This varies when other agents are strangers. Such interactions and proximity to other agents have been less explored for perceived emotion recognition.\n\nOne of our goals is to make Emotion Recognition systems work for real-life scenarios. This implies using modalities that do not require sophisticated equipment to be captured and are readily available. Psychology researchers  [3]  have conducted experiments by mixing faces and body features corresponding to different emotions and found that participants guessed the emotions that matched the body features. This is also because of the ease of \"mocking\" one's facial expressions. Subsequently, researchers  [25, 38]  found the combination of faces and body features to be a reliable measure of inferring human emotion. As a result, it would be useful to combine such face and body features for contextbased emotion recognition.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Main Contributions:",
      "text": "We propose EmotiCon, a contextaware emotion recognition model. The input to Emoti-Con is images/video frames, and the output is a multi-label emotion classification. The novel components of our work include:\n\n1. We present a context-aware multimodal emotion recognition algorithm called EmotiCon. Consistent with Ferge's Context principle, in this work, we try to incorporate three interpretations of context to perform emotion recognition from videos and images. 2. We also present a new approach to modeling the sociodynamic interactions between agents using a depthbased CNN. We compute a depth map of the image and feed that to the network to learn about the proximity of agents to each other.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Though Extendable To Any Number Of Modalities, We",
      "text": "release a new dataset GroupWalk for emotion recognition. To the best of our knowledge, there exist very few datasets captured in uncontrolled settings with both faces and gaits that have emotion label annotations. To enable research in this domain, we make GroupWalk publicly available with emotion annotations. GroupWalk is a collection of 45 videos captured in multiple real-world settings of people walking in dense crowd settings. The videos have about 3544 agents annotated with their emotion labels.\n\nWe compare our work with prior methods by testing our performance on EMOTIC  [28] , a benchmark dataset for context-aware emotion recognition. We report an improved AP score of 35.48 on EMOTIC, which is an improvement of 7 -8 over prior methods  [27, 30, 58] . We also report AP scores of our approach and prior methods on the new dataset, GroupWalk. We perform ablation experiments on both datasets, to justify the need for the three components of EmotiCon. As per the annotations provided in EMOTIC, we perform a multi-label classification over 26 discrete emotion labels. On GroupWalk too, we perform a multi-label classification over 4 discrete emotions (anger, happy, neutral, sad).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "In this section, we give a brief overview of previous works on unimodal and multimodal emotion recognition, context-aware emotion recognition, and existing contextaware datasets.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Uni/Multimodal Emotion Recognition",
      "text": "Prior works in emotion recognition from handcrafted features  [48, 60]  or deep learning networks  [32, 17, 31]  have used single modalities like facial expressions  [46, 1] , voice, and speech expressions  [47] , body gestures  [42] , gaits  [44] , and physiological signals such as respiratory and heart cues  [26] . There has been a shift in the paradigm, where researchers have tried to fuse multiple modalities to perform emotion recognition, also known as Multimodal Emotion Recognition. Fusion methods like early fusion  [49] , late fusion  [21] , and hybrid fusion  [50]  have been explored for emotion recognition from multiple modalities. Multimodal emotion recognition has been motivated by research in psychology and also helped in improving accuracy on in-thewild emotion recognition datasets like IEMOCAP  [9]  and CMU-MOSEI  [57] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Context-Aware Emotion Recognition In Psychology Research",
      "text": "Though introduced in the domain of philosophy of language, Frege  [45]  proposed that words should never be seen in isolation but in the context of their proposition. Researchers in psychology  [6, 29, 37]  also agree that just like most psychological processes, emotional processes cannot be interpreted without context. They suggest that context often produces emotion and also shapes how emotion is perceived. Emotion literature that addresses context  [2, 5, 39]  suggests several broad categories of contextual features: person, situation, and context. Martinez et al.  [36]  conduct experiments about the necessity of context and found that even when the participants' faces and bodies were masked in silent videos, viewers were able to infer the affect successfully. Greenway et al.  [20]  organize these contextual features in three levels, ranging from microlevel (person) to macro-level (cultural). In level 2 (situational), they include factors like the presence and closeness To obtain h1, we use a multiplicative fusion layer (red color) to fuse inputs from both modalities, faces, and gaits. h1, h2 and h3 are then concatenated to obtain hconcat. of other agents. Research shows that the simple presence of another person elicits more expression of emotion than situations where people are alone  [54, 24] . These expressions are more amplified when people know each other and are not strangers  [24] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Context-Aware Emotion Recognition",
      "text": "Recent works in context-aware emotion recognition are based on deep-learning network architectures. Kosti et al.  [27]  and Lee et al.  [30]  present two recent advances in context-aware emotion recognition and they propose similar architectures. Both of them have two-stream architectures followed by a fusion network. One stream focuses on a modality (face for  [30]  and body for  [27] ) and the other focuses on capturing context. Lee et al.  [30]  consider everything other than the face as context, and hence mask the face from the image to feed to the context stream. On the other hand,  [30]  uses a Region Proposal Network (RPN) to extract context elements from the image. These elements become the nodes of an affective graph, which is fed into a Graph Convolution Network (GCN) to encode context. Another problem that has been looked into is group emotion recognition  [19, 53] . The objective here is to label the emotion of the entire set of people in the frame under the assumption that they all share some social identity.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Context-Aware Emotion Recognition Datasets",
      "text": "Most of the emotion recognition datasets in the past have either only focused on a single modality, e.g., faces or body features, or have been collected in controlled set-tings. For example, the GENKI database  [52]  and the UCD-SEE dataset  [51]  are datasets that focus primarily on the facial expressions collected in lab settings. The Emotion Recognition in the Wild (EmotiW) challenges  [14]  host three databases: AFEW dataset  [15]  (collected from TV shows and movies), SFEW (a subset of AFEW with only face frames annotated), and HAPPEI database, which focuses on the problem of group-level emotion estimation. Some of the recent works have realized the potential of using context for emotion recognition and highlighted the lack of such datasets. Context-Aware Emotion Recognition (CAER) dataset  [58]  is a collection of video-clips from TV shows with 7 discrete emotion annotations. EMOTIC dataset  [27]  is a collection of images from datasets like MSCOCO  [34]  and ADE20K  [61]  along with images downloaded from web searches. The dataset is a collection of 23, 571 images, with about 34, 320 people annotated for 26 discrete emotion classes. We have summarised and compared all these datasets in Table  1 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Our Approach: Emoticon",
      "text": "In this section, we give an overview of the approach in Section 3.1 and motivate the three context interpretations in Section 3.2, 3.3, and 3.4.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Notation And Overview",
      "text": "We present an overview of our context-aware multimodal emotion recognition model, EmotiCon, in Figure  2 . Our input consists of an RGB image, I. We process I to We compare GroupWalk with existing emotion recognition datasets such as EMOTIC  [27] , AffectNet  [41] , CAER and CAER-S  [30] , and AFEW  [15] .\n\ngenerate the input data for each network corresponding to the three contexts. The network for Context 1 consists of n streams corresponding to n distinct modalities denoted as m 1 , m 2 , . . . , m n . Each distinct layer outputs a feature vector, f i . The n feature vectors f 1 , f 2 , . . . , f n are combined via multiplicative fusion  [40]  to obtain a feature encoding,\n\nwhere g(•) corresponds to the multiplicative fusion function. Similarly, h 2 , and h 3 are computed through the networks corresponding to the second and third Contexts. h 1 , h 2 , and h 3 are concatenated to perform multi-label emotion classification.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Context 1: Multiple Modalities",
      "text": "In real life, people appear in a multi-sensory context that includes a voice, a body, and a face; these aspects are also perceived as a whole. Combining more than one modality to infer emotion is beneficial because cues from different modalities can complement each other. They also seem to perform better on in-the-wild datasets  [40]  than other unimodal approaches. Our approach is extendable to any number of modalities available. To validate this claim, other than EMOTIC and GroupWalk, which have two modalities, faces, and gaits, we also show results on the IEMOCAP dataset which face, text, and speech as three modalities. From the input image I, we obtain m 1 , m 2 , . . . , m n using processing steps as explained in Section 4.1. These inputs are then passed through their respective neural network architectures to obtain f 1 , f 2 , . . . , f n . To make our algorithm robust to sensor noise and averse to noisy signals, we combine these features multiplicatively to obtain h 1 . As shown in previous research  [35, 40] , multiplicative fusion learns to emphasize reliable modalities and to rely less on other modalities. To train this, we use the modified loss function proposed previously  [40]  defined as:\n\nwhere n is the total number of modalities being considered, and p e i is the prediction for emotion class, e, given by the network for the i th modality.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Context 2: Situational/Background Context",
      "text": "Our goal is to identify semantic context from images and videos to perform perceived emotion recognition. Semantic context includes the understanding of objects -excluding the primary agent-present in the scene, their spatial extents, keywords, and the activity being performed. For instance, in Figure  1 , the input image consists of a group of people gathered around with drinks on a bright sunny day. The \"bright sunny day\", \"drink glasses\", \"hats\" and \"green meadows\" constitute semantic components and may affect judgement of one's perceived emotion.\n\nMotivated by multiple approaches in the computer vision literature  [59, 18]  surrounding semantic scene understanding, we use an attention mechanism to train a model to focus on different aspects of an image while masking the primary agent, to extract the semantic components of the scene. The mask, I mask ∈ R 224×224 , for an input image I is given as\n\nwhere bbox agent denotes the bounding box of the agent in the scene.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Context 3: Inter-Agent Interactions/Socio-Dynamic Context",
      "text": "When an agent is surrounded by other agents, their perceived emotions change. When other agents share an identity or are known to the agent, they often coordinate their behaviors. This varies when other agents are strangers. Such interactions and proximity can help us infer the emotion of agents better.\n\nPrior experimental research has used walking speed, distance, and proximity features to model socio-dynamic interactions between agents to interpret their personality traits. Some of these algorithms, like the social force model  [23] , are based on the assumption that pedestrians are subject to attractive or repulsive forces that drive their dynamics. Nonlinear models like RVO  [56]  aim to model collision avoidance among individuals while walking to their individual goals. But, both of these methods do not capture cohesiveness in a group.\n\nWe propose an approach to model these socio-dynamic interactions by computing proximity features using depth maps. The depth map, I depth ∈ R 224×224 , corresponding to input image, I, is represented through a 2D matrix where,\n\nd(I(i, j), c) represents the distance of the pixel at the i th row and j th column from the camera center, c. We pass I depth as input depth maps through a CNN and obtain h 3 .\n\nIn addition to depth map-based representation, we also use Graph Convolutional Networks (GCNs) to model the proximity-based socio-dynamic interactions between agents. GCNs have been used to model similar interactions in traffic networks  [22]  and activity recognition  [55] . The input to a GCN network consists of the spatial coordinates of all agents, denoted by X ∈ R n×2 , where n represents the number of agents in the image, as well as the unweighted adjacency matrix, A ∈ R n×n , of the agents, which is defined as follows,\n\nThe function f = e -d(vi,vj )  [7]  denotes the interactions between any two agents.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Network Architecture And Implementation Details",
      "text": "In this section, we elaborate on the implementation and network architectures of EmotiCon. The data preprocessing for the streams of EmotiCon are presented in 4.1. We include details about the network architectures of context 1, context 2, and context 3 in Section 4.2. We explain the early fusion technique we use to fuse the features from the three context streams to infer emotion and the loss function used for training the multi-label classification problem.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Data Processing",
      "text": "Context1: We use OpenFace  [4]  to extract a 144dimensional face modality vector, m 1 ∈ R 144 obtained through multiple facial landmarks. We compute the 2D gait modality vectors, m 2 ∈ R 25×2 using OpenPose  [10]  to extract 25-coordinates from the input image I. For each coordinate, we record the x and y pixel values.\n\nContext2: We use RobustTP  [11] , which is a pedestrian tracking method to compute the bounding boxes for all agents in the scene. These bounding boxes are used to compute I mask according to Equation  2 .\n\nContext3: We use Megadepth  [33]  to extract the depth maps from the input image I. The depth map, I depth , is computed using Equation 3.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Network Architecture",
      "text": "Context1: Given a face vector, m 1 , we use three 1D convolutions (depicted in light green color in Figure  2 ) with batch normalization and ReLU non-linearity. This is followed by a max pool operation and three fully-connected layers (cyan color in Figure  2 ) with batch normalization and ReLU. For m 2 , we use the ST-GCN architecture proposed by  [8] , which is currently the SOTA network for emotion classification using gaits. Their method was originally designed to deal with 2D pose information for 16 body joints. We modify their setup for 2D pose inputs for 25 joints. We show the different layers and hyper-parameters used in Figure  2 . The two networks give us f 1 and f 2 , which are then multiplicatively fused (depicted in red color in Figure  2 ) to generate h 1 .\n\nContext 2: For learning the semantic context of the input image I, we use the Attention Branch Network (ABN)  [18]  on the masked image I mask . ABN contains an attention branch which focuses on attention maps to recognize and localize important regions in an image. It outputs these potentially important locations in the form of h 2 .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Context 3:",
      "text": "We perform two experiments using both depth map and a GCN. For depth-based network, we compute the depth map, I depth and pass it through a CNN. The CNN is composed of 5 alternating 2D convolutional layers (depicted in dark green color in Figure  2 ) and max pooling layers (magenta color in Figure  2 ). This is followed by two fully connected layers of dimensions 1000 and 26 (cyan color in Figure  2 ).\n\nFor the graph-based network, we use two graph convolutional layers followed by two linear layers of dimension 100 and 26.\n\nFusing Context Interpretations: To fuse the feature vectors from the three context interpretations, we use an early fusion technique. We concatenate the feature vectors before making any individual emotion inferences.\n\nWe use two fully connected layers of dimensions 52 and 26, followed by a softmax layer. This output is used for computing the loss and the error, and then back-propagating the error back to the network.\n\nLoss Function: Our classification problem is a multi-label classification problem where we assign one or more than one emotion label to an input image or video. To train this network, we use the multi-label soft margin loss function and denote it by L classification . The loss function optimizes a multi-label one-versus-all loss based on max-entropy between the input x and output y.\n\nSo, we combine the two loss functions, L multiplicative (from Eq. 1) and L classification to train EmotiCon.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Datasets",
      "text": "In Section 5.1, we give details about the benchmark dataset for context-aware emotion recognition, EMOTIC. We present details about the new dataset, GroupWalk and also perform a comparison with other existing datasets in Section 5.2. Like summarised in Table  1 , there are a lot more datasets for emotion recognition, but they do not have any context available. Though our approach will work on these datasets, we do not expect any significant improvement over the SOTA on these datasets. Just to reinforce this, we did run our method on IEMOCAP  [9] , which has limited context information, and summarise our results in Appendix B.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Emotic Dataset",
      "text": "The EMOTIC dataset contains 23,571 images of 34,320 annotated people in unconstrained environments. The annotations consist of the apparent emotional states of the people in the images. Each person is annotated for 26 discrete categories, with multiple labels assigned to each image.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Groupwalk Dataset",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Annotation",
      "text": "GroupWalk consists of 45 videos that were captured using stationary cameras in 8 real-world setting including a hospital entrance, an institutional building, a bus stop, a train station, and a marketplace, a tourist attraction, a shopping place and more. The annotators annotated agents with clearly visible faces and gaits across all videos. 10 annotators annotated a total of 3544 agents. The annotations consist of the following emotion labels-Angry, Happy, Neutral, and Sad. Efforts to build on this dataset are still ongoing. The dataset collected and annotated so far can be found at the Project webpage. To prepare train and test splits for the dataset, we randomly selected 36 videos for the training and 9 videos for testing.\n\nWhile perceived emotions are essential, other affects such as dominance and friendliness are important for carrying out joint and/or group tasks. Thus, we additionally label each agent for dominance and friendliness. More details about the annotation process, labelers and labels processing are presented in Appendix A.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiments And Results",
      "text": "In this section, we discuss the experiments conducted for EmotiCon. We present details on hyperparameters and training details in Section 6.1. In section 6.2, we list the prior methods we compare the performance of Emoti-Con with. We present an elaborate analysis of both qualitative and quantitative results in Section 6.3. In Section 6.5, we perform experiments to validate the importance of each component of EmotiCon.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Training Details",
      "text": "For training EmotiCon on the EMOTIC dataset, we use the standard train, val, and test split ratios provided in the dataset. For GroupWalk, we split the dataset into 85% training (85%) and testing (15%) sets. In GroupWalk each sample point is an agent ID; hence the input is all the frames for the agent in the video. To extend EmotiCon on videos, we perform a forward pass for all the frames and take an average of the prediction vector across all the frames and then compute the AP scores and use this for loss calculation and backpropagating the loss. We use a batch size of 32 for EMOTIC and a batchsize of 1 for GroupWalk. We train EmotiCon for 75 epochs. We use the Adam optimizer with a learning rate of 0.0001. All our results were generated on NVIDIA GeForce GTX 1080 Ti GPU. All the code was implemented using PyTorch  [43] .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Evaluation Metrics And Methods",
      "text": "We use the standard metric Average Precision (AP) to evaluate all our methods. For both EMOTIC and Group-Walk datasets, we compare our methods with the following SOTA methods.\n\n1. Kosti et al.  [27]  propose a two-stream network followed by a fusion network. The first stream encodes context and then feeds the entire image as an input to the CNN. The second stream is a CNN for extracting body features. The fusion network combines features of the two CNNs and estimates the discrete emotion categories. 2. Zhang et al.  [58]  build an affective graph with nodes as the context elements extracted from the image.\n\nTo detect the context elements, they use a Region Proposal Network (RPN). This graph is fed into a Graph Convolutional Network (GCN). Another parallel branch in the network encodes the body features using a CNN. The outputs from both the branches are concatenated to infer an emotion label. 3. Lee et al.  [30]  present a network architecture, CAER-Net consisting of two subnetworks, a two-stream encoding network, and an adaptive fusion network. The two-stream encoding network consists of a face stream and a context-stream where facial expression and context (background) are encoded. An adaptive fusion network is used to fuse the two streams.\n\nWe use the publicly available implementation for Kosti et al.  [27]  and train the entire model on GroupWalk. Both Zhang et al.  [58]  and Lee et al.  [30]  do not have publicly available implementations. We reproduce the method by Lee et al.  [30]  to the best of our understanding. For Zhang et al.  [58] , while we report their performance on the EMOTIC dataset, with limited implementation details, it was difficult to build their model to test their performance on Group-Walk.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Analysis And Discussion",
      "text": "Comparison with SOTA: We summarize the evaluation of the APs for all the methods on the EMOTIC and Group-Walk datasets in Table  2 . For EmotiCon, we report the AP scores for both GCN-based and Depth Map-based implementations of Context 3. On both the EMOTIC and Group-Walk datasets, EmotiCon outperforms the SOTA. Generalize to more Modalities: A major factor for the success of EmotiCon is its ability to combine different (a) AP Scores for EMOTIC Dataset. We report the AP scores on the EMOTIC and the GroupWalk datasets. Emoti-Con outperforms all the three methods for most of the classes and also overall.\n\nmodalities effectively via multiplicative fusion. Our approach learns to assign higher weights to more expressive modalities while suppressing weaker ones. For example, in instances where the face may not be visible, Emoti-Coninfers the emotion from context (See Figure  3 , middle row(right)). This is in contrast to Lee et al.  [30] , which relies on the availability of face data. Consequently, they perform poorly on both the EMOTIC and GroupWalk datasets, as both datasets contain many examples where the face is not visible clearly. To further demonstrate the ability of EmotiCon to generalize to any modality, we additionally report our performance on the IEMOCAP dataset  [9]  in Appendix B.\n\nGCN versus Depth Maps: GCN-based methods do not perform as well as depth-based but are a close second. This may be due to the fact that, on average most images of the EMOTIC dataset contain 5 agents. GCN-based methods in the literature have been trained on datasets with a lot more number of agents in each image or video. Moreover, with a depth-based approach, EmotiCon leans a 3D aspect of the scene in general and is not limited to inter-agent interactions.\n\nFailure Cases: We show two examples from EMOTIC dataset in Figure  4  where EmotiCon fails to classify correctly. We also show the ground-truth and predicted emotion labels. In the first image, EmotiConis unable to gather any context information. On the other hand, in the second image, there is a lot of context information like the many visual elements in the image and multiple agents. This leads to an incorrect inference of the perceived emotion.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Qualitative Results",
      "text": "We show qualitative results for three examples, each from both the datasets, respectively, in Figure  3 . The first column is the input image marking the primary agents, the second column shows the corresponding extracted face and gait, the third column shows the attention maps learned by  the model, and lastly, in the fourth column, we show the depth map extracted from the input image.\n\nThe heatmaps in the attention maps indicate what the network has learned. In the bottom row (left) and bottom row (middle) examples, the semantic context of the coffin and the child's kite is clearly identified to convey sadness and pleasure, respectively. The depth maps corresponding to the input images capture the idea of proximity and interagent interactions. In the top row example (left) and middle row example (right), the depth map clearly marks the tennis player about to swing to convey anticipation, and the woman coming from the hospital to convey sadness, respectively.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ablation Experiments",
      "text": "To motivate the importance of Context 2 and Context 3, we run EmotiCon on both EMOTIC and GroupWalk dataset removing the networks corresponding to both contexts, followed by removing either of them one by one. The results of the ablation experiments have been summarized in Table  3 . We choose to retain Context 1 in all these runs because it is only Context 1 that is capturing information from the agent itself.\n\nWe observe from the qualitative results in Figure  3  that Context 2 seems more expressive in the images of EMOTIC dataset, while Context 3 is more representative in Group-Walk. This is supported by the results reported in Table  3 , columns 2 and 3. To understand why this happens, we analyse the two datasets closely. EMOTIC dataset was collected for the task of emotion recognition with context. it is a dataset of pictures collected from multiple datasets and scraped from the Internet. As a result, most of these images have a rich background context. Moreover we also found that more than half the images of EMOTIC contain at most 3 people. These are the reasons we believe that interpretation 2 helps more in EMOTIC than interpretation 3. In the GroupWalk Dataset, the opposite is true. The number of people per frame is much higher. This density gets captured best in interpretation 3 helping the network to make the better inference.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion, Limitations, And Future Work",
      "text": "We present EmotiCon, a context-aware emotion recognition system that borrows and incorporates the context interpretations from psychology. We use multiple modalities (faces and gaits), situational context, and also the sociodynamic context information. We make an effort to use easily available modalities that can be easily captured or extracted using commodity hardware (e.g., cameras). To foster more research on emotion recognition with naturalistic modalities, we also release a new dataset called GroupWalk. Our model has limitations and often confuses between certain class labels. Further, we currently perform multi-class classification over discrete emotion labels. In the future, we would also like to move towards the continuous model of emotions (Valence, Arousal, and Dominance). As part of future work, we would also explore more such context interpretations to improve the accuracies.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A. Groupwalk A.1. Annotation Procedure",
      "text": "We present the human annotated GroupWalk data set which consists of 45 videos captured using stationary cameras in 8 real-world setting including a hospital entrance, an institutional building, a bus stop, a train station, and a marketplace, a tourist attraction, a shopping place and more. 10 annotators annotated 3544 agents with clearly visible faces and gaits across all videos. They were allowed to view the videos as many times as they wanted and had to categorise the emotion they perceived looking at the agent into 7 categories -\"Somewhat Happy\", \"Extremely Happy\", \"Somewhat Sad\", Extremely Sad\", \"Somewhat Angry\", \"Extremely Angry\", \"Neutral\". In addition to perceived emotions, the annotators were also asked to annotate the agents in terms of dominance (5 categories-\"Somewhat Submissive\", \"Extremely Submissive\", \"Somewhat Dominant\", \"Extremely Dominant\", \"Neutral\" ) and friendliness (5 categories-\"Somewhat Friendly\", \"Extremely Friendly\", \"Somewhat Unfriendly\", \"Extremely Unfriendly\", \"Neutral\"). Attempts to build the dataset are still ongoing.\n\nFor the sake of completeness, we show the friendliness label distribution and dominance label distribution for every annotator in Figure  7  and Figure  8  respectively.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "A.2. Labels Processing",
      "text": "4 major labels that have been considered are Angry, Happy, Neutral and Sad. As described in Section A.1, one can observe that the annotations are either \"Extreme\" or \"Somewhat\" variants of these major labels (except Neutral). Target labels were now generated for each agent. Each of them are of the size 1 x 4 with the 4 columns representing the 4 emotions being considered and are initially all 0. For a particular agent id, if the annotation by an annotator was an \"Extreme\" variant of Happy, Sad or Angry, 2 was added to the number in the column representing the corresponding major label. Otherwise for all the other cases, 1 was added to the number in the column representing the corresponding major label. Once we have gone through the entire dataset, we normalize the target label vector so that vector is a combination of only 1s and 0s.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "A.3. Analysis",
      "text": "We show the emotion label distribution for every annotator in Figure  5 . To understand the trend of annotator agreement and disagreement across the 10 annotators, we gather agents labeled similarly in majority (more than 50% of annotators annotated the agent with the same labels) and then study the classes they were confused most with. We show this pictorially for two classes Happy and Sad in Figure  6 . For instance, we see that Happy and Sad labels are often confused with label Neutral. In addition, we also show the label distributions for every annotator for Friendliness as well as Dominance in Figure  7  and Figure  8  respectively.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "B. Emoticon On Iemocap Dataset",
      "text": "To validate that EmotiCon can be generalised for any number of modalities, we report our performance on IEMO-CAP  [9]  in Table  4 . IEMOCAP dataset consists of speech, text and face modalities of 10 actors recorded in the form of conversations (both spontaneous and scripted) using a Motion Capture Camera. The labeled annotations consist of 4 emotions -angry, happy, neutral, and sad. This is a singlelabel classification as opposed to multi-label classification we reported for EMOTIC and GroupWalk. Because of this we choose to report mean classification accuracies rather than AP scores. Most prior work which have shown results on IEMOCAP dataset, report mean classification accuracies too.  As can be seen from the Table  4 , there is not a significant improvement in the accuracy, 84.5% as SOTA works, not essentially based on context have reported an accuracy of 82.7%. We believe that the controlled settings in which the dataset is collected, with minimal context information re-sults in not huge improvements. Moreover we also see that prior works in context, Kosti et al.  [27]  and Lee et al.  [58]  sort of do not get any context to learn from and hence do not perform so well. Even EmotiCon's performance is a result of incorporating modalities, with small contribution from context.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Context-Aware Multimodal Emotion Recognition:",
      "page": 1
    },
    {
      "caption": "Figure 2: EmotiCon: We use three interpretations of context. We ﬁrst extract features for the two modalities to obtain f1 and f2 and",
      "page": 3
    },
    {
      "caption": "Figure 2: Our input consists of an RGB image, I. We process I to",
      "page": 3
    },
    {
      "caption": "Figure 1: , the input image consists of a group of people gath-",
      "page": 4
    },
    {
      "caption": "Figure 2: ) with batch normalization and",
      "page": 5
    },
    {
      "caption": "Figure 2: The two networks give us f1 and f2,",
      "page": 5
    },
    {
      "caption": "Figure 2: ) to generate h1.",
      "page": 5
    },
    {
      "caption": "Figure 2: ). This is followed",
      "page": 5
    },
    {
      "caption": "Figure 3: Qualitative Results: We show the classiﬁcation results on three examples, each from the EMOTIC dataset (left) and Group-",
      "page": 7
    },
    {
      "caption": "Figure 4: where EmotiCon fails to classify cor-",
      "page": 7
    },
    {
      "caption": "Figure 3: . The ﬁrst",
      "page": 7
    },
    {
      "caption": "Figure 4: Misclassiﬁcation by EmotiCon: We show two exam-",
      "page": 8
    },
    {
      "caption": "Figure 7: and Figure 8 respectively.",
      "page": 11
    },
    {
      "caption": "Figure 5: To understand the trend of annotator agree-",
      "page": 11
    },
    {
      "caption": "Figure 6: For instance, we see that Happy and Sad labels are often",
      "page": 11
    },
    {
      "caption": "Figure 7: and Figure 8 respectively.",
      "page": 11
    },
    {
      "caption": "Figure 5: Annotator Annotations of GroupWalkDataset: We",
      "page": 11
    },
    {
      "caption": "Figure 6: Annotator Agreement/Disagreement: For two emo-",
      "page": 11
    },
    {
      "caption": "Figure 7: Friendliness Labeler Annotations: We depict the",
      "page": 12
    },
    {
      "caption": "Figure 8: Dominance Labeler Annotations: We depict the domi-",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Data type\nDataset\nDataset Size\nAgents Annotated\nSetting\nEmotion Labels\nContext": "EMOTIC [27]\n18,316 images\n34,320\nWeb\n26 Categories\nYes\nAffectNet [41]\n450,000 images\n450,000\nWeb\n8 Categories\nNo\nImages\nCAER-S [30]\n70,000 images\n70,000\nTV Shows\n7 Categories\nYes"
        },
        {
          "Data type\nDataset\nDataset Size\nAgents Annotated\nSetting\nEmotion Labels\nContext": "AFEW [15]\n1,809 clips\n1,809\nMovie\n7 Categories\nNo\nCAER [30]\n13,201 clips\n13,201\nTV Show\n7 Categories\nYes\nVideos\nIEMOCAP [9]\n12 hrs\n-\nTV Show\n4 Categories\nYes\nGroupWalk\n45 clips(10 mins each)\n3544\nReal Settings\n4 Categories\nYes"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: Emotion Classification Performance: We report the toanincorrectinferenceoftheperceivedemotion.",
      "data": [
        {
          "Labels": "Affection",
          "Kosti et al.[27]": "27.85",
          "Zhang et al.[58]": "46.89",
          "Lee et al.[30]": "19.9",
          "EmotiCon\nGCN-Based\nDepth-Based": "45.23\n36.78"
        },
        {
          "Labels": "Anger",
          "Kosti et al.[27]": "09.49",
          "Zhang et al.[58]": "10.87",
          "Lee et al.[30]": "11.5",
          "EmotiCon\nGCN-Based\nDepth-Based": "15.46\n14.92"
        },
        {
          "Labels": "Annoyance",
          "Kosti et al.[27]": "14.06",
          "Zhang et al.[58]": "11.23",
          "Lee et al.[30]": "16.4",
          "EmotiCon\nGCN-Based\nDepth-Based": "21.92\n18.45"
        },
        {
          "Labels": "Anticipation",
          "Kosti et al.[27]": "58.64",
          "Zhang et al.[58]": "62.64",
          "Lee et al.[30]": "53.05",
          "EmotiCon\nGCN-Based\nDepth-Based": "72.12\n68.12"
        },
        {
          "Labels": "Aversion",
          "Kosti et al.[27]": "07.48",
          "Zhang et al.[58]": "5.93",
          "Lee et al.[30]": "16.2",
          "EmotiCon\nGCN-Based\nDepth-Based": "17.81\n16.48"
        },
        {
          "Labels": "Conﬁdence",
          "Kosti et al.[27]": "78.35",
          "Zhang et al.[58]": "72.49",
          "Lee et al.[30]": "32.34",
          "EmotiCon\nGCN-Based\nDepth-Based": "68.65\n59.23"
        },
        {
          "Labels": "Disapproval",
          "Kosti et al.[27]": "14.97",
          "Zhang et al.[58]": "11.28",
          "Lee et al.[30]": "16.04",
          "EmotiCon\nGCN-Based\nDepth-Based": "21.21\n19.82"
        },
        {
          "Labels": "Disconnection",
          "Kosti et al.[27]": "21.32",
          "Zhang et al.[58]": "26.91",
          "Lee et al.[30]": "22.80",
          "EmotiCon\nGCN-Based\nDepth-Based": "43.12\n25.17"
        },
        {
          "Labels": "Disquietment",
          "Kosti et al.[27]": "16.89",
          "Zhang et al.[58]": "16.94",
          "Lee et al.[30]": "17.19",
          "EmotiCon\nGCN-Based\nDepth-Based": "18.73\n16.41"
        },
        {
          "Labels": "Doubt/Confusion",
          "Kosti et al.[27]": "29.63",
          "Zhang et al.[58]": "18.68",
          "Lee et al.[30]": "28.98",
          "EmotiCon\nGCN-Based\nDepth-Based": "35.12\n33.15"
        },
        {
          "Labels": "Embarrassment",
          "Kosti et al.[27]": "03.18",
          "Zhang et al.[58]": "1.94",
          "Lee et al.[30]": "15.68",
          "EmotiCon\nGCN-Based\nDepth-Based": "14.37\n11.25"
        },
        {
          "Labels": "Engagement",
          "Kosti et al.[27]": "87.53",
          "Zhang et al.[58]": "88.56",
          "Lee et al.[30]": "46.58",
          "EmotiCon\nGCN-Based\nDepth-Based": "91.12\n90.45"
        },
        {
          "Labels": "Esteem",
          "Kosti et al.[27]": "17.73",
          "Zhang et al.[58]": "13.33",
          "Lee et al.[30]": "19.26",
          "EmotiCon\nGCN-Based\nDepth-Based": "23.62\n22.23"
        },
        {
          "Labels": "Excitement",
          "Kosti et al.[27]": "77.16",
          "Zhang et al.[58]": "71.89",
          "Lee et al.[30]": "35.26",
          "EmotiCon\nGCN-Based\nDepth-Based": "83.26\n82.21"
        },
        {
          "Labels": "Fatigue",
          "Kosti et al.[27]": "09.70",
          "Zhang et al.[58]": "13.26",
          "Lee et al.[30]": "13.04",
          "EmotiCon\nGCN-Based\nDepth-Based": "19.15\n16.23"
        },
        {
          "Labels": "Fear",
          "Kosti et al.[27]": "14.14",
          "Zhang et al.[58]": "4.21",
          "Lee et al.[30]": "10.41",
          "EmotiCon\nGCN-Based\nDepth-Based": "23.65\n11.32"
        },
        {
          "Labels": "Happiness",
          "Kosti et al.[27]": "58.26",
          "Zhang et al.[58]": "73.26",
          "Lee et al.[30]": "49.36",
          "EmotiCon\nGCN-Based\nDepth-Based": "74.71\n68.21"
        },
        {
          "Labels": "Pain",
          "Kosti et al.[27]": "08.94",
          "Zhang et al.[58]": "6.52",
          "Lee et al.[30]": "10.36",
          "EmotiCon\nGCN-Based\nDepth-Based": "13.21\n12.54"
        },
        {
          "Labels": "Peace",
          "Kosti et al.[27]": "21.56",
          "Zhang et al.[58]": "32.85",
          "Lee et al.[30]": "16.72",
          "EmotiCon\nGCN-Based\nDepth-Based": "35.14\n34.27"
        },
        {
          "Labels": "Pleasure",
          "Kosti et al.[27]": "45.46",
          "Zhang et al.[58]": "57.46",
          "Lee et al.[30]": "19.47",
          "EmotiCon\nGCN-Based\nDepth-Based": "65.53\n61.34"
        },
        {
          "Labels": "Sadness",
          "Kosti et al.[27]": "19.66",
          "Zhang et al.[58]": "25.42",
          "Lee et al.[30]": "11.45",
          "EmotiCon\nGCN-Based\nDepth-Based": "26.15\n23.41"
        },
        {
          "Labels": "Sensitivity",
          "Kosti et al.[27]": "09.28",
          "Zhang et al.[58]": "5.99",
          "Lee et al.[30]": "10.34",
          "EmotiCon\nGCN-Based\nDepth-Based": "9.21\n8.32"
        },
        {
          "Labels": "Suffering",
          "Kosti et al.[27]": "18.84",
          "Zhang et al.[58]": "23.39",
          "Lee et al.[30]": "11.68",
          "EmotiCon\nGCN-Based\nDepth-Based": "26.39\n22.81"
        },
        {
          "Labels": "Surprise",
          "Kosti et al.[27]": "18.81",
          "Zhang et al.[58]": "9.02",
          "Lee et al.[30]": "10.92",
          "EmotiCon\nGCN-Based\nDepth-Based": "14.21\n17.37"
        },
        {
          "Labels": "Sympathy",
          "Kosti et al.[27]": "14.71",
          "Zhang et al.[58]": "17.53",
          "Lee et al.[30]": "17.125",
          "EmotiCon\nGCN-Based\nDepth-Based": "34.28\n24.63"
        },
        {
          "Labels": "Yearning",
          "Kosti et al.[27]": "08.34",
          "Zhang et al.[58]": "10.55",
          "Lee et al.[30]": "9.79",
          "EmotiCon\nGCN-Based\nDepth-Based": "14.29\n12.23"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Labels": "",
          "Context Interpretations": "Only 1"
        },
        {
          "Labels": "Affection",
          "Context Interpretations": "29.87"
        },
        {
          "Labels": "Anger",
          "Context Interpretations": "08.52"
        },
        {
          "Labels": "Annoyance",
          "Context Interpretations": "09.65"
        },
        {
          "Labels": "Anticipation",
          "Context Interpretations": "46.23"
        },
        {
          "Labels": "Aversion",
          "Context Interpretations": "06.27"
        },
        {
          "Labels": "Conﬁdence",
          "Context Interpretations": "51.92"
        },
        {
          "Labels": "Disapproval",
          "Context Interpretations": "11.81"
        },
        {
          "Labels": "Disconnection",
          "Context Interpretations": "31.74"
        },
        {
          "Labels": "Disquietment",
          "Context Interpretations": "07.57"
        },
        {
          "Labels": "Doubt/Confusion",
          "Context Interpretations": "21.62"
        },
        {
          "Labels": "Embarrassment",
          "Context Interpretations": "08.43"
        },
        {
          "Labels": "Engagement",
          "Context Interpretations": "78.68"
        },
        {
          "Labels": "Esteem",
          "Context Interpretations": "18.32"
        },
        {
          "Labels": "Excitement",
          "Context Interpretations": "73.19"
        },
        {
          "Labels": "Fatigue",
          "Context Interpretations": "06.34"
        },
        {
          "Labels": "Fear",
          "Context Interpretations": "14.29"
        },
        {
          "Labels": "Happiness",
          "Context Interpretations": "52.52"
        },
        {
          "Labels": "Pain",
          "Context Interpretations": "05.75"
        },
        {
          "Labels": "Peace",
          "Context Interpretations": "13.53"
        },
        {
          "Labels": "Pleasure",
          "Context Interpretations": "58.26"
        },
        {
          "Labels": "Sadness",
          "Context Interpretations": "19.94"
        },
        {
          "Labels": "Sensitivity",
          "Context Interpretations": "03.16"
        },
        {
          "Labels": "Suffering",
          "Context Interpretations": "15.38"
        },
        {
          "Labels": "Surprise",
          "Context Interpretations": "05.29"
        },
        {
          "Labels": "Sympathy",
          "Context Interpretations": "22.38"
        },
        {
          "Labels": "Yearning",
          "Context Interpretations": "04.94"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Facial emotion recognition for intelligent tutoring environment",
      "authors": [
        "Kingsley Oryina Akputu",
        "Kah Phooi Seng",
        "Yun Li"
      ],
      "year": "2013",
      "venue": "IMLCS"
    },
    {
      "citation_id": "2",
      "title": "The future of emotion regulation research: Capturing context",
      "authors": [
        "Amelia Aldao"
      ],
      "year": "2013",
      "venue": "Perspectives on Psychological Science"
    },
    {
      "citation_id": "3",
      "title": "Body cues, not facial expressions, discriminate between intense positive and negative emotions",
      "authors": [
        "Aviezer",
        "Trope"
      ],
      "year": "2012",
      "venue": "Body cues, not facial expressions, discriminate between intense positive and negative emotions"
    },
    {
      "citation_id": "4",
      "title": "Openface: an open source facial behavior analysis toolkit",
      "authors": [
        "Tadas Baltrušaitis",
        "Peter Robinson",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "2016 IEEE Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "5",
      "title": "Context in emotion perception. Current Directions in Psychological",
      "authors": [
        "Lisa Feldman",
        "Batja Mesquita",
        "Maria Gendron"
      ],
      "year": "2011",
      "venue": "Science"
    },
    {
      "citation_id": "6",
      "title": "The context principle. The mind in context, 1",
      "authors": [
        "Lisa Feldman",
        "Batja Mesquita",
        "Eliot Smith"
      ],
      "year": "2010",
      "venue": "The context principle. The mind in context, 1"
    },
    {
      "citation_id": "7",
      "title": "Laplacian eigenmaps for dimensionality reduction and data representation",
      "authors": [
        "Mikhail Belkin",
        "Partha Niyogi"
      ],
      "year": "2003",
      "venue": "Neural computation"
    },
    {
      "citation_id": "8",
      "title": "Tanmay Randhavane, Aniket Bera, and Dinesh Manocha. Step: Spatial temporal graph convolutional networks for emotion perception from gaits",
      "authors": [
        "Uttaran Bhattacharya",
        "Trisha Mittal",
        "Rohan Chandra"
      ],
      "year": "2019",
      "venue": "Tanmay Randhavane, Aniket Bera, and Dinesh Manocha. Step: Spatial temporal graph convolutional networks for emotion perception from gaits",
      "arxiv": "arXiv:1910.12906"
    },
    {
      "citation_id": "9",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "10",
      "title": "Openpose: realtime multi-person 2d pose estimation using part affinity fields",
      "authors": [
        "Zhe Cao",
        "Gines Hidalgo",
        "Tomas Simon",
        "Shih-En Wei",
        "Yaser Sheikh"
      ],
      "year": "2018",
      "venue": "Openpose: realtime multi-person 2d pose estimation using part affinity fields",
      "arxiv": "arXiv:1812.08008"
    },
    {
      "citation_id": "11",
      "title": "Robusttp: End-to-end trajectory prediction for heterogeneous road-agents in dense traffic with noisy sensor inputs",
      "authors": [
        "Rohan Chandra",
        "Uttaran Bhattacharya",
        "Christian Roncal",
        "Aniket Bera",
        "Dinesh Manocha"
      ],
      "year": "2019",
      "venue": "ACM Computer Science in Cars Symposium"
    },
    {
      "citation_id": "12",
      "title": "Fear-type emotion recognition for future audio-based surveillance systems",
      "authors": [
        "Chlo Clavel",
        "Ioana Vasilescu",
        "Laurence Devillers",
        "Gal Richard",
        "Thibaut Ehrette"
      ],
      "year": "2008",
      "venue": "Fear-type emotion recognition for future audio-based surveillance systems"
    },
    {
      "citation_id": "13",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "Roddy Cowie",
        "Ellen Douglas-Cowie",
        "Nicolas Tsapatsoulis",
        "George Votsis",
        "Stefanos Kollias",
        "Winfried Fellenz",
        "J Taylor"
      ],
      "year": "2001",
      "venue": "SP Magazine, IEEE"
    },
    {
      "citation_id": "14",
      "title": "Emotiw 2016: Video and group-level emotion recognition challenges",
      "authors": [
        "Abhinav Dhall",
        "Roland Goecke",
        "Jyoti Joshi",
        "Jesse Hoey",
        "Tom Gedeon"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "15",
      "title": "Collecting large, richly annotated facial-expression databases from movies",
      "authors": [
        "Abhinav Dhall",
        "Roland Goecke",
        "Simon Lucey",
        "Tom Gedeon"
      ],
      "year": "2012",
      "venue": "IEEE multimedia"
    },
    {
      "citation_id": "16",
      "title": "Head and body cues in the judgment of emotion: A reformulation",
      "authors": [
        "Paul Ekman",
        "Wallace Friesen"
      ],
      "year": "1967",
      "venue": "Perceptual and motor skills"
    },
    {
      "citation_id": "17",
      "title": "Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild",
      "authors": [
        "Fabian Benitez-Quiroz",
        "Ramprakash Srinivasan",
        "Aleix Martinez"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "18",
      "title": "Attention branch network: Learning of attention mechanism for visual explanation",
      "authors": [
        "Hiroshi Fukui",
        "Tsubasa Hirakawa",
        "Takayoshi Yamashita",
        "Hironobu Fujiyoshi"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "19",
      "title": "Group emotion recognition using machine learning",
      "authors": [
        "Samanyou Garg"
      ],
      "year": "2019",
      "venue": "Group emotion recognition using machine learning"
    },
    {
      "citation_id": "20",
      "title": "Context is everything (in emotion research). Social and Personality Psychology Compass",
      "authors": [
        "Katharine Greenaway",
        "Elise Kalokerinos",
        "Lisa Williams"
      ],
      "year": "2018",
      "venue": "Context is everything (in emotion research). Social and Personality Psychology Compass"
    },
    {
      "citation_id": "21",
      "title": "Bi-modal emotion recognition from expressive face and body gestures",
      "authors": [
        "Hatice Gunes",
        "Massimo Piccardi"
      ],
      "year": "2007",
      "venue": "Journal of Network and Computer Applications"
    },
    {
      "citation_id": "22",
      "title": "Attention based spatial-temporal graph convolutional networks for traffic flow forecasting",
      "authors": [
        "Shengnan Guo",
        "Youfang Lin",
        "Ning Feng",
        "Chao Song",
        "Huaiyu Wan"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "23",
      "title": "Social force model for pedestrian dynamics",
      "authors": [
        "Dirk Helbing",
        "Peter Molnar"
      ],
      "year": "1995",
      "venue": "Physical review E"
    },
    {
      "citation_id": "24",
      "title": "Social context effects on facial activity in a negative emotional setting",
      "authors": [
        "Esther Jakobs",
        "Antony Manstead",
        "Agneta Fischer"
      ],
      "year": "2001",
      "venue": "Emotion"
    },
    {
      "citation_id": "25",
      "title": "Affective body expression perception and recognition: A survey",
      "authors": [
        "N Kleinsmith",
        "Bianchi-Berthouze"
      ],
      "year": "2013",
      "venue": "IEEE TAC"
    },
    {
      "citation_id": "26",
      "title": "Physiological signals and their use in augmenting emotion recognition for human-machine interaction",
      "authors": [
        "Benjamin Knapp",
        "Jonghwa Kim",
        "Elisabeth André"
      ],
      "year": "2011",
      "venue": "Emotionoriented systems"
    },
    {
      "citation_id": "27",
      "title": "Context based emotion recognition using emotic dataset",
      "authors": [
        "Ronak Kosti",
        "Jose Alvarez",
        "Adria Recasens",
        "Agata Lapedriza"
      ],
      "year": "2019",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "28",
      "title": "Emotion recognition in context",
      "authors": [
        "Ronak Kosti",
        "Jose Alvarez",
        "Adria Recasens",
        "Agata Lapedriza"
      ],
      "year": "2017",
      "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "29",
      "title": "Evaluations in their social context: Distance regulates consistency and context dependence",
      "authors": [
        "Alison Ledgerwood"
      ],
      "year": "2014",
      "venue": "Social and Personality Psychology Compass"
    },
    {
      "citation_id": "30",
      "title": "Context-aware emotion recognition networks",
      "authors": [
        "Jiyoung Lee",
        "Seungryong Kim",
        "Sunok Kim",
        "Jungin Park",
        "Kwanghoon Sohn"
      ],
      "year": "2019",
      "venue": "Context-aware emotion recognition networks",
      "arxiv": "arXiv:1908.05913"
    },
    {
      "citation_id": "31",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild",
      "authors": [
        "Shan Li",
        "Weihong Deng",
        "Junping Du"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "32",
      "title": "Occlusion aware facial expression recognition using cnn with attention mechanism",
      "authors": [
        "Yong Li",
        "Jiabei Zeng",
        "Shiguang Shan",
        "Xilin Chen"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "33",
      "title": "Megadepth: Learning singleview depth prediction from internet photos",
      "authors": [
        "Zhengqi Li",
        "Noah Snavely"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "34",
      "title": "Microsoft coco: Common objects in context",
      "authors": [
        "Tsung-Yi Lin",
        "Michael Maire",
        "Serge Belongie",
        "James Hays",
        "Pietro Perona",
        "Deva Ramanan",
        "Piotr Dollár",
        "C Lawrence"
      ],
      "year": "2014",
      "venue": "European conference on computer vision"
    },
    {
      "citation_id": "35",
      "title": "Learn to combine modalities in multimodal deep learning",
      "authors": [
        "Kuan Liu",
        "Yanen Li",
        "Ning Xu",
        "Prem Natarajan"
      ],
      "year": "2018",
      "venue": "Learn to combine modalities in multimodal deep learning",
      "arxiv": "arXiv:1805.11730"
    },
    {
      "citation_id": "36",
      "title": "Context may reveal how you feel",
      "authors": [
        "M Aleix",
        "Martinez"
      ],
      "year": "2019",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "37",
      "title": "Beyond positive psychology? toward a contextual view of psychological processes and well-being",
      "authors": [
        "K James",
        "Frank Mcnulty",
        "Fincham"
      ],
      "year": "2012",
      "venue": "American Psychologist"
    },
    {
      "citation_id": "38",
      "title": "Rapid perceptual integration of facial expression and emotional body language",
      "authors": [
        "C Meeren",
        "Van Heijnsbergen"
      ],
      "year": "2005",
      "venue": "PNAS"
    },
    {
      "citation_id": "39",
      "title": "Emotions in context: A sociodynamic model of emotions",
      "authors": [
        "Batja Mesquita",
        "Michael Boiger"
      ],
      "year": "2014",
      "venue": "Emotion Review"
    },
    {
      "citation_id": "40",
      "title": "Aniket Bera, and Dinesh Manocha. M3er: Multiplicative multimodal emotion recognition using facial, textual, and speech cues",
      "authors": [
        "Trisha Mittal",
        "Uttaran Bhattacharya",
        "Rohan Chandra"
      ],
      "year": "2019",
      "venue": "Aniket Bera, and Dinesh Manocha. M3er: Multiplicative multimodal emotion recognition using facial, textual, and speech cues"
    },
    {
      "citation_id": "41",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "Ali Mollahosseini",
        "Behzad Hasani",
        "Mohammad Mahoor"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "42",
      "title": "Individuality in communicative bodily behaviours",
      "authors": [
        "Costanza Navarretta"
      ],
      "year": "2012",
      "venue": "Cognitive Behavioural Systems"
    },
    {
      "citation_id": "43",
      "title": "Automatic differentiation in PyTorch",
      "authors": [
        "Adam Paszke",
        "Sam Gross",
        "Soumith Chintala",
        "Gregory Chanan",
        "Edward Yang",
        "Zachary Devito",
        "Zeming Lin",
        "Alban Desmaison",
        "Luca Antiga",
        "Adam Lerer"
      ],
      "year": "2017",
      "venue": "NIPS Autodiff Workshop"
    },
    {
      "citation_id": "44",
      "title": "Identifying emotions from walking using affective and deep features",
      "authors": [
        "Tanmay Randhavane",
        "Aniket Bera",
        "Kyra Kapsaskis",
        "Uttaran Bhattacharya",
        "Kurt Gray",
        "Dinesh Manocha"
      ],
      "year": "2019",
      "venue": "Identifying emotions from walking using affective and deep features",
      "arxiv": "arXiv:1906.11884"
    },
    {
      "citation_id": "45",
      "title": "The context principle in frege's philosophy",
      "authors": [
        "Michael David"
      ],
      "year": "1967",
      "venue": "Philosophy and Phenomenological Research"
    },
    {
      "citation_id": "46",
      "title": "Face alignment through subspace constrained mean-shifts",
      "authors": [
        "Jason Saragih",
        "Simon Lucey",
        "Jeffrey Cohn"
      ],
      "year": "2009",
      "venue": "ICCV"
    },
    {
      "citation_id": "47",
      "title": "Vocal expression of emotion. Handbook of affective sciences",
      "authors": [
        "Klaus Scherer",
        "Tom Johnstone",
        "Gundrun Klasmeyer"
      ],
      "year": "2003",
      "venue": "Vocal expression of emotion. Handbook of affective sciences"
    },
    {
      "citation_id": "48",
      "title": "Facial expression recognition based on local binary patterns: A comprehensive study",
      "authors": [
        "Caifeng Shan",
        "Shaogang Gong",
        "Peter Mcowan"
      ],
      "year": "2009",
      "venue": "Image and vision Computing"
    },
    {
      "citation_id": "49",
      "title": "Multiple kernel learning for emotion recognition in the wild",
      "authors": [
        "Karan Sikka",
        "Karmen Dykstra",
        "Suchitra Sathyanarayana",
        "Gwen Littlewort",
        "Marian Bartlett"
      ],
      "year": "2013",
      "venue": "ICMI"
    },
    {
      "citation_id": "50",
      "title": "Multiple kernel learning for emotion recognition in the wild",
      "authors": [
        "Karan Sikka",
        "Karmen Dykstra",
        "Suchitra Sathyanarayana",
        "Gwen Littlewort",
        "Marian Bartlett"
      ],
      "year": "2013",
      "venue": "Proceedings of the 15th ACM on International conference on multimodal interaction"
    },
    {
      "citation_id": "51",
      "title": "Development of a facs-verified set of basic and selfconscious emotion expressions",
      "authors": [
        "Jessica L Tracy",
        "Richard Robins",
        "Roberta Schriber"
      ],
      "year": "2009",
      "venue": "Emotion"
    },
    {
      "citation_id": "52",
      "title": "The MPLab GENKI Database",
      "venue": "The MPLab GENKI Database"
    },
    {
      "citation_id": "53",
      "title": "Cascade attention networks for group emotion recognition with face, body and image cues",
      "authors": [
        "Kai Wang",
        "Xiaoxing Zeng",
        "Jianfei Yang",
        "Debin Meng",
        "Kaipeng Zhang",
        "Xiaojiang Peng",
        "Yu Qiao"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM International Conference on Multimodal Interaction, ICMI '18"
    },
    {
      "citation_id": "54",
      "title": "The effects of social interaction and personal relationships on facial expressions",
      "authors": [
        "Kyoko Yamamoto",
        "Naoto Suzuki"
      ],
      "year": "2006",
      "venue": "Journal of Nonverbal Behavior"
    },
    {
      "citation_id": "55",
      "title": "Spatial temporal graph convolutional networks for skeleton-based action recognition",
      "authors": [
        "Sijie Yan",
        "Yuanjun Xiong",
        "Dahua Lin"
      ],
      "year": "2018",
      "venue": "Thirty-Second AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "56",
      "title": "Jur van den Berg, Dinesh Manocha, and Ming Lin. Composite agents",
      "authors": [
        "Hengchin Yeh",
        "Sean Curtis",
        "Sachin Patil"
      ],
      "year": "2008",
      "venue": "Proceedings of the 2008 ACM SIGGRAPH/Eurographics Symposium on Computer Animation"
    },
    {
      "citation_id": "57",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Long Papers)"
    },
    {
      "citation_id": "58",
      "title": "Contextaware affective graph reasoning for emotion recognition",
      "authors": [
        "Minghui Zhang",
        "Yumeng Liang",
        "Huadong Ma"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "59",
      "title": "Image captioning with integrated bottom-up and multi-level residual top-down attention for game scene understanding",
      "authors": [
        "Jian Zheng",
        "Sudha Krishnamurthy",
        "Ruxin Chen",
        "Min-Hung Chen",
        "Zhenhao Ge",
        "Xiaohua Li"
      ],
      "year": "2019",
      "venue": "Image captioning with integrated bottom-up and multi-level residual top-down attention for game scene understanding",
      "arxiv": "arXiv:1906.06632"
    },
    {
      "citation_id": "60",
      "title": "Learning active facial patches for expression analysis",
      "authors": [
        "Lin Zhong",
        "Qingshan Liu",
        "Peng Yang",
        "Bo Liu",
        "Junzhou Huang",
        "Dimitris Metaxas"
      ],
      "year": "2012",
      "venue": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "61",
      "title": "Semantic understanding of scenes through the ade20k dataset",
      "authors": [
        "Bolei Zhou",
        "Hang Zhao",
        "Xavier Puig",
        "Tete Xiao",
        "Sanja Fidler",
        "Adela Barriuso",
        "Antonio Torralba"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    }
  ]
}