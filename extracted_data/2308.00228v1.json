{
  "paper_id": "2308.00228v1",
  "title": "Using Scene And Semantic Features For Multi-Modal Emotion Recognition",
  "published": "2023-08-01T01:54:55Z",
  "authors": [
    "Zhifeng Wang",
    "Ramesh Sankaranarayana"
  ],
  "keywords": [
    "Multi-modal Emotion Recognition",
    "Scene and Semantic Features",
    "Deep learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Automatic emotion recognition is a hot topic with a wide range of applications. Much work has been done in the area of automatic emotion recognition in recent years. The focus has been mainly on using the characteristics of a person such as speech, facial expression and pose for this purpose. However, the processing of scene and semantic features for emotion recognition has had limited exploration. In this paper, we propose to use combined scene and semantic features, along with personal features, for multi-modal emotion recognition. Scene features will describe the environment or context in which the target person is operating. The semantic feature can include objects that are present in the environment, as well as their attributes and relationships with the target person. In addition, we use a modified EmbraceNet to extract features from the images, which is trained to learn both the body and pose features simultaneously. By fusing both body and pose features, the EmbraceNet can improve the accuracy and robustness of the model, particularly when dealing with partially missing data. This is because having both body and pose features provides a more complete representation of the subject in the images, which can help the model to make more accurate predictions even when some parts of body are missing. We demonstrate the efficiency of our method on the benchmark EMOTIC dataset. We report an average precision of 40.39% across the 26 emotion categories, which is a 5% improvement over previous approaches.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "People convey their feelings in daily life, and others respond accordingly. Interpersonal ties between many persons are developed via emotional engagement. In particular, automated emotion evaluation is frequently applied in the fields such as robotics  [1] , education  [3] , marketing  [2] , human-computer interaction  [24] . Emotions can be represented mathematically as discrete or continuous points in the continuous space of emotional dimensions. The points of Valence, Arousal, and Dominance in three-dimensional space can be used to represent emotion states in continuous space. We concentrate on identifying emotional state in discrete emotion space in this research. Human emotions such as anger, sadness, happiness, excitement, which will be handled in distinct states.\n\nEarly research on emotion identification largely used unimodal techniques. The single model relates to speech, gaits, physiological signals, text, body stance, and face expression. The multimodal emotion recognition techniques are then put out after that. Different combinations of a single model were employed in the multimodal emotion recognition techniques to infer emotion states.\n\nContext also plays a crucial part in comprehending human emotion, even though single model and multiple model can extract a individual's traits and increase the precision of inferring human emotion. According to psychology's context theory  [25] , three key components can be identified in the context: focus level 1 local personal factors (such as face, body and pose),focal Level 2 global situational factors (such as scene and semantic information) and focal level 3 cultural factors (friends or strangers). These characteristics, which are given as follows, were combined in our model: First: Three modalities-the face feature extraction network, the body feature extraction network, and the pose feature extraction network-are included in Feature 1 (Local Personal Features). From the input picture, these three modalities will extract regionally specific individual traits. The networks may obtain better inference and offer crucial supplementary information when face or body parts are only partially visible by integrating pose, body, and face modalities. Our experiments demonstrate the importance of local personal characteristics when scene and semantic information are lacking. Second: Scene feature extraction network and semantic feature extraction network are two of the modalities included in Feature 2 (Global Situational Factors).A network for extracting scene characteristics will be used to infer certain scene elements near the agent, such a classroom or kitchen scene.Semantic features are the in-depth interpretations or meanings of certain areas or objects in a picture. Semantic cues that surround the agent such as the sky, grass, river, etc., are features that characterise the environment in which the expression is being done. Semantic characteristics will be more precise in their details while scene features will be more abstract.The semantic features will provide scene characteristics additional information.The agent's emotional states can be impacted by both of them  [25] . Third:Psychology researchers have shown that the proximity and presence of people affect how we feel.When compared to being with friends or alone, being around a stranger will cause your emotions to feel more intense  [26] . When two people share an event, they will feel the same feeling  [27] . The closeness and interaction with other agents will be investigated for emotion recognition in Feature 3 (Inter-Agent Interactions Feature).\n\nOur objective is to investigate how scene and semantic variables around the agent might increase the accuracy of an automated system for evaluating emotions.\n\nMain contribution: We consider using a combination of scene and semantic cues to infer emotion states. Our model receives a picture as its input, and as its output, it produces labels for the classification of numerous emotions. The following three aspects can be used to summarise the contributions in this paper:\n\n• We recommend using a combination of scene and semantic characteristics to infer emotion states. A scene feature extraction model will specifically assist in extracting characteristics from certain scenes surrounding the agent, such as the class scene and kitchen scene.The semantic feature extraction model will assist in extracting semantic characteristics from the region surrounding the agent, such as the lake, trees, and sky. The accuracy of the model can be significantly increased by combining scene and semantic information.\n\n• We employ a modified version of EmbraceNet  [28]  to fuse body and pose characteristics in order to collect body and pose information, which can minimise performance loss from partial data missing.\n\n• Our method performs better than previously published methods in the benchmark datasets, the EMOTIC dataset that was introduced in  [4] . An average precision score of 40.39% is reported on EMOTIC, which has 5% improvement than previous methods  [12]  II. RELATED WORK\n\nIn this section, we will give a brief introduction about previous efforts on using unique feature and multiple features for emotion recognition, psychology research on context-aware emotion recognition, and context-aware emotion recognition using machine learning.\n\nUsing unique feature and multiple features for emotion recognition. In latest years, there has been a significant amount of research on emotion detection. Many earlier attempts use unique characteristics to interpret human feeling such as face  [5] , text  [9] , body and pose  [6] , speech  [10] . The capacity to infer emotions utilising the single feature for emotion identification, however, is constrained. Numerous factors have been shown to be useful in multi-label emotion identification in some research  [11] .Numerous multimodal emotion identification techniques have been presented out in latest years. Ronak et al.  [13]  used context and body modalities to categorise emotions. Mittal et al.  [12]  used a multimodal CNN to classify emotions based on depth, pose, context, and facial characteristic to improve the accuracy in the EMOTIC datasets. The interaction of the agents was discovered using the depth modalities. The other streams use CNNs  [35]  to extract body features. These two streams are combined in the fusion component to infer emotion states.\n\nContext-aware emotion recognition using machine learning. Context-aware emotion recognition network architectures have been explored in recent years. Kosti et al.  [13]  deploy two-stream fusion network architecture for context-aware and face-based modalities for emotion detection, which have shown the benefit of context-aware emotion recognition. In order to benefit from the image's context elements, Filntisis et al.  [14]  and Mittal et al.  [12]  extracted context characteristics for emotion identification using context-aware modalities. But Filntisis et al.  [14]  think of optical flow as the second stream and two RGB spatial streams (context and body) as the first stream. By using average score fusion, the two stream fusion's ultimate outcome was obtained. However, Mittal et al.  [12]  think of the body and face as the first stream and the context as the second. The authors add a third layer to indicate the closeness of the agents and then combine these three streams using an early fusion approach to infer emotional states. They obtained best results in the basic context-based datasets, the EMOTIC datasets. In addition, It was suggested to utilise the modified VGG16 network to extract scene characteristics for emotion identification and the pre-trained Xception network to extract body features  [34]  .\n\nAttention mechanisms. With a new attention-based block, Transformers were first presented for machine translation  [33] . The attention mechanisms are layers of neural networks that may gather data from the whole input sequence  [37] . The primary benefits of attention mechanisms over RNNs on long sequence problems are their technically perfect memory and global calculations. In the disciplines of computer vision, natural language processing, and voice processing, transformers are taking the place of RNNs  [38] .\n\nTransformers were mostly employed in earlier years for natural language processing, and they produced satisfactory accuracy. The benefit of transformers for applications requiring long sequences of data has now been recognised by researchers in computer vision domains. Transformers are now being used in vision -based problems  [39] . Dosovitskiy, Alexey, et al.  [17]  used the transformer to the picture patches and successfully completed the image classification tasks with the best outcome. For object detection challenges, Carion, Nicolas, et al.  [36]  employ the transformer as both encoder and decoder to analyse the connection between the objects and the overall image.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Iii. Our Method",
      "text": "It has both individual traits and features related to the overall context of the image. We suggest utilising a multi-modal network to extract these data, using the diverse features it yields to recognise emotions. Figure  1  illustrates the multi-modal network design.RGB picture is the input. In order to create five separate input data for each modality, we first preprocess the RGB image. Face characteristics f 1 are extracted using the face feature extraction network. Body and pose characteristics are extracted using the network for extracting body and pose information. In order to merge body and posture features into the combined body features f 7 , the revised EmbraceNet  [28]  will be employed. Our model's improved EmbraceNet can successfully stop performance decline brought by by missing or incomplete data. We suggest using scene and semantic information to infer the target person's emotional states.The network for extracting scene characteristics is responsible for this task -f 4 . In order to obtain semantic characteristics, we use the semantic feature extraction network-f 5 . When the target person's face and body traits are only partially visible, the scene and semantic elements can significantly increase the performance of the models. The closeness information f 6 between the agents is to be extracted using the depth CNN network. The following step is the concatenation of f 1 , f 7 , f 4 , f 5 , and f 6 for multi-label emotion recognition.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Local Personal Features (Face, Body And Pose)",
      "text": "Prior research have utilised facial features to determine human emotion  [15] . We use MTCNN  [16]  to get the target subject's facial coordinates. Following that, the facial bounding box is used to crop the target's face. Because the size of the cropped face image is different, we must adjust the face image to fit. The Resnet-101 backbone network is loaded with three channels of input pictures. Research from the previous demonstrates that it is useful for recognising emotions when it comes to body characteristics  [15] . In our work, we use a body bounding box to crop the body image from the original picture and then scale it to the same size. Three channels are used as the input for body imaging and are loaded into the Resnet-101 backbone network. In order to obtain the pose coordinates, we utilise Openpose  [30] . We then input the coordinates into the modified STGCN  [31]  model. The body and pose characteristics will then be combined using the improved EmbraceNet, which can reduce performance deterioration caused by missing or incomplete data.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Global Context Features (Scene And Context)",
      "text": "To reason the emotion states, we suggest combining scene and semantic information. The scene feature extraction network will specifically assist in extracting certain scene characteristics near the agent, such as the class scene and kitchen scene. The network for semantic feature extraction will assist in extracting semantic features surrounding the agent, such as the river, grass, sky, and so on. To extract scene characteristics for our model, we employed a pre-trained CNN network. We use the Resnet-18 as backbone network for scene characteristics extraction. We utilise a modified transformer infrastructure  [17]  to capture the semantic characteristics. Multi-head attention, embedded patches, and normalisation are all parts of the transformer design. The use of attention strategies in this section will demonstrate their efficacy in boosting accuracy for tasks like object identification and other ones. The transformer network can assist our model in focusing on some key semantic elements for determining the emotional states of the target individual and giving these key semantic aspects greater weight in our emotion identification tasks. These important semantic features which is helpful for inferring emotion states of target person can be shown in Figure  4  scene and semantic feature attention map. 1D sequence is sent to the standard transformer. When dealing with 2D pictures x ∈ R H×W ×C , the 2D pictures must be transformed into 1D patches x p ∈ R N ×(P 2 C) , C is the channel count, P is each picture patch's width and height, N is the number of picture patches overall, N = HW/P 2 . In the transformer encoder part  [33] , It incorporates MLP blocks and multiheaded self-attention (MSA). Prior to each block, layer norm (LN) will be applied, and following each block, residual connections  [32] . Two layers of the MLP have a GELU nonlinearity.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Inter-Agent Interaction Features",
      "text": "The depth is utilised to represent inter-Agent interaction and it is useful for predicting emotion states, according to earlier study  [12] . when there are additional agents all around the agent. They act in a similar way. A person's feelings can be inferred from comparable conduct. To extract interaction characteristics, we noticed a CNN model.Depth maps are the input. The input image I is represented by the I d epth.\n\nWhere d(I(i, j), c) indicate the pixel's distance at i th row and j th column to the camera center c.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Implementation Details",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Data Processing",
      "text": "Face and body features: To extract the target person's facial picture, we employ MTCNN. The body image of the target individual serves as the MTCNN method's input, and its output is the coordinates of the bounding box for the face that has the best chance of being identified. After that the face images will be resized to [128,128] with three channels. Body features are cropped from the entire image using the crop function and the body bounding box coordinates.\n\nDepth features: To obtain the depth maps from the entire picture, we utilise MiDaS  [18] . Depth maps with the size [224,224,1] are the output. The depth maps can be calculated by Equation  5 B. Network Architecture Context-aware features (Scene and Context): The Resnet network, which was previously trained on the place 365 datasets, serves as the backbone of the scene features extraction network. The entire image with three channels is the input to the semantic features extraction network. In the semantic feature extraction network, it includes multi-headed self-attention, layer norm, and MLP block. The structure can be shown in Fig.  1 .\n\nDepth features: The CNN depth network uses depth maps as input, which are generated using Equation5. There are five 2D convolutional layers in the CNN network.The image size will be reduced after each convolutional layer, which will be used to obtain fine-grained features.\n\nFusion method: We apply the early fusion approaches to merge the face, body, context, pose and depth characteristics. Instead of utilising single features to infer emotion states, the feature vectors for emotion detection are fused,\n\nTwo fully connected layers are applied on merged features. The dimension for first fully connected layer are transfer from 256 to 26.26 are corresponding to 26 emotion categories. The second fully connected layer's dimension transfer from 256 to 3. The softmax layer comes after these fully connected layer. Loss and error will be computed using the output.\n\nLoss function: A weighted mixture of two independent losses makes up the loss function. The prediction of 26 categories plus the forecast of 3 continuous dimensions make up the prediction y pred . y pred = (y disc , y cont )., where y disc = (y disc 1 , y disc ). The definition of the combination loss is\n\nWhere the λ disc and λ cont will take each loss's contribution into account. We employ the weighted Euclidean loss for discrete categories in our studies.Compared to Kullback-Leibler loss and multiclass multi-classification hinge loss, the Euclidean loss is more efficient. The following definition applies to discrete categories loss:\n\nwhere the real label is y realdisc i and the prediction label for the i th emotion category is y preddisc i . The weight for every emotion category is the parameter w i and w i = 1 ln(c+pi) . p i is the probability for i th category and c is to control the range   [23]  TO OTHER DATASETS LIKE AFEW  [20] , AFFECTNET  [22] ,CAER-S  [21]  AND CAER  [21]    of valid values for weight.\n\nIt has been demonstrated that the frequently used L cont is less dependent on outliers and is described as follows  [19] .\n\nwhere v k is weight given to every continuous emotion category and\n\n).\n\nV. DATASETS The context-based EMOTIC datasets will be introduced. There are 23,571 photos of 34,320 labelled people in the wild places in the EMOTIC database  [23] .The EMOTIC dataset contains 26 different kinds of emotions.Each person is capable of having several labels that correlate to various emotion groups. When comparing EMOTIC dataset  [23]  to other datasets like AFEW  [20] , AffectNet  [22] ,CAER-S  [21]  and CAER  [21]  , the photos in the EMOTIC collection were taken in the outdoors and contain a large number of context characteristics. Many of the intended people's faces are hidden, thus we need to develop a network that can successfully extract background data for emotion identification. The EMOTIC dataset has 26 categories for emotions. Only 7-8 emotion categories are included in other datasets. Table  I  provides an overview of the dataset details. Figure  2  displays some of the EMOTIC's sample pictures. Only face photos with little background data are present in the AffectNet  [22]  collection. The AFEW dataset  [20]  contains pictures taken from movies with partial body and facial information, but little background data.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Vi. Experiments And Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Training Details",
      "text": "We use the foundational context-based emotion datasets from EMOTIC to train our model using stochastic gradient descent with momentum. Our approaches have a batch size of 52 and a 45 epoch. The learning rate starts off at 0.001 and decreases by a factor of 10 every 15 epochs when we employ the Adam optimizer.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Ablation Study",
      "text": "We train our model on the new dataset and EMOTIC dataset. The new dataset contains four fundamental emotion categories, in order to comprehend model's components.\n\nBenefits of Local Personal Features. We conduct an experiment using the EMOTIC dataset and a subset of EMOTIC with four fundamental emotions in order to analyse the effects of local personal variables for predicting emotion states of the target person. In the Table  II , using local personal features (Feature 1), our model can achieved good results in following emotion categories such as 'Anticipation', 'Confidence', 'Engagement', 'excitement' and 'Happiness'. The reason is that inferring these emotion categories relies on face and body features. In addition, local personal features are basic features for emotion recognition. Retaining local personal features will improve inferring ability of our model.\n\nBenefits of Global Scene and Semantic Features. Comparing with 2 nd column and 3 rd column in the Table II(a), it shows that after adding scene and semantic features, the Average Precision Score increase about 6%, from 33.52% to 39%. In the Table II(b), the Average Precision Score increase about 13%, from 42.53% to 55.23%. This is due to that the EMOTIC dataset include rich background information. Our global scene and semantic features extraction network can effectively extract these background information. So, our model with combined scene and semantic features can perform well either in the large number of emotion categories or the small number of emotion categories (only 4). The accuracy of the model on all emotion categories have an improvement.   Confusion matrix analysis. Comparing Figure  3 (a) and Figure  3 (c), only using face, body and depth features stream, the networks will perform poorly and the network have limited ability to distinguish 'Anger' and 'Sadness'. After adding context streams, the joint model will greatly improve the",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "C. Analysis And Discussion",
      "text": "Comparison with SOTA: We analyse our findings using the common Average Precision (AP) metric, and we compare our results with SOTA results in Table  III . For EMOTIC Dataset, by using combination loss, our model improves by 3% and achieve 38.84% Average Precision (AP) score, outperforming all other methods. Our best model use the discrete categories loss and achieved 40.39% Average Precision (AP) score, outperforming by a margin of 4.91% Mittal's method. We extract four basic emotion categories and summarize the AP score in the new dataset in Table  IV . The earlier approaches Mittal et al.  [12]  and Zhang et al.  [29]  don't have publicly accessible implementations, therefore we implement their algorithms in accordance with the paper's statement and our best knowledge to evaluate them utilizing the same settings on the new dataset.On a new dataset, our technique outperformed all others, scoring 56.18% Average Precision (AP) .\n\nThe key to our network's efficiency is the integration of several modalities, and we use transformer-based attention modalities in this system to capture background information. For instance, in the Figure  4 .(a), we can see that the person's face is hidden in the first picture.The network is unable to obtain facial features with acceptable quality. Therefore, the global scene and semantic characteristics are given greater consideration by our network. We can see from the scene elements that the baseball players are on the baseball field playing the game. The semantic features shows that there are 'ball players', 'baseball', 'scoreboard', 'croquet ball' and 'football helmet' in baseball field. Our model utilises the global scene and semantic features to successfully infer emotion states of target person.\n\nComparing with Kosti et al.  [13] 's method,the context modality can only give scene category information and does not employ attention strategies in their network, despite the fact that they use two CNN streams to extract context features and body features for emotion identification.Their network is unable to properly change its weights and focus its attention to the context's most important elements. In the Table  III , their method only achieved 27.38% average precision score.\n\nComparing with Zhang et al's  [29]  method, they capture context information using the Region Proposal Network (RPN) and feed the result to the Graph Convolutional Network (GCN). The other stream uses CNN to gather body characteristics. These two streams are combined in the fusion component to estimate emotion. The CNN network can extract limited body features. But this CNN network can not extract pose features. However, our method can utilise pose features for emotion recognition. The pose features can be shown in the Figure  5 . In addition, region proposal network (RPN) do not utilise self attetion techniques to extract context features for emotion recognition. Their network can not arrange higher weights to important part of context. So their method performs poorly in EMOTIC dataset and new dataset. In the Table  III , their method only achieved 28.42% average precision score.\n\nComparing with Mittal et al.  [12]  method, they try to use a self-attention-based CNN to extract semantic features, they achieved great improvement on average precision score for emotion recognition. However, their self-attention-based CNN has limited ability to understand what different objects there are in the same scene. In our model, the scene extraction network can extract scene features and the semantic extraction network can extract semantic features. Our model can effectively distinguish different objects in the same scene. For instance, in the Figure  4 .(b), the scene is recreation room. In this recreation room, our model can get specific semantic features such as billiard table, croquet ball, pole, television and so on. So, our model achieved competitive results on EMOTIC dataset and new dataset.\n\nQualitative results analysis. In the Figure  4 , the first",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Vii. Conclusion, Limitation And Future Work",
      "text": "In this paper, we propose to use combined scene and semantic features with personal features to reason the emotion states. Our results show that the combined scene and semantic features can effectively improve the model's accuracy when the target person's face and body features are partially visible. We also provide the qualitative result and notice that our network can learn scene and semantic features and pay more attention to the critical parts which influence emotion states of the agent. We hope our study can promote the study in influence of semantic features on emotion recognition.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: illustrates the multi-modal",
      "page": 2
    },
    {
      "caption": "Figure 1: The proposed network architecture for emotion identification. In our model, f1 are extracted face features. f7 are extracted body features. f4 are",
      "page": 3
    },
    {
      "caption": "Figure 4: scene and semantic feature attention map. 1D",
      "page": 4
    },
    {
      "caption": "Figure 2: The photos in the EMOTIC collection were taken in the outdoors and contain a large number of context characteristics. Only face photos with",
      "page": 5
    },
    {
      "caption": "Figure 2: displays some of",
      "page": 5
    },
    {
      "caption": "Figure 3: (c), only using face, body and depth features stream,",
      "page": 6
    },
    {
      "caption": "Figure 3: Matrix of confusion confirms that, in order to improve model’s performance, multi-modal combination with merged scene and semantic information",
      "page": 7
    },
    {
      "caption": "Figure 4: (a), we can see that the person’s",
      "page": 7
    },
    {
      "caption": "Figure 5: In addition, region proposal network (RPN) do not",
      "page": 7
    },
    {
      "caption": "Figure 4: (b), the scene is recreation room.",
      "page": 7
    },
    {
      "caption": "Figure 4: , the first",
      "page": 7
    },
    {
      "caption": "Figure 4: In our model, the scene extraction network can extract scene features",
      "page": 8
    },
    {
      "caption": "Figure 5: We employ a fusion network to fuse body and pose characteristics in",
      "page": 8
    },
    {
      "caption": "Figure 4: (a), the face",
      "page": 8
    },
    {
      "caption": "Figure 5: (a), the baseball player’s hands are up in a",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Labels": "Affection",
          "Kosti et al.\n[13]": "27.85",
          "Zhang et al.\n[29]": "46.89",
          "Mittal et al.\n[12]": "45.23",
          "Ours (Lcomb)": "48.89",
          "Ours (Ldisc)": "49.53"
        },
        {
          "Labels": "Anger",
          "Kosti et al.\n[13]": "09.49",
          "Zhang et al.\n[29]": "10.87",
          "Mittal et al.\n[12]": "15.46",
          "Ours (Lcomb)": "24.29",
          "Ours (Ldisc)": "30.46"
        },
        {
          "Labels": "Annoyance",
          "Kosti et al.\n[13]": "14.06",
          "Zhang et al.\n[29]": "11.23",
          "Mittal et al.\n[12]": "21.92",
          "Ours (Lcomb)": "28.42",
          "Ours (Ldisc)": "29.55"
        },
        {
          "Labels": "Anticipation",
          "Kosti et al.\n[13]": "58.64",
          "Zhang et al.\n[29]": "62.64",
          "Mittal et al.\n[12]": "72.12",
          "Ours (Lcomb)": "96.07",
          "Ours (Ldisc)": "96.15"
        },
        {
          "Labels": "Aversion",
          "Kosti et al.\n[13]": "07.48",
          "Zhang et al.\n[29]": "5.93",
          "Mittal et al.\n[12]": "17.81",
          "Ours (Lcomb)": "22.21",
          "Ours (Ldisc)": "23.63"
        },
        {
          "Labels": "Confidence",
          "Kosti et al.\n[13]": "78.35",
          "Zhang et al.\n[29]": "72.49",
          "Mittal et al.\n[12]": "68.65",
          "Ours (Lcomb)": "80.78",
          "Ours (Ldisc)": "81.00"
        },
        {
          "Labels": "Disapproval",
          "Kosti et al.\n[13]": "14.97",
          "Zhang et al.\n[29]": "11.28",
          "Mittal et al.\n[12]": "19.82",
          "Ours (Lcomb)": "31.99",
          "Ours (Ldisc)": "34.33"
        },
        {
          "Labels": "Disconnection",
          "Kosti et al.\n[13]": "21.32",
          "Zhang et al.\n[29]": "26.91",
          "Mittal et al.\n[12]": "43.12",
          "Ours (Lcomb)": "41.63",
          "Ours (Ldisc)": "46.14"
        },
        {
          "Labels": "Disquietment",
          "Kosti et al.\n[13]": "16.89",
          "Zhang et al.\n[29]": "16.94",
          "Mittal et al.\n[12]": "18.73",
          "Ours (Lcomb)": "23.73",
          "Ours (Ldisc)": "24.78"
        },
        {
          "Labels": "Doubt/Confusion",
          "Kosti et al.\n[13]": "29.63",
          "Zhang et al.\n[29]": "18.68",
          "Mittal et al.\n[12]": "35.12",
          "Ours (Lcomb)": "26.13",
          "Ours (Ldisc)": "24.96"
        },
        {
          "Labels": "Embarrassment",
          "Kosti et al.\n[13]": "03.18",
          "Zhang et al.\n[29]": "1.94",
          "Mittal et al.\n[12]": "14.37",
          "Ours (Lcomb)": "8.23",
          "Ours (Ldisc)": "7.55"
        },
        {
          "Labels": "Engagement",
          "Kosti et al.\n[13]": "87.53",
          "Zhang et al.\n[29]": "88.56",
          "Mittal et al.\n[12]": "91.12",
          "Ours (Lcomb)": "98.22",
          "Ours (Ldisc)": "98.36"
        },
        {
          "Labels": "Esteem",
          "Kosti et al.\n[13]": "17.73",
          "Zhang et al.\n[29]": "13.33",
          "Mittal et al.\n[12]": "23.62",
          "Ours (Lcomb)": "27.44",
          "Ours (Ldisc)": "28.63"
        },
        {
          "Labels": "Excitement",
          "Kosti et al.\n[13]": "77.16",
          "Zhang et al.\n[29]": "71.89",
          "Mittal et al.\n[12]": "83.26",
          "Ours (Lcomb)": "83.49",
          "Ours (Ldisc)": "82.92"
        },
        {
          "Labels": "Fatigue",
          "Kosti et al.\n[13]": "09.70",
          "Zhang et al.\n[29]": "13.26",
          "Mittal et al.\n[12]": "16.23",
          "Ours (Lcomb)": "16.79",
          "Ours (Ldisc)": "22.66"
        },
        {
          "Labels": "Fear",
          "Kosti et al.\n[13]": "14.14",
          "Zhang et al.\n[29]": "4.21",
          "Mittal et al.\n[12]": "23.65",
          "Ours (Lcomb)": "11.67",
          "Ours (Ldisc)": "12.18"
        },
        {
          "Labels": "Happiness",
          "Kosti et al.\n[13]": "58.26",
          "Zhang et al.\n[29]": "73.26",
          "Mittal et al.\n[12]": "74.71",
          "Ours (Lcomb)": "87.41",
          "Ours (Ldisc)": "87.56"
        },
        {
          "Labels": "Pain",
          "Kosti et al.\n[13]": "08.94",
          "Zhang et al.\n[29]": "6.52",
          "Mittal et al.\n[12]": "13.21",
          "Ours (Lcomb)": "24.58",
          "Ours (Ldisc)": "25.56"
        },
        {
          "Labels": "Peace",
          "Kosti et al.\n[13]": "21.56",
          "Zhang et al.\n[29]": "32.85",
          "Mittal et al.\n[12]": "34.27",
          "Ours (Lcomb)": "30.18",
          "Ours (Ldisc)": "33.28"
        },
        {
          "Labels": "Pleasure",
          "Kosti et al.\n[13]": "45.46",
          "Zhang et al.\n[29]": "57.46",
          "Mittal et al.\n[12]": "65.53",
          "Ours (Lcomb)": "59.53",
          "Ours (Ldisc)": "58.86"
        },
        {
          "Labels": "Sadness",
          "Kosti et al.\n[13]": "19.66",
          "Zhang et al.\n[29]": "25.52",
          "Mittal et al.\n[12]": "23.41",
          "Ours (Lcomb)": "33.19",
          "Ours (Ldisc)": "35.82"
        },
        {
          "Labels": "Sensitivity",
          "Kosti et al.\n[13]": "09.28",
          "Zhang et al.\n[29]": "5.99",
          "Mittal et al.\n[12]": "8.32",
          "Ours (Lcomb)": "10.34",
          "Ours (Ldisc)": "12.38"
        },
        {
          "Labels": "Suffering",
          "Kosti et al.\n[13]": "18.84",
          "Zhang et al.\n[29]": "23.39",
          "Mittal et al.\n[12]": "26.39",
          "Ours (Lcomb)": "28.46",
          "Ours (Ldisc)": "36.89"
        },
        {
          "Labels": "Surprise",
          "Kosti et al.\n[13]": "18.81",
          "Zhang et al.\n[29]": "9.02",
          "Mittal et al.\n[12]": "17.37",
          "Ours (Lcomb)": "14.01",
          "Ours (Ldisc)": "14.45"
        },
        {
          "Labels": "Sympathy",
          "Kosti et al.\n[13]": "14.71",
          "Zhang et al.\n[29]": "17.53",
          "Mittal et al.\n[12]": "34.28",
          "Ours (Lcomb)": "38.44",
          "Ours (Ldisc)": "38.67"
        },
        {
          "Labels": "Yearning",
          "Kosti et al.\n[13]": "08.34",
          "Zhang et al.\n[29]": "10.55",
          "Mittal et al.\n[12]": "14.29",
          "Ours (Lcomb)": "13.60",
          "Ours (Ldisc)": "13.72"
        },
        {
          "Labels": "mAP",
          "Kosti et al.\n[13]": "27.38",
          "Zhang et al.\n[29]": "28.42",
          "Mittal et al.\n[12]": "35.48",
          "Ours (Lcomb)": "38.84",
          "Ours (Ldisc)": "40.39"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Labels": "Affection",
          "Only 1": "38.35",
          "Only 1 and 2": "48.58",
          "Only1 and Only 3": "42.74",
          "1, 2 and 3": "49.53"
        },
        {
          "Labels": "Anger",
          "Only 1": "21.87",
          "Only 1 and 2": "28.79",
          "Only1 and Only 3": "22.67",
          "1, 2 and 3": "30.46"
        },
        {
          "Labels": "Annoyance",
          "Only 1": "20.99",
          "Only 1 and 2": "23.26",
          "Only1 and Only 3": "25.24",
          "1, 2 and 3": "29.55"
        },
        {
          "Labels": "Anticipation",
          "Only 1": "94.95",
          "Only 1 and 2": "95.74",
          "Only1 and Only 3": "95.57",
          "1, 2 and 3": "96.15"
        },
        {
          "Labels": "Aversion",
          "Only 1": "16.50",
          "Only 1 and 2": "21.15",
          "Only1 and Only 3": "19.65",
          "1, 2 and 3": "23.63"
        },
        {
          "Labels": "Confidence",
          "Only 1": "76.16",
          "Only 1 and 2": "80.46",
          "Only1 and Only 3": "78.90",
          "1, 2 and 3": "81.00"
        },
        {
          "Labels": "Disapproval",
          "Only 1": "25.08",
          "Only 1 and 2": "31.71",
          "Only1 and Only 3": "29.11",
          "1, 2 and 3": "34.33"
        },
        {
          "Labels": "Disconnection",
          "Only 1": "36.60",
          "Only 1 and 2": "42.78",
          "Only1 and Only 3": "40.05",
          "1, 2 and 3": "46.14"
        },
        {
          "Labels": "Disquietment",
          "Only 1": "16.44",
          "Only 1 and 2": "21.50",
          "Only1 and Only 3": "21.67",
          "1, 2 and 3": "24.78"
        },
        {
          "Labels": "Doubt/Confusion",
          "Only 1": "19.63",
          "Only 1 and 2": "20.42",
          "Only1 and Only 3": "25.01",
          "1, 2 and 3": "24.96"
        },
        {
          "Labels": "Embarrassment",
          "Only 1": "5.72",
          "Only 1 and 2": "7.17",
          "Only1 and Only 3": "7.21",
          "1, 2 and 3": "7.55"
        },
        {
          "Labels": "Engagement",
          "Only 1": "98.12",
          "Only 1 and 2": "98.31",
          "Only1 and Only 3": "98.06",
          "1, 2 and 3": "98.36"
        },
        {
          "Labels": "Esteem",
          "Only 1": "27.02",
          "Only 1 and 2": "28.55",
          "Only1 and Only 3": "27.41",
          "1, 2 and 3": "28.63"
        },
        {
          "Labels": "Excitement",
          "Only 1": "76.49",
          "Only 1 and 2": "81.34",
          "Only1 and Only 3": "80.42",
          "1, 2 and 3": "82.92"
        },
        {
          "Labels": "Fatigue",
          "Only 1": "11.65",
          "Only 1 and 2": "21.96",
          "Only1 and Only 3": "12.81",
          "1, 2 and 3": "22.66"
        },
        {
          "Labels": "Fear",
          "Only 1": "9.14",
          "Only 1 and 2": "12.86",
          "Only1 and Only 3": "10.45",
          "1, 2 and 3": "12.18"
        },
        {
          "Labels": "Happiness",
          "Only 1": "79.99",
          "Only 1 and 2": "84.06",
          "Only1 and Only 3": "86.19",
          "1, 2 and 3": "87.56"
        },
        {
          "Labels": "Pain",
          "Only 1": "16.87",
          "Only 1 and 2": "24.88",
          "Only1 and Only 3": "21.74",
          "1, 2 and 3": "25.56"
        },
        {
          "Labels": "Peace",
          "Only 1": "27.86",
          "Only 1 and 2": "33.91",
          "Only1 and Only 3": "28.90",
          "1, 2 and 3": "33.28"
        },
        {
          "Labels": "Pleasure",
          "Only 1": "50.08",
          "Only 1 and 2": "55.68",
          "Only1 and Only 3": "58.16",
          "1, 2 and 3": "58.86"
        },
        {
          "Labels": "Sadness",
          "Only 1": "17.58",
          "Only 1 and 2": "36.22",
          "Only1 and Only 3": "25.63",
          "1, 2 and 3": "35.82"
        },
        {
          "Labels": "Sensitivity",
          "Only 1": "7.31",
          "Only 1 and 2": "11.99",
          "Only1 and Only 3": "8.70",
          "1, 2 and 3": "12.38"
        },
        {
          "Labels": "Suffering",
          "Only 1": "17.09",
          "Only 1 and 2": "36.13",
          "Only1 and Only 3": "21.32",
          "1, 2 and 3": "36.89"
        },
        {
          "Labels": "Surprise",
          "Only 1": "14.92",
          "Only 1 and 2": "15.90",
          "Only1 and Only 3": "14.43",
          "1, 2 and 3": "14.45"
        },
        {
          "Labels": "Sympathy",
          "Only 1": "32.24",
          "Only 1 and 2": "37.50",
          "Only1 and Only 3": "35.29",
          "1, 2 and 3": "38.67"
        },
        {
          "Labels": "Yearning",
          "Only 1": "12.76",
          "Only 1 and 2": "13.17",
          "Only1 and Only 3": "12.84",
          "1, 2 and 3": "13.72"
        },
        {
          "Labels": "mAP",
          "Only 1": "33.52",
          "Only 1 and 2": "39.00",
          "Only1 and Only 3": "36.57",
          "1, 2 and 3": "40.39"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Labels": "Anger",
          "Kosti et al.\n[13]": "21.50",
          "Zhang et al.\n[29]": "-",
          "Mittal et al.\n[12]": "15.85",
          "Ours": "33.17"
        },
        {
          "Labels": "Disconnection",
          "Kosti et al.\n[13]": "37.76",
          "Zhang et al.\n[29]": "-",
          "Mittal et al.\n[12]": "38.76",
          "Ours": "48.15"
        },
        {
          "Labels": "Happiness",
          "Kosti et al.\n[13]": "88.84",
          "Zhang et al.\n[29]": "-",
          "Mittal et al.\n[12]": "85.76",
          "Ours": "93.53"
        },
        {
          "Labels": "Sadness",
          "Kosti et al.\n[13]": "30.33",
          "Zhang et al.\n[29]": "-",
          "Mittal et al.\n[12]": "48.29",
          "Ours": "49.87"
        },
        {
          "Labels": "mAP",
          "Kosti et al.\n[13]": "44.61",
          "Zhang et al.\n[29]": "-",
          "Mittal et al.\n[12]": "47.21",
          "Ours": "56.18"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Labels": "Anger",
          "Only 1": "20.23",
          "Only 1 and 2": "36.41",
          "Only 1 and Only 3": "24.39",
          "1,2 and 3": "33.17"
        },
        {
          "Labels": "Disconnection",
          "Only 1": "35.89",
          "Only 1 and 2": "42.44",
          "Only 1 and Only 3": "45.11",
          "1,2 and 3": "48.15"
        },
        {
          "Labels": "Happiness",
          "Only 1": "87.75",
          "Only 1 and 2": "88.87",
          "Only 1 and Only 3": "92.68",
          "1,2 and 3": "93.53"
        },
        {
          "Labels": "Sadness",
          "Only 1": "26.26",
          "Only 1 and 2": "53.22",
          "Only 1 and Only 3": "27.48",
          "1,2 and 3": "49.87"
        },
        {
          "Labels": "mAP",
          "Only 1": "42.53",
          "Only 1 and 2": "55.23",
          "Only 1 and Only 3": "47.42",
          "1,2 and 3": "56.18"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Ours (L comb ) Ours (L disc )",
      "authors": [
        "Labels Kosti"
      ],
      "venue": "Ours (L comb ) Ours (L disc )"
    },
    {
      "citation_id": "2",
      "title": "Emotion Monitoring from Physiological Signals for Service Robots in the Living Space",
      "authors": [
        "K Rattanyu",
        "M Ohkura",
        "M Mizukawa"
      ],
      "year": "2010",
      "venue": "Proceedings of the ICCAS"
    },
    {
      "citation_id": "3",
      "title": "Nonverbal Emotion Recognition and Salespersons: Linking Ability to Perceived and Actual Success",
      "authors": [
        "Byron Terranova",
        "S Nowicki"
      ],
      "year": "2007",
      "venue": "J. Appl. Soc. Psychol",
      "doi": "10.1111/j.1559-1816.2007.00272.x"
    },
    {
      "citation_id": "4",
      "title": "Emotion Measurement in Intelligent Tutoring Systems: What, When and How to Measure",
      "authors": [
        "M Feidakis",
        "T Daradoumis",
        "S Caballe"
      ],
      "year": "2011",
      "venue": "Proceedings of the 2011 Third International Conference on Intelligent Networking and Collaborative Systems"
    },
    {
      "citation_id": "5",
      "title": "Emotion recognition in context",
      "authors": [
        "R Kosti",
        "J Alvarez",
        "A Recasens",
        "A Lapedriza"
      ],
      "year": "2017",
      "venue": "CVPR"
    },
    {
      "citation_id": "6",
      "title": "Facial Emotion Recognition of Students using Convolutional Neural Network",
      "authors": [
        "A Lasri",
        "M Solh",
        "Belkacemi"
      ],
      "year": "2019",
      "venue": "2019 Third International Conference on Intelligent Computing in Data Sciences (ICDS)",
      "doi": "10.1109/ICDS47004.2019.8942386"
    },
    {
      "citation_id": "7",
      "title": "Emotion Recognition From Body Movement",
      "authors": [
        "F Ahmed",
        "A Bari",
        "M Gavrilova"
      ],
      "year": "2020",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2019.2963113"
    },
    {
      "citation_id": "8",
      "title": "Fusing Body Posture With Facial Expressions for Joint Recognition of Affect in Child-Robot Interaction",
      "authors": [
        "P Filntisis",
        "N Efthymiou",
        "P Koutras",
        "G Potamianos",
        "P Maragos"
      ],
      "year": "2019",
      "venue": "IEEE Robotics and Automation Letters"
    },
    {
      "citation_id": "9",
      "title": "Two-layer feature selection algorithm for recognizing human emotions from 3d motion analysis",
      "authors": [
        "F Ahmed",
        "M Gavrilova"
      ],
      "year": "2019",
      "venue": "Computer Graphics International Conference"
    },
    {
      "citation_id": "10",
      "title": "Text-based Emotion Aware Recommender",
      "authors": [
        "J Leung",
        "I Griva",
        "W Kennedy"
      ],
      "year": "2020",
      "venue": "Text-based Emotion Aware Recommender",
      "arxiv": "arXiv:2007.01455"
    },
    {
      "citation_id": "11",
      "title": "Speech emotion recognition with deep convolutional neural networks",
      "authors": [
        "Dias Issa",
        "M Demirci",
        "Adnan Yazici"
      ],
      "year": "2020",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "12",
      "title": "Learning utterance-level representations for speech emotion and age/gender recognition using deep neural networks",
      "authors": [
        "Z Wang",
        "I Tashev"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "EmotiCon: Context-Aware Multimodal Emotion Recognition Using Frege's Principle",
      "authors": [
        "Trisha Mittal"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "14",
      "title": "Context based emotion recognition using emotic dataset",
      "authors": [
        "Ronak Kosti"
      ],
      "year": "2019",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "15",
      "title": "Emotion Understanding in Videos Through Body, Context, and Visual-Semantic Embedding Loss",
      "authors": [
        "P Filntisis",
        "N Efthymiou",
        "G Potamianos",
        "P Maragos"
      ],
      "year": "2020",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "16",
      "title": "Cascade Attention Networks For Group Emotion Recognition with Face, Body and Image Cues",
      "venue": "Cascade Attention Networks For Group Emotion Recognition with Face, Body and Image Cues"
    },
    {
      "citation_id": "17",
      "title": "FaceNet: A Unified Embedding for Face Recognition and Clustering",
      "authors": [
        "F Schroff",
        "D Kalenichenko",
        "J Philbin"
      ],
      "year": "2015",
      "venue": "FaceNet: A Unified Embedding for Face Recognition and Clustering",
      "arxiv": "arXiv:1503.03832"
    },
    {
      "citation_id": "18",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "Alexey Dosovitskiy"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "19",
      "title": "Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer",
      "authors": [
        "René Ranftl"
      ],
      "year": "2019",
      "venue": "Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer",
      "arxiv": "arXiv:1907.01341"
    },
    {
      "citation_id": "20",
      "title": "Fast r-cnn",
      "authors": [
        "R Girshick"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "21",
      "title": "Acted Facial Expressions in the Wild Database",
      "authors": [
        "A Dhall",
        "R Goecke",
        "S Lucey",
        "T Gedeon"
      ],
      "year": "2011",
      "venue": "Acted Facial Expressions in the Wild Database"
    },
    {
      "citation_id": "22",
      "title": "Contextaware emotion recognition networks",
      "authors": [
        "J Lee",
        "S Kim",
        "S Kim",
        "J Park",
        "K Sohn"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "23",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "Ali Mollahosseini",
        "Behzad Hasani",
        "Mohammad Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "24",
      "title": "EMOTIC: Emotions in Context dataset",
      "authors": [
        "Ronak Kosti"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "25",
      "title": "Emotion recognition from physiological signal analysis: a review",
      "authors": [
        "Maria Egger",
        "Matthias Ley",
        "Sten Hanke"
      ],
      "year": "2019",
      "venue": "Electronic Notes in Theoretical Computer Science"
    },
    {
      "citation_id": "26",
      "title": "Context is everything (in emotion research)",
      "authors": [
        "Katharine Greenaway",
        "K Elise",
        "Lisa Kalokerinos",
        "Williams"
      ],
      "year": "2018",
      "venue": "Social and Personality Psychology Compass"
    },
    {
      "citation_id": "27",
      "title": "Emotion regulation in everyday life",
      "authors": [
        "James Gross",
        "M Jane",
        "Oliver Richards",
        "John"
      ],
      "year": "2006",
      "venue": "Emotion regulation in everyday life"
    },
    {
      "citation_id": "28",
      "title": "Emotional experience as a function of social context: The role of the other",
      "authors": [
        "Esther Jakobs",
        "Agneta Fischer",
        "Antony Manstead"
      ],
      "year": "1997",
      "venue": "Journal of Nonverbal Behavior"
    },
    {
      "citation_id": "29",
      "title": "EmbraceNet: A robust deep learning architecture for multimodal classification",
      "authors": [
        "Jun- Choi",
        "Jong-Seok Ho",
        "Lee"
      ],
      "year": "2019",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "30",
      "title": "Context-aware affective graph reasoning for emotion recognition",
      "authors": [
        "Minghui Zhang",
        "Yumeng Liang",
        "Huadong Ma"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "31",
      "title": "OpenPose: realtime multi-person 2D pose estimation using Part Affinity Fields",
      "authors": [
        "Zhe Cao"
      ],
      "year": "2019",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "32",
      "title": "Step: Spatial temporal graph convolutional networks for emotion perception from gaits",
      "authors": [
        "Uttaran Bhattacharya"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "33",
      "title": "Extracting multiple-relations in one-pass with pretrained transformers",
      "authors": [
        "Haoyu Wang"
      ],
      "year": "2019",
      "venue": "Extracting multiple-relations in one-pass with pretrained transformers",
      "arxiv": "arXiv:1902.01030"
    },
    {
      "citation_id": "34",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani"
      ],
      "year": "2017",
      "venue": "Attention is all you need",
      "arxiv": "arXiv:1706.03762"
    },
    {
      "citation_id": "35",
      "title": "Multi-label, multi-task CNN approach for context-based emotion recognition",
      "authors": [
        "Ilyes Bendjoudi"
      ],
      "year": "2020",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "36",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "37",
      "title": "End-to-end object detection with transformers",
      "authors": [
        "Nicolas Carion"
      ],
      "year": "2020",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "38",
      "title": "Neural machine translation by jointly learning to align and translate",
      "authors": [
        "Dzmitry Bahdanau",
        "Kyunghyun Cho",
        "Yoshua Bengio"
      ],
      "year": "2014",
      "venue": "Neural machine translation by jointly learning to align and translate",
      "arxiv": "arXiv:1409.0473"
    },
    {
      "citation_id": "39",
      "title": "End-to-end asr: from supervised to semi-supervised learning with modern architectures",
      "authors": [
        "Synnaeve",
        "Gabriel"
      ],
      "year": "2019",
      "venue": "End-to-end asr: from supervised to semi-supervised learning with modern architectures",
      "arxiv": "arXiv:1911.08460"
    },
    {
      "citation_id": "40",
      "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet",
      "authors": [
        "Li Yuan"
      ],
      "year": "2021",
      "venue": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet",
      "arxiv": "arXiv:2101.11986"
    }
  ]
}