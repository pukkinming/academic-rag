{
  "paper_id": "2109.07149v1",
  "title": "Fusion With Hierarchical Graphs For Mulitmodal Emotion Recognition",
  "published": "2021-09-15T08:21:01Z",
  "authors": [
    "Shuyun Tang",
    "Zhaojie Luo",
    "Guoshun Nan",
    "Yuichiro Yoshikawa",
    "Ishiguro Hiroshi"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Automatic emotion recognition (AER) based on enriched multimodal inputs, including text, speech, and visual clues, is crucial in the development of emotionally intelligent machines. Although complex modality relationships have been proven effective for AER, they are still largely underexplored because previous works predominantly relied on various fusion mechanisms with simply concatenated features to learn multimodal representations for emotion classification. This paper proposes a novel hierarchical fusion graph convolutional network (HFGCN) model that learns more informative multimodal representations by considering the modality dependencies during the feature fusion procedure. Specifically, the proposed model fuses multimodality inputs using a twostage graph construction approach and encodes the modality dependencies into the conversation representation. We verified the interpretable capabilities of the proposed method by projecting the emotional states to a 2D valence-arousal (VA) subspace. Extensive experiments showed the effectiveness of our proposed model for more accurate AER, which yielded state-of-the-art results on two public datasets, IEMOCAP and MELD.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotions color a language and is a necessary ingredient for human-to-human communications. The objective of automatic emotion recognition (AER) is to detect emotions from enriched input signals, such as audio, texture, facial images, or multimedia signals, and it is critical for building emotionally intelligent machines. Recent years have witnessed a growing trend for the application of AER systems, such as assisting conversation  (Morrison, Wang, and De Silva 2007) , detecting extreme emotions  (Kuppens et al. 2012) , and lifelike human-computer interactions  (Cowie et al. 2001; Fragopanagos and Taylor 2005) .\n\nAER has been extensively studied for audio  (Xu et al. 2021 ), text  (Calefato, Lanubile, and Novielli 2017) , facial clues  (Chen et al. 2016; Luo et al. 2017) , and EEG-based brain waves  (Tripathi et al. 2017) . Previous studies showed Figure  1 : Basic overview of our approach to multimodal emotion recognition. that humans rely more on multiple modalities than on a single modality  (Shimojo and Shams 2001)  to better understand emotions. As highlighted in  (Sebe, Cohen, and Huang 2005) , voice calls are more informative than text messages, indicating that the affective prosody of audio is able to deliver additional information for emotion recognition. Meanwhile, speaking face-to-face is more effective than voice calls  (Drolet and Morris 2000) , indicating that visual cues may contribute more to emotion classification.\n\nHowever, the question of how to effectively associate different modalities to the AER task is still an open research problem. The underlying reason is that different modalities are neither completely independent nor correlated, posing challenges for selecting useful information and filtering out the noise in different modalities during fusion. Previous fusion methods towards this direction used various fusion mechanisms with simply concatenated features to learn multimodal representations. These approaches can be categorized into late fusion  (Poria et al. 2017; Xue et al. 2017) , early fusion  (Sebastian and Pierucci 2019) , and hybrid fusion  (Pan et al. 2020) .\n\nDespite the effectiveness of the above fusion approaches, the interactions between modalities (intermodality interactions), which have been proved effective for the AER task  (Fu et al. 2020) , are still largely underexplored. The intuition behind this is that visual, audio, and textual information are produced by a unique event, and therefore there are strong relationships between each subset of the three modalities. Recently, a graph fusion method, MMGCN  (Hu et al. 2021) , was proposed for modeling the intermodality interactions for emotion recognition in conversations. However, in that work, the relations between modality and emotion were not utilized. In reality, people tend to have more facial expressions when excited while expressing frustration through conversations. We assume that the contribution of each modality to the emotion classification depends on the types of emotions, and the relations between modality and emotion also significantly influence emotion recognition. With this in mind, our goal is to capture both the interactions among modalities and the relations between modality and emotion for improving the AER accuracy.\n\nTo achieve the abovementioned goal, in this paper, we propose a Hierarchical Fusion Graph Convolutional Network (HFGCN) consisting of a two-stage graph construction and a multitask learning loss based on Valence-Arousal (VA) degrees. Fig.  1  provides a basic overview of the proposed approach with multimodal inputs. 1) In the first stage, each modality is initialized as a node connected to another via attention-based edges, which encode intermodality interactions into utterance representations. Compared with previous works that mainly rely on concatenation-based fusion mechanisms, our approach leverages the inherent advantages of graphs to model the relationships between different modalities and distinguish their contribution to each utterance representation. 2) In the second stage, each utterance representation or the first-stage graph is connected to another set of attention-based edges that extend the intermodality interactions from each utterance to the entire conversation. In addition, the designed edge relations are assigned to all the edge sets for relational graph transformation, which captures the relations between modality and emotion.\n\nFurthermore, we also revisit the VA theorem  (Barrett 1998 (Barrett , 2006) )  and simultaneously predict the emotional states and VA degrees with a multitask learning loss, with the aim of modeling the interactions between different types of emotions and their transitions. We empirically observe that introducing the VA degrees to map the emotional states can also facilitate the model interpretation. Extensive experiments on two public datasets show the effectiveness of our proposed method .\n\nOur contributions are threefold:\n\n• We propose a novel graph fusion network called HFGCN that is able to hierarchically fuse multimodal information by encoding the intermodality interactions into the emotion representations. • To the best of our knowledge, this is the first attempt to incorporate the VA degrees to AER in a multitask fashion. Such an approach can better explore the relations between modality and emotion and enhances the emotional states' interpretability. • We conduct extensive experiments on two public multimodal emotion recognition datasets: IEMOCAP (Busso Our code will be released upon acceptance of the paper.\n\net al. 2008) and MELD  (Poria et al. 2019) . Our experimental results show the superiority of our proposed method against previous state-of-the-art methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work Multimodal Fusion",
      "text": "Most multimodal fusion methods are based on the following: (i) Early fusion: The features of different modalities are extracted and concatenated to obtain a feature vector, which is fed into a classifier to obtain the emotion prediction. (Sebastian and Pierucci 2019) utilized early fusion and fed the extracted features through a convolutional neural network.\n\n(ii) Late fusion: The features of different modalities are extracted and directly fed into independent unimodal classifiers. Then, the intermediate predictions are concatenated and fed to a metaclassifier for the final prediction  (Poria et al. 2017) . (iii) Hybrid fusion: Combining (i) and (ii) with more than one network at different levels.  (Majumder et al. 2018)  utilized this method to first fuse each of the two modalities, and then fused the intermediate outputs and passed them to the final network. The key differences between our proposed method and conventional methods are as follows: 1) our novel fusion method is able to embed the intermodality interactions, which have proven effective for AER. Capturing such interactions with existing concatenation-based fusion methods may not yield satisfactory performance for the task; 2) our two-stage graph construction extends the intermodality interactions from each utterance to the entire conversation.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Graph Neural Network",
      "text": "Graph convolutional networks (GCNs) (Kipf and Welling 2016), which have been successfully used to address various problems in computer vision  (Shen et al. 2018 ) and natural language processing  (Yao, Mao, and Luo 2019; Nan et al. 2020 ), provide us with a new solution to address the emotion recognition problem. Several studies have applied the GCN to speech emotion recognition works  (Shirian and Guha 2020; Ghosal et al. 2019) . However, they are only useful for unimodal tasks. Some recent works  (Hu et al. 2021; Wei et al. 2019 ) use the graph fusion method for multimodal emotion recognition without exploring the relations between modality and emotion. In contrast to the above works, we present the designed edge relations and multitask learning loss based on the VA theorem for multimodal emotion recognition, greatly improving the fusion efficiency on graphs.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Feature Extraction",
      "text": "In this work, for feature extraction, following  (Poria et al. 2017) , audio, text, and video frame modalities are extracted from an input video by using individual pretrained networks transferred from other tasks. Audio features, such as the pitch and voice intensity of audio waveforms, are extracted using the OpenSMILE toolkit  (Eyben, Wöllmer, and Schuller 2010 ) with IS13-ComParE  (Schuller et al. 2013) .\n\nText features are embedded by Word2Vec vectors and then",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Model Architecture",
      "text": "In this section, we introduce the HFGCN model in detail. As shown in the framework of the proposed HFGCN in Fig.  2 , suppose there are N utterances in a conversation, i means the i-th utterance included in these utterances (i ∈ 1, ..., N ). We denote a i , t i , v i , and f i as the extracted audio, text, visual, and their early fusion features, respectively. Their GRUs-encoded features are represented as A i , T i , V i , and F i , respectively. The graph transformed feature and final utterance representation are represented as O i and U i , respectively. The task is to predict the emotion labels (happy, sad, neutral, angry, excited, frustrated, disgusted, and fear) and VA degrees for each utterance by the classifiers.\n\nTo achieve this goal, first, each modality's extracted feature and their early fusion feature (a i , t i , v i , f i ) are passed to the sequential context encoder. The output features (A i , T i , V i , F i ) are then passed to the hierarchical graph fusion, which conducts the graph construction and graph transformation. Finally, the graph transformed nodal feature O i is concatenated with the early fusion feature F i to obtain the final utterance representation U i , which is then fed to the classifiers. Meanwhile, a multitask cross-entropy loss function is optimized to guide the emotion classification with the VA degrees.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Sequential Context Encoder",
      "text": "Similar to most proposed methods  (Poria et al. 2017; Pan et al. 2020; Majumder et al. 2018) , RNNs are used to capture the sequential contextual information that flows along the conversation. In the first part of our proposed model, the bidirectional gated recurrent units (GRUs) are used as the encoders to obtain the context-aware representations\n\nThese features are then fed to the hierarchical graph fusion part for embedding the intermodality interactions and the relations between modality and emotion for the multimodal fusion processing.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Hierarchical Graph Fusion",
      "text": "The hierarchical graph fusion processing is the key part of our proposed method. It consists of graph construction for multimodal fusion and graph transformation to accumulate neighbor nodes information. The details of the fusion method are shown in Fig.  3  and described as follows:\n\nGraph Construction: We use a two-stage construction to fuse the multimodal representations. As shown in Fig.  3 , i denotes the i-th utterance of the N utterances in the entire conversation. In the first stage, we denote G i as a relational graph to encode the inter-modality interactions in the utterance. In the second stage, we expand the intermodality interactions to all the utterances by connecting all the first stage graphs G 1 , G 2 , ..., G N with their early fusion nodes F 1 , F 2 , ..., F N . We construct the graph G i as follows:\n\nwhere N i represents the nodes consisting of the contextaware representations A i , T i , V i , F i obtained from Eq. 1. R ij , W ij are the edge relations and edge weights, which are designed as follows:\n\nEdge Relations -The edge relations R ij are constructed depending on the connected nodes N i ∈ (A i , T i , V i , F i ) and N j ∈ (A j , T j , V j , F j ). The detailed relation types are listed in Table  1 . Relations 1-6 are designed for the firststage graphs that are based on node modality, and relations 7-10 are designed for the second-stage graphs that are based on the utterances' positions and speakers. They together distinguish the contribution of each modality (A j , T j , V j ) to the\n\nTable  1 : Detailed relation types between i-th utterance and j-th utterance with two distinct speakers and audio, text, visual, and early fusion nodes.\n\nemotion representation (F i ), which helps to capture the relations between modality and emotion. We summarize the edge relations based on the following three aspects: 1). For modality dependency, within the same utterance (i = j), the relation depends on the modalities to which an edge is connected (i.e., node A 1 and node V 1 are connected with the edge relation 2, which indicates the interactions between audio and visual modality).\n\n2). For speaker dependency, among different utterances (i = j) spoken by different speakers, the relation depends on the speaker to which an edge is connected (i.e., node F 1 that is uttered by speaker 1 and node F 3 that is uttered by speaker 2 are connected with the edge relation 9).\n\n3). For temporal dependency, among different utterances (i = j) spoken by the same speaker, the relation depends on the relative position of different utterances (i.e., node F 1 and node F 2 are connected with the edge relation 7).\n\nEdge weights -For the first-stage graph, within the same utterance (i = j), the edge weights W ij between nodes N i ∈ (A i , T i , V i , F i ) and nodes N j ∈ (A j , T j , V j , F j ) are computed using a similarity-based attention mechanism as follows:\n\nwhere W a denotes the learning weights of the attention mechanisms. The softmax function ensures that for the node of each modality, the edges connected with the remaining modalities have a total attention weight of 1. For example, audio modality node A i receives a total contribution of 1 from T j , V j , andF j .\n\nFor the second-stage graph, the utterances are connected to each other with early fusion nodes F i . The second stage graph's edge weights W ij between node F i and node F j are then calculated based on a multilayer perceptron (MLP)  (Wu et al. 2019 ) as follows:\n\nFi , Fj = Linear(F i ), Linear(F j ),\n\nF i and F j are first passed to two linear layers to obtain the query Fi and key Fj . W b denotes the learning weights of the attention mechanisms. Similarly, the softmax function ensures that all the edges within the utterance window have a total attention weight of 1. This can be understood as a general multiplicative application of the attention of neighboring nodes.\n\nGraph Transformation: As shown in Fig.  3 , after graph construction, the graph G is fed to the graph transformation part, which consists of a relational graph convolutional layer, a graph convolutional layer, and a nodal pooling layer. The first relational graph convolutional layer  (Sukhbaatar et al. 2015)  encodes the modality dependencies and speaker dependencies and outputs the transformed graph features O\n\n(1) i using the edge relations R ij with the input nodes N i ∈ (A i , T i , V i , F i ) and N j ∈ (A j , T j , V j , F j ). The second graph convolutional layer uses O\n\n(1) i as input and further accumulates the local neighborhood information and outputs the transformed graph features O\n\n(2) i as follows:\n\nwhere W ij , W ii are the edge weights of the first transformation, and W\n\n(1)\n\nc , W\n\n(2) d are the independent learning weights of the first and second transformations. N r i denotes the neighbor indices of vertex j under the relation r ∈ R ij . |N r i | is set as the normalization constant. This two-step graph transformation effectively accumulates the normalized sum of the neighborhood information that is enriched with modality and speaker dependencies for each utterance. Because each utterance has four nodes, nodal pooling is performed to reduce the total number of nodes from 4N to N . For each first stage graph that has 4 nodes (N j = A j , T j , V j , F j ), it will output 1 node after the pooling. We applied global average pooling to obtain the O i .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Classification",
      "text": "For the classification part, we obtain the final utterance representation U i for the i-th utterance as follows:\n\nwhere O i is the graph transformed feature obtained in the Graph Transformation section, and F i represents the GRUsencoded early fusion feature processed in the Feature Extraction section. Finally, all the final utterances' representations U 1 , ..., U N are then passed to three fully connected networks: emotion classifier, valence classifier, and arousal classifier. Each includes a fully connected layer with a softmax function to output the probability distribution of the emotion labels, valence degrees, and arousal degrees.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Multitask Training Setup",
      "text": "Based on Psychology, the VA model  (Barrett 1998 (Barrett , 2006;; Yang and Chen 2012)  suggests that the emotions can be categorized or described based on a two-dimensional VA subspace. Thus, we design a special loss function L e,v,a for jointly predicting the emotion labels and VA degrees, which is the categorical cross-entropy L for emotion classification and VA classification as follows:\n\nL e,v,a = L(P e i,j , y e i,j ) + W 1 L(P v i,j , y v i,j ) + W 2 L(P a i,j , y a i,j ), where C is the number of conversations/dialogues, N (i) is the number of utterances in the i-th conversation, P e i,j , P v i,j , P a i,j are the model output probability distributions of the emotion labels, valence degrees, and arousal degrees of the j-th utterance in the i-th conversation, y e i,j , y v i,j , y a i,j\n\nare their corresponding expected labels, and W 1 , W 2 are the loss terms' weights that are set as hyperparameters.\n\nAs the VA degrees are discrete but ordinal, we experimented on our multitask loss with both regression type loss and classification loss. The cross entropy was found to be significantly better than other types of losses.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiments Datasets",
      "text": "We evaluated our HFGCN model on two benchmark datasets: IEMOCAP  (Busso et al. 2008 ) and MELD  (Poria et al. 2019) . Both datasets are multimodal datasets containing text transcription, audio waveform, and visual frame information for every utterance of each conversation/dialogue. They are split into training, test, and validation sets, as shown in Table  2 .\n\nThe IEMOCAP  (Busso et al. 2008 ) dataset contains 10K videos split into 5 min of dyadic conversations for human emotion analysis. Two speakers participated in the conversation. Each conversation is split into spoken utterances. Each utterance in every dialogue is annotated using an emotion label. To align with previous works, we implemented both six-class and four-class AER on IEMOCAP, which are angry, happy, excited, sad, frustrated, and neutral and angry, happy(excited), sad(frustrated), and neutral. In addition, each utterance was also annotated by two evaluators for the valence and arousal degrees, which range from one to five. We took their average to obtain the final VA degrees.\n\nThe MELD  (Poria et al. 2019)  dataset contains more than 1.4K dialogues and 13000 utterances from the Friends TV series. Multiple speakers participated in the conversations. Each utterance in every dialogue is annotated as one of the seven emotion classes: anger, disgust, sadness, joy, surprise, fear, or neutral. In contrast to IEMOCAP, we used MELD for evaluating only the emotion labels, and not the VA degrees. Thus, the HFGCN-VA that includes the multitask training was not performed on the MELD.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Baseline Systems",
      "text": "To evaluate the proposed method, we compared the results with the following models for AER tasks. For the opensourced models, we reimplemented them if they did not report some results on the datasets. BC-LSTM:  (Poria et al. 2017 ) This is the baseline method, in which context-aware utterance representations are generated by capturing the contextual content from the surrounding utterances using a Bi-directional LSTM network.\n\nDialogueRNN:  (Majumder et al. 2019 ) This is a recurrent network that uses two GRUs to track individual speaker states and context for emotion recognition in conversation.\n\nMMAN: (Pan et al. 2020) This is a state-of-the-art LSTM-based multimodal AER method that consists of a hybrid fusion LSTM network with multimodal attention mechanisms.\n\nDialogueGCN: (Ghosal et al. 2019) This is a baseline graph convolutional network that focuses on the textual modality for emotion recognition.\n\nMMGCN:  (Hu et al. 2021)  This is a recent state-of-theart multimodal AER method that uses GCN to fuse multimodal information with the intermodality interactions.\n\nHFGCN variants: This is our proposed method that is efficient with intermodality interactions. HFGCN-VA is the proposed HFGCN with the VA classifiers and is trained under the multitask training setup. For a fair comparison, we did not compare the HFGCN-VA that uses additional valence-arousal degrees as extra training data, but only used it for interpretation.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Implementation Details",
      "text": "We implemented our proposed HFGCN model on the Py-Torch framework and PyTorch geometric for the Hierarchical Graph Fusion parts. During training, we chose the Adam optimizer  (Kingma and Ba 2014)  and set the hyperparameters as follows: learning rate of 0.0001, 50 epochs with early stopping, dropout rate of 0.35, loss term weights W 1 = 0.15, W 2 = 0.15.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Comparison And Analysis",
      "text": "The experimental results for the proposed HFGCN, HFGCN-VA, and previous approaches are shown in Table  3 . To align with previous studies, we report the F1 scores for each emotion class and evaluate the overall classification performance using their weighted averages of the IEMO-CAP with six classes. For the IEMOCAP with four classes, we report both the average accuracy scores and weighted F1 scores. For the MELD, we report the weighted F1 scores.\n\nIn Table  3 , our HFGCN performs better than the compared methods. HFGCN attains the best overall performance with approximately +1% among all the average scores compared to the previous state-of-the-art methods MMGCN and MMAN. In particular, our HFGCN attains a large improvement on happy, which is around 13% compared to the previous best model in classifying happy emotion. We note that happy is the emotion with the least utterances, which demonstrates the ability of HCGCN to recognize minority emotion classes.\n\nCompared to the RNN-based methods, our model significantly improves the results among all datasets, which is possibly due to most of the RNNs' being ignorant of the intermodality interactions. However, for the sad, which is the emotion with the most utterances, the DialogueRNN holds a better F1 score. By checking their confusion matrix, we notice that the DialogueRNN misclassifies a large proportion of other minority emotion classes to the sad emotion.\n\nCompared to the GCN-based methods, our HFGCN also improves the classification results of both average and minority emotion classes (happy, neutral, excited). HFGCN utilizes the hierarchical fusion that consists of early fusion and graph fusion, which outperforms the MMGCN that only uses graph fusion and DialogueGCN that only utilizes early fusion.\n\nWe observed that the additional VA information does improve the results by about 1% among the average F1 scores and accuracy scores. Through the multitask learning, the HFGCN-VA provides the emotional states' visualization and provides better error analysis and interpretation.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ablation Study",
      "text": "We conducted the ablation study for different stage graphs in Table  4 . Removing both of the graphs results in a drastic decrease in performance. It is equivalent to removing the entire GCN part, which is similar to the BC-LSTM  (Poria et al. 2017) . For the removal of the first-stage graphs, we only include the early fusion nodes F during the graph construction, which is similar to the DialogueGCN (Ghosal et al. 2019) with early fused inputs. It fails to capture the intermodality interactions as the early fusion nodes do not capture the correlations between each modality. This proves our graph fusion method's superiority over the traditional early fusion method. For the removal of the second-stage graphs, the model is similar to the MMGCN  (Hu et al. 2021 ) that, during the graph construction, the unimodality nodes connect to one other and their future/past utterances' of unimodality nodes. Without the presence of the early fusion nodes, it fails to capture the relations between modality and emotion as the early fusion nodes connect the first stage graphs and distinguish their contribution to the entire utterance representations. This proves that hierarchically combining graph fusion and early fusion (HFGCN) is more effective than using graph fusion (MMGCN) only.\n\nFurther, we present the effect of edge relations and edge weights in Table  5 . Removing both of the attention-based edge weights and edge relations is equivalent to a vanilla GCN with undirected, unweighted edges, which results in around a 5% drop in the F1 scores. For the removal of the attention-based edge weights, the result drops about 2.7%. For the removal of the edge relations, the results drops about 3.9%, which indicates that the edge relations have more overall importance.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Visualization Of Emotional States",
      "text": "As shown in Fig.  4 , we demonstrate our VA multitask training's visualization, which is corresponded with the original VA model  (Barrett 1998) . For those emotions with low valence degrees, they are usually sad or frustrated with low arousal degrees while angry with high arousal degrees. For those emotions with high valence degrees, they are usually happy with low arousal degrees while excited with high arousal degrees. The neutral emotion usually has intermediate VA degrees. Let us carry out the error analysis. Compared to the expected labels, most of the predictions are over-generalized. For example, the happy and excited emotions are only in large clusters at valence degree = 6 while, in fact, they are much more diverse in the arousal axis. Furthermore, most of the predicted frustrated emotions tend to have lower arousal degrees than the actual ones. At valence degree = 6, arousal degree = 5, our model misclassifies many excited emotions as angry and frustrated emotions, which is unusual as the excited emotions have large valence degrees. After manually checking those misclassified excited results, we notice that most of them are short utterances such as \"Okay\" or \"I just can't\" from the angry emotions, which lie on the transition between neutral and angry. The emotion visualization shows a general idea about how the emotions are distributed on the valence-arousal subspace and suggests that we need to pay more attention to the over-generalized arousal axis.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we proposed HFGCN, a hierarchical fusion graph convolutional network for better multimodal emotion recognition. HFGCN is equipped with a novel graph fusion method that consists of two-stage graph construction, attention-based edge weights, and relational graph transformation that captures the intermodality interactions. We also presented a multitask loss to guide the joint prediction of emotion labels and valence-arousal (VA) degrees. Extensive experiments on two public emotion recognition datasets show the effectiveness of our approach. Ablation studies and visualizations further demonstrate the efficacy of each component of our HFGCN model. Additionally, we provide insightful analysis and interpretation by projecting the emotional states to a 2D VA space. While the model is designed for AER, we believe that our method can be generalized to other classification tasks that need to fuse information from various multiple modalities.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Basic overview of our approach to multimodal",
      "page": 1
    },
    {
      "caption": "Figure 1: provides a basic overview of the pro-",
      "page": 2
    },
    {
      "caption": "Figure 2: Framework of the proposed HFGCN method. It consists of a sequential context encoder, hierarchical graph fusion,",
      "page": 3
    },
    {
      "caption": "Figure 3: and described as follows:",
      "page": 3
    },
    {
      "caption": "Figure 3: Illustration of the Hierarchical Graph Fusion. The green boxes represent the proposed multimodal fusion that connects",
      "page": 4
    },
    {
      "caption": "Figure 3: , after graph",
      "page": 4
    },
    {
      "caption": "Figure 4: Emotion states projected on the VA subspace for the 1000 utterances randomly sampled from the IEMOCAP test",
      "page": 6
    },
    {
      "caption": "Figure 4: , we demonstrate our VA multitask train-",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Detailed relation types between i-th utterance and to each other with early fusion nodes F i . The second stage",
      "data": [
        {
          "Relation": "1\n2\n3\n4\n5\n6\n7\n8\n9\n10",
          "Speakers": "s1, s1\ns1, s1\ns1, s1\ns1, s1\ns1, s1\ns1, s1\ns1, s1\ns1, s1\ns1, s2\ns1, s2",
          "Modality": "A,T\nA,V\nA,F\nT,V\nT,F\nV\n,F\nF,F\nF,F\nF,F\nF,F",
          "Temporal": "i=j\ni=j\ni=j\ni=j\ni=j\ni=j\ni<j\ni>j\ni<j\ni>j",
          "Example": "(A1, T1)\n(A1, V1)\n(A1, F1)\n(T1, V1)\n(T1, F1)\n(V1, F1)\n(F1, F2)\n(F2, F1)\n(F1, F3)\n(F3, F1)"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 3: BC-LSTM, DialogueRNN, and DialogueGCN are designed for the unimodal AER. Thus, we reimplemented them",
      "data": [
        {
          "Methods": "",
          "IEMOCAP (6 class)": "happy\nsad\nneutral\nangry\nexcited\nfrustrated",
          "IEMOCAP (4 class)": "avg. F1\navg. acc",
          "MELD\navg. F1": ""
        },
        {
          "Methods": "BC-LSTM\nDialogueRNN\nDialogueGCN\nMMAN\nMMGCN",
          "IEMOCAP (6 class)": "34.43\n60.87\n51.83\n56.73\n57.95\n58.92\n81.69\n39.16\n59.77\n67.36\n72.91\n60.27\n47.1\n80.88\n58.71\n66.08\n70.97\n61.21\n-\n-\n-\n-\n-\n-\n69.00\n62.32\n42.34\n78.67\n61.73\n74.33",
          "IEMOCAP (4 class)": "60.41\n61.50\n70.85\n70.68\n71.33\n71.29\n74.11\n73.94\n-\n-",
          "MELD\navg. F1": "56.80\n57.11\n58.23\n-\n58.65"
        },
        {
          "Methods": "HFGCN\nHFGCN-VA",
          "IEMOCAP (6 class)": "60.65\n63.28\n75.38\n79.85\n63.38\n60.95\n54.74\n82.47\n67.56\n63.69\n76.07\n60.51",
          "IEMOCAP (4 class)": "74.90\n74.68\n75.98\n75.91",
          "MELD\navg. F1": "59.71\n-"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Discrete Emotions or Dimensions? The Role of Valence Focus and Arousal Focus",
      "authors": [
        "L Barrett"
      ],
      "year": "1998",
      "venue": "Cognition and Emotion"
    },
    {
      "citation_id": "2",
      "title": "Solving the emotion paradox: Categorization and the experience of emotion. Personality and social psychology review",
      "authors": [
        "L Barrett"
      ],
      "year": "2006",
      "venue": "Solving the emotion paradox: Categorization and the experience of emotion. Personality and social psychology review"
    },
    {
      "citation_id": "3",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "4",
      "title": "Emotxt: a toolkit for emotion recognition from text",
      "authors": [
        "F Calefato",
        "F Lanubile",
        "N Novielli"
      ],
      "year": "2017",
      "venue": "2017 seventh international conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)"
    },
    {
      "citation_id": "5",
      "title": "Multithreading cascade of SURF for facial expression recognition",
      "authors": [
        "J Chen",
        "Z Luo",
        "T Takiguchi",
        "Y Ariki"
      ],
      "year": "2016",
      "venue": "EURASIP Journal on Image and Video Processing"
    },
    {
      "citation_id": "6",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "R Cowie",
        "E Douglas-Cowie",
        "N Tsapatsoulis",
        "G Votsis",
        "S Kollias",
        "W Fellenz",
        "J Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal processing magazine"
    },
    {
      "citation_id": "7",
      "title": "Rapport in conflict resolution: Accounting for how face-to-face contact fosters mutual cooperation in mixed-motive conflicts",
      "authors": [
        "A Drolet",
        "M Morris"
      ],
      "year": "2000",
      "venue": "Journal of Experimental Social Psychology"
    },
    {
      "citation_id": "8",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "9",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "N Fragopanagos",
        "J Taylor"
      ],
      "year": "2005",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "10",
      "title": "Multimodality emotion recognition model with GAT-based multihead inter-modality attention",
      "authors": [
        "C Fu",
        "C Liu",
        "C Ishi",
        "H Ishiguro",
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya",
        "A Gelbukh"
      ],
      "year": "2019",
      "venue": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "arxiv": "arXiv:1908.11540"
    },
    {
      "citation_id": "11",
      "title": "MMGCN: Multimodal Fusion via Deep Graph Convolution Network for Emotion Recognition in Conversation",
      "authors": [
        "J Hu",
        "Y Liu",
        "J Zhao"
      ],
      "year": "2021",
      "venue": "MMGCN: Multimodal Fusion via Deep Graph Convolution Network for Emotion Recognition in Conversation",
      "arxiv": "arXiv:2107.06779"
    },
    {
      "citation_id": "12",
      "title": "3D convolutional neural networks for human action recognition. IEEE transactions on pattern analysis and machine intelligence",
      "authors": [
        "S Ji",
        "W Xu",
        "M Yang",
        "K Yu"
      ],
      "year": "2012",
      "venue": "3D convolutional neural networks for human action recognition. IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "13",
      "title": "Large-scale video classification with convolutional neural networks",
      "authors": [
        "A Karpathy",
        "G Toderici",
        "S Shetty",
        "T Leung",
        "R Sukthankar",
        "L Fei-Fei"
      ],
      "year": "2014",
      "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "14",
      "title": "Semi-supervised classification with graph convolutional networks",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "15",
      "title": "Emotional inertia prospectively predicts the onset of depressive disorder in adolescence",
      "authors": [
        "P Kuppens",
        "L Sheeber",
        "M Yap",
        "S Whittle",
        "J Simmons",
        "N Allen"
      ],
      "year": "2012",
      "venue": "Emotion"
    },
    {
      "citation_id": "16",
      "title": "Facial Expression Recognition with deep age",
      "authors": [
        "Z Luo",
        "J Chen",
        "T Takiguchi",
        "Y Ariki"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)"
    },
    {
      "citation_id": "17",
      "title": "DialogueRNN: An Attentive RNN for Emotion Detection in Conversations",
      "authors": [
        "N Majumder",
        "D Hazarika",
        "A Gelbukh",
        "E Cambria",
        "S Poria",
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2007",
      "venue": "Multimodal Sentiment Analysis using Hierarchical Fusion with Context Modeling",
      "arxiv": "arXiv:1806.06228"
    },
    {
      "citation_id": "18",
      "title": "Reasoning with latent structure refinement for document-level relation extraction",
      "authors": [
        "G Nan",
        "Z Guo",
        "I Sekulić",
        "W Lu",
        "Z Luo",
        "Z Yang",
        "J Li"
      ],
      "year": "2020",
      "venue": "Multi-modal Attention for Speech Emotion Recognition",
      "arxiv": "arXiv:2005.06312"
    },
    {
      "citation_id": "19",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "N Majumder",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "20",
      "title": "MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "21",
      "title": "The INTERSPEECH 2013 computational paralinguistics challenge: social signals, conflict, emotion, autism",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner",
        "A Vinciarelli",
        "K Scherer",
        "F Ringeval",
        "M Chetouani",
        "F Weninger",
        "F Eyben",
        "E Marchi"
      ],
      "year": "2013",
      "venue": "Proceedings INTERSPEECH 2013, 14th Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "22",
      "title": "Fusion techniques for utterance-level emotion recognition combining speech and transcripts",
      "authors": [
        "J Sebastian",
        "P Pierucci"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "23",
      "title": "Multimodal emotion recognition",
      "authors": [
        "N Sebe",
        "I Cohen",
        "T Huang"
      ],
      "year": "2005",
      "venue": "Handbook of pattern recognition and computer vision"
    },
    {
      "citation_id": "24",
      "title": "Person Re-identification with Deep Similarity-Guided Graph Neural Network",
      "authors": [
        "Y Shen",
        "H Li",
        "S Yi",
        "D Chen",
        "X Wang"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "25",
      "title": "Sensory modalities are not separate modalities: plasticity and interactions",
      "authors": [
        "S Shimojo",
        "L Shams"
      ],
      "year": "2001",
      "venue": "Current opinion in neurobiology"
    },
    {
      "citation_id": "26",
      "title": "Compact Graph Architecture for Speech Emotion Recognition",
      "authors": [
        "A Shirian",
        "T Guha"
      ],
      "year": "2020",
      "venue": "Compact Graph Architecture for Speech Emotion Recognition",
      "arxiv": "arXiv:2008.02063"
    },
    {
      "citation_id": "27",
      "title": "End-to-end memory networks",
      "authors": [
        "S Sukhbaatar",
        "A Szlam",
        "J Weston",
        "R Fergus"
      ],
      "year": "2015",
      "venue": "End-to-end memory networks",
      "arxiv": "arXiv:1503.08895"
    },
    {
      "citation_id": "28",
      "title": "Using deep and convolutional neural networks for accurate emotion classification on DEAP dataset",
      "authors": [
        "S Tripathi",
        "S Acharya",
        "R Sharma",
        "S Mittal",
        "S Bhattacharya"
      ],
      "year": "2017",
      "venue": "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "29",
      "title": "MMGCN: Multi-modal graph convolution network for personalized recommendation of micro-video",
      "authors": [
        "Y Wei",
        "X Wang",
        "L Nie",
        "X He",
        "R Hong",
        "T.-S Chua"
      ],
      "year": "2019",
      "venue": "Proceedings of the 27th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "30",
      "title": "Simplifying graph convolutional networks",
      "authors": [
        "F Wu",
        "A Souza",
        "T Zhang",
        "C Fifty",
        "T Yu",
        "K Weinberger"
      ],
      "year": "2019",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "31",
      "title": "Speech Emotion Recognition with Multiscale Area Attention and Data Augmentation",
      "authors": [
        "M Xu",
        "F Zhang",
        "X Cui",
        "W Zhang"
      ],
      "year": "2021",
      "venue": "Speech Emotion Recognition with Multiscale Area Attention and Data Augmentation",
      "arxiv": "arXiv:2102.01813"
    },
    {
      "citation_id": "32",
      "title": "A Bayesian nonparametric multimodal data modeling framework for video emotion recognition",
      "authors": [
        "J Xue",
        "Z Luo",
        "K Eguchi",
        "T Takiguchi",
        "T Omoto"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "33",
      "title": "Machine Recognition of Music Emotion: A Review",
      "authors": [
        "Y.-H Yang",
        "H Chen"
      ],
      "year": "2012",
      "venue": "ACM Transactions on Intelligent Systems and Technology"
    },
    {
      "citation_id": "34",
      "title": "Graph convolutional networks for text classification",
      "authors": [
        "L Yao",
        "C Mao",
        "Y Luo"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    }
  ]
}