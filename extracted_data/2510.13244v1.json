{
  "paper_id": "2510.13244v1",
  "title": "Motionbeat: Motion-Aligned Music Representation Via Embodied Contrastive Learning And Bar-Equivariant Contact-Aware Encoding",
  "published": "2025-10-15T07:44:32Z",
  "authors": [
    "Xuanchen Wang",
    "Heng Wang",
    "Weidong Cai"
  ],
  "keywords": [
    "Multimodal Learning",
    "Audio Representation Learning",
    "Cross-Modal Retrieval",
    "Dance Generation"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Music is both an auditory and an embodied phenomenon, closely linked to human motion and naturally expressed through dance. However, most existing audio representations neglect this embodied dimension, limiting their ability to capture rhythmic and structural cues that drive movement. We propose MotionBeat, a framework for motionaligned music representation learning. MotionBeat is trained with two newly proposed objectives: the Embodied Contrastive Loss (ECL), an enhanced InfoNCE formulation with tempo-aware and beat-jitter negatives to achieve fine-grained rhythmic discrimination, and the Structural Rhythm Alignment Loss (SRAL), which ensures rhythm consistency by aligning music accents with corresponding motion events. Architecturally, MotionBeat introduces bar-equivariant phase rotations to capture cyclic rhythmic patterns and contactguided attention to emphasize motion events synchronized with musical accents. Experiments show that MotionBeat outperforms state-of-the-art audio encoders in music-todance generation and transfers effectively to beat tracking, music tagging, genre and instrument classification, emotion recognition, and audio-visual retrieval. Our project demo page: https://motionbeat2025.github.io/.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Music is inherently embodied: people instinctively synchronize their movements with rhythm, linking sound and motion in ways central to dance and rhythm understanding. However, existing audio representation models overlook this embodied dimension, relying on audio-text  [1, 2, 3]  or audio-visual  [4, 5]  training that captures semantics and acoustics but not movement. This gap leaves current audio encoders prone to rhythm-motion misalignment and limits their effectiveness in music-to-dance generation  [6, 7, 8, 9]  and related tasks.\n\nTo address this gap, we propose MotionBeat, a framework for motion-aligned music representation learning. Mo-tionBeat is designed to learn music embeddings that are directly grounded in human motion, capturing not only auditory features but also the embodied rhythmic structures that govern dance. To achieve this, we introduce two novel objectives: Embodied Contrastive Loss (ECL), which enhances InfoNCE  [10]  with tempo-aware and beat-jitter negatives for fine-grained rhythmic discrimination, and Structural Rhythm Alignment Loss (SRAL), which enforces beat-and bar-level consistency via Soft-DTW  [11]  alignment between audio onsets and motion contacts, and optimal transport alignment between accent mass and motion energy.\n\nFrom an architectural perspective, our MotionBeat relies on two fundamental mechanisms tailored for embodied music representation. The first, bar-equivariant phase rotations, encode the cyclic nature of rhythm by applying rotational transformations to embeddings. This enforces structural consistency under phase shifts and makes representations robust to different bar starting points. The second, contact-guided attention, highlights motion frames where embodied events occur by weighting attention scores with contact probabilities. This directs more representational capacity to beats aligned with musical accents, strengthening the audio-motion coupling.\n\nWe evaluate MotionBeat across both generative and recognition tasks. In music-to-dance generation, it surpasses state-of-the-art audio encoders, producing motion that is more rhythmically aligned and structurally consistent. Moreover, MotionBeat transfers effectively to downstream tasks such as beat tracking, music tagging, genre and instrument classification, emotion recognition, and audio-visual retrieval.\n\nOur contributions can be summarized as follows:\n\n• We introduce MotionBeat, a framework for motionaligned music representation learning.\n\nwhere τ is a temperature parameter, s(•, •) denotes cosine similarity, N batch are random in-batch negatives, N tempo are tempo-aware negatives, and N jitter are beat-jitter negatives. The ECL objective is then defined as:",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Structural Rhythm Alignment Loss",
      "text": "While ECL enforces pairwise discriminability, it does not explicitly model higher-level rhythmic structure. We therefore introduce the Structural Rhythm Alignment Loss (SRAL), which aligns audio and motion at both the beat and bar levels.\n\nFor beat-level alignment, let o 1:K denote the audio onset envelope across K beats, and c 1:K the motion contact pulse sequence. We apply differentiable dynamic time warping (Soft-DTW)  [11] :\n\nwhich tolerates small deviations while rewarding synchrony at the beat level. At the bar level, we treat audio as a distribution of accent mass a bar and motion as a distribution of kinetic energy m bar , both normalized within each bar. We align them using Earth Mover's Distance (EMD)  [12] :\n\nwhich measures the minimal effort to transform the audio accent distribution into the motion energy distribution and thus naturally captures flexible bar-level rhythmic shifts. The combined SRAL is:\n\nwhere λ beat and λ bar balance beat-and bar-level terms, set to 0.9 and 0.2, respectively.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Overall Loss",
      "text": "Finally, the total training objective combines ECL and SRAL:\n\nwhere α controls the trade-off between contrastive alignment and structural rhythm alignment, and is empirically set to 0.2 in our experiments.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Model Architecture",
      "text": "MotionBeat, illustrated in Fig.  1 , consists of two modalityspecific encoders and a lightweight set of task heads that feed the losses in Sec. 2.1. The audio encoder f a maps each beatsynchronous audio segment to an embedding z a ∈ R d using N blocks that integrate phase rotation, self-attention, and layer normalization. Similarly, the motion encoder f m maps temporally aligned motion segments to embeddings z m ∈ R d through M blocks combining phase rotation, contact-guided attention, and layer normalization. Two projection heads then map the encoder states into a shared contrastive space for ECL. In parallel, we detect audio onsets and motion contacts to compute onset envelopes, bar accent mass, contact pulses, and bar energy mass, which serve as inputs to SRAL.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Input Representation",
      "text": "All inputs are represented in a beat-synchronous form. Rather than processing at the raw frame level, we segment both audio and motion into K consecutive beat intervals, using estimated tempo and downbeat information. For the audio input, we extract a log-mel spectrogram for each beat interval and apply average pooling across it. This yields one audio token x a t ∈ R F per beat, where t = 1, . . . , K. Bar-phase encodings are added as extra positional features. For motion, we start from 3D body joints and SMPL  [13]  parameters. We compute perframe kinematic features, then average-pool them within each MLP Layer beat interval. This produces one motion token x m t ∈ R M per beat, aligned with the corresponding audio token.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Bar-Equivariant Phase Rotations",
      "text": "Within a bar of B beats, we want embeddings to transform equivariantly under cyclic phase shifts. Let ϕ t = 2π(t mod B)/B denote the bar phase of token t. For each attention head, we pair channels into 2D planes and apply a complex rotation to queries and keys before the dot-product:\n\nwhere R(ϕ) applies (cos ϕ, -sin ϕ; sin ϕ, cos ϕ) to each 2D channel pair. A phase shift by ∆ beats (∆ϕ = 2π∆/B) induces the same rotation on all tokens, so a cyclic shift in time corresponds to a rotation in latent space. This enforces within-bar consistency and improves generalization to different bar starting points.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Contact-Guided Attention",
      "text": "Let r t ∈ [0, 1] be a contact probability at token t. We bias attention toward tokens that coincide with contacts. For each query position t and key position u, the attention weights are defined as:\n\nwhere d h is the dimensionality of each attention head and α logit ≥ 0 is a learnable scalar controlling logit bias. In addition, value vectors are reweighted as:\n\nwith learnable α val ≥ 0. This allocates representational bandwidth to embodied anchors of rhythm without discarding noncontact frames. During training, r t can be predicted by a small contact head.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments And Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dataset And Implementation Details",
      "text": "We use the AIST++ dataset  [7]  of paired music-dance recordings with 3D skeleton annotations, representing audio as beatsynchronous log-mel tokens (128 bands) and motion as normalized SMPL kinematics with contact cues. Models are implemented on a single A6000 GPU using 6-layer Transformers (512 hidden units, 8 heads, 128-dimensional embeddings) and trained with AdamW  [16]  (lr 2 × 10 -4 , batch size 64, τ = 0.07) for up to 100 epochs with early stopping.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Downstream Tasks",
      "text": "We evaluate MotionBeat on dance generation as the primary task and on a broad set of recognition tasks. For dance generation, we assess choreography quality along three axes: physical plausibility via the Physical Foot Contact (PFC) score  [6] , diversity  [17, 7]  using distributional spreads in kinetic (Dist k ) and geometric (Dist g ) feature spaces, and music-motion alignment with the Beat Alignment Score (BAS)  [17] . Beat tracking is evaluated on the GTZAN  [18]  dataset using standard metrics  [19] : F1, CML t , and AML t .\n\nFor music tagging, we use the MagnaTagATune (MTT)  [20]  dataset with ROC and AP as evaluation metrics. Genre classification is performed on the GTZAN dataset with accuracy (ACC) as the measure. Instrument classification is conducted on NSynth  [21] , also using ACC. Finally, emotion regression is evaluated on the Emomusic  [22]  dataset, using the coefficient of determination (R 2 ) for arousal and valence.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Baseline Methods",
      "text": "We benchmark MotionBeat against four representative audio encoders that cover different learning paradigms. wav2vec 2.0  [14]  is a self-supervised model pretrained on large-scale speech and audio, providing general-purpose representations. CLAP  [2]  aligns audio and text in a joint embedding space, Method Dance Generation Beat Tracking Music Tagging Genre Classification Instrument Classification Emotion Recognition  Table  3 . Ablation of architectural components on key metrics: beat alignment (BAS), rhythmic accuracy (Beat F1), physical plausibility (PFC), and cross-modal retrieval (Music→Motion R@1). BEP = Bar-Equivariant Phase Rotations; CGA = Contact-Guided Attention.\n\nexcelling at semantic transfer. Wav2CLIP  [4]  aligns audio with vision-language features from CLIP, offering multimodal grounding and cross-domain generality. Jukebox  [15]  is trained on large-scale music data with a hierarchical VQ-VAE, providing music-specific representations that capture pitch, timbre, and long-term structure. For fairness, we use the same protocol across tasks. All baseline encoders are frozen, and only lightweight heads are trained: a small regressor for beat tracking and emotion recognition, a linear classifier for genre and instrument classification, and a multilabel head for music tagging. For cross-modal retrieval, we add a projection head to map embeddings into a shared space while keeping the backbone fixed. For dance generation, we use EDGE  [6]  to generate dance. This ensures that performance differences reflect the quality of the embeddings rather than task-specific fine-tuning.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Result Analysis",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "We present MotionBeat, a framework for motion-aligned music representation learning. Through novel objectives and rhythm-aware architectural designs, MotionBeat captures embodied rhythmic structure and consistently outperforms strong audio encoders on dance generation and a range of recognition tasks. This demonstrates the value of motion as a supervisory signal for learning music representations.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , consists of two modality-",
      "page": 2
    },
    {
      "caption": "Figure 1: Architecture of MotionBeat. Audio and motion in-",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "School of Computer Science, The University of Sydney, Australia": "govern dance.\nTo achieve this, we introduce two novel ob-"
        },
        {
          "School of Computer Science, The University of Sydney, Australia": "jectives: Embodied Contrastive Loss (ECL), which enhances"
        },
        {
          "School of Computer Science, The University of Sydney, Australia": ""
        },
        {
          "School of Computer Science, The University of Sydney, Australia": "InfoNCE [10] with tempo-aware and beat-jitter negatives for"
        },
        {
          "School of Computer Science, The University of Sydney, Australia": ""
        },
        {
          "School of Computer Science, The University of Sydney, Australia": "fine-grained rhythmic discrimination, and Structural Rhythm"
        },
        {
          "School of Computer Science, The University of Sydney, Australia": ""
        },
        {
          "School of Computer Science, The University of Sydney, Australia": "Alignment Loss (SRAL), which enforces beat- and bar-level"
        },
        {
          "School of Computer Science, The University of Sydney, Australia": ""
        },
        {
          "School of Computer Science, The University of Sydney, Australia": "consistency via Soft-DTW [11] alignment between audio on-"
        },
        {
          "School of Computer Science, The University of Sydney, Australia": ""
        },
        {
          "School of Computer Science, The University of Sydney, Australia": "sets and motion contacts, and optimal transport alignment be-"
        },
        {
          "School of Computer Science, The University of Sydney, Australia": ""
        },
        {
          "School of Computer Science, The University of Sydney, Australia": "tween accent mass and motion energy."
        },
        {
          "School of Computer Science, The University of Sydney, Australia": ""
        },
        {
          "School of Computer Science, The University of Sydney, Australia": "From an architectural perspective, our MotionBeat relies"
        },
        {
          "School of Computer Science, The University of Sydney, Australia": ""
        },
        {
          "School of Computer Science, The University of Sydney, Australia": "on two fundamental mechanisms tailored for embodied music"
        },
        {
          "School of Computer Science, The University of Sydney, Australia": ""
        },
        {
          "School of Computer Science, The University of Sydney, Australia": "representation. The first, bar-equivariant phase rotations, en-"
        },
        {
          "School of Computer Science, The University of Sydney, Australia": ""
        },
        {
          "School of Computer Science, The University of Sydney, Australia": "code the cyclic nature of rhythm by applying rotational trans-"
        },
        {
          "School of Computer Science, The University of Sydney, Australia": ""
        },
        {
          "School of Computer Science, The University of Sydney, Australia": "formations to embeddings.\nThis enforces structural consis-"
        },
        {
          "School of Computer Science, The University of Sydney, Australia": ""
        },
        {
          "School of Computer Science, The University of Sydney, Australia": "tency under phase shifts and makes representations robust\nto"
        },
        {
          "School of Computer Science, The University of Sydney, Australia": ""
        },
        {
          "School of Computer Science, The University of Sydney, Australia": "different bar starting points. The second, contact-guided at-"
        },
        {
          "School of Computer Science, The University of Sydney, Australia": ""
        },
        {
          "School of Computer Science, The University of Sydney, Australia": "tention, highlights motion frames where embodied events oc-"
        },
        {
          "School of Computer Science, The University of Sydney, Australia": ""
        },
        {
          "School of Computer Science, The University of Sydney, Australia": "cur by weighting attention scores with contact probabilities."
        },
        {
          "School of Computer Science, The University of Sydney, Australia": ""
        },
        {
          "School of Computer Science, The University of Sydney, Australia": "This directs more representational capacity to beats aligned"
        },
        {
          "School of Computer Science, The University of Sydney, Australia": ""
        },
        {
          "School of Computer Science, The University of Sydney, Australia": "with musical accents,\nstrengthening the audio–motion cou-"
        },
        {
          "School of Computer Science, The University of Sydney, Australia": ""
        },
        {
          "School of Computer Science, The University of Sydney, Australia": "pling."
        },
        {
          "School of Computer Science, The University of Sydney, Australia": ""
        },
        {
          "School of Computer Science, The University of Sydney, Australia": "We\nevaluate MotionBeat\nacross\nboth\ngenerative\nand"
        },
        {
          "School of Computer Science, The University of Sydney, Australia": ""
        },
        {
          "School of Computer Science, The University of Sydney, Australia": "recognition tasks.\nIn music-to-dance generation,\nit surpasses"
        },
        {
          "School of Computer Science, The University of Sydney, Australia": ""
        },
        {
          "School of Computer Science, The University of Sydney, Australia": "state-of-the-art audio encoders, producing motion that is more"
        },
        {
          "School of Computer Science, The University of Sydney, Australia": ""
        },
        {
          "School of Computer Science, The University of Sydney, Australia": "rhythmically aligned and structurally consistent. Moreover,"
        },
        {
          "School of Computer Science, The University of Sydney, Australia": "MotionBeat transfers effectively to downstream tasks such as"
        },
        {
          "School of Computer Science, The University of Sydney, Australia": "beat tracking, music tagging, genre and instrument classifica-"
        },
        {
          "School of Computer Science, The University of Sydney, Australia": "tion, emotion recognition, and audio–visual retrieval."
        },
        {
          "School of Computer Science, The University of Sydney, Australia": "Our contributions can be summarized as follows:"
        },
        {
          "School of Computer Science, The University of Sydney, Australia": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "tion, emotion recognition, and audio–visual retrieval.": "Our contributions can be summarized as follows:"
        },
        {
          "tion, emotion recognition, and audio–visual retrieval.": ""
        },
        {
          "tion, emotion recognition, and audio–visual retrieval.": "• We\nintroduce MotionBeat,\na\nframework for motion-"
        },
        {
          "tion, emotion recognition, and audio–visual retrieval.": ""
        },
        {
          "tion, emotion recognition, and audio–visual retrieval.": "aligned music representation learning."
        },
        {
          "tion, emotion recognition, and audio–visual retrieval.": ""
        },
        {
          "tion, emotion recognition, and audio–visual retrieval.": ""
        },
        {
          "tion, emotion recognition, and audio–visual retrieval.": "• We propose\ntwo new training objectives, Embodied"
        },
        {
          "tion, emotion recognition, and audio–visual retrieval.": ""
        },
        {
          "tion, emotion recognition, and audio–visual retrieval.": "Contrastive Loss (ECL) and Structural Rhythm Align-"
        },
        {
          "tion, emotion recognition, and audio–visual retrieval.": ""
        },
        {
          "tion, emotion recognition, and audio–visual retrieval.": "ment Loss\n(SRAL),\nto\njointly\ncapture\nfine-grained"
        },
        {
          "tion, emotion recognition, and audio–visual retrieval.": ""
        },
        {
          "tion, emotion recognition, and audio–visual retrieval.": "rhythmic detail and global structural consistency."
        },
        {
          "tion, emotion recognition, and audio–visual retrieval.": ""
        },
        {
          "tion, emotion recognition, and audio–visual retrieval.": ""
        },
        {
          "tion, emotion recognition, and audio–visual retrieval.": "• We develop architectural\ninnovations,\nincluding bar-"
        },
        {
          "tion, emotion recognition, and audio–visual retrieval.": ""
        },
        {
          "tion, emotion recognition, and audio–visual retrieval.": "equivariant phase rotations and contact-guided atten-"
        },
        {
          "tion, emotion recognition, and audio–visual retrieval.": ""
        },
        {
          "tion, emotion recognition, and audio–visual retrieval.": "tion, that explicitly model cyclic rhythm and emphasize"
        },
        {
          "tion, emotion recognition, and audio–visual retrieval.": ""
        },
        {
          "tion, emotion recognition, and audio–visual retrieval.": "motion-synchronized events."
        },
        {
          "tion, emotion recognition, and audio–visual retrieval.": ""
        },
        {
          "tion, emotion recognition, and audio–visual retrieval.": "• Extensive experiments show that MotionBeat surpasses"
        },
        {
          "tion, emotion recognition, and audio–visual retrieval.": "state-of-the-art encoders in dance generation and gener-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "alizes to beat\ntracking,\ntagging, classification, emotion": "recognition, and cross-modal retrieval.",
          "which tolerates small deviations while rewarding synchrony": "at the beat level."
        },
        {
          "alizes to beat\ntracking,\ntagging, classification, emotion": "",
          "which tolerates small deviations while rewarding synchrony": "At the bar level, we treat audio as a distribution of accent"
        },
        {
          "alizes to beat\ntracking,\ntagging, classification, emotion": "",
          "which tolerates small deviations while rewarding synchrony": "mass abar and motion as a distribution of kinetic energy mbar,"
        },
        {
          "alizes to beat\ntracking,\ntagging, classification, emotion": "2. METHOD",
          "which tolerates small deviations while rewarding synchrony": ""
        },
        {
          "alizes to beat\ntracking,\ntagging, classification, emotion": "",
          "which tolerates small deviations while rewarding synchrony": "both normalized within each bar. We align them using Earth"
        },
        {
          "alizes to beat\ntracking,\ntagging, classification, emotion": "2.1. Training Objective",
          "which tolerates small deviations while rewarding synchrony": "Mover’s Distance (EMD) [12]:"
        },
        {
          "alizes to beat\ntracking,\ntagging, classification, emotion": "Standard contrastive learning usually relies on random in-",
          "which tolerates small deviations while rewarding synchrony": "(4)\nLbar = EMD(abar, mbar),"
        },
        {
          "alizes to beat\ntracking,\ntagging, classification, emotion": "batch negatives.\nIn music–motion alignment,\nthese are too",
          "which tolerates small deviations while rewarding synchrony": ""
        },
        {
          "alizes to beat\ntracking,\ntagging, classification, emotion": "",
          "which tolerates small deviations while rewarding synchrony": "which measures the minimal effort to transform the audio ac-"
        },
        {
          "alizes to beat\ntracking,\ntagging, classification, emotion": "easy since negatives often differ\nin genre,\ntimbre, or\ninstru-",
          "which tolerates small deviations while rewarding synchrony": ""
        },
        {
          "alizes to beat\ntracking,\ntagging, classification, emotion": "",
          "which tolerates small deviations while rewarding synchrony": "cent distribution into the motion energy distribution and thus"
        },
        {
          "alizes to beat\ntracking,\ntagging, classification, emotion": "mentation,\nletting the model\nrely on global\nacoustic\ncues",
          "which tolerates small deviations while rewarding synchrony": ""
        },
        {
          "alizes to beat\ntracking,\ntagging, classification, emotion": "",
          "which tolerates small deviations while rewarding synchrony": "naturally captures flexible bar-level rhythmic shifts."
        },
        {
          "alizes to beat\ntracking,\ntagging, classification, emotion": "while ignoring rhythmic alignment.",
          "which tolerates small deviations while rewarding synchrony": ""
        },
        {
          "alizes to beat\ntracking,\ntagging, classification, emotion": "",
          "which tolerates small deviations while rewarding synchrony": "The combined SRAL is:"
        },
        {
          "alizes to beat\ntracking,\ntagging, classification, emotion": "2.1.1. Embodied Contrastive Loss",
          "which tolerates small deviations while rewarding synchrony": ""
        },
        {
          "alizes to beat\ntracking,\ntagging, classification, emotion": "",
          "which tolerates small deviations while rewarding synchrony": "(5)\nLSRAL = λbeat Lbeat + λbar Lbar,"
        },
        {
          "alizes to beat\ntracking,\ntagging, classification, emotion": "Embodied Contrastive Loss (ECL) extends the standard In-",
          "which tolerates small deviations while rewarding synchrony": ""
        },
        {
          "alizes to beat\ntracking,\ntagging, classification, emotion": "",
          "which tolerates small deviations while rewarding synchrony": "terms, set\nto\nwhere λbeat and λbar balance beat- and bar-level"
        },
        {
          "alizes to beat\ntracking,\ntagging, classification, emotion": "foNCE [10] objective by introducing rhythm-sensitive neg-",
          "which tolerates small deviations while rewarding synchrony": ""
        },
        {
          "alizes to beat\ntracking,\ntagging, classification, emotion": "",
          "which tolerates small deviations while rewarding synchrony": "0.9 and 0.2, respectively."
        },
        {
          "alizes to beat\ntracking,\ntagging, classification, emotion": "atives.\nTempo-aware negatives\nshare\na\nsimilar BPM but",
          "which tolerates small deviations while rewarding synchrony": ""
        },
        {
          "alizes to beat\ntracking,\ntagging, classification, emotion": "differ\nin bar-phase or accent patterns,\nforcing the model\nto",
          "which tolerates small deviations while rewarding synchrony": ""
        },
        {
          "alizes to beat\ntracking,\ntagging, classification, emotion": "",
          "which tolerates small deviations while rewarding synchrony": "2.1.3. Overall Loss"
        },
        {
          "alizes to beat\ntracking,\ntagging, classification, emotion": "look beyond global\ntempo.\nBeat-jitter negatives\nare gen-",
          "which tolerates small deviations while rewarding synchrony": ""
        },
        {
          "alizes to beat\ntracking,\ntagging, classification, emotion": "erated by shifting motion or audio by ±1 beat within the",
          "which tolerates small deviations while rewarding synchrony": "Finally, the total training objective combines ECL and SRAL:"
        },
        {
          "alizes to beat\ntracking,\ntagging, classification, emotion": "same clip, keeping acoustics and style intact but disrupting",
          "which tolerates small deviations while rewarding synchrony": ""
        },
        {
          "alizes to beat\ntracking,\ntagging, classification, emotion": "",
          "which tolerates small deviations while rewarding synchrony": "(6)\nLtotal = LECL + α LSRAL,"
        },
        {
          "alizes to beat\ntracking,\ntagging, classification, emotion": "step timing. Given a batch of N paired music–motion clips",
          "which tolerates small deviations while rewarding synchrony": ""
        },
        {
          "alizes to beat\ntracking,\ntagging, classification, emotion": "{(zi",
          "which tolerates small deviations while rewarding synchrony": ""
        },
        {
          "alizes to beat\ntracking,\ntagging, classification, emotion": "a has one positive (its paired\ni=1, each anchor zi",
          "which tolerates small deviations while rewarding synchrony": ""
        },
        {
          "alizes to beat\ntracking,\ntagging, classification, emotion": "",
          "which tolerates small deviations while rewarding synchrony": "where α controls the trade-off between contrastive alignment"
        },
        {
          "alizes to beat\ntracking,\ntagging, classification, emotion": "motion embedding zi\nm) and multiple categories of negatives.",
          "which tolerates small deviations while rewarding synchrony": ""
        },
        {
          "alizes to beat\ntracking,\ntagging, classification, emotion": "",
          "which tolerates small deviations while rewarding synchrony": "and structural rhythm alignment, and is empirically set to 0.2"
        },
        {
          "alizes to beat\ntracking,\ntagging, classification, emotion": "The denominator Di\nis defined as:",
          "which tolerates small deviations while rewarding synchrony": ""
        },
        {
          "alizes to beat\ntracking,\ntagging, classification, emotion": "",
          "which tolerates small deviations while rewarding synchrony": "in our experiments."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 2: Cross-modal retrieval between music and motion MotionBeat achieves the best F1 and AML , while Jukebox",
      "data": [
        {
          "Dance Generation": "",
          "Beat Tracking": "",
          "Music Tagging": "",
          "Genre Classification": "",
          "Instrument Classification": "",
          "Emotion Recognition": ""
        },
        {
          "Dance Generation": "Distg ↑",
          "Beat Tracking": "CMLt ↑",
          "Music Tagging": "ROC ↑",
          "Genre Classification": "ACC ↑",
          "Instrument Classification": "ACC ↑",
          "Emotion Recognition": "R2A ↑"
        },
        {
          "Dance Generation": "5.56",
          "Beat Tracking": "0.791",
          "Music Tagging": "89.1",
          "Genre Classification": "68.6",
          "Instrument Classification": "66.7",
          "Emotion Recognition": "66.5"
        },
        {
          "Dance Generation": "5.75",
          "Beat Tracking": "0.795",
          "Music Tagging": "88.8",
          "Genre Classification": "69.3",
          "Instrument Classification": "65.8",
          "Emotion Recognition": "69.8"
        },
        {
          "Dance Generation": "6.39",
          "Beat Tracking": "0.801",
          "Music Tagging": "89.5",
          "Genre Classification": "71.2",
          "Instrument Classification": "67.2",
          "Emotion Recognition": "70.1"
        },
        {
          "Dance Generation": "7.03",
          "Beat Tracking": "0.804",
          "Music Tagging": "90.5",
          "Genre Classification": "78.5",
          "Instrument Classification": "68.8",
          "Emotion Recognition": "72.1"
        },
        {
          "Dance Generation": "7.55",
          "Beat Tracking": "0.801",
          "Music Tagging": "90.4",
          "Genre Classification": "77.9",
          "Instrument Classification": "68.9",
          "Emotion Recognition": "71.5"
        },
        {
          "Dance Generation": "7.65",
          "Beat Tracking": "0.799",
          "Music Tagging": "89.8",
          "Genre Classification": "78.1",
          "Instrument Classification": "68.2",
          "Emotion Recognition": "72.3"
        },
        {
          "Dance Generation": "7.89",
          "Beat Tracking": "0.803",
          "Music Tagging": "91.2",
          "Genre Classification": "79.2",
          "Instrument Classification": "70.8",
          "Emotion Recognition": "73.8"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: Cross-modal retrieval between music and motion MotionBeat achieves the best F1 and AML , while Jukebox",
      "data": [
        {
          "on AIST++. Retrieval": "Recall@K and Median Rank.",
          "is evaluated in both directions using": ""
        },
        {
          "on AIST++. Retrieval": "Model Variant",
          "is evaluated in both directions using": "Beat F1 ↑"
        },
        {
          "on AIST++. Retrieval": "Baseline (no BEP, no CGA)",
          "is evaluated in both directions using": "0.852"
        },
        {
          "on AIST++. Retrieval": "+ BEP only",
          "is evaluated in both directions using": "0.866"
        },
        {
          "on AIST++. Retrieval": "+ CGA only",
          "is evaluated in both directions using": "0.871"
        },
        {
          "on AIST++. Retrieval": "+ BEP & CGA",
          "is evaluated in both directions using": "0.878"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: Cross-modal retrieval between music and motion MotionBeat achieves the best F1 and AML , while Jukebox",
      "data": [
        {
          "ment Loss, respectively.": "Music → Motion\nMotion → Music\nMethod"
        },
        {
          "ment Loss, respectively.": "R@1 ↑\nR@5 ↑\nR@10 ↑ MedR ↓\nR@1 ↑\nR@5 ↑\nR@10 ↑ MedR ↓"
        },
        {
          "ment Loss, respectively.": ""
        },
        {
          "ment Loss, respectively.": "wav2vec 2.0 [14]\n13.7\n31.2\n44.5\n39\n13.1\n30.8\n43.8\n40"
        },
        {
          "ment Loss, respectively.": "CLAP [2]\n15.6\n33.9\n47.2\n36\n15.2\n33.1\n46.7\n37"
        },
        {
          "ment Loss, respectively.": "Wav2CLIP [4]\n16.2\n34.7\n48.6\n34\n15.8\n34.0\n47.9\n35"
        },
        {
          "ment Loss, respectively.": "Jukebox [15]\n19.8\n40.5\n53.2\n27\n18.8\n38.9\n52.5\n29"
        },
        {
          "ment Loss, respectively.": "22.1\n42.5\n58.3\n22\n21.7\n41.9\n57.6\n23\nMotionBeat"
        },
        {
          "ment Loss, respectively.": ""
        },
        {
          "ment Loss, respectively.": "Table 2.\nCross-modal\nretrieval between music and motion"
        },
        {
          "ment Loss, respectively.": "on AIST++. Retrieval\nis evaluated in both directions using"
        },
        {
          "ment Loss, respectively.": "Recall@K and Median Rank."
        },
        {
          "ment Loss, respectively.": ""
        },
        {
          "ment Loss, respectively.": "Model Variant\nBAS ↑\nBeat F1 ↑\nPFC ↓\nR@1 (M → Mtn) ↑"
        },
        {
          "ment Loss, respectively.": ""
        },
        {
          "ment Loss, respectively.": "Baseline (no BEP, no CGA)\n0.24\n0.852\n1.60\n19.3"
        },
        {
          "ment Loss, respectively.": ""
        },
        {
          "ment Loss, respectively.": "+ BEP only\n0.26\n0.866\n1.57\n20.8"
        },
        {
          "ment Loss, respectively.": "+ CGA only\n0.25\n0.871\n1.56\n20.3"
        },
        {
          "ment Loss, respectively.": "0.27\n0.878\n1.55\n22.1\n+ BEP & CGA"
        },
        {
          "ment Loss, respectively.": ""
        },
        {
          "ment Loss, respectively.": ""
        },
        {
          "ment Loss, respectively.": "Table\n3.\nAblation\nof\narchitectural\ncomponents\non\nkey"
        },
        {
          "ment Loss, respectively.": ""
        },
        {
          "ment Loss, respectively.": "metrics:\nbeat\nalignment\n(BAS),\nrhythmic\naccuracy\n(Beat"
        },
        {
          "ment Loss, respectively.": ""
        },
        {
          "ment Loss, respectively.": "F1), physical plausibility (PFC), and cross-modal\nretrieval"
        },
        {
          "ment Loss, respectively.": ""
        },
        {
          "ment Loss, respectively.": "(Music→Motion R@1). BEP = Bar-Equivariant Phase Ro-"
        },
        {
          "ment Loss, respectively.": ""
        },
        {
          "ment Loss, respectively.": "tations; CGA = Contact-Guided Attention."
        },
        {
          "ment Loss, respectively.": ""
        },
        {
          "ment Loss, respectively.": ""
        },
        {
          "ment Loss, respectively.": ""
        },
        {
          "ment Loss, respectively.": "excelling at\nsemantic transfer. Wav2CLIP [4] aligns audio"
        },
        {
          "ment Loss, respectively.": ""
        },
        {
          "ment Loss, respectively.": "with\nvision–language\nfeatures\nfrom CLIP,\noffering multi-"
        },
        {
          "ment Loss, respectively.": ""
        },
        {
          "ment Loss, respectively.": "modal grounding and cross-domain generality.\nJukebox [15]"
        },
        {
          "ment Loss, respectively.": ""
        },
        {
          "ment Loss, respectively.": "is trained on large-scale music data with a hierarchical VQ-"
        },
        {
          "ment Loss, respectively.": ""
        },
        {
          "ment Loss, respectively.": "VAE, providing music-specific representations\nthat capture"
        },
        {
          "ment Loss, respectively.": ""
        },
        {
          "ment Loss, respectively.": "pitch,\ntimbre, and long-term structure.\nFor\nfairness, we use"
        },
        {
          "ment Loss, respectively.": ""
        },
        {
          "ment Loss, respectively.": "the same protocol across\ntasks.\nAll baseline encoders are"
        },
        {
          "ment Loss, respectively.": ""
        },
        {
          "ment Loss, respectively.": "frozen, and only lightweight heads are trained:\na small\nre-"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5. REFERENCES": "",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": "and Michael Auli,\n“wav2vec 2.0: A framework for"
        },
        {
          "5. REFERENCES": "[1] Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Is-",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": "self-supervised learning of speech representations,”\nin"
        },
        {
          "5. REFERENCES": "mail, and Huaming Wang,\n“Clap: Learning audio con-",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": "NeurIPS, 2020."
        },
        {
          "5. REFERENCES": "cepts from natural\nlanguage supervision,”\nin ICASSP,",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": ""
        },
        {
          "5. REFERENCES": "",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": "[15] Prafulla\nDhariwal,\nHeewoo\nJun,\nChristine\nPayne,"
        },
        {
          "5. REFERENCES": "2023.",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": ""
        },
        {
          "5. REFERENCES": "",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": "Jong Wook Kim, Alec Radford,\nand Ilya Sutskever,"
        },
        {
          "5. REFERENCES": "[2] Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Tay-",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": "arXiv"
        },
        {
          "5. REFERENCES": "",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": "“Jukebox:\nA generative model\nfor music,”"
        },
        {
          "5. REFERENCES": "lor Berg-Kirkpatrick, and Shlomo Dubnov, “Large-scale",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": ""
        },
        {
          "5. REFERENCES": "",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": "preprint arXiv:2005.00341, 2020."
        },
        {
          "5. REFERENCES": "contrastive language-audio pretraining with feature fu-",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": ""
        },
        {
          "5. REFERENCES": "sion and keyword-to-caption augmentation,” in ICASSP,",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": "[16]\nIlya\nLoshchilov\nand\nFrank\nHutter,\n“Decoupled"
        },
        {
          "5. REFERENCES": "",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": "arXiv\npreprint\nweight\ndecay\nregularization,”"
        },
        {
          "5. REFERENCES": "2023.",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": ""
        },
        {
          "5. REFERENCES": "",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": "arXiv:1711.05101, 2017."
        },
        {
          "5. REFERENCES": "[3] Benjamin Elizalde, Soham Deshmukh,\nand Huaming",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": ""
        },
        {
          "5. REFERENCES": "Wang,\n“Natural\nlanguage\nsupervision\nfor\ngeneral-",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": "[17] Li Siyao, Weijiang Yu, Tianpei Gu, Chunze Lin, Quan"
        },
        {
          "5. REFERENCES": "purpose audio representations,” in ICASSP, 2024.",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": "Wang, Chen Qian, Chen Change Loy, and Ziwei Liu,"
        },
        {
          "5. REFERENCES": "",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": "“Bailando: 3d dance generation by actor-critic gpt with"
        },
        {
          "5. REFERENCES": "[4] Ho-Hsiang Wu, Prem Seetharaman, Kundan Kumar, and",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": ""
        },
        {
          "5. REFERENCES": "",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": "choreographic memory,” in CVPR, 2022."
        },
        {
          "5. REFERENCES": "Juan Pablo Bello,\n“Wav2clip: Learning robust audio",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": ""
        },
        {
          "5. REFERENCES": "representations from clip,” in ICASSP, 2022.",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": "[18] George Tzanetakis and Perry Cook,\n“Musical genre"
        },
        {
          "5. REFERENCES": "",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": "classification of audio signals,” IEEE Trans. Speech Au-"
        },
        {
          "5. REFERENCES": "[5] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Man-",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": ""
        },
        {
          "5. REFERENCES": "",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": "dio Process., 2002."
        },
        {
          "5. REFERENCES": "nat Singh, Kalyan Vasudev Alwala, Armand Joulin, and",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": ""
        },
        {
          "5. REFERENCES": "Ishan Misra, “Imagebind: One embedding space to bind",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": "[19] Matthew EP Davies, Norberto Degara,\nand Mark D"
        },
        {
          "5. REFERENCES": "them all,” in CVPR, 2023.",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": "Plumbley,\n“Evaluation methods for musical audio beat"
        },
        {
          "5. REFERENCES": "",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": "tracking algorithms,” QMUL Tech. Rep. C4DM-TR-09-"
        },
        {
          "5. REFERENCES": "[6]\nJonathan Tseng, Rodrigo Castellon,\nand Karen Liu,",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": ""
        },
        {
          "5. REFERENCES": "",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": "06, 2009."
        },
        {
          "5. REFERENCES": "“Edge:\nEditable dance generation from music,”\nin",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": ""
        },
        {
          "5. REFERENCES": "CVPR, 2023.",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": "[20] Edith Law, Kris West, Michael\nI Mandel, Mert Bay,"
        },
        {
          "5. REFERENCES": "",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": "and J Stephen Downie,\n“Evaluation of algorithms us-"
        },
        {
          "5. REFERENCES": "[7] Ruilong Li, Shan Yang, David A Ross,\nand Angjoo",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": ""
        },
        {
          "5. REFERENCES": "",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": "ing games: The case of music tagging,” in ISMIR, 2009."
        },
        {
          "5. REFERENCES": "Kanazawa,\n“Ai choreographer: Music conditioned 3d",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": ""
        },
        {
          "5. REFERENCES": "dance generation with aist++,” in ICCV, 2021.",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": "[21]\nJesse Engel, Cinjon Resnick, Adam Roberts, Sander"
        },
        {
          "5. REFERENCES": "",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": "Dieleman, Mohammad Norouzi, Douglas Eck,\nand"
        },
        {
          "5. REFERENCES": "[8] Xuanchen Wang,\nHeng Wang,\nand Weidong\nCai,",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": ""
        },
        {
          "5. REFERENCES": "",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": "Karen Simonyan,\n“Neural audio synthesis of musical"
        },
        {
          "5. REFERENCES": "“Choreomuse: Robust music-to-dance video generation",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": ""
        },
        {
          "5. REFERENCES": "",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": "notes with wavenet autoencoders,” in ICML, 2017."
        },
        {
          "5. REFERENCES": "with style transfer and beat-adherent motion,”\nin ACM",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": ""
        },
        {
          "5. REFERENCES": "MM, 2025.",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": "[22] Mohammad\nSoleymani, Micheal N Caro,\nErik M"
        },
        {
          "5. REFERENCES": "",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": "Schmidt, Cheng-Ya Sha, and Yi-Hsuan Yang,\n“1000"
        },
        {
          "5. REFERENCES": "[9] Xuanchen Wang, Heng Wang, Dongnan Liu, and Wei-",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": ""
        },
        {
          "5. REFERENCES": "",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": "songs for emotional analysis of music,”\nin ACM Work-"
        },
        {
          "5. REFERENCES": "dong Cai, “Dance any beat: Blending beats with visuals",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": ""
        },
        {
          "5. REFERENCES": "",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": "shop on Crowdsourcing for Multimedia, 2013."
        },
        {
          "5. REFERENCES": "in dance video generation,” in WACV, 2025.",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": ""
        },
        {
          "5. REFERENCES": "[10] Aaron van den Oord, Yazhe Li, and Oriol Vinyals, “Rep-",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": ""
        },
        {
          "5. REFERENCES": "resentation learning with contrastive predictive coding,”",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": ""
        },
        {
          "5. REFERENCES": "arXiv preprint arXiv:1807.03748, 2018.",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": ""
        },
        {
          "5. REFERENCES": "[11] Marco Cuturi and Mathieu Blondel, “Soft-dtw: A differ-",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": ""
        },
        {
          "5. REFERENCES": "entiable loss function for time-series,” in ICML, 2017.",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": ""
        },
        {
          "5. REFERENCES": "[12] Yossi Rubner, Carlo Tomasi,\nand Leonidas\nJ Guibas,",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": ""
        },
        {
          "5. REFERENCES": "“The earth mover’s distance as a metric for\nimage re-",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": ""
        },
        {
          "5. REFERENCES": "International\nJournal\ntrieval,”\nof Computer Vision,",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": ""
        },
        {
          "5. REFERENCES": "2000.",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": ""
        },
        {
          "5. REFERENCES": "[13] Matthew Loper, Naureen Mahmood,\nJavier Romero,",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": ""
        },
        {
          "5. REFERENCES": "Gerard Pons-Moll,\nand Michael\nJ Black,\n“Smpl: A",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": ""
        },
        {
          "5. REFERENCES": "skinned multi-person linear model,” in Seminal Graph-",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": ""
        },
        {
          "5. REFERENCES": "ics Papers, Vol. 2. 2023.",
          "[14] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Clap: Learning audio concepts from natural language supervision",
      "authors": [
        "Benjamin Elizalde",
        "Soham Deshmukh"
      ],
      "year": "2023",
      "venue": "ICASSP"
    },
    {
      "citation_id": "3",
      "title": "Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation",
      "authors": [
        "Yusong Wu",
        "Ke Chen",
        "Tianyu Zhang",
        "Yuchen Hui",
        "Taylor Berg-Kirkpatrick",
        "Shlomo Dubnov"
      ],
      "year": "2023",
      "venue": "ICASSP"
    },
    {
      "citation_id": "4",
      "title": "Natural language supervision for generalpurpose audio representations",
      "authors": [
        "Benjamin Elizalde",
        "Soham Deshmukh",
        "Huaming Wang"
      ],
      "year": "2024",
      "venue": "ICASSP"
    },
    {
      "citation_id": "5",
      "title": "Wav2clip: Learning robust audio representations from clip",
      "authors": [
        "Ho-Hsiang Wu",
        "Prem Seetharaman",
        "Kundan Kumar",
        "Juan Bello"
      ],
      "year": "2022",
      "venue": "ICASSP"
    },
    {
      "citation_id": "6",
      "title": "Imagebind: One embedding space to bind them all",
      "authors": [
        "Rohit Girdhar",
        "Alaaeldin El-Nouby",
        "Zhuang Liu",
        "Mannat Singh",
        "Kalyan Vasudev Alwala",
        "Armand Joulin",
        "Ishan Misra"
      ],
      "year": "2023",
      "venue": "CVPR"
    },
    {
      "citation_id": "7",
      "title": "Edge: Editable dance generation from music",
      "authors": [
        "Jonathan Tseng",
        "Rodrigo Castellon",
        "Karen Liu"
      ],
      "year": "2023",
      "venue": "CVPR"
    },
    {
      "citation_id": "8",
      "title": "Ai choreographer: Music conditioned 3d dance generation with aist++",
      "authors": [
        "Ruilong Li",
        "Shan Yang",
        "David Ross",
        "Angjoo Kanazawa"
      ],
      "year": "2021",
      "venue": "ICCV"
    },
    {
      "citation_id": "9",
      "title": "Choreomuse: Robust music-to-dance video generation with style transfer and beat-adherent motion",
      "authors": [
        "Xuanchen Wang",
        "Heng Wang",
        "Weidong Cai"
      ],
      "year": "2025",
      "venue": "Choreomuse: Robust music-to-dance video generation with style transfer and beat-adherent motion"
    },
    {
      "citation_id": "10",
      "title": "Dance any beat: Blending beats with visuals in dance video generation",
      "authors": [
        "Xuanchen Wang",
        "Heng Wang",
        "Dongnan Liu",
        "Weidong Cai"
      ],
      "year": "2025",
      "venue": "Dance any beat: Blending beats with visuals in dance video generation"
    },
    {
      "citation_id": "11",
      "title": "Representation learning with contrastive predictive coding",
      "authors": [
        "Aaron Van Den Oord",
        "Yazhe Li",
        "Oriol Vinyals"
      ],
      "year": "2018",
      "venue": "Representation learning with contrastive predictive coding",
      "arxiv": "arXiv:1807.03748"
    },
    {
      "citation_id": "12",
      "title": "Soft-dtw: A differentiable loss function for time-series",
      "authors": [
        "Marco Cuturi",
        "Mathieu Blondel"
      ],
      "year": "2017",
      "venue": "ICML"
    },
    {
      "citation_id": "13",
      "title": "The earth mover's distance as a metric for image retrieval",
      "authors": [
        "Yossi Rubner",
        "Carlo Tomasi",
        "Leonidas Guibas"
      ],
      "year": "2000",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "14",
      "title": "Smpl: A skinned multi-person linear model",
      "authors": [
        "Matthew Loper",
        "Naureen Mahmood",
        "Javier Romero",
        "Gerard Pons-Moll",
        "Michael Black"
      ],
      "year": "2023",
      "venue": "Seminal Graphics Papers"
    },
    {
      "citation_id": "15",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "wav2vec 2.0: A framework for self-supervised learning of speech representations"
    },
    {
      "citation_id": "16",
      "title": "Jukebox: A generative model for music",
      "authors": [
        "Prafulla Dhariwal",
        "Heewoo Jun",
        "Christine Payne",
        "Jong Kim",
        "Alec Radford",
        "Ilya Sutskever"
      ],
      "year": "2020",
      "venue": "Jukebox: A generative model for music",
      "arxiv": "arXiv:2005.00341"
    },
    {
      "citation_id": "17",
      "title": "Decoupled weight decay regularization",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2017",
      "venue": "Decoupled weight decay regularization",
      "arxiv": "arXiv:1711.05101"
    },
    {
      "citation_id": "18",
      "title": "Bailando: 3d dance generation by actor-critic gpt with choreographic memory",
      "authors": [
        "Li Siyao",
        "Weijiang Yu",
        "Tianpei Gu",
        "Chunze Lin",
        "Quan Wang",
        "Chen Qian",
        "Chen Loy",
        "Ziwei Liu"
      ],
      "year": "2022",
      "venue": "CVPR"
    },
    {
      "citation_id": "19",
      "title": "Musical genre classification of audio signals",
      "authors": [
        "George Tzanetakis",
        "Perry Cook"
      ],
      "year": "2002",
      "venue": "IEEE Trans. Speech Audio Process"
    },
    {
      "citation_id": "20",
      "title": "Evaluation methods for musical audio beat tracking algorithms",
      "authors": [
        "Norberto Matthew Ep Davies",
        "Mark Degara",
        "Plumbley"
      ],
      "year": "2009",
      "venue": "QMUL Tech. Rep. C"
    },
    {
      "citation_id": "21",
      "title": "Evaluation of algorithms using games: The case of music tagging",
      "authors": [
        "Edith Law",
        "Kris West",
        "I Michael",
        "Mert Mandel",
        "J Bay",
        "Stephen Downie"
      ],
      "year": "2009",
      "venue": "Evaluation of algorithms using games: The case of music tagging"
    },
    {
      "citation_id": "22",
      "title": "Neural audio synthesis of musical notes with wavenet autoencoders",
      "authors": [
        "Jesse Engel",
        "Cinjon Resnick",
        "Adam Roberts",
        "Sander Dieleman",
        "Mohammad Norouzi",
        "Douglas Eck",
        "Karen Simonyan"
      ],
      "year": "2017",
      "venue": "ICML"
    },
    {
      "citation_id": "23",
      "title": "1000 songs for emotional analysis of music",
      "authors": [
        "Mohammad Soleymani",
        "Micheal Caro",
        "Erik Schmidt",
        "Cheng-Ya Sha",
        "Yi-Hsuan Yang"
      ],
      "year": "2013",
      "venue": "ACM Workshop on Crowdsourcing for Multimedia"
    }
  ]
}